{
  "cells": [
    {
      "id": "db59a147-c6bb-45ff-88b7-920eb91a7592",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## 1.2. Imports & Basic Setup\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm.auto import tqdm\n",
        "import gc\n",
        "import importlib\n",
        "import random\n",
        "\n",
        "import utils_preproc\n",
        "importlib.reload(utils_preproc) # Force reload to pick up changes\n",
        "from utils_preproc import load_and_preprocess\n",
        "\n",
        "# --- Determinism ---\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # if use multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Configuration: Pivoting to a full 5-fold CV run based on successful baseline\n",
        "class CFG:\n",
        "    # Execution control\n",
        "    run_single_fold = False # <-- SWITCHING TO FULL 5-FOLD CV\n",
        "    target_fold = 0\n",
        "    seed = 42\n",
        "    \n",
        "    # Paths\n",
        "    data_dir = '.'\n",
        "    train_path = os.path.join(data_dir, 'train')\n",
        "    train_labels_path = os.path.join(data_dir, 'train_labels.csv')\n",
        "    \n",
        "    # Preprocessing\n",
        "    preprocess_transform_type = 'asinh'\n",
        "    clip_percentiles = (0.1, 99.9)\n",
        "    \n",
        "    # Model\n",
        "    model_name = 'tf_efficientnet_b2_ns'\n",
        "    img_size = 256\n",
        "    in_channels = 3\n",
        "    num_classes = 1\n",
        "    \n",
        "    # Training\n",
        "    n_epochs = 15\n",
        "    batch_size = 32\n",
        "    n_folds = 5\n",
        "    \n",
        "    # Optimizer & Scheduler\n",
        "    lr = 3e-4\n",
        "    weight_decay = 1e-6\n",
        "    scheduler_type = 'OneCycleLR'\n",
        "    one_cycle_pct_start = 0.3\n",
        "    grad_clip_norm = 1.0\n",
        "    \n",
        "    # Loss & Early Stopping\n",
        "    use_sampler = False\n",
        "    loss_type = 'BCE' # Sticking with plain BCE as it worked well\n",
        "    pos_weight_val = 2.0\n",
        "    patience = 4\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# --- Apply Seed ---\n",
        "seed_everything(CFG.seed)\n",
        "\n",
        "print(f\"Using device: {CFG.device}\")\n",
        "print(f\"Torch: {torch.__version__}, Timm: {timm.__version__}, Albumentations: {A.__version__}\")\n",
        "\n",
        "print(\"--- STARTING FULL 5-FOLD CV RUN ---\")\n",
        "print(f\"Model: {CFG.model_name}, Img Size: {CFG.img_size}\")\n",
        "print(f\"Loss Type: {CFG.loss_type}\")\n",
        "print(f\"LR: {CFG.lr}, Epochs: {CFG.n_epochs}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "14b2bfbf-0338-40a7-b5ad-c3529979f9fa",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2. EDA & Data Preparation\n",
        "\n",
        "## 2.1. Load Labels and Prepare for CV\n",
        "\n",
        "df = pd.read_csv(CFG.train_labels_path)\n",
        "\n",
        "# Create a 'group' column for StratifiedGroupKFold\n",
        "# We group by the first three characters of the ID for a more granular split, as per expert advice.\n",
        "df['group'] = df['id'].str[:3]\n",
        "\n",
        "print(\"Train labels dataframe:\")\n",
        "print(df.head())\n",
        "print(f\"\\nShape: {df.shape}\")\n",
        "print(f\"\\nNumber of unique groups: {df['group'].nunique()}\")\n",
        "\n",
        "print(\"\\nTarget distribution:\")\n",
        "print(df['target'].value_counts(normalize=True))\n",
        "\n",
        "# Calculate pos_weight and store it in the config to avoid cell order bugs\n",
        "neg_count = df['target'].value_counts()[0]\n",
        "pos_count = df['target'].value_counts()[1]\n",
        "pos_weight_value = neg_count / pos_count\n",
        "CFG.calculated_pos_weight = float(pos_weight_value)\n",
        "print(f\"\\nCalculated positive class weight: {CFG.calculated_pos_weight:.2f}\")\n",
        "print(\"Stored in CFG.calculated_pos_weight\")\n",
        "\n",
        "def get_train_file_path(image_id):\n",
        "    return f\"{CFG.train_path}/{image_id[0]}/{image_id}.npy\"\n",
        "\n",
        "df['file_path'] = df['id'].apply(get_train_file_path)\n",
        "\n",
        "print(\"\\nDataframe with file paths:\")\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "8f501295-c80c-488c-935f-46bce3232648",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## 2.2. Dataset & Augmentations\n",
        "\n",
        "def get_transforms(*, data):\n",
        "    # Per expert advice, re-enabling HorizontalFlip for the full CV run.\n",
        "    if data == 'train':\n",
        "        return A.Compose([\n",
        "            A.Resize(CFG.img_size, CFG.img_size),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "    elif data == 'valid':\n",
        "        return A.Compose([\n",
        "            A.Resize(CFG.img_size, CFG.img_size),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "class SETIDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.file_paths = df['file_path'].values\n",
        "        self.labels = df['target'].values\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_paths[idx]\n",
        "        \n",
        "        # Use the centralized preprocessing function with options from CFG\n",
        "        image = load_and_preprocess(\n",
        "            file_path,\n",
        "            transform_type=CFG.preprocess_transform_type,\n",
        "            clip_percentiles=CFG.clip_percentiles\n",
        "        )\n",
        "        \n",
        "        # EXPERT ADVICE: Robustly handle potential CHW format before Albumentations (which expects HWC)\n",
        "        if image.ndim == 3 and image.shape[0] == 3:\n",
        "            # This condition suggests a CHW format, so we transpose it to HWC.\n",
        "            image = np.transpose(image, (1, 2, 0))\n",
        "        \n",
        "        # Final check to ensure the image is in HWC format for Albumentations\n",
        "        assert image.ndim == 3 and image.shape[2] == 3, f\"Unexpected image shape: {image.shape}. Expected (H, W, 3).\"\n",
        "        \n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image)\n",
        "            image = augmented['image']\n",
        "        \n",
        "        label = torch.tensor(self.labels[idx]).float()\n",
        "        \n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "29d54fec-ee8e-4708-9652-cf606cdc2bc8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3. Model & Training Functions\n",
        "\n",
        "## 3.1. Model Definition\n",
        "\n",
        "class SETIModel(nn.Module):\n",
        "    def __init__(self, model_name=CFG.model_name, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=CFG.in_channels, num_classes=CFG.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "## 3.2. Loss Functions\n",
        "# As per expert advice, adding FocalLoss for ablation experiments.\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        bce_loss = nn.BCEWithLogitsLoss(reduction='none')(inputs, targets)\n",
        "        pt = torch.exp(-bce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt)**self.gamma * bce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "## 3.3. Training & Validation Functions (AMP DISABLED for deterministic run)\n",
        "\n",
        "def train_fn(train_loader, model, criterion, optimizer, scheduler, epoch, device):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    \n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}')\n",
        "    for step, (images, labels) in pbar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "        \n",
        "        # No AMP for this run\n",
        "        y_preds = model(images)\n",
        "        loss = criterion(y_preds, labels)\n",
        "        \n",
        "        # --- Diagnostic print for first batch of first epoch ---\n",
        "        if epoch == 0 and step == 0:\n",
        "            print(f\"\\n  First batch diagnostics:\")\n",
        "            print(f\"    Loss: {loss.item():.4f}\")\n",
        "            print(f\"    Labels mean: {labels.float().mean().item():.4f}\")\n",
        "            print(f\"    Sigmoid preds (first 5): {torch.sigmoid(y_preds[:5].detach()).cpu().numpy().flatten()}\")\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        \n",
        "        # Gradient Clipping\n",
        "        if CFG.grad_clip_norm > 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.grad_clip_norm)\n",
        "            \n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        if CFG.scheduler_type == 'OneCycleLR':\n",
        "            scheduler.step()\n",
        "            \n",
        "        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n",
        "        current_lr = scheduler.get_last_lr()[0] if scheduler else optimizer.param_groups[0]['lr']\n",
        "        pbar.set_postfix(loss=f'{np.mean(losses):.4f}', lr=f'{current_lr:.2e}', mem_gb=f'{mem:.2f}')\n",
        "        \n",
        "    return np.mean(losses)\n",
        "\n",
        "def valid_fn(valid_loader, model, criterion, device):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    preds = []\n",
        "    targets = []\n",
        "    \n",
        "    pbar = tqdm(enumerate(valid_loader), total=len(valid_loader), desc='Validating')\n",
        "    with torch.no_grad():\n",
        "        for step, (images, labels) in pbar:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device).unsqueeze(1)\n",
        "            \n",
        "            # No AMP for this run\n",
        "            y_preds = model(images)\n",
        "            \n",
        "            loss = criterion(y_preds, labels)\n",
        "            losses.append(loss.item())\n",
        "            \n",
        "            preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
        "            targets.append(labels.to('cpu').numpy())\n",
        "            \n",
        "            mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n",
        "            pbar.set_postfix(loss=f'{np.mean(losses):.4f}', mem_gb=f'{mem:.2f}')\n",
        "            \n",
        "    predictions = np.concatenate(preds).flatten()\n",
        "    targets = np.concatenate(targets).flatten()\n",
        "    \n",
        "    # --- Diagnostic print for validation predictions ---\n",
        "    print(f\"  Validation preds stats: Min={predictions.min():.4f}, Mean={predictions.mean():.4f}, Max={predictions.max():.4f}\")\n",
        "    \n",
        "    val_auc = roc_auc_score(targets, predictions)\n",
        "    return np.mean(losses), val_auc, predictions, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "8cf25301-832d-41a0-a2ed-d154ce8907df",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 4. Main Training Loop (Simplified for Ablation)\n",
        "def run_fold(fold, df):\n",
        "    print(f\"========== FOLD {fold} TRAINING ==========\")\n",
        "    \n",
        "    # --- Clean up stale artifacts before run ---\n",
        "    model_path = f'{CFG.model_name}_fold{fold}_best.pth'\n",
        "    if os.path.exists(model_path):\n",
        "        print(f\"Removing stale model checkpoint: {model_path}\")\n",
        "        os.remove(model_path)\n",
        "    if os.path.exists('oof_predictions.csv'):\n",
        "        print(\"Removing stale oof_predictions.csv\")\n",
        "        os.remove('oof_predictions.csv')\n",
        "    \n",
        "    # Create train/valid splits\n",
        "    train_idx = df[df['fold'] != fold].index\n",
        "    valid_idx = df[df['fold'] == fold].index\n",
        "    \n",
        "    train_df = df.loc[train_idx].reset_index(drop=True)\n",
        "    valid_df = df.loc[valid_idx].reset_index(drop=True)\n",
        "    \n",
        "    print(f\"Fold {fold} Train Target Distribution:\\n{train_df['target'].value_counts(normalize=True)}\")\n",
        "    print(f\"Fold {fold} Valid Target Distribution:\\n{valid_df['target'].value_counts(normalize=True)}\")\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset = SETIDataset(train_df, transform=get_transforms(data='train'))\n",
        "    valid_dataset = SETIDataset(valid_df, transform=get_transforms(data='valid'))\n",
        "    \n",
        "    # --- Dataloaders (simplified for deterministic run) ---\n",
        "    def seed_worker(worker_id):\n",
        "        worker_seed = torch.initial_seed() % 2**32\n",
        "        np.random.seed(worker_seed)\n",
        "        random.seed(worker_seed)\n",
        "\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(CFG.seed)\n",
        "\n",
        "    # Sampler is disabled, so shuffle=True\n",
        "    train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=0, pin_memory=True, worker_init_fn=seed_worker, generator=g)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size * 2, shuffle=False, num_workers=0, pin_memory=True)\n",
        "    \n",
        "    # Init model, optimizer, scheduler\n",
        "    model = SETIModel().to(CFG.device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
        "    \n",
        "    if CFG.scheduler_type == 'OneCycleLR':\n",
        "        scheduler = OneCycleLR(optimizer, max_lr=CFG.lr, epochs=CFG.n_epochs, steps_per_epoch=len(train_loader), pct_start=CFG.one_cycle_pct_start)\n",
        "    else:\n",
        "        scheduler = None\n",
        "\n",
        "    # --- Loss Function Ablation ---\n",
        "    if CFG.loss_type == 'BCE':\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        print(\"Using plain BCEWithLogitsLoss.\")\n",
        "    elif CFG.loss_type == 'BCE_weighted':\n",
        "        pos_weight_tensor = torch.tensor(CFG.pos_weight_val, device=CFG.device)\n",
        "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
        "        print(f\"Using BCEWithLogitsLoss with pos_weight: {CFG.pos_weight_val:.2f}\")\n",
        "    elif CFG.loss_type == 'Focal':\n",
        "        criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
        "        print(\"Using FocalLoss (alpha=0.25, gamma=2.0).\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown or unsupported loss_type for this run: {CFG.loss_type}\")\n",
        "    \n",
        "    best_score = 0.\n",
        "    patience_counter = 0\n",
        "    fold_oof_df = None\n",
        "    \n",
        "    for epoch in range(CFG.n_epochs):\n",
        "        train_loss = train_fn(train_loader, model, criterion, optimizer, scheduler, epoch, CFG.device)\n",
        "        valid_loss, val_auc, predictions, _ = valid_fn(valid_loader, model, criterion, CFG.device)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Valid Loss={valid_loss:.4f}, Valid AUC={val_auc:.4f}\")\n",
        "        \n",
        "        if val_auc > best_score:\n",
        "            best_score = val_auc\n",
        "            patience_counter = 0\n",
        "            print(f\"==> New best score: {best_score:.4f}. Saving model and OOF preds.\")\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            temp_df = valid_df.copy()\n",
        "            temp_df['preds'] = predictions\n",
        "            fold_oof_df = temp_df[['id', 'target', 'preds']]\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"Score not improved. Patience: {patience_counter}/{CFG.patience}\")\n",
        "        \n",
        "        if patience_counter >= CFG.patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "            \n",
        "    if fold_oof_df is not None:\n",
        "        print(\"\\nVerifying best model checkpoint...\")\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        _, recomputed_auc, _, _ = valid_fn(valid_loader, model, criterion, CFG.device)\n",
        "        print(f\"  Best recorded AUC: {best_score:.4f}\")\n",
        "        print(f\"  Recomputed AUC on loaded model: {recomputed_auc:.4f}\")\n",
        "    \n",
        "    del model, train_loader, valid_loader, train_dataset, valid_dataset, optimizer, scheduler, criterion\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    return best_score, fold_oof_df\n",
        "\n",
        "# --- Prepare CV Folds (Load or Create) ---\n",
        "folds_csv_path = 'folds.csv'\n",
        "\n",
        "# FIX: Drop 'fold' column from the main dataframe if it exists from a previous run.\n",
        "# This prevents the MergeError caused by re-running this cell.\n",
        "if 'fold' in df.columns:\n",
        "    print(\"Dropping existing 'fold' column from main dataframe to prevent merge error.\")\n",
        "    df = df.drop(columns=['fold'])\n",
        "\n",
        "# Attempt to load and merge folds\n",
        "if os.path.exists(folds_csv_path):\n",
        "    print(\"Loading folds from folds.csv\")\n",
        "    folds_df = pd.read_csv(folds_csv_path)\n",
        "    if 'fold' not in folds_df.columns:\n",
        "        print(\"WARNING: 'fold' column missing in folds.csv. Recreating folds.\")\n",
        "        os.remove(folds_csv_path) # Delete bad file, will trigger recreation below\n",
        "    else:\n",
        "        df = df.merge(folds_df, on='id', how='left')\n",
        "        if df['fold'].isnull().any():\n",
        "            print(\"WARNING: Mismatch or nulls found in folds after merge. Recreating folds.\")\n",
        "            df = df.drop(columns=['fold']) # Drop the newly merged, bad column\n",
        "            os.remove(folds_csv_path) # Delete bad file\n",
        "\n",
        "# If folds.csv didn't exist or was deleted, create it\n",
        "if not os.path.exists(folds_csv_path):\n",
        "    print(\"Creating new folds and saving to folds.csv\")\n",
        "    df['fold'] = -1\n",
        "    skf = StratifiedGroupKFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['target'], df['group'])):\n",
        "        df.loc[val_idx, 'fold'] = int(fold)\n",
        "    df[['id', 'fold']].to_csv(folds_csv_path, index=False)\n",
        "\n",
        "df['fold'] = df['fold'].astype(int)\n",
        "\n",
        "# --- Run Training ---\n",
        "all_oof_dfs = []\n",
        "fold_scores = []\n",
        "\n",
        "# RECOVERY MODE: Manually specify folds to run to recover from previous failure.\n",
        "folds_to_run = [1]\n",
        "print(f\"RECOVERY MODE: Will only run training for folds: {folds_to_run}\")\n",
        "\n",
        "for fold in folds_to_run:\n",
        "    score, oof_df_fold = run_fold(fold, df)\n",
        "    fold_scores.append(score)\n",
        "    if oof_df_fold is not None:\n",
        "        all_oof_dfs.append(oof_df_fold)\n",
        "\n",
        "# --- Summarize Results ---\n",
        "if all_oof_dfs:\n",
        "    oof_df = pd.concat(all_oof_dfs).reset_index(drop=True)\n",
        "    # Note: This OOF score will only be for the folds that were run in this session.\n",
        "    oof_auc = roc_auc_score(oof_df['target'], oof_df['preds'])\n",
        "    \n",
        "    print(f\"\\n========== PARTIAL CV SUMMARY ==========\")\n",
        "    print(f\"Fold scores (best epoch) for folds {folds_to_run}: {fold_scores}\")\n",
        "    print(f\"Mean Fold Score for this run: {np.mean(fold_scores):.4f}\")\n",
        "    print(f\"OOF AUC for this run: {oof_auc:.4f}\")\n",
        "\n",
        "    oof_df.to_csv('oof_predictions_recovery.csv', index=False)\n",
        "    print(\"\\nOOF predictions for this run saved to oof_predictions_recovery.csv\")\n",
        "else:\n",
        "    print(\"\\nTraining did not produce any valid OOF predictions.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "455749f7-55c4-4fd1-ab82-ef59fc872921",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "print(os.listdir('.'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "092ff74e-bd62-4b92-99a7-330ef4d4a6a8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 5. Verify OOF Score\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "try:\n",
        "    oof_df = pd.read_csv('oof_predictions.csv')\n",
        "    oof_auc = roc_auc_score(oof_df['target'], oof_df['preds'])\n",
        "    print(f\"Verified Overall OOF AUC from 'oof_predictions.csv': {oof_auc:.4f}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"'oof_predictions.csv' not found. Cannot verify score.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "8a457cef-32dd-46a7-9419-81cbb9ce1eca",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 6. Manual Verification of Fold 0\n",
        "print(\"Manually verifying the score for the last completed run (Fold 0)...\")\n",
        "\n",
        "# --- Recreate validation set for Fold 0 ---\n",
        "fold_to_verify = 0\n",
        "valid_idx = df[df['fold'] == fold_to_verify].index\n",
        "valid_df = df.loc[valid_idx].reset_index(drop=True)\n",
        "valid_dataset = SETIDataset(valid_df, transform=get_transforms(data='valid'))\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size * 2, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "# --- Load the best model from the last run ---\n",
        "model = SETIModel().to(CFG.device)\n",
        "model_path = f'{CFG.model_name}_fold{fold_to_verify}_best.pth'\n",
        "try:\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    print(f\"Successfully loaded model from: {model_path}\")\n",
        "\n",
        "    # --- Re-run validation ---\n",
        "    criterion = nn.BCEWithLogitsLoss() # The criterion doesn't affect AUC calculation during validation\n",
        "    _, val_auc, _, _ = valid_fn(valid_loader, model, criterion, CFG.device)\n",
        "    print(f\"\\n========== MANUAL VERIFICATION RESULT ==========\")\n",
        "    print(f\"Recomputed AUC for Fold {fold_to_verify}: {val_auc:.4f}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Model checkpoint not found at: {model_path}\")\n",
        "\n",
        "# --- Clean up ---\n",
        "del model, valid_loader, valid_dataset, valid_df\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}