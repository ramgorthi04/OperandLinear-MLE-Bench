{
  "cells": [
    {
      "id": "b7b67524-2f86-4bb8-8860-eccb64ba5213",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SETI Breakthrough Listen - E.T. Signal Search: Revised Plan (Post-Expert Review)\n",
        "\n",
        "This notebook outlines the revised plan to tackle the SETI Breakthrough Listen competition, incorporating feedback from Kaggle Grandmasters. The goal is to achieve a medal-winning score on the AUC-ROC metric.\n",
        "\n",
        "## 1. Initial Setup & Environment\n",
        "*   **Goal:** Prepare the environment for the project.\n",
        "*   **Actions:**\n",
        "    *   Import necessary libraries (pandas, numpy, matplotlib, sklearn, torch, timm, albumentations).\n",
        "    *   Install `timm` and `albumentations` if not present.\n",
        "    *   Define constants for file paths.\n",
        "    *   Set up logging and device (CUDA/CPU).\n",
        "\n",
        "## 2. Exploratory Data Analysis (EDA)\n",
        "*   **Goal:** Understand the data structure and confirm key assumptions.\n",
        "*   **Actions:**\n",
        "    *   Inspect the file structure using `ls -R`. Avoid any files in `old_leaky_data/`.\n",
        "    *   Load `train_labels.csv` and analyze the target distribution. Calculate `pos_weight` for the loss function.\n",
        "    *   Load `sample_submission.csv` to understand the required submission format.\n",
        "    *   Load a single data file (`.npy`) to confirm its shape is `(6, 273, 256)`. The 6 slices represent ON-OFF cadence pairs.\n",
        "    *   Visualize a few positive and negative samples after applying the 3-channel difference preprocessing.\n",
        "\n",
        "## 3. Data Preparation & Preprocessing (Top Priority)\n",
        "*   **Goal:** Create a robust data loading and preprocessing pipeline based on expert advice.\n",
        "*   **Actions:**\n",
        "    *   **Input Representation:**\n",
        "        *   Do NOT use raw 6 channels. The input data is a sequence of 3 ON-OFF cadence pairs.\n",
        "        *   Create a 3-channel image by taking the difference between each ON-OFF pair: `[ON_1 - OFF_1, ON_2 - OFF_2, ON_3 - OFF_3]`.\n",
        "    *   **Normalization:**\n",
        "        *   Apply a per-sample, per-channel normalization scheme. A good starting point is to apply `log1p` for contrast and then standardize (z-score).\n",
        "        *   Ensure identical normalization is applied to train, validation, and test sets.\n",
        "    *   **Dataset Class:**\n",
        "        *   Create a `torch.utils.data.Dataset` class that loads `.npy` files on-the-fly.\n",
        "        *   The `__getitem__` method will perform the 3-channel differencing and normalization.\n",
        "    *   **Augmentations:**\n",
        "        *   Use light, physically-sensible augmentations with `albumentations`.\n",
        "        *   Good choices: HorizontalFlip, VerticalFlip, small time/frequency shifts (e.g., `ShiftScaleRotate` with small shifts and no rotation).\n",
        "        *   Avoid heavy distortions. MixUp/CutMix are noted to be less effective for this problem.\n",
        "\n",
        "## 4. Baseline Model & Training\n",
        "*   **Goal:** Build and train a strong baseline model using a robust validation strategy.\n",
        "*   **Actions:**\n",
        "    *   **Validation Strategy (Crucial):**\n",
        "        *   Use `StratifiedGroupKFold` (e.g., k=5).\n",
        "        *   Group samples by their base ID (e.g., the part of the filename before the first underscore) to prevent leakage from near-duplicate observations.\n",
        "    *   **Model Choice:**\n",
        "        *   Use a pretrained 3-channel CNN from `timm`. Start with `efficientnet_b0` or `convnext_tiny` for speed, then move to `efficientnet_b2/b3` for performance.\n",
        "        *   Modify the model's first convolutional layer if input size is not standard, or simply resize images to `224x224` or `256x256`.\n",
        "    *   **Training Loop:**\n",
        "        *   **Loss Function:** `BCEWithLogitsLoss` with `pos_weight` calculated from the training data imbalance.\n",
        "        *   **Optimizer:** `AdamW`.\n",
        "        *   **Scheduler:** `CosineAnnealingLR` with warmup.\n",
        "        *   **Mixed Precision:** Use `torch.cuda.amp` for faster training.\n",
        "        *   **Metric & Stopping:** Monitor validation AUC-ROC and use Early Stopping to save the best model per fold.\n",
        "\n",
        "## 5. Iteration, Ensembling, and Submission\n",
        "*   **Goal:** Improve the baseline and generate the final submission.\n",
        "*   **Actions:**\n",
        "    *   **Cross-Validation:** Train the model on all 5 folds from the `StratifiedGroupKFold` split.\n",
        "    *   **Test-Time Augmentation (TTA):** For inference, apply augmentations (e.g., horizontal/vertical flips) to each test sample and average the predictions.\n",
        "    *   **Ensembling:** The primary ensemble strategy will be to average the predictions from the 5 models trained on different folds.\n",
        "    *   **Submission:**\n",
        "        *   Run inference on the test set using the ensembled models with TTA.\n",
        "        *   Format the predictions into `submission.csv`.\n",
        "\n",
        "## Performance Targets & Sanity Checks\n",
        "*   **Initial Sanity Check:** A single-fold training run should achieve a validation AUC > 0.73. If not, debug the preprocessing, normalization, and data loading.\n",
        "*   **Medal Target:** A 5-fold cross-validated average AUC should be >= 0.77 for a strong result. The goal is to push this higher with model selection and tuning."
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}