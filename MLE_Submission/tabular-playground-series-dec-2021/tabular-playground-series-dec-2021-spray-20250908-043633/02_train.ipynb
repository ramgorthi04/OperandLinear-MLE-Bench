{
  "cells": [
    {
      "id": "6d41bbbc-331d-4663-a880-5700314b7a3f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stage 2: Train LightGBM on cached Feather/NumPy with robust logging + stdout heartbeats (CPU-only streamlined)\n",
        "import os, sys, time, gc, json, logging, importlib, subprocess, traceback\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print('[HEARTBEAT] 02_train starting...'); sys.stdout.flush()\n",
        "\n",
        "# --- Logging ---\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s [%(levelname)s] %(message)s',\n",
        "                    handlers=[\n",
        "                        logging.FileHandler('run_train.log', mode='w'),\n",
        "                        logging.StreamHandler(sys.stdout)\n",
        "                    ],\n",
        "                    force=True)\n",
        "os.environ['PYTHONUNBUFFERED'] = '1'\n",
        "\n",
        "def ensure_package(pkg: str, import_name: str = None):\n",
        "    name = import_name or pkg\n",
        "    try:\n",
        "        return importlib.import_module(name)\n",
        "    except ImportError:\n",
        "        print(f'[HEARTBEAT] Installing {pkg}...'); sys.stdout.flush()\n",
        "        logging.info(f'Installing {pkg}...')\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg])\n",
        "        return importlib.import_module(name)\n",
        "\n",
        "try:\n",
        "    lgb = ensure_package('lightgbm', 'lightgbm')\n",
        "    print('[HEARTBEAT] lightgbm imported OK'); sys.stdout.flush()\n",
        "\n",
        "    SEED = 42\n",
        "    N_SPLITS = None  # set after loading folds\n",
        "\n",
        "    t0 = time.time()\n",
        "    logging.info('Loading cached datasets (Feather/NumPy)...')\n",
        "    X = pd.read_feather('X.feather')\n",
        "    X_test = pd.read_feather('X_test.feather')\n",
        "    y = np.load('y.npy')\n",
        "    test_ids = np.load('test_ids.npy')\n",
        "    with open('features.json', 'r') as f:\n",
        "        features = json.load(f)\n",
        "    elev_threshold = None\n",
        "    if os.path.exists('preprocess_meta.json'):\n",
        "        try:\n",
        "            with open('preprocess_meta.json', 'r') as f:\n",
        "                meta = json.load(f)\n",
        "                elev_threshold = meta.get('elev_threshold', None)\n",
        "        except Exception:\n",
        "            elev_threshold = None\n",
        "    logging.info(f'X shape: {X.shape}, X_test shape: {X_test.shape}, y shape: {y.shape}, features: {len(features)}')\n",
        "    if elev_threshold is not None:\n",
        "        logging.info(f'Using persisted elev_threshold: {elev_threshold}')\n",
        "    print('[HEARTBEAT] Data loaded'); sys.stdout.flush()\n",
        "\n",
        "    # Enforce feature column order\n",
        "    missing_in_X = [c for c in features if c not in X.columns]\n",
        "    missing_in_Xt = [c for c in features if c not in X_test.columns]\n",
        "    if missing_in_X or missing_in_Xt:\n",
        "        logging.warning(f'Feature mismatch. Missing in X: {missing_in_X[:5]}... Missing in X_test: {missing_in_Xt[:5]}...')\n",
        "    X = X[features]\n",
        "    X_test = X_test[features]\n",
        "\n",
        "    # Convert to NumPy\n",
        "    X_np = X.to_numpy()\n",
        "    X_test_np = X_test.to_numpy()\n",
        "    del X, X_test\n",
        "    gc.collect()\n",
        "    print('[HEARTBEAT] Converted to NumPy'); sys.stdout.flush()\n",
        "\n",
        "    # Load folds\n",
        "    if os.path.exists('fold_indices.npy'):\n",
        "        folds = np.load('fold_indices.npy', allow_pickle=True).tolist()\n",
        "        logging.info('Loaded fold_indices.npy for consistent CV splits.')\n",
        "    else:\n",
        "        logging.info('fold_indices.npy not found; creating new StratifiedKFold splits.')\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "        folds = list(skf.split(X_np, y))\n",
        "        np.save('fold_indices.npy', np.array(folds, dtype=object))\n",
        "\n",
        "    # Tie N_SPLITS to folds\n",
        "    N_SPLITS = len(folds)\n",
        "    logging.info(f'Using N_SPLITS={N_SPLITS} based on loaded folds.')\n",
        "    print(f'[HEARTBEAT] N_SPLITS={N_SPLITS}', flush=True)\n",
        "\n",
        "    # CPU-only LightGBM parameters (stable and efficient for large data).\n",
        "    params = {\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': 7,\n",
        "        'metric': 'multi_logloss',\n",
        "        'learning_rate': 0.025,\n",
        "        'num_leaves': 110,\n",
        "        'min_data_in_leaf': 60,\n",
        "        'max_depth': 9,\n",
        "        'feature_fraction': 0.82,\n",
        "        'bagging_fraction': 0.78,\n",
        "        'bagging_freq': 1,\n",
        "        'lambda_l1': 1.2,\n",
        "        'lambda_l2': 2.5,\n",
        "        'max_bin': 127,\n",
        "        'bin_construct_sample_cnt': 50000,\n",
        "        'force_col_wise': True,\n",
        "        'verbose': -1,\n",
        "        'seed': SEED,\n",
        "        'bagging_seed': SEED,\n",
        "        'feature_fraction_seed': SEED,\n",
        "        'num_threads': 8,\n",
        "        'first_metric_only': True,\n",
        "        'deterministic': True,\n",
        "        'feature_pre_filter': False,\n",
        "        'device': 'cpu'\n",
        "    }\n",
        "\n",
        "    NUM_BOOST_ROUND = 4000\n",
        "    EARLY_STOP_ROUNDS = 200\n",
        "    LOG_PERIOD = 50\n",
        "\n",
        "    oof_preds = np.zeros((y.shape[0], 7), dtype=np.float32)\n",
        "    test_preds = np.zeros((X_test_np.shape[0], 7), dtype=np.float32)\n",
        "    fold_acc = []\n",
        "\n",
        "    logging.info(f'Starting {N_SPLITS}-fold CV training...')\n",
        "    print('[HEARTBEAT] Training loop start'); sys.stdout.flush()\n",
        "    for i, (trn_idx, val_idx) in enumerate(folds, 1):\n",
        "        fts = time.time()\n",
        "        logging.info(f'[FOLD {i}/{N_SPLITS}] Train: {len(trn_idx)} | Valid: {len(val_idx)}')\n",
        "        print(f'[HEARTBEAT] Fold {i} start: trn={len(trn_idx)} val={len(val_idx)}'); sys.stdout.flush()\n",
        "        X_trn = X_np[trn_idx]\n",
        "        y_trn = y[trn_idx]\n",
        "        X_val = X_np[val_idx]\n",
        "        y_val = y[val_idx]\n",
        "\n",
        "        print('[HEARTBEAT] Building Datasets...'); sys.stdout.flush()\n",
        "        dtrain = lgb.Dataset(X_trn, label=y_trn, free_raw_data=True, params={'bin_construct_sample_cnt': 50000})\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val, reference=dtrain, free_raw_data=True)\n",
        "        print('[HEARTBEAT] Datasets ready. Starting lgb.train...'); sys.stdout.flush()\n",
        "\n",
        "        model = lgb.train(\n",
        "            params=params,\n",
        "            train_set=dtrain,\n",
        "            num_boost_round=NUM_BOOST_ROUND,\n",
        "            valid_sets=[dtrain, dvalid],\n",
        "            valid_names=['train', 'valid'],\n",
        "            callbacks=[\n",
        "                lgb.early_stopping(stopping_rounds=EARLY_STOP_ROUNDS, verbose=False),\n",
        "                lgb.log_evaluation(period=LOG_PERIOD)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        val_proba = model.predict(X_val, num_iteration=model.best_iteration)\n",
        "        oof_preds[val_idx] = val_proba\n",
        "        val_pred = np.argmax(val_proba, axis=1)\n",
        "        acc = accuracy_score(y_val, val_pred)\n",
        "        fold_acc.append(acc)\n",
        "        logging.info(f'[FOLD {i}] ACC={acc:.6f} | best_iter={model.best_iteration} | elapsed={time.time()-fts:.1f}s')\n",
        "        print(f'[HEARTBEAT] Fold {i} done: acc={acc:.6f} best_iter={model.best_iteration}'); sys.stdout.flush()\n",
        "\n",
        "        test_fold = model.predict(X_test_np, num_iteration=model.best_iteration)\n",
        "        test_preds += test_fold / N_SPLITS\n",
        "\n",
        "        # Save partial artifacts after each fold\n",
        "        np.save('lgb_oof_preds_partial.npy', oof_preds)\n",
        "        np.save('lgb_test_preds_partial.npy', test_preds)\n",
        "\n",
        "        del X_trn, X_val, y_trn, y_val, dtrain, dvalid, model, val_proba, test_fold\n",
        "        gc.collect()\n",
        "\n",
        "    oof_labels = np.argmax(oof_preds, axis=1)\n",
        "    cv_acc = accuracy_score(y, oof_labels)\n",
        "    logging.info(f'[CV] Mean ACC over folds: {np.mean(fold_acc):.6f} | OOF ACC: {cv_acc:.6f}')\n",
        "    print(f'[HEARTBEAT] CV done: mean_acc={np.mean(fold_acc):.6f} OOF={cv_acc:.6f}'); sys.stdout.flush()\n",
        "\n",
        "    # Save artifacts\n",
        "    np.save('lgb_oof_preds.npy', oof_preds)\n",
        "    np.save('lgb_test_preds.npy', test_preds)\n",
        "    logging.info('Saved lgb_oof_preds.npy and lgb_test_preds.npy')\n",
        "    submission = pd.DataFrame({'Id': test_ids, 'Cover_Type': np.argmax(test_preds, axis=1) + 1})\n",
        "    submission.to_csv('submission.csv', index=False)\n",
        "    logging.info('Saved submission.csv')\n",
        "    logging.info(f'Done. Total elapsed: {time.time()-t0:.1f}s')\n",
        "    print('[HEARTBEAT] 02_train finished.'); sys.stdout.flush()\n",
        "\n",
        "except Exception as e:\n",
        "    err = traceback.format_exc()\n",
        "    print('[ERROR] Exception in 02_train:', e); sys.stdout.flush()\n",
        "    print(err); sys.stdout.flush()\n",
        "    try:\n",
        "        with open('train_error.log', 'w') as f:\n",
        "            f.write(err)\n",
        "    except Exception:\n",
        "        pass\n",
        "    logging.error('Exception occurred in training pipeline:\\n' + err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "a113563e-6d54-4287-ac9e-1847080f5701",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sanity: ensure LightGBM is installed and importable, print version\n",
        "import sys, subprocess, importlib, logging\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    print('[SANITY] lightgbm already installed:', lgb.__version__); sys.stdout.flush()\n",
        "except Exception as e:\n",
        "    print('[SANITY] Installing lightgbm due to import error:', e); sys.stdout.flush()\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "    print('[SANITY] lightgbm installed:', lgb.__version__); sys.stdout.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "caf88fcc-9d77-4639-ad7a-82bd52cca8ad",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Diagnostics: check artifacts, logs, and environment status\n",
        "import os, time, glob, json\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def stat_file(p):\n",
        "    if os.path.exists(p):\n",
        "        sz = os.path.getsize(p)\n",
        "        mt = os.path.getmtime(p)\n",
        "        return f'exists size={sz} mtime={datetime.fromtimestamp(mt)}'\n",
        "    return 'MISSING'\n",
        "\n",
        "targets = [\n",
        "    'run_train.log',\n",
        "    'lgb_oof_preds_partial.npy',\n",
        "    'lgb_test_preds_partial.npy',\n",
        "    'lgb_oof_preds.npy',\n",
        "    'lgb_test_preds.npy',\n",
        "    'submission.csv',\n",
        "    'X.feather',\n",
        "    'X_test.feather',\n",
        "    'y.npy',\n",
        "    'test_ids.npy',\n",
        "    'features.json',\n",
        "    'fold_indices.npy'\n",
        "]\n",
        "print('=== File stats ===')\n",
        "for p in targets:\n",
        "    print(f'{p}: {stat_file(p)}')\n",
        "\n",
        "print('\\n=== Try tail of run_train.log (last 1000 chars) ===')\n",
        "if os.path.exists('run_train.log'):\n",
        "    try:\n",
        "        with open('run_train.log', 'r') as f:\n",
        "            f.seek(0, os.SEEK_END)\n",
        "            size = f.tell()\n",
        "            f.seek(max(size-1000, 0))\n",
        "            print(f.read())\n",
        "    except Exception as e:\n",
        "        print('Could not read run_train.log:', e)\n",
        "else:\n",
        "    print('run_train.log not found')\n",
        "\n",
        "print('\\n=== Quick dataset/metadata checks ===')\n",
        "try:\n",
        "    with open('features.json', 'r') as f:\n",
        "        feats = json.load(f)\n",
        "    print('features.json count:', len(feats))\n",
        "except Exception as e:\n",
        "    print('features.json read error:', e)\n",
        "\n",
        "try:\n",
        "    y = np.load('y.npy')\n",
        "    print('y.npy shape:', y.shape)\n",
        "except Exception as e:\n",
        "    print('y.npy read error:', e)\n",
        "\n",
        "try:\n",
        "    X = pd.read_feather('X.feather')\n",
        "    X_test = pd.read_feather('X_test.feather')\n",
        "    print('X.feather shape:', X.shape, '| X_test.feather shape:', X_test.shape)\n",
        "except Exception as e:\n",
        "    print('Feather read error:', e)\n",
        "\n",
        "print('\\n=== Folds info ===')\n",
        "try:\n",
        "    folds = np.load('fold_indices.npy', allow_pickle=True).tolist()\n",
        "    print('fold_indices.npy len:', len(folds))\n",
        "except Exception as e:\n",
        "    print('fold_indices load error:', e)\n",
        "\n",
        "print('\\n=== Env/threads ===')\n",
        "try:\n",
        "    import multiprocessing as mp\n",
        "    print('CPU count:', mp.cpu_count())\n",
        "except Exception as e:\n",
        "    print('cpu_count error:', e)\n",
        "print('LIGHTGBM_THREADS env:', os.environ.get('OMP_NUM_THREADS'), os.environ.get('NUMEXPR_MAX_THREADS'), os.environ.get('MKL_NUM_THREADS'))\n",
        "\n",
        "print('\\n=== Done diagnostics ===')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "5fd897ef-4ef0-4608-92c3-e4e55335a0ce",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick sanity: write markers to verify execution and environment\n",
        "import os, time, json, numpy as np\n",
        "ts = time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "with open('sanity_marker.txt', 'w') as f:\n",
        "    f.write(f'started at {ts}')\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    with open('sanity_lgb_version.txt', 'w') as f:\n",
        "        f.write(lgb.__version__)\n",
        "except Exception as e:\n",
        "    with open('sanity_lgb_version.txt', 'w') as f:\n",
        "        f.write(f'import_error: {e}')\n",
        "\n",
        "with open('features_count.txt', 'w') as f:\n",
        "    try:\n",
        "        with open('features.json', 'r') as jf:\n",
        "            feats = json.load(jf)\n",
        "        f.write(str(len(feats)))\n",
        "    except Exception as e:\n",
        "        f.write(f'error: {e}')\n",
        "\n",
        "# write a tiny npy so we can confirm file writes work\n",
        "np.save('tmp_probe.npy', np.array([1,2,3], dtype=np.int32))\n",
        "\n",
        "# dump a lightweight directory listing snapshot\n",
        "try:\n",
        "    files = sorted(os.listdir('.'))\n",
        "    with open('dir_list.txt', 'w') as f:\n",
        "        for p in files:\n",
        "            try:\n",
        "                sz = os.path.getsize(p)\n",
        "                f.write(f'{p}\\t{sz}\\n')\n",
        "            except Exception:\n",
        "                f.write(f'{p}\\t-1\\n')\n",
        "except Exception:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "ce96dc2b-d4e6-4eba-8764-926cd1f827a2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick baseline: fast subsample training to validate pipeline and produce a submission (tiny smoke test)\n",
        "import os, sys, time, json, logging, gc, traceback\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print('[QUICK] Starting quick baseline training (SMOKE TEST)...'); sys.stdout.flush()\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s [%(levelname)s] %(message)s',\n",
        "                    handlers=[\n",
        "                        logging.FileHandler('run_quick.log', mode='w'),\n",
        "                        logging.StreamHandler(sys.stdout)\n",
        "                    ],\n",
        "                    force=True)\n",
        "\n",
        "try:\n",
        "    t0 = time.time()\n",
        "    print('[QUICK] Loading cached data...'); sys.stdout.flush()\n",
        "    X = pd.read_feather('X.feather')\n",
        "    X_test = pd.read_feather('X_test.feather')\n",
        "    y = np.load('y.npy')\n",
        "    with open('features.json', 'r') as f:\n",
        "        features = json.load(f)\n",
        "    X = X[features]\n",
        "    X_test = X_test[features]\n",
        "    print('[QUICK] Converting to NumPy...'); sys.stdout.flush()\n",
        "    X_np = X.to_numpy()\n",
        "    X_test_np = X_test.to_numpy()\n",
        "    del X, X_test; gc.collect()\n",
        "    logging.info(f'Data loaded. X: {X_np.shape}, X_test: {X_test_np.shape}')\n",
        "\n",
        "    # Balanced per-class subsample to avoid stratification edge-cases\n",
        "    print('[QUICK] Building balanced per-class subsample...'); sys.stdout.flush()\n",
        "    rng = np.random.default_rng(42)\n",
        "    classes = np.unique(y)\n",
        "    per_class = 8000  # target per-class; will cap by available\n",
        "    idx_list = []\n",
        "    for c in classes:\n",
        "        cls_idx = np.where(y == c)[0]\n",
        "        take = min(per_class, cls_idx.shape[0])\n",
        "        # If class is extremely rare (<=take), take all (no replace)\n",
        "        sel = rng.choice(cls_idx, size=take, replace=False) if take > 0 else np.array([], dtype=np.int64)\n",
        "        idx_list.append(sel)\n",
        "    sub_idx = np.concatenate([arr for arr in idx_list if arr.size > 0])\n",
        "    rng.shuffle(sub_idx)\n",
        "    X_sub = X_np[sub_idx]\n",
        "    y_sub = y[sub_idx]\n",
        "    counts = np.array([(y_sub == c).sum() for c in classes])\n",
        "    logging.info(f'Subsample built: {X_sub.shape}, class counts: ' + ','.join(str(int(k)) for k in counts))\n",
        "\n",
        "    # Decide if stratification is possible (need at least 2 samples in every class)\n",
        "    do_strat = (counts.min() >= 2)\n",
        "    print(f\"[QUICK] Train/valid split... (stratify={'yes' if do_strat else 'no'})\"); sys.stdout.flush()\n",
        "    X_trn, X_val, y_trn, y_val = train_test_split(\n",
        "        X_sub, y_sub, test_size=0.1, random_state=42, stratify=(y_sub if do_strat else None)\n",
        "    )\n",
        "    logging.info(f'Train: {X_trn.shape}, Valid: {X_val.shape}')\n",
        "\n",
        "    import lightgbm as lgb\n",
        "    params = {\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': 7,\n",
        "        'metric': 'multi_logloss',\n",
        "        'learning_rate': 0.05,\n",
        "        'num_leaves': 96,\n",
        "        'min_data_in_leaf': 64,\n",
        "        'max_depth': 8,\n",
        "        'feature_fraction': 0.8,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 1,\n",
        "        'lambda_l1': 1.0,\n",
        "        'lambda_l2': 2.0,\n",
        "        'max_bin': 127,\n",
        "        'bin_construct_sample_cnt': 100000,\n",
        "        'force_col_wise': True,\n",
        "        'num_threads': 8,\n",
        "        'deterministic': True,\n",
        "        'feature_pre_filter': False,\n",
        "        'seed': 42,\n",
        "        'device': 'cpu'\n",
        "    }\n",
        "    print('[QUICK] Building Datasets...'); sys.stdout.flush()\n",
        "    dtrain = lgb.Dataset(X_trn, label=y_trn, free_raw_data=True, params={'bin_construct_sample_cnt': 100000})\n",
        "    dvalid = lgb.Dataset(X_val, label=y_val, reference=dtrain, free_raw_data=True)\n",
        "\n",
        "    print('[QUICK] Training LightGBM (200 rounds, ES=50)...'); sys.stdout.flush()\n",
        "    model = lgb.train(\n",
        "        params=params,\n",
        "        train_set=dtrain,\n",
        "        num_boost_round=200,\n",
        "        valid_sets=[dtrain, dvalid],\n",
        "        valid_names=['train', 'valid'],\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
        "            lgb.log_evaluation(period=20)\n",
        "        ]\n",
        "    )\n",
        "    print('[QUICK] Training complete. Predicting...'); sys.stdout.flush()\n",
        "\n",
        "    val_proba = model.predict(X_val, num_iteration=model.best_iteration)\n",
        "    val_pred = np.argmax(val_proba, axis=1)\n",
        "    val_acc = accuracy_score(y_val, val_pred)\n",
        "    logging.info(f'Quick baseline valid ACC: {val_acc:.6f} at iter {model.best_iteration}')\n",
        "    print(f'[QUICK] Valid ACC: {val_acc:.6f} iter={model.best_iteration}'); sys.stdout.flush()\n",
        "\n",
        "    # Predict test and save submission\n",
        "    test_proba = model.predict(X_test_np, num_iteration=model.best_iteration)\n",
        "    test_pred = np.argmax(test_proba, axis=1) + 1\n",
        "    test_ids = np.load('test_ids.npy')\n",
        "    sub = pd.DataFrame({'Id': test_ids, 'Cover_Type': test_pred})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    logging.info('Saved submission.csv from quick baseline.')\n",
        "    print(f'[QUICK] Done in {time.time()-t0:.1f}s'); sys.stdout.flush()\n",
        "except Exception as e:\n",
        "    print('[QUICK][ERROR]', e); sys.stdout.flush()\n",
        "    with open('quick_error.log', 'w') as f:\n",
        "        f.write(traceback.format_exc())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "6a5116a1-7974-4832-9762-e65f066c2cf2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Diagnostics: check target distribution end-to-end\n",
        "import numpy as np, pandas as pd, json, os\n",
        "from collections import Counter\n",
        "print('[DIAG] Loading y.npy ...');\n",
        "y = np.load('y.npy')\n",
        "uniq, cnts = np.unique(y, return_counts=True)\n",
        "print('[DIAG] y.npy classes:', uniq.tolist())\n",
        "print('[DIAG] y.npy counts:', cnts.tolist(), ' total=', int(cnts.sum()))\n",
        "\n",
        "print('[DIAG] Loading train.csv to cross-check Cover_Type ...')\n",
        "df = pd.read_csv('train.csv', usecols=['Cover_Type'])\n",
        "vc = df['Cover_Type'].value_counts().sort_index()\n",
        "print('[DIAG] train.csv Cover_Type counts (1..7):', vc.to_dict())\n",
        "print('[DIAG] train.csv total:', int(df.shape[0]))\n",
        "\n",
        "print('[DIAG] features.json length:')\n",
        "with open('features.json','r') as f:\n",
        "    feats = json.load(f)\n",
        "print(len(feats))\n",
        "\n",
        "print('[DIAG] test_ids vs sample_submission length:')\n",
        "test_ids = np.load('test_ids.npy')\n",
        "print('test_ids:', len(test_ids))\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "print('sample_submission rows:', ss.shape[0])\n",
        "print('[DIAG] Done.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "42b92e70-3b59-4e76-aa86-d4c0df1a4b81",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Single-model fast training (no CV): sub-sample mid-size train, small valid, quick submission with class weights\n",
        "import os, sys, time, json, logging, gc, traceback\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print('[SINGLE] Starting single-model fast training...'); sys.stdout.flush()\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s [%(levelname)s] %(message)s',\n",
        "                    handlers=[\n",
        "                        logging.FileHandler('run_single.log', mode='w'),\n",
        "                        logging.StreamHandler(sys.stdout)\n",
        "                    ],\n",
        "                    force=True)\n",
        "\n",
        "try:\n",
        "    t0 = time.time()\n",
        "    # Load cached arrays/feather\n",
        "    logging.info('Loading cached data...')\n",
        "    X = pd.read_feather('X.feather')\n",
        "    X_test = pd.read_feather('X_test.feather')\n",
        "    y = np.load('y.npy')\n",
        "    with open('features.json','r') as f:\n",
        "        features = json.load(f)\n",
        "    X = X[features]\n",
        "    X_test = X_test[features]\n",
        "    X_np = X.to_numpy()\n",
        "    X_test_np = X_test.to_numpy()\n",
        "    del X, X_test; gc.collect()\n",
        "    logging.info(f'X: {X_np.shape}, X_test: {X_test_np.shape}')\n",
        "\n",
        "    # Build a mid-size sub-sample for speed, ensure all classes appear\n",
        "    rng = np.random.default_rng(42)\n",
        "    n_total = X_np.shape[0]\n",
        "    target_rows = min(700000, n_total)  # ~0.7M rows for faster training\n",
        "    base_idx = rng.choice(n_total, size=target_rows, replace=False)\n",
        "    # Ensure include at least one sample from every class (especially the rare ones)\n",
        "    classes = np.unique(y)\n",
        "    ensure_idx = []\n",
        "    for c in classes:\n",
        "        loc = np.where(y == c)[0]\n",
        "        if loc.size > 0:\n",
        "            ensure_idx.append(loc[0])\n",
        "    ensure_idx = np.array(list(set(ensure_idx)), dtype=np.int64)\n",
        "    sub_idx = np.unique(np.concatenate([base_idx, ensure_idx]))\n",
        "    X_sub = X_np[sub_idx]\n",
        "    y_sub = y[sub_idx]\n",
        "    logging.info(f'Sub-sample: {X_sub.shape}')\n",
        "\n",
        "    # Train/valid split (not stratified; rare class may be only in train)\n",
        "    X_trn, X_val, y_trn, y_val = train_test_split(X_sub, y_sub, test_size=0.05, random_state=42, shuffle=True, stratify=None)\n",
        "    logging.info(f'Train: {X_trn.shape}, Valid: {X_val.shape}')\n",
        "\n",
        "    # Compute class weights (inverse freq) and assign per-row weights for training\n",
        "    cls, counts = np.unique(y_trn, return_counts=True)\n",
        "    freq = counts / counts.sum()\n",
        "    inv = 1.0 / np.clip(freq, 1e-12, None)\n",
        "    inv = inv / inv.mean()  # normalize around 1\n",
        "    weight_map = {int(c): float(w) for c, w in zip(cls, inv)}\n",
        "    w_trn = np.array([weight_map[int(t)] for t in y_trn], dtype=np.float32)\n",
        "    logging.info('Class weights: ' + ','.join(f'{int(c)}:{weight_map[int(c)]:.3f}' for c in sorted(weight_map.keys())))\n",
        "\n",
        "    import lightgbm as lgb\n",
        "    params = {\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': 7,\n",
        "        'metric': 'multi_logloss',\n",
        "        'learning_rate': 0.025,\n",
        "        'num_leaves': 110,\n",
        "        'min_data_in_leaf': 60,\n",
        "        'max_depth': 9,\n",
        "        'feature_fraction': 0.82,\n",
        "        'bagging_fraction': 0.78,\n",
        "        'bagging_freq': 1,\n",
        "        'lambda_l1': 1.2,\n",
        "        'lambda_l2': 2.5,\n",
        "        'max_bin': 127,\n",
        "        'bin_construct_sample_cnt': 50000,\n",
        "        'force_col_wise': True,\n",
        "        'num_threads': 8,\n",
        "        'deterministic': True,\n",
        "        'feature_pre_filter': False,\n",
        "        'seed': 42,\n",
        "        'device': 'cpu',\n",
        "        'first_metric_only': True\n",
        "    }\n",
        "\n",
        "    dtrain = lgb.Dataset(X_trn, label=y_trn, weight=w_trn, free_raw_data=True, params={'bin_construct_sample_cnt': 50000})\n",
        "    dvalid = lgb.Dataset(X_val, label=y_val, reference=dtrain, free_raw_data=True)\n",
        "    logging.info('Training LightGBM (single model, weighted) ...')\n",
        "    model = lgb.train(\n",
        "        params=params,\n",
        "        train_set=dtrain,\n",
        "        num_boost_round=1500,\n",
        "        valid_sets=[dtrain, dvalid],\n",
        "        valid_names=['train','valid'],\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=150, verbose=False),\n",
        "            lgb.log_evaluation(period=50)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Validation report\n",
        "    val_pred_proba = model.predict(X_val, num_iteration=model.best_iteration)\n",
        "    val_pred = np.argmax(val_pred_proba, axis=1)\n",
        "    val_acc = accuracy_score(y_val, val_pred)\n",
        "    logging.info(f'[SINGLE] Valid ACC: {val_acc:.6f} @iter {model.best_iteration}')\n",
        "    print(f'[SINGLE] Valid ACC: {val_acc:.6f} @iter {model.best_iteration}'); sys.stdout.flush()\n",
        "\n",
        "    # Predict test and save submission\n",
        "    test_pred_proba = model.predict(X_test_np, num_iteration=model.best_iteration)\n",
        "    test_pred = np.argmax(test_pred_proba, axis=1) + 1\n",
        "    test_ids = np.load('test_ids.npy')\n",
        "    sub = pd.DataFrame({'Id': test_ids, 'Cover_Type': test_pred})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    logging.info('Saved submission.csv (single model).')\n",
        "    print(f'[SINGLE] Done in {time.time()-t0:.1f}s'); sys.stdout.flush()\n",
        "except Exception as e:\n",
        "    print('[SINGLE][ERROR]', e); sys.stdout.flush()\n",
        "    with open('single_error.log', 'w') as f:\n",
        "        f.write(traceback.format_exc())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "cd813113-cb67-434d-9b7a-0c54f82cd2d5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# GPU-first Bagging ensemble: stronger LightGBM models on larger subsamples with robust imbalance handling\n",
        "import os, sys, time, json, logging, gc, traceback\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Immediate heartbeat files to verify execution start and CWD\n",
        "try:\n",
        "    with open('bag_start.txt', 'w') as f:\n",
        "        f.write(time.strftime('%Y-%m-%d %H:%M:%S'))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def hb(msg):\n",
        "    try:\n",
        "        with open('bag_heartbeats.txt', 'a') as f:\n",
        "            f.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} | {msg}\\n\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "print('[BAG] Starting bagging ensemble training (GPU-first)...'); sys.stdout.flush()\n",
        "hb('Script start')\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s [%(levelname)s] %(message)s',\n",
        "                    handlers=[\n",
        "                        logging.FileHandler('run_bagging.log', mode='w'),\n",
        "                        logging.StreamHandler(sys.stdout)\n",
        "                    ],\n",
        "                    force=True)\n",
        "hb('Logging configured; run_bagging.log handler created')\n",
        "\n",
        "try:\n",
        "    t0_all = time.time()\n",
        "    logging.info('Loading cached data...')\n",
        "    hb('Loading cached data (feather/npy)')\n",
        "    X = pd.read_feather('X.feather')\n",
        "    X_test = pd.read_feather('X_test.feather')\n",
        "    y = np.load('y.npy')\n",
        "    with open('features.json','r') as f:\n",
        "        features = json.load(f)\n",
        "    X = X[features]\n",
        "    X_test = X_test[features]\n",
        "    X_np = X.to_numpy(); X_test_np = X_test.to_numpy()\n",
        "    test_ids = np.load('test_ids.npy')\n",
        "    del X; gc.collect()\n",
        "    logging.info(f'Data ready. X: {X_np.shape}, X_test: {X_test_np.shape}')\n",
        "    hb(f'Data ready {X_np.shape} / {X_test_np.shape}')\n",
        "\n",
        "    # Thread/env stability before importing LightGBM\n",
        "    os.environ['OMP_NUM_THREADS'] = os.environ.get('OMP_NUM_THREADS', '16')\n",
        "    os.environ['MKL_NUM_THREADS'] = os.environ.get('MKL_NUM_THREADS', '1')\n",
        "    hb('Thread env set')\n",
        "\n",
        "    import lightgbm as lgb\n",
        "    hb('lightgbm imported')\n",
        "\n",
        "    # GPU-optimized base params\n",
        "    gpu_params = {\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': 7,\n",
        "        'metric': 'multi_logloss',\n",
        "        'learning_rate': 0.02,\n",
        "        'num_leaves': 96,\n",
        "        'min_data_in_leaf': 80,\n",
        "        'max_depth': -1,\n",
        "        'feature_fraction': 0.85,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 1,\n",
        "        'lambda_l1': 1.0,\n",
        "        'lambda_l2': 2.0,\n",
        "        'max_bin': 255,\n",
        "        'min_sum_hessian_in_leaf': 1e-2,\n",
        "        'force_col_wise': True,\n",
        "        'num_threads': 16,\n",
        "        'deterministic': True,\n",
        "        'device': 'gpu',\n",
        "        'gpu_platform_id': 0,\n",
        "        'gpu_device_id': 0\n",
        "    }\n",
        "\n",
        "    # CPU fallback params (used on per-model fallback if GPU fails)\n",
        "    cpu_params = {\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': 7,\n",
        "        'metric': 'multi_logloss',\n",
        "        'learning_rate': 0.025,\n",
        "        'num_leaves': 128,\n",
        "        'min_data_in_leaf': 80,\n",
        "        'max_depth': -1,\n",
        "        'feature_fraction': 0.85,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 1,\n",
        "        'lambda_l1': 1.0,\n",
        "        'lambda_l2': 2.0,\n",
        "        'max_bin': 255,\n",
        "        'force_col_wise': True,\n",
        "        'num_threads': 8,\n",
        "        'deterministic': True,\n",
        "        'device': 'cpu'\n",
        "    }\n",
        "\n",
        "    # Ensemble config (scale up for medal attempt)\n",
        "    M = 7\n",
        "    SUB_SIZE = min(600_000, X_np.shape[0])\n",
        "    VAL_FRAC = 0.05\n",
        "    N_ROUNDS_GPU = 4000\n",
        "    ES_ROUNDS_GPU = 200\n",
        "    LOG_PERIOD = 100\n",
        "\n",
        "    # CPU fallback rounds\n",
        "    N_ROUNDS_CPU = 1000\n",
        "    ES_ROUNDS_CPU = 160\n",
        "\n",
        "    # Pre-compute class counts for robust sampling\n",
        "    class_counts = np.bincount(y, minlength=7)\n",
        "    if class_counts.size < 7:\n",
        "        class_counts = np.pad(class_counts, (0, 7 - class_counts.size))\n",
        "    singleton_classes = np.where(class_counts == 1)[0]\n",
        "    singleton_class = int(singleton_classes[0]) if singleton_classes.size > 0 else None\n",
        "    hb(f'class_counts={class_counts.tolist()} singleton={singleton_class}')\n",
        "\n",
        "    test_preds_agg = np.zeros((X_test_np.shape[0], 7), dtype=np.float32)\n",
        "    oof_like_agg = np.zeros((X_np.shape[0], 7), dtype=np.float32)\n",
        "    val_acc_list = []\n",
        "\n",
        "    for m in range(M):\n",
        "        t0 = time.time()\n",
        "        seed = 42 + m\n",
        "        rng = np.random.default_rng(seed)\n",
        "        logging.info(f'[BAG {m+1}/{M}] Sampling {SUB_SIZE} rows with seed {seed}...')\n",
        "        hb(f'Bag {m+1}/{M} sampling start')\n",
        "        idx = rng.choice(X_np.shape[0], size=SUB_SIZE, replace=False)\n",
        "\n",
        "        # Ensure presence of rare classes in each bag\n",
        "        ensure_list = []\n",
        "        for c in range(7):\n",
        "            loc = np.where(y == c)[0]\n",
        "            if loc.size == 0:\n",
        "                continue\n",
        "            if singleton_class is not None and c == singleton_class:\n",
        "                k = 1\n",
        "                sel = rng.choice(loc, size=k, replace=True)\n",
        "                ensure_list.append(sel)\n",
        "            elif class_counts[c] < 2000:\n",
        "                k = min(50, loc.size)\n",
        "                sel = rng.choice(loc, size=k, replace=(loc.size < k))\n",
        "                ensure_list.append(sel)\n",
        "            else:\n",
        "                ensure_list.append(loc[:1])\n",
        "        if ensure_list:\n",
        "            ensure_idx = np.unique(np.concatenate(ensure_list))\n",
        "            idx = np.unique(np.concatenate([idx, ensure_idx]))\n",
        "\n",
        "        X_sub = X_np[idx]\n",
        "        y_sub = y[idx]\n",
        "        hb(f'Bag {m+1}/{M} subsample ready: {X_sub.shape}')\n",
        "\n",
        "        X_trn, X_val, y_trn, y_val, trn_mask, val_mask = train_test_split(\n",
        "            X_sub, y_sub, np.arange(idx.size), test_size=VAL_FRAC, random_state=seed, shuffle=True\n",
        "        )\n",
        "        logging.info(f'[BAG {m+1}/{M}] Train: {X_trn.shape}, Valid: {X_val.shape}')\n",
        "        hb(f'Bag {m+1}/{M} split: trn {X_trn.shape} val {X_val.shape}')\n",
        "\n",
        "        # Per-row weights: inverse-frequency, capped for stability\n",
        "        cls, counts = np.unique(y_trn, return_counts=True)\n",
        "        inv = (counts.sum() / counts)\n",
        "        inv = inv / inv.mean()\n",
        "        inv = np.minimum(inv, 10.0)\n",
        "        w_map = {int(c): float(w) for c, w in zip(cls, inv)}\n",
        "        w_trn = np.array([w_map.get(int(t), 1.0) for t in y_trn], dtype=np.float32)\n",
        "        logging.info(f\"[BAG {m+1}/{M}] Class weights (capped): \" + ','.join(f'{int(c)}:{w_map[int(c)]:.3f}' for c in sorted(w_map.keys())))\n",
        "        hb(f'Bag {m+1}/{M} weights ready')\n",
        "\n",
        "        # Ensure consistent binning across GPU and CPU by fixing Dataset params\n",
        "        dtrain = lgb.Dataset(X_trn, label=y_trn, weight=w_trn, free_raw_data=True, params={'max_bin': 255})\n",
        "        dvalid = lgb.Dataset(X_val, label=y_val, reference=dtrain, free_raw_data=True)\n",
        "\n",
        "        # Try GPU first, fall back to CPU on failure\n",
        "        params = dict(gpu_params)\n",
        "        params['seed'] = seed; params['bagging_seed'] = seed; params['feature_fraction_seed'] = seed\n",
        "        num_rounds = N_ROUNDS_GPU\n",
        "        es_rounds = ES_ROUNDS_GPU\n",
        "        used_device = 'gpu'\n",
        "        logging.info(f'[BAG {m+1}/{M}] Training LightGBM on GPU...')\n",
        "        print(f'[BAG-HEARTBEAT] Model {m+1}/{M} training start (GPU)...', flush=True)\n",
        "        hb(f'Bag {m+1}/{M} lgb.train start (GPU)')\n",
        "        try:\n",
        "            model = lgb.train(\n",
        "                params=params,\n",
        "                train_set=dtrain,\n",
        "                num_boost_round=num_rounds,\n",
        "                valid_sets=[dtrain, dvalid],\n",
        "                valid_names=['train','valid'],\n",
        "                callbacks=[\n",
        "                    lgb.early_stopping(stopping_rounds=es_rounds, verbose=False),\n",
        "                    lgb.log_evaluation(period=LOG_PERIOD)\n",
        "                ]\n",
        "            )\n",
        "        except Exception as e_gpu:\n",
        "            logging.warning(f'[BAG {m+1}/{M}] GPU failed ({e_gpu}); falling back to CPU...')\n",
        "            hb(f'Bag {m+1}/{M} GPU failed; fallback to CPU')\n",
        "            params = dict(cpu_params);\n",
        "            params['seed'] = seed; params['bagging_seed'] = seed; params['feature_fraction_seed'] = seed\n",
        "            num_rounds = N_ROUNDS_CPU\n",
        "            es_rounds = ES_ROUNDS_CPU\n",
        "            used_device = 'cpu'\n",
        "            print(f'[BAG-HEARTBEAT] Model {m+1}/{M} retry training (CPU)...', flush=True)\n",
        "            model = lgb.train(\n",
        "                params=params,\n",
        "                train_set=dtrain,\n",
        "                num_boost_round=num_rounds,\n",
        "                valid_sets=[dtrain, dvalid],\n",
        "                valid_names=['train','valid'],\n",
        "                callbacks=[\n",
        "                    lgb.early_stopping(stopping_rounds=es_rounds, verbose=False),\n",
        "                    lgb.log_evaluation(period=LOG_PERIOD)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        print(f'[BAG-HEARTBEAT] Model {m+1}/{M} trained on {used_device}: best_iter={model.best_iteration}', flush=True)\n",
        "        hb(f'Bag {m+1}/{M} trained on {used_device}, best_iter={model.best_iteration}')\n",
        "\n",
        "        # Validation metrics\n",
        "        val_proba = model.predict(X_val, num_iteration=model.best_iteration)\n",
        "        val_pred = np.argmax(val_proba, axis=1)\n",
        "        val_acc = accuracy_score(y_val, val_pred)\n",
        "        val_acc_list.append(val_acc)\n",
        "        logging.info(f'[BAG {m+1}/{M}] Valid ACC: {val_acc:.6f} @iter {model.best_iteration} (device={used_device})')\n",
        "        hb(f'Bag {m+1}/{M} valid acc={val_acc:.6f}')\n",
        "\n",
        "        # OOF-like storage back to original indices\n",
        "        sub_idx = idx[val_mask]\n",
        "        oof_like_agg[sub_idx] = val_proba\n",
        "\n",
        "        # Predict test\n",
        "        test_fold = model.predict(X_test_np, num_iteration=model.best_iteration)\n",
        "        test_preds_agg += (test_fold / M)\n",
        "\n",
        "        # Save partial artifacts\n",
        "        np.save('bag_test_preds_partial.npy', test_preds_agg)\n",
        "        np.save('bag_oof_like_partial.npy', oof_like_agg)\n",
        "        pd.DataFrame({'Id': test_ids, 'Cover_Type': np.argmax(test_preds_agg, axis=1) + 1}).to_csv('submission_partial.csv', index=False)\n",
        "        logging.info(f'[BAG {m+1}/{M}] Partial artifacts saved. Elapsed this model: {time.time()-t0:.1f}s')\n",
        "        hb(f'Bag {m+1}/{M} partial artifacts saved')\n",
        "\n",
        "        # Cleanup\n",
        "        del X_trn, X_val, y_trn, y_val, dtrain, dvalid, model, val_proba, test_fold, w_trn\n",
        "        gc.collect()\n",
        "        hb(f'Bag {m+1}/{M} cleanup done')\n",
        "\n",
        "    # Post-processing for class 5 (singleton in train) before saving final submission\n",
        "    logging.info('Applying post-processing for class 5...')\n",
        "    hb('Post-processing class 5 start')\n",
        "    try:\n",
        "        X_test_df = pd.read_feather('X_test.feather')[features]\n",
        "        col_c5 = 'Wilderness_Area_4' if 'Wilderness_Area_4' in X_test_df.columns else ('Wilderness_Area4' if 'Wilderness_Area4' in X_test_df.columns else None)\n",
        "        if col_c5 is not None and 'Elevation' in X_test_df.columns:\n",
        "            class5_mask = (X_test_df[col_c5] == 1) & (X_test_df['Elevation'] > 3400)\n",
        "            num_overridden = int(class5_mask.sum())\n",
        "        else:\n",
        "            class5_mask = np.zeros(X_test_df.shape[0], dtype=bool)\n",
        "            num_overridden = 0\n",
        "    except Exception:\n",
        "        class5_mask = np.zeros(X_test_np.shape[0], dtype=bool)\n",
        "        num_overridden = 0\n",
        "\n",
        "    final_pred_labels = np.argmax(test_preds_agg, axis=1)\n",
        "    if num_overridden > 0:\n",
        "        # zero-based class index 4 corresponds to Cover_Type=5\n",
        "        final_pred_labels[class5_mask.values if hasattr(class5_mask, 'values') else class5_mask] = 4\n",
        "        logging.info(f'Overrode {num_overridden} test predictions to class 5 (index 4).')\n",
        "    hb(f'Post-processing overrides: {num_overridden}')\n",
        "\n",
        "    # Final artifacts\n",
        "    np.save('bag_test_preds.npy', test_preds_agg)\n",
        "    np.save('bag_oof_like.npy', oof_like_agg)\n",
        "    submission = pd.DataFrame({'Id': test_ids, 'Cover_Type': final_pred_labels + 1})\n",
        "    submission.to_csv('submission.csv', index=False)\n",
        "    logging.info(f'[BAG] Done. Models: {M}, mean val ACC: {np.mean(val_acc_list):.6f}. Total elapsed: {time.time()-t0_all:.1f}s')\n",
        "    print('[BAG] Finished bagging. Submission saved as submission.csv'); sys.stdout.flush()\n",
        "    hb('Finished and saved submission.csv')\n",
        "\n",
        "except Exception as e:\n",
        "    print('[BAG][ERROR]', e); sys.stdout.flush()\n",
        "    hb(f'ERROR: {e}')\n",
        "    with open('bagging_error.log', 'w') as f:\n",
        "        f.write(traceback.format_exc())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BAG] Starting bagging ensemble training (GPU-first)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-08 16:09:58,938 [INFO] Loading cached data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-08 16:10:01,456 [INFO] Data ready. X: (3600000, 70), X_test: (400000, 70)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-08 16:10:01,507 [INFO] [BAG 1/7] Sampling 600000 rows with seed 42...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-08 16:10:02,511 [INFO] [BAG 1/7] Train: (570044, 70), Valid: (30003, 70)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-08 16:10:02,671 [INFO] [BAG 1/7] Class weights (capped): 0:0.000,1:0.000,2:0.000,3:0.079,4:6.916,5:0.004,6:0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-08 16:10:02,672 [INFO] [BAG 1/7] Training LightGBM on GPU...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BAG-HEARTBEAT] Model 1/7 training start (GPU)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Although \"deterministic\" is set, the results ran by GPU may be non-deterministic.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Although \"deterministic\" is set, the results ran by GPU may be non-deterministic.\n[LightGBM] [Info] This is the GPU trainer!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Total Bins 6045\n[LightGBM] [Info] Number of data points in the train set: 570044, number of used features: 68\n2025-09-08 16:10:03,864 [WARNING] [BAG 1/7] GPU failed (No OpenCL device found); falling back to CPU...\n"
          ]
        }
      ]
    },
    {
      "id": "06883c4e-389b-4f85-bd6e-617f5f495edb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Probe: verify CWD and file writes work, and list current directory\n",
        "import os, time, json, numpy as np, pandas as pd\n",
        "ts = time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "with open('probe_marker.txt', 'w') as f:\n",
        "    f.write(f'probe at {ts}')\n",
        "np.save('probe_array.npy', np.array([42], dtype=np.int32))\n",
        "print('[PROBE] Wrote probe_marker.txt and probe_array.npy at', ts)\n",
        "files = sorted(os.listdir('.'))\n",
        "print('[PROBE] Dir has', len(files), 'entries; showing first 50:')\n",
        "for p in files[:50]:\n",
        "    try:\n",
        "        print(p, os.path.getsize(p))\n",
        "    except Exception:\n",
        "        print(p, -1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PROBE] Wrote probe_marker.txt and probe_array.npy at 2025-09-08 15:48:22\n[PROBE] Dir has 33 entries; showing first 50:\n01_preprocess.ipynb 10811\n02_train.ipynb 48627\nX.feather 243019218\nX_test.feather 27051754\nagent_metadata 4096\ncatboost.ipynb 10575\ncatboost_info 4096\ndescription.md 3903\ndir_list.txt 580\ndocker_run.log 1235073\nfeatures.json 1191\nfeatures_count.txt 2\nfold_indices.npy 144000751\nmain.ipynb 22020\npreprocess_meta.json 26\nprobe_array.npy 132\nprobe_marker.txt 28\nquick_error.log 1168\nrequirements.txt 2021\nrun_preprocess.log 600\nrun_quick.log 409\nrun_train.log 548\nsample_submission.csv 3889096\nsanity_lgb_version.txt 5\nsanity_marker.txt 30\nsubmission.csv 3889096\ntask.txt 2943\ntest.csv 53996233\ntest_ids.npy 1600128\ntmp_probe.npy 140\ntrain.csv 493166255\nxgboost.ipynb 10608\ny.npy 3600128\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}