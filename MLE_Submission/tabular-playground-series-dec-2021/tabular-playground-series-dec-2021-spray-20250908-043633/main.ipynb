{
  "cells": [
    {
      "id": "1c7ad21b-6ffc-4432-b5d2-4b5361f94423",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TPS Dec 2021 - Plan, Log, and Baseline\n",
        "\n",
        "## Plan\n",
        "- Goal: Achieve medal-level accuracy (>= 0.9566) on Tabular Playground Series Dec 2021.\n",
        "- Dataset: Synthetic forest cover type; target classes 1..7. Features likely include numeric geomorphology and one-hot Wilderness_Area and Soil_Type.\n",
        "- Approach:\n",
        "  1) Quick EDA: shapes, dtypes, target distribution.\n",
        "  2) Baseline model: LightGBM with stratified 5-fold CV, strong regularization and early stopping; log CV accuracy.\n",
        "  3) Improve: Tune LGBM params; try CatBoost/XGBoost; blending/ensemble if needed.\n",
        "  4) Feature engineering: simple interactions if beneficial (e.g., sums of soil/wilderness, elevation-related ratios).\n",
        "  5) Generate submission when CV >= 0.9566.\n",
        "- Reproducibility: fixed seeds, clear logging.\n",
        "\n",
        "## Experiment Log\n",
        "- [ ] Exp001: LGBM baseline 5-fold, early stopping, default/tuned params.\n",
        "- [ ] Exp002: Param tune LGBM.\n",
        "- [ ] Exp003: CatBoost baseline.\n",
        "- [ ] Exp004: XGBoost baseline.\n",
        "- [ ] Exp005: Blend/stack best models.\n",
        "\n",
        "We will request expert reviews at key milestones (post-plan, post-EDA, post-baseline, and before long training)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "fa890940-b93d-4fb1-a366-696d835edc72",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, time, gc, warnings, math, subprocess, importlib\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.set_printoptions(suppress=True)\n",
        "pd.set_option('display.max_columns', 200)\n",
        "\n",
        "SEED = 42\n",
        "N_SPLITS = 5  # switched to 5-fold for faster iteration\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "def ensure_package(pkg_name: str, import_name: str = None, extra_install: str = ''):\n",
        "    name = import_name or pkg_name\n",
        "    try:\n",
        "        return importlib.import_module(name)\n",
        "    except ImportError:\n",
        "        print(f\"[INFO] Installing {pkg_name}...\")\n",
        "        cmd = [sys.executable, '-m', 'pip', 'install', pkg_name] + ([extra_install] if extra_install else [])\n",
        "        subprocess.check_call(cmd)\n",
        "        return importlib.import_module(name)\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "# LightGBM\n",
        "lgb = ensure_package('lightgbm', 'lightgbm')\n",
        "\n",
        "t0 = time.time()\n",
        "print(\"[INFO] Loading data...\")\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "print(f\"[INFO] train shape: {train.shape}, test shape: {test.shape}\")\n",
        "\n",
        "# Basic checks\n",
        "assert 'Cover_Type' in train.columns, 'Target Cover_Type not found in train.csv'\n",
        "if 'Id' in train.columns:\n",
        "    print('[INFO] Found Id column in train')\n",
        "if 'Id' in test.columns:\n",
        "    print('[INFO] Found Id column in test')\n",
        "\n",
        "print('[INFO] Missing values train:', train.isnull().sum().sum(), ' | test:', test.isnull().sum().sum())\n",
        "\n",
        "# Target re-indexing to 0..6\n",
        "y_raw = train['Cover_Type'].values\n",
        "y = y_raw - 1\n",
        "\n",
        "# Drop target and Id from features\n",
        "feature_cols = [c for c in train.columns if c not in ['Cover_Type']]\n",
        "if 'Id' in feature_cols:\n",
        "    feature_cols.remove('Id')\n",
        "\n",
        "X = train[feature_cols].copy()\n",
        "X_test = test[[c for c in test.columns if c != 'Id']].copy()\n",
        "\n",
        "# --- Feature Engineering ---\n",
        "print('[INFO] Feature engineering...')\n",
        "# 1) Distribution shift feature on Elevation\n",
        "elev_threshold = X_test['Elevation'].median()  # capture test distribution shift\n",
        "X['is_high_elevation'] = (X['Elevation'] > elev_threshold).astype(np.int8)\n",
        "X_test['is_high_elevation'] = (X_test['Elevation'] > elev_threshold).astype(np.int8)\n",
        "\n",
        "# 2) Hydrology features\n",
        "if set(['Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology']).issubset(X.columns):\n",
        "    X['Hydrology_Euclid'] = np.sqrt(X['Horizontal_Distance_To_Hydrology']**2 + X['Vertical_Distance_To_Hydrology']**2)\n",
        "    X_test['Hydrology_Euclid'] = np.sqrt(X_test['Horizontal_Distance_To_Hydrology']**2 + X_test['Vertical_Distance_To_Hydrology']**2)\n",
        "    X['Elev_minus_VertHydro'] = X['Elevation'] - X['Vertical_Distance_To_Hydrology']\n",
        "    X_test['Elev_minus_VertHydro'] = X_test['Elevation'] - X_test['Vertical_Distance_To_Hydrology']\n",
        "\n",
        "# 3) Hillshade features\n",
        "hill_cols = [c for c in ['Hillshade_9am','Hillshade_Noon','Hillshade_3pm'] if c in X.columns]\n",
        "if len(hill_cols) == 3:\n",
        "    X['Hillshade_Mean'] = X[hill_cols].mean(axis=1)\n",
        "    X_test['Hillshade_Mean'] = X_test[hill_cols].mean(axis=1)\n",
        "    X['Hillshade_Min'] = X[hill_cols].min(axis=1)\n",
        "    X_test['Hillshade_Min'] = X_test[hill_cols].min(axis=1)\n",
        "    X['Hillshade_Max'] = X[hill_cols].max(axis=1)\n",
        "    X_test['Hillshade_Max'] = X_test[hill_cols].max(axis=1)\n",
        "    X['Hillshade_Range'] = X['Hillshade_Max'] - X['Hillshade_Min']\n",
        "    X_test['Hillshade_Range'] = X_test['Hillshade_Max'] - X_test['Hillshade_Min']\n",
        "\n",
        "# 4) Distance interactions commonly used in this dataset\n",
        "dist_cols = ['Horizontal_Distance_To_Fire_Points','Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Hydrology']\n",
        "if set(dist_cols).issubset(X.columns):\n",
        "    hf, rr, hh = dist_cols\n",
        "    # pairwise abs diffs\n",
        "    X['DistDiff_Fire_Road'] = (X[hf] - X[rr]).abs()\n",
        "    X_test['DistDiff_Fire_Road'] = (X_test[hf] - X_test[rr]).abs()\n",
        "    X['DistDiff_Fire_Hydro'] = (X[hf] - X[hh]).abs()\n",
        "    X_test['DistDiff_Fire_Hydro'] = (X_test[hf] - X_test[hh]).abs()\n",
        "    X['DistDiff_Road_Hydro'] = (X[rr] - X[hh]).abs()\n",
        "    X_test['DistDiff_Road_Hydro'] = (X_test[rr] - X_test[hh]).abs()\n",
        "    # aggregates\n",
        "    X['DistMean_FRH'] = (X[hf] + X[rr] + X[hh]) / 3.0\n",
        "    X_test['DistMean_FRH'] = (X_test[hf] + X_test[rr] + X_test[hh]) / 3.0\n",
        "    X['DistSum_FRH'] = (X[hf] + X[rr] + X[hh])\n",
        "    X_test['DistSum_FRH'] = (X_test[hf] + X_test[rr] + X_test[hh])\n",
        "    X['DistMin_FRH'] = X[[hf, rr, hh]].min(axis=1)\n",
        "    X_test['DistMin_FRH'] = X_test[[hf, rr, hh]].min(axis=1)\n",
        "    X['DistMax_FRH'] = X[[hf, rr, hh]].max(axis=1)\n",
        "    X_test['DistMax_FRH'] = X_test[[hf, rr, hh]].max(axis=1)\n",
        "\n",
        "# 5) Sum of one-hot groups\n",
        "soil_cols = [c for c in X.columns if c.startswith('Soil_Type_')]\n",
        "wild_cols = [c for c in X.columns if c.startswith('Wilderness_Area_')]\n",
        "if soil_cols:\n",
        "    X['Soil_Type_Count'] = X[soil_cols].sum(axis=1)\n",
        "    X_test['Soil_Type_Count'] = X_test[soil_cols].sum(axis=1)\n",
        "if wild_cols:\n",
        "    X['Wilderness_Area_Count'] = X[wild_cols].sum(axis=1)\n",
        "    X_test['Wilderness_Area_Count'] = X_test[wild_cols].sum(axis=1)\n",
        "\n",
        "# 6) Aspect encoding (sin/cos)\n",
        "if 'Aspect' in X.columns:\n",
        "    X['Aspect_sin'] = np.sin(np.deg2rad(X['Aspect']))\n",
        "    X_test['Aspect_sin'] = np.sin(np.deg2rad(X_test['Aspect']))\n",
        "    X['Aspect_cos'] = np.cos(np.deg2rad(X['Aspect']))\n",
        "    X_test['Aspect_cos'] = np.cos(np.deg2rad(X_test['Aspect']))\n",
        "\n",
        "features = X.columns.tolist()\n",
        "print(f\"[INFO] Final feature count: {len(features)}\")\n",
        "\n",
        "# --- Cross-Validation Training (LightGBM) ---\n",
        "params = {\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': 7,\n",
        "    'metric': 'multi_logloss',\n",
        "    'learning_rate': 0.03,\n",
        "    'num_leaves': 48,\n",
        "    'min_data_in_leaf': 96,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.75,\n",
        "    'bagging_freq': 1,\n",
        "    'lambda_l1': 1.0,\n",
        "    'lambda_l2': 2.0,\n",
        "    'max_bin': 128,\n",
        "    'bin_construct_sample_cnt': 200000,\n",
        "    'verbose': -1,\n",
        "    'seed': SEED,\n",
        "    'num_threads': 24,\n",
        "    'first_metric_only': True,\n",
        "    'deterministic': True,\n",
        "    'feature_pre_filter': False\n",
        "}\n",
        "\n",
        "# Use consistent folds if available\n",
        "fold_file = 'fold_indices.npy'\n",
        "if os.path.exists(fold_file):\n",
        "    folds = np.load(fold_file, allow_pickle=True).tolist()\n",
        "else:\n",
        "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "    folds = list(skf.split(X, y))\n",
        "    np.save(fold_file, np.array(folds, dtype=object))\n",
        "    print(f\"[INFO] Saved fold indices to {fold_file}\")\n",
        "\n",
        "oof_preds = np.zeros((X.shape[0], 7), dtype=np.float32)\n",
        "test_preds = np.zeros((X_test.shape[0], 7), dtype=np.float32)\n",
        "fold_acc = []\n",
        "\n",
        "print('[INFO] Starting CV training...')\n",
        "for fold, (trn_idx, val_idx) in enumerate(folds, 1):\n",
        "    f_t = time.time()\n",
        "    print(f\"[FOLD {fold}/{N_SPLITS}] Train: {len(trn_idx)}, Valid: {len(val_idx)}\")\n",
        "    X_trn = X.iloc[trn_idx]\n",
        "    y_trn = y[trn_idx]\n",
        "    X_val = X.iloc[val_idx]\n",
        "    y_val = y[val_idx]\n",
        "\n",
        "    lgb_train = lgb.Dataset(X_trn, label=y_trn, free_raw_data=False)\n",
        "    lgb_valid = lgb.Dataset(X_val, label=y_val, free_raw_data=False)\n",
        "\n",
        "    model = lgb.train(\n",
        "        params=params,\n",
        "        train_set=lgb_train,\n",
        "        num_boost_round=5000,\n",
        "        valid_sets=[lgb_train, lgb_valid],\n",
        "        valid_names=['train','valid'],\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=200, verbose=False),\n",
        "            lgb.log_evaluation(period=100)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    val_pred_proba = model.predict(X_val, num_iteration=model.best_iteration)\n",
        "    oof_preds[val_idx] = val_pred_proba\n",
        "    val_pred = np.argmax(val_pred_proba, axis=1)\n",
        "    acc = accuracy_score(y_val, val_pred)\n",
        "    fold_acc.append(acc)\n",
        "    print(f\"[FOLD {fold}] ACC={acc:.6f} | best_iter={model.best_iteration} | elapsed={time.time()-f_t:.1f}s\")\n",
        "    \n",
        "    test_fold_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
        "    test_preds += test_fold_pred / N_SPLITS\n",
        "    \n",
        "    del X_trn, X_val, y_trn, y_val, lgb_train, lgb_valid, model, val_pred_proba, test_fold_pred\n",
        "    gc.collect()\n",
        "\n",
        "oof_pred_labels = np.argmax(oof_preds, axis=1)\n",
        "cv_acc = accuracy_score(y, oof_pred_labels)\n",
        "print(f\"[CV] Mean ACC over {N_SPLITS} folds: {np.mean(fold_acc):.6f}; OOF ACC: {cv_acc:.6f}\")\n",
        "\n",
        "# Save preds for ensembling\n",
        "np.save('lgb_oof_preds.npy', oof_preds)\n",
        "np.save('lgb_test_preds.npy', test_preds)\n",
        "print('[INFO] Saved lgb_oof_preds.npy and lgb_test_preds.npy')\n",
        "\n",
        "# --- Submission ---\n",
        "sub = pd.DataFrame({\n",
        "    'Id': test['Id'].values if 'Id' in test.columns else np.arange(len(test)),\n",
        "    'Cover_Type': np.argmax(test_preds, axis=1) + 1  # back to 1..7\n",
        "})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('[INFO] Saved submission.csv')\n",
        "print(f\"[DONE] Total elapsed: {time.time()-t0:.1f}s\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "id": "e0e4c9bd-be55-4f8b-80bd-38601271603f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ultra-fast sanity run on a small head subset to validate pipeline and get quick CV\n",
        "import sys, time, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "N_ROWS = 120_000  # small head subset for speed\n",
        "SEED_FAST = 2021\n",
        "N_SPLITS_FAST = 5\n",
        "\n",
        "import importlib, subprocess, sys\n",
        "def ensure_pkg(name):\n",
        "    try:\n",
        "        return importlib.import_module(name)\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', name])\n",
        "        return importlib.import_module(name)\n",
        "\n",
        "lgb_fast = ensure_pkg('lightgbm')\n",
        "\n",
        "t0 = time.time()\n",
        "print('[FAST] Loading head subset and test...'); sys.stdout.flush()\n",
        "train = pd.read_csv('train.csv', nrows=N_ROWS)\n",
        "test_full = pd.read_csv('test.csv')\n",
        "print(f\"[FAST] train_head shape: {train.shape} | test shape: {test_full.shape}\"); sys.stdout.flush()\n",
        "\n",
        "y = train['Cover_Type'].values - 1\n",
        "feature_cols = [c for c in train.columns if c not in ['Cover_Type', 'Id']]\n",
        "X = train[feature_cols].copy()\n",
        "X_test = test_full[[c for c in test_full.columns if c != 'Id']].copy()\n",
        "\n",
        "# --- Feature Engineering (same as main) ---\n",
        "print('[FAST] Feature engineering...'); sys.stdout.flush()\n",
        "elev_threshold = X_test['Elevation'].median()\n",
        "X['is_high_elevation'] = (X['Elevation'] > elev_threshold).astype(np.int8)\n",
        "X_test['is_high_elevation'] = (X_test['Elevation'] > elev_threshold).astype(np.int8)\n",
        "\n",
        "if set(['Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology']).issubset(X.columns):\n",
        "    X['Hydrology_Euclid'] = np.sqrt(X['Horizontal_Distance_To_Hydrology']**2 + X['Vertical_Distance_To_Hydrology']**2)\n",
        "    X_test['Hydrology_Euclid'] = np.sqrt(X_test['Horizontal_Distance_To_Hydrology']**2 + X_test['Vertical_Distance_To_Hydrology']**2)\n",
        "    X['Elev_minus_VertHydro'] = X['Elevation'] - X['Vertical_Distance_To_Hydrology']\n",
        "    X_test['Elev_minus_VertHydro'] = X_test['Elevation'] - X_test['Vertical_Distance_To_Hydrology']\n",
        "\n",
        "hill_cols = [c for c in ['Hillshade_9am','Hillshade_Noon','Hillshade_3pm'] if c in X.columns]\n",
        "if len(hill_cols) == 3:\n",
        "    X['Hillshade_Mean'] = X[hill_cols].mean(axis=1)\n",
        "    X_test['Hillshade_Mean'] = X_test[hill_cols].mean(axis=1)\n",
        "    X['Hillshade_Min'] = X[hill_cols].min(axis=1)\n",
        "    X_test['Hillshade_Min'] = X_test[hill_cols].min(axis=1)\n",
        "    X['Hillshade_Max'] = X[hill_cols].max(axis=1)\n",
        "    X_test['Hillshade_Max'] = X_test[hill_cols].max(axis=1)\n",
        "    X['Hillshade_Range'] = X['Hillshade_Max'] - X['Hillshade_Min']\n",
        "    X_test['Hillshade_Range'] = X_test['Hillshade_Max'] - X_test['Hillshade_Min']\n",
        "\n",
        "dist_cols = ['Horizontal_Distance_To_Fire_Points','Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Hydrology']\n",
        "if set(dist_cols).issubset(X.columns):\n",
        "    hf, rr, hh = dist_cols\n",
        "    X['DistDiff_Fire_Road'] = (X[hf] - X[rr]).abs()\n",
        "    X_test['DistDiff_Fire_Road'] = (X_test[hf] - X_test[rr]).abs()\n",
        "    X['DistDiff_Fire_Hydro'] = (X[hf] - X[hh]).abs()\n",
        "    X_test['DistDiff_Fire_Hydro'] = (X_test[hf] - X_test[hh]).abs()\n",
        "    X['DistDiff_Road_Hydro'] = (X[rr] - X[hh]).abs()\n",
        "    X_test['DistDiff_Road_Hydro'] = (X_test[rr] - X_test[hh]).abs()\n",
        "    X['DistMean_FRH'] = (X[hf] + X[rr] + X[hh]) / 3.0\n",
        "    X_test['DistMean_FRH'] = (X_test[hf] + X_test[rr] + X_test[hh]) / 3.0\n",
        "    X['DistSum_FRH'] = (X[hf] + X[rr] + X[hh])\n",
        "    X_test['DistSum_FRH'] = (X_test[hf] + X_test[rr] + X_test[hh])\n",
        "    X['DistMin_FRH'] = X[[hf, rr, hh]].min(axis=1)\n",
        "    X_test['DistMin_FRH'] = X_test[[hf, rr, hh]].min(axis=1)\n",
        "    X['DistMax_FRH'] = X[[hf, rr, hh]].max(axis=1)\n",
        "    X_test['DistMax_FRH'] = X_test[[hf, rr, hh]].max(axis=1)\n",
        "\n",
        "soil_cols = [c for c in X.columns if c.startswith('Soil_Type_')]\n",
        "wild_cols = [c for c in X.columns if c.startswith('Wilderness_Area_')]\n",
        "if soil_cols:\n",
        "    X['Soil_Type_Count'] = X[soil_cols].sum(axis=1)\n",
        "    X_test['Soil_Type_Count'] = X_test[soil_cols].sum(axis=1)\n",
        "if wild_cols:\n",
        "    X['Wilderness_Area_Count'] = X[wild_cols].sum(axis=1)\n",
        "    X_test['Wilderness_Area_Count'] = X_test[wild_cols].sum(axis=1)\n",
        "\n",
        "if 'Aspect' in X.columns:\n",
        "    X['Aspect_sin'] = np.sin(np.deg2rad(X['Aspect']))\n",
        "    X_test['Aspect_sin'] = np.sin(np.deg2rad(X_test['Aspect']))\n",
        "    X['Aspect_cos'] = np.cos(np.deg2rad(X['Aspect']))\n",
        "    X_test['Aspect_cos'] = np.cos(np.deg2rad(X_test['Aspect']))\n",
        "\n",
        "params_fast = {\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': 7,\n",
        "    'metric': 'multi_logloss',\n",
        "    'learning_rate': 0.05,\n",
        "    'num_leaves': 48,\n",
        "    'min_data_in_leaf': 96,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.75,\n",
        "    'bagging_freq': 1,\n",
        "    'lambda_l1': 1.0,\n",
        "    'lambda_l2': 2.0,\n",
        "    'max_bin': 128,\n",
        "    'bin_construct_sample_cnt': 200000,\n",
        "    'verbose': -1,\n",
        "    'seed': SEED_FAST,\n",
        "    'num_threads': 16,\n",
        "    'first_metric_only': True,\n",
        "    'deterministic': True,\n",
        "    'feature_pre_filter': False\n",
        "}\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS_FAST, shuffle=True, random_state=SEED_FAST)\n",
        "oof = np.zeros((X.shape[0], 7), dtype=np.float32)\n",
        "tst = np.zeros((X_test.shape[0], 7), dtype=np.float32)\n",
        "accs = []\n",
        "print('[FAST] Starting 5-fold LGBM on head subset...'); sys.stdout.flush()\n",
        "for i, (tr, va) in enumerate(skf.split(X, y), 1):\n",
        "    fts = time.time()\n",
        "    print(f'[FAST][FOLD {i}] tr={len(tr)} va={len(va)}'); sys.stdout.flush()\n",
        "    dtr = lgb_fast.Dataset(X.iloc[tr], label=y[tr], free_raw_data=False)\n",
        "    dva = lgb_fast.Dataset(X.iloc[va], label=y[va], free_raw_data=False)\n",
        "    model = lgb_fast.train(\n",
        "        params_fast,\n",
        "        dtr,\n",
        "        num_boost_round=2000,\n",
        "        valid_sets=[dtr, dva],\n",
        "        valid_names=['train','valid'],\n",
        "        callbacks=[\n",
        "            lgb_fast.early_stopping(stopping_rounds=100, verbose=False),\n",
        "            lgb_fast.log_evaluation(period=100)\n",
        "        ]\n",
        "    )\n",
        "    pva = model.predict(X.iloc[va], num_iteration=model.best_iteration)\n",
        "    oof[va] = pva\n",
        "    pred = np.argmax(pva, axis=1)\n",
        "    acc = accuracy_score(y[va], pred)\n",
        "    accs.append(acc)\n",
        "    print(f'[FAST][FOLD {i}] acc={acc:.6f} best_iter={model.best_iteration} elapsed={time.time()-fts:.1f}s'); sys.stdout.flush()\n",
        "    pt = model.predict(X_test, num_iteration=model.best_iteration)\n",
        "    tst += pt / N_SPLITS_FAST\n",
        "    del dtr, dva, model, pva, pt\n",
        "    gc.collect()\n",
        "\n",
        "oof_lbl = np.argmax(oof, axis=1)\n",
        "cv_acc = accuracy_score(y, oof_lbl)\n",
        "print(f'[FAST][CV] mean_acc={np.mean(accs):.6f} | OOF={cv_acc:.6f} | total_elapsed={time.time()-t0:.1f}s'); sys.stdout.flush()\n",
        "\n",
        "sub_fast = pd.DataFrame({'Id': test_full['Id'].values, 'Cover_Type': np.argmax(tst, axis=1) + 1})\n",
        "sub_fast.to_csv('submission_fast.csv', index=False)\n",
        "print('[FAST] Saved submission_fast.csv')"
      ],
      "execution_count": 12,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}