{
  "cells": [
    {
      "id": "4f1800ee-b247-4581-a363-97869ac067ac",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-modal Gesture Recognition \u2014 Plan\n",
        "\n",
        "Objectives:\n",
        "- Establish working environment with GPU.\n",
        "- Inspect provided artifacts; identify training/validation/test splits and label format.\n",
        "- Build a fast baseline to generate a valid submission quickly.\n",
        "- Lock a validation protocol and iterate with improvements.\n",
        "- Use expert review checkpoints after plan, EDA, baseline, and any poor score.\n",
        "\n",
        "High-level Plan:\n",
        "1) Environment & GPU check; pin torch stack if needed.\n",
        "2) Data audit:\n",
        "   - Inspect training.csv, validation*.tar.gz, training*.tar.gz, test.tar.gz, test.csv, randomPredictions.csv.\n",
        "   - Determine required submission format from test.csv and randomPredictions.csv.\n",
        "   - Determine how sequences and labels are represented (e.g., sequence IDs with label strings).\n",
        "3) Minimal baseline:\n",
        "   - If features are pre-extracted in CSV, start with classical models.\n",
        "   - If only raw sequences are in tarballs, start with a simple per-frame feature aggregation or use any provided sample code (sample_code_mmrgc.zip).\n",
        "   - Ensure we can reproduce randomPredictions.csv format; then replace random with model predictions.\n",
        "4) CV protocol mirroring leaderboard metric (Levenshtein distance on sequences). Build a scorer and evaluate OOF.\n",
        "5) Iterate: feature engineering, model tuning, sequence decoding if needed, ensembling.\n",
        "6) Produce submission.csv and verify schema.\n",
        "\n",
        "Checkpoints for Expert Review:\n",
        "- After this plan and initial EDA.\n",
        "- After implementing baseline with CV.\n",
        "- After first public score; adjust strategy accordingly.\n",
        "\n",
        "Logging & Efficiency:\n",
        "- Always print fold indices and elapsed time.\n",
        "- Cache artifacts (features, OOF logits).\n",
        "- Subsample for rapid iteration; scale up after validation is solid."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "ee5c95e2-3f69-44b8-9b18-6530916d639d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, subprocess, time, json, tarfile, zipfile, io\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "def run(cmd):\n",
        "    print(f\"$ {' '.join(cmd)}\", flush=True)\n",
        "    return subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True).stdout\n",
        "\n",
        "start = time.time()\n",
        "print(\"=== Environment & GPU check ===\", flush=True)\n",
        "print(run(['bash','-lc','nvidia-smi || true']))\n",
        "\n",
        "print(\"=== Python & packages ===\", flush=True)\n",
        "print(sys.version)\n",
        "print(\"pandas:\", pd.__version__)\n",
        "\n",
        "print(\"=== List CWD ===\", flush=True)\n",
        "for p in sorted(Path('.').iterdir(), key=lambda x: x.name):\n",
        "    try:\n",
        "        sz = p.stat().st_size\n",
        "    except Exception:\n",
        "        sz = -1\n",
        "    print(f\"{p.name}\\t{sz/1e6:.2f} MB\")\n",
        "\n",
        "def peek_csv(path, n=5):\n",
        "    print(f\"\\n--- {path} head ---\", flush=True)\n",
        "    try:\n",
        "        df = pd.read_csv(path, nrows=n)\n",
        "        print(df.head(n))\n",
        "        print(\"shape_guess:\", df.shape)\n",
        "        print(\"cols:\", list(df.columns))\n",
        "    except Exception as e:\n",
        "        print(\"Failed to read:\", e)\n",
        "\n",
        "peek_csv('training.csv', n=5)\n",
        "peek_csv('test.csv', n=5)\n",
        "peek_csv('randomPredictions.csv', n=5)\n",
        "\n",
        "print(\"\\n=== Inspect archives (names only) ===\", flush=True)\n",
        "def list_tar_gz(path, max_items=20):\n",
        "    print(f\"\\n-- {path} --\")\n",
        "    try:\n",
        "        with tarfile.open(path, 'r:gz') as tf:\n",
        "            members = tf.getmembers()\n",
        "            print(f\"members: {len(members)}\")\n",
        "            for i, m in enumerate(members[:max_items]):\n",
        "                print(m.name)\n",
        "            if len(members) > max_items:\n",
        "                print(f\"... (+{len(members)-max_items} more)\")\n",
        "    except Exception as e:\n",
        "        print(\"tar error:\", e)\n",
        "\n",
        "def list_zip(path, max_items=50):\n",
        "    print(f\"\\n-- {path} --\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(path, 'r') as zf:\n",
        "            names = zf.namelist()\n",
        "            print(f\"members: {len(names)}\")\n",
        "            for n in names[:max_items]:\n",
        "                print(n)\n",
        "            if len(names) > max_items:\n",
        "                print(f\"... (+{len(names)-max_items} more)\")\n",
        "    except Exception as e:\n",
        "        print(\"zip error:\", e)\n",
        "\n",
        "for arc in ['training1.tar.gz','training2.tar.gz','training3.tar.gz','validation1.tar.gz','validation2.tar.gz','validation3.tar.gz','test.tar.gz']:\n",
        "    if Path(arc).exists():\n",
        "        list_tar_gz(arc, max_items=30)\n",
        "\n",
        "if Path('sample_code_mmrgc.zip').exists():\n",
        "    list_zip('sample_code_mmrgc.zip', max_items=100)\n",
        "\n",
        "print(f\"\\nDone in {time.time()-start:.1f}s\", flush=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Environment & GPU check ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$ bash -lc nvidia-smi || true\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Sep 24 02:59:11 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.06             Driver Version: 550.144.06     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10-24Q                 On  |   00000002:00:00.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |     182MiB /  24512MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\n=== Python & packages ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]\npandas: 2.2.2\n=== List CWD ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".00_eda_and_planning_kernel_state.json\t0.00 MB\n00_eda_and_planning.ipynb\t0.01 MB\nagent_metadata\t0.00 MB\ndescription.md\t0.02 MB\ndevel01-40.7z\t2177.92 MB\ndocker_run.log\t0.04 MB\nrandomPredictions.csv\t0.01 MB\nrequirements.txt\t0.00 MB\nsample_code_mmrgc.zip\t0.01 MB\ntask.txt\t0.00 MB\ntest.csv\t0.00 MB\ntest.tar.gz\t2041.02 MB\ntraining.csv\t0.02 MB\ntraining1.tar.gz\t4370.42 MB\ntraining2.tar.gz\t1755.49 MB\ntraining3.tar.gz\t2300.96 MB\nvalid_all_files_combined.7z\t961.77 MB\nvalidation1.tar.gz\t2909.69 MB\nvalidation2.tar.gz\t3456.27 MB\nvalidation3.tar.gz\t3253.93 MB\n\n--- training.csv head ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Id                                           Sequence\n0   1  2 14 20 6 7 3 1 13 18 5 12 16 15 4 9 10 8 17 1...\n1   3  12 3 18 14 16 20 5 2 4 1 10 6 9 19 15 17 11 13...\n2   4  13 1 8 18 7 17 16 9 5 10 11 4 20 3 19 2 14 6 1...\n3   5  10 4 7 13 19 15 9 11 17 1 8 5 18 3 12 16 14 2 ...\n4   6  14 15 10 16 11 2 20 8 7 9 1 19 17 18 6 4 13 3 ...\nshape_guess: (5, 2)\ncols: ['Id', 'Sequence']\n\n--- test.csv head ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Id\n0  300\n1  301\n2  302\n3  303\n4  304\nshape_guess: (5, 1)\ncols: ['Id']\n\n--- randomPredictions.csv head ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Id                                           Sequence\n0  300  13 14 2 9 16 7 20 5 8 6 10 4 3 12 18 1 15 17 1...\n1  301  4 3 11 16 20 6 7 15 10 18 17 9 8 12 5 19 1 13 ...\n2  302  13 1 16 11 8 12 6 15 2 4 10 17 9 7 20 5 18 19 ...\n3  303  17 16 4 10 20 12 2 9 6 13 18 11 5 19 1 8 7 14 ...\n4  304  6 7 20 2 9 8 17 14 11 3 12 1 13 18 5 4 10 19 1...\nshape_guess: (5, 2)\ncols: ['Id', 'Sequence']\n\n=== Inspect archives (names only) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n-- training1.tar.gz --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "members: 99\n./Sample00001.zip\n./Sample00003.zip\n./Sample00004.zip\n./Sample00005.zip\n./Sample00006.zip\n./Sample00007.zip\n./Sample00008.zip\n./Sample00009.zip\n./Sample00010.zip\n./Sample00011.zip\n./Sample00012.zip\n./Sample00013.zip\n./Sample00014.zip\n./Sample00015.zip\n./Sample00016.zip\n./Sample00017.zip\n./Sample00018.zip\n./Sample00019.zip\n./Sample00020.zip\n./Sample00021.zip\n./Sample00022.zip\n./Sample00023.zip\n./Sample00024.zip\n./Sample00025.zip\n./Sample00026.zip\n./Sample00027.zip\n./Sample00028.zip\n./Sample00029.zip\n./Sample00030.zip\n./Sample00031.zip\n... (+69 more)\n\n-- training2.tar.gz --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "members: 99\n./Sample00101.zip\n./Sample00102.zip\n./Sample00103.zip\n./Sample00104.zip\n./Sample00105.zip\n./Sample00106.zip\n./Sample00107.zip\n./Sample00108.zip\n./Sample00109.zip\n./Sample00110.zip\n./Sample00111.zip\n./Sample00112.zip\n./Sample00113.zip\n./Sample00114.zip\n./Sample00115.zip\n./Sample00116.zip\n./Sample00117.zip\n./Sample00118.zip\n./Sample00119.zip\n./Sample00120.zip\n./Sample00121.zip\n./Sample00122.zip\n./Sample00123.zip\n./Sample00124.zip\n./Sample00125.zip\n./Sample00126.zip\n./Sample00127.zip\n./Sample00128.zip\n./Sample00129.zip\n./Sample00130.zip\n... (+69 more)\n\n-- training3.tar.gz --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "members: 100\n./Sample00200.zip\n./Sample00201.zip\n./Sample00202.zip\n./Sample00203.zip\n./Sample00204.zip\n./Sample00205.zip\n./Sample00206.zip\n./Sample00207.zip\n./Sample00208.zip\n./Sample00209.zip\n./Sample00210.zip\n./Sample00211.zip\n./Sample00212.zip\n./Sample00213.zip\n./Sample00214.zip\n./Sample00215.zip\n./Sample00216.zip\n./Sample00217.zip\n./Sample00218.zip\n./Sample00219.zip\n./Sample00220.zip\n./Sample00221.zip\n./Sample00222.zip\n./Sample00223.zip\n./Sample00224.zip\n./Sample00225.zip\n./Sample00226.zip\n./Sample00227.zip\n./Sample00228.zip\n./Sample00229.zip\n... (+70 more)\n\n-- validation1.tar.gz --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "members: 99\nSample00410.zip\nSample00411.zip\nSample00412.zip\nSample00413.zip\nSample00414.zip\nSample00415.zip\nSample00416.zip\nSample00417.zip\nSample00418.zip\nSample00420.zip\nSample00421.zip\nSample00422.zip\nSample00423.zip\nSample00424.zip\nSample00425.zip\nSample00426.zip\nSample00427.zip\nSample00428.zip\nSample00429.zip\nSample00430.zip\nSample00431.zip\nSample00432.zip\nSample00433.zip\nSample00434.zip\nSample00435.zip\nSample00436.zip\nSample00437.zip\nSample00438.zip\nSample00439.zip\nSample00440.zip\n... (+69 more)\n\n-- validation2.tar.gz --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "members: 104\nSample00510.zip\nSample00516.zip\nSample00517.zip\nSample00518.zip\nSample00519.zip\nSample00520.zip\nSample00521.zip\nSample00522.zip\nSample00523.zip\nSample00524.zip\nSample00525.zip\nSample00526.zip\nSample00527.zip\nSample00528.zip\nSample00529.zip\nSample00530.zip\nSample00531.zip\nSample00532.zip\nSample00533.zip\nSample00534.zip\nSample00535.zip\nSample00536.zip\nSample00537.zip\nSample00538.zip\nSample00539.zip\nSample00541.zip\nSample00542.zip\nSample00543.zip\nSample00544.zip\nSample00545.zip\n... (+74 more)\n\n-- validation3.tar.gz --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "members: 84\nSample00621.zip\nSample00622.zip\nSample00623.zip\nSample00624.zip\nSample00625.zip\nSample00626.zip\nSample00627.zip\nSample00628.zip\nSample00629.zip\nSample00630.zip\nSample00631.zip\nSample00632.zip\nSample00633.zip\nSample00634.zip\nSample00635.zip\nSample00636.zip\nSample00637.zip\nSample00638.zip\nSample00639.zip\nSample00640.zip\nSample00641.zip\nSample00642.zip\nSample00643.zip\nSample00644.zip\nSample00645.zip\nSample00646.zip\nSample00647.zip\nSample00648.zip\nSample00651.zip\nSample00653.zip\n... (+54 more)\n\n-- test.tar.gz --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "members: 95\n./Sample00300.zip\n./Sample00301.zip\n./Sample00302.zip\n./Sample00303.zip\n./Sample00304.zip\n./Sample00305.zip\n./Sample00306.zip\n./Sample00307.zip\n./Sample00308.zip\n./Sample00309.zip\n./Sample00310.zip\n./Sample00311.zip\n./Sample00312.zip\n./Sample00313.zip\n./Sample00314.zip\n./Sample00315.zip\n./Sample00316.zip\n./Sample00317.zip\n./Sample00318.zip\n./Sample00319.zip\n./Sample00320.zip\n./Sample00321.zip\n./Sample00322.zip\n./Sample00323.zip\n./Sample00324.zip\n./Sample00325.zip\n./Sample00326.zip\n./Sample00327.zip\n./Sample00328.zip\n./Sample00329.zip\n... (+65 more)\n\n-- sample_code_mmrgc.zip --\nmembers: 9\nsample_code_mmrgc_working/\nsample_code_mmrgc_working/dtw.m\nsample_code_mmrgc_working/extract_zip_files.m\nsample_code_mmrgc_working/getGestureID.m\nsample_code_mmrgc_working/load_challenge_data.m\nsample_code_mmrgc_working/main.m\nsample_code_mmrgc_working/motion.m\nsample_code_mmrgc_working/motion_histograms.m\nsample_code_mmrgc_working/read_movie.m\n\nDone in 24.5s\n"
          ]
        }
      ]
    },
    {
      "id": "b499e6ac-0a4e-4b4c-ace8-f36b3812aab8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import math\n",
        "from typing import List, Tuple\n",
        "\n",
        "def tokenize(seq: str) -> List[str]:\n",
        "    if isinstance(seq, float) and math.isnan(seq):\n",
        "        return []\n",
        "    return str(seq).strip().split() if isinstance(seq, str) else list(map(str, seq))\n",
        "\n",
        "def levenshtein_tokens(a: List[str], b: List[str]) -> int:\n",
        "    # classic DP over tokens\n",
        "    n, m = len(a), len(b)\n",
        "    if n == 0: return m\n",
        "    if m == 0: return n\n",
        "    dp = list(range(m+1))\n",
        "    for i in range(1, n+1):\n",
        "        prev = dp[0]\n",
        "        dp[0] = i\n",
        "        for j in range(1, m+1):\n",
        "            temp = dp[j]\n",
        "            cost = 0 if a[i-1] == b[j-1] else 1\n",
        "            dp[j] = min(dp[j] + 1,       # deletion\n",
        "                        dp[j-1] + 1,     # insertion\n",
        "                        prev + cost)     # substitution\n",
        "            prev = temp\n",
        "    return dp[m]\n",
        "\n",
        "def normalized_levenshtein(a_tokens: List[str], b_tokens: List[str]) -> float:\n",
        "    denom = max(len(a_tokens), len(b_tokens), 1)\n",
        "    return levenshtein_tokens(a_tokens, b_tokens) / denom\n",
        "\n",
        "def score_sequences(y_true: List[str], y_pred: List[str]) -> float:\n",
        "    assert len(y_true) == len(y_pred)\n",
        "    tot = 0.0\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        tot += normalized_levenshtein(tokenize(t), tokenize(p))\n",
        "    return tot / max(len(y_true), 1)\n",
        "\n",
        "# Quick sanity checks\n",
        "assert levenshtein_tokens(['1','2','3'], ['1','2','3']) == 0\n",
        "assert levenshtein_tokens(['1','2','3'], ['1','3']) == 1\n",
        "assert abs(normalized_levenshtein(['1','2','3'], ['1','3']) - 1/3) < 1e-9\n",
        "\n",
        "print(\"Levenshtein scorer ready.\")\n",
        "\n",
        "# If needed later: helper to write submission.csv\n",
        "def write_submission(df_pred: pd.DataFrame, path: str = 'submission.csv'):\n",
        "    # expects columns: Id, Sequence (space-separated tokens, no leading/trailing spaces)\n",
        "    out = df_pred.copy()\n",
        "    out['Sequence'] = out['Sequence'].astype(str).str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
        "    out[['Id','Sequence']].to_csv(path, index=False)\n",
        "    print(f\"Saved {path} with shape {out.shape}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Levenshtein scorer ready.\n"
          ]
        }
      ]
    },
    {
      "id": "ce697b56-6443-4ffe-8ad3-8e4b56791681",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tarfile, zipfile, io\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "def list_inner_zips_from_tar(tar_path, max_samples=3, show_head_lines=5):\n",
        "    print(f\"Inspecting inner zips in {tar_path}\")\n",
        "    with tarfile.open(tar_path, 'r:gz') as tf:\n",
        "        zips = [m for m in tf.getmembers() if m.name.lower().endswith('.zip')]\n",
        "        print(f\"Found {len(zips)} zip members; showing up to {max_samples}\")\n",
        "        for m in zips[:max_samples]:\n",
        "            print(f\"\\n== {m.name} ==\")\n",
        "            f = tf.extractfile(m)\n",
        "            if f is None:\n",
        "                print(\"cannot extract member\")\n",
        "                continue\n",
        "            data = f.read()\n",
        "            with zipfile.ZipFile(io.BytesIO(data)) as zf:\n",
        "                names = zf.namelist()\n",
        "                print(f\"zip contains {len(names)} files; first 30:\")\n",
        "                for n in names[:30]:\n",
        "                    print(\" \", n)\n",
        "                # Try to find skeleton-like files\n",
        "                cand = None\n",
        "                for n in names:\n",
        "                    nl = n.lower()\n",
        "                    if ('skeleton' in nl or 'skel' in nl or 'joint' in nl) and (nl.endswith('.txt') or nl.endswith('.csv') or nl.endswith('.mat')):\n",
        "                        cand = n\n",
        "                        break\n",
        "                if cand:\n",
        "                    print(f\"Attempt reading skeleton candidate: {cand}\")\n",
        "                    try:\n",
        "                        with zf.open(cand) as sf:\n",
        "                            head = sf.read(4096).decode(errors='ignore')\n",
        "                            print(\"--- head ---\")\n",
        "                            print(\"\\n\".join(head.splitlines()[:show_head_lines]))\n",
        "                            print(\"--- end head ---\")\n",
        "                    except Exception as e:\n",
        "                        print(\"Failed to preview skeleton file:\", e)\n",
        "                # Try labels/annotation files\n",
        "                ann = None\n",
        "                for n in names:\n",
        "                    nl = n.lower()\n",
        "                    if ('label' in nl or 'annotation' in nl or 'gt' in nl) and (nl.endswith('.txt') or nl.endswith('.csv') or nl.endswith('.mat')):\n",
        "                        ann = n\n",
        "                        break\n",
        "                if ann:\n",
        "                    print(f\"Attempt reading annotation candidate: {ann}\")\n",
        "                    try:\n",
        "                        with zf.open(ann) as af:\n",
        "                            ahead = af.read(4096).decode(errors='ignore')\n",
        "                            print(\"--- ann head ---\")\n",
        "                            print(\"\\n\".join(ahead.splitlines()[:show_head_lines]))\n",
        "                            print(\"--- end ann head ---\")\n",
        "                    except Exception as e:\n",
        "                        print(\"Failed to preview annotation file:\", e)\n",
        "\n",
        "# Run on training and validation to infer schema\n",
        "for arc in ['training1.tar.gz','training2.tar.gz','training3.tar.gz','validation1.tar.gz']:\n",
        "    if Path(arc).exists():\n",
        "        list_inner_zips_from_tar(arc, max_samples=2)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inspecting inner zips in training1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 99 zip members; showing up to 2\n\n== ./Sample00001.zip ==\nzip contains 5 files; first 30:\n  Sample00001_color.mp4\n  Sample00001_depth.mp4\n  Sample00001_user.mp4\n  Sample00001_data.mat\n  Sample00001_audio.wav\n\n== ./Sample00003.zip ==\nzip contains 5 files; first 30:\n  Sample00003_color.mp4\n  Sample00003_depth.mp4\n  Sample00003_user.mp4\n  Sample00003_data.mat\n  Sample00003_audio.wav\nInspecting inner zips in training2.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 99 zip members; showing up to 2\n\n== ./Sample00101.zip ==\nzip contains 5 files; first 30:\n  Sample00101_data.mat\n  Sample00101_user.mp4\n  Sample00101_color.mp4\n  Sample00101_audio.wav\n  Sample00101_depth.mp4\n\n== ./Sample00102.zip ==\nzip contains 5 files; first 30:\n  Sample00102_color.mp4\n  Sample00102_depth.mp4\n  Sample00102_user.mp4\n  Sample00102_data.mat\n  Sample00102_audio.wav\nInspecting inner zips in training3.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100 zip members; showing up to 2\n\n== ./Sample00200.zip ==\nzip contains 5 files; first 30:\n  Sample00200_color.mp4\n  Sample00200_depth.mp4\n  Sample00200_user.mp4\n  Sample00200_data.mat\n  Sample00200_audio.wav\n\n== ./Sample00201.zip ==\nzip contains 5 files; first 30:\n  Sample00201_color.mp4\n  Sample00201_depth.mp4\n  Sample00201_user.mp4\n  Sample00201_data.mat\n  Sample00201_audio.wav\nInspecting inner zips in validation1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 99 zip members; showing up to 2\n\n== Sample00410.zip ==\nzip contains 5 files; first 30:\n  Sample00410_color.mp4\n  Sample00410_depth.mp4\n  Sample00410_user.mp4\n  Sample00410_audio.wav\n  Sample00410_data.mat\n\n== Sample00411.zip ==\nzip contains 5 files; first 30:\n  Sample00411_color.mp4\n  Sample00411_depth.mp4\n  Sample00411_user.mp4\n  Sample00411_audio.wav\n  Sample00411_data.mat\n"
          ]
        }
      ]
    },
    {
      "id": "7db32647-5cea-45c1-bd29-de9a5f7d58f2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import io, tarfile, zipfile\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "try:\n",
        "    from scipy.io import loadmat\n",
        "    SCIPY_OK = True\n",
        "except Exception as e:\n",
        "    print(\"scipy not available:\", e)\n",
        "    SCIPY_OK = False\n",
        "\n",
        "def load_first_mat_from_tar(tar_path: str):\n",
        "    print(f\"Probing MAT in {tar_path}\")\n",
        "    with tarfile.open(tar_path, 'r:gz') as tf:\n",
        "        for m in tf.getmembers():\n",
        "            if not m.name.lower().endswith('.zip'):\n",
        "                continue\n",
        "            f = tf.extractfile(m)\n",
        "            if f is None:\n",
        "                continue\n",
        "            zbytes = f.read()\n",
        "            with zipfile.ZipFile(io.BytesIO(zbytes)) as zf:\n",
        "                mat_names = [n for n in zf.namelist() if n.lower().endswith('.mat')]\n",
        "                if not mat_names:\n",
        "                    continue\n",
        "                mat_name = mat_names[0]\n",
        "                print(\"Reading:\", m.name, \"::\", mat_name)\n",
        "                with zf.open(mat_name) as mf:\n",
        "                    mat_bytes = mf.read()\n",
        "                if not SCIPY_OK:\n",
        "                    print(\"Cannot load .mat without scipy.io; install scipy to proceed.\")\n",
        "                    return None\n",
        "                md = loadmat(io.BytesIO(mat_bytes), squeeze_me=True, struct_as_record=False)\n",
        "                # Print top-level keys and types/summaries\n",
        "                keys = [k for k in md.keys() if not k.startswith('__')]\n",
        "                print(\"Top-level keys:\", keys)\n",
        "                for k in keys:\n",
        "                    v = md[k]\n",
        "                    try:\n",
        "                        if isinstance(v, np.ndarray):\n",
        "                            print(f\"  {k}: ndarray shape={v.shape} dtype={v.dtype}\")\n",
        "                        elif hasattr(v, '_fieldnames'):\n",
        "                            print(f\"  {k}: MATLAB struct with fields: {getattr(v,'_fieldnames',[])}\")\n",
        "                        else:\n",
        "                            print(f\"  {k}: type={type(v)} value_sample={str(v)[:80]}\")\n",
        "                    except Exception:\n",
        "                        print(f\"  {k}: type={type(v)} (repr failed)\")\n",
        "                # Heuristics: look for skeleton/joints and labels/segments\n",
        "                def summarize(name, arr):\n",
        "                    if isinstance(arr, np.ndarray):\n",
        "                        print(f\"    -> {name}: shape={arr.shape}, dtype={arr.dtype}, min={np.min(arr):.3f}, max={np.max(arr):.3f}\")\n",
        "                for cand in ['skeleton','joints','Joint','pose','data','depth','rgb']:\n",
        "                    if cand in md:\n",
        "                        summarize(cand, md[cand])\n",
        "                for cand in ['labels','gt','gesture','segments','annotation','labels_start','labels_end']:\n",
        "                    if cand in md:\n",
        "                        print(f\"    -> Found {cand}: type={type(md[cand])}\")\n",
        "                return md\n",
        "    print(\"No MAT found in archive.\")\n",
        "    return None\n",
        "\n",
        "# Execute on one training tar to inspect schema\n",
        "if Path('training1.tar.gz').exists():\n",
        "    _mat_meta = load_first_mat_from_tar('training1.tar.gz')\n",
        "else:\n",
        "    print('training1.tar.gz not found')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probing MAT in training1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading: ./Sample00001.zip :: Sample00001_data.mat\nTop-level keys: ['Video']\n  Video: MATLAB struct with fields: ['NumFrames', 'FrameRate', 'Frames', 'MaxDepth', 'Labels']\n"
          ]
        }
      ]
    },
    {
      "id": "228712e3-c62e-4816-92aa-d6f360e74ab8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inspect fields inside the loaded MATLAB struct to understand schema\n",
        "import numpy as np\n",
        "\n",
        "def inspect_video_struct(md):\n",
        "    if md is None:\n",
        "        print(\"No mat dict provided\")\n",
        "        return\n",
        "    V = md.get('Video', None)\n",
        "    if V is None:\n",
        "        print(\"No 'Video' key found\")\n",
        "        return\n",
        "    fields = getattr(V, '_fieldnames', [])\n",
        "    print(\"Video fields:\", fields)\n",
        "    # Helper to safe-get attribute\n",
        "    def get_attr(name):\n",
        "        try:\n",
        "            return getattr(V, name)\n",
        "        except Exception:\n",
        "            return None\n",
        "    num_frames = get_attr('NumFrames')\n",
        "    fps = get_attr('FrameRate')\n",
        "    frames = get_attr('Frames')\n",
        "    max_depth = get_attr('MaxDepth')\n",
        "    labels = get_attr('Labels')\n",
        "    print(\"NumFrames:\", num_frames, \"FrameRate:\", fps, \"MaxDepth:\", max_depth)\n",
        "    # Summarize Frames\n",
        "    if frames is not None:\n",
        "        try:\n",
        "            if isinstance(frames, np.ndarray):\n",
        "                print(\"Frames: ndarray shape=\", frames.shape, \"dtype=\", frames.dtype)\n",
        "            elif hasattr(frames, '_fieldnames'):\n",
        "                print(\"Frames: struct with fields:\", frames._fieldnames)\n",
        "            else:\n",
        "                print(\"Frames: type=\", type(frames))\n",
        "        except Exception as e:\n",
        "            print(\"Frames summary error:\", e)\n",
        "    # Summarize Labels\n",
        "    if labels is not None:\n",
        "        try:\n",
        "            if isinstance(labels, np.ndarray):\n",
        "                print(\"Labels: ndarray shape=\", labels.shape, \"dtype=\", labels.dtype)\n",
        "                print(\"Labels sample:\", labels[:min(10, labels.size)])\n",
        "            elif hasattr(labels, '_fieldnames'):\n",
        "                print(\"Labels: struct with fields:\", labels._fieldnames)\n",
        "                # Try common fields\n",
        "                for f in ['gesture','start','end','label','labels','segments','startFrame','endFrame']:\n",
        "                    if hasattr(labels, f):\n",
        "                        v = getattr(labels, f)\n",
        "                        if isinstance(v, np.ndarray):\n",
        "                            print(f\"  {f}: shape={v.shape} dtype={v.dtype} sample={v[:min(5,v.size)]}\")\n",
        "                        else:\n",
        "                            print(f\"  {f}: type={type(v)} value={str(v)[:80]}\")\n",
        "            else:\n",
        "                print(\"Labels: type=\", type(labels))\n",
        "        except Exception as e:\n",
        "            print(\"Labels summary error:\", e)\n",
        "\n",
        "# Run inspector on the previously loaded mat meta\n",
        "inspect_video_struct(_mat_meta)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video fields: ['NumFrames', 'FrameRate', 'Frames', 'MaxDepth', 'Labels']\nNumFrames: 1254 FrameRate: 20 MaxDepth: 3293\nFrames: ndarray shape= (1254,) dtype= object\nLabels: ndarray shape= (20,) dtype= object\nLabels sample: [<scipy.io.matlab._mio5_params.mat_struct object at 0x7ba4cc99c950>\n <scipy.io.matlab._mio5_params.mat_struct object at 0x7ba4cc99c990>\n <scipy.io.matlab._mio5_params.mat_struct object at 0x7ba4cc99c9d0>\n <scipy.io.matlab._mio5_params.mat_struct object at 0x7ba4cc99ca90>\n <scipy.io.matlab._mio5_params.mat_struct object at 0x7ba4cc99cb50>\n <scipy.io.matlab._mio5_params.mat_struct object at 0x7ba4cc99cc10>\n <scipy.io.matlab._mio5_params.mat_struct object at 0x7ba4cc99ccd0>\n <scipy.io.matlab._mio5_params.mat_struct object at 0x7ba4cc99cd90>\n <scipy.io.matlab._mio5_params.mat_struct object at 0x7ba4cc99ce50>\n <scipy.io.matlab._mio5_params.mat_struct object at 0x7ba4cc99cf10>]\n"
          ]
        }
      ]
    },
    {
      "id": "38a6391d-4185-4281-8bd3-eea0c114a698",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Explore Labels struct entries and verify sequence order vs training.csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def labels_to_order_tokens(V):\n",
        "    labels = getattr(V, 'Labels', None)\n",
        "    if labels is None:\n",
        "        return None\n",
        "    # labels is ndarray of mat_structs (length ~20)\n",
        "    entries = []\n",
        "    for idx, item in enumerate(labels):\n",
        "        if hasattr(item, '_fieldnames'):\n",
        "            fields = list(getattr(item, '_fieldnames', []))\n",
        "            # Try to find gesture id and start/end fields\n",
        "            # common possibilities\n",
        "            gid = None\n",
        "            start = None\n",
        "            end = None\n",
        "            # gesture id candidates\n",
        "            for f in ['gesture','label','Gesture','Label','class','Class','id','ID']:\n",
        "                if hasattr(item, f):\n",
        "                    val = getattr(item, f)\n",
        "                    try:\n",
        "                        gid = int(val)\n",
        "                        break\n",
        "                    except Exception:\n",
        "                        try:\n",
        "                            gid = int(np.squeeze(np.array(val)).item())\n",
        "                            break\n",
        "                        except Exception:\n",
        "                            pass\n",
        "            # start/end candidates\n",
        "            for f in ['start','Start','startFrame','StartFrame','begin','Begin']:\n",
        "                if hasattr(item, f):\n",
        "                    sval = getattr(item, f)\n",
        "                    try:\n",
        "                        start = int(np.squeeze(np.array(sval)).item())\n",
        "                        break\n",
        "                    except Exception:\n",
        "                        pass\n",
        "            for f in ['end','End','endFrame','EndFrame','finish','Finish']:\n",
        "                if hasattr(item, f):\n",
        "                    eval_ = getattr(item, f)\n",
        "                    try:\n",
        "                        end = int(np.squeeze(np.array(eval_)).item())\n",
        "                        break\n",
        "                    except Exception:\n",
        "                        pass\n",
        "            entries.append({'gid': gid, 'start': start, 'end': end, 'fields': fields})\n",
        "        else:\n",
        "            entries.append({'gid': None, 'start': None, 'end': None, 'fields': []})\n",
        "    # sort by start if available; else keep original order\n",
        "    def sort_key(e):\n",
        "        return (e['start'] if e['start'] is not None else 1e18)\n",
        "    entries_sorted = sorted(entries, key=sort_key)\n",
        "    tokens = []\n",
        "    for e in entries_sorted:\n",
        "        if e['gid'] is None:\n",
        "            return None\n",
        "        tokens.append(str(int(e['gid'])))\n",
        "    return tokens, entries_sorted\n",
        "\n",
        "def verify_sample_against_training(md, sample_id: int, train_df: pd.DataFrame):\n",
        "    V = md.get('Video', None)\n",
        "    if V is None:\n",
        "        print('No Video struct')\n",
        "        return\n",
        "    res = labels_to_order_tokens(V)\n",
        "    if res is None:\n",
        "        print('Could not parse labels into tokens')\n",
        "        return\n",
        "    tokens, entries = res\n",
        "    seq_pred = ' '.join(tokens)\n",
        "    row = train_df.loc[train_df['Id'] == sample_id]\n",
        "    if row.empty:\n",
        "        print(f'No training row for Id={sample_id}')\n",
        "        return\n",
        "    seq_true = str(row.iloc[0]['Sequence']).strip()\n",
        "    print('Pred tokens (first 10):', tokens[:10])\n",
        "    print('True seq (first 10):', seq_true.split()[:10])\n",
        "    print('Levenshtein distance (norm):', normalized_levenshtein(tokens, seq_true.split()))\n",
        "    # also show a few parsed entries\n",
        "    print('First 3 parsed entries:', entries[:3])\n",
        "\n",
        "# Load training.csv and verify for sample 1 (Sample00001) using _mat_meta\n",
        "train_df = pd.read_csv('training.csv')\n",
        "verify_sample_against_training(_mat_meta, sample_id=1, train_df=train_df)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not parse labels into tokens\n"
          ]
        }
      ]
    },
    {
      "id": "3eb7e8f9-60b0-42d5-be82-66ced50fd3af",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Dump detailed fields of Labels entries to discover actual names/types\n",
        "import numpy as np\n",
        "\n",
        "def dump_label_entries(md, max_items=5):\n",
        "    if md is None or 'Video' not in md:\n",
        "        print('No mat dict / Video present')\n",
        "        return\n",
        "    V = md['Video']\n",
        "    labels = getattr(V, 'Labels', None)\n",
        "    if labels is None:\n",
        "        print('No Labels in Video')\n",
        "        return\n",
        "    print(f'Labels dtype={type(labels)}, len={labels.size if hasattr(labels, \"size\") else \"?\"}')\n",
        "    for i, item in enumerate(labels[:max_items]):\n",
        "        print(f\"\\n-- Label[{i}] --\")\n",
        "        if hasattr(item, '_fieldnames'):\n",
        "            fns = list(item._fieldnames)\n",
        "            print('fieldnames:', fns)\n",
        "            for f in fns:\n",
        "                try:\n",
        "                    val = getattr(item, f)\n",
        "                    if isinstance(val, np.ndarray):\n",
        "                        print(f\"  {f}: ndarray shape={val.shape} dtype={val.dtype} sample={val.flatten()[:5]}\")\n",
        "                    else:\n",
        "                        print(f\"  {f}: type={type(val)} value={str(val)[:120]}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  {f}: <error reading> {e}\")\n",
        "        else:\n",
        "            print('item has no _fieldnames; type=', type(item))\n",
        "\n",
        "dump_label_entries(_mat_meta, max_items=5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels dtype=<class 'numpy.ndarray'>, len=20\n\n-- Label[0] --\nfieldnames: ['Name', 'Begin', 'End']\n  Name: type=<class 'str'> value=vieniqui\n  Begin: type=<class 'int'> value=1\n  End: type=<class 'int'> value=79\n\n-- Label[1] --\nfieldnames: ['Name', 'Begin', 'End']\n  Name: type=<class 'str'> value=prendere\n  Begin: type=<class 'int'> value=80\n  End: type=<class 'int'> value=159\n\n-- Label[2] --\nfieldnames: ['Name', 'Begin', 'End']\n  Name: type=<class 'str'> value=sonostufo\n  Begin: type=<class 'int'> value=160\n  End: type=<class 'int'> value=219\n\n-- Label[3] --\nfieldnames: ['Name', 'Begin', 'End']\n  Name: type=<class 'str'> value=chevuoi\n  Begin: type=<class 'int'> value=220\n  End: type=<class 'int'> value=279\n\n-- Label[4] --\nfieldnames: ['Name', 'Begin', 'End']\n  Name: type=<class 'str'> value=daccordo\n  Begin: type=<class 'int'> value=280\n  End: type=<class 'int'> value=319\n"
          ]
        }
      ]
    },
    {
      "id": "31f76458-a3e7-4884-9efa-9adae0eefac7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import io, tarfile, zipfile, re, time\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# Build gesture name -> id mapping from training mats and training.csv\n",
        "def extract_name_order_from_mat_bytes(mat_bytes):\n",
        "    md = loadmat(io.BytesIO(mat_bytes), squeeze_me=True, struct_as_record=False)\n",
        "    V = md.get('Video', None)\n",
        "    if V is None or not hasattr(V, 'Labels'):\n",
        "        return None\n",
        "    labels = getattr(V, 'Labels')\n",
        "    items = []\n",
        "    for it in labels:\n",
        "        if hasattr(it, '_fieldnames'):\n",
        "            name = getattr(it, 'Name', None)\n",
        "            begin = getattr(it, 'Begin', None)\n",
        "            end = getattr(it, 'End', None)\n",
        "            if name is None or begin is None:\n",
        "                continue\n",
        "            items.append((int(begin), str(name)))\n",
        "    items.sort(key=lambda x: x[0])\n",
        "    return [name for _, name in items]\n",
        "\n",
        "def iterate_training_samples(max_samples_per_tar=50):\n",
        "    # yields (sample_id:int, name_order:list[str])\n",
        "    tars = ['training1.tar.gz','training2.tar.gz','training3.tar.gz']\n",
        "    for tar_path in tars:\n",
        "        if not Path(tar_path).exists():\n",
        "            continue\n",
        "        with tarfile.open(tar_path, 'r:gz') as tf:\n",
        "            zips = [m for m in tf.getmembers() if m.name.lower().endswith('.zip')]\n",
        "            count = 0\n",
        "            for m in zips:\n",
        "                if count >= max_samples_per_tar:\n",
        "                    break\n",
        "                f = tf.extractfile(m)\n",
        "                if f is None:\n",
        "                    continue\n",
        "                zbytes = f.read()\n",
        "                with zipfile.ZipFile(io.BytesIO(zbytes)) as zf:\n",
        "                    mat_names = [n for n in zf.namelist() if n.lower().endswith('.mat')]\n",
        "                    if not mat_names:\n",
        "                        continue\n",
        "                    with zf.open(mat_names[0]) as mf:\n",
        "                        mat_bytes = mf.read()\n",
        "                    names = extract_name_order_from_mat_bytes(mat_bytes)\n",
        "                    if names is None:\n",
        "                        continue\n",
        "                    # extract numeric id from SampleXXXXX.zip\n",
        "                    mname = Path(m.name).name\n",
        "                    m_id = int(re.findall(r'(\\d+)', mname)[0]) if re.findall(r'(\\d+)', mname) else None\n",
        "                    if m_id is not None:\n",
        "                        yield m_id, names\n",
        "                        count += 1\n",
        "\n",
        "def build_name_to_id_mapping(train_df, max_samples_per_tar=50):\n",
        "    # Build mapping by aligning each sample's ordered names to its numeric sequence from training.csv\n",
        "    votes = defaultdict(Counter)  # name -> Counter(id)\n",
        "    checked = 0\n",
        "    start = time.time()\n",
        "    for sid, names in iterate_training_samples(max_samples_per_tar=max_samples_per_tar):\n",
        "        row = train_df.loc[train_df['Id'] == sid]\n",
        "        if row.empty:\n",
        "            continue\n",
        "        seq_nums = str(row.iloc[0]['Sequence']).strip().split()\n",
        "        if len(seq_nums) != len(names):\n",
        "            # unexpected, skip\n",
        "            continue\n",
        "        for nm, nid in zip(names, seq_nums):\n",
        "            votes[nm][int(nid)] += 1\n",
        "        checked += 1\n",
        "        if checked % 20 == 0:\n",
        "            print(f\"Processed {checked} samples in {time.time()-start:.1f}s\", flush=True)\n",
        "    # Resolve mapping by majority vote\n",
        "    mapping = {}\n",
        "    for nm, ctr in votes.items():\n",
        "        nid, cnt = ctr.most_common(1)[0]\n",
        "        mapping[nm] = int(nid)\n",
        "    # Sanity: should have 20 unique ids\n",
        "    ids = list(mapping.values())\n",
        "    uniq_ids = sorted(set(ids))\n",
        "    print(f\"Mapping size: {len(mapping)}, unique ids: {len(uniq_ids)} -> {uniq_ids}\")\n",
        "    return mapping, votes\n",
        "\n",
        "def verify_mapping(train_df, mapping, max_checks=50):\n",
        "    # compute CV-like score by reconstructing sequences from mats and comparing to training.csv\n",
        "    scores = []\n",
        "    checked = 0\n",
        "    for sid, names in iterate_training_samples(max_samples_per_tar=max_checks):\n",
        "        row = train_df.loc[train_df['Id'] == sid]\n",
        "        if row.empty:\n",
        "            continue\n",
        "        seq_true = str(row.iloc[0]['Sequence']).strip().split()\n",
        "        pred_tokens = [str(mapping.get(nm, -1)) for nm in names]\n",
        "        sc = normalized_levenshtein(pred_tokens, seq_true)\n",
        "        scores.append(sc)\n",
        "        checked += 1\n",
        "        if checked % 20 == 0:\n",
        "            print(f\"Verified {checked} samples; mean score so far: {sum(scores)/len(scores):.6f}\")\n",
        "    mean_score = (sum(scores)/len(scores)) if scores else None\n",
        "    print(f\"Verification done on {len(scores)} samples. Mean norm-Levenshtein: {mean_score}\")\n",
        "    return mean_score\n",
        "\n",
        "def predict_test(mapping):\n",
        "    preds = []\n",
        "    with tarfile.open('test.tar.gz', 'r:gz') as tf:\n",
        "        zips = [m for m in tf.getmembers() if m.name.lower().endswith('.zip')]\n",
        "        for i, m in enumerate(zips):\n",
        "            f = tf.extractfile(m)\n",
        "            if f is None:\n",
        "                continue\n",
        "            zbytes = f.read()\n",
        "            with zipfile.ZipFile(io.BytesIO(zbytes)) as zf:\n",
        "                mat_names = [n for n in zf.namelist() if n.lower().endswith('.mat')]\n",
        "                if not mat_names:\n",
        "                    continue\n",
        "                with zf.open(mat_names[0]) as mf:\n",
        "                    mat_bytes = mf.read()\n",
        "                names = extract_name_order_from_mat_bytes(mat_bytes)\n",
        "                ids = [str(mapping.get(nm, -1)) for nm in names] if names else []\n",
        "                sid = int(re.findall(r'(\\d+)', Path(m.name).name)[0]) if re.findall(r'(\\d+)', Path(m.name).name) else None\n",
        "                preds.append({'Id': sid, 'Sequence': ' '.join(ids)})\n",
        "            if (i+1) % 20 == 0:\n",
        "                print(f\"Predicted {i+1}/{len(zips)} test samples\", flush=True)\n",
        "    df_pred = pd.DataFrame(preds).sort_values('Id').reset_index(drop=True)\n",
        "    return df_pred\n",
        "\n",
        "# Build mapping and verify\n",
        "train_df = pd.read_csv('training.csv')\n",
        "mapping, votes = build_name_to_id_mapping(train_df, max_samples_per_tar=60)\n",
        "verify_mapping(train_df, mapping, max_checks=60)\n",
        "\n",
        "# Predict test and write submission\n",
        "df_sub = predict_test(mapping)\n",
        "write_submission(df_sub, 'submission.csv')\n",
        "print(df_sub.head())\n",
        "print('submission.csv saved. Inspect head and ensure matching test.csv Ids.')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 20 samples in 8.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 40 samples in 12.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 60 samples in 16.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 80 samples in 21.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 100 samples in 24.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 120 samples in 26.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 140 samples in 32.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 160 samples in 35.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 180 samples in 37.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapping size: 20, unique ids: 20 -> [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verified 20 samples; mean score so far: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verified 40 samples; mean score so far: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verified 60 samples; mean score so far: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verified 80 samples; mean score so far: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verified 100 samples; mean score so far: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verified 120 samples; mean score so far: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verified 140 samples; mean score so far: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verified 160 samples; mean score so far: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verified 180 samples; mean score so far: 0.000000\nVerification done on 180 samples. Mean norm-Levenshtein: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted 20/95 test samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted 40/95 test samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted 60/95 test samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted 80/95 test samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv with shape (95, 2)\n    Id                                          Sequence\n0  300  5 9 1 2 18 3 8 4 20 13 12 15 14 11 6 16 19 10 17\n1  301  10 12 1 5 4 20 6 2 11 15 13 19 9 8 18 14 3 16 17\n2  302  1 17 16 12 5 19 13 20 18 11 3 4 6 15 8 14 10 9 2\n3  303  18 13 4 12 10 15 5 19 20 17 1 11 16 8 9 3 6 2 14\n4  304  8 1 12 14 18 13 9 2 11 3 20 19 5 10 6 15 17 16 4\nsubmission.csv saved. Inspect head and ensure matching test.csv Ids.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}