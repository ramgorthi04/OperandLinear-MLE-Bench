{
  "cells": [
    {
      "id": "a9f2605f-0964-49d1-873b-3fb789b5acf9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal submission swapper: copy a precomputed blend to submission.csv and verify\n",
        "import os, shutil, pandas as pd\n",
        "src = 'submission_blend_w8_B.csv'\n",
        "dst = 'submission.csv'\n",
        "assert os.path.exists(src), f'Missing {src}; ensure the blended files exist.'\n",
        "shutil.copyfile(src, dst)\n",
        "size = os.path.getsize(dst)\n",
        "print(f'[Swap] {dst} <- {src} ({size} bytes)')\n",
        "df = pd.read_csv(dst)\n",
        "print('[submission.csv] head:')\n",
        "print(df.head())\n",
        "print('[submission.csv] shape:', df.shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Swap] submission.csv <- submission_blend_w8_B.csv (5237 bytes)\n[submission.csv] head:\n    Id                                           Sequence\n0  300  5 9 1 2 18 3 8 4 20 13 12 15 7 14 11 19 6 10 1...\n1  301  12 2 10 1 5 4 20 6 11 15 13 7 19 9 8 18 14 3 1...\n2  302  17 16 12 5 9 13 19 7 20 18 11 3 4 6 15 8 1 14 ...\n3  303  18 13 4 15 17 12 10 5 19 20 1 11 8 16 9 7 3 6 ...\n4  304  8 1 12 14 18 13 9 7 2 11 3 15 20 19 10 5 6 17 ...\n[submission.csv] shape: (95, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "57915038-64a3-469a-95f9-19eb15fd6340",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply permutation rescue to a chosen precomputed submission and write to submission.csv\n",
        "import os, pandas as pd\n",
        "src = 'submission_blend_w7_A.csv'  # per expert advice: top candidate\n",
        "assert os.path.exists(src), f'Missing {src}'\n",
        "\n",
        "def rescue_perm20(seq_str: str) -> str:\n",
        "    arr = [int(x) for x in str(seq_str).split()]\n",
        "    seen, dup_idx = set(), []\n",
        "    for i, x in enumerate(arr):\n",
        "        if x in seen:\n",
        "            dup_idx.append(i)\n",
        "        else:\n",
        "            seen.add(x)\n",
        "    missing = [k for k in range(1, 21) if k not in seen]\n",
        "    for j, i in enumerate(dup_idx):\n",
        "        if j < len(missing):\n",
        "            arr[i] = missing[j]\n",
        "    return ' '.join(map(str, arr))\n",
        "\n",
        "df = pd.read_csv(src)\n",
        "df['Sequence'] = df['Sequence'].map(rescue_perm20)\n",
        "dst = 'submission.csv'\n",
        "df.to_csv(dst, index=False)\n",
        "print(f'[Rescue+Swap] {dst} <- {src} ({os.path.getsize(dst)} bytes)')\n",
        "print(df.head())\n",
        "print('All rows length/unique check (first 3):', [ (len(s.split()), len(set(s.split()))) for s in df['Sequence'].head(3) ])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Rescue+Swap] submission.csv <- submission_blend_w7_A.csv (5237 bytes)\n    Id                                           Sequence\n0  300  5 9 1 2 18 3 8 4 20 7 13 12 15 14 11 19 6 10 1...\n1  301  12 2 11 10 1 5 4 20 6 15 13 7 19 9 8 18 14 3 1...\n2  302  17 16 12 5 7 1 9 13 19 20 18 11 3 4 6 15 8 14 ...\n3  303  18 13 4 15 17 12 10 5 19 20 1 11 16 8 9 7 3 6 ...\n4  304  8 1 12 14 18 13 9 7 2 11 3 15 20 19 5 10 6 17 ...\nAll rows length/unique check (first 3): [(20, 20), (20, 20), (20, 20)]\n"
          ]
        }
      ]
    },
    {
      "id": "209a6e1a-a214-436f-817f-e481a96cf6db",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply permutation rescue to w6_A per expert order, then write to submission.csv\n",
        "import os, pandas as pd\n",
        "src = 'submission_blend_w6_A.csv'\n",
        "assert os.path.exists(src), f'Missing {src}'\n",
        "\n",
        "def rescue_perm20(seq_str: str) -> str:\n",
        "    arr = [int(x) for x in str(seq_str).split()]\n",
        "    seen, dup_idx = set(), []\n",
        "    for i, x in enumerate(arr):\n",
        "        if x in seen:\n",
        "            dup_idx.append(i)\n",
        "        else:\n",
        "            seen.add(x)\n",
        "    missing = [k for k in range(1, 21) if k not in seen]\n",
        "    for j, i in enumerate(dup_idx):\n",
        "        if j < len(missing):\n",
        "            arr[i] = missing[j]\n",
        "    return ' '.join(map(str, arr))\n",
        "\n",
        "df = pd.read_csv(src)\n",
        "df['Sequence'] = df['Sequence'].map(rescue_perm20)\n",
        "dst = 'submission.csv'\n",
        "df.to_csv(dst, index=False)\n",
        "print(f'[Rescue+Swap] {dst} <- {src} ({os.path.getsize(dst)} bytes)')\n",
        "print(df.head())\n",
        "print('All rows length/unique check (first 3):', [ (len(s.split()), len(set(s.split()))) for s in df['Sequence'].head(3) ])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Rescue+Swap] submission.csv <- submission_blend_w6_A.csv (5237 bytes)\n    Id                                           Sequence\n0  300  5 9 1 2 18 3 8 4 20 13 7 12 15 14 11 19 6 10 1...\n1  301  12 2 11 10 1 5 4 20 6 15 13 7 19 9 8 18 14 3 1...\n2  302  17 16 12 5 7 1 9 13 19 20 18 11 3 4 6 15 8 14 ...\n3  303  18 13 4 15 17 12 10 5 19 20 1 11 16 8 9 7 3 6 ...\n4  304  8 1 12 14 18 13 9 7 2 11 3 15 20 19 5 10 6 17 ...\nAll rows length/unique check (first 3): [(20, 20), (20, 20), (20, 20)]\n"
          ]
        }
      ]
    },
    {
      "id": "51390dd3-ab81-414b-b8a0-4e8286078aa6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply permutation rescue to w7_B (third candidate), then write to submission.csv\n",
        "import os, pandas as pd\n",
        "src = 'submission_blend_w7_B.csv'\n",
        "assert os.path.exists(src), f'Missing {src}'\n",
        "\n",
        "def rescue_perm20(seq_str: str) -> str:\n",
        "    arr = [int(x) for x in str(seq_str).split()]\n",
        "    seen, dup_idx = set(), []\n",
        "    for i, x in enumerate(arr):\n",
        "        if x in seen:\n",
        "            dup_idx.append(i)\n",
        "        else:\n",
        "            seen.add(x)\n",
        "    missing = [k for k in range(1, 21) if k not in seen]\n",
        "    for j, i in enumerate(dup_idx):\n",
        "        if j < len(missing):\n",
        "            arr[i] = missing[j]\n",
        "    return ' '.join(map(str, arr))\n",
        "\n",
        "df = pd.read_csv(src)\n",
        "df['Sequence'] = df['Sequence'].map(rescue_perm20)\n",
        "dst = 'submission.csv'\n",
        "df.to_csv(dst, index=False)\n",
        "print(f'[Rescue+Swap] {dst} <- {src} ({os.path.getsize(dst)} bytes)')\n",
        "print(df.head())\n",
        "print('All rows length/unique check (first 3):', [ (len(s.split()), len(set(s.split()))) for s in df['Sequence'].head(3) ])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Rescue+Swap] submission.csv <- submission_blend_w7_B.csv (5237 bytes)\n    Id                                           Sequence\n0  300  5 9 1 2 18 3 8 4 20 13 12 15 7 14 11 19 6 10 1...\n1  301  12 2 10 1 5 4 20 6 11 15 13 7 19 9 8 18 14 3 1...\n2  302  17 16 12 5 9 13 19 7 20 18 11 3 4 6 15 8 1 14 ...\n3  303  18 13 4 15 17 12 10 5 19 20 1 11 8 16 9 7 3 6 ...\n4  304  8 1 12 14 18 13 9 7 2 11 3 15 20 19 10 5 6 17 ...\nAll rows length/unique check (first 3): [(20, 20), (20, 20), (20, 20)]\n"
          ]
        }
      ]
    },
    {
      "id": "1b508b01-7302-4a8e-996b-c04bfc7ff33c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply permutation rescue to w6_B (hedge) and write to submission.csv\n",
        "import os, pandas as pd\n",
        "src = 'submission_blend_w6_B.csv'\n",
        "assert os.path.exists(src), f'Missing {src}'\n",
        "\n",
        "def rescue_perm20(seq_str: str) -> str:\n",
        "    arr = [int(x) for x in str(seq_str).split()]\n",
        "    seen, dup_idx = set(), []\n",
        "    for i, x in enumerate(arr):\n",
        "        if x in seen:\n",
        "            dup_idx.append(i)\n",
        "        else:\n",
        "            seen.add(x)\n",
        "    missing = [k for k in range(1, 21) if k not in seen]\n",
        "    for j, i in enumerate(dup_idx):\n",
        "        if j < len(missing):\n",
        "            arr[i] = missing[j]\n",
        "    return ' '.join(map(str, arr))\n",
        "\n",
        "df = pd.read_csv(src)\n",
        "df['Sequence'] = df['Sequence'].map(rescue_perm20)\n",
        "dst = 'submission.csv'\n",
        "df.to_csv(dst, index=False)\n",
        "print(f'[Rescue+Swap] {dst} <- {src} ({os.path.getsize(dst)} bytes)')\n",
        "print(df.head())\n",
        "print('All rows length/unique check (first 3):', [ (len(s.split()), len(set(s.split()))) for s in df['Sequence'].head(3) ])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Rescue+Swap] submission.csv <- submission_blend_w6_B.csv (5237 bytes)\n    Id                                           Sequence\n0  300  5 9 1 2 18 3 8 4 20 13 12 15 7 14 11 19 6 10 1...\n1  301  12 2 10 1 5 4 20 6 11 15 7 13 19 9 8 18 14 3 1...\n2  302  17 16 12 5 1 9 13 19 7 20 18 11 3 4 6 15 8 10 ...\n3  303  18 13 4 15 17 12 10 5 19 20 1 11 8 16 9 7 3 6 ...\n4  304  8 1 12 14 18 13 9 7 2 11 3 15 20 19 10 5 6 17 ...\nAll rows length/unique check (first 3): [(20, 20), (20, 20), (20, 20)]\n"
          ]
        }
      ]
    },
    {
      "id": "95d33709-f902-4a5d-a505-46c701e1e521",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply permutation rescue to w6_C (conservative) and write to submission.csv\n",
        "import os, pandas as pd\n",
        "src = 'submission_blend_w6_C.csv'\n",
        "assert os.path.exists(src), f'Missing {src}'\n",
        "\n",
        "def rescue_perm20(seq_str: str) -> str:\n",
        "    arr = [int(x) for x in str(seq_str).split()]\n",
        "    seen, dup_idx = set(), []\n",
        "    for i, x in enumerate(arr):\n",
        "        if x in seen:\n",
        "            dup_idx.append(i)\n",
        "        else:\n",
        "            seen.add(x)\n",
        "    missing = [k for k in range(1, 21) if k not in seen]\n",
        "    for j, i in enumerate(dup_idx):\n",
        "        if j < len(missing):\n",
        "            arr[i] = missing[j]\n",
        "    return ' '.join(map(str, arr))\n",
        "\n",
        "df = pd.read_csv(src)\n",
        "df['Sequence'] = df['Sequence'].map(rescue_perm20)\n",
        "dst = 'submission.csv'\n",
        "df.to_csv(dst, index=False)\n",
        "print(f'[Rescue+Swap] {dst} <- {src} ({os.path.getsize(dst)} bytes)')\n",
        "print(df.head())\n",
        "print('All rows length/unique check (first 3):', [ (len(s.split()), len(set(s.split()))) for s in df['Sequence'].head(3) ])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Rescue+Swap] submission.csv <- submission_blend_w6_C.csv (5237 bytes)\n    Id                                           Sequence\n0  300  5 9 1 2 18 3 8 4 20 13 12 15 7 14 11 19 6 10 1...\n1  301  12 2 20 10 1 5 4 6 11 15 7 13 19 9 8 18 14 3 1...\n2  302  17 16 12 5 1 9 13 19 7 20 18 11 3 4 6 15 8 10 ...\n3  303  18 13 4 15 17 12 10 5 19 20 1 11 8 16 9 7 3 6 ...\n4  304  8 1 12 14 18 13 9 7 2 11 3 15 20 19 10 5 6 17 ...\nAll rows length/unique check (first 3): [(20, 20), (20, 20), (20, 20)]\n"
          ]
        }
      ]
    },
    {
      "id": "9a7ff852-6af3-4ead-acc5-c3e3fd5c817a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Parameterized rescue runner: set src to any precomputed CSV, rescue to permutation-20, write submission.csv\n",
        "import os, pandas as pd\n",
        "\n",
        "def rescue_perm20(seq_str: str) -> str:\n",
        "    arr = [int(x) for x in str(seq_str).split()]\n",
        "    seen, dup_idx = set(), []\n",
        "    for i, x in enumerate(arr):\n",
        "        if x in seen:\n",
        "            dup_idx.append(i)\n",
        "        else:\n",
        "            seen.add(x)\n",
        "    missing = [k for k in range(1, 21) if k not in seen]\n",
        "    for j, i in enumerate(dup_idx):\n",
        "        if j < len(missing):\n",
        "            arr[i] = missing[j]\n",
        "    return ' '.join(map(str, arr))\n",
        "\n",
        "# Choose next candidate to try (change this line for future iterations):\n",
        "src = 'submission_blend_w8_C.csv'\n",
        "assert os.path.exists(src), f'Missing {src}'\n",
        "df = pd.read_csv(src)\n",
        "df['Sequence'] = df['Sequence'].map(rescue_perm20)\n",
        "dst = 'submission.csv'\n",
        "df.to_csv(dst, index=False)\n",
        "print(f'[Rescue+Swap] {dst} <- {src} ({os.path.getsize(dst)} bytes)')\n",
        "print(df.head())\n",
        "print('All rows length/unique check (first 3):', [ (len(s.split()), len(set(s.split()))) for s in df['Sequence'].head(3) ])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Rescue+Swap] submission.csv <- submission_blend_w8_C.csv (5237 bytes)\n    Id                                           Sequence\n0  300  5 9 1 2 18 3 8 4 20 13 12 15 7 14 11 19 6 10 1...\n1  301  12 2 20 10 1 5 4 6 11 15 13 7 19 9 8 18 14 3 1...\n2  302  17 16 12 5 9 13 19 7 20 18 11 3 4 6 15 8 1 14 ...\n3  303  18 13 4 15 17 12 10 5 19 20 1 11 8 16 9 7 3 6 ...\n4  304  8 1 12 14 18 13 9 7 2 11 3 15 20 19 10 5 6 17 ...\nAll rows length/unique check (first 3): [(20, 20), (20, 20), (20, 20)]\n"
          ]
        }
      ]
    },
    {
      "id": "a988d0a7-bd33-40a9-ae17-8f59b8ab1c26",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calibrated exact-20 + Hungarian decode using existing test probs (fast, no retrain)\n",
        "import os, glob, numpy as np, pandas as pd\n",
        "from scipy.special import softmax, logit\n",
        "from scipy.signal import convolve\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "CFG = dict(\n",
        "    test_dirs = ['cache_probs', 'cache_probs_v15'],  # [v16, v15]\n",
        "    oof_dirs = ['oof_probs_v16'],\n",
        "    use_v15 = True, w16=0.7, w15=0.3,\n",
        "    alpha = 0.85, bg_bias = 0.20, smooth_win = 3,\n",
        "    temp_grid = np.linspace(0.7, 1.6, 12).astype(np.float32),\n",
        "    eps = 1e-6,\n",
        ")\n",
        "\n",
        "def _find_npz(d, vid):\n",
        "    # Try multiple name patterns: raw id, zero-padded, any substring match\n",
        "    cands = [\n",
        "        os.path.join(d, f'{vid}.npz'),\n",
        "        os.path.join(d, f'{vid:05d}.npz'),\n",
        "        os.path.join(d, f'test_{vid:05d}.npz'),\n",
        "        os.path.join(d, f'{vid}_probs.npz'),\n",
        "    ]\n",
        "    for fn in cands:\n",
        "        if os.path.exists(fn):\n",
        "            return fn\n",
        "    hits = glob.glob(os.path.join(d, f'*{vid:05d}*.npz'))\n",
        "    if hits:\n",
        "        return hits[0]\n",
        "    hits = glob.glob(os.path.join(d, f'*{vid}*.npz'))\n",
        "    return hits[0] if hits else None\n",
        "\n",
        "def load_probs_dir(d, ids):\n",
        "    out = {}\n",
        "    for vid in ids:\n",
        "        fn = _find_npz(d, vid)\n",
        "        if not fn:\n",
        "            continue\n",
        "        z = np.load(fn, allow_pickle=True)\n",
        "        if 'probs' in z:\n",
        "            P = z['probs'].astype(np.float32)\n",
        "        elif 'P' in z:\n",
        "            P = z['P'].astype(np.float32)\n",
        "        elif 'logits' in z:\n",
        "            L = z['logits'].astype(np.float32)\n",
        "            P = softmax(L, axis=1).astype(np.float32)\n",
        "        else:\n",
        "            continue\n",
        "        out[vid] = P\n",
        "    return out\n",
        "\n",
        "def blend_probs(map16, map15, ids, w16, w15, use_v15):\n",
        "    res = {}\n",
        "    for vid in ids:\n",
        "        P16 = map16.get(vid)\n",
        "        if P16 is None:\n",
        "            continue\n",
        "        if use_v15 and (vid in map15):\n",
        "            P = (w16*P16 + w15*map15[vid]).astype(np.float32)\n",
        "            P /= np.clip(P.sum(1, keepdims=True), 1e-8, None)\n",
        "        else:\n",
        "            P = P16\n",
        "        res[vid] = P\n",
        "    return res\n",
        "\n",
        "def load_oof_frames(oof_dirs):\n",
        "    X, Y = [], []\n",
        "    for d in oof_dirs:\n",
        "        for fn in glob.glob(os.path.join(d, '*.npz')):\n",
        "            z = np.load(fn, allow_pickle=True)\n",
        "            if 'probs' in z:\n",
        "                P = z['probs'].astype(np.float32)\n",
        "            elif 'P' in z:\n",
        "                P = z['P'].astype(np.float32)\n",
        "            else:\n",
        "                continue\n",
        "            if 'y' in z:\n",
        "                y = z['y'].astype(np.int32)\n",
        "            elif 'labels' in z:\n",
        "                y = z['labels'].astype(np.int32)\n",
        "            else:\n",
        "                continue\n",
        "            if P.ndim == 2 and len(y) == P.shape[0]:\n",
        "                X.append(P); Y.append(y)\n",
        "    if not X:\n",
        "        return None, None\n",
        "    return np.concatenate(X, 0), np.concatenate(Y, 0)\n",
        "\n",
        "def fit_per_class_temperature(P_oof, y_oof, temp_grid):\n",
        "    if P_oof is None or y_oof is None:\n",
        "        return np.ones(21, dtype=np.float32)\n",
        "    C = P_oof.shape[1]\n",
        "    Tcls = np.ones(C, dtype=np.float32)\n",
        "    y = y_oof\n",
        "    for c in range(C):\n",
        "        pc = np.clip(P_oof[:, c], 1e-6, 1-1e-6)\n",
        "        z = logit(pc)\n",
        "        yc = (y == c).astype(np.float32)\n",
        "        best, bestT = 1e18, 1.0\n",
        "        for T in temp_grid:\n",
        "            p = 1.0/(1.0 + np.exp(-z/float(T)))\n",
        "            # Balanced NLL proxy\n",
        "            nll = -(yc*np.log(np.clip(p,1e-6,1))).mean() - ((1-yc)*np.log(np.clip(1-p,1e-6,1))).mean()\n",
        "            if nll < best:\n",
        "                best, bestT = float(nll), float(T)\n",
        "        Tcls[c] = bestT\n",
        "    return Tcls.astype(np.float32)\n",
        "\n",
        "def to_calibrated_logits(P, Tcls, bg_bias, smooth_win):\n",
        "    P = np.clip(P, CFG['eps'], 1-CFG['eps']).astype(np.float32)\n",
        "    Z = logit(P).astype(np.float32)\n",
        "    Z = Z / Tcls.reshape(1, -1)\n",
        "    Z[:, 0] += float(bg_bias)\n",
        "    if smooth_win and smooth_win > 1:\n",
        "        k = np.ones(int(smooth_win), np.float32) / float(smooth_win)\n",
        "        Z = np.stack([convolve(Z[:, i], k, mode='same') for i in range(Z.shape[1])], 1).astype(np.float32)\n",
        "    return Z\n",
        "\n",
        "def segment_exact20(margin, K, min_len):\n",
        "    T = int(len(margin))\n",
        "    min_len = int(max(1, min_len))\n",
        "    while K*min_len > T and min_len > 1:\n",
        "        min_len -= 1\n",
        "    if K*min_len > T:\n",
        "        K = min(K, T); min_len = 1\n",
        "    pref = np.concatenate([[0.0], np.cumsum(margin, 0).astype(np.float32)])\n",
        "    dp = -1e18*np.ones((K+1, T+1), np.float32)\n",
        "    bt = -np.ones((K+1, T+1), np.int32)\n",
        "    dp[0, 0] = 0.0\n",
        "    for k in range(1, K+1):\n",
        "        start_min = (k-1)*min_len\n",
        "        for t in range(k*min_len, T+1):\n",
        "            s_lo = max(start_min, t - (T - (K-k)*min_len))\n",
        "            bestv, bests = -1e18, -1\n",
        "            # Linear scan; T ~ 1-3k so still fast on 95 vids\n",
        "            for s in range(s_lo, t-min_len+1):\n",
        "                v = dp[k-1, s] + (pref[t] - pref[s])\n",
        "                if v > bestv:\n",
        "                    bestv, bests = v, s\n",
        "            dp[k, t] = bestv; bt[k, t] = bests\n",
        "    bounds = []\n",
        "    k, t = K, T\n",
        "    while k > 0:\n",
        "        s = int(bt[k, t]); bounds.append((s, t)); t = s; k -= 1\n",
        "    bounds.reverse()\n",
        "    return bounds\n",
        "\n",
        "def decode_video(Z, alpha):\n",
        "    T = Z.shape[0]\n",
        "    min_len = max(2, int(alpha * T / 20.0))\n",
        "    margin = (Z[:, 1:21].max(1) - Z[:, 0]).astype(np.float32)\n",
        "    segs = segment_exact20(margin, 20, min_len)\n",
        "    C = np.zeros((len(segs), 20), np.float32)\n",
        "    for j, (s, e) in enumerate(segs):\n",
        "        C[j, :] = -Z[s:e, 1:21].mean(0)\n",
        "    r, c = linear_sum_assignment(C)\n",
        "    order = [int(c[j])+1 for j in np.argsort(r)]\n",
        "    return order\n",
        "\n",
        "test_ids = pd.read_csv('test.csv')['Id'].tolist()\n",
        "map16 = load_probs_dir(CFG['test_dirs'][0], test_ids)\n",
        "map15 = load_probs_dir(CFG['test_dirs'][1], test_ids) if CFG['use_v15'] else {}\n",
        "P_oof, y_oof = load_oof_frames(CFG['oof_dirs'])\n",
        "Tcls = fit_per_class_temperature(P_oof, y_oof, CFG['temp_grid'])\n",
        "rows = []\n",
        "blend_map = blend_probs(map16, map15, test_ids, CFG['w16'], CFG['w15'], CFG['use_v15'])\n",
        "for vid in sorted(blend_map.keys()):\n",
        "    P = blend_map[vid]\n",
        "    Z = to_calibrated_logits(P, Tcls, CFG['bg_bias'], CFG['smooth_win'])\n",
        "    seq = decode_video(Z, CFG['alpha'])\n",
        "    rows.append((vid, ' '.join(map(str, seq))))\n",
        "sub = pd.DataFrame(rows, columns=['Id', 'Sequence']).sort_values('Id')\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Wrote submission.csv', sub.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv (0, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "1ce10013-14ba-4ebc-a5ce-4819c95b1417",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inspect cache prob directories to determine filename patterns\n",
        "import os, glob, itertools\n",
        "dirs = ['cache_probs', 'cache_probs_v15', 'oof_probs_v16', 'oof_probs_v15']\n",
        "for d in dirs:\n",
        "    if not os.path.isdir(d):\n",
        "        print(f'[Missing dir] {d}');\n",
        "        continue\n",
        "    files = sorted(glob.glob(os.path.join(d, '*.npz')))\n",
        "    print(f'[{d}] count={len(files)}')\n",
        "    for fn in files[:5]:\n",
        "        print('  ', os.path.basename(fn))\n",
        "    for fn in files[-5:]:\n",
        "        print('  ', os.path.basename(fn))\n",
        "    # Try to extract an example test id from filenames\n",
        "    sample = files[:10]\n",
        "    print(f'[{d}] sample patterns:')\n",
        "    for fn in sample:\n",
        "        base = os.path.basename(fn)\n",
        "        print('   -', base)\n",
        "\n",
        "# Also list any files matching likely test id patterns\n",
        "for pat in ['*003*.npz','*test_*.npz','*Sample*.npz','*train_*.npz','*val*.npz','*id*.npz']:\n",
        "    hits = sorted(glob.glob(os.path.join('cache_probs', pat)))\n",
        "    print(f'[cache_probs] pattern {pat}: {len(hits)} hits')\n",
        "    for h in hits[:5]:\n",
        "        print('   ', os.path.basename(h))\n",
        "    hits15 = sorted(glob.glob(os.path.join('cache_probs_v15', pat)))\n",
        "    print(f'[cache_probs_v15] pattern {pat}: {len(hits15)} hits')\n",
        "    for h in hits15[:5]:\n",
        "        print('   ', os.path.basename(h))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[cache_probs] count=277\n   valprobs_00410.npz\n   valprobs_00411.npz\n   valprobs_00412.npz\n   valprobs_00413.npz\n   valprobs_00414.npz\n   valprobs_00706.npz\n   valprobs_00707.npz\n   valprobs_00708.npz\n   valprobs_00709.npz\n   valprobs_00710.npz\n[cache_probs] sample patterns:\n   - valprobs_00410.npz\n   - valprobs_00411.npz\n   - valprobs_00412.npz\n   - valprobs_00413.npz\n   - valprobs_00414.npz\n   - valprobs_00415.npz\n   - valprobs_00416.npz\n   - valprobs_00417.npz\n   - valprobs_00418.npz\n   - valprobs_00420.npz\n[cache_probs_v15] count=277\n   valprobs_00410.npz\n   valprobs_00411.npz\n   valprobs_00412.npz\n   valprobs_00413.npz\n   valprobs_00414.npz\n   valprobs_00706.npz\n   valprobs_00707.npz\n   valprobs_00708.npz\n   valprobs_00709.npz\n   valprobs_00710.npz\n[cache_probs_v15] sample patterns:\n   - valprobs_00410.npz\n   - valprobs_00411.npz\n   - valprobs_00412.npz\n   - valprobs_00413.npz\n   - valprobs_00414.npz\n   - valprobs_00415.npz\n   - valprobs_00416.npz\n   - valprobs_00417.npz\n   - valprobs_00418.npz\n   - valprobs_00420.npz\n[oof_probs_v16] count=297\n   oof_00001.npz\n   oof_00003.npz\n   oof_00004.npz\n   oof_00005.npz\n   oof_00006.npz\n   oof_00295.npz\n   oof_00296.npz\n   oof_00297.npz\n   oof_00298.npz\n   oof_00299.npz\n[oof_probs_v16] sample patterns:\n   - oof_00001.npz\n   - oof_00003.npz\n   - oof_00004.npz\n   - oof_00005.npz\n   - oof_00006.npz\n   - oof_00007.npz\n   - oof_00008.npz\n   - oof_00009.npz\n   - oof_00010.npz\n   - oof_00011.npz\n[oof_probs_v15] count=297\n   oof_00001.npz\n   oof_00003.npz\n   oof_00004.npz\n   oof_00005.npz\n   oof_00006.npz\n   oof_00295.npz\n   oof_00296.npz\n   oof_00297.npz\n   oof_00298.npz\n   oof_00299.npz\n[oof_probs_v15] sample patterns:\n   - oof_00001.npz\n   - oof_00003.npz\n   - oof_00004.npz\n   - oof_00005.npz\n   - oof_00006.npz\n   - oof_00007.npz\n   - oof_00008.npz\n   - oof_00009.npz\n   - oof_00010.npz\n   - oof_00011.npz\n[cache_probs] pattern *003*.npz: 0 hits\n[cache_probs_v15] pattern *003*.npz: 0 hits\n[cache_probs] pattern *test_*.npz: 0 hits\n[cache_probs_v15] pattern *test_*.npz: 0 hits\n[cache_probs] pattern *Sample*.npz: 0 hits\n[cache_probs_v15] pattern *Sample*.npz: 0 hits\n[cache_probs] pattern *train_*.npz: 0 hits\n[cache_probs_v15] pattern *train_*.npz: 0 hits\n[cache_probs] pattern *val*.npz: 277 hits\n    valprobs_00410.npz\n    valprobs_00411.npz\n    valprobs_00412.npz\n    valprobs_00413.npz\n    valprobs_00414.npz\n[cache_probs_v15] pattern *val*.npz: 277 hits\n    valprobs_00410.npz\n    valprobs_00411.npz\n    valprobs_00412.npz\n    valprobs_00413.npz\n    valprobs_00414.npz\n[cache_probs] pattern *id*.npz: 0 hits\n[cache_probs_v15] pattern *id*.npz: 0 hits\n"
          ]
        }
      ]
    },
    {
      "id": "bd5f32fb-3763-4dbc-8b38-f72a9ea9eabb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Auto-discovered weighted rank-ensemble over all precomputed submission_*.csv (excluding current submission.csv)\n",
        "import pandas as pd, numpy as np, os, glob\n",
        "\n",
        "cand = sorted([f for f in glob.glob('submission*.csv') if os.path.basename(f) != 'submission.csv'])\n",
        "assert cand, 'No candidate submission_*.csv files found'\n",
        "dfs = []\n",
        "kept = []\n",
        "for f in cand:\n",
        "    try:\n",
        "        df = pd.read_csv(f).sort_values('Id')\n",
        "        if df.shape[0] == 95 and set(df.columns) == {'Id','Sequence'}:\n",
        "            dfs.append(df); kept.append(f)\n",
        "    except Exception:\n",
        "        pass\n",
        "assert dfs, 'No valid candidate CSVs with 95 rows'\n",
        "\n",
        "def weight_for(name: str) -> float:\n",
        "    # Heuristic weights by filename pattern\n",
        "    n = name.lower()\n",
        "    w = 1.0\n",
        "    if 'w8_b' in n: w = 0.70\n",
        "    elif 'w8_' in n: w = 0.80\n",
        "    elif 'w7_c' in n or 'w6_c' in n: w = 0.90\n",
        "    elif 'w7_b' in n or 'w6_b' in n: w = 0.95\n",
        "    elif 'blend_w7_a' in n or 'blend_w6_a' in n: w = 1.00\n",
        "    elif 'blend_a' in n: w = 0.95\n",
        "    elif name == 'submission_A.csv': w = 0.85\n",
        "    elif name == 'submission_B.csv': w = 0.88\n",
        "    elif name == 'submission_C.csv': w = 0.88\n",
        "    else:\n",
        "        w = 0.90\n",
        "    return float(w)\n",
        "\n",
        "weights = np.array([weight_for(os.path.basename(f)) for f in kept], dtype=np.float32)\n",
        "ids = dfs[0]['Id'].tolist()\n",
        "K = 20\n",
        "seqs = []\n",
        "for i in range(len(ids)):\n",
        "    rank_sum = np.zeros(K+1, dtype=np.float32)  # classes 1..20\n",
        "    for j, df in enumerate(dfs):\n",
        "        arr = [int(x) for x in str(df.iloc[i].Sequence).split()]\n",
        "        for pos, cls in enumerate(arr):\n",
        "            if 1 <= cls <= 20:\n",
        "                rank_sum[cls] += weights[j] * (pos + 1)\n",
        "    order = np.argsort(rank_sum[1:]) + 1  # best (lowest) rank first\n",
        "    seqs.append(' '.join(map(str, order)))\n",
        "out = pd.DataFrame({'Id': ids, 'Sequence': seqs})\n",
        "out.to_csv('submission.csv', index=False)\n",
        "print('[RankEnsemble-Auto] Wrote submission.csv', out.shape, 'from', kept, 'weights', weights.tolist())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RankEnsemble-Extended] Wrote submission.csv (95, 2) from ['submission_blend_w7_A.csv', 'submission_blend_w6_A.csv', 'submission_blend_w7_B.csv', 'submission_blend_A.csv', 'submission_A.csv', 'submission_B.csv', 'submission_C.csv', 'submission_blend_w8_A.csv', 'submission_blend_w8_C.csv'] weights [1.0, 0.949999988079071, 0.8999999761581421, 0.75, 0.6000000238418579, 0.6499999761581421, 0.6499999761581421, 0.550000011920929, 0.550000011920929]\n"
          ]
        }
      ]
    },
    {
      "id": "d18429da-ef82-48e9-8379-6db171964b10",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build test probs from cached features + saved XGB models, then calibrated exact-20 + Hungarian decode\n",
        "import os, numpy as np, pandas as pd, xgboost as xgb\n",
        "\n",
        "test_ids = pd.read_csv('test.csv')['Id'].tolist()\n",
        "map16, map15 = {}, {}\n",
        "\n",
        "# Try to load both boosters and infer their expected feature counts to map to cache_v16 (193) vs cache_v15 (120)\n",
        "bst_a = bst_b = None\n",
        "if os.path.exists('xgb_train.model'):\n",
        "    try:\n",
        "        bst_a = xgb.Booster(); bst_a.load_model('xgb_train.model')\n",
        "    except Exception as e:\n",
        "        print('[Warn] Failed load xgb_train.model:', e); bst_a=None\n",
        "if os.path.exists('xgb_train_v15.model'):\n",
        "    try:\n",
        "        bst_b = xgb.Booster(); bst_b.load_model('xgb_train_v15.model')\n",
        "    except Exception as e:\n",
        "        print('[Warn] Failed load xgb_train_v15.model:', e); bst_b=None\n",
        "\n",
        "def booster_nfeat(bst):\n",
        "    try:\n",
        "        return int(bst.num_features())\n",
        "    except Exception:\n",
        "        # Fallback: try predict on dummy\n",
        "        return None\n",
        "\n",
        "pairs = []  # list of (bst, cache_dir, tag)\n",
        "for bst, tag in ((bst_a,'A'), (bst_b,'B')):\n",
        "    if bst is None: continue\n",
        "    nf = booster_nfeat(bst)\n",
        "    if nf is None:\n",
        "        # Guess by trying v16 first\n",
        "        nf = -1\n",
        "    if nf == 193:\n",
        "        pairs.append((bst, 'cache_v16', f'{tag}:v16(193)'))\n",
        "    elif nf == 120:\n",
        "        pairs.append((bst, 'cache_v15', f'{tag}:v15(120)'))\n",
        "    else:\n",
        "        # Try to detect by checking one file's feature count\n",
        "        probe_id = test_ids[0]\n",
        "        for cdir, expect, label in (('cache_v16', 193, 'v16'), ('cache_v15', 120, 'v15')):\n",
        "            fn = os.path.join(cdir, f'test_{probe_id:05d}.npz')\n",
        "            if os.path.exists(fn):\n",
        "                X = np.load(fn, allow_pickle=False)['X']\n",
        "                if X.shape[1] == expect:\n",
        "                    pairs.append((bst, cdir, f'{tag}:{label}({expect})'))\n",
        "                    break\n",
        "\n",
        "print('[Models] Using pairs:', [p[2] for p in pairs])\n",
        "\n",
        "def load_test_feats(cache_dir, sid):\n",
        "    fn = os.path.join(cache_dir, f'test_{sid:05d}.npz')\n",
        "    if not os.path.exists(fn): return None\n",
        "    z = np.load(fn, allow_pickle=False); return z['X']\n",
        "\n",
        "# Predict for each booster/cache pair\n",
        "for bst, cdir, tag in pairs:\n",
        "    is_v16 = ('cache_v16' in cdir)\n",
        "    for i, sid in enumerate(test_ids, 1):\n",
        "        X = load_test_feats(cdir, sid)\n",
        "        if X is None: continue\n",
        "        dm = xgb.DMatrix(X)\n",
        "        try:\n",
        "            P = bst.predict(dm)\n",
        "        except Exception as e:\n",
        "            print(f'[ErrPredict][{tag}] sid={sid}:', e); continue\n",
        "        if is_v16:\n",
        "            map16[sid] = P.astype(np.float32)\n",
        "        else:\n",
        "            map15[sid] = P.astype(np.float32)\n",
        "        if i % 20 == 0:\n",
        "            print(f'[Predict][{tag}] {i}/{len(test_ids)}')\n",
        "\n",
        "# Use helper fns from cell 7: load_oof_frames, fit_per_class_temperature, to_calibrated_logits, decode_video, CFG\n",
        "use_v15 = (len(map15) > 0)\n",
        "P_oof16, y_oof16 = load_oof_frames(['oof_probs_v16'])\n",
        "Tcls16 = fit_per_class_temperature(P_oof16, y_oof16, CFG['temp_grid'])\n",
        "if use_v15:\n",
        "    P_oof15, y_oof15 = load_oof_frames(['oof_probs_v15'])\n",
        "    Tcls15 = fit_per_class_temperature(P_oof15, y_oof15, CFG['temp_grid'])\n",
        "else:\n",
        "    Tcls15 = Tcls16\n",
        "\n",
        "rows = []\n",
        "for sid in test_ids:\n",
        "    if sid not in map16 and sid not in map15:\n",
        "        continue\n",
        "    Z = None\n",
        "    if sid in map16:\n",
        "        Z16 = to_calibrated_logits(map16[sid], Tcls16, CFG['bg_bias'], CFG['smooth_win'])\n",
        "        Z = Z16 if Z is None else Z\n",
        "    if use_v15 and (sid in map15):\n",
        "        Z15 = to_calibrated_logits(map15[sid], Tcls15, CFG['bg_bias'], CFG['smooth_win'])\n",
        "        if Z is None:\n",
        "            Z = Z15\n",
        "        else:\n",
        "            Z = (CFG['w16']*Z + CFG['w15']*Z15).astype(np.float32)\n",
        "    if Z is None: continue\n",
        "    seq = decode_video(Z, CFG['alpha'])\n",
        "    rows.append((sid, ' '.join(map(str, seq))))\n",
        "\n",
        "sub = pd.DataFrame(rows, columns=['Id','Sequence']).sort_values('Id')\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('[CalibratedDecode] Wrote submission.csv', sub.shape, 'use_v15=', use_v15)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Models] Using pairs: ['A:v15(120)', 'B:v15(120)']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Predict][A:v15(120)] 20/95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Predict][A:v15(120)] 40/95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Predict][A:v15(120)] 60/95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Predict][A:v15(120)] 80/95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Predict][B:v15(120)] 20/95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Predict][B:v15(120)] 40/95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Predict][B:v15(120)] 60/95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Predict][B:v15(120)] 80/95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CalibratedDecode] Wrote submission.csv (95, 2) use_v15= True\n"
          ]
        }
      ]
    },
    {
      "id": "1944215c-83b7-4826-b0b4-a179556cd9e1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train XGBoost on v16 features (193 cols) and write per-video test probs to test_probs_v16\n",
        "import os, time, glob, numpy as np, pandas as pd, xgboost as xgb, random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "random.seed(42); np.random.seed(42)\n",
        "train_meta = pd.read_csv('training.csv')\n",
        "train_ids = train_meta['Id'].astype(int).tolist()\n",
        "test_ids = pd.read_csv('test.csv')['Id'].astype(int).tolist()\n",
        "\n",
        "def load_train_video(vid):\n",
        "    fn = os.path.join('cache_v16', f'train_{vid:05d}.npz')\n",
        "    if not os.path.exists(fn):\n",
        "        return None, None\n",
        "    z = np.load(fn, allow_pickle=False)\n",
        "    X = z['X'].astype(np.float32)\n",
        "    y = z['y'].astype(np.int32) if 'y' in z else (z['labels'].astype(np.int32) if 'labels' in z else None)\n",
        "    return X, y\n",
        "\n",
        "X_list_tr, y_list_tr, vids_avail = [], [], []\n",
        "t0 = time.time()\n",
        "for i, vid in enumerate(train_ids, 1):\n",
        "    Xv, yv = load_train_video(vid)\n",
        "    if Xv is None or yv is None:\n",
        "        continue\n",
        "    if Xv.shape[0] != yv.shape[0]:\n",
        "        continue\n",
        "    X_list_tr.append(Xv); y_list_tr.append(yv); vids_avail.append(vid)\n",
        "    if i % 20 == 0:\n",
        "        print(f'[LoadTrain] {i}/{len(train_ids)} vids processed; kept={len(vids_avail)}', flush=True)\n",
        "print(f'[LoadTrain] Done. vids_kept={len(vids_avail)} elapsed={time.time()-t0:.1f}s')\n",
        "\n",
        "assert X_list_tr and y_list_tr, 'No training videos loaded from cache_v16'\n",
        "X_all = np.concatenate(X_list_tr, axis=0)\n",
        "y_all = np.concatenate(y_list_tr, axis=0)\n",
        "n_feat = X_all.shape[1]\n",
        "print('[TrainData] X_all', X_all.shape, 'y_all', y_all.shape, 'n_feat', n_feat)\n",
        "assert n_feat == 193 or n_feat > 150, 'Unexpected feature count; expected v16 ~193 features'\n",
        "\n",
        "# 10% video-level holdout\n",
        "vids_arr = np.array(vids_avail, dtype=np.int32)\n",
        "vids_tr, vids_va = train_test_split(vids_arr, test_size=0.10, random_state=42, shuffle=True)\n",
        "vid_to_split = {int(v): 'train' for v in vids_tr}\n",
        "for v in vids_va: vid_to_split[int(v)] = 'valid'\n",
        "\n",
        "# Build frame-level indices for split\n",
        "idx_tr, idx_va = [], []\n",
        "offset = 0\n",
        "for vid, Xv, yv in zip(vids_avail, X_list_tr, y_list_tr):\n",
        "    n = Xv.shape[0]\n",
        "    if vid_to_split.get(int(vid), 'train') == 'train':\n",
        "        idx_tr.extend(range(offset, offset+n))\n",
        "    else:\n",
        "        idx_va.extend(range(offset, offset+n))\n",
        "    offset += n\n",
        "idx_tr = np.array(idx_tr, dtype=np.int64); idx_va = np.array(idx_va, dtype=np.int64)\n",
        "print('[Split] frames train:', idx_tr.size, 'valid:', idx_va.size, 'videos valid:', len(vids_va))\n",
        "\n",
        "dtrain = xgb.DMatrix(X_all[idx_tr], label=y_all[idx_tr])\n",
        "dvalid = xgb.DMatrix(X_all[idx_va], label=y_all[idx_va])\n",
        "\n",
        "params = {\n",
        "    'objective': 'multi:softprob',\n",
        "    'num_class': 21,\n",
        "    'tree_method': 'gpu_hist',\n",
        "    'predictor': 'gpu_predictor',\n",
        "    'max_bin': 256,\n",
        "    'max_depth': 8,\n",
        "    'eta': 0.05,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'min_child_weight': 3,\n",
        "    'gamma': 0.0,\n",
        "    'reg_lambda': 1.0,\n",
        "    'reg_alpha': 0.0,\n",
        "    'eval_metric': 'mlogloss'\n",
        "}\n",
        "print('[XGB] Params:', params)\n",
        "evals = [(dtrain, 'train'), (dvalid, 'valid')]\n",
        "t1 = time.time()\n",
        "bst = xgb.train(\n",
        "    params,\n",
        "    dtrain,\n",
        "    num_boost_round=1500,\n",
        "    evals=evals,\n",
        "    early_stopping_rounds=80,\n",
        "    verbose_eval=50\n",
        ")\n",
        "print('[XGB] Training done in {:.1f}s; best_iter={} valid_mlogloss={:.5f}'.format(time.time()-t1, bst.best_iteration, bst.best_score if hasattr(bst, 'best_score') else float('nan')))\n",
        "bst.save_model('xgb_train_v16.model')\n",
        "print('[XGB] Saved model -> xgb_train_v16.model')\n",
        "\n",
        "# Predict test videos and save as test_probs_v16/test_{Id:05d}.npz with key probs\n",
        "os.makedirs('test_probs_v16', exist_ok=True)\n",
        "def load_test_feats(sid):\n",
        "    fn = os.path.join('cache_v16', f'test_{sid:05d}.npz')\n",
        "    if not os.path.exists(fn):\n",
        "        return None\n",
        "    z = np.load(fn, allow_pickle=False);\n",
        "    return z['X'].astype(np.float32)\n",
        "\n",
        "t2 = time.time()\n",
        "cnt = 0\n",
        "for i, sid in enumerate(test_ids, 1):\n",
        "    X = load_test_feats(sid)\n",
        "    if X is None:\n",
        "        continue\n",
        "    dm = xgb.DMatrix(X)\n",
        "    P = bst.predict(dm, iteration_range=(0, bst.best_iteration+1)) if hasattr(bst, 'best_iteration') else bst.predict(dm)\n",
        "    P = P.astype(np.float32)\n",
        "    # Sanity\n",
        "    if not np.all(np.isfinite(P)):\n",
        "        P = np.nan_to_num(P, nan=1.0/21, posinf=1.0/21, neginf=1.0/21).astype(np.float32)\n",
        "    # Normalize rows to sum 1\n",
        "    P /= np.clip(P.sum(1, keepdims=True), 1e-8, None)\n",
        "    out_fn = os.path.join('test_probs_v16', f'test_{sid:05d}.npz')\n",
        "    np.savez_compressed(out_fn, probs=P)\n",
        "    cnt += 1\n",
        "    if i % 10 == 0:\n",
        "        print(f'[Predict][v16] {i}/{len(test_ids)} wrote {cnt} files', flush=True)\n",
        "print(f'[Predict][v16] Done. wrote={cnt} elapsed={time.time()-t2:.1f}s')\n",
        "\n",
        "print('[Next] Run decoder sweep with v16+v15 blend, pointing CFG.test_dirs to [\"test_probs_v16\", \"<v15_dir_if_ready>\"] and try S1/S4/S5 settings.')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LoadTrain] 20/297 vids processed; kept=20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LoadTrain] 40/297 vids processed; kept=40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LoadTrain] 60/297 vids processed; kept=60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LoadTrain] 80/297 vids processed; kept=80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LoadTrain] 100/297 vids processed; kept=100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LoadTrain] 120/297 vids processed; kept=120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LoadTrain] 140/297 vids processed; kept=140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LoadTrain] 160/297 vids processed; kept=160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LoadTrain] 180/297 vids processed; kept=180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LoadTrain] 200/297 vids processed; kept=200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LoadTrain] 220/297 vids processed; kept=220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LoadTrain] 240/297 vids processed; kept=240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LoadTrain] 260/297 vids processed; kept=260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LoadTrain] 280/297 vids processed; kept=280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LoadTrain] Done. vids_kept=297 elapsed=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrainData] X_all (187296, 193) y_all (187296,) n_feat 193\n[Split] frames train: 167968 valid: 19328 videos valid: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[XGB] Params: {'objective': 'multi:softprob', 'num_class': 21, 'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor', 'max_bin': 256, 'max_depth': 8, 'eta': 0.05, 'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 3, 'gamma': 0.0, 'reg_lambda': 1.0, 'reg_alpha': 0.0, 'eval_metric': 'mlogloss'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [23:02:03] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [23:02:03] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"predictor\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-mlogloss:2.85951\tvalid-mlogloss:2.87358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\ttrain-mlogloss:1.25022\tvalid-mlogloss:1.57548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain-mlogloss:0.86216\tvalid-mlogloss:1.39493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\ttrain-mlogloss:0.64347\tvalid-mlogloss:1.32724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mlogloss:0.48896\tvalid-mlogloss:1.29030\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\ttrain-mlogloss:0.38071\tvalid-mlogloss:1.26785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain-mlogloss:0.30071\tvalid-mlogloss:1.25274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[350]\ttrain-mlogloss:0.24270\tvalid-mlogloss:1.24435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mlogloss:0.20023\tvalid-mlogloss:1.23738\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\ttrain-mlogloss:0.16897\tvalid-mlogloss:1.23703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\ttrain-mlogloss:0.14541\tvalid-mlogloss:1.23578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[550]\ttrain-mlogloss:0.12746\tvalid-mlogloss:1.23745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[578]\ttrain-mlogloss:0.11962\tvalid-mlogloss:1.23840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[XGB] Training done in 136.5s; best_iter=499 valid_mlogloss=1.23563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [23:04:20] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [23:04:20] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[XGB] Saved model -> xgb_train_v16.model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Predict][v16] 10/95 wrote 10 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Predict][v16] 20/95 wrote 20 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Predict][v16] 30/95 wrote 30 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Predict][v16] 40/95 wrote 40 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Predict][v16] 50/95 wrote 50 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Predict][v16] 60/95 wrote 60 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Predict][v16] 70/95 wrote 70 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Predict][v16] 80/95 wrote 80 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Predict][v16] 90/95 wrote 90 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Predict][v16] Done. wrote=95 elapsed=14.7s\n[Next] Run decoder sweep with v16+v15 blend, pointing CFG.test_dirs to [\"test_probs_v16\", \"<v15_dir_if_ready>\"] and try S1/S4/S5 settings.\n"
          ]
        }
      ]
    },
    {
      "id": "8ff290b7-60f4-4678-a825-87b5ddeba23a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Decode sweep S1 (v16-only): alpha=0.85, bg_bias=0.20, smooth_win=1, cost=mean\n",
        "import os, glob, numpy as np, pandas as pd\n",
        "\n",
        "# Reuse helpers from cell 7: to_calibrated_logits, decode_video, fit_per_class_temperature, load_oof_frames\n",
        "\n",
        "def _find_npz_simple(d, vid):\n",
        "    cands = [\n",
        "        os.path.join(d, f'{vid}.npz'),\n",
        "        os.path.join(d, f'{vid:05d}.npz'),\n",
        "        os.path.join(d, f'test_{vid:05d}.npz'),\n",
        "    ]\n",
        "    for fn in cands:\n",
        "        if os.path.exists(fn):\n",
        "            return fn\n",
        "    hits = glob.glob(os.path.join(d, f'*{vid:05d}*.npz'))\n",
        "    return hits[0] if hits else None\n",
        "\n",
        "def load_probs_dir_simple(d, ids):\n",
        "    out = {}\n",
        "    for vid in ids:\n",
        "        fn = _find_npz_simple(d, vid)\n",
        "        if not fn:\n",
        "            continue\n",
        "        z = np.load(fn, allow_pickle=True)\n",
        "        if 'probs' in z:\n",
        "            P = z['probs'].astype(np.float32)\n",
        "        elif 'P' in z:\n",
        "            P = z['P'].astype(np.float32)\n",
        "        else:\n",
        "            continue\n",
        "        out[vid] = P\n",
        "    return out\n",
        "\n",
        "# S1 config (v16-only) per expert plan\n",
        "S1 = dict(\n",
        "    test_dir='test_probs_v16',\n",
        "    oof_dirs=['oof_probs_v16'],\n",
        "    alpha=0.85,\n",
        "    bg_bias=0.20,\n",
        "    smooth_win=1,\n",
        ")\n",
        "\n",
        "test_ids = pd.read_csv('test.csv')['Id'].tolist()\n",
        "P_oof, y_oof = load_oof_frames(S1['oof_dirs'])\n",
        "Tcls = fit_per_class_temperature(P_oof, y_oof, np.linspace(0.7, 1.6, 12).astype(np.float32))\n",
        "m = load_probs_dir_simple(S1['test_dir'], test_ids)\n",
        "rows = []\n",
        "for vid in sorted(m.keys()):\n",
        "    P = m[vid]\n",
        "    Z = to_calibrated_logits(P, Tcls, S1['bg_bias'], S1['smooth_win'])\n",
        "    seq = decode_video(Z, S1['alpha'])\n",
        "    rows.append((vid, ' '.join(map(str, seq))))\n",
        "sub = pd.DataFrame(rows, columns=['Id','Sequence']).sort_values('Id')\n",
        "sub.to_csv('submission_v16_S1.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('[S1] Wrote submission_v16_S1.csv and submission.csv', sub.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[S1] Wrote submission_v16_S1.csv and submission.csv (95, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "a07488f2-8657-4178-b96f-a9a37d41f909",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Decoder sweep S4 and S5 (v16-only): median and trimmed mean costs\n",
        "import os, glob, numpy as np, pandas as pd\n",
        "\n",
        "# Assumes helper funcs from cell 7 exist: load_oof_frames, fit_per_class_temperature, to_calibrated_logits\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "def load_probs_dir_simple(d, ids):\n",
        "    out = {}\n",
        "    for vid in ids:\n",
        "        for pat in (f'{vid}.npz', f'{vid:05d}.npz', f'test_{vid:05d}.npz'):\n",
        "            fn = os.path.join(d, pat)\n",
        "            if os.path.exists(fn):\n",
        "                z = np.load(fn, allow_pickle=True)\n",
        "                if 'probs' in z: out[vid] = z['probs'].astype(np.float32)\n",
        "                elif 'P' in z: out[vid] = z['P'].astype(np.float32)\n",
        "                break\n",
        "    return out\n",
        "\n",
        "def segment_exact20_simple(margin, K, min_len):\n",
        "    T = int(len(margin))\n",
        "    min_len = int(max(1, min_len))\n",
        "    while K*min_len > T and min_len > 1:\n",
        "        min_len -= 1\n",
        "    if K*min_len > T:\n",
        "        K = min(K, T); min_len = 1\n",
        "    pref = np.concatenate([[0.0], np.cumsum(margin, 0).astype(np.float32)])\n",
        "    dp = -1e18*np.ones((K+1, T+1), np.float32)\n",
        "    bt = -np.ones((K+1, T+1), np.int32)\n",
        "    dp[0, 0] = 0.0\n",
        "    for k in range(1, K+1):\n",
        "        start_min = (k-1)*min_len\n",
        "        for t in range(k*min_len, T+1):\n",
        "            s_lo = max(start_min, t - (T - (K-k)*min_len))\n",
        "            bestv, bests = -1e18, -1\n",
        "            for s in range(s_lo, t-min_len+1):\n",
        "                v = dp[k-1, s] + (pref[t] - pref[s])\n",
        "                if v > bestv:\n",
        "                    bestv, bests = v, s\n",
        "            dp[k, t] = bestv; bt[k, t] = bests\n",
        "    bounds = []\n",
        "    k, t = K, T\n",
        "    while k > 0:\n",
        "        s = int(bt[k, t]); bounds.append((s, t)); t = s; k -= 1\n",
        "    bounds.reverse()\n",
        "    return bounds\n",
        "\n",
        "def decode_with_cost(Z, alpha, cost_mode='median'):\n",
        "    T = Z.shape[0]\n",
        "    min_len = max(2, int(alpha * T / 20.0))\n",
        "    margin = (Z[:, 1:21].max(1) - Z[:, 0]).astype(np.float32)\n",
        "    segs = segment_exact20_simple(margin, 20, min_len)\n",
        "    C = np.zeros((len(segs), 20), np.float32)\n",
        "    for j, (s, e) in enumerate(segs):\n",
        "        seg = Z[s:e, 1:21]\n",
        "        if cost_mode == 'median':\n",
        "            agg = np.median(seg, axis=0)\n",
        "        elif cost_mode == 'trimmed_mean_20':\n",
        "            L = seg.shape[0]; lo = int(0.1*L); hi = max(lo+1, int(0.9*L))\n",
        "            agg = np.sort(seg, axis=0)[lo:hi].mean(0)\n",
        "        else:  # mean\n",
        "            agg = seg.mean(0)\n",
        "        C[j, :] = -agg\n",
        "    r, c = linear_sum_assignment(C)\n",
        "    order = [int(c[j])+1 for j in np.argsort(r)]\n",
        "    return order\n",
        "\n",
        "def run_decode_variant(name, alpha, bg_bias, smooth_win, cost_mode):\n",
        "    test_ids = pd.read_csv('test.csv')['Id'].tolist()\n",
        "    P_oof, y_oof = load_oof_frames(['oof_probs_v16'])\n",
        "    Tcls = fit_per_class_temperature(P_oof, y_oof, np.linspace(0.7, 1.6, 12).astype(np.float32))\n",
        "    mp = load_probs_dir_simple('test_probs_v16', test_ids)\n",
        "    rows = []\n",
        "    for vid in sorted(mp.keys()):\n",
        "        P = mp[vid]\n",
        "        Z = to_calibrated_logits(P, Tcls, bg_bias, smooth_win)\n",
        "        seq = decode_with_cost(Z, alpha, cost_mode=cost_mode)\n",
        "        rows.append((vid, ' '.join(map(str, seq))))\n",
        "    sub = pd.DataFrame(rows, columns=['Id','Sequence']).sort_values('Id')\n",
        "    out_fn = f'submission_v16_{name}.csv'\n",
        "    sub.to_csv(out_fn, index=False)\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print(f'[{name}] Wrote {out_fn} and submission.csv', sub.shape)\n",
        "\n",
        "# S4: alpha=0.85, bg_bias=0.25, smooth=3, cost=median\n",
        "run_decode_variant('S4', alpha=0.85, bg_bias=0.25, smooth_win=3, cost_mode='median')\n",
        "# S5: alpha=0.85, bg_bias=0.20, smooth=3, cost=trimmed_mean_20\n",
        "run_decode_variant('S5', alpha=0.85, bg_bias=0.20, smooth_win=3, cost_mode='trimmed_mean_20')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[S4] Wrote submission_v16_S4.csv and submission.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[S5] Wrote submission_v16_S5.csv and submission.csv (95, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "b2a1ff2d-8ea4-47b5-8154-35187e00759c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# v16+v15 blended calibrated decode: generate S1/S4/S5 with w16 in {0.65,0.70,0.75}\n",
        "import os, glob, time, numpy as np, pandas as pd, xgboost as xgb\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "test_ids = pd.read_csv('test.csv')['Id'].astype(int).tolist()\n",
        "\n",
        "# Load v16 per-frame probs from disk\n",
        "def load_probs_dir_simple(d, ids):\n",
        "    out = {}\n",
        "    for vid in ids:\n",
        "        for pat in (f'{vid}.npz', f'{vid:05d}.npz', f'test_{vid:05d}.npz'):\n",
        "            fn = os.path.join(d, pat)\n",
        "            if os.path.exists(fn):\n",
        "                z = np.load(fn, allow_pickle=True)\n",
        "                if 'probs' in z: out[vid] = z['probs'].astype(np.float32)\n",
        "                elif 'P' in z: out[vid] = z['P'].astype(np.float32)\n",
        "                break\n",
        "    return out\n",
        "\n",
        "map16 = load_probs_dir_simple('test_probs_v16', test_ids)\n",
        "print('[Blend] Loaded v16 test probs:', len(map16))\n",
        "\n",
        "# Predict v15 per-frame probs on-the-fly using available v15 boosters\n",
        "bstA = bstB = None\n",
        "try:\n",
        "    if os.path.exists('xgb_train.model'):\n",
        "        tmp = xgb.Booster(); tmp.load_model('xgb_train.model')\n",
        "        if int(tmp.num_features()) == 120: bstA = tmp\n",
        "except Exception as e:\n",
        "    print('[v15] skip A:', e)\n",
        "try:\n",
        "    if os.path.exists('xgb_train_v15.model'):\n",
        "        tmp = xgb.Booster(); tmp.load_model('xgb_train_v15.model')\n",
        "        if int(tmp.num_features()) == 120: bstB = tmp\n",
        "except Exception as e:\n",
        "    print('[v15] skip B:', e)\n",
        "assert (bstA is not None) or (bstB is not None), 'No v15 booster available'\n",
        "\n",
        "def load_test_v15_X(sid):\n",
        "    fn = os.path.join('cache_v15', f'test_{sid:05d}.npz')\n",
        "    if not os.path.exists(fn): return None\n",
        "    z = np.load(fn, allow_pickle=False);\n",
        "    X = z['X'].astype(np.float32)\n",
        "    return X\n",
        "\n",
        "map15 = {}\n",
        "t0 = time.time()\n",
        "for i, sid in enumerate(test_ids, 1):\n",
        "    X = load_test_v15_X(sid)\n",
        "    if X is None: continue\n",
        "    dm = xgb.DMatrix(X)\n",
        "    Ps = []\n",
        "    if bstA is not None:\n",
        "        Ps.append(bstA.predict(dm))\n",
        "    if bstB is not None:\n",
        "        Ps.append(bstB.predict(dm))\n",
        "    if Ps:\n",
        "        P = np.mean(Ps, axis=0).astype(np.float32)\n",
        "        P /= np.clip(P.sum(1, keepdims=True), 1e-8, None)\n",
        "        map15[sid] = P\n",
        "    if i % 20 == 0:\n",
        "        print(f'[v15 Predict] {i}/{len(test_ids)}')\n",
        "print('[Blend] Built v15 test probs:', len(map15), 'elapsed', f'{time.time()-t0:.1f}s')\n",
        "\n",
        "# Helpers from earlier cells (fallbacks if not in scope)\n",
        "def load_oof_frames(oof_dirs):\n",
        "    X, Y = [], []\n",
        "    for d in oof_dirs:\n",
        "        for fn in glob.glob(os.path.join(d, '*.npz')):\n",
        "            z = np.load(fn, allow_pickle=True)\n",
        "            if 'probs' in z:\n",
        "                P = z['probs'].astype(np.float32)\n",
        "            elif 'P' in z:\n",
        "                P = z['P'].astype(np.float32)\n",
        "            else:\n",
        "                continue\n",
        "            if 'y' in z: y = z['y'].astype(np.int32)\n",
        "            elif 'labels' in z: y = z['labels'].astype(np.int32)\n",
        "            else: continue\n",
        "            if P.ndim == 2 and len(y) == P.shape[0]:\n",
        "                X.append(P); Y.append(y)\n",
        "    if not X: return None, None\n",
        "    return np.concatenate(X, 0), np.concatenate(Y, 0)\n",
        "\n",
        "from scipy.special import logit\n",
        "from scipy.signal import convolve\n",
        "def fit_per_class_temperature(P_oof, y_oof, temp_grid):\n",
        "    if P_oof is None or y_oof is None:\n",
        "        return np.ones(21, dtype=np.float32)\n",
        "    C = P_oof.shape[1]; Tcls = np.ones(C, dtype=np.float32)\n",
        "    y = y_oof\n",
        "    for c in range(C):\n",
        "        pc = np.clip(P_oof[:, c], 1e-6, 1-1e-6); z = logit(pc)\n",
        "        yc = (y == c).astype(np.float32)\n",
        "        best, bestT = 1e18, 1.0\n",
        "        for T in temp_grid:\n",
        "            p = 1.0/(1.0 + np.exp(-z/float(T)))\n",
        "            nll = -(yc*np.log(np.clip(p,1e-6,1))).mean() - ((1-yc)*np.log(np.clip(1-p,1e-6,1))).mean()\n",
        "            if nll < best: best, bestT = float(nll), float(T)\n",
        "        Tcls[c] = bestT\n",
        "    return Tcls.astype(np.float32)\n",
        "\n",
        "def to_calibrated_logits(P, Tcls, bg_bias, smooth_win):\n",
        "    P = np.clip(P, 1e-6, 1-1e-6).astype(np.float32)\n",
        "    Z = logit(P).astype(np.float32)\n",
        "    Z = Z / Tcls.reshape(1, -1)\n",
        "    Z[:, 0] += float(bg_bias)\n",
        "    if smooth_win and smooth_win > 1:\n",
        "        k = np.ones(int(smooth_win), np.float32) / float(smooth_win)\n",
        "        Z = np.stack([convolve(Z[:, i], k, mode='same') for i in range(Z.shape[1])], 1).astype(np.float32)\n",
        "    return Z\n",
        "\n",
        "def segment_exact20_simple(margin, K, min_len):\n",
        "    T = int(len(margin)); min_len = int(max(1, min_len))\n",
        "    while K*min_len > T and min_len > 1: min_len -= 1\n",
        "    if K*min_len > T: K = min(K, T); min_len = 1\n",
        "    pref = np.concatenate([[0.0], np.cumsum(margin, 0).astype(np.float32)])\n",
        "    dp = -1e18*np.ones((K+1, T+1), np.float32); bt = -np.ones((K+1, T+1), np.int32)\n",
        "    dp[0, 0] = 0.0\n",
        "    for k in range(1, K+1):\n",
        "        start_min = (k-1)*min_len\n",
        "        for t in range(k*min_len, T+1):\n",
        "            s_lo = max(start_min, t - (T - (K-k)*min_len))\n",
        "            bestv, bests = -1e18, -1\n",
        "            for s in range(s_lo, t-min_len+1):\n",
        "                v = dp[k-1, s] + (pref[t] - pref[s])\n",
        "                if v > bestv: bestv, bests = v, s\n",
        "            dp[k, t] = bestv; bt[k, t] = bests\n",
        "    bounds = []; k, t = K, T\n",
        "    while k > 0:\n",
        "        s = int(bt[k, t]); bounds.append((s, t)); t = s; k -= 1\n",
        "    bounds.reverse(); return bounds\n",
        "\n",
        "def decode_with_cost(Z, alpha, cost_mode='mean'):\n",
        "    T = Z.shape[0]\n",
        "    min_len = max(2, int(alpha * T / 20.0))\n",
        "    margin = (Z[:, 1:21].max(1) - Z[:, 0]).astype(np.float32)\n",
        "    segs = segment_exact20_simple(margin, 20, min_len)\n",
        "    C = np.zeros((len(segs), 20), np.float32)\n",
        "    for j, (s, e) in enumerate(segs):\n",
        "        seg = Z[s:e, 1:21]\n",
        "        if cost_mode == 'median':\n",
        "            agg = np.median(seg, axis=0)\n",
        "        elif cost_mode == 'trimmed_mean_20':\n",
        "            L = seg.shape[0]; lo = int(0.1*L); hi = max(lo+1, int(0.9*L))\n",
        "            agg = np.sort(seg, axis=0)[lo:hi].mean(0)\n",
        "        else:\n",
        "            agg = seg.mean(0)\n",
        "        C[j, :] = -agg\n",
        "    r, c = linear_sum_assignment(C)\n",
        "    order = [int(c[j])+1 for j in np.argsort(r)]\n",
        "    return order\n",
        "\n",
        "# Calibrate per-class temperatures on OOF (separately for v16 and v15)\n",
        "temp_grid = np.linspace(0.7, 1.6, 12).astype(np.float32)\n",
        "P_oof16, y_oof16 = load_oof_frames(['oof_probs_v16'])\n",
        "P_oof15, y_oof15 = load_oof_frames(['oof_probs_v15'])\n",
        "Tcls16 = fit_per_class_temperature(P_oof16, y_oof16, temp_grid)\n",
        "Tcls15 = fit_per_class_temperature(P_oof15, y_oof15, temp_grid)\n",
        "\n",
        "def run_blend_variant(name, alpha, bg_bias, smooth_win, cost_mode, w16):\n",
        "    rows = []\n",
        "    for vid in sorted(test_ids):\n",
        "        P16 = map16.get(vid)\n",
        "        P15 = map15.get(vid)\n",
        "        if P16 is None and P15 is None: continue\n",
        "        Z_mix = None\n",
        "        if P16 is not None:\n",
        "            Z16 = to_calibrated_logits(P16, Tcls16, bg_bias, smooth_win)\n",
        "            Z_mix = Z16 if Z_mix is None else Z_mix\n",
        "        if P15 is not None:\n",
        "            Z15 = to_calibrated_logits(P15, Tcls15, bg_bias, smooth_win)\n",
        "            if Z_mix is None:\n",
        "                Z_mix = Z15\n",
        "            else:\n",
        "                w15 = 1.0 - w16\n",
        "                Z_mix = (w16*Z_mix + w15*Z15).astype(np.float32)\n",
        "        seq = decode_with_cost(Z_mix, alpha, cost_mode=cost_mode)\n",
        "        rows.append((vid, ' '.join(map(str, seq))))\n",
        "    sub = pd.DataFrame(rows, columns=['Id','Sequence']).sort_values('Id')\n",
        "    out_fn = f'submission_v16v15_{name}_w{int(round(w16*100)):02d}.csv'\n",
        "    sub.to_csv(out_fn, index=False)\n",
        "    print(f'[{name}] w16={w16:.2f} -> {out_fn}', sub.shape)\n",
        "    return out_fn\n",
        "\n",
        "# Run S1, S4, S5 for w16 in {0.65, 0.70, 0.75}; set submission.csv to S4 w16=0.70 as primary\n",
        "out_files = []\n",
        "for w in (0.65, 0.70, 0.75):\n",
        "    out_files.append(run_blend_variant('S1_mean', alpha=0.85, bg_bias=0.20, smooth_win=1, cost_mode='mean', w16=w))\n",
        "for w in (0.65, 0.70, 0.75):\n",
        "    out_files.append(run_blend_variant('S4_median', alpha=0.85, bg_bias=0.25, smooth_win=3, cost_mode='median', w16=w))\n",
        "for w in (0.65, 0.70, 0.75):\n",
        "    out_files.append(run_blend_variant('S5_trimmed', alpha=0.85, bg_bias=0.20, smooth_win=3, cost_mode='trimmed_mean_20', w16=w))\n",
        "\n",
        "# Set primary submission.csv to S4_median w16=0.70\n",
        "primary = 'submission_v16v15_S4_median_w70.csv'\n",
        "if os.path.exists(primary):\n",
        "    pd.read_csv(primary).to_csv('submission.csv', index=False)\n",
        "    print('[Primary] Wrote submission.csv from', primary)\n",
        "else:\n",
        "    print('[Primary] Missing preferred file; not overwriting submission.csv')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Blend] Loaded v16 test probs: 95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v15 Predict] 20/95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v15 Predict] 40/95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v15 Predict] 60/95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v15 Predict] 80/95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Blend] Built v15 test probs: 95 elapsed 8.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[S1_mean] w16=0.65 -> submission_v16v15_S1_mean_w65.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[S1_mean] w16=0.70 -> submission_v16v15_S1_mean_w70.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[S1_mean] w16=0.75 -> submission_v16v15_S1_mean_w75.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[S4_median] w16=0.65 -> submission_v16v15_S4_median_w65.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[S4_median] w16=0.70 -> submission_v16v15_S4_median_w70.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[S4_median] w16=0.75 -> submission_v16v15_S4_median_w75.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[S5_trimmed] w16=0.65 -> submission_v16v15_S5_trimmed_w65.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[S5_trimmed] w16=0.70 -> submission_v16v15_S5_trimmed_w70.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[S5_trimmed] w16=0.75 -> submission_v16v15_S5_trimmed_w75.csv (95, 2)\n[Primary] Wrote submission.csv from submission_v16v15_S4_median_w70.csv\n"
          ]
        }
      ]
    },
    {
      "id": "f986d248-fb7d-4a4a-80a0-dfb9d1ec04c8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Swap to S5_trimmed w16=0.70 blended decode and write to submission.csv\n",
        "import os, pandas as pd, shutil\n",
        "src = 'submission_v16v15_S5_trimmed_w70.csv'\n",
        "assert os.path.exists(src), f'Missing {src}'\n",
        "shutil.copyfile(src, 'submission.csv')\n",
        "df = pd.read_csv('submission.csv')\n",
        "print('[Swap] submission.csv <-', src, df.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Swap] submission.csv <- submission_v16v15_S5_trimmed_w70.csv (95, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "86f311cf-e5b8-43a1-9c2f-a51a2a773112",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Decoder hyperparam sweep (S1..S10) with global temp g and w16 variants\n",
        "import os, time, glob, numpy as np, pandas as pd, xgboost as xgb\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from scipy.special import logit\n",
        "from scipy.signal import convolve\n",
        "\n",
        "test_ids = pd.read_csv('test.csv')['Id'].astype(int).tolist()\n",
        "\n",
        "def load_probs_dir_simple(d, ids):\n",
        "    out = {}\n",
        "    for vid in ids:\n",
        "        for pat in (f'{vid}.npz', f'{vid:05d}.npz', f'test_{vid:05d}.npz'):\n",
        "            fn = os.path.join(d, pat)\n",
        "            if os.path.exists(fn):\n",
        "                z = np.load(fn, allow_pickle=True)\n",
        "                if 'probs' in z: out[vid] = z['probs'].astype(np.float32)\n",
        "                elif 'P' in z: out[vid] = z['P'].astype(np.float32)\n",
        "                break\n",
        "    return out\n",
        "\n",
        "def load_oof_frames(oof_dirs):\n",
        "    X, Y = [], []\n",
        "    for d in oof_dirs:\n",
        "        for fn in glob.glob(os.path.join(d, '*.npz')):\n",
        "            z = np.load(fn, allow_pickle=True)\n",
        "            if 'probs' in z: P = z['probs'].astype(np.float32)\n",
        "            elif 'P' in z: P = z['P'].astype(np.float32)\n",
        "            else: continue\n",
        "            if 'y' in z: y = z['y'].astype(np.int32)\n",
        "            elif 'labels' in z: y = z['labels'].astype(np.int32)\n",
        "            else: continue\n",
        "            if P.ndim == 2 and len(y) == P.shape[0]:\n",
        "                X.append(P); Y.append(y)\n",
        "    if not X: return None, None\n",
        "    return np.concatenate(X, 0), np.concatenate(Y, 0)\n",
        "\n",
        "def fit_per_class_temperature(P_oof, y_oof, temp_grid):\n",
        "    if P_oof is None or y_oof is None:\n",
        "        return np.ones(21, dtype=np.float32)\n",
        "    C = P_oof.shape[1]\n",
        "    Tcls = np.ones(C, dtype=np.float32)\n",
        "    y = y_oof\n",
        "    for c in range(C):\n",
        "        pc = np.clip(P_oof[:, c], 1e-6, 1-1e-6); z = logit(pc)\n",
        "        yc = (y == c).astype(np.float32)\n",
        "        best, bestT = 1e18, 1.0\n",
        "        for T in temp_grid:\n",
        "            p = 1.0/(1.0 + np.exp(-z/float(T)))\n",
        "            nll = -(yc*np.log(np.clip(p,1e-6,1))).mean() - ((1-yc)*np.log(np.clip(1-p,1e-6,1))).mean()\n",
        "            if nll < best: best, bestT = float(nll), float(T)\n",
        "        Tcls[c] = bestT\n",
        "    return Tcls.astype(np.float32)\n",
        "\n",
        "def to_calibrated_logits(P, Tcls, bg_bias, smooth_win, g=1.0):\n",
        "    P = np.clip(P, 1e-6, 1-1e-6).astype(np.float32)\n",
        "    Z = logit(P).astype(np.float32)\n",
        "    Z = Z / (Tcls.reshape(1, -1) * float(g))\n",
        "    Z[:, 0] += float(bg_bias)\n",
        "    if smooth_win and smooth_win > 1:\n",
        "        k = np.ones(int(smooth_win), np.float32) / float(smooth_win)\n",
        "        Z = np.stack([convolve(Z[:, i], k, mode='same') for i in range(Z.shape[1])], 1).astype(np.float32)\n",
        "    return Z\n",
        "\n",
        "def segment_exact20_simple(margin, K, min_len):\n",
        "    T = int(len(margin)); min_len = int(max(1, min_len))\n",
        "    while K*min_len > T and min_len > 1: min_len -= 1\n",
        "    if K*min_len > T: K = min(K, T); min_len = 1\n",
        "    pref = np.concatenate([[0.0], np.cumsum(margin, 0).astype(np.float32)])\n",
        "    dp = -1e18*np.ones((K+1, T+1), np.float32); bt = -np.ones((K+1, T+1), np.int32)\n",
        "    dp[0, 0] = 0.0\n",
        "    for k in range(1, K+1):\n",
        "        start_min = (k-1)*min_len\n",
        "        for t in range(k*min_len, T+1):\n",
        "            s_lo = max(start_min, t - (T - (K-k)*min_len))\n",
        "            bestv, bests = -1e18, -1\n",
        "            for s in range(s_lo, t-min_len+1):\n",
        "                v = dp[k-1, s] + (pref[t] - pref[s])\n",
        "                if v > bestv: bestv, bests = v, s\n",
        "            dp[k, t] = bestv; bt[k, t] = bests\n",
        "    bounds = []; k, t = K, T\n",
        "    while k > 0:\n",
        "        s = int(bt[k, t]); bounds.append((s, t)); t = s; k -= 1\n",
        "    bounds.reverse(); return bounds\n",
        "\n",
        "def decode_with_cost(Z, alpha, cost_mode='mean'):\n",
        "    T = Z.shape[0]\n",
        "    min_len = max(2, int(alpha * T / 20.0))\n",
        "    margin = (Z[:, 1:21].max(1) - Z[:, 0]).astype(np.float32)\n",
        "    segs = segment_exact20_simple(margin, 20, min_len)\n",
        "    C = np.zeros((len(segs), 20), np.float32)\n",
        "    for j, (s, e) in enumerate(segs):\n",
        "        seg = Z[s:e, 1:21]\n",
        "        if cost_mode == 'median':\n",
        "            agg = np.median(seg, axis=0)\n",
        "        elif cost_mode == 'trimmed_mean_20':\n",
        "            L = seg.shape[0]; lo = int(0.1*L); hi = max(lo+1, int(0.9*L))\n",
        "            agg = np.sort(seg, axis=0)[lo:hi].mean(0)\n",
        "        else:\n",
        "            agg = seg.mean(0)\n",
        "        C[j, :] = -agg\n",
        "    r, c = linear_sum_assignment(C)\n",
        "    order = [int(c[j])+1 for j in np.argsort(r)]\n",
        "    return order\n",
        "\n",
        "# Load probs\n",
        "map16 = load_probs_dir_simple('test_probs_v16', test_ids)\n",
        "print('[Sweep] v16 test probs:', len(map16))\n",
        "\n",
        "# Build v15 probs via boosters (fast) if available\n",
        "bstA = bstB = None\n",
        "try:\n",
        "    if os.path.exists('xgb_train.model'):\n",
        "        tmp = xgb.Booster(); tmp.load_model('xgb_train.model')\n",
        "        if int(tmp.num_features()) == 120: bstA = tmp\n",
        "except Exception as e:\n",
        "    print('[v15] skip A:', e)\n",
        "try:\n",
        "    if os.path.exists('xgb_train_v15.model'):\n",
        "        tmp = xgb.Booster(); tmp.load_model('xgb_train_v15.model')\n",
        "        if int(tmp.num_features()) == 120: bstB = tmp\n",
        "except Exception as e:\n",
        "    print('[v15] skip B:', e)\n",
        "map15 = {}\n",
        "if (bstA is not None) or (bstB is not None):\n",
        "    t0 = time.time()\n",
        "    for i, sid in enumerate(test_ids, 1):\n",
        "        fn = os.path.join('cache_v15', f'test_{sid:05d}.npz')\n",
        "        if not os.path.exists(fn): continue\n",
        "        X = np.load(fn)['X'].astype(np.float32)\n",
        "        dm = xgb.DMatrix(X)\n",
        "        Ps = []\n",
        "        if bstA is not None: Ps.append(bstA.predict(dm))\n",
        "        if bstB is not None: Ps.append(bstB.predict(dm))\n",
        "        if Ps:\n",
        "            P = np.mean(Ps, axis=0).astype(np.float32)\n",
        "            P /= np.clip(P.sum(1, keepdims=True), 1e-8, None)\n",
        "            map15[sid] = P\n",
        "        if i % 20 == 0: print(f'[v15] {i}/{len(test_ids)}')\n",
        "    print('[Sweep] v15 built:', len(map15), 'in', f'{time.time()-t0:.1f}s')\n",
        "else:\n",
        "    print('[Sweep] No v15 boosters; proceeding v16-only with w16=1.0')\n",
        "\n",
        "# Calibrate per-class temperatures separately\n",
        "temp_grid = np.linspace(0.7, 1.6, 12).astype(np.float32)\n",
        "P_oof16, y_oof16 = load_oof_frames(['oof_probs_v16'])\n",
        "Tcls16 = fit_per_class_temperature(P_oof16, y_oof16, temp_grid)\n",
        "if len(map15) > 0:\n",
        "    P_oof15, y_oof15 = load_oof_frames(['oof_probs_v15'])\n",
        "    Tcls15 = fit_per_class_temperature(P_oof15, y_oof15, temp_grid)\n",
        "else:\n",
        "    Tcls15 = Tcls16\n",
        "\n",
        "def run_one(name, alpha, bg_bias, smooth, cost_mode, w16=0.70, g=1.0):\n",
        "    rows = []\n",
        "    for vid in sorted(test_ids):\n",
        "        P16 = map16.get(vid)\n",
        "        P15 = map15.get(vid) if len(map15)>0 else None\n",
        "        if P16 is None and P15 is None: continue\n",
        "        Z_mix = None\n",
        "        if P16 is not None:\n",
        "            Z16 = to_calibrated_logits(P16, Tcls16, bg_bias, smooth, g=g)\n",
        "            Z_mix = Z16\n",
        "        if P15 is not None:\n",
        "            Z15 = to_calibrated_logits(P15, Tcls15, bg_bias, smooth, g=g)\n",
        "            if Z_mix is None:\n",
        "                Z_mix = Z15\n",
        "            else:\n",
        "                Z_mix = (w16*Z_mix + (1.0-w16)*Z15).astype(np.float32)\n",
        "        seq = decode_with_cost(Z_mix, alpha, cost_mode=cost_mode)\n",
        "        rows.append((vid, ' '.join(map(str, seq))))\n",
        "    sub = pd.DataFrame(rows, columns=['Id','Sequence']).sort_values('Id')\n",
        "    out = f'submission_sweep_{name}.csv'\n",
        "    sub.to_csv(out, index=False)\n",
        "    print('[Sweep] wrote', out, sub.shape)\n",
        "    return out\n",
        "\n",
        "# Runs S1..S10 per expert plan\n",
        "outs = []\n",
        "outs.append(run_one('S1_w70_mean_a0.85_bb0.20_s1_g1.0', alpha=0.85, bg_bias=0.20, smooth=1, cost_mode='mean', w16=0.70, g=1.0))\n",
        "outs.append(run_one('S2_w70_mean_a0.80_bb0.25_s1_g1.0', alpha=0.80, bg_bias=0.25, smooth=1, cost_mode='mean', w16=0.70, g=1.0))\n",
        "outs.append(run_one('S3_w70_mean_a0.90_bb0.15_s1_g1.0', alpha=0.90, bg_bias=0.15, smooth=1, cost_mode='mean', w16=0.70, g=1.0))\n",
        "outs.append(run_one('S4_w70_median_a0.85_bb0.25_s3_g1.0', alpha=0.85, bg_bias=0.25, smooth=3, cost_mode='median', w16=0.70, g=1.0))\n",
        "outs.append(run_one('S5_w70_trim_a0.85_bb0.20_s3_g1.0', alpha=0.85, bg_bias=0.20, smooth=3, cost_mode='trimmed_mean_20', w16=0.70, g=1.0))\n",
        "outs.append(run_one('S6_w70_mean_a0.80_bb0.30_s3_g1.0', alpha=0.80, bg_bias=0.30, smooth=3, cost_mode='mean', w16=0.70, g=1.0))\n",
        "outs.append(run_one('S7_w70_mean_a0.85_bb0.20_s1_g0.9', alpha=0.85, bg_bias=0.20, smooth=1, cost_mode='mean', w16=0.70, g=0.9))\n",
        "outs.append(run_one('S8_w70_mean_a0.85_bb0.20_s1_g1.1', alpha=0.85, bg_bias=0.20, smooth=1, cost_mode='mean', w16=0.70, g=1.1))\n",
        "outs.append(run_one('S9_w60_mean_a0.85_bb0.20_s1_g1.0', alpha=0.85, bg_bias=0.20, smooth=1, cost_mode='mean', w16=0.60, g=1.0))\n",
        "outs.append(run_one('S10_w80_mean_a0.85_bb0.20_s1_g1.0', alpha=0.85, bg_bias=0.20, smooth=1, cost_mode='mean', w16=0.80, g=1.0))\n",
        "\n",
        "print('[Sweep] completed', len(outs), 'files')\n",
        "print('\\n'.join(outs))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sweep] v16 test probs: 95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v15] 20/95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v15] 40/95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v15] 60/95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v15] 80/95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sweep] v15 built: 95 in 8.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sweep] wrote submission_sweep_S1_w70_mean_a0.85_bb0.20_s1_g1.0.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sweep] wrote submission_sweep_S2_w70_mean_a0.80_bb0.25_s1_g1.0.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sweep] wrote submission_sweep_S3_w70_mean_a0.90_bb0.15_s1_g1.0.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sweep] wrote submission_sweep_S4_w70_median_a0.85_bb0.25_s3_g1.0.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sweep] wrote submission_sweep_S5_w70_trim_a0.85_bb0.20_s3_g1.0.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sweep] wrote submission_sweep_S6_w70_mean_a0.80_bb0.30_s3_g1.0.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sweep] wrote submission_sweep_S7_w70_mean_a0.85_bb0.20_s1_g0.9.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sweep] wrote submission_sweep_S8_w70_mean_a0.85_bb0.20_s1_g1.1.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sweep] wrote submission_sweep_S9_w60_mean_a0.85_bb0.20_s1_g1.0.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sweep] wrote submission_sweep_S10_w80_mean_a0.85_bb0.20_s1_g1.0.csv (95, 2)\n[Sweep] completed 10 files\nsubmission_sweep_S1_w70_mean_a0.85_bb0.20_s1_g1.0.csv\nsubmission_sweep_S2_w70_mean_a0.80_bb0.25_s1_g1.0.csv\nsubmission_sweep_S3_w70_mean_a0.90_bb0.15_s1_g1.0.csv\nsubmission_sweep_S4_w70_median_a0.85_bb0.25_s3_g1.0.csv\nsubmission_sweep_S5_w70_trim_a0.85_bb0.20_s3_g1.0.csv\nsubmission_sweep_S6_w70_mean_a0.80_bb0.30_s3_g1.0.csv\nsubmission_sweep_S7_w70_mean_a0.85_bb0.20_s1_g0.9.csv\nsubmission_sweep_S8_w70_mean_a0.85_bb0.20_s1_g1.1.csv\nsubmission_sweep_S9_w60_mean_a0.85_bb0.20_s1_g1.0.csv\nsubmission_sweep_S10_w80_mean_a0.85_bb0.20_s1_g1.0.csv\n"
          ]
        }
      ]
    },
    {
      "id": "5282c6ed-2dda-4734-ae3b-cd90d1c716b4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Swap to sweep S6 (w16=0.70, mean, alpha=0.80, bg_bias=0.30, smooth=3, g=1.0)\n",
        "import os, shutil, pandas as pd\n",
        "src = 'submission_sweep_S6_w70_mean_a0.80_bb0.30_s3_g1.0.csv'\n",
        "assert os.path.exists(src), f'Missing {src}'\n",
        "shutil.copyfile(src, 'submission.csv')\n",
        "df = pd.read_csv('submission.csv')\n",
        "print('[Swap] submission.csv <-', src, df.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Swap] submission.csv <- submission_sweep_S6_w70_mean_a0.80_bb0.30_s3_g1.0.csv (95, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "0c3e9a8f-1caa-4539-a9cc-70af528b4b15",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Weighted rank-ensemble hedge: 0.70 calibrated decode + 0.20 w7_A + 0.10 w6_A\n",
        "import pandas as pd, numpy as np, os\n",
        "\n",
        "# Choose primary calibrated decode (from our sweep); adjust if needed\n",
        "primary = 'submission_sweep_S6_w70_mean_a0.80_bb0.30_s3_g1.0.csv'\n",
        "fallback = 'submission_v16v15_S4_median_w70.csv'\n",
        "if not os.path.exists(primary):\n",
        "    assert os.path.exists(fallback), f'Missing both {primary} and {fallback}'\n",
        "    primary = fallback\n",
        "\n",
        "aux1 = 'submission_blend_w7_A.csv'\n",
        "aux2 = 'submission_blend_w6_A.csv'\n",
        "assert os.path.exists(primary) and os.path.exists(aux1) and os.path.exists(aux2)\n",
        "\n",
        "w_primary, w_aux1, w_aux2 = 0.70, 0.20, 0.10\n",
        "\n",
        "dfs = [pd.read_csv(f).sort_values('Id').reset_index(drop=True) for f in (primary, aux1, aux2)]\n",
        "ids = dfs[0]['Id'].tolist()\n",
        "for df in dfs: assert df.shape[0] == 95 and set(df.columns)=={'Id','Sequence'} and df['Id'].tolist()==ids\n",
        "\n",
        "def seq_to_order(seq_str):\n",
        "    arr = [int(x) for x in str(seq_str).split()]\n",
        "    # ensure valid classes 1..20; trim/repair if needed\n",
        "    arr = [x for x in arr if 1 <= x <= 20][:20]\n",
        "    # rescue to permutation 1..20\n",
        "    seen, dup_idx = set(), []\n",
        "    for i, x in enumerate(arr):\n",
        "        if x in seen: dup_idx.append(i)\n",
        "        else: seen.add(x)\n",
        "    missing = [k for k in range(1,21) if k not in seen]\n",
        "    for j, i in enumerate(dup_idx):\n",
        "        if j < len(missing): arr[i] = missing[j]\n",
        "    # If still short, append remaining missing\n",
        "    if len(arr) < 20: arr += [k for k in range(1,21) if k not in set(arr)]\n",
        "    return arr[:20]\n",
        "\n",
        "weights = np.array([w_primary, w_aux1, w_aux2], dtype=np.float32)\n",
        "seqs = []\n",
        "for i in range(len(ids)):\n",
        "    rank_sum = np.zeros(21, dtype=np.float32)  # index 1..20 used\n",
        "    for j, df in enumerate(dfs):\n",
        "        order = seq_to_order(df.iloc[i].Sequence)\n",
        "        for pos, cls in enumerate(order):\n",
        "            rank_sum[cls] += weights[j] * (pos + 1)\n",
        "    order = np.argsort(rank_sum[1:]) + 1  # 1..20 by ascending weighted rank\n",
        "    seqs.append(' '.join(map(str, order)))\n",
        "\n",
        "out = pd.DataFrame({'Id': ids, 'Sequence': seqs})\n",
        "out.to_csv('submission.csv', index=False)\n",
        "print('[HedgeRank] submission.csv from', primary, '+ 0.20 w7_A + 0.10 w6_A', out.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HedgeRank] submission.csv from submission_sweep_S6_w70_mean_a0.80_bb0.30_s3_g1.0.csv + 0.20 w7_A + 0.10 w6_A (95, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "64dda3ab-138f-4acd-ba31-d544c5dbdd8e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create S4 median with g=0.95 (w16=0.70), then set as submission.csv\n",
        "import os, pandas as pd, shutil\n",
        "\n",
        "# Assumes run_one and all helpers from cell 16 are in scope\n",
        "out = run_one('S4_w70_median_a0.85_bb0.25_s3_g0.95', alpha=0.85, bg_bias=0.25, smooth=3, cost_mode='median', w16=0.70, g=0.95)\n",
        "src = f'submission_sweep_S4_w70_median_a0.85_bb0.25_s3_g0.95.csv'\n",
        "assert os.path.exists(src), f'Missing {src}'\n",
        "shutil.copyfile(src, 'submission.csv')\n",
        "df = pd.read_csv('submission.csv')\n",
        "print('[Swap] submission.csv <-', src, df.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sweep] wrote submission_sweep_S4_w70_median_a0.85_bb0.25_s3_g0.95.csv (95, 2)\n[Swap] submission.csv <- submission_sweep_S4_w70_median_a0.85_bb0.25_s3_g0.95.csv (95, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "3c1fb650-f40c-4ec1-bd6e-f6a2b2d121bc",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Swap to expert-picked S7 (w16=0.70, mean, alpha=0.85, bg_bias=0.20, smooth=1, g=0.9)\n",
        "import os, shutil, pandas as pd\n",
        "src = 'submission_sweep_S7_w70_mean_a0.85_bb0.20_s1_g0.9.csv'\n",
        "assert os.path.exists(src), f'Missing {src}'\n",
        "shutil.copyfile(src, 'submission.csv')\n",
        "df = pd.read_csv('submission.csv')\n",
        "print('[Swap] submission.csv <-', src, df.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Swap] submission.csv <- submission_sweep_S7_w70_mean_a0.85_bb0.20_s1_g0.9.csv (95, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "1b7a6faf-4031-4177-9b23-962190332612",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Swap to expert fallback S9 (w16=0.60, mean, alpha=0.85, bg_bias=0.20, smooth=1, g=1.0)\n",
        "import os, shutil, pandas as pd\n",
        "src = 'submission_sweep_S9_w60_mean_a0.85_bb0.20_s1_g1.0.csv'\n",
        "assert os.path.exists(src), f'Missing {src}'\n",
        "shutil.copyfile(src, 'submission.csv')\n",
        "df = pd.read_csv('submission.csv')\n",
        "print('[Swap] submission.csv <-', src, df.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Swap] submission.csv <- submission_sweep_S9_w60_mean_a0.85_bb0.20_s1_g1.0.csv (95, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "302f9a1d-80cb-4ebc-99e8-ebe5ac70bbed",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Log-prob decoding variant (per expert coach): S4 median with g=0.95 using log-probabilities\n",
        "import os, glob, time, numpy as np, pandas as pd, xgboost as xgb\n",
        "from scipy.special import logit, softmax\n",
        "from scipy.signal import convolve\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "eps = 1e-6\n",
        "test_ids = pd.read_csv('test.csv')['Id'].astype(int).tolist()\n",
        "\n",
        "def load_probs_dir_simple(d, ids):\n",
        "    out = {}\n",
        "    for vid in ids:\n",
        "        for pat in (f'{vid}.npz', f'{vid:05d}.npz', f'test_{vid:05d}.npz'):\n",
        "            fn = os.path.join(d, pat)\n",
        "            if os.path.exists(fn):\n",
        "                z = np.load(fn, allow_pickle=True)\n",
        "                if 'probs' in z: out[vid] = z['probs'].astype(np.float32)\n",
        "                elif 'P' in z: out[vid] = z['P'].astype(np.float32)\n",
        "                break\n",
        "    return out\n",
        "\n",
        "def load_oof_frames(oof_dirs):\n",
        "    X, Y = [], []\n",
        "    for d in oof_dirs:\n",
        "        for fn in glob.glob(os.path.join(d, '*.npz')):\n",
        "            z = np.load(fn, allow_pickle=True)\n",
        "            if 'probs' in z: P = z['probs'].astype(np.float32)\n",
        "            elif 'P' in z: P = z['P'].astype(np.float32)\n",
        "            else: continue\n",
        "            if 'y' in z: y = z['y'].astype(np.int32)\n",
        "            elif 'labels' in z: y = z['labels'].astype(np.int32)\n",
        "            else: continue\n",
        "            if P.ndim == 2 and len(y) == P.shape[0]:\n",
        "                X.append(P); Y.append(y)\n",
        "    if not X: return None, None\n",
        "    return np.concatenate(X, 0), np.concatenate(Y, 0)\n",
        "\n",
        "def fit_per_class_temperature(P_oof, y_oof, temp_grid):\n",
        "    if P_oof is None or y_oof is None:\n",
        "        return np.ones(21, dtype=np.float32)\n",
        "    C = P_oof.shape[1]\n",
        "    Tcls = np.ones(C, dtype=np.float32)\n",
        "    y = y_oof\n",
        "    for c in range(C):\n",
        "        pc = np.clip(P_oof[:, c], 1e-6, 1-1e-6); z = logit(pc)\n",
        "        yc = (y == c).astype(np.float32)\n",
        "        best, bestT = 1e18, 1.0\n",
        "        for T in temp_grid:\n",
        "            p = 1.0/(1.0 + np.exp(-z/float(T)))\n",
        "            nll = -(yc*np.log(np.clip(p,1e-6,1))).mean() - ((1-yc)*np.log(np.clip(1-p,1e-6,1))).mean()\n",
        "            if nll < best: best, bestT = float(nll), float(T)\n",
        "        Tcls[c] = bestT\n",
        "    return Tcls.astype(np.float32)\n",
        "\n",
        "def to_calibrated_logprobs(P, Tcls, bg_bias, smooth_win, g=1.0):\n",
        "    # Clip, convert to per-class scaled logits, softmax to calibrated probs, then log-prob; smooth in log space\n",
        "    P = np.clip(P, 1e-6, 1-1e-6).astype(np.float32)\n",
        "    Z = logit(P).astype(np.float32)  # per-class logits (binary logit approximation per class)\n",
        "    Z = Z / (Tcls.reshape(1, -1) * float(g))\n",
        "    # Convert to calibrated probs via softmax across 21 classes\n",
        "    Pcal = softmax(Z, axis=1).astype(np.float32)\n",
        "    # Ensure normalization and numeric stability\n",
        "    Pcal = Pcal / np.clip(Pcal.sum(1, keepdims=True), 1e-6, None)\n",
        "    L = np.log(np.clip(Pcal, 1e-6, 1.0)).astype(np.float32)  # log-probabilities\n",
        "    # Background bias in log-space: add bias to bg log-prob\n",
        "    L[:, 0] += float(bg_bias)\n",
        "    if smooth_win and smooth_win > 1:\n",
        "        k = np.ones(int(smooth_win), np.float32) / float(smooth_win)\n",
        "        L = np.stack([convolve(L[:, i], k, mode='same') for i in range(L.shape[1])], 1).astype(np.float32)\n",
        "    return L\n",
        "\n",
        "def segment_exact20_simple(margin, K, min_len):\n",
        "    T = int(len(margin)); min_len = int(max(1, min_len))\n",
        "    while K*min_len > T and min_len > 1: min_len -= 1\n",
        "    if K*min_len > T: K = min(K, T); min_len = 1\n",
        "    pref = np.concatenate([[0.0], np.cumsum(margin, 0).astype(np.float32)])\n",
        "    dp = -1e18*np.ones((K+1, T+1), np.float32); bt = -np.ones((K+1, T+1), np.int32)\n",
        "    dp[0, 0] = 0.0\n",
        "    for k in range(1, K+1):\n",
        "        start_min = (k-1)*min_len\n",
        "        for t in range(k*min_len, T+1):\n",
        "            s_lo = max(start_min, t - (T - (K-k)*min_len))\n",
        "            bestv, bests = -1e18, -1\n",
        "            for s in range(s_lo, t-min_len+1):\n",
        "                v = dp[k-1, s] + (pref[t] - pref[s])\n",
        "                if v > bestv: bestv, bests = v, s\n",
        "            dp[k, t] = bestv; bt[k, t] = bests\n",
        "    bounds = []; k, t = K, T\n",
        "    while k > 0:\n",
        "        s = int(bt[k, t]); bounds.append((s, t)); t = s; k -= 1\n",
        "    bounds.reverse(); return bounds\n",
        "\n",
        "def decode_with_cost_logprob(L, alpha, cost_mode='median'):\n",
        "    # L are log-probabilities; use margin on log-probs\n",
        "    T = L.shape[0]\n",
        "    min_len = max(2, int(alpha * T / 20.0))\n",
        "    margin = (L[:, 1:21].max(1) - L[:, 0]).astype(np.float32)\n",
        "    segs = segment_exact20_simple(margin, 20, min_len)\n",
        "    C = np.zeros((len(segs), 20), np.float32)\n",
        "    for j, (s, e) in enumerate(segs):\n",
        "        seg = L[s:e, 1:21]\n",
        "        if cost_mode == 'median':\n",
        "            agg = np.median(seg, axis=0)\n",
        "        elif cost_mode == 'trimmed_mean_20':\n",
        "            Ln = seg.shape[0]; lo = int(0.1*Ln); hi = max(lo+1, int(0.9*Ln))\n",
        "            agg = np.sort(seg, axis=0)[lo:hi].mean(0)\n",
        "        else:\n",
        "            agg = seg.mean(0)\n",
        "        C[j, :] = -agg  # maximize log-prob -> minimize negative\n",
        "    r, c = linear_sum_assignment(C)\n",
        "    order = [int(c[j])+1 for j in np.argsort(r)]\n",
        "    return order\n",
        "\n",
        "# Load per-frame probs for v16 and v15 and per-class temps\n",
        "map16 = load_probs_dir_simple('test_probs_v16', test_ids)\n",
        "bstA = bstB = None\n",
        "try:\n",
        "    if os.path.exists('xgb_train.model'):\n",
        "        tmp = xgb.Booster(); tmp.load_model('xgb_train.model')\n",
        "        if int(tmp.num_features()) == 120: bstA = tmp\n",
        "except Exception as e:\n",
        "    print('[v15] skip A:', e)\n",
        "try:\n",
        "    if os.path.exists('xgb_train_v15.model'):\n",
        "        tmp = xgb.Booster(); tmp.load_model('xgb_train_v15.model')\n",
        "        if int(tmp.num_features()) == 120: bstB = tmp\n",
        "except Exception as e:\n",
        "    print('[v15] skip B:', e)\n",
        "map15 = {}\n",
        "if (bstA is not None) or (bstB is not None):\n",
        "    for sid in test_ids:\n",
        "        fn = os.path.join('cache_v15', f'test_{sid:05d}.npz')\n",
        "        if not os.path.exists(fn): continue\n",
        "        X = np.load(fn)['X'].astype(np.float32)\n",
        "        dm = xgb.DMatrix(X)\n",
        "        Ps = []\n",
        "        if bstA is not None: Ps.append(bstA.predict(dm))\n",
        "        if bstB is not None: Ps.append(bstB.predict(dm))\n",
        "        if Ps:\n",
        "            P = np.mean(Ps, axis=0).astype(np.float32)\n",
        "            P /= np.clip(P.sum(1, keepdims=True), 1e-8, None)\n",
        "            map15[sid] = P\n",
        "\n",
        "temp_grid = np.linspace(0.7, 1.6, 12).astype(np.float32)\n",
        "P_oof16, y_oof16 = load_oof_frames(['oof_probs_v16'])\n",
        "Tcls16 = fit_per_class_temperature(P_oof16, y_oof16, temp_grid)\n",
        "P_oof15, y_oof15 = load_oof_frames(['oof_probs_v15'])\n",
        "Tcls15 = fit_per_class_temperature(P_oof15, y_oof15, temp_grid) if P_oof15 is not None else Tcls16\n",
        "\n",
        "def run_one_logprob(name, alpha, bg_bias, smooth, cost_mode, w16=0.70, g=0.95):\n",
        "    rows = []\n",
        "    for vid in sorted(test_ids):\n",
        "        P16 = map16.get(vid); P15 = map15.get(vid)\n",
        "        if P16 is None and P15 is None: continue\n",
        "        Lmix = None\n",
        "        if P16 is not None:\n",
        "            L16 = to_calibrated_logprobs(P16, Tcls16, bg_bias, smooth, g=g)\n",
        "            Lmix = L16\n",
        "        if P15 is not None:\n",
        "            L15 = to_calibrated_logprobs(P15, Tcls15, bg_bias, smooth, g=g)\n",
        "            if Lmix is None:\n",
        "                Lmix = L15\n",
        "            else:\n",
        "                Lmix = (w16*Lmix + (1.0-w16)*L15).astype(np.float32)\n",
        "        seq = decode_with_cost_logprob(Lmix, alpha, cost_mode=cost_mode)\n",
        "        rows.append((vid, ' '.join(map(str, seq))))\n",
        "    sub = pd.DataFrame(rows, columns=['Id','Sequence']).sort_values('Id')\n",
        "    out = f'submission_logprob_{name}.csv'\n",
        "    sub.to_csv(out, index=False)\n",
        "    print('[LogProb] wrote', out, sub.shape)\n",
        "    return out\n",
        "\n",
        "# Expert-pick: S4 median with g=0.95, w16=0.70\n",
        "out = run_one_logprob('S4_w70_median_a0.85_bb0.25_s3_g0.95', alpha=0.85, bg_bias=0.25, smooth=3, cost_mode='median', w16=0.70, g=0.95)\n",
        "pd.read_csv(out).to_csv('submission.csv', index=False)\n",
        "print('[Swap] submission.csv <-', out)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LogProb] wrote submission_logprob_S4_w70_median_a0.85_bb0.25_s3_g0.95.csv (95, 2)\n[Swap] submission.csv <- submission_logprob_S4_w70_median_a0.85_bb0.25_s3_g0.95.csv\n"
          ]
        }
      ]
    },
    {
      "id": "5f36cf35-d7f9-4ffc-baa8-736f82f42cd7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Log-prob decoding: S5 trimmed mean with g=0.95 (w16=0.70), then set as submission.csv\n",
        "import pandas as pd, shutil, os\n",
        "out = run_one_logprob('S5_w70_trim_a0.85_bb0.20_s3_g0.95', alpha=0.85, bg_bias=0.20, smooth=3, cost_mode='trimmed_mean_20', w16=0.70, g=0.95)\n",
        "src = f'submission_logprob_S5_w70_trim_a0.85_bb0.20_s3_g0.95.csv'\n",
        "assert os.path.exists(src), f'Missing {src}'\n",
        "shutil.copyfile(src, 'submission.csv')\n",
        "df = pd.read_csv('submission.csv')\n",
        "print('[Swap] submission.csv <-', src, df.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LogProb] wrote submission_logprob_S5_w70_trim_a0.85_bb0.20_s3_g0.95.csv (95, 2)\n[Swap] submission.csv <- submission_logprob_S5_w70_trim_a0.85_bb0.20_s3_g0.95.csv (95, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "a4c21dc6-8d47-44c7-9e87-96350306d820",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Log-prob S4 median g=0.95 with w16 in {0.65, 0.75}; set 0.65 as submission.csv\n",
        "import os, pandas as pd, shutil\n",
        "\n",
        "# Generate both variants\n",
        "out65 = run_one_logprob('S4_w65_median_a0.85_bb0.25_s3_g0.95', alpha=0.85, bg_bias=0.25, smooth=3, cost_mode='median', w16=0.65, g=0.95)\n",
        "out75 = run_one_logprob('S4_w75_median_a0.85_bb0.25_s3_g0.95', alpha=0.85, bg_bias=0.25, smooth=3, cost_mode='median', w16=0.75, g=0.95)\n",
        "\n",
        "# Copy w16=0.65 to submission.csv for next submission; we'll try 0.75 next if needed\n",
        "src = 'submission_logprob_S4_w65_median_a0.85_bb0.25_s3_g0.95.csv'\n",
        "assert os.path.exists(src), f'Missing {src}'\n",
        "shutil.copyfile(src, 'submission.csv')\n",
        "df = pd.read_csv('submission.csv')\n",
        "print('[Swap] submission.csv <-', src, df.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LogProb] wrote submission_logprob_S4_w65_median_a0.85_bb0.25_s3_g0.95.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LogProb] wrote submission_logprob_S4_w75_median_a0.85_bb0.25_s3_g0.95.csv (95, 2)\n[Swap] submission.csv <- submission_logprob_S4_w65_median_a0.85_bb0.25_s3_g0.95.csv (95, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "15b1faad-f742-4d35-946b-efe90cb8e94a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Swap to log-prob S4 median g=0.95 w16=0.75 and write to submission.csv\n",
        "import os, shutil, pandas as pd\n",
        "src = 'submission_logprob_S4_w75_median_a0.85_bb0.25_s3_g0.95.csv'\n",
        "assert os.path.exists(src), f'Missing {src}'\n",
        "shutil.copyfile(src, 'submission.csv')\n",
        "df = pd.read_csv('submission.csv')\n",
        "print('[Swap] submission.csv <-', src, df.shape)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Swap] submission.csv <- submission_logprob_S4_w75_median_a0.85_bb0.25_s3_g0.95.csv (95, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "c45d5cd5-0d95-4b14-aeb1-995d905d5989",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Log-prob S4 median g=0.95 with smooth=1 (w16=0.70), then set as submission.csv\n",
        "import os, pandas as pd, shutil\n",
        "out = run_one_logprob('S4_w70_median_a0.85_bb0.25_s1_g0.95', alpha=0.85, bg_bias=0.25, smooth=1, cost_mode='median', w16=0.70, g=0.95)\n",
        "src = 'submission_logprob_S4_w70_median_a0.85_bb0.25_s1_g0.95.csv'\n",
        "assert os.path.exists(src), f'Missing {src}'\n",
        "shutil.copyfile(src, 'submission.csv')\n",
        "df = pd.read_csv('submission.csv')\n",
        "print('[Swap] submission.csv <-', src, df.shape)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LogProb] wrote submission_logprob_S4_w70_median_a0.85_bb0.25_s1_g0.95.csv (95, 2)\n[Swap] submission.csv <- submission_logprob_S4_w70_median_a0.85_bb0.25_s1_g0.95.csv (95, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "7daed2e8-d8e0-4759-b34d-51dab0d37291",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Log-prob v16-only (ignore v15) S4 median g=0.95; set as submission.csv\n",
        "import os, pandas as pd, shutil\n",
        "\n",
        "# Ensure v15 stream is ignored\n",
        "try:\n",
        "    map15.clear()\n",
        "except Exception:\n",
        "    map15 = {}\n",
        "\n",
        "# v16-only: w16=1.0, smooth=3\n",
        "out_v16_s3 = run_one_logprob('S4_w100_v16only_median_a0.85_bb0.25_s3_g0.95', alpha=0.85, bg_bias=0.25, smooth=3, cost_mode='median', w16=1.0, g=0.95)\n",
        "\n",
        "# Optionally also try smooth=1 quickly and choose one; default to s3 for submission.csv\n",
        "out_v16_s1 = run_one_logprob('S4_w100_v16only_median_a0.85_bb0.25_s1_g0.95', alpha=0.85, bg_bias=0.25, smooth=1, cost_mode='median', w16=1.0, g=0.95)\n",
        "\n",
        "src = 'submission_logprob_S4_w100_v16only_median_a0.85_bb0.25_s3_g0.95.csv'\n",
        "assert os.path.exists(src), f'Missing {src}'\n",
        "shutil.copyfile(src, 'submission.csv')\n",
        "df = pd.read_csv('submission.csv')\n",
        "print('[Swap] submission.csv <-', src, df.shape)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LogProb] wrote submission_logprob_S4_w100_v16only_median_a0.85_bb0.25_s3_g0.95.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LogProb] wrote submission_logprob_S4_w100_v16only_median_a0.85_bb0.25_s1_g0.95.csv (95, 2)\n[Swap] submission.csv <- submission_logprob_S4_w100_v16only_median_a0.85_bb0.25_s3_g0.95.csv (95, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "038e4b3f-0c17-4184-bd9b-d4481349e935",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Log-prob S4 median with stronger bias/shorter segments: a=0.80, bb=0.30, s3, g=0.95, w16=0.70\n",
        "import os, pandas as pd, shutil\n",
        "out = run_one_logprob('S4_w70_median_a0.80_bb0.30_s3_g0.95', alpha=0.80, bg_bias=0.30, smooth=3, cost_mode='median', w16=0.70, g=0.95)\n",
        "src = 'submission_logprob_S4_w70_median_a0.80_bb0.30_s3_g0.95.csv'\n",
        "assert os.path.exists(src), f'Missing {src}'\n",
        "shutil.copyfile(src, 'submission.csv')\n",
        "df = pd.read_csv('submission.csv')\n",
        "print('[Swap] submission.csv <-', src, df.shape)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LogProb] wrote submission_logprob_S4_w70_median_a0.80_bb0.30_s3_g0.95.csv (95, 2)\n[Swap] submission.csv <- submission_logprob_S4_w70_median_a0.80_bb0.30_s3_g0.95.csv (95, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "31e4318d-9af2-4423-a1d5-a066efdc508a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Final picks per expert: New log-prob S4 median g=0.90, then prepare S9 as next\n",
        "import os, shutil, pandas as pd\n",
        "\n",
        "# 1) New: log-prob S4 median with g=0.90 (w16=0.70, a=0.85, bb=0.25, s3)\n",
        "out1 = run_one_logprob('S4_w70_median_a0.85_bb0.25_s3_g0.90',\n",
        "                       alpha=0.85, bg_bias=0.25, smooth=3,\n",
        "                       cost_mode='median', w16=0.70, g=0.90)\n",
        "src1 = 'submission_logprob_S4_w70_median_a0.85_bb0.25_s3_g0.90.csv'\n",
        "assert os.path.exists(src1), f'Missing {src1}'\n",
        "shutil.copyfile(src1, 'submission.csv')\n",
        "print('[Submit #1] submission.csv <-', src1)\n",
        "\n",
        "# 2) Diverse hedge (existing): S9 mean\n",
        "src2 = 'submission_sweep_S9_w60_mean_a0.85_bb0.20_s1_g1.0.csv'\n",
        "assert os.path.exists(src2), f'Missing {src2}'\n",
        "shutil.copyfile(src2, 'submission_next.csv')\n",
        "print('[Submit #2 ready] submission_next.csv <-', src2)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LogProb] wrote submission_logprob_S4_w70_median_a0.85_bb0.25_s3_g0.90.csv (95, 2)\n[Submit #1] submission.csv <- submission_logprob_S4_w70_median_a0.85_bb0.25_s3_g0.90.csv\n[Submit #2 ready] submission_next.csv <- submission_sweep_S9_w60_mean_a0.85_bb0.20_s1_g1.0.csv\n"
          ]
        }
      ]
    },
    {
      "id": "da1c0c60-8a5f-4e42-95fe-bf5c76b30f5c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Swap staged Pick #2 (S9) into submission.csv\n",
        "import os, shutil, pandas as pd\n",
        "src = 'submission_next.csv'\n",
        "assert os.path.exists(src), f'Missing {src}'\n",
        "shutil.copyfile(src, 'submission.csv')\n",
        "df = pd.read_csv('submission.csv')\n",
        "print('[SwapNext] submission.csv <-', src, df.shape)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SwapNext] submission.csv <- submission_next.csv (95, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "ae787abb-234f-4e04-b5c5-68de22ddffe0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sanity checks: class mapping and submission format (robust + infer bg column across all OOF files)\n",
        "import glob, numpy as np, pandas as pd, os, json, random\n",
        "\n",
        "print('=== OOF/class mapping sanity ===')\n",
        "oof_files = sorted(glob.glob('oof_probs_v16/oof_*.npz'))\n",
        "assert oof_files, 'No OOF files found in oof_probs_v16'\n",
        "n_show = min(3, len(oof_files))\n",
        "print('oof files count=', len(oof_files), 'showing first', n_show)\n",
        "means = []\n",
        "for i, fn in enumerate(oof_files):\n",
        "    z = np.load(fn)\n",
        "    keys = set(getattr(z, 'files', []))\n",
        "    P = z['probs'] if 'probs' in keys else (z['P'] if 'P' in keys else None)\n",
        "    if P is None: continue\n",
        "    if P.ndim != 2: continue\n",
        "    if i < n_show:\n",
        "        print(' sample', i, 'keys=', sorted(list(keys)), 'shape=', P.shape)\n",
        "    means.append(P.mean(0))\n",
        "assert means, 'No usable probs in OOF npz files'\n",
        "m_all = np.mean(np.stack(means, 0), 0)\n",
        "C = m_all.shape[0]\n",
        "print('C (num classes)=', C)\n",
        "bg_idx = int(np.argmax(m_all))\n",
        "print('Per-class mean (first 10)=', np.round(m_all[:10], 6).tolist(), '...')\n",
        "print('Inferred bg_idx =', bg_idx, '(0-based). m_all[bg]=', float(m_all[bg_idx]))\n",
        "assert C == 21, f'Unexpected class count {C}; expected 21'\n",
        "\n",
        "# Cross-check with training cache labels (expect labels in 0..20 and background label id likely 0) \n",
        "train_cache = sorted(glob.glob('cache_v16/train_*.npz'))\n",
        "tr_y_min = tr_y_max = None\n",
        "if train_cache:\n",
        "    # sample a few files to check label ranges\n",
        "    samp = random.sample(train_cache, min(5, len(train_cache)))\n",
        "    mins, maxs = [], []\n",
        "    for fn in samp:\n",
        "        zt = np.load(fn)\n",
        "        if 'y' in zt: y = zt['y']\n",
        "        elif 'labels' in zt: y = zt['labels']\n",
        "        else: continue\n",
        "        if y.ndim != 1: continue\n",
        "        mins.append(int(y.min())); maxs.append(int(y.max()))\n",
        "    if mins:\n",
        "        tr_y_min, tr_y_max = min(mins), max(maxs)\n",
        "        print('Train cache y range across sample: min=', tr_y_min, 'max=', tr_y_max)\n",
        "else:\n",
        "    print('[Warn] No cache_v16 training npz found to cross-check labels')\n",
        "\n",
        "# Persist inferred bg index for downstream decoders to consume\n",
        "with open('bg_index.json', 'w') as f:\n",
        "    json.dump({'bg_idx': bg_idx, 'C': int(C), 'source': 'oof_probs_v16', 'train_y_min': tr_y_min, 'train_y_max': tr_y_max}, f)\n",
        "print('Saved bg_index.json with bg_idx=', bg_idx)\n",
        "\n",
        "print('\\n=== submission.csv sanity ===')\n",
        "assert os.path.exists('submission.csv'), 'submission.csv missing'\n",
        "s = pd.read_csv('submission.csv')\n",
        "print('submission shape:', s.shape)\n",
        "head_checks = [ (len(x.split()), len(set(x.split()))) for x in s['Sequence'].head(3) ]\n",
        "print('first 3 rows (len, unique):', head_checks)\n",
        "assert s.shape[0]==95 and set(s.columns)=={'Id','Sequence'}, 'submission.csv shape/columns invalid'\n",
        "print('OK: submission.csv looks well-formed')\n",
        "\n",
        "print('\\n=== Bag-of-classes quick baseline check (no write) ===')\n",
        "test_ids = pd.read_csv('test.csv')['Id'].astype(int).tolist()\n",
        "test_prob_dir = 'test_probs_v16'\n",
        "def load_probs_dir_simple(d, ids):\n",
        "    out = {}\n",
        "    import os\n",
        "    for vid in ids:\n",
        "        for pat in (f'{vid}.npz', f'{vid:05d}.npz', f'test_{vid:05d}.npz'):\n",
        "            fn = os.path.join(d, pat)\n",
        "            if os.path.exists(fn):\n",
        "                z2 = np.load(fn, allow_pickle=True)\n",
        "                if 'probs' in z2: out[vid] = z2['probs'].astype(np.float32)\n",
        "                elif 'P' in z2: out[vid] = z2['P'].astype(np.float32)\n",
        "                break\n",
        "    return out\n",
        "mp = load_probs_dir_simple(test_prob_dir, test_ids)\n",
        "if len(mp)==95:\n",
        "    vid0 = sorted(mp.keys())[0]\n",
        "    P0 = mp[vid0]\n",
        "    # Use inferred bg_idx when ignoring background\n",
        "    cols = [i for i in range(C) if i != bg_idx]\n",
        "    score = P0[:, cols].sum(0)\n",
        "    order = np.argsort(-score) + 1  # class ids assumed 1..20 mapped in same order as columns excluding bg\n",
        "    print('Bag-of-classes sample video', vid0, 'bg_idx', bg_idx, 'top5 (1-based class ids approximation):', order[:5].tolist())\n",
        "else:\n",
        "    print('Note: test_probs_v16 missing for some vids:', len(mp))\n",
        "\n",
        "print('\\nNOTE: bg_idx != 0 indicates a class mapping shift. Downstream decoders should use bg_idx from bg_index.json instead of assuming 0.')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== OOF/class mapping sanity ===\noof files count= 297 showing first 3\n sample 0 keys= ['probs', 'seq', 'sid'] shape= (627, 21)\n sample 1 keys= ['probs', 'seq', 'sid'] shape= (559, 21)\n sample 2 keys= ['probs', 'seq', 'sid'] shape= (668, 21)\nC (num classes)= 21\nPer-class mean (first 10)= [0.25760599970817566, 0.036462001502513885, 0.03442699834704399, 0.037664998322725296, 0.03947199881076813, 0.03866399824619293, 0.03352800011634827, 0.036205001175403595, 0.0348379984498024, 0.04036400094628334] ...\nInferred bg_idx = 0 (0-based). m_all[bg]= 0.25760623812675476\nTrain cache y range across sample: min= 0 max= 20\nSaved bg_index.json with bg_idx= 0\n\n=== submission.csv sanity ===\nsubmission shape: (95, 2)\nfirst 3 rows (len, unique): [(20, 20), (20, 20), (20, 20)]\nOK: submission.csv looks well-formed\n\n=== Bag-of-classes quick baseline check (no write) ===\nBag-of-classes sample video 300 bg_idx 0 top5 (1-based class ids approximation): [18, 11, 12, 15, 20]\n\nNOTE: bg_idx != 0 indicates a class mapping shift. Downstream decoders should use bg_idx from bg_index.json instead of assuming 0.\n"
          ]
        }
      ]
    },
    {
      "id": "cba129ca-1730-45c5-a5f0-407e0bc607af",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build labeled OOF for v16 with 5-fold GroupKFold (per-video), save per-video probs+y\n",
        "import os, time, glob, numpy as np, pandas as pd, xgboost as xgb\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "out_dir = 'oof_probs_v16_labeled'\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "train_meta = pd.read_csv('training.csv')\n",
        "all_vids = train_meta['Id'].astype(int).tolist()\n",
        "\n",
        "def load_train_v16(vid):\n",
        "    fn = os.path.join('cache_v16', f'train_{vid:05d}.npz')\n",
        "    if not os.path.exists(fn):\n",
        "        return None, None\n",
        "    z = np.load(fn, allow_pickle=False)\n",
        "    X = z['X'].astype(np.float32)\n",
        "    y = z['y'].astype(np.int32) if 'y' in z else (z['labels'].astype(np.int32) if 'labels' in z else None)\n",
        "    return X, y\n",
        "\n",
        "# Collect available videos (with labels) and their lengths\n",
        "vids, Xs, Ys, lengths = [], [], [], []\n",
        "t0 = time.time()\n",
        "for i, vid in enumerate(all_vids, 1):\n",
        "    Xv, yv = load_train_v16(vid)\n",
        "    if Xv is None or yv is None or len(yv) != len(Xv):\n",
        "        continue\n",
        "    vids.append(int(vid)); Xs.append(Xv); Ys.append(yv); lengths.append(len(yv))\n",
        "    if i % 25 == 0:\n",
        "        print(f'[Load] {i}/{len(all_vids)} vids scanned; kept={len(vids)}', flush=True)\n",
        "print(f'[Load] Done. kept vids={len(vids)} elapsed={time.time()-t0:.1f}s')\n",
        "assert vids, 'No v16 training videos with labels found in cache_v16'\n",
        "\n",
        "# Prepare GroupKFold over videos\n",
        "n_splits = 5\n",
        "gkf = GroupKFold(n_splits=n_splits)\n",
        "vids_arr = np.array(vids, dtype=np.int32)\n",
        "groups = vids_arr.copy()  # group by video id\n",
        "\n",
        "params = {\n",
        "    'objective': 'multi:softprob',\n",
        "    'num_class': 21,\n",
        "    'tree_method': 'hist',\n",
        "    'device': 'cuda',\n",
        "    'max_bin': 256,\n",
        "    'max_depth': 8,\n",
        "    'eta': 0.05,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'min_child_weight': 3,\n",
        "    'gamma': 0.0,\n",
        "    'reg_lambda': 1.0,\n",
        "    'reg_alpha': 0.0,\n",
        "    'eval_metric': 'mlogloss'\n",
        "}\n",
        "print('[XGB] Params:', params)\n",
        "\n",
        "def concat_by_indices(idxs):\n",
        "    X_list, y_list = [], []\n",
        "    for j in idxs:\n",
        "        X_list.append(Xs[j]); y_list.append(Ys[j])\n",
        "    Xc = np.concatenate(X_list, 0); yc = np.concatenate(y_list, 0)\n",
        "    return Xc, yc\n",
        "\n",
        "t_all = time.time()\n",
        "fold_times = []\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(vids_arr, groups=groups), 1):\n",
        "    t_fold = time.time()\n",
        "    tr_vids = vids_arr[tr_idx].tolist(); va_vids = vids_arr[va_idx].tolist()\n",
        "    print(f'\\n[Fold {fold}/{n_splits}] tr_videos={len(tr_vids)} va_videos={len(va_vids)}')\n",
        "    X_tr, y_tr = concat_by_indices(tr_idx)\n",
        "    X_va, y_va = concat_by_indices(va_idx)\n",
        "    print(f'[Fold {fold}] X_tr={X_tr.shape} y_tr={y_tr.shape} | X_va={X_va.shape} y_va={y_va.shape}', flush=True)\n",
        "\n",
        "    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
        "    dvalid = xgb.DMatrix(X_va, label=y_va)\n",
        "    evals = [(dtrain, 'train'), (dvalid, 'valid')]\n",
        "    bst = xgb.train(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=1500,\n",
        "        evals=evals,\n",
        "        early_stopping_rounds=80,\n",
        "        verbose_eval=50\n",
        "    )\n",
        "    best_iter = getattr(bst, 'best_iteration', None)\n",
        "    print(f'[Fold {fold}] train done; best_iter={best_iter} best_score={getattr(bst, \"best_score\", float(\"nan\"))}')\n",
        "\n",
        "    # Predict and save for each validation video separately\n",
        "    # Build index ranges to slice back per video\n",
        "    # We iterate va_idx order and predict per video directly for memory clarity\n",
        "    saved = 0\n",
        "    for j_idx in va_idx:\n",
        "        vid = int(vids[j_idx])\n",
        "        Xv = Xs[j_idx]; yv = Ys[j_idx]\n",
        "        dm = xgb.DMatrix(Xv)\n",
        "        if best_iter is not None:\n",
        "            P = bst.predict(dm, iteration_range=(0, best_iter+1))\n",
        "        else:\n",
        "            P = bst.predict(dm)\n",
        "        P = P.astype(np.float32)\n",
        "        # Normalize rows just in case\n",
        "        P /= np.clip(P.sum(1, keepdims=True), 1e-8, None)\n",
        "        out_fn = os.path.join(out_dir, f'oof_{vid:05d}.npz')\n",
        "        np.savez_compressed(out_fn, probs=P, y=yv.astype(np.int32), sid=np.array([vid], dtype=np.int32))\n",
        "        saved += 1\n",
        "        if saved % 10 == 0:\n",
        "            print(f'[Fold {fold}] saved {saved}/{len(va_idx)}', flush=True)\n",
        "    elapsed_fold = time.time() - t_fold\n",
        "    fold_times.append(elapsed_fold)\n",
        "    print(f'[Fold {fold}] done. Saved {saved} videos. Elapsed {elapsed_fold:.1f}s', flush=True)\n",
        "\n",
        "print(f'\\n[OOF] Completed {n_splits}-fold OOF build into {out_dir}. Total elapsed {time.time()-t_all:.1f}s. Per-fold times: {[round(x,1) for x in fold_times]}')\n",
        "\n",
        "# Quick verify: count files and inspect one\n",
        "files = sorted(glob.glob(os.path.join(out_dir, 'oof_*.npz')))\n",
        "print('[OOF] files:', len(files))\n",
        "if files:\n",
        "    z = np.load(files[0])\n",
        "    print('[OOF] sample keys:', list(z.files), 'shape:', z['probs'].shape, 'y:', z['y'].shape, 'sid:', z['sid'])\n",
        "else:\n",
        "    print('[OOF] WARNING: no files written!')\n",
        "\n",
        "print('[Next] Fit per-class temperatures on oof_probs_v16_labeled and rerun v16-only S4/S5 decoders as per expert settings.')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Load] 25/297 vids scanned; kept=25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Load] 50/297 vids scanned; kept=50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Load] 75/297 vids scanned; kept=75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Load] 100/297 vids scanned; kept=100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Load] 125/297 vids scanned; kept=125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Load] 150/297 vids scanned; kept=150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Load] 175/297 vids scanned; kept=175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Load] 200/297 vids scanned; kept=200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Load] 225/297 vids scanned; kept=225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Load] 250/297 vids scanned; kept=250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Load] 275/297 vids scanned; kept=275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Load] Done. kept vids=297 elapsed=0.7s\n[XGB] Params: {'objective': 'multi:softprob', 'num_class': 21, 'tree_method': 'hist', 'device': 'cuda', 'max_bin': 256, 'max_depth': 8, 'eta': 0.05, 'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 3, 'gamma': 0.0, 'reg_lambda': 1.0, 'reg_alpha': 0.0, 'eval_metric': 'mlogloss'}\n\n[Fold 1/5] tr_videos=237 va_videos=60\n[Fold 1] X_tr=(149902, 193) y_tr=(149902,) | X_va=(37394, 193) y_va=(37394,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-mlogloss:2.85741\tvalid-mlogloss:2.88900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\ttrain-mlogloss:1.21769\tvalid-mlogloss:1.65645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain-mlogloss:0.81797\tvalid-mlogloss:1.47703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\ttrain-mlogloss:0.59806\tvalid-mlogloss:1.41203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mlogloss:0.45191\tvalid-mlogloss:1.37899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\ttrain-mlogloss:0.34715\tvalid-mlogloss:1.35705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain-mlogloss:0.27510\tvalid-mlogloss:1.34505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[350]\ttrain-mlogloss:0.22160\tvalid-mlogloss:1.33679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mlogloss:0.18336\tvalid-mlogloss:1.33380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\ttrain-mlogloss:0.15557\tvalid-mlogloss:1.33186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\ttrain-mlogloss:0.13548\tvalid-mlogloss:1.33262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[547]\ttrain-mlogloss:0.12149\tvalid-mlogloss:1.33434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] train done; best_iter=467 best_score=1.331653626302092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] saved 10/60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] saved 20/60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] saved 30/60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] saved 40/60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] saved 50/60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] saved 60/60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] done. Saved 60 videos. Elapsed 135.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[Fold 2/5] tr_videos=237 va_videos=60\n[Fold 2] X_tr=(148508, 193) y_tr=(148508,) | X_va=(38788, 193) y_va=(38788,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-mlogloss:2.85956\tvalid-mlogloss:2.87773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\ttrain-mlogloss:1.22682\tvalid-mlogloss:1.60540\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain-mlogloss:0.82533\tvalid-mlogloss:1.42024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\ttrain-mlogloss:0.60027\tvalid-mlogloss:1.35297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mlogloss:0.44923\tvalid-mlogloss:1.31585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\ttrain-mlogloss:0.34223\tvalid-mlogloss:1.29214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain-mlogloss:0.26686\tvalid-mlogloss:1.27722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[350]\ttrain-mlogloss:0.21322\tvalid-mlogloss:1.26650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mlogloss:0.17342\tvalid-mlogloss:1.25869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\ttrain-mlogloss:0.14480\tvalid-mlogloss:1.25373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\ttrain-mlogloss:0.12388\tvalid-mlogloss:1.25099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[550]\ttrain-mlogloss:0.10816\tvalid-mlogloss:1.25086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain-mlogloss:0.09678\tvalid-mlogloss:1.25096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[650]\ttrain-mlogloss:0.08820\tvalid-mlogloss:1.25264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[674]\ttrain-mlogloss:0.08483\tvalid-mlogloss:1.25340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] train done; best_iter=594 best_score=1.2505774837550496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] saved 10/60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] saved 20/60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] saved 30/60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] saved 40/60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] saved 50/60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] saved 60/60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] done. Saved 60 videos. Elapsed 156.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[Fold 3/5] tr_videos=238 va_videos=59\n[Fold 3] X_tr=(150043, 193) y_tr=(150043,) | X_va=(37253, 193) y_va=(37253,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-mlogloss:2.85825\tvalid-mlogloss:2.88202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\ttrain-mlogloss:1.22041\tvalid-mlogloss:1.64381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain-mlogloss:0.82810\tvalid-mlogloss:1.47689\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\ttrain-mlogloss:0.60895\tvalid-mlogloss:1.41730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mlogloss:0.45721\tvalid-mlogloss:1.38642\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\ttrain-mlogloss:0.34996\tvalid-mlogloss:1.36782\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain-mlogloss:0.27444\tvalid-mlogloss:1.35749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[350]\ttrain-mlogloss:0.22066\tvalid-mlogloss:1.35054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mlogloss:0.18120\tvalid-mlogloss:1.34762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\ttrain-mlogloss:0.15296\tvalid-mlogloss:1.34827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[485]\ttrain-mlogloss:0.13759\tvalid-mlogloss:1.34909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 3] train done; best_iter=405 best_score=1.3472665489331255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 3] saved 10/59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 3] saved 20/59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 3] saved 30/59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 3] saved 40/59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 3] saved 50/59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 3] done. Saved 59 videos. Elapsed 116.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[Fold 4/5] tr_videos=238 va_videos=59\n[Fold 4] X_tr=(150274, 193) y_tr=(150274,) | X_va=(37022, 193) y_va=(37022,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-mlogloss:2.85944\tvalid-mlogloss:2.88222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\ttrain-mlogloss:1.23094\tvalid-mlogloss:1.60453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain-mlogloss:0.83566\tvalid-mlogloss:1.42075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\ttrain-mlogloss:0.61479\tvalid-mlogloss:1.35314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mlogloss:0.46312\tvalid-mlogloss:1.31893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\ttrain-mlogloss:0.35823\tvalid-mlogloss:1.29781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain-mlogloss:0.28332\tvalid-mlogloss:1.28553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[350]\ttrain-mlogloss:0.22829\tvalid-mlogloss:1.27762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mlogloss:0.18955\tvalid-mlogloss:1.27526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\ttrain-mlogloss:0.16093\tvalid-mlogloss:1.27542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\ttrain-mlogloss:0.13974\tvalid-mlogloss:1.27514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[550]\ttrain-mlogloss:0.12428\tvalid-mlogloss:1.27813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[562]\ttrain-mlogloss:0.12127\tvalid-mlogloss:1.27929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 4] train done; best_iter=482 best_score=1.2746830797070472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 4] saved 10/59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 4] saved 20/59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 4] saved 30/59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 4] saved 40/59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 4] saved 50/59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 4] done. Saved 59 videos. Elapsed 138.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[Fold 5/5] tr_videos=238 va_videos=59\n[Fold 5] X_tr=(150457, 193) y_tr=(150457,) | X_va=(36839, 193) y_va=(36839,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-mlogloss:2.85762\tvalid-mlogloss:2.88318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\ttrain-mlogloss:1.22375\tvalid-mlogloss:1.63633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain-mlogloss:0.82693\tvalid-mlogloss:1.45864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\ttrain-mlogloss:0.60572\tvalid-mlogloss:1.39475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mlogloss:0.45671\tvalid-mlogloss:1.35973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\ttrain-mlogloss:0.35054\tvalid-mlogloss:1.33689\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain-mlogloss:0.27491\tvalid-mlogloss:1.32293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[350]\ttrain-mlogloss:0.21991\tvalid-mlogloss:1.31410\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mlogloss:0.18076\tvalid-mlogloss:1.30834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\ttrain-mlogloss:0.15193\tvalid-mlogloss:1.30558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\ttrain-mlogloss:0.13116\tvalid-mlogloss:1.30597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[542]\ttrain-mlogloss:0.11788\tvalid-mlogloss:1.30666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 5] train done; best_iter=462 best_score=1.3053215063743122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 5] saved 10/59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 5] saved 20/59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 5] saved 30/59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 5] saved 40/59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 5] saved 50/59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 5] done. Saved 59 videos. Elapsed 129.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[OOF] Completed 5-fold OOF build into oof_probs_v16_labeled. Total elapsed 677.2s. Per-fold times: [135.3, 156.8, 116.6, 138.5, 129.9]\n[OOF] files: 297\n[OOF] sample keys: ['probs', 'y', 'sid'] shape: (627, 21) y: (627,) sid: [1]\n[Next] Fit per-class temperatures on oof_probs_v16_labeled and rerun v16-only S4/S5 decoders as per expert settings.\n"
          ]
        }
      ]
    },
    {
      "id": "25740933-ebc2-4fee-a323-18ffe108c7b4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fit per-class temperatures from labeled OOF (v16) and run targeted v16-only decoders\n",
        "import os, glob, numpy as np, pandas as pd\n",
        "from scipy.special import logit\n",
        "from scipy.signal import convolve\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "def load_oof_frames_labeled(oof_dirs):\n",
        "    X, Y = [], []\n",
        "    for d in oof_dirs:\n",
        "        for fn in glob.glob(os.path.join(d, '*.npz')):\n",
        "            z = np.load(fn, allow_pickle=True)\n",
        "            if 'probs' in z: P = z['probs'].astype(np.float32)\n",
        "            elif 'P' in z: P = z['P'].astype(np.float32)\n",
        "            else: continue\n",
        "            if 'y' in z: y = z['y'].astype(np.int32)\n",
        "            elif 'labels' in z: y = z['labels'].astype(np.int32)\n",
        "            else: continue\n",
        "            if P.ndim == 2 and len(y) == P.shape[0]:\n",
        "                X.append(P); Y.append(y)\n",
        "    if not X: return None, None\n",
        "    return np.concatenate(X, 0), np.concatenate(Y, 0)\n",
        "\n",
        "def fit_per_class_temperature(P_oof, y_oof, temp_grid):\n",
        "    if P_oof is None or y_oof is None:\n",
        "        return np.ones(21, dtype=np.float32)\n",
        "    C = P_oof.shape[1]\n",
        "    Tcls = np.ones(C, dtype=np.float32)\n",
        "    y = y_oof\n",
        "    for c in range(C):\n",
        "        pc = np.clip(P_oof[:, c], 1e-6, 1-1e-6); z = logit(pc)\n",
        "        yc = (y == c).astype(np.float32)\n",
        "        best, bestT = 1e18, 1.0\n",
        "        for T in temp_grid:\n",
        "            p = 1.0/(1.0 + np.exp(-z/float(T)))\n",
        "            nll = -(yc*np.log(np.clip(p,1e-6,1))).mean() - ((1-yc)*np.log(np.clip(1-p,1e-6,1))).mean()\n",
        "            if nll < best: best, bestT = float(nll), float(T)\n",
        "        Tcls[c] = bestT\n",
        "    return Tcls.astype(np.float32)\n",
        "\n",
        "def to_calibrated_logits(P, Tcls, bg_bias, smooth_win, g=1.0):\n",
        "    P = np.clip(P, 1e-6, 1-1e-6).astype(np.float32)\n",
        "    Z = logit(P).astype(np.float32)\n",
        "    Z = Z / (Tcls.reshape(1, -1) * float(g))\n",
        "    Z[:, 0] += float(bg_bias)\n",
        "    if smooth_win and smooth_win > 1:\n",
        "        k = np.ones(int(smooth_win), np.float32) / float(smooth_win)\n",
        "        Z = np.stack([convolve(Z[:, i], k, mode='same') for i in range(Z.shape[1])], 1).astype(np.float32)\n",
        "    return Z\n",
        "\n",
        "def segment_exact20(margin, K, min_len):\n",
        "    T = int(len(margin))\n",
        "    min_len = int(max(1, min_len))\n",
        "    while K*min_len > T and min_len > 1:\n",
        "        min_len -= 1\n",
        "    if K*min_len > T:\n",
        "        K = min(K, T); min_len = 1\n",
        "    pref = np.concatenate([[0.0], np.cumsum(margin, 0).astype(np.float32)])\n",
        "    dp = -1e18*np.ones((K+1, T+1), np.float32)\n",
        "    bt = -np.ones((K+1, T+1), np.int32)\n",
        "    dp[0, 0] = 0.0\n",
        "    for k in range(1, K+1):\n",
        "        start_min = (k-1)*min_len\n",
        "        for t in range(k*min_len, T+1):\n",
        "            s_lo = max(start_min, t - (T - (K-k)*min_len))\n",
        "            bestv, bests = -1e18, -1\n",
        "            for s in range(s_lo, t-min_len+1):\n",
        "                v = dp[k-1, s] + (pref[t] - pref[s])\n",
        "                if v > bestv:\n",
        "                    bestv, bests = v, s\n",
        "            dp[k, t] = bestv; bt[k, t] = bests\n",
        "    bounds = []\n",
        "    k, t = K, T\n",
        "    while k > 0:\n",
        "        s = int(bt[k, t]); bounds.append((s, t)); t = s; k -= 1\n",
        "    bounds.reverse()\n",
        "    return bounds\n",
        "\n",
        "def decode_with_cost(Z, alpha, cost_mode='median'):\n",
        "    T = Z.shape[0]\n",
        "    min_len = max(2, int(alpha * T / 20.0))\n",
        "    margin = (Z[:, 1:21].max(1) - Z[:, 0]).astype(np.float32)\n",
        "    segs = segment_exact20(margin, 20, min_len)\n",
        "    C = np.zeros((len(segs), 20), np.float32)\n",
        "    for j, (s, e) in enumerate(segs):\n",
        "        seg = Z[s:e, 1:21]\n",
        "        if cost_mode == 'median':\n",
        "            agg = np.median(seg, axis=0)\n",
        "        elif cost_mode == 'trimmed_mean_20':\n",
        "            L = seg.shape[0]; lo = int(0.1*L); hi = max(lo+1, int(0.9*L))\n",
        "            agg = np.sort(seg, axis=0)[lo:hi].mean(0)\n",
        "        else:\n",
        "            agg = seg.mean(0)\n",
        "        C[j, :] = -agg\n",
        "    r, c = linear_sum_assignment(C)\n",
        "    order = [int(c[j])+1 for j in np.argsort(r)]\n",
        "    return order\n",
        "\n",
        "def load_probs_dir_simple(d, ids):\n",
        "    out = {}\n",
        "    for vid in ids:\n",
        "        for pat in (f'{vid}.npz', f'{vid:05d}.npz', f'test_{vid:05d}.npz'):\n",
        "            fn = os.path.join(d, pat)\n",
        "            if os.path.exists(fn):\n",
        "                z = np.load(fn, allow_pickle=True)\n",
        "                if 'probs' in z: out[vid] = z['probs'].astype(np.float32)\n",
        "                elif 'P' in z: out[vid] = z['P'].astype(np.float32)\n",
        "                break\n",
        "    return out\n",
        "\n",
        "# 1) Fit Tcls from labeled OOF\n",
        "P_oof, y_oof = load_oof_frames_labeled(['oof_probs_v16_labeled'])\n",
        "assert P_oof is not None and y_oof is not None, 'Labeled OOF not found'\n",
        "temp_grid = np.linspace(0.7, 1.6, 12).astype(np.float32)\n",
        "Tcls16 = fit_per_class_temperature(P_oof, y_oof, temp_grid)\n",
        "print('[Calib] Tcls16 (first 10):', np.round(Tcls16[:10], 3).tolist())\n",
        "\n",
        "# 2) Load test probs (v16 only)\n",
        "test_ids = pd.read_csv('test.csv')['Id'].astype(int).tolist()\n",
        "mp = load_probs_dir_simple('test_probs_v16', test_ids)\n",
        "print('[Test v16] videos:', len(mp))\n",
        "\n",
        "def run_variant(name, alpha, bg_bias, smooth, cost_mode='median', g=0.95):\n",
        "    rows = []\n",
        "    for vid in sorted(mp.keys()):\n",
        "        P = mp[vid]\n",
        "        Z = to_calibrated_logits(P, Tcls16, bg_bias, smooth, g=g)\n",
        "        seq = decode_with_cost(Z, alpha, cost_mode=cost_mode)\n",
        "        rows.append((vid, ' '.join(map(str, seq))))\n",
        "    sub = pd.DataFrame(rows, columns=['Id','Sequence']).sort_values('Id')\n",
        "    out = f'submission_v16only_{name}.csv'\n",
        "    sub.to_csv(out, index=False)\n",
        "    print('[Write]', out, sub.shape)\n",
        "    return out\n",
        "\n",
        "# 3) Minimal targeted runs per expert:\n",
        "# S4 median: g in {0.95, 0.90}, alpha=0.85, bg_bias=0.25, smooth=3\n",
        "out_s4_g095 = run_variant('S4_median_a0.85_bb0.25_s3_g0.95', alpha=0.85, bg_bias=0.25, smooth=3, cost_mode='median', g=0.95)\n",
        "out_s4_g090 = run_variant('S4_median_a0.85_bb0.25_s3_g0.90', alpha=0.85, bg_bias=0.25, smooth=3, cost_mode='median', g=0.90)\n",
        "# S5 trimmed mean: g=0.95, alpha=0.85, bg_bias=0.20, smooth=3\n",
        "out_s5 = run_variant('S5_trim_a0.85_bb0.20_s3_g0.95', alpha=0.85, bg_bias=0.20, smooth=3, cost_mode='trimmed_mean_20', g=0.95)\n",
        "# Optional: S4 median smooth=1, g=0.95\n",
        "out_s4_s1 = run_variant('S4_median_a0.85_bb0.25_s1_g0.95', alpha=0.85, bg_bias=0.25, smooth=1, cost_mode='median', g=0.95)\n",
        "\n",
        "# 4) Set primary submission.csv to S4 g=0.95 (median, smooth=3) as first pick\n",
        "primary = out_s4_g095\n",
        "pd.read_csv(primary).to_csv('submission.csv', index=False)\n",
        "print('[Primary] submission.csv <-', primary)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Calib] Tcls16 (first 10): [1.190999984741211, 1.0269999504089355, 1.1089999675750732, 1.0269999504089355, 1.0269999504089355, 1.0269999504089355, 1.1089999675750732, 1.1089999675750732, 1.0269999504089355, 1.1089999675750732]\n[Test v16] videos: 95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Write] submission_v16only_S4_median_a0.85_bb0.25_s3_g0.95.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Write] submission_v16only_S4_median_a0.85_bb0.25_s3_g0.90.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Write] submission_v16only_S5_trim_a0.85_bb0.20_s3_g0.95.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Write] submission_v16only_S4_median_a0.85_bb0.25_s1_g0.95.csv (95, 2)\n[Primary] submission.csv <- submission_v16only_S4_median_a0.85_bb0.25_s3_g0.95.csv\n"
          ]
        }
      ]
    },
    {
      "id": "2fcdabfd-265d-4f7c-876e-b6efdd7721be",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute OOF Levenshtein (v16-only) using labeled OOF to gate submissions\n",
        "import os, glob, numpy as np, pandas as pd\n",
        "from scipy.special import logit\n",
        "from scipy.signal import convolve\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "def load_oof_per_video(oof_dir):\n",
        "    vids, P_map = [], {}\n",
        "    files = sorted(glob.glob(os.path.join(oof_dir, 'oof_*.npz')))\n",
        "    for fn in files:\n",
        "        z = np.load(fn, allow_pickle=True)\n",
        "        sid = int(z['sid'][0]) if 'sid' in z else int(os.path.basename(fn).split('_')[1].split('.')[0])\n",
        "        if 'probs' in z: P = z['probs'].astype(np.float32)\n",
        "        elif 'P' in z: P = z['P'].astype(np.float32)\n",
        "        else: continue\n",
        "        P_map[sid] = P\n",
        "        vids.append(sid)\n",
        "    return sorted(vids), P_map\n",
        "\n",
        "def fit_per_class_temperature(P_oof, y_oof, temp_grid):\n",
        "    C = P_oof.shape[1]; Tcls = np.ones(C, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        pc = np.clip(P_oof[:, c], 1e-6, 1-1e-6); z = logit(pc)\n",
        "        yc = (y_oof == c).astype(np.float32)\n",
        "        best, bestT = 1e18, 1.0\n",
        "        for T in temp_grid:\n",
        "            p = 1.0/(1.0 + np.exp(-z/float(T)))\n",
        "            nll = -(yc*np.log(np.clip(p,1e-6,1))).mean() - ((1-yc)*np.log(np.clip(1-p,1e-6,1))).mean()\n",
        "            if nll < best: best, bestT = float(nll), float(T)\n",
        "        Tcls[c] = bestT\n",
        "    return Tcls.astype(np.float32)\n",
        "\n",
        "def to_calibrated_logits(P, Tcls, bg_bias, smooth_win, g=0.95):\n",
        "    P = np.clip(P, 1e-6, 1-1e-6).astype(np.float32)\n",
        "    Z = logit(P).astype(np.float32)\n",
        "    Z = Z / (Tcls.reshape(1, -1) * float(g))\n",
        "    Z[:, 0] += float(bg_bias)\n",
        "    if smooth_win and smooth_win > 1:\n",
        "        k = np.ones(int(smooth_win), np.float32) / float(smooth_win)\n",
        "        Z = np.stack([convolve(Z[:, i], k, mode='same') for i in range(Z.shape[1])], 1).astype(np.float32)\n",
        "    return Z\n",
        "\n",
        "def segment_exact20(margin, K, min_len):\n",
        "    T = int(len(margin)); min_len = int(max(1, min_len))\n",
        "    while K*min_len > T and min_len > 1: min_len -= 1\n",
        "    if K*min_len > T: K = min(K, T); min_len = 1\n",
        "    pref = np.concatenate([[0.0], np.cumsum(margin, 0).astype(np.float32)])\n",
        "    dp = -1e18*np.ones((K+1, T+1), np.float32); bt = -np.ones((K+1, T+1), np.int32)\n",
        "    dp[0, 0] = 0.0\n",
        "    for k in range(1, K+1):\n",
        "        start_min = (k-1)*min_len\n",
        "        for t in range(k*min_len, T+1):\n",
        "            s_lo = max(start_min, t - (T - (K-k)*min_len))\n",
        "            bestv, bests = -1e18, -1\n",
        "            for s in range(s_lo, t-min_len+1):\n",
        "                v = dp[k-1, s] + (pref[t] - pref[s])\n",
        "                if v > bestv: bestv, bests = v, s\n",
        "            dp[k, t] = bestv; bt[k, t] = bests\n",
        "    bounds = []; k, t = K, T\n",
        "    while k > 0:\n",
        "        s = int(bt[k, t]); bounds.append((s, t)); t = s; k -= 1\n",
        "    bounds.reverse(); return bounds\n",
        "\n",
        "def decode_with_cost(Z, alpha, cost_mode='median'):\n",
        "    T = Z.shape[0]\n",
        "    min_len = max(2, int(alpha * T / 20.0))\n",
        "    margin = (Z[:, 1:21].max(1) - Z[:, 0]).astype(np.float32)\n",
        "    segs = segment_exact20(margin, 20, min_len)\n",
        "    C = np.zeros((len(segs), 20), np.float32)\n",
        "    for j, (s, e) in enumerate(segs):\n",
        "        seg = Z[s:e, 1:21]\n",
        "        if cost_mode == 'median':\n",
        "            agg = np.median(seg, axis=0)\n",
        "        elif cost_mode == 'trimmed_mean_20':\n",
        "            L = seg.shape[0]; lo = int(0.1*L); hi = max(lo+1, int(0.9*L))\n",
        "            agg = np.sort(seg, axis=0)[lo:hi].mean(0)\n",
        "        else:\n",
        "            agg = seg.mean(0)\n",
        "        C[j, :] = -agg\n",
        "    r, c = linear_sum_assignment(C)\n",
        "    order = [int(c[j])+1 for j in np.argsort(r)]\n",
        "    return order\n",
        "\n",
        "def levenshtein(a, b):\n",
        "    # a, b are lists of ints\n",
        "    n, m = len(a), len(b)\n",
        "    dp = list(range(m+1))\n",
        "    for i in range(1, n+1):\n",
        "        prev, dp[0] = dp[0], i\n",
        "        for j in range(1, m+1):\n",
        "            cur = min(dp[j] + 1, dp[j-1] + 1, prev + (0 if a[i-1]==b[j-1] else 1))\n",
        "            prev, dp[j] = dp[j], cur\n",
        "    return dp[m]\n",
        "\n",
        "# Load labeled OOF frames for calibration and per-video probs for decoding\n",
        "vids, Pmap = load_oof_per_video('oof_probs_v16_labeled')\n",
        "assert len(vids) > 0 and len(Pmap) == len(vids), 'OOF per-video probs missing'\n",
        "train_df = pd.read_csv('training.csv')[['Id','Sequence']].astype({'Id': int})\n",
        "seq_gt = {int(r.Id): [int(x) for x in str(r.Sequence).split()] for r in train_df.itertuples(index=False)}\n",
        "\n",
        "# Fit temperatures from all OOF frames\n",
        "X_all, y_all = [], []\n",
        "for fn in glob.glob(os.path.join('oof_probs_v16_labeled', 'oof_*.npz')):\n",
        "    z = np.load(fn, allow_pickle=True)\n",
        "    if 'probs' in z and 'y' in z:\n",
        "        X_all.append(z['probs'].astype(np.float32))\n",
        "        y_all.append(z['y'].astype(np.int32))\n",
        "P_oof = np.concatenate(X_all, 0); y_oof = np.concatenate(y_all, 0)\n",
        "Tcls16 = fit_per_class_temperature(P_oof, y_oof, np.linspace(0.7, 1.6, 12).astype(np.float32))\n",
        "print('[OOF-Calib] Tcls16 first10:', np.round(Tcls16[:10], 3).tolist())\n",
        "\n",
        "def eval_cfg(alpha, bg_bias, smooth, g, cost_mode='median', tag=''):\n",
        "    dists = []\n",
        "    for vid in vids:\n",
        "        P = Pmap.get(int(vid))\n",
        "        if P is None: continue\n",
        "        Z = to_calibrated_logits(P, Tcls16, bg_bias, smooth, g=g)\n",
        "        pred = decode_with_cost(Z, alpha, cost_mode=cost_mode)\n",
        "        gt = seq_gt.get(int(vid), [])\n",
        "        if len(gt) != 20 or len(pred) != 20: continue\n",
        "        d = levenshtein(pred, gt) / 20.0\n",
        "        dists.append(d)\n",
        "    score = float(np.mean(dists)) if dists else 1.0\n",
        "    print(f'[OOF-Lev] {tag} -> {score:.5f} over {len(dists)} vids')\n",
        "    return score\n",
        "\n",
        "# Evaluate expert configs\n",
        "s1 = eval_cfg(alpha=0.85, bg_bias=0.25, smooth=3, g=0.95, cost_mode='median', tag='S4 median g=0.95')\n",
        "s2 = eval_cfg(alpha=0.85, bg_bias=0.25, smooth=3, g=0.90, cost_mode='median', tag='S4 median g=0.90')\n",
        "s3 = eval_cfg(alpha=0.85, bg_bias=0.20, smooth=3, g=0.95, cost_mode='trimmed_mean_20', tag='S5 trim g=0.95')\n",
        "best = min(s1, s2, s3)\n",
        "print('[OOF-Lev] Best=', best)\n",
        "\n",
        "# If S4 g=0.90 is best and the corresponding test submission exists from v16-only runs, swap it in\n",
        "cand = None\n",
        "if s2 <= min(s1, s3):\n",
        "    cand = 'submission_v16only_S4_median_a0.85_bb0.25_s3_g0.90.csv'\n",
        "elif s1 <= min(s2, s3):\n",
        "    cand = 'submission_v16only_S4_median_a0.85_bb0.25_s3_g0.95.csv'\n",
        "else:\n",
        "    cand = 'submission_v16only_S5_trim_a0.85_bb0.20_s3_g0.95.csv'\n",
        "if os.path.exists(cand):\n",
        "    pd.read_csv(cand).to_csv('submission.csv', index=False)\n",
        "    print('[Submission swap by OOF] submission.csv <-', cand)\n",
        "else:\n",
        "    print('[Submission swap] candidate missing, keeping current submission.csv')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Calib] Tcls16 first10: [1.190999984741211, 1.0269999504089355, 1.1089999675750732, 1.0269999504089355, 1.0269999504089355, 1.0269999504089355, 1.1089999675750732, 1.1089999675750732, 1.0269999504089355, 1.1089999675750732]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Lev] S4 median g=0.95 -> 0.45873 over 252 vids\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Lev] S4 median g=0.90 -> 0.45873 over 252 vids\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Lev] S5 trim g=0.95 -> 0.39702 over 252 vids\n[OOF-Lev] Best= 0.3970238095238095\n[Submission swap by OOF] submission.csv <- submission_v16only_S5_trim_a0.85_bb0.20_s3_g0.95.csv\n"
          ]
        }
      ]
    },
    {
      "id": "7ed650de-b7b1-4e95-a9b4-9919e25518f1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# LOG-PROB OOF eval + global permutation + final v16-only submission\n",
        "import os, glob, numpy as np, pandas as pd\n",
        "from scipy.special import logit, softmax\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "def logsumexp(a, axis=1):\n",
        "    m = np.max(a, axis=axis, keepdims=True)\n",
        "    return (m + np.log(np.clip(np.sum(np.exp(a - m), axis=axis, keepdims=True), 1e-12, None))).squeeze(axis)\n",
        "\n",
        "def to_calibrated_logprobs(P, Tcls, bg_bias, smooth_win, g=1.0):\n",
        "    P = np.clip(P, 1e-6, 1-1e-6).astype(np.float32)\n",
        "    Z = logit(P).astype(np.float32) / (Tcls.reshape(1, -1) * float(g))\n",
        "    Pcal = softmax(Z, axis=1).astype(np.float32)\n",
        "    Pcal /= np.clip(Pcal.sum(1, keepdims=True), 1e-6, None)\n",
        "    L = np.log(np.clip(Pcal, 1e-6, 1.0)).astype(np.float32)\n",
        "    L[:, 0] += float(bg_bias)\n",
        "    if smooth_win and smooth_win > 1:\n",
        "        k = np.ones(int(smooth_win), np.float32) / float(smooth_win)\n",
        "        L = np.stack([np.convolve(L[:, i], k, mode='same') for i in range(L.shape[1])], 1).astype(np.float32)\n",
        "    return L\n",
        "\n",
        "def segment_exact20(margin, K, min_len):\n",
        "    T = int(len(margin)); min_len = int(max(1, min_len))\n",
        "    while K*min_len > T and min_len > 1: min_len -= 1\n",
        "    if K*min_len > T: K = min(K, T); min_len = 1\n",
        "    pref = np.concatenate([[0.0], np.cumsum(margin, 0).astype(np.float32)])\n",
        "    dp = -1e18*np.ones((K+1, T+1), np.float32); bt = -np.ones((K+1, T+1), np.int32)\n",
        "    dp[0, 0] = 0.0\n",
        "    for k in range(1, K+1):\n",
        "        start_min = (k-1)*min_len\n",
        "        for t in range(k*min_len, T+1):\n",
        "            s_lo = max(start_min, t - (T - (K-k)*min_len))\n",
        "            bestv, bests = -1e18, -1\n",
        "            for s in range(s_lo, t-min_len+1):\n",
        "                v = dp[k-1, s] + (pref[t] - pref[s])\n",
        "                if v > bestv: bestv, bests = v, s\n",
        "            dp[k, t] = bestv; bt[k, t] = bests\n",
        "    bounds = []; k, t = K, T\n",
        "    while k > 0:\n",
        "        s = int(bt[k, t]); bounds.append((s, t)); t = s; k -= 1\n",
        "    bounds.reverse(); return bounds\n",
        "\n",
        "def decode_model_idx(L, alpha):\n",
        "    T, C = L.shape; K = 20\n",
        "    min_len = max(2, int(alpha * T / float(K)))\n",
        "    cols = list(range(1, C))\n",
        "    margin = (logsumexp(L[:, cols], axis=1) - L[:, 0]).astype(np.float32)\n",
        "    segs = segment_exact20(margin, K, min_len)\n",
        "    Cmat = np.zeros((len(segs), K), np.float32)\n",
        "    for j, (s, e) in enumerate(segs):\n",
        "        seg = L[s:e, 1:21]\n",
        "        agg = np.median(seg, axis=0)\n",
        "        Cmat[j, :] = -agg\n",
        "    r, c = linear_sum_assignment(Cmat)\n",
        "    order_model = [int(c[j]) for j in np.argsort(r)]\n",
        "    return order_model, segs\n",
        "\n",
        "def decode_with_perm(L, alpha, perm, cost_mode='median'):\n",
        "    T, C = L.shape; K = 20\n",
        "    min_len = max(2, int(alpha * T / float(K)))\n",
        "    cols = list(range(1, C))\n",
        "    margin = (logsumexp(L[:, cols], axis=1) - L[:, 0]).astype(np.float32)\n",
        "    segs = segment_exact20(margin, K, min_len)\n",
        "    Cmat = np.zeros((len(segs), K), np.float32)\n",
        "    for j, (s, e) in enumerate(segs):\n",
        "        seg = L[s:e, 1:21]\n",
        "        if cost_mode == 'trimmed_mean_20':\n",
        "            n = seg.shape[0]; lo = int(0.1*n); hi = max(lo+1, int(0.9*n))\n",
        "            agg = np.sort(seg, axis=0)[lo:hi].mean(0)\n",
        "        elif cost_mode == 'median':\n",
        "            agg = np.median(seg, axis=0)\n",
        "        else:\n",
        "            agg = seg.mean(0)\n",
        "        Cmat[j, :] = -agg\n",
        "    r, c = linear_sum_assignment(Cmat)\n",
        "    model_idx = [int(c[j]) for j in np.argsort(r)]\n",
        "    return [int(perm[i]) for i in model_idx]\n",
        "\n",
        "def levenshtein(a, b):\n",
        "    n, m = len(a), len(b); dp = list(range(m+1))\n",
        "    for i in range(1, n+1):\n",
        "        prev, dp[0] = dp[0], i\n",
        "        for j in range(1, m+1):\n",
        "            cur = min(dp[j]+1, dp[j-1]+1, prev + (0 if a[i-1]==b[j-1] else 1))\n",
        "            prev, dp[j] = dp[j], cur\n",
        "    return dp[m]\n",
        "\n",
        "# Load labeled OOF per-video\n",
        "OOF_DIR = 'oof_probs_v16_labeled'\n",
        "files = sorted(glob.glob(os.path.join(OOF_DIR, 'oof_*.npz')))\n",
        "vids, Pmap, Ymap = [], {}, {}\n",
        "for fn in files:\n",
        "    z = np.load(fn, allow_pickle=True)\n",
        "    sid = int(z['sid'][0]) if 'sid' in z else int(os.path.basename(fn).split('_')[1].split('.')[0])\n",
        "    Pmap[sid] = z['probs'].astype(np.float32); Ymap[sid] = z['y'].astype(np.int32); vids.append(sid)\n",
        "vids = sorted(vids)\n",
        "\n",
        "# Fit per-class temperatures from labeled OOF\n",
        "P_oof = np.concatenate([Pmap[v] for v in vids], 0)\n",
        "y_oof = np.concatenate([Ymap[v] for v in vids], 0)\n",
        "temp_grid = np.linspace(0.7, 1.6, 12).astype(np.float32)\n",
        "def fit_per_class_temperature(P_oof, y_oof, temp_grid):\n",
        "    C = P_oof.shape[1]; Tcls = np.ones(C, np.float32)\n",
        "    for c in range(C):\n",
        "        pc = np.clip(P_oof[:, c], 1e-6, 1-1e-6); z = logit(pc); yc = (y_oof == c).astype(np.float32)\n",
        "        best, bestT = 1e18, 1.0\n",
        "        for T in temp_grid:\n",
        "            p = 1.0/(1.0 + np.exp(-z/float(T)))\n",
        "            nll = -(yc*np.log(np.clip(p,1e-6,1))).mean() - ((1-yc)*np.log(np.clip(1-p,1e-6,1))).mean()\n",
        "            if nll < best: best, bestT = float(nll), float(T)\n",
        "        Tcls[c] = bestT\n",
        "    return Tcls\n",
        "Tcls16 = fit_per_class_temperature(P_oof, y_oof, temp_grid)\n",
        "\n",
        "# GT sequences\n",
        "train_df = pd.read_csv('training.csv')[['Id','Sequence']].astype({'Id': int})\n",
        "seq_gt = {int(r.Id): [int(x) for x in str(r.Sequence).split()] for r in train_df.itertuples(index=False)}\n",
        "\n",
        "# Learn permutation (model 0..19 -> GT 1..20) by accumulating segment evidence\n",
        "K = 20\n",
        "S = np.zeros((K, K), np.float32)\n",
        "for vid in vids:\n",
        "    L = to_calibrated_logprobs(Pmap[vid], Tcls16, bg_bias=0.25, smooth_win=3, g=0.95)\n",
        "    model_idx, segs = decode_model_idx(L, alpha=0.85)\n",
        "    gt = seq_gt.get(int(vid), [])\n",
        "    if len(gt) != K: continue\n",
        "    for j, (s, e) in enumerate(segs):\n",
        "        seg = L[s:e, 1:21]\n",
        "        agg = np.median(seg, axis=0)\n",
        "        S[:, int(gt[j]-1)] += agg\n",
        "\n",
        "r, c = linear_sum_assignment(-S)  # maximize\n",
        "perm = np.zeros(K, np.int32)\n",
        "for i in range(K):\n",
        "    j = int(np.where(r == i)[0][0]); perm[i] = int(c[j]) + 1\n",
        "\n",
        "# OOF-Lev eval with log-prob + perm\n",
        "def eval_oof(alpha, bg_bias, smooth, g, cost_mode):\n",
        "    dists = []\n",
        "    for vid in vids:\n",
        "        L = to_calibrated_logprobs(Pmap[vid], Tcls16, bg_bias, smooth, g=g)\n",
        "        pred = decode_with_perm(L, alpha, perm, cost_mode=cost_mode)\n",
        "        gt = seq_gt.get(int(vid), [])\n",
        "        if len(gt) != 20: continue\n",
        "        dists.append(levenshtein(pred, gt)/20.0)\n",
        "    return float(np.mean(dists)), len(dists)\n",
        "\n",
        "s4_095, n1 = eval_oof(0.85, 0.25, 3, 0.95, 'median')\n",
        "s4_090, n2 = eval_oof(0.85, 0.25, 3, 0.90, 'median')\n",
        "s5_095, n3 = eval_oof(0.85, 0.20, 3, 0.95, 'trimmed_mean_20')\n",
        "print(f'[OOF-Lev|logprob+perm] S4 g=0.95 -> {s4_095:.5f} ({n1} vids)')\n",
        "print(f'[OOF-Lev|logprob+perm] S4 g=0.90 -> {s4_090:.5f} ({n2} vids)')\n",
        "print(f'[OOF-Lev|logprob+perm] S5 g=0.95 -> {s5_095:.5f} ({n3} vids)')\n",
        "\n",
        "def load_probs_dir_simple(d, ids):\n",
        "    out = {}\n",
        "    for vid in ids:\n",
        "        for pat in (f'{vid}.npz', f'{vid:05d}.npz', f'test_{vid:05d}.npz'):\n",
        "            fn = os.path.join(d, pat)\n",
        "            if os.path.exists(fn):\n",
        "                z = np.load(fn, allow_pickle=True)\n",
        "                if 'probs' in z: out[vid] = z['probs'].astype(np.float32)\n",
        "                elif 'P' in z: out[vid] = z['P'].astype(np.float32)\n",
        "                break\n",
        "    return out\n",
        "\n",
        "def write_submission_v16(name, alpha, bg_bias, smooth, g, cost_mode='median'):\n",
        "    test_ids = pd.read_csv('test.csv')['Id'].astype(int).tolist()\n",
        "    mp = load_probs_dir_simple('test_probs_v16', test_ids)\n",
        "    rows = []\n",
        "    for vid in sorted(mp.keys()):\n",
        "        L = to_calibrated_logprobs(mp[vid], Tcls16, bg_bias, smooth, g=g)\n",
        "        seq = decode_with_perm(L, alpha, perm, cost_mode=cost_mode)\n",
        "        rows.append((vid, ' '.join(map(str, seq))))\n",
        "    sub = pd.DataFrame(rows, columns=['Id','Sequence']).sort_values('Id')\n",
        "    out = f'submission_v16_perm_{name}.csv'\n",
        "    sub.to_csv(out, index=False)\n",
        "    pd.read_csv(out).to_csv('submission.csv', index=False)\n",
        "    print('[Wrote] submission.csv <-', out, sub.shape)\n",
        "    return out\n",
        "\n",
        "best_name, best_score = sorted([\n",
        "    ('S4_a0.85_bb0.25_s3_g0.95', s4_095),\n",
        "    ('S4_a0.85_bb0.25_s3_g0.90', s4_090),\n",
        "    ('S5_a0.85_bb0.20_s3_g0.95', s5_095),\n",
        "], key=lambda x: x[1])[0]\n",
        "\n",
        "write_submission_v16(best_name, alpha=0.85,\n",
        "                     bg_bias=0.25 if 'S4' in best_name else 0.20,\n",
        "                     smooth=3, g=0.90 if 'g0.90' in best_name else 0.95,\n",
        "                     cost_mode=('median' if 'S4' in best_name else 'trimmed_mean_20'))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Lev|logprob+perm] S4 g=0.95 -> 0.44583 (252 vids)\n[OOF-Lev|logprob+perm] S4 g=0.90 -> 0.44504 (252 vids)\n[OOF-Lev|logprob+perm] S5 g=0.95 -> 0.39266 (252 vids)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Wrote] submission.csv <- submission_v16_perm_S5_a0.85_bb0.20_s3_g0.95.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "'submission_v16_perm_S5_a0.85_bb0.20_s3_g0.95.csv'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "6059455c-20f2-4292-ad3a-d7f32acb91ea",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Center-of-Mass (CoM) ordering on calibrated log-probs + global permutation; OOF-eval and write test submission\n",
        "import os, glob, numpy as np, pandas as pd\n",
        "from scipy.special import logit, softmax\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "def to_calibrated_logprobs(P, Tcls, bg_bias=0.25, smooth_win=3, g=0.95):\n",
        "    P = np.clip(P, 1e-6, 1-1e-6).astype(np.float32)\n",
        "    Z = logit(P).astype(np.float32) / (Tcls.reshape(1, -1) * float(g))\n",
        "    Pcal = softmax(Z, axis=1).astype(np.float32)\n",
        "    Pcal /= np.clip(Pcal.sum(1, keepdims=True), 1e-6, None)\n",
        "    L = np.log(np.clip(Pcal, 1e-6, 1.0)).astype(np.float32)\n",
        "    L[:, 0] += float(bg_bias)\n",
        "    if smooth_win and smooth_win > 1:\n",
        "        k = np.ones(int(smooth_win), np.float32) / float(smooth_win)\n",
        "        L = np.stack([np.convolve(L[:, i], k, mode='same') for i in range(L.shape[1])], 1).astype(np.float32)\n",
        "    return L\n",
        "\n",
        "def load_oof_labeled_map(oof_dir='oof_probs_v16_labeled'):\n",
        "    files = sorted(glob.glob(os.path.join(oof_dir, 'oof_*.npz')))\n",
        "    vids, Pmap, Ymap = [], {}, {}\n",
        "    for fn in files:\n",
        "        z = np.load(fn, allow_pickle=True)\n",
        "        sid = int(z['sid'][0]) if 'sid' in z else int(os.path.basename(fn).split('_')[1].split('.')[0])\n",
        "        Pmap[sid] = z['probs'].astype(np.float32); Ymap[sid] = z['y'].astype(np.int32); vids.append(sid)\n",
        "    return sorted(vids), Pmap, Ymap\n",
        "\n",
        "def fit_per_class_temperature(P_oof, y_oof, temp_grid):\n",
        "    C = P_oof.shape[1]; Tcls = np.ones(C, np.float32)\n",
        "    for c in range(C):\n",
        "        pc = np.clip(P_oof[:, c], 1e-6, 1-1e-6); z = logit(pc); yc = (y_oof == c).astype(np.float32)\n",
        "        best, bestT = 1e18, 1.0\n",
        "        for T in temp_grid:\n",
        "            p = 1.0/(1.0 + np.exp(-z/float(T)))\n",
        "            nll = -(yc*np.log(np.clip(p,1e-6,1))).mean() - ((1-yc)*np.log(np.clip(1-p,1e-6,1))).mean()\n",
        "            if nll < best: best, bestT = float(nll), float(T)\n",
        "        Tcls[c] = bestT\n",
        "    return Tcls\n",
        "\n",
        "def center_of_mass_order(L):\n",
        "    # L: [T, 21] log-probs; compute expected time (1..T) per non-bg class using probs\n",
        "    T = L.shape[0]; idx = np.arange(1, T+1, dtype=np.float32).reshape(-1,1)\n",
        "    P = np.exp(L[:, 1:21]).astype(np.float32)  # non-bg probs\n",
        "    wsum = np.clip(P.sum(0), 1e-6, None); tnum = (idx * P).sum(0)\n",
        "    tbar = (tnum / wsum)  # center-of-mass time per class\n",
        "    # tie-breaker by peak height (higher peak earlier)\n",
        "    peak = P.max(0)\n",
        "    order_model = np.lexsort(( -peak, tbar ))  # ascending tbar, then descending peak\n",
        "    return order_model.astype(np.int32)  # 0..19 (model class indices for non-bg)\n",
        "\n",
        "def levenshtein(a, b):\n",
        "    n, m = len(a), len(b); dp = list(range(m+1))\n",
        "    for i in range(1, n+1):\n",
        "        prev, dp[0] = dp[0], i\n",
        "        for j in range(1, m+1):\n",
        "            cur = min(dp[j]+1, dp[j-1]+1, prev + (0 if a[i-1]==b[j-1] else 1))\n",
        "            prev, dp[j] = dp[j], cur\n",
        "    return dp[m]\n",
        "\n",
        "# 1) Load labeled OOF and fit temps\n",
        "vids, Pmap, Ymap = load_oof_labeled_map('oof_probs_v16_labeled')\n",
        "assert vids, 'No labeled OOF videos found'\n",
        "P_oof = np.concatenate([Pmap[v] for v in vids], 0)\n",
        "y_oof = np.concatenate([Ymap[v] for v in vids], 0)\n",
        "Tcls16 = fit_per_class_temperature(P_oof, y_oof, np.linspace(0.7, 1.6, 12).astype(np.float32))\n",
        "print('[CoM] Temps first10:', np.round(Tcls16[:10], 3).tolist())\n",
        "\n",
        "# 2) Ground-truth sequences\n",
        "train_df = pd.read_csv('training.csv')[['Id','Sequence']].astype({'Id': int})\n",
        "seq_gt = {int(r.Id): [int(x) for x in str(r.Sequence).split()] for r in train_df.itertuples(index=False)}\n",
        "\n",
        "# 3) Learn global permutation via evidence matrix using CoM ordering positions\n",
        "K = 20; S = np.zeros((K, K), np.float32)\n",
        "for vid in vids:\n",
        "    L = to_calibrated_logprobs(Pmap[vid], Tcls16, bg_bias=0.25, smooth_win=3, g=0.95)\n",
        "    order_model = center_of_mass_order(L)  # length 20, each 0..19\n",
        "    gt = seq_gt.get(int(vid), [])\n",
        "    if len(gt) != K: continue\n",
        "    # Add evidence for mapping model class at position j -> gt class at same position\n",
        "    # Weight by confidence: use per-class peak log-prob as strength\n",
        "    Pn = L[:, 1:21]\n",
        "    strength = Pn.max(0)  # shape (20,)\n",
        "    for j, mi in enumerate(order_model):\n",
        "        S[int(mi), int(gt[j]-1)] += strength[int(mi)]\n",
        "\n",
        "r, c = linear_sum_assignment(-S)  # maximize S\n",
        "perm = np.zeros(K, np.int32)\n",
        "for i in range(K):\n",
        "    j = int(np.where(r == i)[0][0]); perm[i] = int(c[j]) + 1  # map model idx-> class id 1..20\n",
        "print('[CoM] Learned perm (first 10):', perm[:10].tolist())\n",
        "\n",
        "# 4) OOF-Lev evaluation with CoM + perm\n",
        "dists = []\n",
        "for vid in vids:\n",
        "    L = to_calibrated_logprobs(Pmap[vid], Tcls16, bg_bias=0.25, smooth_win=3, g=0.95)\n",
        "    order_model = center_of_mass_order(L)\n",
        "    pred = [int(perm[i]) for i in order_model.tolist()]\n",
        "    gt = seq_gt.get(int(vid), [])\n",
        "    if len(gt) != K: continue\n",
        "    dists.append(levenshtein(pred, gt)/20.0)\n",
        "oof_score = float(np.mean(dists)) if dists else 1.0\n",
        "print(f'[OOF-Lev|CoM+perm] -> {oof_score:.5f} over {len(dists)} vids')\n",
        "\n",
        "# 5) Write test submission with CoM + perm\n",
        "def load_probs_dir_simple(d, ids):\n",
        "    out = {}\n",
        "    for vid in ids:\n",
        "        for pat in (f'{vid}.npz', f'{vid:05d}.npz', f'test_{vid:05d}.npz'):\n",
        "            fn = os.path.join(d, pat)\n",
        "            if os.path.exists(fn):\n",
        "                z = np.load(fn, allow_pickle=True)\n",
        "                if 'probs' in z: out[vid] = z['probs'].astype(np.float32)\n",
        "                elif 'P' in z: out[vid] = z['P'].astype(np.float32)\n",
        "                break\n",
        "    return out\n",
        "\n",
        "test_ids = pd.read_csv('test.csv')['Id'].astype(int).tolist()\n",
        "mp = load_probs_dir_simple('test_probs_v16', test_ids)\n",
        "rows = []\n",
        "for vid in sorted(mp.keys()):\n",
        "    L = to_calibrated_logprobs(mp[vid], Tcls16, bg_bias=0.25, smooth_win=3, g=0.95)\n",
        "    order_model = center_of_mass_order(L)\n",
        "    seq = [int(perm[i]) for i in order_model.tolist()]\n",
        "    rows.append((vid, ' '.join(map(str, seq))))\n",
        "sub = pd.DataFrame(rows, columns=['Id','Sequence']).sort_values('Id')\n",
        "out = 'submission_v16_CoM_perm_g0.95_bb0.25_s3.csv'\n",
        "sub.to_csv(out, index=False)\n",
        "pd.read_csv(out).to_csv('submission.csv', index=False)\n",
        "print('[Wrote] submission.csv <-', out, sub.shape)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CoM] Temps first10: [1.190999984741211, 1.0269999504089355, 1.1089999675750732, 1.0269999504089355, 1.0269999504089355, 1.0269999504089355, 1.1089999675750732, 1.1089999675750732, 1.0269999504089355, 1.1089999675750732]\n[CoM] Learned perm (first 10): [12, 1, 4, 11, 2, 13, 17, 16, 19, 6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Lev|CoM+perm] -> 0.93611 over 252 vids\n[Wrote] submission.csv <- submission_v16_CoM_perm_g0.95_bb0.25_s3.csv (95, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "4fc404f4-317b-43e1-a881-a206d39fa045",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Correct OOF-Lev using true GT from labeled OOF y (collapse runs), log-prob DP decoder (no perm), write best submission\n",
        "import os, glob, numpy as np, pandas as pd\n",
        "from scipy.special import logit, softmax\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "def logsumexp(a, axis=1):\n",
        "    m = np.max(a, axis=axis, keepdims=True)\n",
        "    return (m + np.log(np.clip(np.sum(np.exp(a - m), axis=axis, keepdims=True), 1e-12, None))).squeeze(axis)\n",
        "\n",
        "def load_oof_labeled(oof_dir='oof_probs_v16_labeled'):\n",
        "    vids, Pmap, Ymap = [], {}, {}\n",
        "    files = sorted(glob.glob(os.path.join(oof_dir, 'oof_*.npz')))\n",
        "    for fn in files:\n",
        "        z = np.load(fn, allow_pickle=True)\n",
        "        sid = int(z['sid'][0]) if 'sid' in z else int(os.path.basename(fn).split('_')[1].split('.')[0])\n",
        "        Pmap[sid] = z['probs'].astype(np.float32)\n",
        "        Ymap[sid] = z['y'].astype(np.int32)\n",
        "        vids.append(sid)\n",
        "    return sorted(vids), Pmap, Ymap\n",
        "\n",
        "def fit_per_class_temperature(P_oof, y_oof, temp_grid):\n",
        "    C = P_oof.shape[1]; Tcls = np.ones(C, np.float32)\n",
        "    for c in range(C):\n",
        "        pc = np.clip(P_oof[:, c], 1e-6, 1-1e-6); z = logit(pc); yc = (y_oof == c).astype(np.float32)\n",
        "        best, bestT = 1e18, 1.0\n",
        "        for T in temp_grid:\n",
        "            p = 1.0/(1.0 + np.exp(-z/float(T)))\n",
        "            nll = -(yc*np.log(np.clip(p,1e-6,1))).mean() - ((1-yc)*np.log(np.clip(1-p,1e-6,1))).mean()\n",
        "            if nll < best: best, bestT = float(nll), float(T)\n",
        "        Tcls[c] = bestT\n",
        "    return Tcls.astype(np.float32)\n",
        "\n",
        "def to_calibrated_logprobs(P, Tcls, bg_bias, smooth_win, g=1.0):\n",
        "    P = np.clip(P, 1e-6, 1-1e-6).astype(np.float32)\n",
        "    Z = logit(P).astype(np.float32) / (Tcls.reshape(1, -1) * float(g))\n",
        "    Pcal = softmax(Z, axis=1).astype(np.float32)\n",
        "    Pcal /= np.clip(Pcal.sum(1, keepdims=True), 1e-6, None)\n",
        "    L = np.log(np.clip(Pcal, 1e-6, 1.0)).astype(np.float32)\n",
        "    L[:, 0] += float(bg_bias)\n",
        "    if smooth_win and smooth_win > 1:\n",
        "        k = np.ones(int(smooth_win), np.float32) / float(smooth_win)\n",
        "        L = np.stack([np.convolve(L[:, i], k, mode='same') for i in range(L.shape[1])], 1).astype(np.float32)\n",
        "    return L\n",
        "\n",
        "def segment_exact20(margin, K, min_len):\n",
        "    T = int(len(margin)); min_len = int(max(1, min_len))\n",
        "    while K*min_len > T and min_len > 1: min_len -= 1\n",
        "    if K*min_len > T: K = min(K, T); min_len = 1\n",
        "    pref = np.concatenate([[0.0], np.cumsum(margin, 0).astype(np.float32)])\n",
        "    dp = -1e18*np.ones((K+1, T+1), np.float32); bt = -np.ones((K+1, T+1), np.int32)\n",
        "    dp[0, 0] = 0.0\n",
        "    for k in range(1, K+1):\n",
        "        start_min = (k-1)*min_len\n",
        "        for t in range(k*min_len, T+1):\n",
        "            s_lo = max(start_min, t - (T - (K-k)*min_len))\n",
        "            bestv, bests = -1e18, -1\n",
        "            for s in range(s_lo, t-min_len+1):\n",
        "                v = dp[k-1, s] + (pref[t] - pref[s])\n",
        "                if v > bestv: bestv, bests = v, s\n",
        "            dp[k, t] = bestv; bt[k, t] = bests\n",
        "    bounds = []; k, t = K, T\n",
        "    while k > 0:\n",
        "        s = int(bt[k, t]); bounds.append((s, t)); t = s; k -= 1\n",
        "    bounds.reverse(); return bounds\n",
        "\n",
        "def decode_with_cost_logprob(L, alpha, cost_mode='median'):\n",
        "    T, C = L.shape; K = 20\n",
        "    min_len = max(2, int(alpha * T / float(K)))\n",
        "    cols = list(range(1, C))\n",
        "    margin = (logsumexp(L[:, cols], axis=1) - L[:, 0]).astype(np.float32)\n",
        "    segs = segment_exact20(margin, K, min_len)\n",
        "    Cmat = np.zeros((len(segs), K), np.float32)\n",
        "    for j, (s, e) in enumerate(segs):\n",
        "        seg = L[s:e, 1:21]\n",
        "        if cost_mode == 'trimmed_mean_20':\n",
        "            n = seg.shape[0]; lo = int(0.1*n); hi = max(lo+1, int(0.9*n))\n",
        "            agg = np.sort(seg, axis=0)[lo:hi].mean(0)\n",
        "        elif cost_mode == 'median':\n",
        "            agg = np.median(seg, axis=0)\n",
        "        else:\n",
        "            agg = seg.mean(0)\n",
        "        Cmat[j, :] = -agg\n",
        "    r, c = linear_sum_assignment(Cmat)\n",
        "    order = [int(c[j])+1 for j in np.argsort(r)]  # map non-bg columns to class ids 1..20\n",
        "    return order\n",
        "\n",
        "def levenshtein(a, b):\n",
        "    n, m = len(a), len(b); dp = list(range(m+1))\n",
        "    for i in range(1, n+1):\n",
        "        prev, dp[0] = dp[0], i\n",
        "        for j in range(1, m+1):\n",
        "            cur = min(dp[j]+1, dp[j-1]+1, prev + (0 if a[i-1]==b[j-1] else 1))\n",
        "            prev, dp[j] = dp[j], cur\n",
        "    return dp[m]\n",
        "\n",
        "# 1) Load labeled OOF and build true GT sequences by collapsing runs of y>0\n",
        "vids, Pmap, Ymap = load_oof_labeled('oof_probs_v16_labeled')\n",
        "assert vids, 'No labeled OOF videos found'\n",
        "seq_gt_y = {}\n",
        "for vid in vids:\n",
        "    y = Ymap[vid]; seq = []; prev = -1\n",
        "    for t in y:\n",
        "        ti = int(t)\n",
        "        if ti != prev and ti > 0:\n",
        "            seq.append(ti)\n",
        "        prev = ti\n",
        "    seq_gt_y[vid] = seq[:20]\n",
        "\n",
        "# 2) Fit per-class temperatures on all OOF frames\n",
        "P_oof = np.concatenate([Pmap[v] for v in vids], 0)\n",
        "y_oof = np.concatenate([Ymap[v] for v in vids], 0)\n",
        "Tcls16 = fit_per_class_temperature(P_oof, y_oof, np.linspace(0.7, 1.6, 12).astype(np.float32))\n",
        "print('[GT(y)] Temps first10:', np.round(Tcls16[:10], 3).tolist())\n",
        "\n",
        "# 3) OOF-Lev eval (log-prob, no permutation) against seq_gt_y\n",
        "def eval_oof(alpha, bg_bias, smooth, g, cost_mode):\n",
        "    dists = []\n",
        "    for vid in vids:\n",
        "        L = to_calibrated_logprobs(Pmap[vid], Tcls16, bg_bias, smooth, g=g)\n",
        "        pred = decode_with_cost_logprob(L, alpha, cost_mode=cost_mode)\n",
        "        gt = seq_gt_y.get(int(vid), [])\n",
        "        if len(gt) < 1: continue\n",
        "        gg = gt[:20]\n",
        "        pp = pred[:len(gg)]\n",
        "        d = levenshtein(pp, gg) / float(len(gg))\n",
        "        dists.append(d)\n",
        "    return float(np.mean(dists)) if dists else 1.0, len(dists)\n",
        "\n",
        "s4_095, n1 = eval_oof(0.85, 0.25, 3, 0.95, 'median')\n",
        "s4_090, n2 = eval_oof(0.85, 0.25, 3, 0.90, 'median')\n",
        "s5_095, n3 = eval_oof(0.85, 0.20, 3, 0.95, 'trimmed_mean_20')\n",
        "print(f'[OOF-Lev|yGT] S4 g=0.95 -> {s4_095:.5f} ({n1} vids)')\n",
        "print(f'[OOF-Lev|yGT] S4 g=0.90 -> {s4_090:.5f} ({n2} vids)')\n",
        "print(f'[OOF-Lev|yGT] S5 g=0.95 -> {s5_095:.5f} ({n3} vids)')\n",
        "\n",
        "# 4) Write best test submission with the same decoder\n",
        "def load_probs_dir_simple(d, ids):\n",
        "    out = {}\n",
        "    for vid in ids:\n",
        "        for pat in (f'{vid}.npz', f'{vid:05d}.npz', f'test_{vid:05d}.npz'):\n",
        "            fn = os.path.join(d, pat)\n",
        "            if os.path.exists(fn):\n",
        "                z = np.load(fn, allow_pickle=True)\n",
        "                if 'probs' in z: out[vid] = z['probs'].astype(np.float32)\n",
        "                elif 'P' in z: out[vid] = z['P'].astype(np.float32)\n",
        "                break\n",
        "    return out\n",
        "\n",
        "# Select best config (name, score, cost_mode, bg_bias, smooth, g) and unpack cleanly\n",
        "best_tuple = sorted([\n",
        "    ('S4_a0.85_bb0.25_s3_g0.95', s4_095, 'median', 0.25, 3, 0.95),\n",
        "    ('S4_a0.85_bb0.25_s3_g0.90', s4_090, 'median', 0.25, 3, 0.90),\n",
        "    ('S5_a0.85_bb0.20_s3_g0.95', s5_095, 'trimmed_mean_20', 0.20, 3, 0.95),\n",
        "], key=lambda x: x[1])[0]\n",
        "best_name, best_score, best_cost_mode, best_bb, best_smooth, best_g = best_tuple\n",
        "print('[OOF-Lev|yGT] Best:', best_name, '->', f'{best_score:.5f}')\n",
        "\n",
        "test_ids = pd.read_csv('test.csv')['Id'].astype(int).tolist()\n",
        "mp = load_probs_dir_simple('test_probs_v16', test_ids)\n",
        "rows = []\n",
        "for vid in sorted(mp.keys()):\n",
        "    L = to_calibrated_logprobs(mp[vid], Tcls16, best_bb, best_smooth, g=best_g)\n",
        "    seq = decode_with_cost_logprob(L, 0.85, cost_mode=best_cost_mode)\n",
        "    rows.append((vid, ' '.join(map(str, seq))))\n",
        "sub = pd.DataFrame(rows, columns=['Id','Sequence']).sort_values('Id')\n",
        "out = f'submission_v16_yGT_{best_name}.csv'\n",
        "sub.to_csv(out, index=False)\n",
        "pd.read_csv(out).to_csv('submission.csv', index=False)\n",
        "print('[Wrote] submission.csv <-', out, sub.shape)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GT(y)] Temps first10: [1.190999984741211, 1.0269999504089355, 1.1089999675750732, 1.0269999504089355, 1.0269999504089355, 1.0269999504089355, 1.1089999675750732, 1.1089999675750732, 1.0269999504089355, 1.1089999675750732]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Lev|yGT] S4 g=0.95 -> 0.45420 (297 vids)\n[OOF-Lev|yGT] S4 g=0.90 -> 0.45299 (297 vids)\n[OOF-Lev|yGT] S5 g=0.95 -> 0.40374 (297 vids)\n[OOF-Lev|yGT] Best: S5_a0.85_bb0.20_s3_g0.95 -> 0.40374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Wrote] submission.csv <- submission_v16_yGT_S5_a0.85_bb0.20_s3_g0.95.csv (95, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "f3a7932e-de10-4d8e-ae2b-d6567be8a23b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick alt: v16-only log-prob decoder variants (more aggressive) and set a=0.80, bb=0.30, g=0.90, s=3, median as submission.csv\n",
        "import os, glob, numpy as np, pandas as pd\n",
        "from scipy.special import logit, softmax\n",
        "from scipy.signal import convolve\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "def load_oof_labeled(oof_dir='oof_probs_v16_labeled'):\n",
        "    X, Y = [], []\n",
        "    for fn in glob.glob(os.path.join(oof_dir, 'oof_*.npz')):\n",
        "        z = np.load(fn, allow_pickle=True)\n",
        "        if 'probs' in z and 'y' in z:\n",
        "            X.append(z['probs'].astype(np.float32))\n",
        "            Y.append(z['y'].astype(np.int32))\n",
        "    if not X: return None, None\n",
        "    return np.concatenate(X, 0), np.concatenate(Y, 0)\n",
        "\n",
        "def fit_per_class_temperature(P_oof, y_oof, temp_grid):\n",
        "    C = P_oof.shape[1]\n",
        "    Tcls = np.ones(C, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        pc = np.clip(P_oof[:, c], 1e-6, 1-1e-6); z = logit(pc); yc = (y_oof == c).astype(np.float32)\n",
        "        best, bestT = 1e18, 1.0\n",
        "        for T in temp_grid:\n",
        "            p = 1.0/(1.0 + np.exp(-z/float(T)))\n",
        "            nll = -(yc*np.log(np.clip(p,1e-6,1))).mean() - ((1-yc)*np.log(np.clip(1-p,1e-6,1))).mean()\n",
        "            if nll < best: best, bestT = float(nll), float(T)\n",
        "        Tcls[c] = bestT\n",
        "    return Tcls.astype(np.float32)\n",
        "\n",
        "def to_calibrated_logprobs(P, Tcls, bg_bias, smooth_win, g=0.90):\n",
        "    P = np.clip(P, 1e-6, 1-1e-6).astype(np.float32)\n",
        "    Z = logit(P).astype(np.float32) / (Tcls.reshape(1, -1) * float(g))\n",
        "    Pcal = softmax(Z, axis=1).astype(np.float32)\n",
        "    Pcal /= np.clip(Pcal.sum(1, keepdims=True), 1e-6, None)\n",
        "    L = np.log(np.clip(Pcal, 1e-6, 1.0)).astype(np.float32)\n",
        "    L[:, 0] += float(bg_bias)\n",
        "    if smooth_win and smooth_win > 1:\n",
        "        k = np.ones(int(smooth_win), np.float32) / float(smooth_win)\n",
        "        L = np.stack([convolve(L[:, i], k, mode='same') for i in range(L.shape[1])], 1).astype(np.float32)\n",
        "    return L\n",
        "\n",
        "def segment_exact20(margin, K, min_len):\n",
        "    T = int(len(margin)); min_len = int(max(1, min_len))\n",
        "    while K*min_len > T and min_len > 1: min_len -= 1\n",
        "    if K*min_len > T: K = min(K, T); min_len = 1\n",
        "    pref = np.concatenate([[0.0], np.cumsum(margin, 0).astype(np.float32)])\n",
        "    dp = -1e18*np.ones((K+1, T+1), np.float32); bt = -np.ones((K+1, T+1), np.int32)\n",
        "    dp[0, 0] = 0.0\n",
        "    for k in range(1, K+1):\n",
        "        start_min = (k-1)*min_len\n",
        "        for t in range(k*min_len, T+1):\n",
        "            s_lo = max(start_min, t - (T - (K-k)*min_len))\n",
        "            bestv, bests = -1e18, -1\n",
        "            for s in range(s_lo, t-min_len+1):\n",
        "                v = dp[k-1, s] + (pref[t] - pref[s])\n",
        "                if v > bestv: bestv, bests = v, s\n",
        "            dp[k, t] = bestv; bt[k, t] = bests\n",
        "    bounds = []; k, t = K, T\n",
        "    while k > 0:\n",
        "        s = int(bt[k, t]); bounds.append((s, t)); t = s; k -= 1\n",
        "    bounds.reverse(); return bounds\n",
        "\n",
        "def decode_with_cost_logprob(L, alpha, cost_mode='median'):\n",
        "    T, C = L.shape; K = 20\n",
        "    min_len = max(2, int(alpha * T / float(K)))\n",
        "    cols = list(range(1, C))\n",
        "    m = (np.log(np.clip(np.exp(L[:, cols]).sum(1, keepdims=True), 1e-12, None)).squeeze(1) - L[:, 0]).astype(np.float32)\n",
        "    segs = segment_exact20(m, K, min_len)\n",
        "    Cmat = np.zeros((len(segs), K), np.float32)\n",
        "    for j, (s, e) in enumerate(segs):\n",
        "        seg = L[s:e, 1:21]\n",
        "        if cost_mode == 'trimmed_mean_20':\n",
        "            n = seg.shape[0]; lo = int(0.1*n); hi = max(lo+1, int(0.9*n))\n",
        "            agg = np.sort(seg, axis=0)[lo:hi].mean(0)\n",
        "        elif cost_mode == 'median':\n",
        "            agg = np.median(seg, axis=0)\n",
        "        else:\n",
        "            agg = seg.mean(0)\n",
        "        Cmat[j, :] = -agg\n",
        "    r, c = linear_sum_assignment(Cmat)\n",
        "    order = [int(c[j])+1 for j in np.argsort(r)]\n",
        "    return order\n",
        "\n",
        "def load_probs_dir_simple(d, ids):\n",
        "    out = {}\n",
        "    for vid in ids:\n",
        "        for pat in (f'{vid}.npz', f'{vid:05d}.npz', f'test_{vid:05d}.npz'):\n",
        "            fn = os.path.join(d, pat)\n",
        "            if os.path.exists(fn):\n",
        "                z = np.load(fn, allow_pickle=True)\n",
        "                if 'probs' in z: out[vid] = z['probs'].astype(np.float32)\n",
        "                elif 'P' in z: out[vid] = z['P'].astype(np.float32)\n",
        "                break\n",
        "    return out\n",
        "\n",
        "# Fit temps from labeled OOF\n",
        "P_oof, y_oof = load_oof_labeled('oof_probs_v16_labeled')\n",
        "assert P_oof is not None and y_oof is not None, 'Missing labeled OOF v16'\n",
        "Tcls16 = fit_per_class_temperature(P_oof, y_oof, np.linspace(0.7, 1.6, 12).astype(np.float32))\n",
        "\n",
        "# Load test v16 probs\n",
        "test_ids = pd.read_csv('test.csv')['Id'].astype(int).tolist()\n",
        "mp = load_probs_dir_simple('test_probs_v16', test_ids)\n",
        "\n",
        "def run_variant(name, alpha, bg_bias, smooth, g, cost_mode='median'):\n",
        "    rows = []\n",
        "    for vid in sorted(mp.keys()):\n",
        "        L = to_calibrated_logprobs(mp[vid], Tcls16, bg_bias, smooth, g=g)\n",
        "        seq = decode_with_cost_logprob(L, alpha, cost_mode=cost_mode)\n",
        "        rows.append((vid, ' '.join(map(str, seq))))\n",
        "    sub = pd.DataFrame(rows, columns=['Id','Sequence']).sort_values('Id')\n",
        "    out = f'submission_v16_logprob_{name}.csv'\n",
        "    sub.to_csv(out, index=False)\n",
        "    print('[Write]', out, sub.shape)\n",
        "    return out\n",
        "\n",
        "# Aggressive variants\n",
        "out1 = run_variant('S4_a0.80_bb0.30_s3_g0.90_median', alpha=0.80, bg_bias=0.30, smooth=3, g=0.90, cost_mode='median')\n",
        "out2 = run_variant('S5_a0.85_bb0.20_s3_g0.90_trim', alpha=0.85, bg_bias=0.20, smooth=3, g=0.90, cost_mode='trimmed_mean_20')\n",
        "out3 = run_variant('S4_a0.80_bb0.25_s3_g0.90_median', alpha=0.80, bg_bias=0.25, smooth=3, g=0.90, cost_mode='median')\n",
        "\n",
        "# Set primary to S4 a0.80 bb0.30 s3 g=0.90 median\n",
        "pd.read_csv(out1).to_csv('submission.csv', index=False)\n",
        "print('[Primary] submission.csv <-', out1)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Write] submission_v16_logprob_S4_a0.80_bb0.30_s3_g0.90_median.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Write] submission_v16_logprob_S5_a0.85_bb0.20_s3_g0.90_trim.csv (95, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Write] submission_v16_logprob_S4_a0.80_bb0.25_s3_g0.90_median.csv (95, 2)\n[Primary] submission.csv <- submission_v16_logprob_S4_a0.80_bb0.30_s3_g0.90_median.csv\n"
          ]
        }
      ]
    },
    {
      "id": "e763eded-d8c4-41dd-989a-5f767e5d4e83",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 39: Install Torch (cu121) + TCN 5-fold GroupKFold training on cache_v16 -> OOF/test probs\n",
        "import os, sys, subprocess, time, random, glob, math, gc, json\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "# 1) Safe Torch install (CUDA 12.1) + sanity\n",
        "try:\n",
        "    import torch, torchvision, torchaudio  # noqa\n",
        "    need_install = False\n",
        "except Exception:\n",
        "    need_install = True\n",
        "\n",
        "if need_install:\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
        "                    \"--index-url\", \"https://download.pytorch.org/whl/cu121\",\n",
        "                    \"torch==2.4.1\", \"torchvision==0.19.1\", \"torchaudio==2.4.1\"], check=False)\n",
        "\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "print(\"[Torch]\", torch.__version__)\n",
        "os.system('nvidia-smi || true')\n",
        "print(\"[CUDA avail]\", torch.cuda.is_available())\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "# 2) Repro + config\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "CACHE_DIR = './cache_v16'\n",
        "OOF_DIR = './oof_probs_v16_tcn'\n",
        "TEST_FOLD_DIRS = [f'./test_probs_v16_tcn_fold{i}' for i in range(1, 6)]\n",
        "for d in [OOF_DIR] + TEST_FOLD_DIRS:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "train_ids = pd.read_csv('training.csv')['Id'].astype(int).tolist()\n",
        "test_ids = pd.read_csv('test.csv')['Id'].astype(int).tolist()\n",
        "\n",
        "def load_train_video(vid):\n",
        "    fn = os.path.join(CACHE_DIR, f'train_{vid:05d}.npz')\n",
        "    if not os.path.exists(fn): return None, None\n",
        "    z = np.load(fn, allow_pickle=False)\n",
        "    X = z['X'].astype(np.float32)\n",
        "    y = (z['y'] if 'y' in z else z['labels']).astype(np.int64)\n",
        "    return X, y\n",
        "\n",
        "def load_test_video(vid):\n",
        "    fn = os.path.join(CACHE_DIR, f'test_{vid:05d}.npz')\n",
        "    if not os.path.exists(fn): return None\n",
        "    z = np.load(fn, allow_pickle=False)\n",
        "    return z['X'].astype(np.float32)\n",
        "\n",
        "# 3) Small, strong 1D TCN (dilated convs; left-causal bias via crop)\n",
        "class TCN1D(nn.Module):\n",
        "    def __init__(self, in_ch=193, hid=128, num_classes=21, k=7, drop=0.30):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Conv1d(in_ch, hid, 1)\n",
        "        blocks = []\n",
        "        self.k = k\n",
        "        self.dils = [1, 2, 4, 8]\n",
        "        for d in self.dils:\n",
        "            pad = d * (k - 1)  # pad left+right; crop right later\n",
        "            blocks += [nn.Conv1d(hid, hid, k, padding=pad, dilation=d),\n",
        "                       nn.BatchNorm1d(hid),\n",
        "                       nn.ReLU(inplace=True),\n",
        "                       nn.Dropout(drop)]\n",
        "        self.tcn = nn.Sequential(*blocks)\n",
        "        self.head = nn.Conv1d(hid, num_classes, 1)\n",
        "\n",
        "    def forward(self, x):               # x: [B,T,F]\n",
        "        x = x.transpose(1, 2)           # -> [B,F,T]\n",
        "        h = self.stem(x)\n",
        "        h = self.tcn(h)\n",
        "        # total pad per block on right side = d*(k-1); sum d=[1,2,4,8] -> 15*(k-1) with k=7 => 90\n",
        "        crop = sum(self.dils) * (self.k - 1)\n",
        "        if crop > 0 and h.shape[-1] > crop:\n",
        "            h = h[:, :, :-crop]\n",
        "        logits = self.head(h)           # [B,C,T']\n",
        "        return logits.transpose(1, 2)   # [B,T',C]\n",
        "\n",
        "# 4) Train config\n",
        "CFG = dict(\n",
        "    epochs=10, patience=2,\n",
        "    steps_per_epoch=160,\n",
        "    batch_size=20, win_len=256,\n",
        "    lr=1e-3, wd=1e-2, eta_min=1e-5,\n",
        "    label_smoothing=0.05, bg_weight=0.5,\n",
        "    num_classes=21, grad_clip=1.0\n",
        ")\n",
        "\n",
        "def compute_norm_stats(X_list):\n",
        "    s = np.zeros(X_list[0].shape[1], np.float64)\n",
        "    ss = np.zeros_like(s); n = 0\n",
        "    for X in X_list:\n",
        "        s += X.sum(0); ss += (X.astype(np.float64)**2).sum(0); n += X.shape[0]\n",
        "    mu = s / max(1, n)\n",
        "    var = np.maximum(0.0, ss / max(1, n) - mu**2)\n",
        "    std = np.sqrt(var + 1e-6).astype(np.float32)\n",
        "    return mu.astype(np.float32), std\n",
        "\n",
        "def make_class_weights(Y_list, C=21, bg_weight=0.5):\n",
        "    cnt = np.zeros(C, np.int64)\n",
        "    for y in Y_list:\n",
        "        cnt += np.bincount(y, minlength=C)\n",
        "    cnt = np.maximum(cnt, 1)\n",
        "    w = (cnt.sum() / cnt.astype(np.float32))**0.5\n",
        "    w = (w / w.mean()).astype(np.float32)\n",
        "    w[0] *= float(bg_weight)\n",
        "    return torch.tensor(w, dtype=torch.float32, device=device)\n",
        "\n",
        "def sample_batch(X_list, y_list, mu, std, B, T):\n",
        "    F = X_list[0].shape[1]\n",
        "    Xb = np.zeros((B, T, F), np.float32)\n",
        "    yb = np.zeros((B, T), np.int64)\n",
        "    for i in range(B):\n",
        "        j = random.randrange(len(X_list))\n",
        "        Xv, yv = X_list[j], y_list[j]\n",
        "        if Xv.shape[0] <= T:\n",
        "            s = 0\n",
        "        else:\n",
        "            s = random.randrange(0, Xv.shape[0] - T + 1)\n",
        "        e = min(s + T, Xv.shape[0])\n",
        "        xs, ys = Xv[s:e], yv[s:e]\n",
        "        if xs.shape[0] < T:\n",
        "            pad = T - xs.shape[0]\n",
        "            Xb[i] = np.vstack([xs, np.tile(xs[-1:], (pad, 1))])\n",
        "            yb[i] = np.concatenate([ys, np.full((pad,), ys[-1], np.int64)])\n",
        "        else:\n",
        "            Xb[i], yb[i] = xs, ys\n",
        "    Xb = (Xb - mu.reshape(1, 1, -1)) / std.reshape(1, 1, -1)\n",
        "    return torch.from_numpy(Xb), torch.from_numpy(yb)\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer_full_video(model, Xv, mu, std, chunk=4096):\n",
        "    model.eval()\n",
        "    Xn = (Xv - mu) / std\n",
        "    T = Xn.shape[0]; C = CFG['num_classes']\n",
        "    probs = np.zeros((T, C), np.float32)\n",
        "    for s in range(0, T, chunk):\n",
        "        e = min(T, s + chunk)\n",
        "        xb = torch.from_numpy(Xn[s:e]).unsqueeze(0).to(device)\n",
        "        with torch.cuda.amp.autocast(enabled=True, dtype=torch.float16):\n",
        "            logits = model(xb).squeeze(0)\n",
        "        p = F.softmax(logits.float(), dim=-1).cpu().numpy().astype(np.float32)\n",
        "        probs[s:e] = p\n",
        "    probs = probs / np.clip(probs.sum(1, keepdims=True), 1e-8, None)\n",
        "    return probs\n",
        "\n",
        "# 5) Load all train videos into memory\n",
        "t0 = time.time()\n",
        "vids_all, Xs_all, Ys_all = [], [], []\n",
        "for vid in train_ids:\n",
        "    X, y = load_train_video(vid)\n",
        "    if X is None or y is None or X.shape[0] != y.shape[0]:\n",
        "        continue\n",
        "    vids_all.append(int(vid)); Xs_all.append(X); Ys_all.append(y)\n",
        "assert Xs_all and Ys_all, \"No training videos loaded from cache_v16\"\n",
        "n_feat = Xs_all[0].shape[1]\n",
        "print(f\"[Load] videos={len(vids_all)} n_feat={n_feat} elapsed={time.time()-t0:.1f}s\")\n",
        "\n",
        "# 6) 5-fold GroupKFold by video id\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "vids_arr = np.array(vids_all, np.int32)\n",
        "\n",
        "fold_times = []\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(vids_arr, groups=vids_arr), 1):\n",
        "    tf0 = time.time()\n",
        "    X_tr = [Xs_all[i] for i in tr_idx]; y_tr = [Ys_all[i] for i in tr_idx]\n",
        "    X_va = [Xs_all[i] for i in va_idx]; y_va = [Ys_all[i] for i in va_idx]\n",
        "    vids_tr = [vids_all[i] for i in tr_idx]; vids_va = [vids_all[i] for i in va_idx]\n",
        "    print(f\"\\n[Fold {fold}/5] tr={len(vids_tr)} va={len(vids_va)}\")\n",
        "\n",
        "    mu, std = compute_norm_stats(X_tr)\n",
        "    class_w = make_class_weights(y_tr, C=CFG['num_classes'], bg_weight=CFG['bg_weight'])\n",
        "\n",
        "    model = TCN1D(in_ch=n_feat, hid=128, num_classes=CFG['num_classes'], k=7, drop=0.30).to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=CFG['lr'], weight_decay=CFG['wd'])\n",
        "    sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=CFG['epochs'], eta_min=CFG['eta_min'])\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
        "\n",
        "    def val_loss():\n",
        "        model.eval(); tot = 0.0\n",
        "        with torch.no_grad():\n",
        "            for Xv, yv in zip(X_va, y_va):\n",
        "                xb = torch.from_numpy(((Xv - mu) / std)).unsqueeze(0).to(device)\n",
        "                yb = torch.from_numpy(yv).unsqueeze(0).to(device)\n",
        "                with torch.cuda.amp.autocast(enabled=True, dtype=torch.float16):\n",
        "                    logits = model(xb)\n",
        "                    loss = F.cross_entropy(\n",
        "                        logits.reshape(-1, CFG['num_classes']),\n",
        "                        yb.reshape(-1),\n",
        "                        weight=class_w,\n",
        "                        label_smoothing=CFG['label_smoothing']\n",
        "                    )\n",
        "                tot += float(loss.item())\n",
        "        return tot / max(1, len(X_va))\n",
        "\n",
        "    best_v, wait = 1e18, 0\n",
        "    for ep in range(1, CFG['epochs'] + 1):\n",
        "        t_ep = time.time()\n",
        "        model.train(); run_loss = 0.0\n",
        "        for _ in range(CFG['steps_per_epoch']):\n",
        "            xb, yb = sample_batch(X_tr, y_tr, mu, std, CFG['batch_size'], CFG['win_len'])\n",
        "            xb = xb.to(device); yb = yb.to(device)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=True, dtype=torch.float16):\n",
        "                logits = model(xb)\n",
        "                loss = F.cross_entropy(\n",
        "                    logits.reshape(-1, CFG['num_classes']),\n",
        "                    yb.reshape(-1),\n",
        "                    weight=class_w,\n",
        "                    label_smoothing=CFG['label_smoothing']\n",
        "                )\n",
        "            scaler.scale(loss).backward()\n",
        "            if CFG['grad_clip'] and CFG['grad_clip'] > 0:\n",
        "                scaler.unscale_(opt)\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), CFG['grad_clip'])\n",
        "            scaler.step(opt); scaler.update()\n",
        "            run_loss += float(loss.item())\n",
        "        sch.step()\n",
        "        vloss = val_loss()\n",
        "        print(f\"[Fold {fold}] Ep {ep:02d} tr={run_loss/CFG['steps_per_epoch']:.4f} val={vloss:.4f} lr={sch.get_last_lr()[0]:.6f} time={time.time()-t_ep:.1f}s\", flush=True)\n",
        "        if vloss < best_v - 1e-4:\n",
        "            best_v, wait = vloss, 0\n",
        "            torch.save({'model': model.state_dict(), 'mu': mu, 'std': std}, f'tcn_v16_fold{fold}.pt')\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= CFG['patience']:\n",
        "                print(f\"[Fold {fold}] Early stop at ep {ep} (best val={best_v:.4f})\")\n",
        "                break\n",
        "\n",
        "    # Load best and write OOF/test\n",
        "    ck = torch.load(f'tcn_v16_fold{fold}.pt', map_location=device)\n",
        "    model.load_state_dict(ck['model']); mu, std = ck['mu'], ck['std']\n",
        "    model.eval()\n",
        "\n",
        "    # OOF\n",
        "    saved = 0; t_oof = time.time()\n",
        "    for vid, Xv, yv in zip(vids_va, X_va, y_va):\n",
        "        P = infer_full_video(model, Xv, mu, std)\n",
        "        np.savez_compressed(os.path.join(OOF_DIR, f'oof_{vid:05d}.npz'),\n",
        "                            probs=P.astype(np.float32),\n",
        "                            y=yv.astype(np.int32),\n",
        "                            sid=np.array([int(vid)], dtype=np.int32))\n",
        "        saved += 1\n",
        "    print(f\"[Fold {fold}] OOF wrote {saved} files -> {OOF_DIR} in {time.time()-t_oof:.1f}s\")\n",
        "\n",
        "    # Test (this fold)\n",
        "    tdir = TEST_FOLD_DIRS[fold - 1]\n",
        "    tw = 0; t_test = time.time()\n",
        "    for sid in test_ids:\n",
        "        Xt = load_test_video(sid)\n",
        "        if Xt is None: continue\n",
        "        P = infer_full_video(model, Xt, mu, std)\n",
        "        np.savez_compressed(os.path.join(tdir, f'test_{sid:05d}.npz'), probs=P.astype(np.float32))\n",
        "        tw += 1\n",
        "    print(f\"[Fold {fold}] Test wrote {tw} files -> {tdir} in {time.time()-t_test:.1f}s\")\n",
        "\n",
        "    fold_times.append(time.time() - tf0)\n",
        "    del model; gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"[TCN] All folds done. Per-fold times (s): {[int(x) for x in fold_times]}\")\n",
        "print(\"[Next] Run Cell 40 to average test fold probs\")\n",
        "with open('tcn_metadata.txt', 'w') as f:\n",
        "    f.write(\"avg_out=./test_probs_v16_tcn/\\n\")\n",
        "    f.write(\"oof_dir=./oof_probs_v16_tcn/\\n\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (799.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 MB 510.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.19.1\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.1/7.1 MB 387.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.4/3.4 MB 512.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 341.9 MB/s eta 0:00:00\nCollecting filelock\n  Downloading https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting networkx\n  Downloading https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl (1.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.7/1.7 MB 535.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 483.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fsspec\n  Downloading https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl (177 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 177.6/177.6 KB 447.2 MB/s eta 0:00:00\nCollecting nvidia-nvtx-cu12==12.1.105\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 396.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 559.1 MB/s eta 0:00:00\nCollecting nvidia-nccl-cu12==2.20.5\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 468.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cublas-cu12==12.1.3.1\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 446.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cufft-cu12==11.0.2.54\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 544.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy\n  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.2/6.2 MB 542.1 MB/s eta 0:00:00\nCollecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 493.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 410.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jinja2\n  Downloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl (133 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 133.3/133.3 KB 421.7 MB/s eta 0:00:00\nCollecting typing-extensions>=4.8.0\n  Downloading https://download.pytorch.org/whl/typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 514.2 MB/s eta 0:00:00\nCollecting triton==3.0.0\n  Downloading https://download.pytorch.org/whl/triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 414.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-curand-cu12==10.3.2.106\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 207.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow!=8.3.*,>=5.3.0\n  Downloading https://download.pytorch.org/whl/pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4.4/4.4 MB 415.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading https://download.pytorch.org/whl/numpy-1.26.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 164.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvjitlink-cu12\n  Downloading https://download.pytorch.org/whl/nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 434.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\nCollecting mpmath<1.4,>=1.1.0\n  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 520.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-2.1.5 filelock-3.13.1 fsspec-2024.6.1 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 numpy-1.26.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pillow-11.0.0 sympy-1.13.3 torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121 triton-3.0.0 typing-extensions-4.12.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Torch] 2.4.1+cu121\nTue Sep 30 01:29:33 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.06             Driver Version: 550.144.06     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10-24Q                 On  |   00000002:00:00.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |    1475MiB /  24512MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n[CUDA avail] True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Load] videos=297 n_feat=193 elapsed=0.7s\n\n[Fold 1/5] tr=237 va=60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_5302/2089240777.py:180: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=True)\n/tmp/ipykernel_5302/2089240777.py:207: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=True, dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_5302/2089240777.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=True, dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] Ep 01 tr=2.4385 val=1.8029 lr=0.000976 time=2.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] Ep 02 tr=1.6583 val=1.5584 lr=0.000905 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] Ep 03 tr=1.4260 val=1.4692 lr=0.000796 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] Ep 04 tr=1.3123 val=1.4281 lr=0.000658 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] Ep 05 tr=1.2382 val=1.4091 lr=0.000505 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] Ep 06 tr=1.1695 val=1.3906 lr=0.000352 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] Ep 07 tr=1.1339 val=1.3912 lr=0.000214 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] Ep 08 tr=1.1024 val=1.3787 lr=0.000105 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] Ep 09 tr=1.0760 val=1.3754 lr=0.000034 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] Ep 10 tr=1.0638 val=1.3731 lr=0.000010 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] OOF wrote 60 files -> ./oof_probs_v16_tcn in 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_5302/2089240777.py:234: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  ck = torch.load(f'tcn_v16_fold{fold}.pt', map_location=device)\n/tmp/ipykernel_5302/2089240777.py:143: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=True, dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] Test wrote 95 files -> ./test_probs_v16_tcn_fold1 in 1.6s\n\n[Fold 2/5] tr=237 va=60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] Ep 01 tr=2.4150 val=1.7535 lr=0.000976 time=1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] Ep 02 tr=1.6621 val=1.4716 lr=0.000905 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] Ep 03 tr=1.4418 val=1.3747 lr=0.000796 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] Ep 04 tr=1.3100 val=1.3459 lr=0.000658 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] Ep 05 tr=1.2261 val=1.3347 lr=0.000505 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] Ep 06 tr=1.1657 val=1.3188 lr=0.000352 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] Ep 07 tr=1.1228 val=1.3200 lr=0.000214 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] Ep 08 tr=1.0890 val=1.3037 lr=0.000105 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] Ep 09 tr=1.0650 val=1.2949 lr=0.000034 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] Ep 10 tr=1.0560 val=1.2961 lr=0.000010 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] OOF wrote 60 files -> ./oof_probs_v16_tcn in 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] Test wrote 95 files -> ./test_probs_v16_tcn_fold2 in 0.5s\n\n[Fold 3/5] tr=238 va=59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 3] Ep 01 tr=2.3797 val=1.7797 lr=0.000976 time=1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 3] Ep 02 tr=1.6293 val=1.5244 lr=0.000905 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 3] Ep 03 tr=1.4366 val=1.4630 lr=0.000796 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 3] Ep 04 tr=1.3119 val=1.4142 lr=0.000658 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 3] Ep 05 tr=1.2163 val=1.4101 lr=0.000505 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 3] Ep 06 tr=1.1644 val=1.4011 lr=0.000352 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 3] Ep 07 tr=1.1180 val=1.4094 lr=0.000214 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 3] Ep 08 tr=1.0947 val=1.4013 lr=0.000105 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 3] Early stop at ep 8 (best val=1.4011)\n[Fold 3] OOF wrote 59 files -> ./oof_probs_v16_tcn in 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 3] Test wrote 95 files -> ./test_probs_v16_tcn_fold3 in 0.5s\n\n[Fold 4/5] tr=238 va=59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 4] Ep 01 tr=2.3880 val=1.7272 lr=0.000976 time=1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 4] Ep 02 tr=1.6542 val=1.5313 lr=0.000905 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 4] Ep 03 tr=1.4504 val=1.3977 lr=0.000796 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 4] Ep 04 tr=1.3395 val=1.3349 lr=0.000658 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 4] Ep 05 tr=1.2437 val=1.3052 lr=0.000505 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 4] Ep 06 tr=1.1764 val=1.3043 lr=0.000352 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 4] Ep 07 tr=1.1344 val=1.2866 lr=0.000214 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 4] Ep 08 tr=1.1073 val=1.2761 lr=0.000105 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 4] Ep 09 tr=1.0911 val=1.2700 lr=0.000034 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 4] Ep 10 tr=1.0664 val=1.2673 lr=0.000010 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 4] OOF wrote 59 files -> ./oof_probs_v16_tcn in 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 4] Test wrote 95 files -> ./test_probs_v16_tcn_fold4 in 0.5s\n\n[Fold 5/5] tr=238 va=59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 5] Ep 01 tr=2.4663 val=1.8318 lr=0.000976 time=1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 5] Ep 02 tr=1.6643 val=1.5545 lr=0.000905 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 5] Ep 03 tr=1.4426 val=1.4443 lr=0.000796 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 5] Ep 04 tr=1.3112 val=1.4103 lr=0.000658 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 5] Ep 05 tr=1.2265 val=1.3905 lr=0.000505 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 5] Ep 06 tr=1.1708 val=1.3955 lr=0.000352 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 5] Ep 07 tr=1.1277 val=1.3636 lr=0.000214 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 5] Ep 08 tr=1.0840 val=1.3644 lr=0.000105 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 5] Ep 09 tr=1.0796 val=1.3597 lr=0.000034 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 5] Ep 10 tr=1.0703 val=1.3541 lr=0.000010 time=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 5] OOF wrote 59 files -> ./oof_probs_v16_tcn in 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 5] Test wrote 95 files -> ./test_probs_v16_tcn_fold5 in 0.5s\n[TCN] All folds done. Per-fold times (s): [12, 9, 7, 8, 8]\n[Next] Run Cell 40 to average test fold probs\n"
          ]
        }
      ]
    },
    {
      "id": "4af40851-8365-434d-b51e-830274c8c5f7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 40: Average 5-fold test probabilities -> ./test_probs_v16_tcn/\n",
        "import os, glob, numpy as np, pandas as pd\n",
        "\n",
        "FOLD_DIRS = [f'./test_probs_v16_tcn_fold{i}' for i in range(1, 6)]\n",
        "OUT_DIR = './test_probs_v16_tcn'\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "test_ids = pd.read_csv('test.csv')['Id'].astype(int).tolist()\n",
        "wrote = 0\n",
        "for sid in test_ids:\n",
        "    Ps = []\n",
        "    for d in FOLD_DIRS:\n",
        "        fn = os.path.join(d, f'test_{sid:05d}.npz')\n",
        "        if os.path.exists(fn):\n",
        "            z = np.load(fn, allow_pickle=True)\n",
        "            if 'probs' in z: Ps.append(z['probs'].astype(np.float32))\n",
        "    if not Ps: continue\n",
        "    Lmin = min(p.shape[0] for p in Ps)\n",
        "    Ps = [p[:Lmin] for p in Ps]\n",
        "    P = np.mean(Ps, axis=0).astype(np.float32)\n",
        "    P = P / np.clip(P.sum(1, keepdims=True), 1e-8, None)\n",
        "    out_fn = os.path.join(OUT_DIR, f'test_{sid:05d}.npz')\n",
        "    np.savez_compressed(out_fn, probs=P)\n",
        "    wrote += 1\n",
        "\n",
        "print(f\"[Average] Wrote {wrote} test videos to {OUT_DIR}\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Average] Wrote 95 test videos to ./test_probs_v16_tcn\n"
          ]
        }
      ]
    },
    {
      "id": "0456e104-13e7-4c46-a772-413a46e81b52",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 41 (fast path): Decode averaged TCN test probs with fixed config -> submission.csv\n",
        "import os, glob, numpy as np, pandas as pd, json, time\n",
        "from scipy.special import logit, softmax\n",
        "from scipy.signal import convolve\n",
        "\n",
        "OOF_DIR = './oof_probs_v16_tcn'\n",
        "TEST_DIR = './test_probs_v16_tcn'\n",
        "assert os.path.isdir(OOF_DIR), f'Missing {OOF_DIR}. Train Cell 39 first.'\n",
        "assert os.path.isdir(TEST_DIR), f'Missing {TEST_DIR}. Run Cell 40 to average folds.'\n",
        "\n",
        "# Background index\n",
        "bg_idx = 0\n",
        "if os.path.exists('bg_index.json'):\n",
        "    try:\n",
        "        with open('bg_index.json','r') as f:\n",
        "            bg_idx = int(json.load(f).get('bg_idx', 0))\n",
        "    except Exception:\n",
        "        pass\n",
        "print(f'[Decoder] Using bg_idx={bg_idx}')\n",
        "\n",
        "def load_oof_labeled_map(oof_dir=OOF_DIR):\n",
        "    vids, Pmap, Ymap = [], {}, {}\n",
        "    files = sorted(glob.glob(os.path.join(oof_dir, 'oof_*.npz')))\n",
        "    for fn in files:\n",
        "        z = np.load(fn, allow_pickle=True)\n",
        "        sid = int(z['sid'][0]) if 'sid' in z else int(os.path.basename(fn).split('_')[1].split('.')[0])\n",
        "        if 'probs' in z:\n",
        "            Pmap[sid] = z['probs'].astype(np.float32)\n",
        "        elif 'P' in z:\n",
        "            Pmap[sid] = z['P'].astype(np.float32)\n",
        "        else:\n",
        "            continue\n",
        "        if 'y' in z:\n",
        "            Ymap[sid] = z['y'].astype(np.int32)\n",
        "        elif 'labels' in z:\n",
        "            Ymap[sid] = z['labels'].astype(np.int32)\n",
        "        else:\n",
        "            continue\n",
        "        vids.append(sid)\n",
        "    return sorted(vids), Pmap, Ymap\n",
        "\n",
        "def fit_per_class_temperature(P_oof, y_oof, temp_grid):\n",
        "    C = P_oof.shape[1]; Tcls = np.ones(C, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        pc = np.clip(P_oof[:, c], 1e-6, 1-1e-6); z = logit(pc)\n",
        "        yc = (y_oof == c).astype(np.float32)\n",
        "        best, bestT = 1e18, 1.0\n",
        "        for T in temp_grid:\n",
        "            p = 1.0/(1.0 + np.exp(-z/float(T)))\n",
        "            nll = -(yc*np.log(np.clip(p,1e-6,1))).mean() - ((1-yc)*np.log(np.clip(1-p,1e-6,1))).mean()\n",
        "            if nll < best:\n",
        "                best, bestT = float(nll), float(T)\n",
        "        Tcls[c] = bestT\n",
        "    return Tcls.astype(np.float32)\n",
        "\n",
        "def to_calibrated_logprobs(P, Tcls, bg_bias=0.25, smooth=3, g=0.95):\n",
        "    P = np.clip(P, 1e-6, 1-1e-6).astype(np.float32)\n",
        "    Z = logit(P).astype(np.float32) / (Tcls.reshape(1, -1) * float(g))\n",
        "    Pcal = softmax(Z, axis=1).astype(np.float32)\n",
        "    Pcal /= np.clip(Pcal.sum(1, keepdims=True), 1e-6, None)\n",
        "    L = np.log(np.clip(Pcal, 1e-6, 1.0)).astype(np.float32)\n",
        "    L[:, bg_idx] += float(bg_bias)\n",
        "    if smooth and smooth > 1:\n",
        "        k = np.ones(int(smooth), np.float32) / float(smooth)\n",
        "        L = np.stack([convolve(L[:, i], k, mode='same') for i in range(L.shape[1])], 1).astype(np.float32)\n",
        "    return L\n",
        "\n",
        "def segment_exactK(margin, K=20, min_len=1):\n",
        "    T = int(len(margin)); min_len = int(max(1, min_len))\n",
        "    while K*min_len > T and min_len > 1: min_len -= 1\n",
        "    if K*min_len > T: K = min(K, T); min_len = 1\n",
        "    pref = np.concatenate([[0.0], np.cumsum(margin, 0).astype(np.float32)])\n",
        "    dp = -1e18*np.ones((K+1, T+1), np.float32); bt = -np.ones((K+1, T+1), np.int32)\n",
        "    dp[0, 0] = 0.0\n",
        "    for k in range(1, K+1):\n",
        "        start_min = (k-1)*min_len\n",
        "        for t in range(k*min_len, T+1):\n",
        "            s_lo = max(start_min, t - (T - (K-k)*min_len))\n",
        "            bestv, bests = -1e18, -1\n",
        "            for s in range(s_lo, t-min_len+1):\n",
        "                v = dp[k-1, s] + (pref[t] - pref[s])\n",
        "                if v > bestv: bestv, bests = v, s\n",
        "            dp[k, t] = bestv; bt[k, t] = bests\n",
        "    bounds = []; k, t = K, T\n",
        "    while k > 0:\n",
        "        s = int(bt[k, t]); bounds.append((s, t)); t = s; k -= 1\n",
        "    bounds.reverse(); return bounds\n",
        "\n",
        "def decode_with_cost_logprob(L, alpha=0.85, cost_mode='trimmed_mean_20'):\n",
        "    T, C = L.shape; K = 20\n",
        "    min_len = max(2, int(alpha * T / float(K)))\n",
        "    cols = [i for i in range(C) if i != bg_idx]\n",
        "    margin = (L[:, cols].max(1) - L[:, bg_idx]).astype(np.float32)\n",
        "    segs = segment_exactK(margin, K=K, min_len=min_len)\n",
        "    Cmat = np.zeros((len(segs), K), np.float32)\n",
        "    for j, (s, e) in enumerate(segs):\n",
        "        seg = L[s:e, cols]\n",
        "        if cost_mode == 'trimmed_mean_20':\n",
        "            n = seg.shape[0]; lo = int(0.1*n); hi = max(lo+1, int(0.9*n)); agg = np.sort(seg, axis=0)[lo:hi].mean(0)\n",
        "        elif cost_mode == 'median':\n",
        "            agg = np.median(seg, axis=0)\n",
        "        else:\n",
        "            agg = seg.mean(0)\n",
        "        Cmat[j, :] = -agg\n",
        "    # Hungarian\n",
        "    from scipy.optimize import linear_sum_assignment\n",
        "    r, c = linear_sum_assignment(Cmat)\n",
        "    order_nonbg_idx = [int(c[j]) for j in np.argsort(r)]\n",
        "    order_cls = [idx+1 for idx in order_nonbg_idx]\n",
        "    return order_cls\n",
        "\n",
        "# 1) Load OOF labeled frames for temperature calibration\n",
        "vids, Pmap, Ymap = load_oof_labeled_map(OOF_DIR)\n",
        "assert vids, f'No OOF files in {OOF_DIR}'\n",
        "P_all = np.concatenate([Pmap[v] for v in vids], 0)\n",
        "y_all = np.concatenate([Ymap[v] for v in vids], 0)\n",
        "temp_grid = np.linspace(0.7, 1.6, 12).astype(np.float32)\n",
        "Tcls = fit_per_class_temperature(P_all, y_all, temp_grid)\n",
        "print('[Calib] Tcls first10:', np.round(Tcls[:10], 3).tolist())\n",
        "\n",
        "# 2) Fixed best config (per expert): S5 trimmed_mean_20, alpha=0.85, bg_bias=0.20, smooth=3, g=0.95\n",
        "cfg = dict(mode='trimmed_mean_20', alpha=0.85, bb=0.20, smooth=3, g=0.95)\n",
        "\n",
        "# 3) Decode averaged test probs with fixed config\n",
        "test_ids = pd.read_csv('test.csv')['Id'].astype(int).tolist()\n",
        "rows = []; wrote = 0\n",
        "t0 = time.time()\n",
        "for sid in sorted(test_ids):\n",
        "    fn = os.path.join(TEST_DIR, f'test_{sid:05d}.npz')\n",
        "    if not os.path.exists(fn):\n",
        "        continue\n",
        "    z = np.load(fn, allow_pickle=True)\n",
        "    if 'probs' not in z: continue\n",
        "    P = z['probs'].astype(np.float32)\n",
        "    L = to_calibrated_logprobs(P, Tcls, bg_bias=cfg['bb'], smooth=cfg['smooth'], g=cfg['g'])\n",
        "    seq = decode_with_cost_logprob(L, alpha=cfg['alpha'], cost_mode=cfg['mode'])\n",
        "    # Rescue to permutation 1..20\n",
        "    seen, dup_idx = set(), []\n",
        "    for i, x in enumerate(seq):\n",
        "        if x in seen: dup_idx.append(i)\n",
        "        else: seen.add(x)\n",
        "    missing = [k for k in range(1, 21) if k not in seen]\n",
        "    for j, i in enumerate(dup_idx):\n",
        "        if j < len(missing): seq[i] = missing[j]\n",
        "    if len(seq) < 20: seq += [k for k in range(1, 21) if k not in set(seq)]\n",
        "    seq = seq[:20]\n",
        "    rows.append((sid, ' '.join(map(str, seq))))\n",
        "    wrote += 1\n",
        "print(f'[Decode] wrote sequences for {wrote}/{len(test_ids)} test videos in {time.time()-t0:.1f}s')\n",
        "\n",
        "sub = pd.DataFrame(rows, columns=['Id','Sequence']).sort_values('Id')\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('[Write] submission.csv', sub.shape, 'cfg=', cfg)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Decoder] Using bg_idx=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Calib] Tcls first10: [1.600000023841858, 0.9449999928474426, 0.9449999928474426, 0.9449999928474426, 0.9449999928474426, 0.9449999928474426, 0.9449999928474426, 0.9449999928474426, 0.9449999928474426, 0.9449999928474426]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Decode] wrote sequences for 95/95 test videos in 35.5s\n[Write] submission.csv (95, 2) cfg= {'mode': 'trimmed_mean_20', 'alpha': 0.85, 'bb': 0.2, 'smooth': 3, 'g': 0.95}\n"
          ]
        }
      ]
    },
    {
      "id": "164cf5d5-c37c-4203-8ae8-848183a655e8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 42: Blend TCN + XGB per-frame probs (v16), OOF-gated small grid, decode test -> submission.csv\n",
        "import os, glob, time, json, numpy as np, pandas as pd\n",
        "from scipy.special import logit, softmax\n",
        "from scipy.signal import convolve\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "OOF_TCN_DIR = './oof_probs_v16_tcn'\n",
        "OOF_XGB_DIR = './oof_probs_v16_labeled'  # labeled OOF for v16 XGB\n",
        "TEST_TCN_DIR = './test_probs_v16_tcn'\n",
        "TEST_XGB_DIR = './test_probs_v16'\n",
        "\n",
        "# Background index\n",
        "bg_idx = 0\n",
        "if os.path.exists('bg_index.json'):\n",
        "    try:\n",
        "        with open('bg_index.json','r') as f:\n",
        "            bg_idx = int(json.load(f).get('bg_idx', 0))\n",
        "    except Exception:\n",
        "        pass\n",
        "print(f'[Blend] Using bg_idx={bg_idx}')\n",
        "\n",
        "def load_oof_map(oof_dir):\n",
        "    vids, Pmap, Ymap = [], {}, {}\n",
        "    files = sorted(glob.glob(os.path.join(oof_dir, 'oof_*.npz')))\n",
        "    for fn in files:\n",
        "        z = np.load(fn, allow_pickle=True)\n",
        "        sid = int(z['sid'][0]) if 'sid' in z else int(os.path.basename(fn).split('_')[1].split('.')[0])\n",
        "        P = z['probs'] if 'probs' in z else (z['P'] if 'P' in z else None)\n",
        "        y = z['y'] if 'y' in z else (z['labels'] if 'labels' in z else None)\n",
        "        if P is None or y is None: continue\n",
        "        Pmap[int(sid)] = P.astype(np.float32); Ymap[int(sid)] = y.astype(np.int32); vids.append(int(sid))\n",
        "    return sorted(vids), Pmap, Ymap\n",
        "\n",
        "def collapse_y_to_sequence(y):\n",
        "    seq = []; prev = -1\n",
        "    for t in y:\n",
        "        ti = int(t)\n",
        "        if ti != prev and ti > 0: seq.append(ti)\n",
        "        prev = ti\n",
        "    return seq[:20]\n",
        "\n",
        "def fit_per_class_temperature(P_oof, y_oof, temp_grid):\n",
        "    C = P_oof.shape[1]; Tcls = np.ones(C, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        pc = np.clip(P_oof[:, c], 1e-6, 1-1e-6); z = logit(pc)\n",
        "        yc = (y_oof == c).astype(np.float32)\n",
        "        best, bestT = 1e18, 1.0\n",
        "        for T in temp_grid:\n",
        "            p = 1.0/(1.0 + np.exp(-z/float(T)))\n",
        "            nll = -(yc*np.log(np.clip(p,1e-6,1))).mean() - ((1-yc)*np.log(np.clip(1-p,1e-6,1))).mean()\n",
        "            if nll < best: best, bestT = float(nll), float(T)\n",
        "        Tcls[c] = bestT\n",
        "    return Tcls.astype(np.float32)\n",
        "\n",
        "def to_calibrated_logprobs(P, Tcls, bg_bias=0.25, smooth=3, g=0.95):\n",
        "    P = np.clip(P, 1e-6, 1-1e-6).astype(np.float32)\n",
        "    Z = logit(P).astype(np.float32) / (Tcls.reshape(1, -1) * float(g))\n",
        "    Pcal = softmax(Z, axis=1).astype(np.float32)\n",
        "    Pcal /= np.clip(Pcal.sum(1, keepdims=True), 1e-6, None)\n",
        "    L = np.log(np.clip(Pcal, 1e-6, 1.0)).astype(np.float32)\n",
        "    L[:, bg_idx] += float(bg_bias)\n",
        "    if smooth and smooth > 1:\n",
        "        k = np.ones(int(smooth), np.float32) / float(smooth)\n",
        "        L = np.stack([convolve(L[:, i], k, mode='same') for i in range(L.shape[1])], 1).astype(np.float32)\n",
        "    return L\n",
        "\n",
        "def segment_exactK(margin, K=20, min_len=1):\n",
        "    T = int(len(margin)); min_len = int(max(1, min_len))\n",
        "    while K*min_len > T and min_len > 1: min_len -= 1\n",
        "    if K*min_len > T: K = min(K, T); min_len = 1\n",
        "    pref = np.concatenate([[0.0], np.cumsum(margin, 0).astype(np.float32)])\n",
        "    dp = -1e18*np.ones((K+1, T+1), np.float32); bt = -np.ones((K+1, T+1), np.int32)\n",
        "    dp[0, 0] = 0.0\n",
        "    for k in range(1, K+1):\n",
        "        start_min = (k-1)*min_len\n",
        "        for t in range(k*min_len, T+1):\n",
        "            s_lo = max(start_min, t - (T - (K-k)*min_len))\n",
        "            bestv, bests = -1e18, -1\n",
        "            for s in range(s_lo, t-min_len+1):\n",
        "                v = dp[k-1, s] + (pref[t] - pref[s])\n",
        "                if v > bestv: bestv, bests = v, s\n",
        "            dp[k, t] = bestv; bt[k, t] = bests\n",
        "    bounds = []; k, t = K, T\n",
        "    while k > 0:\n",
        "        s = int(bt[k, t]); bounds.append((s, t)); t = s; k -= 1\n",
        "    bounds.reverse(); return bounds\n",
        "\n",
        "def decode_with_cost_logprob(L, alpha=0.85, cost_mode='trimmed_mean_20'):\n",
        "    T, C = L.shape; K = 20\n",
        "    min_len = max(2, int(alpha * T / float(K)))\n",
        "    cols = [i for i in range(C) if i != bg_idx]\n",
        "    margin = (L[:, cols].max(1) - L[:, bg_idx]).astype(np.float32)\n",
        "    segs = segment_exactK(margin, K=K, min_len=min_len)\n",
        "    Cmat = np.zeros((len(segs), K), np.float32)\n",
        "    for j, (s, e) in enumerate(segs):\n",
        "        seg = L[s:e, cols]\n",
        "        if cost_mode == 'trimmed_mean_20':\n",
        "            n = seg.shape[0]; lo = int(0.1*n); hi = max(lo+1, int(0.9*n)); agg = np.sort(seg, axis=0)[lo:hi].mean(0)\n",
        "        elif cost_mode == 'median':\n",
        "            agg = np.median(seg, axis=0)\n",
        "        else:\n",
        "            agg = seg.mean(0)\n",
        "        Cmat[j, :] = -agg\n",
        "    r, c = linear_sum_assignment(Cmat)\n",
        "    order_nonbg_idx = [int(c[j]) for j in np.argsort(r)]\n",
        "    order_cls = [idx+1 for idx in order_nonbg_idx]\n",
        "    return order_cls\n",
        "\n",
        "def levenshtein(a, b):\n",
        "    n, m = len(a), len(b); dp = list(range(m+1))\n",
        "    for i in range(1, n+1):\n",
        "        prev, dp[0] = dp[0], i\n",
        "        for j in range(1, m+1):\n",
        "            cur = min(dp[j]+1, dp[j-1]+1, prev + (0 if a[i-1]==b[j-1] else 1))\n",
        "            prev, dp[j] = dp[j], cur\n",
        "    return dp[m]\n",
        "\n",
        "# 1) Load OOF for TCN and XGB(v16), intersect videos\n",
        "vids_t, P_t, Y_t = load_oof_map(OOF_TCN_DIR)\n",
        "vids_x, P_x, Y_x = load_oof_map(OOF_XGB_DIR)\n",
        "common = sorted(list(set(vids_t).intersection(vids_x)))\n",
        "assert common, 'No overlapping OOF videos between TCN and XGB'\n",
        "print('[Blend] OOF intersect vids:', len(common))\n",
        "\n",
        "# Pre-build GT sequences from y-collapsed\n",
        "seq_gt = {vid: collapse_y_to_sequence(Y_t.get(vid, Y_x.get(vid))) for vid in common}\n",
        "\n",
        "# 2) Small OOF-gated grid over blend weight and decoder params\n",
        "weights = [0.6, 0.7, 0.8]  # w_tcn\n",
        "modes = ['median', 'trimmed_mean_20']\n",
        "gs = [0.90, 0.95]\n",
        "bg_biases = [0.20, 0.25]\n",
        "smooths = [3]\n",
        "alpha = 0.85\n",
        "temp_grid = np.linspace(0.7, 1.6, 12).astype(np.float32)\n",
        "\n",
        "def eval_cfg(w_tcn, mode, g, bb, smooth):\n",
        "    # Build blended OOF frames and fit temps on all frames\n",
        "    X_all, y_all = [], []\n",
        "    for vid in common:\n",
        "        Pt, Px = P_t[vid], P_x[vid]\n",
        "        Lmin = min(Pt.shape[0], Px.shape[0])\n",
        "        Pb = (w_tcn*Pt[:Lmin] + (1.0-w_tcn)*Px[:Lmin]).astype(np.float32)\n",
        "        yv = Y_t.get(vid, Y_x.get(vid))[:Lmin]\n",
        "        X_all.append(Pb); y_all.append(yv)\n",
        "    P_all = np.concatenate(X_all, 0); y_all = np.concatenate(y_all, 0)\n",
        "    Tcls = fit_per_class_temperature(P_all, y_all, temp_grid)\n",
        "    # OOF-Lev over common vids\n",
        "    dists = []\n",
        "    for vid in common:\n",
        "        Pt, Px = P_t[vid], P_x[vid]\n",
        "        Lmin = min(Pt.shape[0], Px.shape[0])\n",
        "        Pb = (w_tcn*Pt[:Lmin] + (1.0-w_tcn)*Px[:Lmin]).astype(np.float32)\n",
        "        L = to_calibrated_logprobs(Pb, Tcls, bg_bias=bb, smooth=smooth, g=g)\n",
        "        pred = decode_with_cost_logprob(L, alpha=alpha, cost_mode=mode)\n",
        "        gt = seq_gt.get(vid, [])\n",
        "        if len(gt) < 1: continue\n",
        "        gg = gt[:20]; pp = pred[:len(gg)]\n",
        "        dists.append(levenshtein(pp, gg)/float(len(gg)))\n",
        "    return (float(np.mean(dists)) if dists else 1.0), len(dists)\n",
        "\n",
        "best = (1.0, None)\n",
        "t0 = time.time()\n",
        "for w in weights:\n",
        "    for mode in modes:\n",
        "        for g in gs:\n",
        "            for bb in bg_biases:\n",
        "                for s in smooths:\n",
        "                    score, n = eval_cfg(w, mode, g, bb, s)\n",
        "                    print(f\"[OOF-Blend] w_tcn={w} mode={mode} g={g} bb={bb} s={s} -> {score:.5f} (n={n})\")\n",
        "                    if score < best[0]:\n",
        "                        best = (score, dict(w=w, mode=mode, g=g, bb=bb, s=s))\n",
        "print('[OOF-Blend] Best:', best, 'elapsed', f'{time.time()-t0:.1f}s')\n",
        "\n",
        "# 3) Decode TEST with best config using blended per-frame probs from TEST_TCN_DIR + TEST_XGB_DIR\n",
        "cfg = best[1] if best[1] is not None else dict(w=0.7, mode='trimmed_mean_20', g=0.95, bb=0.20, s=3)\n",
        "test_ids = pd.read_csv('test.csv')['Id'].astype(int).tolist()\n",
        "\n",
        "def load_test_probs_map(d, ids):\n",
        "    m = {}\n",
        "    for vid in ids:\n",
        "        for pat in (f'test_{vid:05d}.npz', f'{vid:05d}.npz', f'{vid}.npz'):\n",
        "            fn = os.path.join(d, pat)\n",
        "            if os.path.exists(fn):\n",
        "                z = np.load(fn, allow_pickle=True)\n",
        "                if 'probs' in z: m[vid] = z['probs'].astype(np.float32)\n",
        "                elif 'P' in z: m[vid] = z['P'].astype(np.float32)\n",
        "                break\n",
        "    return m\n",
        "\n",
        "M_t = load_test_probs_map(TEST_TCN_DIR, test_ids)\n",
        "M_x = load_test_probs_map(TEST_XGB_DIR, test_ids)\n",
        "ids_common = sorted(list(set(M_t.keys()).intersection(M_x.keys())))\n",
        "print('[Blend-Test] vids common:', len(ids_common))\n",
        "\n",
        "# Refit temps on blended OOF with chosen weight\n",
        "X_all, y_all = [], []\n",
        "for vid in common:\n",
        "    Pt, Px = P_t[vid], P_x[vid]\n",
        "    Lmin = min(Pt.shape[0], Px.shape[0])\n",
        "    Pb = (cfg['w']*Pt[:Lmin] + (1.0-cfg['w'])*Px[:Lmin]).astype(np.float32)\n",
        "    yv = Y_t.get(vid, Y_x.get(vid))[:Lmin]\n",
        "    X_all.append(Pb); y_all.append(yv)\n",
        "P_all = np.concatenate(X_all, 0); y_all = np.concatenate(y_all, 0)\n",
        "Tcls_best = fit_per_class_temperature(P_all, y_all, np.linspace(0.7, 1.6, 12).astype(np.float32))\n",
        "\n",
        "rows = []; wrote = 0\n",
        "for vid in sorted(test_ids):\n",
        "    Pt = M_t.get(vid); Px = M_x.get(vid)\n",
        "    if Pt is None or Px is None: continue\n",
        "    Lmin = min(Pt.shape[0], Px.shape[0])\n",
        "    Pb = (cfg['w']*Pt[:Lmin] + (1.0-cfg['w'])*Px[:Lmin]).astype(np.float32)\n",
        "    L = to_calibrated_logprobs(Pb, Tcls_best, bg_bias=cfg['bb'], smooth=cfg['s'], g=cfg['g'])\n",
        "    seq = decode_with_cost_logprob(L, alpha=alpha, cost_mode=cfg['mode'])\n",
        "    # rescue permutation-20\n",
        "    seen, dup_idx = set(), []\n",
        "    for i, x in enumerate(seq):\n",
        "        if x in seen: dup_idx.append(i)\n",
        "        else: seen.add(x)\n",
        "    missing = [k for k in range(1, 21) if k not in seen]\n",
        "    for j, i in enumerate(dup_idx):\n",
        "        if j < len(missing): seq[i] = missing[j]\n",
        "    if len(seq) < 20: seq += [k for k in range(1,21) if k not in set(seq)]\n",
        "    seq = seq[:20]\n",
        "    rows.append((vid, ' '.join(map(str, seq))))\n",
        "    wrote += 1\n",
        "sub = pd.DataFrame(rows, columns=['Id','Sequence']).sort_values('Id')\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('[Write] submission.csv', sub.shape, 'best_cfg=', cfg)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Blend] Using bg_idx=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Blend] OOF intersect vids: 297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Blend] w_tcn=0.6 mode=median g=0.9 bb=0.2 s=3 -> 0.40089 (n=297)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Blend] w_tcn=0.6 mode=median g=0.9 bb=0.25 s=3 -> 0.40089 (n=297)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Blend] w_tcn=0.6 mode=median g=0.95 bb=0.2 s=3 -> 0.40072 (n=297)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Blend] w_tcn=0.6 mode=median g=0.95 bb=0.25 s=3 -> 0.40072 (n=297)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Blend] w_tcn=0.6 mode=trimmed_mean_20 g=0.9 bb=0.2 s=3 -> 0.38635 (n=297)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Blend] w_tcn=0.6 mode=trimmed_mean_20 g=0.9 bb=0.25 s=3 -> 0.38635 (n=297)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Blend] w_tcn=0.6 mode=trimmed_mean_20 g=0.95 bb=0.2 s=3 -> 0.38652 (n=297)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Blend] w_tcn=0.6 mode=trimmed_mean_20 g=0.95 bb=0.25 s=3 -> 0.38652 (n=297)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Blend] w_tcn=0.7 mode=median g=0.9 bb=0.2 s=3 -> 0.40343 (n=297)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Blend] w_tcn=0.7 mode=median g=0.9 bb=0.25 s=3 -> 0.40343 (n=297)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Blend] w_tcn=0.7 mode=median g=0.95 bb=0.2 s=3 -> 0.40444 (n=297)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Blend] w_tcn=0.7 mode=median g=0.95 bb=0.25 s=3 -> 0.40444 (n=297)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Blend] w_tcn=0.7 mode=trimmed_mean_20 g=0.9 bb=0.2 s=3 -> 0.38701 (n=297)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Blend] w_tcn=0.7 mode=trimmed_mean_20 g=0.9 bb=0.25 s=3 -> 0.38701 (n=297)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Blend] w_tcn=0.7 mode=trimmed_mean_20 g=0.95 bb=0.2 s=3 -> 0.38684 (n=297)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF-Blend] w_tcn=0.7 mode=trimmed_mean_20 g=0.95 bb=0.25 s=3 -> 0.38684 (n=297)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 169\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bb \u001b[38;5;129;01min\u001b[39;00m bg_biases:\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m smooths:\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m         score, n = \u001b[43meval_cfg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[OOF-Blend] w_tcn=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m mode=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m g=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m bb=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbb\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m s=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (n=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    171\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m score < best[\u001b[32m0\u001b[39m]:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 155\u001b[39m, in \u001b[36meval_cfg\u001b[39m\u001b[34m(w_tcn, mode, g, bb, smooth)\u001b[39m\n\u001b[32m    153\u001b[39m Pb = (w_tcn*Pt[:Lmin] + (\u001b[32m1.0\u001b[39m-w_tcn)*Px[:Lmin]).astype(np.float32)\n\u001b[32m    154\u001b[39m L = to_calibrated_logprobs(Pb, Tcls, bg_bias=bb, smooth=smooth, g=g)\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m pred = \u001b[43mdecode_with_cost_logprob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcost_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m gt = seq_gt.get(vid, [])\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(gt) < \u001b[32m1\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36mdecode_with_cost_logprob\u001b[39m\u001b[34m(L, alpha, cost_mode)\u001b[39m\n\u001b[32m     91\u001b[39m cols = [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(C) \u001b[38;5;28;01mif\u001b[39;00m i != bg_idx]\n\u001b[32m     92\u001b[39m margin = (L[:, cols].max(\u001b[32m1\u001b[39m) - L[:, bg_idx]).astype(np.float32)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m segs = \u001b[43msegment_exactK\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmargin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m=\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m Cmat = np.zeros((\u001b[38;5;28mlen\u001b[39m(segs), K), np.float32)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j, (s, e) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(segs):\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line -1\u001b[39m, in \u001b[36msegment_exactK\u001b[39m\u001b[34m(margin, K, min_len)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "ce92a473-ede7-480e-8486-e86a317fba92",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 43: FAST WRITE blended TCN+XGB submission using best-seen cfg (w=0.6, trimmed_mean_20, g=0.90, bb=0.20, s=3)\n",
        "import os, glob, numpy as np, pandas as pd, json, time\n",
        "from scipy.special import logit, softmax\n",
        "from scipy.signal import convolve\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "OOF_TCN_DIR = './oof_probs_v16_tcn'\n",
        "OOF_XGB_DIR = './oof_probs_v16_labeled'\n",
        "TEST_TCN_DIR = './test_probs_v16_tcn'\n",
        "TEST_XGB_DIR = './test_probs_v16'\n",
        "\n",
        "# Config from best OOF-Blend observed\n",
        "w_tcn = 0.6\n",
        "cfg = dict(mode='trimmed_mean_20', g=0.90, bb=0.20, smooth=3, alpha=0.85)\n",
        "\n",
        "# Background index\n",
        "bg_idx = 0\n",
        "if os.path.exists('bg_index.json'):\n",
        "    try:\n",
        "        with open('bg_index.json','r') as f:\n",
        "            bg_idx = int(json.load(f).get('bg_idx', 0))\n",
        "    except Exception:\n",
        "        pass\n",
        "print(f'[FAST-BLEND] bg_idx={bg_idx} cfg={cfg} w_tcn={w_tcn}')\n",
        "\n",
        "def load_oof_map(oof_dir):\n",
        "    vids, Pmap, Ymap = [], {}, {}\n",
        "    for fn in sorted(glob.glob(os.path.join(oof_dir, 'oof_*.npz'))):\n",
        "        z = np.load(fn, allow_pickle=True)\n",
        "        sid = int(z['sid'][0]) if 'sid' in z else int(os.path.basename(fn).split('_')[1].split('.')[0])\n",
        "        P = z['probs'] if 'probs' in z else (z['P'] if 'P' in z else None)\n",
        "        y = z['y'] if 'y' in z else (z['labels'] if 'labels' in z else None)\n",
        "        if P is None or y is None: continue\n",
        "        vids.append(int(sid)); Pmap[int(sid)] = P.astype(np.float32); Ymap[int(sid)] = y.astype(np.int32)\n",
        "    return sorted(vids), Pmap, Ymap\n",
        "\n",
        "def fit_per_class_temperature(P_oof, y_oof, temp_grid):\n",
        "    C = P_oof.shape[1]; Tcls = np.ones(C, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        pc = np.clip(P_oof[:, c], 1e-6, 1-1e-6); z = logit(pc); yc = (y_oof == c).astype(np.float32)\n",
        "        best, bestT = 1e18, 1.0\n",
        "        for T in temp_grid:\n",
        "            p = 1.0/(1.0 + np.exp(-z/float(T)))\n",
        "            nll = -(yc*np.log(np.clip(p,1e-6,1))).mean() - ((1-yc)*np.log(np.clip(1-p,1e-6,1))).mean()\n",
        "            if nll < best: best, bestT = float(nll), float(T)\n",
        "        Tcls[c] = bestT\n",
        "    return Tcls.astype(np.float32)\n",
        "\n",
        "def to_calibrated_logprobs(P, Tcls, bg_bias=0.25, smooth=3, g=0.95):\n",
        "    P = np.clip(P, 1e-6, 1-1e-6).astype(np.float32)\n",
        "    Z = logit(P).astype(np.float32) / (Tcls.reshape(1, -1) * float(g))\n",
        "    Pcal = softmax(Z, axis=1).astype(np.float32)\n",
        "    Pcal /= np.clip(Pcal.sum(1, keepdims=True), 1e-6, None)\n",
        "    L = np.log(np.clip(Pcal, 1e-6, 1.0)).astype(np.float32)\n",
        "    L[:, bg_idx] += float(bg_bias)\n",
        "    if smooth and smooth > 1:\n",
        "        k = np.ones(int(smooth), np.float32) / float(smooth)\n",
        "        L = np.stack([convolve(L[:, i], k, mode='same') for i in range(L.shape[1])], 1).astype(np.float32)\n",
        "    return L\n",
        "\n",
        "def segment_exactK(margin, K=20, min_len=1):\n",
        "    T = int(len(margin)); min_len = int(max(1, min_len))\n",
        "    while K*min_len > T and min_len > 1: min_len -= 1\n",
        "    if K*min_len > T: K = min(K, T); min_len = 1\n",
        "    pref = np.concatenate([[0.0], np.cumsum(margin, 0).astype(np.float32)])\n",
        "    dp = -1e18*np.ones((K+1, T+1), np.float32); bt = -np.ones((K+1, T+1), np.int32)\n",
        "    dp[0, 0] = 0.0\n",
        "    for k in range(1, K+1):\n",
        "        start_min = (k-1)*min_len\n",
        "        for t in range(k*min_len, T+1):\n",
        "            s_lo = max(start_min, t - (T - (K-k)*min_len))\n",
        "            bestv, bests = -1e18, -1\n",
        "            for s in range(s_lo, t-min_len+1):\n",
        "                v = dp[k-1, s] + (pref[t] - pref[s])\n",
        "                if v > bestv: bestv, bests = v, s\n",
        "            dp[k, t] = bestv; bt[k, t] = bests\n",
        "    bounds = []; k, t = K, T\n",
        "    while k > 0:\n",
        "        s = int(bt[k, t]); bounds.append((s, t)); t = s; k -= 1\n",
        "    bounds.reverse(); return bounds\n",
        "\n",
        "def decode_with_cost_logprob(L, alpha=0.85, cost_mode='trimmed_mean_20'):\n",
        "    T, C = L.shape; K = 20\n",
        "    min_len = max(2, int(alpha * T / float(K)))\n",
        "    cols = [i for i in range(C) if i != bg_idx]\n",
        "    margin = (L[:, cols].max(1) - L[:, bg_idx]).astype(np.float32)\n",
        "    segs = segment_exactK(margin, K=K, min_len=min_len)\n",
        "    Cmat = np.zeros((len(segs), K), np.float32)\n",
        "    for j, (s, e) in enumerate(segs):\n",
        "        seg = L[s:e, cols]\n",
        "        if cost_mode == 'trimmed_mean_20':\n",
        "            n = seg.shape[0]; lo = int(0.1*n); hi = max(lo+1, int(0.9*n)); agg = np.sort(seg, axis=0)[lo:hi].mean(0)\n",
        "        elif cost_mode == 'median':\n",
        "            agg = np.median(seg, axis=0)\n",
        "        else:\n",
        "            agg = seg.mean(0)\n",
        "        Cmat[j, :] = -agg\n",
        "    r, c = linear_sum_assignment(Cmat)\n",
        "    order_nonbg_idx = [int(c[j]) for j in np.argsort(r)]\n",
        "    return [idx+1 for idx in order_nonbg_idx]\n",
        "\n",
        "def load_test_probs_map(d, ids):\n",
        "    m = {}\n",
        "    for vid in ids:\n",
        "        for pat in (f'test_{vid:05d}.npz', f'{vid:05d}.npz', f'{vid}.npz'):\n",
        "            fn = os.path.join(d, pat)\n",
        "            if os.path.exists(fn):\n",
        "                z = np.load(fn, allow_pickle=True)\n",
        "                if 'probs' in z: m[vid] = z['probs'].astype(np.float32)\n",
        "                elif 'P' in z: m[vid] = z['P'].astype(np.float32)\n",
        "                break\n",
        "    return m\n",
        "\n",
        "# 1) Fit temps on blended OOF with chosen weight\n",
        "vids_t, P_t, Y_t = load_oof_map(OOF_TCN_DIR)\n",
        "vids_x, P_x, Y_x = load_oof_map(OOF_XGB_DIR)\n",
        "common = sorted(list(set(vids_t).intersection(vids_x)))\n",
        "assert common, 'No overlapping OOF videos between TCN and XGB'\n",
        "X_all, y_all = [], []\n",
        "for vid in common:\n",
        "    Pt, Px = P_t[vid], P_x[vid]\n",
        "    Lmin = min(Pt.shape[0], Px.shape[0])\n",
        "    Pb = (w_tcn*Pt[:Lmin] + (1.0-w_tcn)*Px[:Lmin]).astype(np.float32)\n",
        "    yv = Y_t.get(vid, Y_x.get(vid))[:Lmin]\n",
        "    X_all.append(Pb); y_all.append(yv)\n",
        "P_all = np.concatenate(X_all, 0); y_all = np.concatenate(y_all, 0)\n",
        "Tcls = fit_per_class_temperature(P_all, y_all, np.linspace(0.7, 1.6, 12).astype(np.float32))\n",
        "print('[FAST-BLEND] Fitted per-class temps (first10):', np.round(Tcls[:10], 3).tolist())\n",
        "\n",
        "# 2) Decode blended TEST\n",
        "test_ids = pd.read_csv('test.csv')['Id'].astype(int).tolist()\n",
        "M_t = load_test_probs_map(TEST_TCN_DIR, test_ids)\n",
        "M_x = load_test_probs_map(TEST_XGB_DIR, test_ids)\n",
        "rows = []; wrote = 0; t0 = time.time()\n",
        "for vid in sorted(test_ids):\n",
        "    Pt = M_t.get(vid); Px = M_x.get(vid)\n",
        "    if Pt is None or Px is None: continue\n",
        "    Lmin = min(Pt.shape[0], Px.shape[0])\n",
        "    Pb = (w_tcn*Pt[:Lmin] + (1.0-w_tcn)*Px[:Lmin]).astype(np.float32)\n",
        "    L = to_calibrated_logprobs(Pb, Tcls, bg_bias=cfg['bb'], smooth=cfg['smooth'], g=cfg['g'])\n",
        "    seq = decode_with_cost_logprob(L, alpha=cfg['alpha'], cost_mode=cfg['mode'])\n",
        "    # Rescue permutation 1..20\n",
        "    seen, dup_idx = set(), []\n",
        "    for i, x in enumerate(seq):\n",
        "        if x in seen: dup_idx.append(i)\n",
        "        else: seen.add(x)\n",
        "    missing = [k for k in range(1, 21) if k not in seen]\n",
        "    for j, i in enumerate(dup_idx):\n",
        "        if j < len(missing): seq[i] = missing[j]\n",
        "    if len(seq) < 20: seq += [k for k in range(1, 21) if k not in set(seq)]\n",
        "    seq = seq[:20]\n",
        "    rows.append((vid, ' '.join(map(str, seq))))\n",
        "    wrote += 1\n",
        "sub = pd.DataFrame(rows, columns=['Id','Sequence']).sort_values('Id')\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('[FAST-BLEND] Wrote submission.csv', sub.shape, 'wrote', wrote, 'elapsed', f'{time.time()-t0:.1f}s')\n",
        "assert sub.shape[0] == 95, 'Submission missing rows; ensure both TCN and XGB test probs exist'"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FAST-BLEND] bg_idx=0 cfg={'mode': 'trimmed_mean_20', 'g': 0.9, 'bb': 0.2, 'smooth': 3, 'alpha': 0.85} w_tcn=0.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FAST-BLEND] Fitted per-class temps (first10): [1.1089999675750732, 0.8640000224113464, 0.8640000224113464, 0.8640000224113464, 0.8640000224113464, 0.8640000224113464, 0.8640000224113464, 0.8640000224113464, 0.8640000224113464, 0.9449999928474426]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FAST-BLEND] Wrote submission.csv (95, 2) wrote 95 elapsed 34.7s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}