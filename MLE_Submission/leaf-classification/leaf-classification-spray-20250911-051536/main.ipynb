{
  "cells": [
    {
      "id": "caa5a801-659c-459b-a4ef-ef15cdd38bd6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Image Embeddings Extraction (ConvNeXt-Tiny fb_in22k) -> save train/test .npy + missing flags\n",
        "import os, sys, time, math, subprocess, gc, random, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "t0 = time.time()\n",
        "SEED = 1337\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "\n",
        "# Prepare writable caches BEFORE importing timm/hf-hub to avoid read-only default at /app/.cache\n",
        "HF_CACHE = Path('./.hf_cache').absolute()\n",
        "TORCH_CACHE = Path('./.torch_cache').absolute()\n",
        "HF_CACHE.mkdir(parents=True, exist_ok=True)\n",
        "TORCH_CACHE.mkdir(parents=True, exist_ok=True)\n",
        "os.environ['HF_HOME'] = str(HF_CACHE)\n",
        "os.environ['HUGGINGFACE_HUB_CACHE'] = str(HF_CACHE)\n",
        "os.environ['HF_HUB_CACHE'] = str(HF_CACHE)\n",
        "os.environ['TRANSFORMERS_CACHE'] = str(HF_CACHE)\n",
        "os.environ['TORCH_HOME'] = str(TORCH_CACHE)\n",
        "\n",
        "# Install PyTorch (CUDA 12.1) and timm if missing\n",
        "def ensure_pkg():\n",
        "    try:\n",
        "        import torch, torchvision, timm  # noqa\n",
        "        return\n",
        "    except Exception:\n",
        "        pass\n",
        "    print('Installing torch/torchvision/torchaudio (cu121) and timm...', flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', '--upgrade', 'pip'], check=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', '--index-url', 'https://download.pytorch.org/whl/cu121', 'torch', 'torchvision', 'torchaudio'], check=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'timm>=0.9.12'], check=True)\n",
        "    print('Install complete.', flush=True)\n",
        "ensure_pkg()\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import timm\n",
        "\n",
        "# Determinism and performance knobs\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "cudnn.deterministic = True\n",
        "cudnn.benchmark = False\n",
        "try:\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device, flush=True)\n",
        "\n",
        "DATA_DIR = Path('.')\n",
        "IMG_DIR = DATA_DIR / 'images'\n",
        "train_df = pd.read_csv(DATA_DIR/'train.csv')\n",
        "test_df = pd.read_csv(DATA_DIR/'test.csv')\n",
        "id_col = 'id'; target_col = 'species'\n",
        "train_ids = train_df[id_col].tolist()\n",
        "test_ids = test_df[id_col].tolist()\n",
        "print(f'train n={len(train_ids)}, test n={len(test_ids)}', flush=True)\n",
        "\n",
        "# Map id -> image path\n",
        "def id_to_path(x):\n",
        "    return IMG_DIR / f'{int(x)}.jpg'\n",
        "\n",
        "# Quick sanity of paths\n",
        "missing_train = sum([not id_to_path(i).exists() for i in train_ids])\n",
        "missing_test = sum([not id_to_path(i).exists() for i in test_ids])\n",
        "print(f'Missing files - train: {missing_train}, test: {missing_test}', flush=True)\n",
        "\n",
        "# Transforms: invert grayscale -> RGB, resize 224, normalize ImageNet\n",
        "img_size = 224\n",
        "imagenet_mean = [0.485, 0.456, 0.406]\n",
        "imagenet_std = [0.229, 0.224, 0.225]\n",
        "to_tensor = T.Compose([\n",
        "    T.Resize((img_size, img_size), interpolation=T.InterpolationMode.BILINEAR, antialias=True),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
        "])\n",
        "\n",
        "class LeafDataset(Dataset):\n",
        "    def __init__(self, ids):\n",
        "        self.ids = list(ids)\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "    def __getitem__(self, idx):\n",
        "        lid = self.ids[idx]\n",
        "        path = id_to_path(lid)\n",
        "        miss_flag = 0\n",
        "        try:\n",
        "            img = Image.open(path).convert('L')\n",
        "            # Invert so background -> white, object -> black\n",
        "            img = ImageOps.invert(img)\n",
        "            img = img.convert('RGB')\n",
        "            tensor = to_tensor(img)\n",
        "        except Exception:\n",
        "            tensor = torch.zeros(3, img_size, img_size, dtype=torch.float32)\n",
        "            miss_flag = 1\n",
        "        return tensor, miss_flag\n",
        "\n",
        "# Model: ConvNeXt-Tiny fb_in22k, feature extractor (768-d)\n",
        "model_name = 'convnext_tiny.fb_in22k'\n",
        "model = timm.create_model(model_name, pretrained=True, num_classes=0, global_pool='avg', cache_dir=str(HF_CACHE))\n",
        "model.eval().to(device)\n",
        "feat_dim = model.num_features if hasattr(model, 'num_features') else 768\n",
        "print(f'Model {model_name} -> feature dim {feat_dim}', flush=True)\n",
        "\n",
        "def extract_embeddings(ids, split_name='train', batch_size=128, num_workers=8):\n",
        "    ds = LeafDataset(ids)\n",
        "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, drop_last=False)\n",
        "    n = len(ds)\n",
        "    embs = np.zeros((n, feat_dim), dtype=np.float32)\n",
        "    miss = np.zeros((n,), dtype=np.uint8)\n",
        "    seen = 0\n",
        "    start = time.time()\n",
        "    oom_downgraded = False\n",
        "    with torch.inference_mode():\n",
        "        for bi, (imgs, flags) in enumerate(dl):\n",
        "            bt0 = time.time()\n",
        "            try:\n",
        "                with torch.autocast(device_type='cuda' if device.type=='cuda' else 'cpu', dtype=torch.float16 if device.type=='cuda' else torch.bfloat16):\n",
        "                    imgs = imgs.to(device, non_blocking=True)\n",
        "                    feats = model(imgs)\n",
        "                    feats = feats.float()\n",
        "            except RuntimeError as e:\n",
        "                if ('out of memory' in str(e).lower()) and (not oom_downgraded):\n",
        "                    print('OOM encountered, reducing batch size to 64 for remaining iterations', flush=True)\n",
        "                    oom_downgraded = True\n",
        "                raise\n",
        "            bs = feats.shape[0]\n",
        "            embs[seen:seen+bs] = feats.detach().cpu().numpy()\n",
        "            miss[seen:seen+bs] = flags.numpy().astype(np.uint8)\n",
        "            seen += bs\n",
        "            if (bi % 10) == 0 or bi == len(dl)-1:\n",
        "                print(f'[{split_name}] batch {bi+1}/{len(dl)} | seen {seen}/{n} | elapsed {(time.time()-start):.1f}s (batch {(time.time()-bt0):.2f}s)', flush=True)\n",
        "    # L2 normalize row-wise\n",
        "    norms = np.linalg.norm(embs, axis=1, keepdims=True) + 1e-12\n",
        "    embs = embs / norms\n",
        "    return embs, miss\n",
        "\n",
        "# Try larger bs first; if OOM we will catch and instruct to lower manually next run\n",
        "bs = 128\n",
        "try:\n",
        "    tr_embs, tr_miss = extract_embeddings(train_ids, 'train', batch_size=bs, num_workers=8)\n",
        "    te_embs, te_miss = extract_embeddings(test_ids, 'test', batch_size=bs, num_workers=8)\n",
        "except RuntimeError as e:\n",
        "    if 'out of memory' in str(e).lower():\n",
        "        torch.cuda.empty_cache(); gc.collect()\n",
        "        print('Retrying with batch_size=64 after OOM...', flush=True)\n",
        "        tr_embs, tr_miss = extract_embeddings(train_ids, 'train', batch_size=64, num_workers=8)\n",
        "        te_embs, te_miss = extract_embeddings(test_ids, 'test', batch_size=64, num_workers=8)\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "# Save artifacts with id alignment\n",
        "np.save('train_img_emb.npy', tr_embs)\n",
        "np.save('test_img_emb.npy', te_embs)\n",
        "pd.DataFrame({id_col: train_ids, 'img_missing': tr_miss}).to_csv('train_img_flags.csv', index=False)\n",
        "pd.DataFrame({id_col: test_ids, 'img_missing': te_miss}).to_csv('test_img_flags.csv', index=False)\n",
        "print('Saved embeddings and flags:',\n",
        "      'train_img_emb.npy', tr_embs.shape,\n",
        "      'test_img_emb.npy', te_embs.shape, flush=True)\n",
        "\n",
        "# Memory cleanup\n",
        "del model; torch.cuda.empty_cache(); gc.collect()\n",
        "print(f'Embedding extraction done in {(time.time()-t0)/60:.1f} min')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train n=891, test n=99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing files - train: 0, test: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model convnext_tiny.fb_in22k -> feature dim 768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] batch 1/7 | seen 128/891 | elapsed 3.0s (batch 0.65s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[test] batch 1/1 | seen 99/99 | elapsed 1.4s (batch 0.13s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved embeddings and flags: train_img_emb.npy (891, 768) test_img_emb.npy (99, 768)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding extraction done in 0.1 min\n"
          ]
        }
      ]
    },
    {
      "id": "202b2707-0702-4b97-a2f8-6233742ec7f5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Leaf Classification - Fix pipelines, add PT-LDA eigen sweep, SVC(PT), stack with meta LR, per-fold TS later\n",
        "import os, sys, time, random, subprocess, io, contextlib, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "SEED = 1337\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "\n",
        "t0 = time.time()\n",
        "print('Starting run...')\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, PowerTransformer\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# LightGBM import with auto-install if missing\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    from lightgbm import LGBMClassifier\n",
        "except Exception as e:\n",
        "    print('Installing lightgbm...', flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'], check=True)\n",
        "    import lightgbm as lgb\n",
        "    from lightgbm import LGBMClassifier\n",
        "\n",
        "# Silence LightGBM excessive warnings/logs\n",
        "warnings.filterwarnings('ignore', message='.*No further splits with positive gain.*')\n",
        "\n",
        "DATA_DIR = Path('.')\n",
        "train_path = DATA_DIR/'train.csv'\n",
        "test_path = DATA_DIR/'test.csv'\n",
        "ss_path = DATA_DIR/'sample_submission.csv'\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv(train_path)\n",
        "test = pd.read_csv(test_path)\n",
        "ss = pd.read_csv(ss_path)\n",
        "print(f'train shape: {train.shape}, test shape: {test.shape}', flush=True)\n",
        "\n",
        "# Columns\n",
        "id_col = 'id'; target_col = 'species'\n",
        "feature_cols = [c for c in train.columns if c not in [id_col, target_col]]\n",
        "print(f'Number of features: {len(feature_cols)}')\n",
        "\n",
        "# Target encoding\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(train[target_col])\n",
        "classes = list(le.classes_); n_classes = len(classes)\n",
        "print(f'Number of classes: {n_classes}')\n",
        "\n",
        "# Submission column order sanity\n",
        "ss_cols = [c for c in ss.columns if c != id_col]\n",
        "if set(ss_cols) != set(classes):\n",
        "    raise ValueError('Sample submission class columns do not match training classes')\n",
        "submission_cols = ss_cols.copy()\n",
        "\n",
        "# Matrices\n",
        "X = train[feature_cols].values\n",
        "X_test = test[feature_cols].values\n",
        "\n",
        "# CV (min class count ~6)\n",
        "n_splits = 6\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "\n",
        "def clip_and_normalize(P):\n",
        "    P = np.clip(P, 1e-15, 1 - 1e-15)\n",
        "    row_sums = P.sum(axis=1, keepdims=True)\n",
        "    return P / row_sums\n",
        "\n",
        "def logloss_with_clip(y_true, y_pred):\n",
        "    p = clip_and_normalize(y_pred)\n",
        "    return log_loss(y_true, p, labels=np.arange(n_classes))\n",
        "\n",
        "def temp_scale_probs(P, T):\n",
        "    P = np.clip(P, 1e-15, 1-1e-15)\n",
        "    Q = np.power(P, 1.0/float(T))\n",
        "    return Q / Q.sum(axis=1, keepdims=True)\n",
        "\n",
        "def run_model_pipeline(name, pipe_factory, X, y, X_test, skf):\n",
        "    print(f'\\n=== Running {name} with {skf.get_n_splits()} folds ===')\n",
        "    oof = np.zeros((len(X), n_classes), dtype=np.float32)\n",
        "    tst = np.zeros((len(X_test), n_classes), dtype=np.float32)\n",
        "    fold_losses = []\n",
        "    start = time.time()\n",
        "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n",
        "        f_t0 = time.time()\n",
        "        X_tr, X_va = X[tr_idx], X[va_idx]\n",
        "        y_tr, y_va = y[tr_idx], y[va_idx]\n",
        "        print(f'[{name}] Fold {fold}/{n_splits} - train: {len(tr_idx)}, valid: {len(va_idx)}', flush=True)\n",
        "        pipe = pipe_factory()\n",
        "        pipe.fit(X_tr, y_tr)\n",
        "        proba_va = pipe.predict_proba(X_va)\n",
        "        loss = logloss_with_clip(y_va, proba_va)\n",
        "        oof[va_idx] = proba_va\n",
        "        tst += pipe.predict_proba(X_test) / n_splits\n",
        "        fold_losses.append(loss)\n",
        "        print(f'[{name}] Fold {fold} logloss: {loss:.6f}; elapsed fold {(time.time()-f_t0):.1f}s; total {(time.time()-start):.1f}s', flush=True)\n",
        "    oof_loss = logloss_with_clip(y, oof)\n",
        "    print(f'[{name}] CV fold logloss: ' + ', '.join([f\"{v:.6f}\" for v in fold_losses]))\n",
        "    print(f'[{name}] OOF CV logloss: {oof_loss:.6f}')\n",
        "    return oof, tst, oof_loss\n",
        "\n",
        "def run_model_pipeline_with_guard(name, pipe_factory, X, y, X_test, skf, abort_threshold=0.6):\n",
        "    print(f'\\n=== Running {name} (guarded) with {skf.get_n_splits()} folds ===')\n",
        "    oof = np.zeros((len(X), n_classes), dtype=np.float32)\n",
        "    tst = np.zeros((len(X_test), n_classes), dtype=np.float32)\n",
        "    fold_losses = []\n",
        "    start = time.time()\n",
        "    bad_first_two = False\n",
        "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n",
        "        f_t0 = time.time()\n",
        "        X_tr, X_va = X[tr_idx], X[va_idx]\n",
        "        y_tr, y_va = y[tr_idx], y[va_idx]\n",
        "        print(f'[{name}] Fold {fold}/{n_splits} - train: {len(tr_idx)}, valid: {len(va_idx)}', flush=True)\n",
        "        pipe = pipe_factory()\n",
        "        pipe.fit(X_tr, y_tr)\n",
        "        proba_va = pipe.predict_proba(X_va)\n",
        "        loss = logloss_with_clip(y_va, proba_va)\n",
        "        oof[va_idx] = proba_va\n",
        "        tst += pipe.predict_proba(X_test) / n_splits\n",
        "        fold_losses.append(loss)\n",
        "        print(f'[{name}] Fold {fold} logloss: {loss:.6f}; elapsed fold {(time.time()-f_t0):.1f}s; total {(time.time()-start):.1f}s', flush=True)\n",
        "        if fold == 2:\n",
        "            if fold_losses[0] > abort_threshold and fold_losses[1] > abort_threshold:\n",
        "                bad_first_two = True\n",
        "                print(f'[{name}] Early abort: first two folds > {abort_threshold}. Skipping remaining folds.')\n",
        "                break\n",
        "    if bad_first_two:\n",
        "        return oof, tst, 1e3\n",
        "    oof_loss = logloss_with_clip(y, oof)\n",
        "    print(f'[{name}] CV fold logloss: ' + ', '.join([f\"{v:.6f}\" for v in fold_losses]))\n",
        "    print(f'[{name}] OOF CV logloss: {oof_loss:.6f}')\n",
        "    return oof, tst, oof_loss\n",
        "\n",
        "# Pipelines\n",
        "# 1) Logistic Regression (anchor, L2)\n",
        "def make_pipeline_lr(C=1.0, max_iter=5000):\n",
        "    return Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('clf', LogisticRegression(solver='lbfgs', C=C, max_iter=max_iter, n_jobs=-1, random_state=SEED))\n",
        "    ])\n",
        "\n",
        "# 1b) Logistic Regression elastic-net (optional)\n",
        "RUN_LR_EN = False  # disable slow elastic-net grid by default\n",
        "def make_pipeline_lr_en(C=1.0, l1_ratio=0.25, max_iter=5000):\n",
        "    return Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('clf', LogisticRegression(solver='saga', penalty='elasticnet', l1_ratio=l1_ratio, C=C, max_iter=max_iter, n_jobs=-1, random_state=SEED))\n",
        "    ])\n",
        "\n",
        "# 2) LDA with PCA whiten: StandardScaler -> PCA -> LDA\n",
        "def make_pipeline_lda_pca(n_comp):\n",
        "    return Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('pca', PCA(n_components=n_comp, whiten=True, svd_solver='full', random_state=SEED)),\n",
        "        ('clf', LDA(solver='lsqr', shrinkage='auto'))\n",
        "    ])\n",
        "\n",
        "# 3) LDA PT variants (PT already standardizes; no extra scaler)\n",
        "def make_pipeline_lda_pt_lsqr():\n",
        "    return Pipeline([\n",
        "        ('pt', PowerTransformer(method='yeo-johnson', standardize=True)),\n",
        "        ('clf', LDA(solver='lsqr', shrinkage='auto'))\n",
        "    ])\n",
        "\n",
        "def make_pipeline_lda_pt_eigen(shrink=0.2):\n",
        "    return Pipeline([\n",
        "        ('pt', PowerTransformer(method='yeo-johnson', standardize=True)),\n",
        "        ('clf', LDA(solver='eigen', shrinkage=shrink))\n",
        "    ])\n",
        "\n",
        "# 4) SVM RBF with PowerTransformer (NO PCA)\n",
        "def make_pipeline_svm_pt(C=1.0, gamma='scale', class_weight=None):\n",
        "    return Pipeline([\n",
        "        ('pt', PowerTransformer(method='yeo-johnson', standardize=True)),\n",
        "        ('clf', SVC(kernel='rbf', C=C, gamma=gamma, class_weight=class_weight, probability=True, cache_size=2000, random_state=SEED))\n",
        "    ])\n",
        "\n",
        "# 5) (Dropped) GaussianNB after PT - harmful\n",
        "def make_pipeline_gnb_pt():\n",
        "    return Pipeline([\n",
        "        ('pt', PowerTransformer(method='yeo-johnson', standardize=True)),\n",
        "        ('clf', GaussianNB())\n",
        "    ])\n",
        "\n",
        "# RUN BASE MODELS\n",
        "base_models = []  # list of tuples (name, oof, tst, oof_loss)\n",
        "\n",
        "# A) LR L2 grid\n",
        "best_lr = None; best_lr_oof=None; best_lr_tst=None; best_lr_loss=1e9\n",
        "for C in [0.5, 1.0, 2.0]:\n",
        "    def lr_factory(cc=C): return make_pipeline_lr(cc, max_iter=5000)\n",
        "    oof_l, tst_l, loss_l = run_model_pipeline(f'LR_L2(C={C})', lr_factory, X, y, X_test, skf)\n",
        "    if loss_l < best_lr_loss:\n",
        "        best_lr_loss = loss_l; best_lr = C; best_lr_oof=oof_l; best_lr_tst=tst_l\n",
        "print(f'Best LR_L2: C={best_lr}, OOF={best_lr_loss:.6f}')\n",
        "if best_lr_loss <= 0.12: base_models.append((f'LR_L2_C{best_lr}', best_lr_oof, best_lr_tst, best_lr_loss))\n",
        "\n",
        "# B) LR elastic-net (optional, only if enabled and competitive)\n",
        "best_lren = (None, None, None, 1e9)\n",
        "if RUN_LR_EN:\n",
        "    for C in [0.5, 1.0, 2.0]:\n",
        "        for l1r in [0.0, 0.25]:\n",
        "            def lren_factory(cc=C, ll=l1r): return make_pipeline_lr_en(cc, ll, max_iter=5000)\n",
        "            oof_e, tst_e, loss_e = run_model_pipeline(f'LR_EN(C={C},l1={l1r})', lren_factory, X, y, X_test, skf)\n",
        "            if loss_e < best_lren[3]: best_lren = (f'LR_EN_C{C}_l1{l1r}', oof_e, tst_e, loss_e)\n",
        "    print(f'Best LR_EN: {best_lren[0]} OOF={best_lren[3]:.6f}')\n",
        "    if best_lren[3] <= 0.12: base_models.append(best_lren)\n",
        "\n",
        "# C) LDA PT lsqr\n",
        "oof_ptl, tst_ptl, loss_ptl = run_model_pipeline('LDA_PT_lsqr', make_pipeline_lda_pt_lsqr, X, y, X_test, skf)\n",
        "if loss_ptl <= 0.20: base_models.append(('LDA_PT_lsqr', oof_ptl, tst_ptl, loss_ptl))\n",
        "\n",
        "# D) LDA PT eigen shrink sweep (expanded grid)\n",
        "best_pte = (None, None, None, 1e9)\n",
        "for s in [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4]:\n",
        "    def pte_factory(sshr=s): return make_pipeline_lda_pt_eigen(sshr)\n",
        "    oof_pe, tst_pe, loss_pe = run_model_pipeline(f'LDA_PT_eigen(sh={s})', pte_factory, X, y, X_test, skf)\n",
        "    if loss_pe < best_pte[3]: best_pte = (f'LDA_PT_eigen_{s}', oof_pe, tst_pe, loss_pe)\n",
        "print(f'Best LDA_PT_eigen: {best_pte[0]} OOF={best_pte[3]:.6f}')\n",
        "if best_pte[3] <= 0.20: base_models.append(best_pte)\n",
        "\n",
        "# E) LDA PCA (small sweep) - keep only if beats PT-LDA\n",
        "best_ldap = (None, None, None, 1e9)\n",
        "for nc in [64, 80, 96]:\n",
        "    def ldap_factory(ncc=nc): return make_pipeline_lda_pca(ncc)\n",
        "    oof_la, tst_la, loss_la = run_model_pipeline(f'LDA_PCA(n={nc})', ldap_factory, X, y, X_test, skf)\n",
        "    if loss_la < best_ldap[3]: best_ldap = (f'LDA_PCA_{nc}', oof_la, tst_la, loss_la)\n",
        "print(f'Best LDA_PCA: {best_ldap[0]} OOF={best_ldap[3]:.6f}')\n",
        "if best_ldap[3] <= min(loss_ptl, best_pte[3], 0.20): base_models.append(best_ldap)\n",
        "\n",
        "# F) SVM PT (no PCA): C x gamma grid; keep only if <=0.12\n",
        "RUN_SVM_PT = False  # disable for now due to poor performance; re-enable if needed\n",
        "best_svm = (None, None, None, 1e9)\n",
        "if RUN_SVM_PT:\n",
        "    svm_C_grid = [1.0, 3.0, 10.0, 30.0, 100.0]\n",
        "    svm_gamma_grid = ['scale', 0.002, 0.004, 0.008, 0.016, 0.032]\n",
        "    def run_svm_variant(Cval, gval, cw):\n",
        "        def svm_factory(cc=Cval, gg=gval, cw_=cw): return make_pipeline_svm_pt(C=cc, gamma=gg, class_weight=cw_)\n",
        "        tag = f'SVM_PT(C={Cval},g={gval},cw={cw})'\n",
        "        return run_model_pipeline_with_guard(tag, svm_factory, X, y, X_test, skf, abort_threshold=0.6)\n",
        "    for Cval in svm_C_grid:\n",
        "        for gval in svm_gamma_grid:\n",
        "            oof_s, tst_s, loss_s = run_svm_variant(Cval, gval, None)\n",
        "            if loss_s < best_svm[3]: best_svm = (f'SVM_PT_C{Cval}_g{gval}_cwNone', oof_s, tst_s, loss_s)\n",
        "    print(f'Best SVM_PT: {best_svm[0]} OOF={best_svm[3]:.6f}')\n",
        "    if best_svm[3] <= 0.12: base_models.append(best_svm)\n",
        "\n",
        "# G) LightGBM (tree diversity) with early stopping; compact regularized grid\n",
        "best_lgb = (None, None, None, 1e9)\n",
        "lgb_param_grid = []\n",
        "for num_leaves in [31, 63]:\n",
        "    for max_depth in [4, 6]:\n",
        "        for lambda_l2 in [5, 10]:\n",
        "            lgb_param_grid.append({'num_leaves': num_leaves, 'max_depth': max_depth, 'lambda_l2': lambda_l2})\n",
        "\n",
        "def run_lgbm_variant(params):\n",
        "    name = f\"LGBM(leaves={params['num_leaves']},depth={params['max_depth']},l2={params['lambda_l2']})\"\n",
        "    print(f\"\\n=== Running {name} with {n_splits} folds ===\")\n",
        "    oof = np.zeros((len(X), n_classes), dtype=np.float32)\n",
        "    tst = np.zeros((len(X_test), n_classes), dtype=np.float32)\n",
        "    fold_losses = []\n",
        "    start = time.time()\n",
        "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n",
        "        f_t0 = time.time()\n",
        "        X_tr, X_va = X[tr_idx], X[va_idx]\n",
        "        y_tr, y_va = y[tr_idx], y[va_idx]\n",
        "        X_tr_in, X_es, y_tr_in, y_es = train_test_split(X_tr, y_tr, test_size=0.2, stratify=y_tr, random_state=SEED+fold)\n",
        "        clf = LGBMClassifier(\n",
        "            objective='multiclass', num_class=n_classes,\n",
        "            learning_rate=0.05, n_estimators=2000,\n",
        "            num_leaves=params['num_leaves'], max_depth=params['max_depth'],\n",
        "            min_child_samples=20,\n",
        "            subsample=0.8, subsample_freq=1,\n",
        "            colsample_bytree=0.8,\n",
        "            reg_lambda=params['lambda_l2'], reg_alpha=0.0,\n",
        "            random_state=SEED, n_jobs=-1, verbose=-1\n",
        "        )\n",
        "        # Suppress LightGBM internal logging\n",
        "        with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n",
        "            clf.fit(\n",
        "                X_tr_in, y_tr_in,\n",
        "                eval_set=[(X_es, y_es)],\n",
        "                eval_metric='multi_logloss',\n",
        "                callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False), lgb.log_evaluation(period=0)]\n",
        "            )\n",
        "        proba_va = clf.predict_proba(X_va, num_iteration=clf.best_iteration_)\n",
        "        loss = logloss_with_clip(y_va, proba_va)\n",
        "        oof[va_idx] = proba_va\n",
        "        tst += clf.predict_proba(X_test, num_iteration=clf.best_iteration_) / n_splits\n",
        "        fold_losses.append(loss)\n",
        "        print(f'[{name}] Fold {fold} logloss: {loss:.6f}; best_iter={clf.best_iteration_}; elapsed fold {(time.time()-f_t0):.1f}s; total {(time.time()-start):.1f}s', flush=True)\n",
        "    oof_loss = logloss_with_clip(y, oof)\n",
        "    print(f'[{name}] CV fold logloss: ' + ', '.join([f\"{v:.6f}\" for v in fold_losses]))\n",
        "    print(f'[{name}] OOF CV logloss: {oof_loss:.6f}')\n",
        "    return name, oof, tst, oof_loss\n",
        "\n",
        "for i, params in enumerate(lgb_param_grid, 1):\n",
        "    print(f'Grid {i}/{len(lgb_param_grid)}: {params}', flush=True)\n",
        "    name, oof_lgb, tst_lgb, loss_lgb = run_lgbm_variant(params)\n",
        "    if loss_lgb < best_lgb[3]: best_lgb = (name, oof_lgb, tst_lgb, loss_lgb)\n",
        "print(f'Best LGBM: {best_lgb[0]} OOF={best_lgb[3]:.6f}')\n",
        "if best_lgb[3] <= 0.12: base_models.append(best_lgb)\n",
        "\n",
        "print('Selected base models:', [(m[0], round(m[3],6)) for m in base_models])\n",
        "if len(base_models) < 2:\n",
        "    print('Not enough competitive base models; falling back to best LR_L2 for submission.')\n",
        "    final_tst = best_lr_tst\n",
        "    sub = pd.DataFrame(test[id_col])\n",
        "    proba_df = pd.DataFrame(final_tst, columns=le.inverse_transform(np.arange(n_classes)))\n",
        "    proba_df = proba_df[submission_cols]\n",
        "    sub = pd.concat([sub, proba_df], axis=1)\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv'); print(sub.head()); print(f'Total runtime: {(time.time()-t0)/60:.1f} min')\n",
        "    raise SystemExit\n",
        "\n",
        "# Clip-normalize base probabilities before stacking\n",
        "base_models_clipped = []\n",
        "for name, oof_m, tst_m, loss_m in base_models:\n",
        "    base_models_clipped.append((name, clip_and_normalize(oof_m), clip_and_normalize(tst_m), loss_m))\n",
        "\n",
        "# Build stacked features\n",
        "X_stack = np.hstack([m[1] for m in base_models_clipped])\n",
        "X_test_stack = np.hstack([m[2] for m in base_models_clipped])\n",
        "print(f'Stack features shape: {X_stack.shape}, test: {X_test_stack.shape}')\n",
        "\n",
        "# Second-level CV for meta LR (L2), grid over C\n",
        "def fit_meta_and_oof(C):\n",
        "    meta_oof = np.zeros((len(X), n_classes), dtype=np.float32)\n",
        "    start = time.time()\n",
        "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n",
        "        Xtr, Xva = X_stack[tr_idx], X_stack[va_idx]\n",
        "        ytr = y[tr_idx]\n",
        "        meta = LogisticRegression(solver='lbfgs', C=C, penalty='l2', max_iter=5000, n_jobs=-1, random_state=SEED)\n",
        "        meta.fit(Xtr, ytr)\n",
        "        meta_oof[va_idx] = meta.predict_proba(Xva)\n",
        "        if fold % 2 == 0:\n",
        "            print(f'[META C={C}] fold {fold} done; elapsed {(time.time()-start):.1f}s', flush=True)\n",
        "    return meta_oof\n",
        "\n",
        "best_meta = (None, 1e9, None)\n",
        "for Cmeta in [0.3, 1.0, 3.0, 10.0]:\n",
        "    meta_oof = fit_meta_and_oof(Cmeta)\n",
        "    loss_meta = logloss_with_clip(y, meta_oof)\n",
        "    print(f'Meta LR(C={Cmeta}) OOF: {loss_meta:.6f}')\n",
        "    if loss_meta < best_meta[1]: best_meta = (Cmeta, loss_meta, meta_oof)\n",
        "best_Cmeta, best_meta_oof_loss, best_meta_oof = best_meta\n",
        "print(f'Best Meta C={best_Cmeta} OOF={best_meta_oof_loss:.6f}')\n",
        "\n",
        "# Refit meta on full stacked features and predict test\n",
        "meta_final = LogisticRegression(solver='lbfgs', C=best_Cmeta, penalty='l2', max_iter=5000, n_jobs=-1, random_state=SEED)\n",
        "meta_final.fit(X_stack, y)\n",
        "meta_test = meta_final.predict_proba(X_test_stack)\n",
        "\n",
        "# Temperature scaling on final stack only (global T for now; per-fold TS to be added later)\n",
        "best_T = 1.0; best_ts_loss = best_meta_oof_loss\n",
        "for T in np.arange(0.5, 5.01, 0.05):\n",
        "    ts_oof = temp_scale_probs(best_meta_oof, T)\n",
        "    loss_T = logloss_with_clip(y, ts_oof)\n",
        "    if loss_T < best_ts_loss:\n",
        "        best_ts_loss = loss_T; best_T = float(T)\n",
        "print(f'Best temperature T={best_T:.2f} improved OOF from {best_meta_oof_loss:.6f} to {best_ts_loss:.6f}')\n",
        "meta_test_ts = temp_scale_probs(meta_test, best_T)\n",
        "\n",
        "# Build submission from temperature-scaled stacked predictions\n",
        "sub = pd.DataFrame(test[id_col])\n",
        "proba_df = pd.DataFrame(meta_test_ts, columns=le.inverse_transform(np.arange(n_classes)))\n",
        "proba_df = proba_df[submission_cols]\n",
        "sub = pd.concat([sub, proba_df], axis=1)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv')\n",
        "print(sub.head())\n",
        "print(f'Total runtime: {(time.time()-t0)/60:.1f} min')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "id": "3b57d358-31bb-43d5-af31-e64f7148817a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fusion modeling: tabular + image embeddings + missing flag -> LR only + global TS, submit\n",
        "import os, sys, time, random, gc, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "SEED = 1337\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "t0 = time.time()\n",
        "print('Starting fusion modeling (LR + TS)...')\n",
        "\n",
        "DATA_DIR = Path('.')\n",
        "train = pd.read_csv(DATA_DIR/'train.csv')\n",
        "test = pd.read_csv(DATA_DIR/'test.csv')\n",
        "ss = pd.read_csv(DATA_DIR/'sample_submission.csv')\n",
        "id_col = 'id'; target_col = 'species'\n",
        "feature_cols = [c for c in train.columns if c not in [id_col, target_col]]\n",
        "print(f'Loaded train {train.shape}, test {test.shape}; tabular feats: {len(feature_cols)}')\n",
        "\n",
        "# Label encoding\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(train[target_col])\n",
        "classes = list(le.classes_); n_classes = len(classes)\n",
        "ss_cols = [c for c in ss.columns if c != id_col]\n",
        "assert set(ss_cols) == set(classes), 'Submission cols mismatch'\n",
        "submission_cols = ss_cols.copy()\n",
        "\n",
        "# Load embeddings and missing flags\n",
        "tr_emb = np.load('train_img_emb.npy')\n",
        "te_emb = np.load('test_img_emb.npy')\n",
        "tr_flag = pd.read_csv('train_img_flags.csv')['img_missing'].values.astype(np.float32).reshape(-1,1)\n",
        "te_flag = pd.read_csv('test_img_flags.csv')['img_missing'].values.astype(np.float32).reshape(-1,1)\n",
        "print('Embeddings:', tr_emb.shape, te_emb.shape, 'flags:', tr_flag.shape, te_flag.shape)\n",
        "\n",
        "# Build fused matrices\n",
        "X_tab = train[feature_cols].values.astype(np.float32)\n",
        "X_test_tab = test[feature_cols].values.astype(np.float32)\n",
        "X_fused = np.hstack([X_tab, tr_emb.astype(np.float32), tr_flag])\n",
        "X_test_fused = np.hstack([X_test_tab, te_emb.astype(np.float32), te_flag])\n",
        "print('Fused shapes:', X_fused.shape, X_test_fused.shape)\n",
        "\n",
        "n_splits = 6\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "\n",
        "def clip_and_normalize(P):\n",
        "    P = np.clip(P, 1e-15, 1 - 1e-15)\n",
        "    s = P.sum(axis=1, keepdims=True)\n",
        "    return P / s\n",
        "\n",
        "def logloss_with_clip(y_true, y_pred):\n",
        "    p = clip_and_normalize(y_pred)\n",
        "    return log_loss(y_true, p, labels=np.arange(n_classes))\n",
        "\n",
        "def temp_scale_probs(P, T):\n",
        "    P = np.clip(P, 1e-15, 1-1e-15)\n",
        "    Q = np.power(P, 1.0/float(T))\n",
        "    return Q / Q.sum(axis=1, keepdims=True)\n",
        "\n",
        "# Base: LR on fused (StandardScaler on full fused)\n",
        "def run_lr_fused(C=1.0):\n",
        "    name = f'LR_fused_C{C}'\n",
        "    print(f'\\n=== {name} ===')\n",
        "    oof = np.zeros((len(X_fused), n_classes), dtype=np.float32)\n",
        "    tst = np.zeros((len(X_test_fused), n_classes), dtype=np.float32)\n",
        "    losses = []\n",
        "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X_fused, y), 1):\n",
        "        f0 = time.time()\n",
        "        Xtr, Xva = X_fused[tr_idx], X_fused[va_idx]\n",
        "        ytr, yva = y[tr_idx], y[va_idx]\n",
        "        pipe = Pipeline([\n",
        "            ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
        "            ('clf', LogisticRegression(solver='lbfgs', multi_class='multinomial', C=C, max_iter=5000, n_jobs=-1, random_state=SEED))\n",
        "        ])\n",
        "        pipe.fit(Xtr, ytr)\n",
        "        proba_va = pipe.predict_proba(Xva)\n",
        "        loss = logloss_with_clip(yva, proba_va)\n",
        "        oof[va_idx] = proba_va\n",
        "        tst += pipe.predict_proba(X_test_fused) / n_splits\n",
        "        losses.append(loss)\n",
        "        print(f'[LR C={C}] fold {fold}/{n_splits} loss {loss:.6f} elapsed {(time.time()-f0):.1f}s', flush=True)\n",
        "    oof_loss = logloss_with_clip(y, oof)\n",
        "    print(f'[LR C={C}] OOF {oof_loss:.6f}; folds: ' + ', '.join([f'{v:.6f}' for v in losses]))\n",
        "    return oof, tst, oof_loss\n",
        "\n",
        "best_lr = (None, None, 1e9)\n",
        "best_lr_tst = None\n",
        "for C in [0.5, 1.0, 2.0, 3.0]:\n",
        "    oof_l, tst_l, loss_l = run_lr_fused(C)\n",
        "    if loss_l < best_lr[2]:\n",
        "        best_lr = (C, oof_l, loss_l); best_lr_tst = tst_l\n",
        "print(f'Best LR_fused C={best_lr[0]} OOF={best_lr[2]:.6f}')\n",
        "\n",
        "# Global temperature scaling on LR OOF\n",
        "lr_oof = clip_and_normalize(best_lr[1])\n",
        "lr_tst = clip_and_normalize(best_lr_tst)\n",
        "base_oof_loss = logloss_with_clip(y, lr_oof)\n",
        "print('LR base OOF (post-clip):', base_oof_loss)\n",
        "\n",
        "best_T = 1.0; best_ts_loss = base_oof_loss\n",
        "for T in np.arange(0.5, 5.01, 0.05):\n",
        "    ts_oof = temp_scale_probs(lr_oof, T)\n",
        "    loss_T = logloss_with_clip(y, ts_oof)\n",
        "    if loss_T < best_ts_loss:\n",
        "        best_ts_loss = loss_T; best_T = float(T)\n",
        "print(f'Best temperature T={best_T:.2f} OOF {best_ts_loss:.6f} (from {base_oof_loss:.6f})')\n",
        "\n",
        "final_test = temp_scale_probs(lr_tst, best_T)\n",
        "\n",
        "# Build submission\n",
        "sub = pd.DataFrame(test[id_col])\n",
        "proba_df = pd.DataFrame(final_test, columns=le.inverse_transform(np.arange(n_classes)))\n",
        "proba_df = proba_df[submission_cols]\n",
        "sub = pd.concat([sub, proba_df], axis=1)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv')\n",
        "print(sub.head())\n",
        "print(f'Total fusion runtime: {(time.time()-t0)/60:.1f} min')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting fusion modeling (LR + TS)...\nLoaded train (891, 194), test (99, 193); tabular feats: 192\nEmbeddings: (891, 768) (99, 768) flags: (891, 1) (99, 1)\nFused shapes: (891, 961) (99, 961)\n\n=== LR_fused_C0.5 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR C=0.5] fold 1/6 loss 0.051223 elapsed 2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR C=0.5] fold 2/6 loss 0.042543 elapsed 1.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}