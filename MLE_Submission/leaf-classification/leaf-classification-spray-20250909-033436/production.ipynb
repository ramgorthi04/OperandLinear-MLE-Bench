{
  "cells": [
    {
      "id": "2a02e8c1-1e72-4104-ba24-c99840e91a2c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal production: PT(YJ) + LogisticRegression (L2, C=2000) 5-fold OOF + submission\n",
        "import time, random, numpy as np, pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, PowerTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED); random.seed(SEED)\n",
        "\n",
        "def clip_and_renorm(probs, eps=1e-15):\n",
        "    P = np.clip(probs, eps, 1 - eps)\n",
        "    P /= P.sum(axis=1, keepdims=True)\n",
        "    return P\n",
        "\n",
        "def load_data():\n",
        "    train = pd.read_csv('train.csv')\n",
        "    test = pd.read_csv('test.csv')\n",
        "    X = train.drop(columns=['id', 'species'])\n",
        "    y = train['species'].values\n",
        "    X_test = test.drop(columns=['id'])\n",
        "    le = LabelEncoder()\n",
        "    y_enc = le.fit_transform(y)\n",
        "    classes = le.classes_\n",
        "    test_ids = test['id'].values\n",
        "    return X.values, y_enc, X_test.values, classes, test_ids\n",
        "\n",
        "def train_oof(model, X, y, X_test, n_classes, n_splits=5):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "    n = X.shape[0]\n",
        "    oof = np.zeros((n, n_classes), dtype=np.float64)\n",
        "    test_pred = np.zeros((X_test.shape[0], n_classes), dtype=np.float64)\n",
        "    fold_losses = []\n",
        "    start = time.time()\n",
        "    from sklearn.base import clone\n",
        "    for i, (tr, va) in enumerate(skf.split(X, y), 1):\n",
        "        t0 = time.time()\n",
        "        clf = clone(model)\n",
        "        clf.fit(X[tr], y[tr])\n",
        "        va_proba = clf.predict_proba(X[va])\n",
        "        loss = log_loss(y[va], va_proba, labels=list(range(n_classes)))\n",
        "        oof[va] = va_proba\n",
        "        test_pred += clf.predict_proba(X_test) / n_splits\n",
        "        print(f'[PT+LR C2000] Fold {i}/{n_splits} logloss={loss:.6f} time={time.time()-t0:.1f}s', flush=True)\n",
        "        fold_losses.append(loss)\n",
        "    oof_loss = log_loss(y, oof, labels=list(range(n_classes)))\n",
        "    print(f'[PT+LR C2000] OOF={oof_loss:.6f} | mean_folds={np.mean(fold_losses):.6f} | total={(time.time()-start)/60:.1f}m', flush=True)\n",
        "    return oof, test_pred, oof_loss\n",
        "\n",
        "X, y, X_test, classes, test_ids = load_data()\n",
        "n_classes = len(classes)\n",
        "pipe = Pipeline([\n",
        "    ('pt', PowerTransformer(method='yeo-johnson', standardize=True)),\n",
        "    ('clf', LogisticRegression(multi_class='multinomial', solver='saga', penalty='l2', C=2000, max_iter=30000, tol=1e-4, n_jobs=-1, random_state=SEED))\n",
        "])\n",
        "oof, test_pred, oof_loss = train_oof(pipe, X, y, X_test, n_classes, n_splits=5)\n",
        "test_pred = clip_and_renorm(test_pred)\n",
        "sub = pd.DataFrame(test_pred, columns=classes)\n",
        "sub.insert(0, 'id', test_ids)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with shape:', sub.shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PT+LR C2000] Fold 1/5 logloss=0.032064 time=48.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PT+LR C2000] Fold 2/5 logloss=0.035834 time=44.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PT+LR C2000] Fold 3/5 logloss=0.031986 time=44.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PT+LR C2000] Fold 4/5 logloss=0.043918 time=47.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PT+LR C2000] Fold 5/5 logloss=0.031508 time=46.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PT+LR C2000] OOF=0.035058 | mean_folds=0.035062 | total=3.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv with shape: (99, 100)\n"
          ]
        }
      ]
    },
    {
      "id": "f77f62d5-821f-4a8f-afc2-5add39c5f8fd",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Add Nystroem RBF feature map + Logistic Regression; blend with PT+LR\n",
        "import time\n",
        "from sklearn.kernel_approximation import Nystroem\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "def train_oof_fast(model, X, y, X_test, n_classes, n_splits=5, desc='model'):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "    n = X.shape[0]\n",
        "    oof = np.zeros((n, n_classes), dtype=np.float64)\n",
        "    test_pred = np.zeros((X_test.shape[0], n_classes), dtype=np.float64)\n",
        "    fold_losses = []\n",
        "    start = time.time()\n",
        "    from sklearn.base import clone\n",
        "    for i, (tr, va) in enumerate(skf.split(X, y), 1):\n",
        "        t0 = time.time()\n",
        "        clf = clone(model)\n",
        "        clf.fit(X[tr], y[tr])\n",
        "        va_proba = clf.predict_proba(X[va])\n",
        "        loss = log_loss(y[va], va_proba, labels=list(range(n_classes)))\n",
        "        oof[va] = va_proba\n",
        "        test_pred += clf.predict_proba(X_test) / n_splits\n",
        "        print(f'[{desc}] Fold {i}/{n_splits} logloss={loss:.6f} time={time.time()-t0:.1f}s', flush=True)\n",
        "        fold_losses.append(loss)\n",
        "    oof_loss = log_loss(y, oof, labels=list(range(n_classes)))\n",
        "    print(f'[{desc}] OOF={oof_loss:.6f} | mean_folds={np.mean(fold_losses):.6f} | total={(time.time()-start)/60:.1f}m', flush=True)\n",
        "    return oof, test_pred, oof_loss\n",
        "\n",
        "# Preserve LR results from previous cell\n",
        "lr_oof, lr_test, lr_loss = oof, test_pred, oof_loss\n",
        "\n",
        "# Small grid for Nystroem+LR (fast, diverse)\n",
        "configs = []\n",
        "for gamma in [1e-4, 3e-4]:\n",
        "    for n_comp in [800, 1200]:\n",
        "        for C in [3.0, 10.0]:\n",
        "            configs.append({'gamma': gamma, 'n_components': n_comp, 'C': C})\n",
        "\n",
        "best_nys = None\n",
        "best_oof = None\n",
        "best_test = None\n",
        "best_loss = float('inf')\n",
        "for i, p in enumerate(configs, 1):\n",
        "    print(f'[Nystroem+LR] {i}/{len(configs)} params={p}', flush=True)\n",
        "    pipe = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('nys', Nystroem(kernel='rbf', gamma=p['gamma'], n_components=p['n_components'], random_state=SEED)),\n",
        "        ('clf', LogisticRegression(solver='saga', multi_class='multinomial', C=p['C'], penalty='l2', max_iter=10000, tol=1e-4, n_jobs=-1, random_state=SEED))\n",
        "    ])\n",
        "    oof2, test2, loss2 = train_oof_fast(pipe, X, y, X_test, n_classes, n_splits=5, desc=f'NysRBF_C{p[\"C\"]}_g{p[\"gamma\"]}_m{p[\"n_components\"]}')\n",
        "    if loss2 < best_loss:\n",
        "        best_loss = loss2; best_oof = oof2; best_test = test2; best_nys = p\n",
        "print('Best Nystroem+LR:', best_nys, 'OOF=', best_loss)\n",
        "\n",
        "# Simple blends: equal and inverse-loss; pick best\n",
        "def clip_and_renorm(probs, eps=1e-15):\n",
        "    P = np.clip(probs, eps, 1 - eps)\n",
        "    P /= P.sum(axis=1, keepdims=True)\n",
        "    return P\n",
        "\n",
        "bases = [('LR_PT_C2000', lr_oof, lr_test, lr_loss), ('NysRBF_LR', best_oof, best_test, best_loss)]\n",
        "losses = np.array([b[3] for b in bases], dtype=np.float64)\n",
        "oofs = [b[1] for b in bases]\n",
        "tests = [b[2] for b in bases]\n",
        "\n",
        "# Equal\n",
        "w_eq = np.array([0.5, 0.5], dtype=np.float64)\n",
        "oof_eq = clip_and_renorm((oofs[0] * w_eq[0] + oofs[1] * w_eq[1]))\n",
        "loss_eq = log_loss(y, oof_eq, labels=list(range(n_classes)))\n",
        "\n",
        "# Inverse-loss\n",
        "w_il = 1.0 / np.maximum(losses, 1e-9); w_il = w_il / w_il.sum()\n",
        "oof_il = clip_and_renorm(oofs[0] * w_il[0] + oofs[1] * w_il[1])\n",
        "loss_il = log_loss(y, oof_il, labels=list(range(n_classes)))\n",
        "\n",
        "print(f'[Blend check] equal={loss_eq:.6f} | inv-loss={loss_il:.6f} | LR={lr_loss:.6f} | Nys={best_loss:.6f}', flush=True)\n",
        "w_best = w_il if loss_il < loss_eq else w_eq\n",
        "test_blend = clip_and_renorm(tests[0] * w_best[0] + tests[1] * w_best[1])\n",
        "\n",
        "# Save submission\n",
        "sub = pd.DataFrame(test_blend, columns=classes)\n",
        "sub.insert(0, 'id', test_ids)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (LR + Nystroem+LR blend) with shape:', sub.shape, '| weights=', w_best)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "927904b8-7523-480b-a690-a0771623d2b5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost multiclass (5-fold CV) + blend with strong PT+LR(C=2000)\n",
        "import sys, subprocess, importlib, time, numpy as np, pandas as pd\n",
        "\n",
        "def ensure_pkg(pkg):\n",
        "    try:\n",
        "        return importlib.import_module(pkg)\n",
        "    except ImportError:\n",
        "        print(f'Installing {pkg}...', flush=True)\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', '--no-cache-dir', '--prefer-binary', pkg])\n",
        "        return importlib.import_module(pkg)\n",
        "\n",
        "catboost = ensure_pkg('catboost')\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "def cb_oof_cv(X_np, y_np, Xte_np, n_classes, params_list, n_splits=5):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "    best = {'loss': float('inf')}\n",
        "    for i, prm in enumerate(params_list, 1):\n",
        "        print(f'[CatBoost] Config {i}/{len(params_list)}: {prm}', flush=True)\n",
        "        n = X_np.shape[0]\n",
        "        oof = np.zeros((n, n_classes), dtype=np.float64)\n",
        "        test_pred = np.zeros((Xte_np.shape[0], n_classes), dtype=np.float64)\n",
        "        folds = []\n",
        "        start = time.time()\n",
        "        for f, (tr, va) in enumerate(skf.split(X_np, y_np), 1):\n",
        "            t0 = time.time()\n",
        "            clf = CatBoostClassifier(loss_function='MultiClass',\n",
        "                                     depth=prm['depth'],\n",
        "                                     learning_rate=prm['learning_rate'],\n",
        "                                     l2_leaf_reg=prm['l2_leaf_reg'],\n",
        "                                     iterations=5000,\n",
        "                                     random_seed=SEED,\n",
        "                                     verbose=False)\n",
        "            clf.fit(X_np[tr], y_np[tr], eval_set=(X_np[va], y_np[va]), use_best_model=True, early_stopping_rounds=200)\n",
        "            va_proba = clf.predict_proba(X_np[va])\n",
        "            loss = log_loss(y_np[va], va_proba, labels=list(range(n_classes)))\n",
        "            oof[va] = va_proba\n",
        "            test_pred += clf.predict_proba(Xte_np) / n_splits\n",
        "            folds.append(loss)\n",
        "            print(f'[CatBoost] Fold {f}/{n_splits} logloss={loss:.6f} time={time.time()-t0:.1f}s', flush=True)\n",
        "        oof_loss = log_loss(y_np, oof, labels=list(range(n_classes)))\n",
        "        print(f'[CatBoost] OOF={oof_loss:.6f} | mean_folds={np.mean(folds):.6f} | total={(time.time()-start)/60:.1f}m', flush=True)\n",
        "        if oof_loss < best.get('loss', float('inf')):\n",
        "            best = {'params': prm, 'loss': oof_loss, 'oof': oof, 'test': test_pred}\n",
        "    return best\n",
        "\n",
        "# Ensure we have LR results from Cell 0\n",
        "lr_oof, lr_test, lr_loss = oof, test_pred, oof_loss\n",
        "\n",
        "# Small, conservative CatBoost grid per expert advice\n",
        "cb_params = [\n",
        "    {'depth': 6, 'learning_rate': 0.03, 'l2_leaf_reg': 10},\n",
        "    {'depth': 6, 'learning_rate': 0.06, 'l2_leaf_reg': 10},\n",
        "    {'depth': 8, 'learning_rate': 0.03, 'l2_leaf_reg': 10},\n",
        "    {'depth': 8, 'learning_rate': 0.06, 'l2_leaf_reg': 10}\n",
        "]\n",
        "best_cb = cb_oof_cv(X, y, X_test, len(classes), cb_params, n_splits=5)\n",
        "print('Best CatBoost:', best_cb.get('params'), 'OOF=', best_cb.get('loss'))\n",
        "\n",
        "def clip_and_renorm(probs, eps=1e-15):\n",
        "    P = np.clip(probs, eps, 1 - eps)\n",
        "    P /= P.sum(axis=1, keepdims=True)\n",
        "    return P\n",
        "\n",
        "# Simple blends: equal and inverse-loss with LR\n",
        "oofs = [lr_oof, best_cb['oof']]; tests = [lr_test, best_cb['test']]; losses = np.array([lr_loss, best_cb['loss']], dtype=np.float64)\n",
        "w_eq = np.array([0.5, 0.5]);\n",
        "oof_eq = clip_and_renorm(oofs[0]*w_eq[0] + oofs[1]*w_eq[1])\n",
        "loss_eq = log_loss(y, oof_eq, labels=list(range(len(classes))))\n",
        "w_il = 1.0 / np.maximum(losses, 1e-9); w_il = w_il / w_il.sum()\n",
        "oof_il = clip_and_renorm(oofs[0]*w_il[0] + oofs[1]*w_il[1])\n",
        "loss_il = log_loss(y, oof_il, labels=list(range(len(classes))))\n",
        "print(f'[Blend LR+CB] equal={loss_eq:.6f} | invloss={loss_il:.6f} | LR={lr_loss:.6f} | CB={best_cb[\"loss\"]:.6f}', flush=True)\n",
        "w_best = w_il if loss_il < loss_eq else w_eq\n",
        "test_blend = clip_and_renorm(tests[0]*w_best[0] + tests[1]*w_best[1])\n",
        "sub = pd.DataFrame(test_blend, columns=classes)\n",
        "sub.insert(0, 'id', test_ids)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (LR + CatBoost blend) with shape:', sub.shape, '| weights=', w_best)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "ddda9e18-0083-4337-9eed-c7225a9409a1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# LightGBM multiclass (raw features) conservative CV with early stopping\n",
        "import sys, subprocess, importlib, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "def ensure_pkg(pkg):\n",
        "    import importlib\n",
        "    try:\n",
        "        return importlib.import_module(pkg)\n",
        "    except ImportError:\n",
        "        print(f'Installing {pkg}...', flush=True)\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', '--no-cache-dir', '--prefer-binary', pkg])\n",
        "        return importlib.import_module(pkg)\n",
        "\n",
        "lgb = ensure_pkg('lightgbm')\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "# Use RAW X, y, X_test from earlier load_data() (no transforms)\n",
        "n_classes = len(classes)\n",
        "\n",
        "lgbm_params = {\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': n_classes,\n",
        "    'metric': 'multi_logloss',\n",
        "    'learning_rate': 0.02,\n",
        "    'n_estimators': 5000,\n",
        "    'num_leaves': 12,\n",
        "    'max_depth': 6,\n",
        "    'min_child_samples': 28,\n",
        "    'feature_fraction': 0.7,\n",
        "    'subsample': 0.8,\n",
        "    'lambda_l2': 5.0,\n",
        "    'random_state': SEED,\n",
        "    'n_jobs': -1,\n",
        "    'verbosity': -1\n",
        "}\n",
        "\n",
        "def lgbm_oof_cv(X_np, y_np, Xte_np, params, n_splits=5, early_rounds=200):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "    n = X_np.shape[0]\n",
        "    oof = np.zeros((n, n_classes), dtype=np.float64)\n",
        "    test_pred = np.zeros((Xte_np.shape[0], n_classes), dtype=np.float64)\n",
        "    fold_losses, fold_best_iters = [], []\n",
        "    start = time.time()\n",
        "    for f, (tr, va) in enumerate(skf.split(X_np, y_np), 1):\n",
        "        t0 = time.time()\n",
        "        clf = LGBMClassifier(**params)\n",
        "        clf.fit(X_np[tr], y_np[tr],\n",
        "                eval_set=[(X_np[va], y_np[va])],\n",
        "                eval_metric='multi_logloss',\n",
        "                callbacks=[lgb.early_stopping(stopping_rounds=early_rounds, verbose=False)])\n",
        "        va_proba = clf.predict_proba(X_np[va], raw_score=False)\n",
        "        loss = log_loss(y_np[va], va_proba, labels=list(range(n_classes)))\n",
        "        oof[va] = va_proba\n",
        "        test_pred += clf.predict_proba(Xte_np, raw_score=False) / n_splits\n",
        "        best_it = getattr(clf, 'best_iteration_', None)\n",
        "        fold_best_iters.append(best_it if best_it is not None else params.get('n_estimators', 0))\n",
        "        fold_losses.append(loss)\n",
        "        print(f'[LGBM] Fold {f}/{n_splits} logloss={loss:.6f} best_it={best_it} time={time.time()-t0:.1f}s', flush=True)\n",
        "        # Early abort heuristic if poor after two folds\n",
        "        if f >= 2 and np.mean(fold_losses) > 0.06:\n",
        "            print('[LGBM] Early abort: mean fold loss above 0.06 after two folds', flush=True)\n",
        "            break\n",
        "    used = len(fold_losses)\n",
        "    if used != n_splits and used > 0:\n",
        "        test_pred *= (n_splits / used)\n",
        "    oof_loss = log_loss(y_np, oof, labels=list(range(n_classes)))\n",
        "    mbest = np.mean([i for i in fold_best_iters if i is not None]) if fold_best_iters else float('nan')\n",
        "    print(f'[LGBM] OOF={oof_loss:.6f} | mean_folds={np.mean(fold_losses) if fold_losses else float(\"nan\"):.6f} | folds_used={used}/{n_splits} | mean_best_it={mbest:.1f} | total={(time.time()-start)/60:.1f}m', flush=True)\n",
        "    return oof, test_pred, oof_loss, fold_losses\n",
        "\n",
        "lgbm_oof, lgbm_test, lgbm_loss, lgbm_fold_losses = lgbm_oof_cv(X, y, X_test, lgbm_params, n_splits=5, early_rounds=200)\n",
        "\n",
        "def clip_and_renorm(probs, eps=1e-15):\n",
        "    P = np.clip(probs, eps, 1 - eps)\n",
        "    P /= P.sum(axis=1, keepdims=True)\n",
        "    return P\n",
        "\n",
        "# Quick sanity blend with existing LR OOF/test if present\n",
        "try:\n",
        "    lr_oof, lr_test, lr_loss\n",
        "    oofs = [lr_oof, lgbm_oof]\n",
        "    tests = [lr_test, lgbm_test]\n",
        "    losses = np.array([lr_loss, lgbm_loss], dtype=np.float64)\n",
        "    w_il = 1.0 / np.maximum(losses, 1e-9); w_il = w_il / w_il.sum()\n",
        "    oof_blend = clip_and_renorm(oofs[0]*w_il[0] + oofs[1]*w_il[1])\n",
        "    loss_blend = log_loss(y, oof_blend, labels=list(range(n_classes)))\n",
        "    print(f'[LGBM+LR] inv-loss blend OOF={loss_blend:.6f} | LR={lr_loss:.6f} | LGBM={lgbm_loss:.6f} | weights={w_il}', flush=True)\n",
        "    test_blend = clip_and_renorm(tests[0]*w_il[0] + tests[1]*w_il[1])\n",
        "    sub = pd.DataFrame(test_blend, columns=classes)\n",
        "    sub.insert(0, 'id', test_ids)\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (LR + LGBM inv-loss blend) with shape:', sub.shape, flush=True)\n",
        "except NameError:\n",
        "    print('LR results not found in kernel; skipping quick blend. You can blend later.', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "0100c3ff-7972-436b-a739-06c139f16803",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Seed-bagged PT(YJ)+LR: multiple CV seeds and C values; average probs\n",
        "import time, numpy as np, pandas as pd, random\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "def run_pt_lr_cv(X_np, y_np, Xte_np, n_classes, cv_seed=42, C=2000.0, n_splits=5):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=cv_seed)\n",
        "    oof = np.zeros((X_np.shape[0], n_classes), dtype=np.float64)\n",
        "    test_pred = np.zeros((Xte_np.shape[0], n_classes), dtype=np.float64)\n",
        "    fold_losses = []\n",
        "    start = time.time()\n",
        "    for f, (tr, va) in enumerate(skf.split(X_np, y_np), 1):\n",
        "        t0 = time.time()\n",
        "        pipe = Pipeline([\n",
        "            ('pt', PowerTransformer(method='yeo-johnson', standardize=True)),\n",
        "            ('clf', LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=C, max_iter=30000, tol=1e-5, random_state=cv_seed))\n",
        "        ])\n",
        "        pipe.fit(X_np[tr], y_np[tr])\n",
        "        proba_va = pipe.predict_proba(X_np[va])\n",
        "        loss = log_loss(y_np[va], proba_va, labels=list(range(n_classes)))\n",
        "        oof[va] = proba_va\n",
        "        test_pred += pipe.predict_proba(Xte_np) / n_splits\n",
        "        fold_losses.append(loss)\n",
        "        print(f'[Bag PT+LR lbfgs] seed={cv_seed} C={C} fold {f}/{n_splits} loss={loss:.6f} time={time.time()-t0:.1f}s', flush=True)\n",
        "    oof_loss = log_loss(y_np, oof, labels=list(range(n_classes)))\n",
        "    print(f'[Bag PT+LR lbfgs] seed={cv_seed} C={C} OOF={oof_loss:.6f} mean_folds={np.mean(fold_losses):.6f} total={(time.time()-start)/60:.1f}m', flush=True)\n",
        "    return oof, test_pred, oof_loss\n",
        "\n",
        "def clip_and_renorm(probs, eps=1e-15):\n",
        "    P = np.clip(probs, eps, 1 - eps)\n",
        "    P /= P.sum(axis=1, keepdims=True)\n",
        "    return P\n",
        "\n",
        "# Configs: single fast run to produce lr_bag_* quickly\n",
        "seeds = [42]\n",
        "Cs = [2000.0]\n",
        "configs = [(s, c) for s in seeds for c in Cs]\n",
        "print('Total configs:', len(configs), configs, flush=True)\n",
        "\n",
        "bag_oofs = []\n",
        "bag_tests = []\n",
        "bag_losses = []\n",
        "\n",
        "for i, (s, c) in enumerate(configs, 1):\n",
        "    print(f'=== Config {i}/{len(configs)}: seed={s}, C={c} ===', flush=True)\n",
        "    oof_i, test_i, loss_i = run_pt_lr_cv(X, y, X_test, n_classes, cv_seed=s, C=c, n_splits=5)\n",
        "    bag_oofs.append(oof_i)\n",
        "    bag_tests.append(test_i)\n",
        "    bag_losses.append(loss_i)\n",
        "\n",
        "# Average across bag members\n",
        "avg_oof = clip_and_renorm(np.mean(bag_oofs, axis=0))\n",
        "avg_test = clip_and_renorm(np.mean(bag_tests, axis=0))\n",
        "avg_loss = log_loss(y, avg_oof, labels=list(range(n_classes)))\n",
        "print(f'[Bag PT+LR lbfgs] Averaged OOF={avg_loss:.6f} | single best={np.min(bag_losses):.6f} | single mean={np.mean(bag_losses):.6f}', flush=True)\n",
        "\n",
        "# Persist for later stacking/blending\n",
        "lr_bag_oof = avg_oof\n",
        "lr_bag_test = avg_test\n",
        "lr_bag_loss = avg_loss\n",
        "\n",
        "# Save bagged LR submission\n",
        "sub = pd.DataFrame(avg_test, columns=classes)\n",
        "sub.insert(0, 'id', test_ids)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (Bagged PT+LR lbfgs) with shape:', sub.shape, flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "ea54816c-32c4-4082-9bf9-5a45ae7625cc",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fallback: alias single LR OOF/test as bagged outputs if bagging not completed\n",
        "if 'lr_bag_oof' not in globals() or 'lr_bag_test' not in globals():\n",
        "    print('Bagged LR not available; using single PT+LR(C=2000) outputs as fallback.', flush=True)\n",
        "    lr_bag_oof = oof.copy()\n",
        "    lr_bag_test = test_pred.copy()\n",
        "    lr_bag_loss = oof_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "ace5fe85-4bdc-4274-8328-51f66faa46d4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensemble (weighted blend) + Temperature Scaling, then submission\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "def clip_and_renorm(probs, eps=1e-15):\n",
        "    P = np.clip(probs, eps, 1 - eps)\n",
        "    P /= P.sum(axis=1, keepdims=True)\n",
        "    return P\n",
        "\n",
        "# Fallback: if bagged LR not available, use single strong PT+LR outputs\n",
        "if 'lr_bag_oof' not in globals() or 'lr_bag_test' not in globals():\n",
        "    print('Bagged LR not available; using single PT+LR(C=2000) outputs as fallback.', flush=True)\n",
        "    lr_bag_oof = oof.copy()\n",
        "    lr_bag_test = test_pred.copy()\n",
        "    lr_bag_loss = oof_loss\n",
        "\n",
        "# Collect bases (LR bag; optionally LGBM if available)\n",
        "bases_oof = [lr_bag_oof]\n",
        "bases_test = [lr_bag_test]\n",
        "labels = ['LR_bag']\n",
        "if 'lgbm_oof' in globals() and 'lgbm_test' in globals():\n",
        "    bases_oof.append(lgbm_oof)\n",
        "    bases_test.append(lgbm_test)\n",
        "    labels.append('LGBM')\n",
        "\n",
        "K = len(bases_oof)\n",
        "print('Ensembling bases:', labels, flush=True)\n",
        "\n",
        "# Optimize 2-model weight w in [0,1]; if only LR, w=1\n",
        "if K == 1:\n",
        "    best_w = np.array([1.0])\n",
        "    oof_blend = clip_and_renorm(bases_oof[0])\n",
        "    test_blend = clip_and_renorm(bases_test[0])\n",
        "else:\n",
        "    ws = np.linspace(0.0, 1.0, 101, dtype=np.float64)\n",
        "    best_w = None; best_loss = float('inf'); oof_blend = None; test_blend = None\n",
        "    for w in ws:\n",
        "        oof_tmp = clip_and_renorm(w * bases_oof[0] + (1.0 - w) * bases_oof[1])\n",
        "        loss = log_loss(y, oof_tmp, labels=list(range(n_classes)))\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss; best_w = np.array([w, 1.0 - w]); oof_blend = oof_tmp\n",
        "    test_blend = clip_and_renorm(best_w[0] * bases_test[0] + best_w[1] * bases_test[1])\n",
        "    print(f'[Blend] best_w={best_w} OOF={best_loss:.6f}', flush=True)\n",
        "\n",
        "# Temperature scaling via power transform: p^alpha then renorm; alpha in (0, +inf).\n",
        "def apply_temp_scaling(P, alpha):\n",
        "    Pp = np.power(np.clip(P, 1e-15, 1.0), alpha)\n",
        "    Pp /= Pp.sum(axis=1, keepdims=True)\n",
        "    return Pp\n",
        "\n",
        "# Grid search alpha in [0.5, 2.0]\n",
        "alphas = np.linspace(0.5, 2.0, 61)\n",
        "best_alpha = 1.0; best_cal_loss = log_loss(y, oof_blend, labels=list(range(n_classes)))\n",
        "for a in alphas:\n",
        "    oof_cal = apply_temp_scaling(oof_blend, a)\n",
        "    loss = log_loss(y, oof_cal, labels=list(range(n_classes)))\n",
        "    if loss < best_cal_loss:\n",
        "        best_cal_loss = loss; best_alpha = a\n",
        "print(f'[TempScale] best_alpha={best_alpha:.3f} OOF={best_cal_loss:.6f}', flush=True)\n",
        "\n",
        "# Apply to test\n",
        "test_final = apply_temp_scaling(test_blend, best_alpha)\n",
        "\n",
        "# Save submission\n",
        "sub = pd.DataFrame(test_final, columns=classes)\n",
        "sub.insert(0, 'id', test_ids)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (Ensembled + TempScaled) with shape:', sub.shape, flush=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagged LR not available; using single PT+LR(C=2000) outputs as fallback.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensembling bases: ['LR_bag']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TempScale] best_alpha=1.675 OOF=0.023995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (Ensembled + TempScaled) with shape: (99, 100)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}