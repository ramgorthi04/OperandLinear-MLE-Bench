{
  "cells": [
    {
      "id": "0f75b087-f176-414f-99b1-123e7b2fbfa7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# XGBoost gblinear on fixed 6-folds with fold-wise StandardScaler and fixed n_estimators\n",
        "import numpy as np, pandas as pd, json, time, sys, subprocess, warnings, os\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED = 2025\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Ensure xgboost is available\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'xgboost>=1.7'], check=True)\n",
        "    import xgboost as xgb\n",
        "\n",
        "def clip_norm(P):\n",
        "    P = np.clip(P, 1e-15, 1-1e-15)\n",
        "    return P / P.sum(axis=1, keepdims=True)\n",
        "\n",
        "def save_probs_and_logits(prefix: str, oof: np.ndarray, test_pred: np.ndarray):\n",
        "    np.save(f'oof_{prefix}.npy', oof.astype(np.float32))\n",
        "    np.save(f'test_{prefix}.npy', test_pred.astype(np.float32))\n",
        "    oof_log = np.log(np.clip(oof, 1e-15, 1.0))\n",
        "    test_log = np.log(np.clip(test_pred, 1e-15, 1.0))\n",
        "    np.save(f'oof_{prefix}_logits.npy', oof_log.astype(np.float32))\n",
        "    np.save(f'test_{prefix}_logits.npy', test_log.astype(np.float32))\n",
        "    print(f'Saved oof_{prefix}.npy, test_{prefix}.npy and *_logits.npy', flush=True)\n",
        "\n",
        "# Load data and folds\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "id_col = 'id'; target_col = 'species'\n",
        "feature_cols = [c for c in train.columns if c not in [id_col, target_col]]\n",
        "X = train[feature_cols].values.astype(np.float64, copy=True)\n",
        "X_test = test[feature_cols].values.astype(np.float64, copy=True)\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(train[target_col].values)\n",
        "K = len(le.classes_)\n",
        "with open('folds_6.json', 'r') as f:\n",
        "    folds = [(np.array(a, dtype=np.int64), np.array(b, dtype=np.int64)) for a,b in json.load(f)]\n",
        "print('Data ready:', X.shape, X_test.shape, 'Classes:', K, 'Folds:', len(folds), flush=True)\n",
        "\n",
        "def run_xgb_gblinear(lrs=(0.1,), lambdas=(100,), alphas=(0.1,), n_estimators=200):\n",
        "    best = (None, 1e9)\n",
        "    best_oof = None\n",
        "    best_test = None\n",
        "    grid = []\n",
        "    for eta in lrs:\n",
        "        for lam in lambdas:\n",
        "            for alp in alphas:\n",
        "                grid.append((eta, lam, alp))\n",
        "    print('Grid size:', len(grid), flush=True)\n",
        "    for gi, (eta, lam, alp) in enumerate(grid, 1):\n",
        "        t0 = time.time()\n",
        "        oof = np.zeros((len(X), K), dtype=np.float64)\n",
        "        test_pred = np.zeros((len(X_test), K), dtype=np.float64)\n",
        "        print(f'[{gi}/{len(grid)}] eta={eta}, lambda={lam}, alpha={alp}', flush=True)\n",
        "        for fi, (trn_idx, val_idx) in enumerate(folds, 1):\n",
        "            trn_idx = np.array(trn_idx, dtype=np.int64); val_idx = np.array(val_idx, dtype=np.int64)\n",
        "            sc = StandardScaler(with_mean=True, with_std=True)\n",
        "            X_tr = sc.fit_transform(X[trn_idx])\n",
        "            X_va = sc.transform(X[val_idx])\n",
        "            X_te = sc.transform(X_test)\n",
        "            clf = xgb.XGBClassifier(\n",
        "                booster='gblinear',\n",
        "                objective='multi:softprob',\n",
        "                num_class=K,\n",
        "                n_estimators=n_estimators,\n",
        "                learning_rate=eta,\n",
        "                reg_lambda=lam,\n",
        "                reg_alpha=alp,\n",
        "                eval_metric='mlogloss',\n",
        "                n_jobs=-1,\n",
        "                random_state=SEED,\n",
        "                verbosity=0\n",
        "            )\n",
        "            fstart = time.time()\n",
        "            clf.fit(X_tr, y[trn_idx], eval_set=[(X_va, y[val_idx])], verbose=False)\n",
        "            P_va = clf.predict_proba(X_va)\n",
        "            P_te = clf.predict_proba(X_te)\n",
        "            oof[val_idx] = P_va\n",
        "            test_pred += P_te / len(folds)\n",
        "            print(f'  [fold {fi}/{len(folds)}] time={time.time()-fstart:.2f}s', flush=True)\n",
        "        ll = log_loss(y, clip_norm(oof), labels=list(range(K)))\n",
        "        print(f'--> OOF={ll:.6f} | params: eta={eta}, lambda={lam}, alpha={alp} | time {time.time()-t0:.2f}s', flush=True)\n",
        "        if ll < best[1]:\n",
        "            best = ((eta, lam, alp), ll)\n",
        "            best_oof = oof\n",
        "            best_test = test_pred\n",
        "    print('Best gblinear:', best, flush=True)\n",
        "    if best_oof is not None:\n",
        "        save_probs_and_logits('xgb_gblinear', best_oof, best_test)\n",
        "    return best, best_oof, best_test\n",
        "\n",
        "best_params, oof_xgb, test_xgb = run_xgb_gblinear()\n",
        "print('Done. Best params:', best_params)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data ready: (891, 192) (99, 192) Classes: 99 Folds: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid size: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/1] eta=0.1, lambda=100, alpha=0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [fold 1/6] time=1.52s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [fold 2/6] time=1.51s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [fold 3/6] time=1.50s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [fold 4/6] time=1.58s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [fold 5/6] time=1.59s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [fold 6/6] time=1.51s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> OOF=4.612579 | params: eta=0.1, lambda=100, alpha=0.1 | time 9.23s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best gblinear: ((0.1, 100, 0.1), 4.6125794410508)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_xgb_gblinear.npy, test_xgb_gblinear.npy and *_logits.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. Best params: ((0.1, 100, 0.1), 4.6125794410508)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}