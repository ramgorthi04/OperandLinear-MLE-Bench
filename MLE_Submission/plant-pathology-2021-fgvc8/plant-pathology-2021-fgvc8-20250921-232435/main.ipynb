{
  "cells": [
    {
      "id": "8e485edb-3ce2-44ba-ab8e-0bc6f3bd991f",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plant Pathology 2021 - FGVC8: Plan to Medal\n",
        "\n",
        "Objectives:\n",
        "- Build a strong, fast baseline for multilabel image classification (micro-F1 metric).\n",
        "- Establish trustworthy CV and iterate with augmentations, better backbones, and ensembling if time allows.\n",
        "\n",
        "Data & Metric:\n",
        "- Images: train_images/, test_images/.\n",
        "- Labels: train.csv has space-separated labels per image.\n",
        "- Evaluation: micro-F1; threshold tuning likely needed.\n",
        "\n",
        "Validation:\n",
        "- Use iterative stratified K-Fold (e.g., 5 folds) on multilabel targets.\n",
        "- Fix a single CV protocol and reuse across experiments.\n",
        "\n",
        "Baseline Model:\n",
        "- timm pretrained CNN (efficientnet_b0/b3 or convnext_tiny) with BCEWithLogitsLoss.\n",
        "- Input size 384 (start with 256 for speed), mixed precision, cosine schedule, warmup, AdamW.\n",
        "- Augs: A.HorizontalFlip, A.RandomResizedCrop, ColorJitter, Normalize.\n",
        "- Inference: TTA (hflip) if useful.\n",
        "\n",
        "Pipeline:\n",
        "1) Environment check: install torch, torchvision, timm, albumentations, iterative-stratification; verify GPU.\n",
        "2) Load train.csv; parse unique classes; build multilabel binarizer.\n",
        "3) Create folds (iterative stratification).\n",
        "4) Dataset/Dataloader with on-the-fly augmentations (albumentations, cv2).\n",
        "5) Train per fold with AMP + early stopping; log per-epoch micro-F1 on val.\n",
        "6) Save OOF logits and test logits per fold; average logits across folds.\n",
        "7) Threshold tuning on OOF (global threshold and optionally per-class).\n",
        "8) Generate submission.csv; verify format matches sample_submission.csv.\n",
        "\n",
        "Iteration Targets:\n",
        "- Baseline: effnet_b0, img_size=256, 5 folds, 5-8 epochs -> get solid OOF.\n",
        "- Improve: bigger backbone (b3/convnext_tiny), better augs, img_size=384, more epochs.\n",
        "- Ensembling: blend diverse seeds/backbones/resolutions.\n",
        "\n",
        "Milestones (request expert review at each):\n",
        "A) After environment + data sanity checks.\n",
        "B) After CV split + baseline training setup.\n",
        "C) After first OOF results (analyze error buckets + thresholding).\n",
        "D) After improved backbone / resolution or blends.\n",
        "\n",
        "Risks & Mitigations:\n",
        "- GPU issues: verify before training.\n",
        "- Overfitting/leakage: strict fold discipline; fit transforms inside folds.\n",
        "- Slow iterations: start with 2 folds/2-3 epochs smoke test before full runs."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "12f082b3-57a7-4849-9df0-0926bb9e6535",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment check + installs (idempotent) + data sanity\n",
        "import sys, subprocess, os, warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "print('Python:', sys.version)\n",
        "print('CWD:', os.getcwd())\n",
        "\n",
        "def ensure(pkg, import_name=None):\n",
        "    import importlib\n",
        "    name = import_name or pkg\n",
        "    try:\n",
        "        importlib.import_module(name)\n",
        "        print(f'OK: {pkg}')\n",
        "    except Exception as e:\n",
        "        print(f'Installing {pkg} ...')\n",
        "        subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n",
        "        importlib.import_module(name)\n",
        "        print(f'Installed: {pkg}')\n",
        "\n",
        "# Core deps (torch likely preinstalled) - install only if missing\n",
        "ensure('torch')\n",
        "ensure('torchvision')\n",
        "ensure('timm')\n",
        "ensure('albumentations')\n",
        "ensure('iterative-stratification', 'iterstrat')\n",
        "ensure('opencv-python', 'cv2')\n",
        "ensure('scikit-learn', 'sklearn')\n",
        "ensure('pandas')\n",
        "ensure('numpy')\n",
        "\n",
        "import torch, torchvision\n",
        "import pandas as pd, numpy as np, cv2\n",
        "print(f'GPU Available: {torch.cuda.is_available()}')\n",
        "print(f'GPU Count: {torch.cuda.device_count()}')\n",
        "if torch.cuda.is_available():\n",
        "    try:\n",
        "        print(f'GPU Name: {torch.cuda.get_device_name(0)}')\n",
        "        props = torch.cuda.get_device_properties(0)\n",
        "        print(f'GPU Memory: {props.total_memory / 1024**3:.2f} GB')\n",
        "    except Exception as e:\n",
        "        print('GPU query error:', e)\n",
        "\n",
        "# Load CSVs\n",
        "train_csv = 'train.csv'\n",
        "sub_csv = 'sample_submission.csv'\n",
        "train_df = pd.read_csv(train_csv)\n",
        "sub_df = pd.read_csv(sub_csv)\n",
        "print('Train shape:', train_df.shape)\n",
        "print('Sample submission shape:', sub_df.shape)\n",
        "print(train_df.head(3))\n",
        "print(sub_df.head(3))\n",
        "\n",
        "# Submission format is two columns: image, labels (space-separated)\n",
        "id_col = sub_df.columns[0]\n",
        "assert id_col == 'image', f'Unexpected ID column: {id_col}'\n",
        "print('ID column:', id_col)\n",
        "\n",
        "# Derive class list from training labels (space-separated tokens)\n",
        "tokens = train_df['labels'].fillna('').str.split(' ')\n",
        "all_labels = sorted({t for lst in tokens for t in lst if t != ''})\n",
        "class_cols = all_labels\n",
        "class_to_idx = {c:i for i,c in enumerate(class_cols)}\n",
        "print('Classes (derived from train):', class_cols)\n",
        "\n",
        "# Build multi-hot matrix Y in class order\n",
        "Y = np.zeros((len(train_df), len(class_cols)), dtype=np.int8)\n",
        "for i, labs in enumerate(tokens):\n",
        "    for l in labs:\n",
        "        Y[i, class_to_idx[l]] = 1\n",
        "print('Label matrix shape:', Y.shape, 'Positives:', int(Y.sum()))\n",
        "print('Per-class positives:', dict(zip(class_cols, Y.sum(0).astype(int))))\n",
        "\n",
        "# Quick file existence checks\n",
        "train_dir = 'train_images'\n",
        "test_dir = 'test_images'\n",
        "n_train_imgs = len([f for f in os.listdir(train_dir) if f.lower().endswith('.jpg')]) if os.path.isdir(train_dir) else -1\n",
        "n_test_imgs = len([f for f in os.listdir(test_dir) if f.lower().endswith('.jpg')]) if os.path.isdir(test_dir) else -1\n",
        "print('Train images count:', n_train_imgs, 'Test images count:', n_test_imgs)\n",
        "\n",
        "# Verify a few paths\n",
        "for p in train_df['image'].head(5).tolist():\n",
        "    fp = os.path.join(train_dir, p)\n",
        "    print(p, 'exists' if os.path.isfile(fp) else 'MISSING')\n",
        "\n",
        "print('Setup complete.')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]\nCWD: /var/lib/simon/agent_run_states/plant-pathology-2021-fgvc8-20250921-232435\nOK: torch\nOK: torchvision\nOK: timm\nOK: albumentations\nOK: iterative-stratification\nOK: opencv-python\nOK: scikit-learn\nOK: pandas\nOK: numpy\nGPU Available: True\nGPU Count: 1\nGPU Name: NVIDIA A10-24Q\nGPU Memory: 23.72 GB\nTrain shape: (14905, 2)\nSample submission shape: (3727, 2)\n                  image                      labels\n0  95cb4b75ad1d842a.jpg                     complex\n1  be80d450dac87d7c.jpg  frog_eye_leaf_spot complex\n2  80cac5abcfa9c9b4.jpg                     healthy\n                  image   labels\n0  ca6a50c5d2adb8ae.jpg  healthy\n1  b686d217a1e2e3a5.jpg  healthy\n2  c9a5345ec78b4ac5.jpg  healthy\nID column: image\nClasses (derived from train): ['complex', 'frog_eye_leaf_spot', 'healthy', 'powdery_mildew', 'rust', 'scab']\nLabel matrix shape: (14905, 6) Positives: 16153\nPer-class positives: {'complex': 1711, 'frog_eye_leaf_spot': 3511, 'healthy': 3673, 'powdery_mildew': 1008, 'rust': 1650, 'scab': 4600}\nTrain images count: 14905 Test images count: 3727\n95cb4b75ad1d842a.jpg exists\nbe80d450dac87d7c.jpg exists\n80cac5abcfa9c9b4.jpg exists\nd4fd02931f17c86a.jpg exists\nafbaa52a94e51a86.jpg exists\nSetup complete.\n"
          ]
        }
      ]
    },
    {
      "id": "a4c210ac-544c-4e6a-9ad7-3a4460519bc0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create multilabel stratified folds (seed=42); save to disk for reuse\n",
        "import os, numpy as np, pandas as pd\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "# Expect train_df, Y, id_col, class_cols from previous cell\n",
        "assert 'train_df' in globals() and 'Y' in globals(), 'Run env/data cell first'\n",
        "\n",
        "n_splits = 5\n",
        "seed = 42\n",
        "mskf = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "folds = np.full(len(train_df), -1, dtype=int)\n",
        "for f, (_, val_idx) in enumerate(mskf.split(train_df, Y)):\n",
        "    folds[val_idx] = f\n",
        "\n",
        "train_df_folds = train_df.copy()\n",
        "train_df_folds['fold'] = folds\n",
        "out_path = 'train_folds.csv'\n",
        "train_df_folds.to_csv(out_path, index=False)\n",
        "print('Saved folds to', out_path)\n",
        "\n",
        "# Per-fold label distribution sanity\n",
        "for f in range(n_splits):\n",
        "    idx = np.where(folds == f)[0]\n",
        "    y_sum = Y[idx].sum(0)\n",
        "    print(f'Fold {f}: n={len(idx)}, positives per class:', dict(zip(class_cols, map(int, y_sum))))\n",
        "print('Folds ready.')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved folds to train_folds.csv\nFold 0: n=2977, positives per class: {'complex': 342, 'frog_eye_leaf_spot': 702, 'healthy': 735, 'powdery_mildew': 202, 'rust': 330, 'scab': 920}\nFold 1: n=2992, positives per class: {'complex': 342, 'frog_eye_leaf_spot': 702, 'healthy': 734, 'powdery_mildew': 202, 'rust': 330, 'scab': 920}\nFold 2: n=2975, positives per class: {'complex': 343, 'frog_eye_leaf_spot': 703, 'healthy': 735, 'powdery_mildew': 201, 'rust': 330, 'scab': 920}\nFold 3: n=2993, positives per class: {'complex': 342, 'frog_eye_leaf_spot': 702, 'healthy': 734, 'powdery_mildew': 202, 'rust': 330, 'scab': 920}\nFold 4: n=2968, positives per class: {'complex': 342, 'frog_eye_leaf_spot': 702, 'healthy': 735, 'powdery_mildew': 201, 'rust': 330, 'scab': 920}\nFolds ready.\n"
          ]
        }
      ]
    },
    {
      "id": "91397a47-6a25-4b63-b17f-30ff76788595",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Smoke test training: Dataset, Model, 1-fold quick run (convnext_tiny @ 256, 2 epochs)\n",
        "import os, time, math, random, gc\n",
        "import numpy as np, pandas as pd, cv2, torch, timm\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "IMG_SIZE = 256  # 256 for smoke test; later 384+\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 2\n",
        "LR = 2e-4\n",
        "WD = 1e-4\n",
        "NUM_CLASSES = len(class_cols)\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "ID_COL = id_col\n",
        "TRAIN_DIR = 'train_images'\n",
        "TEST_DIR = 'test_images'\n",
        "\n",
        "train_tfms = A.Compose([\n",
        "    A.RandomResizedCrop(size=(IMG_SIZE, IMG_SIZE), scale=(0.7, 1.0), ratio=(0.75, 1.33), p=1.0),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.2),\n",
        "    A.RandomRotate90(p=0.2),\n",
        "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=20, p=0.5, border_mode=cv2.BORDER_REFLECT101),\n",
        "    A.ColorJitter(0.2,0.2,0.2,0.1,p=0.5),\n",
        "    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "val_tfms = A.Compose([\n",
        "    A.Resize(height=IMG_SIZE, width=IMG_SIZE),\n",
        "    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "class PlantDataset(Dataset):\n",
        "    def __init__(self, df, labels=None, img_dir=TRAIN_DIR, tfms=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.labels = labels\n",
        "        self.img_dir = img_dir\n",
        "        self.tfms = tfms\n",
        "        self.has_labels = labels is not None\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['image'])\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        if img is None:\n",
        "            raise FileNotFoundError(f'Image not found: {img_path}')\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        if self.tfms: \n",
        "            img = self.tfms(image=img)['image']\n",
        "        if self.has_labels:\n",
        "            target = self.labels[idx].astype(np.float32)\n",
        "            return img, torch.from_numpy(target)\n",
        "        else:\n",
        "            return img, row['image']\n",
        "\n",
        "def build_model():\n",
        "    model = timm.create_model('convnext_tiny', pretrained=True, num_classes=NUM_CLASSES)\n",
        "    return model\n",
        "\n",
        "def get_loaders(fold=0):\n",
        "    df = pd.read_csv('train_folds.csv')\n",
        "    trn_idx = df.index[df.fold != fold].tolist()\n",
        "    val_idx = df.index[df.fold == fold].tolist()\n",
        "    trn_ds = PlantDataset(df.loc[trn_idx, ['image']], labels=Y[trn_idx], img_dir=TRAIN_DIR, tfms=train_tfms)\n",
        "    val_ds = PlantDataset(df.loc[val_idx, ['image']], labels=Y[val_idx], img_dir=TRAIN_DIR, tfms=val_tfms)\n",
        "    trn_ld = DataLoader(trn_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
        "    val_ld = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "    return trn_ld, val_ld, val_idx\n",
        "\n",
        "def micro_f1_from_logits(logits, targets, th=0.3):\n",
        "    probs = torch.sigmoid(torch.tensor(logits)) if not torch.is_tensor(logits) else torch.sigmoid(logits)\n",
        "    preds = (probs.cpu().numpy() >= th).astype(int)\n",
        "    t = targets.cpu().numpy() if torch.is_tensor(targets) else targets\n",
        "    # enforce at-least-one rule\n",
        "    rows_all_zero = preds.sum(1) == 0\n",
        "    if rows_all_zero.any():\n",
        "        top1 = probs.cpu().numpy().argmax(1)\n",
        "        preds[rows_all_zero, top1[rows_all_zero]] = 1\n",
        "    return f1_score(t.reshape(-1), preds.reshape(-1), average='micro')\n",
        "\n",
        "def train_one_fold(fold=0):\n",
        "    set_seed(42)\n",
        "    trn_ld, val_ld, val_idx = get_loaders(fold)\n",
        "    model = build_model().to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    total_steps = EPOCHS * len(trn_ld)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE=='cuda'))\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    best_f1, best_state = -1.0, None\n",
        "    print(f'Start fold {fold} | steps/epoch={len(trn_ld)} | val_batches={len(val_ld)}', flush=True)\n",
        "    start = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        running = 0.0\n",
        "        for bi, (imgs, tgts) in enumerate(trn_ld):\n",
        "            imgs = imgs.to(DEVICE, non_blocking=True)\n",
        "            tgts = tgts.to(DEVICE, non_blocking=True)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=(DEVICE=='cuda')):\n",
        "                logits = model(imgs)\n",
        "                loss = criterion(logits, tgts)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            running += loss.item()\n",
        "            if (bi+1) % 50 == 0:\n",
        "                elapsed = time.time() - start\n",
        "                print(f'Epoch {epoch+1}/{EPOCHS} | step {bi+1}/{len(trn_ld)} | loss {running/(bi+1):.4f} | {elapsed/60:.1f} min', flush=True)\n",
        "        # validate\n",
        "        model.eval()\n",
        "        val_logits, val_targets = [], []\n",
        "        with torch.no_grad():\n",
        "            for imgs, tgts in val_ld:\n",
        "                imgs = imgs.to(DEVICE, non_blocking=True)\n",
        "                logits = model(imgs).detach().cpu()\n",
        "                val_logits.append(logits)\n",
        "                val_targets.append(tgts)\n",
        "        val_logits = torch.cat(val_logits, 0)\n",
        "        val_targets = torch.cat(val_targets, 0)\n",
        "        f1 = micro_f1_from_logits(val_logits, val_targets, th=0.3)\n",
        "        print(f'Fold {fold} Epoch {epoch+1}: val micro-F1 @0.3 = {f1:.4f}', flush=True)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n",
        "    print(f'Best val micro-F1: {best_f1:.4f}')\n",
        "    # Return best model and val logits for threshold tuning later\n",
        "    model.load_state_dict(best_state)\n",
        "    return model, val_idx\n",
        "\n",
        "def predict_test(model):\n",
        "    test_df = pd.read_csv('sample_submission.csv')[[ID_COL]].copy()\n",
        "    ds = PlantDataset(test_df, labels=None, img_dir=TEST_DIR, tfms=val_tfms)\n",
        "    dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, names in dl:\n",
        "            imgs = imgs.to(DEVICE, non_blocking=True)\n",
        "            logits = model(imgs)\n",
        "            preds.append(logits.detach().cpu())\n",
        "    return torch.cat(preds, 0).numpy(), test_df[ID_COL].tolist()\n",
        "\n",
        "def logits_to_labels_str(logits, th=0.3):\n",
        "    probs = 1/(1+np.exp(-logits))\n",
        "    bin_ = (probs >= th).astype(int)\n",
        "    # at-least-one rule\n",
        "    rows_zero = bin_.sum(1) == 0\n",
        "    if rows_zero.any():\n",
        "        top1 = probs.argmax(1)\n",
        "        bin_[rows_zero, top1[rows_zero]] = 1\n",
        "    labels = []\n",
        "    for r in bin_:\n",
        "        labels.append(' '.join([class_cols[i] for i in np.where(r==1)[0]]))\n",
        "    return labels\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model, _ = train_one_fold(fold=0)\n",
        "    test_logits, test_names = predict_test(model)\n",
        "    labels_str = logits_to_labels_str(test_logits, th=0.3)\n",
        "    sub = pd.DataFrame({ID_COL: test_names, 'labels': labels_str})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Wrote submission.csv with shape', sub.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start fold 0 | steps/epoch=186 | val_batches=47\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}