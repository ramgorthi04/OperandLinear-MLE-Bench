{
  "cells": [
    {
      "id": "7e30831c-9257-4ab8-8194-ee820f8527c3",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plant Pathology 2021 - FGVC8: Plan\n",
        "\n",
        "Objectives:\n",
        "- Verify GPU availability and environment.\n",
        "- Inspect data schema (train.csv, sample_submission.csv) and image directories.\n",
        "- Establish robust CV and baseline image model (torch/TF) with multilabel micro-F1.\n",
        "- Produce a working submission quickly, then iterate on aug/resolution/architectures and ensembling.\n",
        "\n",
        "Initial Plan:\n",
        "1) Env check (GPU).\n",
        "2) Load train.csv/test images; verify multilabel format and submission format.\n",
        "3) Quick EDA: label distribution, any leaks, filename mapping.\n",
        "4) Baseline: torchvision + timm (e.g., tf_efficientnet_b0/b3, resnet50), resolution 512, strong aug (albumentations), BCEWithLogitsLoss, sigmoid threshold tuning on CV.\n",
        "5) CV: StratifiedKFold for multilabel via iterative stratification (skmultilearn/iterstrat) or multilabel stratifier; otherwise GroupKFold if groups exist.\n",
        "6) Logging: per-fold micro-F1, OOF saving, test-time augmentation, threshold calibration.\n",
        "7) Submit baseline; iterate with higher res, different backbones, seeds, and blends.\n",
        "\n",
        "Checkpoints for Expert Review:\n",
        "- After env/EDA, after baseline CV setup, after first baseline OOF, after improvements/ensembles."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "abae4a80-a218-4f52-8a77-32a4b942a024",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, subprocess, time, glob, sys\n",
        "import pandas as pd\n",
        "\n",
        "def run(cmd):\n",
        "    print(\"$\", \" \".join(cmd), flush=True)\n",
        "    try:\n",
        "        out = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, check=False)\n",
        "        print(out.stdout)\n",
        "    except Exception as e:\n",
        "        print(\"Command failed:\", e)\n",
        "\n",
        "print(\"Env check: nvidia-smi\")\n",
        "run(['bash','-lc','nvidia-smi || true'])\n",
        "\n",
        "base = os.getcwd()\n",
        "print(\"CWD:\", base)\n",
        "print(\"Listing top-level files:\")\n",
        "for p in sorted(os.listdir(base)):\n",
        "    print(\" -\", p)\n",
        "\n",
        "train_csv = 'train.csv'\n",
        "sample_csv = 'sample_submission.csv' if os.path.exists('sample_submission.csv') else None\n",
        "print(\"\\nLoading CSVs...\")\n",
        "train_df = pd.read_csv(train_csv)\n",
        "print(\"train.csv shape:\", train_df.shape)\n",
        "print(train_df.head(3))\n",
        "if sample_csv:\n",
        "    sample_df = pd.read_csv(sample_csv)\n",
        "    print(\"sample_submission.csv shape:\", sample_df.shape)\n",
        "    print(sample_df.head(3))\n",
        "\n",
        "train_images_dir = 'train_images'\n",
        "test_images_dir = 'test_images'\n",
        "train_imgs = glob.glob(os.path.join(train_images_dir, '*'))\n",
        "test_imgs = glob.glob(os.path.join(test_images_dir, '*'))\n",
        "print(f\"Train images: {len(train_imgs)} | Test images: {len(test_imgs)}\")\n",
        "if len(train_imgs)>0:\n",
        "    print(\"Sample train images:\", [os.path.basename(p) for p in train_imgs[:5]])\n",
        "if len(test_imgs)>0:\n",
        "    print(\"Sample test images:\", [os.path.basename(p) for p in test_imgs[:5]])\n",
        "\n",
        "print(\"\\nColumns in train.csv:\", list(train_df.columns))\n",
        "print(\"Done env/data probe.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "1b359415-dad7-4ce0-8aed-51e225b0ad71",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, subprocess, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "def pip(*args):\n",
        "    print('>', *args, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "# 0) Uninstall any preinstalled torch stack to avoid mismatches\n",
        "for pkg in ('torch','torchvision','torchaudio'):\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', pkg], check=False)\n",
        "\n",
        "# Clean stray site dirs (idempotent)\n",
        "for d in (\n",
        "    '/app/.pip-target/torch',\n",
        "    '/app/.pip-target/torchvision',\n",
        "    '/app/.pip-target/torchaudio',\n",
        "    '/app/.pip-target/torchgen',\n",
        "    '/app/.pip-target/functorch',\n",
        "):\n",
        "    if os.path.exists(d):\n",
        "        print('Removing', d)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "# 1) Install exact CUDA 12.1 torch stack\n",
        "pip('install',\n",
        "    '--index-url', 'https://download.pytorch.org/whl/cu121',\n",
        "    '--extra-index-url', 'https://pypi.org/simple',\n",
        "    'torch==2.4.1', 'torchvision==0.19.1', 'torchaudio==2.4.1')\n",
        "\n",
        "# 2) Freeze versions\n",
        "Path('constraints.txt').write_text('torch==2.4.1\\ntorchvision==0.19.1\\ntorchaudio==2.4.1\\n')\n",
        "\n",
        "# 3) Install non-torch deps honoring constraints\n",
        "pip('install', '-c', 'constraints.txt',\n",
        "    'timm==1.0.9',\n",
        "    'albumentations==1.4.14',\n",
        "    'opencv-python-headless',\n",
        "    'iterative-stratification',\n",
        "    'scikit-learn',\n",
        "    'pandas', 'numpy',\n",
        "    '--upgrade-strategy', 'only-if-needed')\n",
        "\n",
        "# 4) Sanity check GPU\n",
        "import torch\n",
        "print('torch:', torch.__version__, 'built CUDA:', getattr(torch.version, 'cuda', None))\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "assert str(getattr(torch.version,'cuda','')).startswith('12.1'), f'Wrong CUDA build: {torch.version.cuda}'\n",
        "assert torch.cuda.is_available(), 'CUDA not available'\n",
        "print('GPU:', torch.cuda.get_device_name(0))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.4.1+cu121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uninstalling torch-2.4.1+cu121:\n  Successfully uninstalled torch-2.4.1+cu121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchvision 0.19.1+cu121\nUninstalling torchvision-0.19.1+cu121:\n  Successfully uninstalled torchvision-0.19.1+cu121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchaudio 2.4.1+cu121\nUninstalling torchaudio-2.4.1+cu121:\n  Successfully uninstalled torchaudio-2.4.1+cu121\n> install --index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://pypi.org/simple torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121, https://pypi.org/simple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (799.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 MB 313.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.19.1\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.1/7.1 MB 455.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.4/3.4 MB 494.9 MB/s eta 0:00:00\nCollecting typing-extensions>=4.8.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 4.0 MB/s eta 0:00:00\nCollecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 83.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 234.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 165.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 243.4 MB/s eta 0:00:00\nCollecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 306.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 133.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 190.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 188.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fsspec\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 490.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 193.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 192.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 150.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 158.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 384.6 MB/s eta 0:00:00\nCollecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 151.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 139.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 190.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow!=8.3.*,>=5.3.0\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 247.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 177.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 501.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.2 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pillow-11.3.0 sympy-1.14.0 torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121 triton-3.0.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/nvidia_cusolver_cu12-11.4.5.107.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2-3.1.6.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cudnn_cu12-9.1.0.70.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusparse_cu12-12.1.0.106.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton-3.0.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock-3.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec-2025.9.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/MarkupSafe-3.0.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx-3.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cublas_cu12-12.1.3.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_cupti_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_nvrtc_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_runtime_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cufft_cu12-11.0.2.54.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_curand_cu12-10.3.2.106.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nccl_cu12-2.20.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvjitlink_cu12-12.9.86.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvtx_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow-11.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/PIL already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy-1.14.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/isympy.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath-1.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/share already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> install -c constraints.txt timm==1.0.9 albumentations==1.4.14 opencv-python-headless iterative-stratification scikit-learn pandas numpy --upgrade-strategy only-if-needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm==1.0.9\n  Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.3/2.3 MB 55.7 MB/s eta 0:00:00\nCollecting albumentations==1.4.14\n  Downloading albumentations-1.4.14-py3-none-any.whl (177 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 178.0/178.0 KB 266.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-python-headless\n  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 54.0/54.0 MB 193.3 MB/s eta 0:00:00\nCollecting iterative-stratification\n  Downloading iterative_stratification-0.1.9-py3-none-any.whl (8.5 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.7/9.7 MB 115.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas\n  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.4/12.4 MB 34.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 228.1 MB/s eta 0:00:00\nCollecting torch\n  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 797.1/797.1 MB 72.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface_hub\n  Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 563.3/563.3 KB 378.2 MB/s eta 0:00:00\nCollecting safetensors\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 485.8/485.8 KB 437.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyyaml\n  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 806.6/806.6 KB 527.2 MB/s eta 0:00:00\nCollecting torchvision\n  Downloading torchvision-0.19.1-cp311-cp311-manylinux1_x86_64.whl (7.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.0/7.0 MB 207.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy>=1.10.0\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.9/35.9 MB 135.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydantic>=2.7.0\n  Downloading pydantic-2.11.9-py3-none-any.whl (444 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 444.9/444.9 KB 510.7 MB/s eta 0:00:00\nCollecting albucore>=0.0.13\n  Downloading albucore-0.0.33-py3-none-any.whl (18 kB)\nCollecting scikit-image>=0.21.0\n  Downloading scikit_image-0.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.8/14.8 MB 54.5 MB/s eta 0:00:00\nCollecting typing-extensions>=4.9.0\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 415.0 MB/s eta 0:00:00\nCollecting eval-type-backport\n  Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\nCollecting opencv-python-headless\n  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 50.0/50.0 MB 222.9 MB/s eta 0:00:00\nCollecting joblib>=1.2.0\n  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 308.4/308.4 KB 507.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting threadpoolctl>=3.1.0\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nCollecting python-dateutil>=2.8.2\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 229.9/229.9 KB 473.8 MB/s eta 0:00:00\nCollecting tzdata>=2022.7\n  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 347.8/347.8 KB 505.0 MB/s eta 0:00:00\nCollecting pytz>=2020.1\n  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 509.2/509.2 KB 493.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting simsimd>=5.9.2\n  Downloading simsimd-6.5.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.1/1.1 MB 137.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stringzilla>=3.10.4\n  Downloading stringzilla-4.0.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (496 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 496.5/496.5 KB 408.2 MB/s eta 0:00:00\nCollecting annotated-types>=0.6.0\n  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydantic-core==2.33.2\n  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 520.0 MB/s eta 0:00:00\nCollecting typing-inspection>=0.4.0\n  Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\nCollecting six>=1.5\n  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\nCollecting lazy-loader>=0.4\n  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow>=10.1\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 173.1 MB/s eta 0:00:00\nCollecting tifffile>=2022.8.12\n  Downloading tifffile-2025.9.20-py3-none-any.whl (230 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 230.1/230.1 KB 460.3 MB/s eta 0:00:00\nCollecting packaging>=21\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 392.1 MB/s eta 0:00:00\nCollecting imageio!=2.35.0,>=2.33\n  Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 315.8/315.8 KB 452.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting networkx>=3.0\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 490.3 MB/s eta 0:00:00\nCollecting fsspec>=2023.5.0\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 418.2 MB/s eta 0:00:00\nCollecting requests\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.7/64.7 KB 320.3 MB/s eta 0:00:00\nCollecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\nCollecting hf-xet<2.0.0,>=1.1.3\n  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.2/3.2 MB 424.8 MB/s eta 0:00:00\nCollecting tqdm>=4.42.1\n  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.5/78.5 KB 431.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 292.9 MB/s eta 0:00:00\nCollecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 181.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 245.6 MB/s eta 0:00:00\nCollecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 238.6 MB/s eta 0:00:00\nCollecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 562.1 MB/s eta 0:00:00\nCollecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 262.3 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 253.2 MB/s eta 0:00:00\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 217.6 MB/s eta 0:00:00\nCollecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 217.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 423.0 MB/s eta 0:00:00\nCollecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 451.5 MB/s eta 0:00:00\nCollecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 37.5 MB/s eta 0:00:00\nCollecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 508.3 MB/s eta 0:00:00\nCollecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 180.6 MB/s eta 0:00:00\nCollecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 233.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\nCollecting idna<4,>=2.5\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 70.4/70.4 KB 400.1 MB/s eta 0:00:00\nCollecting urllib3<3,>=1.21.1\n  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 129.8/129.8 KB 477.5 MB/s eta 0:00:00\nCollecting charset_normalizer<4,>=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 150.3/150.3 KB 492.9 MB/s eta 0:00:00\nCollecting certifi>=2017.4.17\n  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 161.2/161.2 KB 461.5 MB/s eta 0:00:00\nCollecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 519.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: simsimd, pytz, mpmath, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, sympy, stringzilla, six, safetensors, pyyaml, pillow, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, joblib, idna, hf-xet, fsspec, filelock, eval-type-backport, charset_normalizer, certifi, annotated-types, typing-inspection, triton, tifffile, scipy, requests, python-dateutil, pydantic-core, opencv-python-headless, nvidia-cusparse-cu12, nvidia-cudnn-cu12, lazy-loader, jinja2, imageio, scikit-learn, scikit-image, pydantic, pandas, nvidia-cusolver-cu12, huggingface_hub, albucore, torch, iterative-stratification, albumentations, torchvision, timm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.2 albucore-0.0.33 albumentations-1.4.14 annotated-types-0.7.0 certifi-2025.8.3 charset_normalizer-3.4.3 eval-type-backport-0.2.2 filelock-3.19.1 fsspec-2025.9.0 hf-xet-1.1.10 huggingface_hub-0.35.1 idna-3.10 imageio-2.37.0 iterative-stratification-0.1.9 jinja2-3.1.6 joblib-1.5.2 lazy-loader-0.4 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 opencv-python-headless-4.11.0.86 packaging-25.0 pandas-2.3.2 pillow-11.3.0 pydantic-2.11.9 pydantic-core-2.33.2 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.3 requests-2.32.5 safetensors-0.6.2 scikit-image-0.25.2 scikit-learn-1.7.2 scipy-1.16.2 simsimd-6.5.3 six-1.17.0 stringzilla-4.0.14 sympy-1.14.0 threadpoolctl-3.6.0 tifffile-2025.9.20 timm-1.0.9 torch-2.4.1 torchvision-0.19.1 tqdm-4.67.1 triton-3.0.0 typing-extensions-4.15.0 typing-inspection-0.4.1 tzdata-2025.2 urllib3-2.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/timm already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/timm-1.0.9.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchvision already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchvision.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchvision-0.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/albumentations already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/iterative_stratification-0.1.9.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tests already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/iterstrat already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/functorch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchgen already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torch-2.4.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/huggingface_hub-0.35.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/huggingface_hub already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusolver_cu12-11.4.5.107.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pandas-2.3.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pandas already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pydantic-2.11.9.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pydantic already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scikit_image-0.25.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/skimage already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scikit_learn-1.7.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sklearn already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scikit_learn.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/imageio-2.37.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/imageio already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2-3.1.6.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/lazy_loader-0.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/lazy_loader already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cudnn_cu12-9.1.0.70.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusparse_cu12-12.1.0.106.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/cv2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/opencv_python_headless-4.11.0.86.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/opencv_python_headless.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pydantic_core already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pydantic_core-2.33.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/python_dateutil-2.9.0.post0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/dateutil already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/requests-2.32.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/requests already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy-1.16.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tifffile-2025.9.20.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tifffile already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton-3.0.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_inspection-0.4.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_inspection already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/annotated_types-0.7.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/annotated_types already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/certifi-2025.8.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/certifi already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/charset_normalizer-3.4.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/charset_normalizer already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/eval_type_backport-0.2.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/eval_type_backport already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock-3.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec-2025.9.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/hf_xet-1.1.10.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/hf_xet already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/idna-3.10.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/idna already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/joblib-1.5.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/joblib already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/MarkupSafe-3.0.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx-3.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cublas_cu12-12.1.3.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_cupti_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_nvrtc_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_runtime_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cufft_cu12-11.0.2.54.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_curand_cu12-10.3.2.106.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nccl_cu12-2.20.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvjitlink_cu12-12.9.86.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvtx_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging-25.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow-11.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/PIL already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pyyaml-6.0.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/yaml already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/_yaml already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/safetensors already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/safetensors-0.6.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/six-1.17.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/six.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/cli already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/stringzilla-4.0.14.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/stringzilla.cpython-311-x86_64-linux-gnu.so already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy-1.14.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/isympy.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/threadpoolctl-3.6.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/threadpoolctl.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tqdm-4.67.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tqdm already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tzdata-2025.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tzdata already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/urllib3-2.5.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/urllib3 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath-1.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pytz-2025.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pytz already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/simsimd-6.5.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/simsimd.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/simsimd already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/simsimd.cpython-311-x86_64-linux-gnu.so already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/share already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.4.1+cu121 built CUDA: 12.1\nCUDA available: True\nGPU: NVIDIA A10-24Q\n"
          ]
        }
      ]
    },
    {
      "id": "9037ae79-140b-4ca3-aa56-d11eb988f353",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, json, numpy as np, pandas as pd\n",
        "from collections import Counter\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "# Reload train to be safe in this cell context\n",
        "train_df = pd.read_csv('train.csv')\n",
        "\n",
        "# Parse space-delimited multilabels\n",
        "label_lists = train_df['labels'].astype(str).str.strip().str.split()\n",
        "all_labels = [lab for labs in label_lists for lab in labs]\n",
        "label_counts = Counter(all_labels)\n",
        "classes = sorted(label_counts.keys())  # preserve alpha order; alternative: sort by freq\n",
        "print('Num classes:', len(classes))\n",
        "print('Classes:', classes)\n",
        "print('Top counts:', label_counts.most_common(10))\n",
        "\n",
        "# Multi-hot encode\n",
        "cls2id = {c:i for i,c in enumerate(classes)}\n",
        "y = np.zeros((len(train_df), len(classes)), dtype=np.uint8)\n",
        "for i, labs in enumerate(label_lists):\n",
        "    for lab in labs:\n",
        "        y[i, cls2id[lab]] = 1\n",
        "\n",
        "# Save class list for reuse\n",
        "with open('classes.json','w') as f:\n",
        "    json.dump({'classes': classes}, f)\n",
        "print('Saved classes.json')\n",
        "\n",
        "# 5-fold Multilabel Stratified CV\n",
        "mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = np.full(len(train_df), -1, dtype=int)\n",
        "for fold, (_, val_idx) in enumerate(mskf.split(train_df['image'].values, y)):\n",
        "    folds[val_idx] = fold\n",
        "assert (folds>=0).all(), 'Fold assignment failed'\n",
        "\n",
        "train_folds = train_df.copy()\n",
        "train_folds['fold'] = folds\n",
        "train_folds.to_csv('train_folds.csv', index=False)\n",
        "print('Saved train_folds.csv with fold distribution:')\n",
        "print(train_folds['fold'].value_counts().sort_index())\n",
        "\n",
        "# Basic sanity: label distribution per fold\n",
        "for f in range(5):\n",
        "    idx = (folds==f)\n",
        "    cnt = y[idx].sum(axis=0)\n",
        "    print(f'Fold {f}: n={idx.sum()} | positive labels total={int(cnt.sum())}')\n",
        "print('CV setup complete.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "e4e7e0df-ea6a-49d4-8dd8-99fb08f735ce",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, time, math, json, random, gc\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "import timm\n",
        "from timm.utils import ModelEmaV2\n",
        "from timm.loss import AsymmetricLossMultiLabel\n",
        "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "if hasattr(torch.backends, 'cuda') and hasattr(torch.backends.cuda, 'matmul'):\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_fp16_reduced_precision_reduction = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = os.environ.get('PYTORCH_CUDA_ALLOC_CONF','') or 'expandable_segments:True'\n",
        "cv2.setNumThreads(0)\n",
        "\n",
        "# Config\n",
        "IMG_SIZE = 512\n",
        "BATCH_SIZE = 10\n",
        "EPOCHS = 15  # per expert, train up to 15 with early stopping\n",
        "LR = 3e-4\n",
        "WD = 1e-2\n",
        "NUM_FOLDS = 5  # full CV\n",
        "MODEL_NAME = 'tf_efficientnetv2_m.in21k'\n",
        "DROP_PATH = 0.2\n",
        "DROP_RATE = 0.05\n",
        "TRAIN_DIR = 'train_images'\n",
        "TEST_DIR = 'test_images'\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "DO_TRAIN = True  # train full 5-folds\n",
        "\n",
        "# Performance/stability toggles\n",
        "USE_CHANNELS_LAST = True\n",
        "USE_BF16_AMP = True\n",
        "USE_GRAD_CKPT = False\n",
        "USE_EMA = True\n",
        "EMA_DECAY = 0.9998\n",
        "\n",
        "# Mixup config (manual for multilabel)\n",
        "MIXUP_ALPHA = 0.4\n",
        "MIXUP_PROB = 0.5  # will be turned off for last 2 epochs\n",
        "\n",
        "# Defaults; will be overridden per model via timm.data.resolve_model_data_config(model)\n",
        "MEAN = (0.485, 0.456, 0.406)\n",
        "STD = (0.229, 0.224, 0.225)\n",
        "INTERP = cv2.INTER_CUBIC\n",
        "\n",
        "# Load metadata\n",
        "train_folds = pd.read_csv('train_folds.csv')\n",
        "train_df = pd.read_csv('train.csv')\n",
        "with open('classes.json') as f:\n",
        "    classes = json.load(f)['classes']\n",
        "C = len(classes)\n",
        "cls2id = {c:i for i,c in enumerate(classes)}\n",
        "\n",
        "# Parse labels to multi-hot\n",
        "def labels_to_multi_hot(s):\n",
        "    arr = np.zeros(C, dtype=np.float32)\n",
        "    for lab in str(s).strip().split():\n",
        "        if lab in cls2id:\n",
        "            arr[cls2id[lab]] = 1.0\n",
        "    return arr\n",
        "y_all = np.stack(train_df.labels.apply(labels_to_multi_hot).values)\n",
        "\n",
        "# Simple CV2-based transforms with RRC-like crop and Random Erasing\n",
        "class SimpleTransform:\n",
        "    def __init__(self, train=True, img_size=448, mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225), interp=cv2.INTER_CUBIC,\n",
        "                 rrc_scale=(0.8, 1.0), rrc_ratio=(0.75, 1.3333),\n",
        "                 erase_p=0.2, erase_area=(0.02, 0.2)):\n",
        "        self.train = train\n",
        "        self.img_size = img_size\n",
        "        self.mean = np.array(mean, dtype=np.float32)\n",
        "        self.std = np.array(std, dtype=np.float32)\n",
        "        self.interp = interp\n",
        "        self.rrc_scale = rrc_scale\n",
        "        self.rrc_ratio = rrc_ratio\n",
        "        self.erase_p = erase_p\n",
        "        self.erase_area = erase_area\n",
        "\n",
        "    def random_resized_crop(self, img):\n",
        "        h, w = img.shape[:2]\n",
        "        area = h * w\n",
        "        for _ in range(10):\n",
        "            target_area = area * random.uniform(self.rrc_scale[0], self.rrc_scale[1])\n",
        "            aspect = random.uniform(self.rrc_ratio[0], self.rrc_ratio[1])\n",
        "            new_w = int(round(math.sqrt(target_area * aspect)))\n",
        "            new_h = int(round(math.sqrt(target_area / aspect)))\n",
        "            if new_w <= w and new_h <= h and new_w > 0 and new_h > 0:\n",
        "                x0 = random.randint(0, w - new_w)\n",
        "                y0 = random.randint(0, h - new_h)\n",
        "                return img[y0:y0+new_h, x0:x0+new_w]\n",
        "        # Fallback to center crop\n",
        "        min_side = min(h, w)\n",
        "        y0 = (h - min_side) // 2\n",
        "        x0 = (w - min_side) // 2\n",
        "        return img[y0:y0+min_side, x0:x0+min_side]\n",
        "\n",
        "    def random_erasing(self, img):\n",
        "        # img is float32 normalized HWC\n",
        "        if random.random() >= self.erase_p:\n",
        "            return img\n",
        "        h, w = img.shape[:2]\n",
        "        area = h * w\n",
        "        for _ in range(10):\n",
        "            erase_area = area * random.uniform(self.erase_area[0], self.erase_area[1])\n",
        "            aspect = random.uniform(0.3, 3.3)\n",
        "            eh = int(round(math.sqrt(erase_area / aspect)))\n",
        "            ew = int(round(math.sqrt(erase_area * aspect)))\n",
        "            if eh <= h and ew <= w and eh > 0 and ew > 0:\n",
        "                y0 = random.randint(0, h - eh)\n",
        "                x0 = random.randint(0, w - ew)\n",
        "                # Fill with mean color (0 mean in normalized space isn't correct; use dataset mean/std to approximate original mean=0)\n",
        "                fill = np.zeros((eh, ew, 3), dtype=img.dtype)\n",
        "                img[y0:y0+eh, x0:x0+ew, :] = fill\n",
        "                return img\n",
        "        return img\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if self.train:\n",
        "            img = self.random_resized_crop(img)\n",
        "            if random.random() < 0.5:\n",
        "                img = cv2.flip(img, 1)\n",
        "            if random.random() < 0.2:\n",
        "                img = cv2.flip(img, 0)\n",
        "            if random.random() < 0.3:\n",
        "                angle = random.uniform(-15, 15)\n",
        "                M = cv2.getRotationMatrix2D((img.shape[1]/2, img.shape[0]/2), angle, 1.0)\n",
        "                img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]), flags=self.interp, borderMode=cv2.BORDER_REFLECT_101)\n",
        "            if random.random() < 0.2:\n",
        "                hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV).astype(np.int32)\n",
        "                hsv[...,0] = np.clip(hsv[...,0] + random.randint(-5,5), 0, 179)\n",
        "                hsv[...,1] = np.clip(hsv[...,1] + random.randint(-10,10), 0, 255)\n",
        "                hsv[...,2] = np.clip(hsv[...,2] + random.randint(-10,10), 0, 255)\n",
        "                img = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)\n",
        "            if random.random() < 0.1:\n",
        "                k = random.choice([3,5])\n",
        "                img = cv2.GaussianBlur(img, (k,k), 0)\n",
        "        # resize, normalize\n",
        "        img = cv2.resize(img, (self.img_size, self.img_size), interpolation=self.interp)\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        img = (img - self.mean) / self.std\n",
        "        # random erasing after norm\n",
        "        if self.train:\n",
        "            img = self.random_erasing(img)\n",
        "        return img\n",
        "\n",
        "def get_transforms(train=True):\n",
        "    return SimpleTransform(train=train, img_size=IMG_SIZE, mean=MEAN, std=STD, interp=INTERP)\n",
        "\n",
        "class PlantDataset(Dataset):\n",
        "    def __init__(self, df, labels=None, img_dir=TRAIN_DIR, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['image'])\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            raise FileNotFoundError(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        if isinstance(img, np.ndarray):\n",
        "            img = torch.from_numpy(img.transpose(2,0,1)).float()\n",
        "        if self.labels is not None:\n",
        "            target = self.labels[idx]\n",
        "            return img, torch.from_numpy(target).float()\n",
        "        else:\n",
        "            return img, row['image']\n",
        "\n",
        "def micro_f1(y_true, y_prob, thresh=0.3):\n",
        "    y_pred = (y_prob >= thresh).astype(np.int32)\n",
        "    if (y_pred.sum(axis=1)==0).any():\n",
        "        for i in np.where(y_pred.sum(axis=1)==0)[0]:\n",
        "            y_pred[i, y_prob[i].argmax()] = 1\n",
        "    if 'healthy' in cls2id:\n",
        "        h = cls2id['healthy']\n",
        "        disease_idx = [i for i,c in enumerate(classes) if c!='healthy']\n",
        "        disease_on = (y_pred[:, disease_idx].sum(axis=1) > 0)\n",
        "        y_pred[disease_on, h] = 0\n",
        "    return f1_score(y_true.ravel(), y_pred.ravel(), average='micro')\n",
        "\n",
        "def micro_f1_vec(y_true, y_prob, thrs):\n",
        "    y_pred = (y_prob >= thrs[None, :]).astype(np.int32)\n",
        "    if (y_pred.sum(axis=1)==0).any():\n",
        "        for i in np.where(y_pred.sum(axis=1)==0)[0]:\n",
        "            y_pred[i, y_prob[i].argmax()] = 1\n",
        "    if 'healthy' in cls2id:\n",
        "        h = cls2id['healthy']\n",
        "        disease_idx = [i for i,c in enumerate(classes) if c!='healthy']\n",
        "        disease_on = (y_pred[:, disease_idx].sum(axis=1) > 0)\n",
        "        y_pred[disease_on, h] = 0\n",
        "    return f1_score(y_true.ravel(), y_pred.ravel(), average='micro')\n",
        "\n",
        "def tune_global_threshold(y_true, y_prob, grid=None):\n",
        "    if grid is None:\n",
        "        grid = np.linspace(0.05, 0.6, 12)\n",
        "    best_t, best_f1 = 0.3, -1\n",
        "    for t in grid:\n",
        "        f1 = micro_f1(y_true, y_prob, t)\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_t = f1, t\n",
        "    return best_t, best_f1\n",
        "\n",
        "def tune_thresholds_coordinate_descent(y_true, y_prob, base_thr=0.5, grid=None, iters=2):\n",
        "    if grid is None:\n",
        "        grid = np.linspace(0.05, 0.8, 31)  # ~0.025 step\n",
        "    thrs = np.full(y_prob.shape[1], base_thr, dtype=np.float32)\n",
        "    best = micro_f1_vec(y_true, y_prob, thrs)\n",
        "    for _ in range(iters):\n",
        "        improved = False\n",
        "        for c in range(y_prob.shape[1]):\n",
        "            best_c_thr = thrs[c]\n",
        "            best_c_f1 = best\n",
        "            for t in grid:\n",
        "                thrs_try = thrs.copy(); thrs_try[c] = t\n",
        "                f1 = micro_f1_vec(y_true, y_prob, thrs_try)\n",
        "                if f1 > best_c_f1:\n",
        "                    best_c_f1 = f1; best_c_thr = t\n",
        "            if best_c_thr != thrs[c]:\n",
        "                thrs[c] = best_c_thr\n",
        "                best = best_c_f1\n",
        "                improved = True\n",
        "        if not improved:\n",
        "            break\n",
        "    # clamp to avoid extreme overfit\n",
        "    thrs = np.clip(thrs, 0.05, 0.80).astype(np.float32)\n",
        "    return thrs, best\n",
        "\n",
        "def train_one_fold(fold):\n",
        "    global MEAN, STD, INTERP\n",
        "    t0 = time.time()\n",
        "    print(f'===== Fold {fold} start =====')\n",
        "    trn_idx = train_folds.index[train_folds['fold'] != fold].values\n",
        "    val_idx = train_folds.index[train_folds['fold'] == fold].values\n",
        "    df_trn = train_folds.iloc[trn_idx][['image']].reset_index(drop=True)\n",
        "    df_val = train_folds.iloc[val_idx][['image']].reset_index(drop=True)\n",
        "    y_trn = y_all[trn_idx]\n",
        "    y_val = y_all[val_idx]\n",
        "\n",
        "    # Model & data config\n",
        "    model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=C, drop_path_rate=DROP_PATH, drop_rate=DROP_RATE)\n",
        "    data_cfg = timm.data.resolve_model_data_config(model)\n",
        "    MEAN, STD = tuple(data_cfg.get('mean', MEAN)), tuple(data_cfg.get('std', STD))\n",
        "    interp_name = str(data_cfg.get('interpolation', 'bicubic')).lower()\n",
        "    INTERP = cv2.INTER_CUBIC if 'bicubic' in interp_name else cv2.INTER_LINEAR\n",
        "\n",
        "    if USE_GRAD_CKPT and hasattr(model, 'set_grad_checkpointing'):\n",
        "        try:\n",
        "            model.set_grad_checkpointing(True)\n",
        "            print('Enabled gradient checkpointing')\n",
        "        except Exception:\n",
        "            pass\n",
        "    model.to(DEVICE)\n",
        "    if USE_CHANNELS_LAST:\n",
        "        model.to(memory_format=torch.channels_last)\n",
        "\n",
        "    ema = ModelEmaV2(model, decay=EMA_DECAY, device=DEVICE) if USE_EMA else None\n",
        "\n",
        "    # Datasets now that MEAN/STD/INTERP are set\n",
        "    train_ds = PlantDataset(df_trn, y_trn, img_dir=TRAIN_DIR, transform=get_transforms(True))\n",
        "    val_ds = PlantDataset(df_val, y_val, img_dir=TRAIN_DIR, transform=get_transforms(False))\n",
        "    nw = min(8, os.cpu_count() or 4)\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=nw, pin_memory=True, drop_last=True, persistent_workers=True, prefetch_factor=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=nw, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD, eps=1e-8, betas=(0.9,0.999))\n",
        "    warmup_steps = max(1, len(train_loader))\n",
        "    total_steps = max(warmup_steps+1, EPOCHS * len(train_loader))\n",
        "    sched_warm = LinearLR(opt, start_factor=0.01, total_iters=warmup_steps)\n",
        "    sched_cos = CosineAnnealingLR(opt, T_max=max(1, total_steps - warmup_steps))\n",
        "    scheduler = SequentialLR(opt, schedulers=[sched_warm, sched_cos], milestones=[warmup_steps])\n",
        "    criterion = AsymmetricLossMultiLabel(gamma_neg=4.0, gamma_pos=0.0, clip=0.05, eps=1e-8)\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_path = f'model_fold{fold}.pt'\n",
        "    patience = 3\n",
        "    bad_epochs = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        t_ep = time.time()\n",
        "        # decay Mixup prob to 0 in last 2 epochs\n",
        "        mixup_prob_now = MIXUP_PROB if epoch < EPOCHS - 2 else 0.0\n",
        "        for it, (imgs, targets) in enumerate(train_loader):\n",
        "            if USE_CHANNELS_LAST:\n",
        "                imgs = imgs.to(DEVICE, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            else:\n",
        "                imgs = imgs.to(DEVICE, non_blocking=True)\n",
        "            targets = targets.to(DEVICE, non_blocking=True)\n",
        "            # Manual Mixup for multilabel\n",
        "            if MIXUP_ALPHA > 0 and random.random() < mixup_prob_now:\n",
        "                lam = float(np.random.beta(MIXUP_ALPHA, MIXUP_ALPHA))\n",
        "                idx = torch.randperm(imgs.size(0), device=imgs.device)\n",
        "                imgs = lam * imgs + (1.0 - lam) * imgs[idx]\n",
        "                targets = lam * targets + (1.0 - lam) * targets[idx]\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            if USE_BF16_AMP and DEVICE=='cuda':\n",
        "                with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
        "                    logits = model(imgs)\n",
        "                    loss = criterion(logits.float(), targets)\n",
        "            else:\n",
        "                logits = model(imgs)\n",
        "                loss = criterion(logits, targets)\n",
        "\n",
        "            if not torch.isfinite(loss):\n",
        "                print(f'Non-finite loss detected at iter {it}: {loss.item()} -> skipping step')\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                continue\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "            scheduler.step()\n",
        "            if ema is not None:\n",
        "                ema.update(model)\n",
        "            if it % 50 == 0:\n",
        "                cur_lr = scheduler.get_last_lr()[0] if hasattr(scheduler, 'get_last_lr') else opt.param_groups[0]['lr']\n",
        "                print(f'Fold {fold} Epoch {epoch} Iter {it}/{len(train_loader)} loss={loss.item():.4f} lr={cur_lr:.6f}')\n",
        "\n",
        "        model.eval()\n",
        "        eval_model = ema.module if ema is not None else model\n",
        "        preds = []; gts = []\n",
        "        with torch.no_grad():\n",
        "            for imgs, targets in val_loader:\n",
        "                if USE_CHANNELS_LAST:\n",
        "                    imgs = imgs.to(DEVICE, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "                else:\n",
        "                    imgs = imgs.to(DEVICE, non_blocking=True)\n",
        "                if USE_BF16_AMP and DEVICE=='cuda':\n",
        "                    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
        "                        logits = eval_model(imgs)\n",
        "                else:\n",
        "                    logits = eval_model(imgs)\n",
        "                preds.append(torch.sigmoid(logits.float()).cpu().numpy())\n",
        "                gts.append(targets.cpu().numpy())\n",
        "        y_prob = np.concatenate(preds, axis=0)\n",
        "        y_true = np.concatenate(gts, axis=0)\n",
        "        t_opt, f1_opt = tune_global_threshold(y_true, y_prob)\n",
        "        print(f'Epoch {epoch} val micro-F1={f1_opt:.5f} @thr={t_opt:.3f} | time {time.time()-t_ep:.1f}s')\n",
        "        improved = f1_opt > best_f1 + 1e-5\n",
        "        if improved and np.isfinite(f1_opt):\n",
        "            best_f1 = f1_opt\n",
        "            torch.save({'model': eval_model.state_dict(), 'thr': t_opt}, best_path)\n",
        "            bad_epochs = 0\n",
        "        else:\n",
        "            bad_epochs += 1\n",
        "            if bad_epochs >= patience:\n",
        "                print('Early stopping due to no improvement')\n",
        "                break\n",
        "\n",
        "    ckpt = torch.load(best_path, map_location=DEVICE)\n",
        "    (ema.module if ema is not None else model).load_state_dict(ckpt['model'])\n",
        "    thr = ckpt.get('thr', 0.3)\n",
        "    (ema.module if ema is not None else model).eval()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in val_loader:\n",
        "            if USE_CHANNELS_LAST:\n",
        "                imgs = imgs.to(DEVICE, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            else:\n",
        "                imgs = imgs.to(DEVICE, non_blocking=True)\n",
        "            if USE_BF16_AMP and DEVICE=='cuda':\n",
        "                with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
        "                    logits = (ema.module if ema is not None else model)(imgs)\n",
        "            else:\n",
        "                logits = (ema.module if ema is not None else model)(imgs)\n",
        "            preds.append(torch.sigmoid(logits.float()).cpu().numpy())\n",
        "    y_prob = np.concatenate(preds, axis=0)\n",
        "    print(f'Fold {fold} done in {time.time()-t0:.1f}s, best_f1={best_f1:.5f}, thr={thr:.3f}')\n",
        "    return y_prob, y_true, thr\n",
        "\n",
        "def infer_test(models_paths, tta=2):\n",
        "    global MEAN, STD, INTERP\n",
        "    # Ensure normalization matches model cfg even in fresh kernels\n",
        "    tmp_model = timm.create_model(MODEL_NAME, pretrained=False, num_classes=C, drop_path_rate=DROP_PATH, drop_rate=DROP_RATE)\n",
        "    data_cfg = timm.data.resolve_model_data_config(tmp_model)\n",
        "    MEAN, STD = tuple(data_cfg.get('mean', MEAN)), tuple(data_cfg.get('std', STD))\n",
        "    interp_name = str(data_cfg.get('interpolation', 'bicubic')).lower()\n",
        "    INTERP = cv2.INTER_CUBIC if 'bicubic' in interp_name else cv2.INTER_LINEAR\n",
        "\n",
        "    test_df = pd.read_csv('sample_submission.csv')[['image']].copy()\n",
        "    ds = PlantDataset(test_df, labels=None, img_dir=TEST_DIR, transform=get_transforms(False))\n",
        "    loader = DataLoader(ds, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=min(8, os.cpu_count() or 4), pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "    model_level_logits = []  # list of (N_test, C)\n",
        "    for mp in models_paths:\n",
        "        model = timm.create_model(MODEL_NAME, pretrained=False, num_classes=C, drop_path_rate=DROP_PATH, drop_rate=DROP_RATE)\n",
        "        ckpt = torch.load(mp, map_location=DEVICE)\n",
        "        model.load_state_dict(ckpt['model'])\n",
        "        model.to(DEVICE)\n",
        "        if USE_CHANNELS_LAST:\n",
        "            model.to(memory_format=torch.channels_last)\n",
        "        model.eval()\n",
        "        view_logits = []  # per-TTA view logits (N_test, C)\n",
        "        with torch.no_grad():\n",
        "            # view 1: original\n",
        "            outs = []\n",
        "            for imgs, names in loader:\n",
        "                if USE_CHANNELS_LAST:\n",
        "                    imgs = imgs.to(DEVICE, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "                else:\n",
        "                    imgs = imgs.to(DEVICE, non_blocking=True)\n",
        "                if USE_BF16_AMP and DEVICE=='cuda':\n",
        "                    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
        "                        logits = model(imgs)\n",
        "                else:\n",
        "                    logits = model(imgs)\n",
        "                outs.append(logits.float().cpu().numpy())\n",
        "            view_logits.append(np.concatenate(outs, axis=0))\n",
        "            # view 2: hflip\n",
        "            if tta >= 2:\n",
        "                outs = []\n",
        "                for imgs, names in loader:\n",
        "                    imgs = imgs.flip(-1)\n",
        "                    if USE_CHANNELS_LAST:\n",
        "                        imgs = imgs.to(DEVICE, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "                    else:\n",
        "                        imgs = imgs.to(DEVICE, non_blocking=True)\n",
        "                    if USE_BF16_AMP and DEVICE=='cuda':\n",
        "                        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
        "                            logits = model(imgs)\n",
        "                    else:\n",
        "                        logits = model(imgs)\n",
        "                    outs.append(logits.float().cpu().numpy())\n",
        "                view_logits.append(np.concatenate(outs, axis=0))\n",
        "            # view 3: vflip\n",
        "            if tta >= 3:\n",
        "                outs = []\n",
        "                for imgs, names in loader:\n",
        "                    imgs = imgs.flip(-2)\n",
        "                    if USE_CHANNELS_LAST:\n",
        "                        imgs = imgs.to(DEVICE, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "                    else:\n",
        "                        imgs = imgs.to(DEVICE, non_blocking=True)\n",
        "                    if USE_BF16_AMP and DEVICE=='cuda':\n",
        "                        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
        "                            logits = model(imgs)\n",
        "                    else:\n",
        "                        logits = model(imgs)\n",
        "                    outs.append(logits.float().cpu().numpy())\n",
        "                view_logits.append(np.concatenate(outs, axis=0))\n",
        "            # view 4: hvflip\n",
        "            if tta >= 4:\n",
        "                outs = []\n",
        "                for imgs, names in loader:\n",
        "                    imgs = imgs.flip(-1).flip(-2)\n",
        "                    if USE_CHANNELS_LAST:\n",
        "                        imgs = imgs.to(DEVICE, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "                    else:\n",
        "                        imgs = imgs.to(DEVICE, non_blocking=True)\n",
        "                    if USE_BF16_AMP and DEVICE=='cuda':\n",
        "                        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
        "                            logits = model(imgs)\n",
        "                    else:\n",
        "                        logits = model(imgs)\n",
        "                    outs.append(logits.float().cpu().numpy())\n",
        "                view_logits.append(np.concatenate(outs, axis=0))\n",
        "        logits_avg = np.mean(np.stack(view_logits, axis=0), axis=0)  # (N_test, C)\n",
        "        model_level_logits.append(logits_avg)\n",
        "    logits = np.mean(np.stack(model_level_logits, axis=0), axis=0)  # (N_test, C)\n",
        "    probs = 1/(1+np.exp(-logits))\n",
        "    return test_df['image'].values, probs\n",
        "\n",
        "# Orchestrate K-fold training or skip to inference\n",
        "if DO_TRAIN:\n",
        "    oof_probs = np.zeros((len(train_df), C), dtype=np.float32)\n",
        "    oof_targets = y_all.copy()\n",
        "    fold_thresholds = []\n",
        "    for fold in range(NUM_FOLDS):\n",
        "        t_fold = time.time()\n",
        "        y_prob, y_true, thr = train_one_fold(fold)\n",
        "        val_idx = train_folds.index[train_folds['fold'] == fold].values\n",
        "        oof_probs[val_idx] = y_prob\n",
        "        fold_thresholds.append(thr)\n",
        "        print(f'Fold {fold} completed in {time.time()-t_fold:.1f}s')\n",
        "        gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    np.save('oof_probs.npy', oof_probs)\n",
        "    np.save('oof_targets.npy', oof_targets)\n",
        "    print('Saved OOF probs/targets')\n",
        "    mask = train_folds['fold'].isin(list(range(NUM_FOLDS))).values\n",
        "    t_best, f1_best = tune_global_threshold(oof_targets[mask], oof_probs[mask])\n",
        "    # Per-class threshold tuning and save\n",
        "    thrs_vec, f1_best_vec = tune_thresholds_coordinate_descent(oof_targets[mask], oof_probs[mask], base_thr=t_best, grid=np.linspace(0.05,0.8,31), iters=3)\n",
        "    thrs_vec = np.clip(thrs_vec, 0.05, 0.80).astype(np.float32)\n",
        "    np.save('thr_per_class.npy', thrs_vec)\n",
        "    print(f'OOF (folds< {NUM_FOLDS}) micro-F1={f1_best:.5f} @thr={t_best:.3f}; per-class tuned micro-F1={f1_best_vec:.5f} | n={mask.sum()}')\n",
        "else:\n",
        "    if os.path.exists('oof_probs.npy') and os.path.exists('oof_targets.npy'):    \n",
        "        oof_probs = np.load('oof_probs.npy')\n",
        "        oof_targets = np.load('oof_targets.npy')\n",
        "        mask = train_folds['fold'].isin(list(range(NUM_FOLDS))).values\n",
        "        t_best, f1_best = tune_global_threshold(oof_targets[mask], oof_probs[mask])\n",
        "        thrs_vec, f1_best_vec = tune_thresholds_coordinate_descent(oof_targets[mask], oof_probs[mask], base_thr=t_best, grid=np.linspace(0.05,0.8,31), iters=3)\n",
        "        thrs_vec = np.clip(thrs_vec, 0.05, 0.80).astype(np.float32)\n",
        "        np.save('thr_per_class.npy', thrs_vec)\n",
        "        print(f'Loaded OOF (folds< {NUM_FOLDS}); tuned global thr={t_best:.3f} (micro-F1={f1_best:.5f}); per-class tuned micro-F1={f1_best_vec:.5f} | n={mask.sum()}')\n",
        "    else:\n",
        "        t_best = 0.6\n",
        "        thrs_vec = np.full(C, t_best, dtype=np.float32)\n",
        "        print('OOF not found; using default thr=0.6')\n",
        "\n",
        "# Inference on test using best fold checkpoints\n",
        "model_paths = [f'model_fold{i}.pt' for i in range(NUM_FOLDS)]\n",
        "names, test_probs = infer_test(model_paths, tta=4)\n",
        "\n",
        "def probs_to_labels_row(p, thr_or_vec):\n",
        "    if np.ndim(thr_or_vec)==0:\n",
        "        lab_mask = (p >= float(thr_or_vec))\n",
        "    else:\n",
        "        lab_mask = (p >= thr_or_vec)\n",
        "    if lab_mask.sum() == 0:\n",
        "        lab_mask[p.argmax()] = 1\n",
        "    if 'healthy' in cls2id:\n",
        "        h = cls2id['healthy']\n",
        "        disease_idx = [i for i,c in enumerate(classes) if c!='healthy']\n",
        "        if lab_mask[disease_idx].sum() > 0:\n",
        "            lab_mask[h] = 0\n",
        "    return ' '.join([classes[i] for i in np.where(lab_mask)[0]])\n",
        "\n",
        "thr_to_use = np.load('thr_per_class.npy') if os.path.exists('thr_per_class.npy') else t_best\n",
        "labels_out = [probs_to_labels_row(p, thr_to_use) for p in test_probs]\n",
        "sub = pd.DataFrame({'image': names, 'labels': labels_out})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Wrote submission.csv with shape', sub.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Fold 0 start =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0 Epoch 0 Iter 0/1192 loss=47.3398 lr=0.000003\n"
          ]
        }
      ]
    },
    {
      "id": "2dc25839-ff3a-4fb2-b63e-62ac6b949d48",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys, subprocess, os\n",
        "def pip(*args):\n",
        "    print('>', *args, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "# Fix albumentations/albucore mismatch by pinning albumentations<1.4 (no albucore dependency)\n",
        "subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', 'albumentations', 'albucore'], check=False)\n",
        "pip('install', 'albumentations==1.3.1', 'opencv-python-headless', '--upgrade-strategy', 'only-if-needed')\n",
        "import albumentations as A\n",
        "print('Albumentations loaded from:', getattr(A, '__file__', 'unknown'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: albumentations 1.3.1\nUninstalling albumentations-1.3.1:\n  Successfully uninstalled albumentations-1.3.1\n> install albumentations==1.3.1 opencv-python-headless --upgrade-strategy only-if-needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping albucore as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting albumentations==1.3.1\n  Downloading albumentations-1.3.1-py3-none-any.whl (125 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 125.7/125.7 KB 5.9 MB/s eta 0:00:00\nCollecting opencv-python-headless\n  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 54.0/54.0 MB 199.3 MB/s eta 0:00:00\nCollecting PyYAML\n  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 806.6/806.6 KB 504.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy>=1.11.1\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 401.0 MB/s eta 0:00:00\nCollecting qudida>=0.0.4\n  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy>=1.1.0\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.9/35.9 MB 138.2 MB/s eta 0:00:00\nCollecting scikit-image>=0.16.1\n  Downloading scikit_image-0.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.8/14.8 MB 209.7 MB/s eta 0:00:00\nCollecting opencv-python-headless\n  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 50.0/50.0 MB 176.7 MB/s eta 0:00:00\nCollecting typing-extensions\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 300.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn>=0.19.1\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.7/9.7 MB 190.4 MB/s eta 0:00:00\nCollecting tifffile>=2022.8.12\n  Downloading tifffile-2025.9.20-py3-none-any.whl (230 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 230.1/230.1 KB 389.1 MB/s eta 0:00:00\nCollecting networkx>=3.0\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 432.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lazy-loader>=0.4\n  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\nCollecting imageio!=2.35.0,>=2.33\n  Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 315.8/315.8 KB 524.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow>=10.1\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 334.6 MB/s eta 0:00:00\nCollecting packaging>=21\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 364.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting threadpoolctl>=3.1.0\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nCollecting joblib>=1.2.0\n  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 308.4/308.4 KB 530.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed PyYAML-6.0.3 albumentations-1.3.1 imageio-2.37.0 joblib-1.5.2 lazy-loader-0.4 networkx-3.5 numpy-1.26.4 opencv-python-headless-4.11.0.86 packaging-25.0 pillow-11.3.0 qudida-0.0.4 scikit-image-0.25.2 scikit-learn-1.7.2 scipy-1.16.2 threadpoolctl-3.6.0 tifffile-2025.9.20 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/albumentations already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/qudida-0.0.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/qudida already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scikit_image-0.25.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/skimage already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scikit_learn-1.7.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sklearn already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scikit_learn.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/imageio-2.37.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/imageio already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/lazy_loader-0.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/lazy_loader already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/cv2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/opencv_python_headless-4.11.0.86.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/opencv_python_headless.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy-1.16.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tifffile-2025.9.20.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tifffile already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/joblib-1.5.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/joblib already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx-3.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging-25.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow-11.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/PIL already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pyyaml-6.0.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/yaml already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/_yaml already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/threadpoolctl-3.6.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/threadpoolctl.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Albumentations loaded from: None\n"
          ]
        }
      ]
    },
    {
      "id": "188557f8-a9a4-4024-85a3-8cd89d112815",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick near-duplicate scan via pHash (run separately from training).\n",
        "import os, math, time, itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "\n",
        "def phash64(img_bgr):\n",
        "    # Convert to grayscale and compute 8x8 DCT-based perceptual hash (64-bit)\n",
        "    try:\n",
        "        img = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "        img = cv2.resize(img, (32, 32), interpolation=cv2.INTER_AREA)\n",
        "        img = np.float32(img)\n",
        "        dct = cv2.dct(img)\n",
        "        dct_low = dct[:8, :8].copy()\n",
        "        dct_low[0,0] = 0.0  # remove DC\n",
        "        med = np.median(dct_low)\n",
        "        bits = (dct_low > med).astype(np.uint8).reshape(-1)\n",
        "        # pack into 64-bit int\n",
        "        h = 0\n",
        "        for b in bits:\n",
        "            h = (h << 1) | int(b)\n",
        "        return np.uint64(h)\n",
        "    except Exception:\n",
        "        return np.uint64(0)\n",
        "\n",
        "def hamming64(a, b):\n",
        "    return int(bin(int(a ^ b)).count('1'))\n",
        "\n",
        "def run_phash_scan(images_dir='train_images', max_bucket_size=200, prefix_bits=16, ham_thresh=5, sample=None):\n",
        "    t0 = time.time()\n",
        "    imgs = sorted(os.listdir(images_dir))\n",
        "    if sample is not None and sample < len(imgs):\n",
        "        imgs = imgs[:sample]\n",
        "    paths = [os.path.join(images_dir, x) for x in imgs]\n",
        "    hashes = []\n",
        "    for i, p in enumerate(paths):\n",
        "        im = cv2.imread(p)\n",
        "        if im is None:\n",
        "            hashes.append(np.uint64(0)); continue\n",
        "        hashes.append(phash64(im))\n",
        "        if i % 1000 == 0:\n",
        "            print(f'pHash {i}/{len(paths)} processed')\n",
        "    hashes = np.array(hashes, dtype=np.uint64)\n",
        "    print('Computed hashes in', f'{time.time()-t0:.1f}s')\n",
        "\n",
        "    # Exact duplicates (identical pHash)\n",
        "    df = pd.DataFrame({'image': imgs, 'phash': hashes})\n",
        "    dup_groups = df.groupby('phash').filter(lambda x: len(x) > 1)\n",
        "    if len(dup_groups) > 0:\n",
        "        print('Exact-duplicate pHash groups:', dup_groups.groupby('phash').size().shape[0])\n",
        "    else:\n",
        "        print('No exact-duplicate pHash groups found')\n",
        "\n",
        "    # Approximate duplicates by prefix bucketing to limit pairwise work\n",
        "    prefix_shift = 64 - prefix_bits\n",
        "    prefixes = (hashes >> np.uint64(prefix_shift)).astype(np.uint64)\n",
        "    buckets = {}\n",
        "    for idx, pref in enumerate(prefixes):\n",
        "        buckets.setdefault(int(pref), []).append(idx)\n",
        "    print('Buckets:', len(buckets))\n",
        "\n",
        "    pairs = []  # (img_a, img_b, ham)\n",
        "    checked = 0\n",
        "    for pref, idxs in buckets.items():\n",
        "        if len(idxs) <= 1:\n",
        "            continue\n",
        "        if len(idxs) > max_bucket_size:\n",
        "            # skip giant buckets to keep runtime bounded\n",
        "            continue\n",
        "        for i, j in itertools.combinations(idxs, 2):\n",
        "            ham = hamming64(hashes[i], hashes[j])\n",
        "            if ham <= ham_thresh:\n",
        "                pairs.append((imgs[i], imgs[j], ham))\n",
        "        checked += 1\n",
        "        if checked % 200 == 0:\n",
        "            print(f'Checked {checked}/{len(buckets)} buckets; pairs so far={len(pairs)}')\n",
        "\n",
        "    dup_df = pd.DataFrame(pairs, columns=['image_a','image_b','hamming'])\n",
        "    dup_df.to_csv('near_duplicate_pairs.csv', index=False)\n",
        "    print('Saved near_duplicate_pairs.csv with', len(dup_df), 'pairs; total time', f'{time.time()-t0:.1f}s')\n",
        "\n",
        "    # If folds exist, summarize cross-fold duplicates\n",
        "    if os.path.exists('train_folds.csv'):\n",
        "        folds = pd.read_csv('train_folds.csv')[['image','fold']]\n",
        "        m = dup_df.merge(folds.rename(columns={'image':'image_a'}), on='image_a', how='left')\n",
        "        m = m.merge(folds.rename(columns={'image':'image_b','fold':'fold_b'}), on='image_b', how='left')\n",
        "        m = m.rename(columns={'fold':'fold_a'})\n",
        "        m.to_csv('near_duplicate_pairs_with_folds.csv', index=False)\n",
        "        if len(m):\n",
        "            cross = (m['fold_a'] != m['fold_b']).mean()\n",
        "            print(f'Cross-fold duplicate rate: {cross:.3f} over {len(m)} pairs')\n",
        "        else:\n",
        "            print('No near-duplicate pairs to summarize')\n",
        "\n",
        "print('To run: run_phash_scan(images_dir=\"train_images\", sample=None)')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}