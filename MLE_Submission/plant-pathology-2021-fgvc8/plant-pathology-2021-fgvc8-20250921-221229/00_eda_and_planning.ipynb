{
  "cells": [
    {
      "id": "528e6960-2fed-435e-986c-76baffa0e4c9",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan to Win a Medal: Plant Pathology 2021 - FGVC8\n",
        "\n",
        "Objectives:\n",
        "- Achieve micro-F1 medal threshold via strong CV, GPU-accelerated CNNs, and robust ensembling.\n",
        "\n",
        "Strategy Overview:\n",
        "1) Environment & GPU:\n",
        "- Install and validate PyTorch + GPU. If GPU not available, stop and re-provision.\n",
        "\n",
        "2) Data audit & CV definition:\n",
        "- Inspect train.csv, labels format (multi-label vs multi-class), image counts, missing values.\n",
        "- Determine proper CV (StratifiedKFold for multilabel via iterative stratification). Save folds to disk for reuse.\n",
        "\n",
        "3) Baseline model (fast):\n",
        "- Pretrained timm backbone (e.g., tf_efficientnet_b3_ns or convnext_tiny) @ 384px.\n",
        "- Loss: BCEWithLogitsLoss for multilabel.\n",
        "- Augmentations: light albumentations (flips, brightness/contrast).\n",
        "- 5-fold CV, early stopping, mixed precision, gradient accumulation if needed.\n",
        "- Log OOF predictions and per-class F1; compute micro-F1.\n",
        "\n",
        "4) Iterations:\n",
        "- Improve aug: color jitter, CutMix/Mixup for multilabel.\n",
        "- Higher res (512), stronger backbones (swin_t, convnext_base) if time allows.\n",
        "- TTA for test (e.g., 4-8 flips).\n",
        "- Blend diverse seeds/backbones via logits average; calibrate threshold via OOF to maximize micro-F1.\n",
        "\n",
        "5) Submission:\n",
        "- Use OOF-optimized thresholds per class or global, apply to test logits.\n",
        "- Save predictions to submission.csv matching sample_submission format.\n",
        "\n",
        "Checkpoints for Expert Review:\n",
        "- After this plan.\n",
        "- After data audit + CV folds.\n",
        "- After first baseline OOF.\n",
        "- After each major model/ensemble improvement.\n",
        "\n",
        "Risk Mitigation:\n",
        "- Subsample smoke runs to validate pipeline.\n",
        "- Ensure no leakage: fit transforms within folds only.\n",
        "- Save artifacts (folds, OOF logits, test logits).\n",
        "\n",
        "Next:\n",
        "- Run environment check + quick dataset inspection."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "a872613a-50c3-4a29-a062-3f87e2625797",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment check + quick dataset audit\n",
        "import sys, subprocess, time, os, json, gc\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "def pip_install(pkgs):\n",
        "    print(f\"Installing: {pkgs}\", flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q'] + pkgs, check=True)\n",
        "\n",
        "# 1) Install torch and basic libs\n",
        "pip_install(['torch'])\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "    print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "        props = torch.cuda.get_device_properties(0)\n",
        "        print(f\"GPU Memory: {props.total_memory / 1024**3:.1f} GB\")\n",
        "except Exception as e:\n",
        "    print('Torch import/check failed:', e)\n",
        "\n",
        "# 2) Inspect files\n",
        "base = Path('.')\n",
        "print('CWD:', base.resolve())\n",
        "print('Files:', os.listdir())\n",
        "\n",
        "train_csv = Path('train.csv')\n",
        "ss_csv = Path('sample_submission.csv')\n",
        "assert train_csv.exists() and ss_csv.exists(), 'Missing train.csv or sample_submission.csv'\n",
        "\n",
        "df = pd.read_csv(train_csv)\n",
        "print('train.csv shape:', df.shape)\n",
        "print(df.head(3))\n",
        "\n",
        "# Expected columns: image, labels (space-separated) or one-hot; infer\n",
        "cols = df.columns.tolist()\n",
        "print('Columns:', cols)\n",
        "label_cols = [c for c in cols if c not in ('image','id','image_id','filepath','path')]\n",
        "if 'labels' in df.columns:\n",
        "    # space-separated multilabel\n",
        "    all_labels = sorted({l for s in df['labels'].astype(str).tolist() for l in s.split()})\n",
        "    print('Detected multilabel with label set:', all_labels)\n",
        "    # one-hot frame\n",
        "    for l in all_labels:\n",
        "        df[l] = df['labels'].astype(str).str.contains(fr'\\b{l}\\b').astype(int)\n",
        "    label_cols = all_labels\n",
        "elif set({'image','id','image_id'}).intersection(df.columns) and len(label_cols) > 0:\n",
        "    # likely one-hot multilabel\n",
        "    print('Detected one-hot multilabel columns:', label_cols)\n",
        "else:\n",
        "    print('Unable to confidently detect label schema; will inspect more later.')\n",
        "\n",
        "# Basic sanity checks\n",
        "if 'healthy' in df.columns:\n",
        "    conflict = ((df[[c for c in df.columns if c != 'labels' and c != 'healthy' and c in label_cols]].sum(axis=1) > 0) & (df['healthy'] == 1)).sum()\n",
        "    print('Rows with healthy + disease (should be few/none):', int(conflict))\n",
        "\n",
        "# Image dirs\n",
        "train_dir = Path('train_images')\n",
        "test_dir = Path('test_images')\n",
        "print('Train images dir exists:', train_dir.exists(), 'count:', len(list(train_dir.glob('*.jpg'))))\n",
        "print('Test images dir exists:', test_dir.exists(), 'count:', len(list(test_dir.glob('*.jpg'))))\n",
        "\n",
        "# Sample submission check\n",
        "ss = pd.read_csv(ss_csv)\n",
        "print('sample_submission shape:', ss.shape)\n",
        "print(ss.head(3))\n",
        "\n",
        "print('Environment & audit complete.')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing: ['torch']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: True\nGPU Count: 1\nGPU Name: NVIDIA A10-24Q\nGPU Memory: 23.7 GB\nCWD: /var/lib/simon/agent_run_states/plant-pathology-2021-fgvc8-20250921-221229\nFiles: ['sample_submission.csv', '.00_eda_and_planning_kernel_state.json', 'docker_run.log', 'requirements.txt', 'test_images', 'submission.csv', 'train_images', 'train.csv', 'agent_metadata', 'task.txt', '00_eda_and_planning.ipynb', 'description.md']\ntrain.csv shape: (14905, 2)\n                  image                      labels\n0  95cb4b75ad1d842a.jpg                     complex\n1  be80d450dac87d7c.jpg  frog_eye_leaf_spot complex\n2  80cac5abcfa9c9b4.jpg                     healthy\nColumns: ['image', 'labels']\nDetected multilabel with label set: ['complex', 'frog_eye_leaf_spot', 'healthy', 'powdery_mildew', 'rust', 'scab']\nRows with healthy + disease (should be few/none): 0\nTrain images dir exists: True count: 14905\nTest images dir exists: True count: 3727\nsample_submission shape: (3727, 2)\n                  image   labels\n0  ca6a50c5d2adb8ae.jpg  healthy\n1  b686d217a1e2e3a5.jpg  healthy\n2  c9a5345ec78b4ac5.jpg  healthy\nEnvironment & audit complete.\n"
          ]
        }
      ]
    },
    {
      "id": "0ae1f2e6-51b9-4f75-9742-d7855f31763b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create multilabel folds (Iterative Stratification) and save\n",
        "import sys, subprocess, os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def pip_install(pkgs):\n",
        "    print(f\"Installing: {pkgs}\", flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q'] + pkgs, check=True)\n",
        "\n",
        "# Install needed libs for CV and modeling later\n",
        "pip_install(['iterative-stratification', 'timm', 'albumentations', 'opencv-python-headless'])\n",
        "\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "train_csv = Path('train.csv')\n",
        "df = pd.read_csv(train_csv)\n",
        "\n",
        "# Parse multilabels from space-separated 'labels' column\n",
        "assert 'labels' in df.columns and 'image' in df.columns, 'Expected columns image and labels'\n",
        "all_labels = sorted({l for s in df['labels'].astype(str).tolist() for l in s.split()})\n",
        "for l in all_labels:\n",
        "    df[l] = df['labels'].astype(str).str.contains(fr'\\b{l}\\b').astype(int)\n",
        "\n",
        "label_cols = all_labels\n",
        "X = df[['image']].copy()\n",
        "Y = df[label_cols].values\n",
        "\n",
        "print('Label set:', label_cols)\n",
        "print('Class frequencies:', df[label_cols].sum().to_dict())\n",
        "\n",
        "# Build 5-fold multilabel iterative stratification\n",
        "mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "df['fold'] = -1\n",
        "for fold, (trn_idx, val_idx) in enumerate(mskf.split(X, Y)):\n",
        "    df.loc[val_idx, 'fold'] = fold\n",
        "    print(f'Assigned fold {fold}: val size {len(val_idx)}')\n",
        "\n",
        "assert (df['fold'] >= 0).all(), 'Some rows not assigned a fold'\n",
        "\n",
        "# Inspect per-fold label distribution\n",
        "for fold in range(5):\n",
        "    cnts = df[df['fold'] == fold][label_cols].sum().to_dict()\n",
        "    print(f'Fold {fold} counts:', cnts)\n",
        "\n",
        "# Save folds to disk\n",
        "folds_path = Path('folds.csv')\n",
        "df[['image', 'labels', 'fold'] + label_cols].to_csv(folds_path, index=False)\n",
        "print('Saved folds to', folds_path.resolve())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing: ['iterative-stratification', 'timm', 'albumentations', 'opencv-python-headless']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/torch-2.8.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchgen already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/functorch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusolver_cu12-11.7.3.90.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2-3.1.6.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cudnn_cu12-9.10.2.21.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cufft_cu12-11.3.3.83.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusparse_cu12-12.5.8.93.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/triton-3.4.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock-3.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec-2025.9.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/MarkupSafe-3.0.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx-3.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cublas_cu12-12.8.4.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_cupti_cu12-12.8.90.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_nvrtc_cu12-12.8.93.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_runtime_cu12-12.8.90.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cufile_cu12-1.13.1.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_curand_cu12-10.3.9.90.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nccl_cu12-2.27.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvjitlink_cu12-12.8.93.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvtx_cu12-12.8.90.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/setuptools-80.9.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/setuptools already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pkg_resources already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/_distutils_hack already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/distutils-precedence.pth already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy-1.14.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/isympy.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath-1.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusparselt_cu12-0.7.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/share already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label set: ['complex', 'frog_eye_leaf_spot', 'healthy', 'powdery_mildew', 'rust', 'scab']\nClass frequencies: {'complex': 1711, 'frog_eye_leaf_spot': 3511, 'healthy': 3673, 'powdery_mildew': 1008, 'rust': 1650, 'scab': 4600}\nAssigned fold 0: val size 2977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigned fold 1: val size 2992\nAssigned fold 2: val size 2975\nAssigned fold 3: val size 2993\nAssigned fold 4: val size 2968\nFold 0 counts: {'complex': 342, 'frog_eye_leaf_spot': 702, 'healthy': 735, 'powdery_mildew': 202, 'rust': 330, 'scab': 920}\nFold 1 counts: {'complex': 342, 'frog_eye_leaf_spot': 702, 'healthy': 734, 'powdery_mildew': 202, 'rust': 330, 'scab': 920}\nFold 2 counts: {'complex': 343, 'frog_eye_leaf_spot': 703, 'healthy': 735, 'powdery_mildew': 201, 'rust': 330, 'scab': 920}\nFold 3 counts: {'complex': 342, 'frog_eye_leaf_spot': 702, 'healthy': 734, 'powdery_mildew': 202, 'rust': 330, 'scab': 920}\nFold 4 counts: {'complex': 342, 'frog_eye_leaf_spot': 702, 'healthy': 735, 'powdery_mildew': 201, 'rust': 330, 'scab': 920}\nSaved folds to /var/lib/simon/agent_run_states/plant-pathology-2021-fgvc8-20250921-221229/folds.csv\n"
          ]
        }
      ]
    },
    {
      "id": "fcb4e824-6597-44a9-8732-ebf83334838e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Near-duplicate detection (phash) and fold assignment by groups\n",
        "import sys, subprocess, os, math, time\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def pip_install(pkgs):\n",
        "    print(f\"Installing: {pkgs}\", flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q'] + pkgs, check=True)\n",
        "\n",
        "pip_install(['imagehash', 'Pillow'])\n",
        "from PIL import Image\n",
        "import imagehash\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "train_df = pd.read_csv('train.csv')\n",
        "all_labels = sorted({l for s in train_df['labels'].astype(str).tolist() for l in s.split()})\n",
        "for l in all_labels:\n",
        "    train_df[l] = train_df['labels'].astype(str).str.contains(fr'\\b{l}\\b').astype(int)\n",
        "label_cols = all_labels\n",
        "\n",
        "img_dir = Path('train_images')\n",
        "hash_cache_path = Path('phash_train.csv')\n",
        "\n",
        "def compute_phashes(df):\n",
        "    rows = []\n",
        "    t0 = time.time()\n",
        "    for i, img_name in enumerate(df['image'].tolist()):\n",
        "        p = img_dir / img_name\n",
        "        try:\n",
        "            with Image.open(p) as im:\n",
        "                im = im.convert('RGB')\n",
        "                h = imagehash.phash(im)  # 64-bit hash\n",
        "        except Exception as e:\n",
        "            print(f'Error hashing {img_name}: {e}', flush=True)\n",
        "            h = None\n",
        "        rows.append({'image': img_name, 'phash': str(h) if h is not None else ''})\n",
        "        if (i+1) % 1000 == 0:\n",
        "            elapsed = time.time() - t0\n",
        "            print(f'Hashed {i+1}/{len(df)} images in {elapsed:.1f}s', flush=True)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "if hash_cache_path.exists():\n",
        "    ph = pd.read_csv(hash_cache_path)\n",
        "    print('Loaded cached phashes:', ph.shape)\n",
        "else:\n",
        "    ph = compute_phashes(train_df)\n",
        "    ph.to_csv(hash_cache_path, index=False)\n",
        "    print('Saved phashes to', hash_cache_path.resolve())\n",
        "\n",
        "# Union-Find for grouping near duplicates\n",
        "parent = {}\n",
        "rank = {}\n",
        "imgs = train_df['image'].tolist()\n",
        "for img in imgs:\n",
        "    parent[img] = img\n",
        "    rank[img] = 0\n",
        "\n",
        "def find(x):\n",
        "    while parent[x] != x:\n",
        "        parent[x] = parent[parent[x]]\n",
        "        x = parent[x]\n",
        "    return x\n",
        "\n",
        "def union(a, b):\n",
        "    ra, rb = find(a), find(b)\n",
        "    if ra == rb: return\n",
        "    if rank[ra] < rank[rb]:\n",
        "        parent[ra] = rb\n",
        "    elif rank[ra] > rank[rb]:\n",
        "        parent[rb] = ra\n",
        "    else:\n",
        "        parent[rb] = ra\n",
        "        rank[ra] += 1\n",
        "\n",
        "# Build LSH buckets using overlapping slices of the hex hash to limit comparisons\n",
        "ph_map = dict(zip(ph['image'], ph['phash']))\n",
        "buckets = {}  # key -> list of image names\n",
        "slice_len = 6  # hex chars per slice\n",
        "overlaps = [(0, slice_len), (4, 4+slice_len), (8, 8+slice_len)]  # overlapping windows\n",
        "\n",
        "def add_to_bucket(key, img):\n",
        "    if key not in buckets: buckets[key] = []\n",
        "    buckets[key].append(img)\n",
        "\n",
        "valid = 0\n",
        "for img, h in ph_map.items():\n",
        "    if not h or len(h) < 16:\n",
        "        continue\n",
        "    valid += 1\n",
        "    for s, e in overlaps:\n",
        "        if e <= len(h):\n",
        "            add_to_bucket(f'{s}:{h[s:e]}', img)\n",
        "print('Valid hashed images:', valid, 'Buckets:', len(buckets))\n",
        "\n",
        "def hamming_hex(a, b):\n",
        "    # a,b hex strings of equal length\n",
        "    if len(a) != len(b):\n",
        "        return 64  # large\n",
        "    # Convert hex to int and count bits\n",
        "    return bin(int(a, 16) ^ int(b, 16)).count('1')\n",
        "\n",
        "threshold = 5  # max Hamming distance to consider duplicates\n",
        "checked_pairs = 0\n",
        "linked = 0\n",
        "t0 = time.time()\n",
        "for key, lst in buckets.items():\n",
        "    n = len(lst)\n",
        "    if n <= 1: continue\n",
        "    # pairwise within bucket\n",
        "    for i in range(n):\n",
        "        hi = ph_map.get(lst[i], '')\n",
        "        if not hi: continue\n",
        "        for j in range(i+1, n):\n",
        "            hj = ph_map.get(lst[j], '')\n",
        "            if not hj: continue\n",
        "            checked_pairs += 1\n",
        "            if hamming_hex(hi, hj) <= threshold:\n",
        "                union(lst[i], lst[j])\n",
        "                linked += 1\n",
        "    if checked_pairs and checked_pairs % 100000 == 0:\n",
        "        print(f'Checked {checked_pairs} pairs, linked {linked}, elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "print(f'Pair checks: {checked_pairs}, links: {linked}, time: {time.time()-t0:.1f}s')\n",
        "\n",
        "# Build groups\n",
        "groups = {}  # root -> list of images\n",
        "for img in imgs:\n",
        "    r = find(img)\n",
        "    groups.setdefault(r, []).append(img)\n",
        "group_sizes = [len(v) for v in groups.values()]\n",
        "print('Groups:', len(groups), 'Avg size:', np.mean(group_sizes), 'Max:', np.max(group_sizes))\n",
        "\n",
        "# Representative per group\n",
        "rep_list = []\n",
        "rep_index = {}  # image -> rep\n",
        "for root, members in groups.items():\n",
        "    # pick lexicographically smallest as representative\n",
        "    rep = sorted(members)[0]\n",
        "    for m in members:\n",
        "        rep_index[m] = rep\n",
        "    rep_list.append(rep)\n",
        "rep_df = train_df.set_index('image').loc[rep_list].reset_index()\n",
        "\n",
        "# Multilabel stratification on representatives\n",
        "X_rep = rep_df[['image']].copy()\n",
        "Y_rep = rep_df[label_cols].values\n",
        "mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "rep_df['fold'] = -1\n",
        "for fold, (_, val_idx) in enumerate(mskf.split(X_rep, Y_rep)):\n",
        "    rep_df.loc[val_idx, 'fold'] = fold\n",
        "assert (rep_df['fold'] >= 0).all()\n",
        "\n",
        "# Broadcast folds to all members\n",
        "rep_fold_map = dict(zip(rep_df['image'], rep_df['fold']))\n",
        "train_df['fold_grouped'] = train_df['image'].map(lambda x: rep_fold_map[rep_index[x]])\n",
        "\n",
        "# Inspect distribution\n",
        "for fold in range(5):\n",
        "    cnts = train_df[train_df['fold_grouped'] == fold][label_cols].sum().to_dict()\n",
        "    print(f'Grouped Fold {fold} counts:', cnts)\n",
        "\n",
        "# Save grouped folds\n",
        "out_path = Path('folds_grouped.csv')\n",
        "cols_out = ['image', 'labels', 'fold_grouped'] + label_cols\n",
        "train_df[cols_out].to_csv(out_path, index=False)\n",
        "print('Saved grouped folds to', out_path.resolve())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing: ['imagehash', 'Pillow']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/scipy-1.16.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow-11.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/PIL already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hashed 1000/14905 images in 62.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hashed 2000/14905 images in 123.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hashed 3000/14905 images in 184.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hashed 4000/14905 images in 245.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hashed 5000/14905 images in 307.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hashed 6000/14905 images in 368.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hashed 7000/14905 images in 429.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hashed 8000/14905 images in 491.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hashed 9000/14905 images in 552.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hashed 10000/14905 images in 613.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hashed 11000/14905 images in 675.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hashed 12000/14905 images in 735.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hashed 13000/14905 images in 797.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hashed 14000/14905 images in 858.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved phashes to /var/lib/simon/agent_run_states/plant-pathology-2021-fgvc8-20250921-221229/phash_train.csv\nValid hashed images: 14905 Buckets: 44576\nPair checks: 139, links: 63, time: 0.0s\nGroups: 14879 Avg size: 1.0017474292627193 Max: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grouped Fold 0 counts: {'complex': 343, 'frog_eye_leaf_spot': 702, 'healthy': 735, 'powdery_mildew': 201, 'rust': 333, 'scab': 920}\nGrouped Fold 1 counts: {'complex': 342, 'frog_eye_leaf_spot': 703, 'healthy': 735, 'powdery_mildew': 201, 'rust': 330, 'scab': 920}\nGrouped Fold 2 counts: {'complex': 341, 'frog_eye_leaf_spot': 702, 'healthy': 734, 'powdery_mildew': 202, 'rust': 329, 'scab': 920}\nGrouped Fold 3 counts: {'complex': 342, 'frog_eye_leaf_spot': 702, 'healthy': 734, 'powdery_mildew': 202, 'rust': 327, 'scab': 920}\nGrouped Fold 4 counts: {'complex': 343, 'frog_eye_leaf_spot': 702, 'healthy': 735, 'powdery_mildew': 202, 'rust': 331, 'scab': 920}\nSaved grouped folds to /var/lib/simon/agent_run_states/plant-pathology-2021-fgvc8-20250921-221229/folds_grouped.csv\n"
          ]
        }
      ]
    },
    {
      "id": "9ea59528-a27c-4461-b3a9-e052885da8cc",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Smoke baseline training: convnext_tiny @384, 1-fold, 3 epochs, AMP\n",
        "import os, time, math, json, random, gc\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms as T\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import timm\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "# Config\n",
        "IMG_SIZE = 384\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 3\n",
        "FOLD = 0\n",
        "MODEL_NAME = 'convnext_tiny'  # timm\n",
        "LR = 2e-4\n",
        "WD = 1e-4\n",
        "NUM_WORKERS = 4\n",
        "OUTPUT_DIR = Path('outputs_smoke'); OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Data & labels\n",
        "train_csv = pd.read_csv('train.csv')\n",
        "all_labels = sorted({l for s in train_csv['labels'].astype(str).tolist() for l in s.split()})\n",
        "label_to_idx = {l:i for i,l in enumerate(all_labels)}\n",
        "num_classes = len(all_labels)\n",
        "print('Labels:', all_labels)\n",
        "\n",
        "# Use grouped folds if available\n",
        "folds_path = Path('folds_grouped.csv') if Path('folds_grouped.csv').exists() else Path('folds.csv')\n",
        "folds = pd.read_csv(folds_path)\n",
        "fold_col = 'fold_grouped' if 'fold_grouped' in folds.columns else 'fold'\n",
        "print('Using folds from', folds_path, 'with fold column', fold_col)\n",
        "\n",
        "# Build one-hot targets\n",
        "def labels_to_onehot(s: str) -> np.ndarray:\n",
        "    vec = np.zeros(num_classes, dtype=np.float32)\n",
        "    if isinstance(s, str):\n",
        "        for t in s.split():\n",
        "            if t in label_to_idx: vec[label_to_idx[t]] = 1.0\n",
        "    return vec\n",
        "\n",
        "folds['target_vec'] = folds['labels'].astype(str).apply(labels_to_onehot)\n",
        "\n",
        "train_df = folds[folds[fold_col] != FOLD].reset_index(drop=True)\n",
        "valid_df = folds[folds[fold_col] == FOLD].reset_index(drop=True)\n",
        "print('Train size:', len(train_df), 'Valid size:', len(valid_df))\n",
        "\n",
        "IMG_DIR = Path('train_images')\n",
        "TEST_DIR = Path('test_images')\n",
        "\n",
        "class PlantDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, img_dir: Path, aug: A.Compose):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.aug = aug\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = self.img_dir / row['image']\n",
        "        with Image.open(img_path) as im:\n",
        "            im = im.convert('RGB')\n",
        "            im = np.array(im)\n",
        "        if self.aug is not None:\n",
        "            img = self.aug(image=im)['image']\n",
        "        else:\n",
        "            img = T.ToTensor()(Image.fromarray(im))\n",
        "        target = np.array(row['target_vec'], dtype=np.float32) if 'target_vec' in row else None\n",
        "        return img, torch.from_numpy(target) if target is not None else torch.zeros(num_classes, dtype=torch.float32)\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, img_names: List[str], img_dir: Path, aug: A.Compose):\n",
        "        self.names = img_names\n",
        "        self.img_dir = img_dir\n",
        "        self.aug = aug\n",
        "    def __len__(self): return len(self.names)\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.names[idx]\n",
        "        img_path = self.img_dir / name\n",
        "        with Image.open(img_path) as im:\n",
        "            im = im.convert('RGB')\n",
        "            im = np.array(im)\n",
        "        img = self.aug(image=im)['image'] if self.aug is not None else T.ToTensor()(Image.fromarray(im))\n",
        "        return img, name\n",
        "\n",
        "# Albumentations transforms (aligned with current API requiring size tuple for RRC)\n",
        "train_tfms = A.Compose([\n",
        "    A.RandomResizedCrop(size=(IMG_SIZE, IMG_SIZE), scale=(0.85, 1.0), ratio=(0.9, 1.1)),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=20, p=0.7),\n",
        "    A.RandomBrightnessContrast(0.2, 0.2, p=0.5),\n",
        "    A.HueSaturationValue(10, 15, 10, p=0.5),\n",
        "    A.CoarseDropout(max_holes=6, max_height=IMG_SIZE//16, max_width=IMG_SIZE//16, p=0.3),\n",
        "    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "val_tfms = A.Compose([\n",
        "    A.Resize(height=IMG_SIZE, width=IMG_SIZE),\n",
        "    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "train_ds = PlantDataset(train_df, IMG_DIR, train_tfms)\n",
        "valid_ds = PlantDataset(valid_df, IMG_DIR, val_tfms)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n",
        "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "# Model\n",
        "model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=num_classes).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "num_train_steps = EPOCHS * math.ceil(len(train_loader))\n",
        "warmup_steps = max(1, len(train_loader))\n",
        "def lr_lambda(step):\n",
        "    if step < warmup_steps:\n",
        "        return float(step + 1) / float(warmup_steps)\n",
        "    # cosine decay\n",
        "    progress = (step - warmup_steps) / max(1, (num_train_steps - warmup_steps))\n",
        "    return 0.5 * (1 + math.cos(math.pi * progress))\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
        "\n",
        "def sigmoid_np(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "def eval_micro_f1_from_logits(logits: np.ndarray, targets: np.ndarray, thr: float=0.5) -> float:\n",
        "    probs = sigmoid_np(logits)\n",
        "    preds = (probs >= thr).astype(int)\n",
        "    return f1_score(targets.reshape(-1), preds.reshape(-1), average='micro')\n",
        "\n",
        "best_f1 = -1.0\n",
        "oof_logits = np.zeros((len(valid_df), num_classes), dtype=np.float32)\n",
        "val_targets = np.stack(valid_df['target_vec'].values, axis=0).astype(np.float32)\n",
        "\n",
        "global_step = 0\n",
        "t_start = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    ep_loss = 0.0\n",
        "    t0 = time.time()\n",
        "    for it, (imgs, targets) in enumerate(train_loader):\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, targets)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "        ep_loss += loss.item()\n",
        "        global_step += 1\n",
        "        if (it+1) % 50 == 0:\n",
        "            elapsed = time.time() - t0\n",
        "            print(f'Epoch {epoch+1}/{EPOCHS} it {it+1}/{len(train_loader)} loss {ep_loss/(it+1):.4f} lr {scheduler.get_last_lr()[0]:.6f} elapsed {elapsed:.1f}s', flush=True)\n",
        "    # Validate\n",
        "    model.eval()\n",
        "    val_logits_epoch = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in valid_loader:\n",
        "            imgs = imgs.to(device, non_blocking=True)\n",
        "            with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "                logits = model(imgs)\n",
        "            val_logits_epoch.append(logits.float().cpu().numpy())\n",
        "    val_logits_epoch = np.concatenate(val_logits_epoch, axis=0)\n",
        "    # Save as current oof\n",
        "    oof_logits = val_logits_epoch\n",
        "    micro_f1_050 = eval_micro_f1_from_logits(oof_logits, val_targets, thr=0.5)\n",
        "    print(f'Epoch {epoch+1}: val micro-F1@0.50 = {micro_f1_050:.4f} | epoch time {(time.time()-t0):.1f}s | total {(time.time()-t_start)/60:.1f}m')\n",
        "    if micro_f1_050 > best_f1:\n",
        "        best_f1 = micro_f1_050\n",
        "        torch.save({'model': model.state_dict(), 'epoch': epoch, 'f1': best_f1}, OUTPUT_DIR / f'best_fold{FOLD}.pth')\n",
        "\n",
        "# Threshold optimization (global then per-class)\n",
        "def optimize_thresholds(logits: np.ndarray, targets: np.ndarray) -> Tuple[np.ndarray, float, float]:\n",
        "    probs = sigmoid_np(logits)\n",
        "    # global search\n",
        "    global_thrs = np.arange(0.1, 0.6, 0.01)\n",
        "    best_global, best_score = 0.5, -1.0\n",
        "    for th in global_thrs:\n",
        "        s = f1_score(targets.reshape(-1), (probs >= th).astype(int).reshape(-1), average='micro')\n",
        "        if s > best_score: best_score, best_global = s, th\n",
        "    # per-class refine\n",
        "    thrs = np.full(num_classes, best_global, dtype=np.float32)\n",
        "    for c in range(num_classes):\n",
        "        best_c, best_s = thrs[c], -1.0\n",
        "        for th in np.arange(max(0.05, best_global-0.2), min(0.7, best_global+0.2), 0.01):\n",
        "            preds = (probs >= thrs)  # start from current\n",
        "            preds[:, c] = (probs[:, c] >= th)\n",
        "            s = f1_score(targets.reshape(-1), preds.astype(int).reshape(-1), average='micro')\n",
        "            if s > best_s: best_s, best_c = s, th\n",
        "        thrs[c] = best_c\n",
        "    final_preds = (probs >= thrs).astype(int)\n",
        "    final_score = f1_score(targets.reshape(-1), final_preds.reshape(-1), average='micro')\n",
        "    return thrs, best_global, final_score\n",
        "\n",
        "thrs, best_global, oof_score = optimize_thresholds(oof_logits, val_targets)\n",
        "print('OOF micro-F1 (per-class optimized):', f'{oof_score:.4f}', 'global@', f'{best_global:.2f}')\n",
        "np.save(OUTPUT_DIR / f'oof_logits_fold{FOLD}.npy', oof_logits)\n",
        "np.save(OUTPUT_DIR / f'val_targets_fold{FOLD}.npy', val_targets)\n",
        "np.save(OUTPUT_DIR / f'thresholds_fold{FOLD}.npy', thrs)\n",
        "\n",
        "# Train on train folds already done; proceed to test inference with best checkpoint\n",
        "ckpt = torch.load(OUTPUT_DIR / f'best_fold{FOLD}.pth', map_location='cpu')\n",
        "model.load_state_dict(ckpt['model'])\n",
        "model.to(device); model.eval()\n",
        "\n",
        "# Prepare test loader\n",
        "test_names = sorted([p.name for p in TEST_DIR.glob('*.jpg')])\n",
        "test_ds = TestDataset(test_names, TEST_DIR, val_tfms)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_logits = []\n",
        "t0 = time.time()\n",
        "with torch.no_grad():\n",
        "    for it, (imgs, names) in enumerate(test_loader):\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "            logits = model(imgs)\n",
        "        test_logits.append(logits.float().cpu().numpy())\n",
        "        if (it+1) % 50 == 0:\n",
        "            print(f'Test infer {it+1}/{len(test_loader)} elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "test_logits = np.concatenate(test_logits, axis=0)\n",
        "np.save(OUTPUT_DIR / f'test_logits_fold{FOLD}.npy', test_logits)\n",
        "\n",
        "# Post-processing for submission: per-class thresholds, healthy exclusivity, at-least-one\n",
        "probs_test = sigmoid_np(test_logits)\n",
        "preds_bin = (probs_test >= thrs).astype(int)\n",
        "\n",
        "# healthy exclusivity\n",
        "if 'healthy' in all_labels:\n",
        "    h_idx = all_labels.index('healthy')\n",
        "    disease_idx = [i for i,l in enumerate(all_labels) if l != 'healthy']\n",
        "    for i in range(preds_bin.shape[0]):\n",
        "        if preds_bin[i, disease_idx].max() == 1:\n",
        "            preds_bin[i, h_idx] = 0\n",
        "        # at-least-one\n",
        "        if preds_bin[i].sum() == 0:\n",
        "            top1 = probs_test[i].argmax()\n",
        "            preds_bin[i, top1] = 1\n",
        "else:\n",
        "    # at-least-one\n",
        "    for i in range(preds_bin.shape[0]):\n",
        "        if preds_bin[i].sum() == 0:\n",
        "            top1 = probs_test[i].argmax()\n",
        "            preds_bin[i, top1] = 1\n",
        "\n",
        "pred_labels = []\n",
        "for i in range(preds_bin.shape[0]):\n",
        "    labs = [all_labels[j] for j in range(num_classes) if preds_bin[i, j] == 1]\n",
        "    pred_labels.append(' '.join(labs) if len(labs)>0 else all_labels[int(probs_test[i].argmax())])\n",
        "\n",
        "sub = pd.DataFrame({'image': test_names, 'labels': pred_labels})\n",
        "sub_path = Path('submission.csv')\n",
        "sub.to_csv(sub_path, index=False)\n",
        "print('Saved submission to', sub_path.resolve())\n",
        "print('Smoke baseline complete. Elapsed total: {:.1f}m'.format((time.time()-t_start)/60.0))\n",
        "\n",
        "gc.collect(); torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\nLabels: ['complex', 'frog_eye_leaf_spot', 'healthy', 'powdery_mildew', 'rust', 'scab']\nUsing folds from folds_grouped.csv with fold column fold_grouped\nTrain size: 11918 Valid size: 2987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n  original_init(self, **validated_kwargs)\n/tmp/ipykernel_61/4013949269.py:108: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n  A.CoarseDropout(max_holes=6, max_height=IMG_SIZE//16, max_width=IMG_SIZE//16, p=0.3),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/4013949269.py:136: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/4013949269.py:160: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}