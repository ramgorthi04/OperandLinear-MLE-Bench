{
  "cells": [
    {
      "id": "50268922-e6ee-4a66-8a7f-fab1ed5d10c1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, subprocess, time, json, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print('== Environment & GPU Check ==', flush=True)\n",
        "t0 = time.time()\n",
        "try:\n",
        "    print(subprocess.run(['bash','-lc','nvidia-smi || true'], capture_output=True, text=True).stdout)\n",
        "except Exception as e:\n",
        "    print('nvidia-smi failed:', e, flush=True)\n",
        "\n",
        "print('Python:', sys.version)\n",
        "print('CUDA_VISIBLE_DEVICES:', os.environ.get('CUDA_VISIBLE_DEVICES'))\n",
        "\n",
        "print('Checking torch (may not be installed yet)...', flush=True)\n",
        "try:\n",
        "    import torch\n",
        "    print('torch:', torch.__version__)\n",
        "    print('CUDA available:', torch.cuda.is_available())\n",
        "    if torch.cuda.is_available():\n",
        "        print('GPU:', torch.cuda.get_device_name(0))\n",
        "except Exception as e:\n",
        "    print('torch not available or import failed:', e)\n",
        "\n",
        "print('Elapsed: %.2fs' % (time.time()-t0), flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "bb28fe23-a440-4832-94f3-706da4ef1710",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan: Google Smartphone Decimeter Challenge 2022\n",
        "\n",
        "Objectives:\n",
        "- Ship a medal-capable baseline quickly: ENU Kalman Filter (constant-velocity) + RTS smoother with Doppler speed pseudo-measurement.\n",
        "- Use strict route-level CV (GroupKFold by Path) and avoid leakage.\n",
        "- Add multi-phone fusion and light post-processing; only add IMU/ML if CV justifies.\n",
        "\n",
        "Data understanding checklist:\n",
        "- Inspect train/* routes for ground_truth.csv, device_gnss.csv, device_imu.csv; confirm schemas.\n",
        "- Confirm sample_submission columns and exact key: [Path, Phone, MillisSinceGpsEpoch, LatitudeDegrees, LongitudeDegrees].\n",
        "- Determine train/test split and phone models.\n",
        "\n",
        "Baseline v1 (fast, reliable):\n",
        "- Parse device_gnss.csv (lat/lon/HorizontalAccuracyMeters/SpeedMps if present).\n",
        "- Convert WGS84 -> ECEF -> ENU (double precision) using route-level anchor (median position).\n",
        "- Constant-velocity KF on state [x, y, vx, vy] with variable dt; process noise q_a ~1.5\u20133.0 m/s^2.\n",
        "- Measurement updates:\n",
        "  - Position z = [x, y] with Rpos from HorizontalAccuracyMeters^2 and floor (\u2265 3 m^2), Mahalanobis gating (p\u22480.99).\n",
        "  - Doppler speed pseudo-measurement z = ||v|| with R_speed ~ 0.5^2 (if Doppler), else ~1.5^2.\n",
        "- Outlier handling: drop high hAcc (>50 m), unrealistic speed/acc, and gated innovations.\n",
        "- Backward pass: RTS smoothing over full route.\n",
        "- Optional small gain: Savitzky\u2013Golay in ENU after RTS.\n",
        "- Convert ENU -> ECEF -> WGS84 for output.\n",
        "\n",
        "CV protocol:\n",
        "- GroupKFold with groups = Path (entire route). No mixing phones within the same Path across folds.\n",
        "- Metric: average haversine; report overall and per-route.\n",
        "\n",
        "Feature engineering / extensions (prioritized):\n",
        "1) Multi-phone fusion per route: time-align within \u00b1200 ms; weighted ENU average (\u22481/Rpos) then re-smooth.\n",
        "2) Phone-specific tuning: per-phone R multipliers; OOF-learned ENU bias correction.\n",
        "3) IMU complementary (optional after stable CV): interpolate IMU to GNSS times; use yaw-rate to stabilize heading.\n",
        "4) Lightweight learning (optional): XGBoost on KF+RTS residuals (predict \u0394E, \u0394N).\n",
        "\n",
        "Modeling roadmap:\n",
        "1) I/O + scorer + ENU utils.\n",
        "2) KF + RTS with hAcc-weighted R and Doppler speed.\n",
        "3) Outlier gating + optional SG post-filter.\n",
        "4) Phone tuning and multi-phone fusion.\n",
        "5) Optional IMU complementary or ML residuals if needed.\n",
        "\n",
        "Runtime/Env:\n",
        "- CPU/Numpy/SciPy sufficient. Keep GPU idle unless adding deep models.\n",
        "\n",
        "Milestones & Expert check-ins:\n",
        "A) After data inspection/schema confirmation.\n",
        "B) After KF+RTS baseline and CV wiring.\n",
        "C) After first OOF; decide on fusion/IMU/ML.\n",
        "D) Before any heavy training.\n",
        "\n",
        "Pitfalls to avoid:\n",
        "- Never smooth directly in lat/lon; always in ENU.\n",
        "- Strict route-level CV; no within-route leakage.\n",
        "- Floor tiny reported accuracies; avoid over-trusting hAcc < 2\u20133 m.\n",
        "- Use float64 for ECEF/ENU; propagate with actual dt; do not fabricate GNSS.\n",
        "- Validate submission keys/order and lat/lon ranges.\n",
        "\n",
        "Next actions:\n",
        "1) Explore train/* and sample_submission; implement haversine scorer.\n",
        "2) Build robust WGS84\u2194ECEF\u2194ENU utils and route anchoring.\n",
        "3) Implement KF+RTS baseline with adaptive R and Doppler speed.\n",
        "4) Route-level CV; produce first OOF and a sanity submission."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "a4e3012d-7ec1-43cc-9267-ab9deb443ac4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd, numpy as np, os, glob, json, time\n",
        "from pathlib import Path\n",
        "\n",
        "print('== Data exploration & scorer setup ==', flush=True)\n",
        "ROOT = Path('.')\n",
        "TRAIN_DIR = ROOT / 'train'\n",
        "TEST_DIR = ROOT / 'test'\n",
        "SAMPLE_PATH = ROOT / 'sample_submission.csv'\n",
        "\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371000.0\n",
        "    p1 = np.radians(lat1)\n",
        "    p2 = np.radians(lat2)\n",
        "    dphi = np.radians(lat2 - lat1)\n",
        "    dlmb = np.radians(lon2 - lon1)\n",
        "    a = np.sin(dphi/2.0)**2 + np.cos(p1) * np.cos(p2) * np.sin(dlmb/2.0)**2\n",
        "    return 2*R*np.arcsin(np.sqrt(a))\n",
        "\n",
        "def mean_haversine_df(df, lat_col_pred, lon_col_pred, lat_col_true, lon_col_true):\n",
        "    return np.mean(haversine(df[lat_col_pred].values, df[lon_col_pred].values, df[lat_col_true].values, df[lon_col_true].values))\n",
        "\n",
        "# Inspect sample submission\n",
        "sample = pd.read_csv(SAMPLE_PATH)\n",
        "print('sample_submission shape:', sample.shape)\n",
        "print('sample_submission columns:', list(sample.columns))\n",
        "print(sample.head(3))\n",
        "\n",
        "# Enumerate train routes and phones\n",
        "train_routes = sorted([p for p in TRAIN_DIR.glob('*') if p.is_dir()])\n",
        "print('num train routes:', len(train_routes))\n",
        "if train_routes[:3]:\n",
        "    print('example train routes:', [p.name for p in train_routes[:3]])\n",
        "\n",
        "# Check expected files within a train route (ground_truth?)\n",
        "if train_routes:\n",
        "    tr = train_routes[0]\n",
        "    cand = list(tr.rglob('*.csv'))\n",
        "    print('first route csvs (up to 10):', [str(p.relative_to(ROOT)) for p in cand[:10]])\n",
        "    gt = list(tr.rglob('ground_truth.csv'))\n",
        "    print('ground_truth present?', bool(gt), 'paths:', [str(g.relative_to(ROOT)) for g in gt[:3]])\n",
        "\n",
        "# Inspect a test route's device_gnss/imu schema\n",
        "test_routes = sorted([p for p in TEST_DIR.glob('*') if p.is_dir()])\n",
        "print('num test routes:', len(test_routes))\n",
        "if test_routes:\n",
        "    # pick first route and first phone\n",
        "    phones = sorted([p for p in test_routes[0].glob('*') if p.is_dir()])\n",
        "    if phones:\n",
        "        gnss_path = phones[0] / 'device_gnss.csv'\n",
        "        imu_path = phones[0] / 'device_imu.csv'\n",
        "        if gnss_path.exists():\n",
        "            gnss_head = pd.read_csv(gnss_path, nrows=5)\n",
        "            print('device_gnss columns:', list(gnss_head.columns))\n",
        "            print(gnss_head.head(3))\n",
        "        if imu_path.exists():\n",
        "            imu_head = pd.read_csv(imu_path, nrows=5)\n",
        "            print('device_imu columns:', list(imu_head.columns))\n",
        "            print(imu_head.head(3))\n",
        "\n",
        "print('== Done exploration setup ==', flush=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Data exploration & scorer setup ==\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_submission shape: (37087, 4)\nsample_submission columns: ['tripId', 'UnixTimeMillis', 'LatitudeDegrees', 'LongitudeDegrees']\n                             tripId  UnixTimeMillis  LatitudeDegrees  \\\n0  2020-06-04-US-MTV-1-GooglePixel4   1591304310441        37.904611   \n1  2020-06-04-US-MTV-1-GooglePixel4   1591304311441        37.904611   \n2  2020-06-04-US-MTV-1-GooglePixel4   1591304312441        37.904611   \n\n   LongitudeDegrees  \n0        -86.481078  \n1        -86.481078  \n2        -86.481078  \nnum train routes: 54\nexample train routes: ['2020-05-15-US-MTV-1', '2020-05-21-US-MTV-1', '2020-05-21-US-MTV-2']\nfirst route csvs (up to 10): ['train/2020-05-15-US-MTV-1/GooglePixel4XL/device_gnss.csv', 'train/2020-05-15-US-MTV-1/GooglePixel4XL/device_imu.csv', 'train/2020-05-15-US-MTV-1/GooglePixel4XL/ground_truth.csv']\nground_truth present? True paths: ['train/2020-05-15-US-MTV-1/GooglePixel4XL/ground_truth.csv']\nnum test routes: 8\ndevice_gnss columns: ['MessageType', 'utcTimeMillis', 'TimeNanos', 'LeapSecond', 'FullBiasNanos', 'BiasNanos', 'BiasUncertaintyNanos', 'DriftNanosPerSecond', 'DriftUncertaintyNanosPerSecond', 'HardwareClockDiscontinuityCount', 'Svid', 'TimeOffsetNanos', 'State', 'ReceivedSvTimeNanos', 'ReceivedSvTimeUncertaintyNanos', 'Cn0DbHz', 'PseudorangeRateMetersPerSecond', 'PseudorangeRateUncertaintyMetersPerSecond', 'AccumulatedDeltaRangeState', 'AccumulatedDeltaRangeMeters', 'AccumulatedDeltaRangeUncertaintyMeters', 'CarrierFrequencyHz', 'MultipathIndicator', 'ConstellationType', 'CodeType', 'ChipsetElapsedRealtimeNanos', 'ArrivalTimeNanosSinceGpsEpoch', 'RawPseudorangeMeters', 'RawPseudorangeUncertaintyMeters', 'SignalType', 'ReceivedSvTimeNanosSinceGpsEpoch', 'SvPositionXEcefMeters', 'SvPositionYEcefMeters', 'SvPositionZEcefMeters', 'SvElevationDegrees', 'SvAzimuthDegrees', 'SvVelocityXEcefMetersPerSecond', 'SvVelocityYEcefMetersPerSecond', 'SvVelocityZEcefMetersPerSecond', 'SvClockBiasMeters', 'SvClockDriftMetersPerSecond', 'IsrbMeters', 'IonosphericDelayMeters', 'TroposphericDelayMeters', 'WlsPositionXEcefMeters', 'WlsPositionYEcefMeters', 'WlsPositionZEcefMeters']\n  MessageType  utcTimeMillis      TimeNanos  LeapSecond        FullBiasNanos  \\\n0         Raw  1591304310441  2738801000000         NaN -1275336789640276310   \n1         Raw  1591304310441  2738801000000         NaN -1275336789640276310   \n2         Raw  1591304310441  2738801000000         NaN -1275336789640276310   \n\n   BiasNanos  BiasUncertaintyNanos  DriftNanosPerSecond  \\\n0  -0.354471             35.854868             -1.63412   \n1  -0.354471             35.854868             -1.63412   \n2  -0.354471             35.854868             -1.63412   \n\n   DriftUncertaintyNanosPerSecond  HardwareClockDiscontinuityCount  ...  \\\n0                        11.55743                                5  ...   \n1                        11.55743                                5  ...   \n2                        11.55743                                5  ...   \n\n   SvVelocityYEcefMetersPerSecond  SvVelocityZEcefMetersPerSecond  \\\n0                      940.546655                     1272.568689   \n1                     1355.261595                     -190.558404   \n2                    -2202.171139                    -1997.130335   \n\n   SvClockBiasMeters  SvClockDriftMetersPerSecond  IsrbMeters  \\\n0     -139997.826343                     0.000003         0.0   \n1      -84965.454443                    -0.001705         0.0   \n2       32945.787533                    -0.001711         0.0   \n\n   IonosphericDelayMeters  TroposphericDelayMeters  WlsPositionXEcefMeters  \\\n0                3.601089                 2.725299           -2.693806e+06   \n1                5.635305                 4.892562           -2.693806e+06   \n2                3.412855                 2.505213           -2.693806e+06   \n\n   WlsPositionYEcefMeters  WlsPositionZEcefMeters  \n0           -4.297551e+06            3.854181e+06  \n1           -4.297551e+06            3.854181e+06  \n2           -4.297551e+06            3.854181e+06  \n\n[3 rows x 47 columns]\ndevice_imu columns: ['MessageType', 'utcTimeMillis', 'MeasurementX', 'MeasurementY', 'MeasurementZ', 'BiasX', 'BiasY', 'BiasZ']\n  MessageType  utcTimeMillis  MeasurementX  MeasurementY  MeasurementZ  \\\n0    UncalMag  1591304310444     17.000017    -37.054750    -16.671238   \n1  UncalAccel  1591304310448      0.024007      9.814959      0.398068   \n2   UncalGyro  1591304310448     -0.100704      0.033672     -0.011885   \n\n      BiasX     BiasY      BiasZ  \n0  3.452433  3.855671  11.627296  \n1  0.000000  0.000000   0.000000  \n2  0.000000  0.000000   0.000000  \n== Done exploration setup ==\n"
          ]
        }
      ]
    },
    {
      "id": "f9e96675-8b92-4972-ba2d-8c0047183a97",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# === WGS84 <-> ECEF <-> ENU utilities (float64) ===\n",
        "WGS84_A = 6378137.0\n",
        "WGS84_F = 1.0/298.257223563\n",
        "WGS84_E2 = WGS84_F*(2 - WGS84_F)\n",
        "\n",
        "def geodetic_to_ecef(lat_deg, lon_deg, h_m=0.0):\n",
        "    lat = np.radians(lat_deg, dtype=np.float64)\n",
        "    lon = np.radians(lon_deg, dtype=np.float64)\n",
        "    a = WGS84_A\n",
        "    e2 = WGS84_E2\n",
        "    s = np.sin(lat)\n",
        "    N = a/np.sqrt(1 - e2*s*s)\n",
        "    X = (N + h_m) * np.cos(lat) * np.cos(lon)\n",
        "    Y = (N + h_m) * np.cos(lat) * np.sin(lon)\n",
        "    Z = (N*(1 - e2) + h_m) * np.sin(lat)\n",
        "    return X, Y, Z\n",
        "\n",
        "def ecef_to_geodetic(X, Y, Z):\n",
        "    a = WGS84_A\n",
        "    e2 = WGS84_E2\n",
        "    b = a*np.sqrt(1 - e2)\n",
        "    ep = np.sqrt((a*a - b*b)/(b*b))\n",
        "    p = np.sqrt(X*X + Y*Y)\n",
        "    th = np.arctan2(a*Z, b*p)\n",
        "    lon = np.arctan2(Y, X)\n",
        "    lat = np.arctan2(Z + ep*ep*b*np.sin(th)**3, p - e2*a*np.cos(th)**3)\n",
        "    N = a/np.sqrt(1 - e2*np.sin(lat)**2)\n",
        "    h = p/np.cos(lat) - N\n",
        "    return np.degrees(lat), np.degrees(lon), h\n",
        "\n",
        "def ecef_to_enu(X, Y, Z, lat0_deg, lon0_deg, h0_m=0.0):\n",
        "    # Translate to origin\n",
        "    X0, Y0, Z0 = geodetic_to_ecef(lat0_deg, lon0_deg, h0_m)\n",
        "    dX, dY, dZ = X - X0, Y - Y0, Z - Z0\n",
        "    lat0 = np.radians(lat0_deg, dtype=np.float64)\n",
        "    lon0 = np.radians(lon0_deg, dtype=np.float64)\n",
        "    slat, clat = np.sin(lat0), np.cos(lat0)\n",
        "    slon, clon = np.sin(lon0), np.cos(lon0)\n",
        "    t = np.array([\n",
        "        [-slon,             clon,              0.0],\n",
        "        [-slat*clon, -slat*slon,  clat],\n",
        "        [ clat*clon,  clat*slon,  slat]\n",
        "    ], dtype=np.float64)\n",
        "    d = np.vstack([dX, dY, dZ])\n",
        "    enu = t @ d\n",
        "    E, N, U = enu[0], enu[1], enu[2]\n",
        "    return E, N, U\n",
        "\n",
        "def enu_to_ecef(E, N, U, lat0_deg, lon0_deg, h0_m=0.0):\n",
        "    lat0 = np.radians(lat0_deg, dtype=np.float64)\n",
        "    lon0 = np.radians(lon0_deg, dtype=np.float64)\n",
        "    slat, clat = np.sin(lat0), np.cos(lat0)\n",
        "    slon, clon = np.sin(lon0), np.cos(lon0)\n",
        "    R = np.array([\n",
        "        [-slon,             clon,              0.0],\n",
        "        [-slat*clon, -slat*slon,  clat],\n",
        "        [ clat*clon,  clat*slon,  slat]\n",
        "    ], dtype=np.float64)\n",
        "    R_T = R.T\n",
        "    e = np.vstack([E, N, U])\n",
        "    dX, dY, dZ = (R_T @ e)\n",
        "    X0, Y0, Z0 = geodetic_to_ecef(lat0_deg, lon0_deg, h0_m)\n",
        "    return X0 + dX, Y0 + dY, Z0 + dZ\n",
        "\n",
        "def enu_to_geodetic(E, N, U, lat0_deg, lon0_deg, h0_m=0.0):\n",
        "    X, Y, Z = enu_to_ecef(E, N, U, lat0_deg, lon0_deg, h0_m)\n",
        "    return ecef_to_geodetic(X, Y, Z)\n",
        "\n",
        "# === Data loaders for per-phone trajectory (using WLS ECEF from device_gnss) ===\n",
        "def load_phone_gnss_positions(gnss_csv: Path) -> pd.DataFrame:\n",
        "    # Use WlsPosition* columns and utcTimeMillis; drop NaNs; sort and dedup by time\n",
        "    usecols = ['utcTimeMillis', 'WlsPositionXEcefMeters', 'WlsPositionYEcefMeters', 'WlsPositionZEcefMeters']\n",
        "    df = pd.read_csv(gnss_csv, usecols=usecols)\n",
        "    df = df.dropna(subset=['WlsPositionXEcefMeters', 'WlsPositionYEcefMeters', 'WlsPositionZEcefMeters'])\n",
        "    df = df.drop_duplicates(subset=['utcTimeMillis'])\n",
        "    df = df.sort_values('utcTimeMillis').reset_index(drop=True)\n",
        "    df['t'] = df['utcTimeMillis'].astype(np.int64)\n",
        "    df.rename(columns={\n",
        "        'WlsPositionXEcefMeters': 'X',\n",
        "        'WlsPositionYEcefMeters': 'Y',\n",
        "        'WlsPositionZEcefMeters': 'Z'\n",
        "    }, inplace=True)\n",
        "    return df[['t','X','Y','Z']].astype({'t':np.int64, 'X':np.float64, 'Y':np.float64, 'Z':np.float64})\n",
        "\n",
        "def anchor_route_latlon(df_ecef: pd.DataFrame):\n",
        "    # Anchor via median geodetic from ECEF positions\n",
        "    lat_list, lon_list = ecef_to_geodetic(df_ecef['X'].values, df_ecef['Y'].values, df_ecef['Z'].values)[:2]\n",
        "    lat0 = np.median(lat_list)\n",
        "    lon0 = np.median(lon_list)\n",
        "    return float(lat0), float(lon0)\n",
        "\n",
        "def ecef_df_to_enu(df_ecef: pd.DataFrame, lat0: float, lon0: float):\n",
        "    E, N, U = ecef_to_enu(df_ecef['X'].values.astype(np.float64),\n",
        "                           df_ecef['Y'].values.astype(np.float64),\n",
        "                           df_ecef['Z'].values.astype(np.float64),\n",
        "                           lat0, lon0, 0.0)\n",
        "    out = df_ecef.copy()\n",
        "    out['E'] = E\n",
        "    out['N'] = N\n",
        "    out['U'] = U\n",
        "    return out\n",
        "\n",
        "def enu_to_latlon_series(E: np.ndarray, N: np.ndarray, U: np.ndarray, lat0: float, lon0: float):\n",
        "    lat, lon, _ = enu_to_geodetic(E, N, U, lat0, lon0, 0.0)\n",
        "    return np.asarray(lat), np.asarray(lon)\n",
        "\n",
        "print('Utils loaded: coord transforms and GNSS WLS loader.', flush=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utils loaded: coord transforms and GNSS WLS loader.\n"
          ]
        }
      ]
    },
    {
      "id": "ff149d11-979f-44eb-b776-db21ce8b656a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# === Constant-Velocity Kalman Filter + RTS Smoother (2D ENU) ===\n",
        "def kf_rts_smooth(E: np.ndarray, N: np.ndarray, t_ms: np.ndarray,\n",
        "                  r_pos_var: float = 36.0, q_acc: float = 2.25,\n",
        "                  gate_chi2: float = 9.21):\n",
        "    # Inputs: arrays of measurements (E,N) and timestamps (ms) sorted by time\n",
        "    # r_pos_var: measurement variance per axis (m^2); q_acc: accel variance (m^2/s^4) used to scale Q\n",
        "    n = len(t_ms)\n",
        "    if n == 0:\n",
        "        return np.array([]), np.array([])\n",
        "    x = np.zeros((n, 4), dtype=np.float64)  # [E, N, vE, vN]\n",
        "    P = np.zeros((n, 4, 4), dtype=np.float64)\n",
        "    Fm = np.zeros((n, 4, 4), dtype=np.float64)  # store F per step for RTS\n",
        "    Qm = np.zeros((n, 4, 4), dtype=np.float64)\n",
        "    # Init\n",
        "    x0 = np.array([E[0], N[0], 0.0, 0.0], dtype=np.float64)\n",
        "    P0 = np.diag([r_pos_var, r_pos_var, 25.0, 25.0]).astype(np.float64)\n",
        "    x[0] = x0\n",
        "    P[0] = P0\n",
        "    H = np.array([[1,0,0,0],[0,1,0,0]], dtype=np.float64)\n",
        "    R = np.diag([r_pos_var, r_pos_var]).astype(np.float64)\n",
        "    for k in range(1, n):\n",
        "        dt = max(1e-3, (t_ms[k] - t_ms[k-1]) * 1e-3)\n",
        "        F = np.array([[1,0,dt,0],\n",
        "                      [0,1,0,dt],\n",
        "                      [0,0,1, 0],\n",
        "                      [0,0,0, 1]], dtype=np.float64)\n",
        "        q = q_acc\n",
        "        dt2 = dt*dt\n",
        "        dt3 = dt2*dt\n",
        "        dt4 = dt2*dt2\n",
        "        Q = q * np.array([[dt4/4,    0.0,   dt3/2,  0.0],\n",
        "                          [   0.0, dt4/4,     0.0, dt3/2],\n",
        "                          [dt3/2,    0.0,    dt2,  0.0],\n",
        "                          [   0.0, dt3/2,     0.0,   dt2]], dtype=np.float64)\n",
        "        # Predict\n",
        "        x_pred = F @ x[k-1]\n",
        "        P_pred = F @ P[k-1] @ F.T + Q\n",
        "        # Update with position measurement (Mahalanobis gating)\n",
        "        z = np.array([E[k], N[k]], dtype=np.float64)\n",
        "        y = z - (H @ x_pred)\n",
        "        S = H @ P_pred @ H.T + R\n",
        "        try:\n",
        "            Sinv = np.linalg.inv(S)\n",
        "        except np.linalg.LinAlgError:\n",
        "            Sinv = np.linalg.pinv(S)\n",
        "        maha2 = float(y.T @ Sinv @ y)\n",
        "        if maha2 <= gate_chi2:\n",
        "            K = P_pred @ H.T @ Sinv\n",
        "            x_upd = x_pred + K @ y\n",
        "            P_upd = (np.eye(4) - K @ H) @ P_pred\n",
        "        else:\n",
        "            # Reject update\n",
        "            x_upd, P_upd = x_pred, P_pred\n",
        "        x[k] = x_upd\n",
        "        P[k] = P_upd\n",
        "        Fm[k] = F\n",
        "        Qm[k] = Q\n",
        "    # RTS smoothing\n",
        "    xs = x.copy()\n",
        "    Ps = P.copy()\n",
        "    for k in range(n-2, -1, -1):\n",
        "        F = Fm[k+1]\n",
        "        Pk = P[k]\n",
        "        P_pred = F @ Pk @ F.T + Qm[k+1]\n",
        "        try:\n",
        "            Ck = Pk @ F.T @ np.linalg.inv(P_pred)\n",
        "        except np.linalg.LinAlgError:\n",
        "            Ck = Pk @ F.T @ np.linalg.pinv(P_pred)\n",
        "        xs[k] = x[k] + Ck @ (xs[k+1] - (F @ x[k]))\n",
        "        Ps[k] = Pk + Ck @ (Ps[k+1] - P_pred) @ Ck.T\n",
        "    return xs[:,0], xs[:,1]\n",
        "\n",
        "def run_phone_kf(gnss_csv: Path, sample_times: np.ndarray):\n",
        "    # Load WLS ECEF and convert to ENU, run KF+RTS, then interpolate to sample_times\n",
        "    df_ecef = load_phone_gnss_positions(gnss_csv)\n",
        "    if len(df_ecef) == 0:\n",
        "        return pd.DataFrame({'UnixTimeMillis': sample_times, 'LatitudeDegrees': np.nan, 'LongitudeDegrees': np.nan})\n",
        "    lat0, lon0 = anchor_route_latlon(df_ecef)\n",
        "    df_enu = ecef_df_to_enu(df_ecef, lat0, lon0)\n",
        "    # Kalman smoother\n",
        "    Es, Ns = kf_rts_smooth(df_enu['E'].values, df_enu['N'].values, df_enu['t'].values,\n",
        "                            r_pos_var=36.0, q_acc=2.25, gate_chi2=9.21)\n",
        "    # Interpolate E,N to sample times\n",
        "    t_train = df_enu['t'].values.astype(np.int64)\n",
        "    # Ensure strictly increasing for interpolation\n",
        "    uniq_mask = np.concatenate([[True], t_train[1:] != t_train[:-1]])\n",
        "    t_train = t_train[uniq_mask]\n",
        "    Es = Es[uniq_mask]\n",
        "    Ns = Ns[uniq_mask]\n",
        "    # Linear interpolation; extrapolate with nearest\n",
        "    def interp_nearest(x, xp, fp):\n",
        "        y = np.interp(x, xp, fp)\n",
        "        y[x < xp[0]] = fp[0]\n",
        "        y[x > xp[-1]] = fp[-1]\n",
        "        return y\n",
        "    Eq = interp_nearest(sample_times.astype(np.int64), t_train, Es)\n",
        "    Nq = interp_nearest(sample_times.astype(np.int64), t_train, Ns)\n",
        "    lat, lon = enu_to_latlon_series(Eq, Nq, np.zeros_like(Eq), lat0, lon0)\n",
        "    return pd.DataFrame({'UnixTimeMillis': sample_times, 'LatitudeDegrees': lat, 'LongitudeDegrees': lon})\n",
        "\n",
        "def build_submission_from_sample(sample_path: Path, test_root: Path) -> pd.DataFrame:\n",
        "    sub = pd.read_csv(sample_path)\n",
        "    out_rows = []\n",
        "    for trip_id, grp in sub.groupby('tripId', sort=False):\n",
        "        phone = trip_id.rsplit('-', 1)[-1]\n",
        "        route = trip_id[:-(len(phone)+1)]\n",
        "        gnss_csv = test_root / route / phone / 'device_gnss.csv'\n",
        "        if not gnss_csv.exists():\n",
        "            # Fallback: fill with first row coords (will be penalized but keeps format)\n",
        "            tmp = grp[['UnixTimeMillis']].copy()\n",
        "            tmp['LatitudeDegrees'] = grp['LatitudeDegrees'].iloc[0]\n",
        "            tmp['LongitudeDegrees'] = grp['LongitudeDegrees'].iloc[0]\n",
        "            tmp['tripId'] = trip_id\n",
        "            out_rows.append(tmp)\n",
        "            continue\n",
        "        pred_df = run_phone_kf(gnss_csv, grp['UnixTimeMillis'].values.astype(np.int64))\n",
        "        pred_df['tripId'] = trip_id\n",
        "        out_rows.append(pred_df[['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees']])\n",
        "    pred = pd.concat(out_rows, ignore_index=True)\n",
        "    # Ensure order matches sample\n",
        "    pred = pred.merge(sub[['tripId','UnixTimeMillis']].assign(_ord=np.arange(len(sub))),\n",
        "                      on=['tripId','UnixTimeMillis'], how='right').sort_values('_ord').drop(columns=['_ord'])\n",
        "    # Basic sanity:\n",
        "    pred['LatitudeDegrees'] = pred['LatitudeDegrees'].clip(-90, 90)\n",
        "    pred['LongitudeDegrees'] = ((pred['LongitudeDegrees'] + 180) % 360) - 180\n",
        "    return pred\n",
        "\n",
        "def save_submission(df: pd.DataFrame, path: Path):\n",
        "    df.to_csv(path, index=False)\n",
        "    print('Saved submission:', path, 'shape:', df.shape, flush=True)\n",
        "\n",
        "print('KF+RTS and submission builders ready.', flush=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KF+RTS and submission builders ready.\n"
          ]
        }
      ]
    },
    {
      "id": "a14963a0-57cb-4ff4-8c75-8aff09ab344b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "print('== Building submission from sample ==', flush=True)\n",
        "pred = build_submission_from_sample(Path('sample_submission.csv'), Path('test'))\n",
        "print('Pred head:\\n', pred.head(3))\n",
        "print('Ranges: lat[%.6f, %.6f] lon[%.6f, %.6f]' % (pred.LatitudeDegrees.min(), pred.LatitudeDegrees.max(), pred.LongitudeDegrees.min(), pred.LongitudeDegrees.max()))\n",
        "save_submission(pred, Path('submission.csv'))\n",
        "print('== Done ==', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "b8d4a106-8fa0-49d8-9ca3-2b835f74807f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "print('== Inspect ground_truth schema and set up CV utils ==', flush=True)\n",
        "TRAIN_DIR = Path('train')\n",
        "\n",
        "# Inspect one ground_truth.csv\n",
        "gt_paths = list((TRAIN_DIR / '2020-05-15-US-MTV-1').rglob('ground_truth.csv'))\n",
        "if gt_paths:\n",
        "    gt_head = pd.read_csv(gt_paths[0], nrows=5)\n",
        "    print('ground_truth columns:', list(gt_head.columns))\n",
        "    print(gt_head.head(3))\n",
        "else:\n",
        "    # fallback: search any route\n",
        "    any_gt = list(TRAIN_DIR.rglob('ground_truth.csv'))\n",
        "    if any_gt:\n",
        "        gt_head = pd.read_csv(any_gt[0], nrows=5)\n",
        "        print('ground_truth columns:', list(gt_head.columns))\n",
        "        print(gt_head.head(3))\n",
        "    else:\n",
        "        print('No ground_truth.csv found under train/*/*')\n",
        "\n",
        "def load_train_phone_truth(route_dir: Path, phone_dir: Path) -> pd.DataFrame:\n",
        "    gt_csv = phone_dir / 'ground_truth.csv'\n",
        "    if not gt_csv.exists():\n",
        "        return pd.DataFrame()\n",
        "    df = pd.read_csv(gt_csv)\n",
        "    # Expect columns: utcTimeMillis, LatitudeDegrees, LongitudeDegrees (common format)\n",
        "    # Normalize column names if needed\n",
        "    cols = {c.lower(): c for c in df.columns}\n",
        "    # Map possible variants\n",
        "    tcol = 'utcTimeMillis' if 'utctimemillis' in cols else ('UnixTimeMillis' if 'unixtimemillis' in cols else None)\n",
        "    latcol = 'LatitudeDegrees' if 'latitudedegrees' in cols else ('latDeg' if 'latdeg' in cols else None)\n",
        "    loncol = 'LongitudeDegrees' if 'longitudedegrees' in cols else ('lonDeg' if 'londeg' in cols else None)\n",
        "    if tcol is None or latcol is None or loncol is None:\n",
        "        # Try to guess\n",
        "        for c in df.columns:\n",
        "            if 'utc' in c.lower() and 'millis' in c.lower(): tcol = c\n",
        "            if 'lat' in c.lower(): latcol = c\n",
        "            if 'lon' in c.lower(): loncol = c\n",
        "    df = df[[tcol, latcol, loncol]].rename(columns={tcol:'utcTimeMillis', latcol:'LatitudeDegrees', loncol:'LongitudeDegrees'})\n",
        "    df['utcTimeMillis'] = df['utcTimeMillis'].astype(np.int64)\n",
        "    return df\n",
        "\n",
        "def predict_train_phone(route_dir: Path, phone_dir: Path) -> pd.DataFrame:\n",
        "    gnss_csv = phone_dir / 'device_gnss.csv'\n",
        "    if not gnss_csv.exists():\n",
        "        return pd.DataFrame()\n",
        "    # Use measurement timestamps for prediction; evaluation will merge to GT times\n",
        "    df_ecef = load_phone_gnss_positions(gnss_csv)\n",
        "    if len(df_ecef) == 0:\n",
        "        return pd.DataFrame()\n",
        "    lat0, lon0 = anchor_route_latlon(df_ecef)\n",
        "    df_enu = ecef_df_to_enu(df_ecef, lat0, lon0)\n",
        "    Es, Ns = kf_rts_smooth(df_enu['E'].values, df_enu['N'].values, df_enu['t'].values, r_pos_var=36.0, q_acc=2.25, gate_chi2=9.21)\n",
        "    lat, lon = enu_to_latlon_series(Es, Ns, np.zeros_like(Es), lat0, lon0)\n",
        "    pred = pd.DataFrame({'utcTimeMillis': df_enu['t'].values.astype(np.int64), 'LatitudeDegrees': lat, 'LongitudeDegrees': lon})\n",
        "    return pred\n",
        "\n",
        "def score_route_phone(route_dir: Path, phone_dir: Path) -> float:\n",
        "    gt = load_train_phone_truth(route_dir, phone_dir)\n",
        "    if gt.empty:\n",
        "        return np.nan\n",
        "    pred = predict_train_phone(route_dir, phone_dir)\n",
        "    if pred.empty:\n",
        "        return np.nan\n",
        "    # Align by nearest timestamp (since sampling may differ). Use forward fill on merge_asof both directions and average? For now, nearest within 200 ms.\n",
        "    gt_sorted = gt.sort_values('utcTimeMillis')\n",
        "    pred_sorted = pred.sort_values('utcTimeMillis')\n",
        "    m = pd.merge_asof(gt_sorted, pred_sorted, on='utcTimeMillis', direction='nearest', tolerance=pd.Timedelta(milliseconds=200) if False else 200, allow_exact_matches=True)\n",
        "    # Note: pandas merge_asof with integer tolerance uses same units as key; here ms, tolerance=200\n",
        "    m = m.dropna(subset=['LatitudeDegrees_y', 'LongitudeDegrees_y'])\n",
        "    if len(m) == 0:\n",
        "        return np.nan\n",
        "    dist = haversine(m['LatitudeDegrees_y'].values, m['LongitudeDegrees_y'].values, m['LatitudeDegrees_x'].values, m['LongitudeDegrees_x'].values)\n",
        "    return float(np.mean(dist))\n",
        "\n",
        "print('Utils ready. Next: run per-route quick score to sanity-check CV wiring.', flush=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Inspect ground_truth schema and set up CV utils ==\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ground_truth columns: ['MessageType', 'Provider', 'LatitudeDegrees', 'LongitudeDegrees', 'AltitudeMeters', 'SpeedMps', 'AccuracyMeters', 'BearingDegrees', 'UnixTimeMillis']\n  MessageType Provider  LatitudeDegrees  LongitudeDegrees  AltitudeMeters  \\\n0         Fix       GT        37.416619       -122.082065             NaN   \n1         Fix       GT        37.416619       -122.082065             NaN   \n2         Fix       GT        37.416619       -122.082065             NaN   \n\n   SpeedMps  AccuracyMeters  BearingDegrees  UnixTimeMillis  \n0  0.002044             0.1       92.968750   1589573679445  \n1  0.002198             0.1       92.969666   1589573680445  \n2  0.001414             0.1       92.969850   1589573681445  \nUtils ready. Next: run per-route quick score to sanity-check CV wiring.\n"
          ]
        }
      ]
    },
    {
      "id": "2b79aa03-7e2a-439c-8760-599450d8f398",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "print('== Quick train sanity score over a few routes ==', flush=True)\n",
        "t0 = time.time()\n",
        "train_root = Path('train')\n",
        "routes = sorted([p for p in train_root.glob('*') if p.is_dir()])\n",
        "routes = routes[:5]  # limit for quick pass\n",
        "all_scores = []\n",
        "for ri, r in enumerate(routes):\n",
        "    phones = sorted([p for p in r.glob('*') if p.is_dir()])\n",
        "    for pi, ph in enumerate(phones):\n",
        "        st = time.time()\n",
        "        s = score_route_phone(r, ph)\n",
        "        all_scores.append(s)\n",
        "        print(f'[Route {ri}/{len(routes)}] {r.name}/{ph.name}: score={s:.3f} m  (elapsed {time.time()-st:.2f}s)', flush=True)\n",
        "print('Mean score over evaluated pairs:', float(np.nanmean(all_scores)))\n",
        "print('Pairs counted:', int(np.sum(~np.isnan(all_scores))))\n",
        "print('Elapsed total: %.2fs' % (time.time()-t0), flush=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Quick train sanity score over a few routes ==\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Route 0/5] 2020-05-15-US-MTV-1/GooglePixel4XL: score=2.037 m  (elapsed 0.36s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Route 1/5] 2020-05-21-US-MTV-1/GooglePixel4: score=2.038 m  (elapsed 0.24s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Route 1/5] 2020-05-21-US-MTV-1/GooglePixel4XL: score=1.636 m  (elapsed 0.24s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Route 2/5] 2020-05-21-US-MTV-2/GooglePixel4: score=1.175 m  (elapsed 0.22s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Route 2/5] 2020-05-21-US-MTV-2/GooglePixel4XL: score=1.387 m  (elapsed 0.23s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Route 3/5] 2020-05-28-US-MTV-2/GooglePixel4: score=1.369 m  (elapsed 0.25s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Route 3/5] 2020-05-28-US-MTV-2/GooglePixel4XL: score=1.119 m  (elapsed 0.26s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Route 4/5] 2020-05-29-US-MTV-1/GooglePixel4: score=1.863 m  (elapsed 0.22s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Route 4/5] 2020-05-29-US-MTV-1/GooglePixel4XL: score=1.783 m  (elapsed 0.23s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean score over evaluated pairs: 1.600905154309104\nPairs counted: 9\nElapsed total: 2.26s\n"
          ]
        }
      ]
    },
    {
      "id": "fde456ae-ca22-49e2-82fd-71606b788df5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# === Adaptive Rpos, Doppler speed pseudo-measurement, enhanced KF, and multi-phone fusion ===\n",
        "\n",
        "def phone_base_std_from_name(phone_name: str) -> float:\n",
        "    p = phone_name.lower()\n",
        "    if 'pixel4' in p or 'pixel5' in p:\n",
        "        return 6.0\n",
        "    if 's20' in p or 'samsung' in p:\n",
        "        return 8.0\n",
        "    if 'xiaomi' in p or 'mi8' in p:\n",
        "        return 9.0\n",
        "    return 7.0\n",
        "\n",
        "def phone_quality_multiplier(phone_name: str) -> float:\n",
        "    # Multiplier on posterior variance (higher = noisier phone gets down-weighted)\n",
        "    p = phone_name.lower()\n",
        "    if 'pixel' in p:\n",
        "        return 1.0\n",
        "    if 's20' in p or 'samsung' in p:\n",
        "        return 1.3\n",
        "    if 'mi8' in p or 'xiaomi' in p:\n",
        "        return 1.6\n",
        "    return 1.15\n",
        "\n",
        "def load_epoch_stats(gnss_csv: Path) -> pd.DataFrame:\n",
        "    usecols = ['utcTimeMillis','Cn0DbHz','PseudorangeRateUncertaintyMetersPerSecond','RawPseudorangeUncertaintyMeters']\n",
        "    head = pd.read_csv(gnss_csv, nrows=1)\n",
        "    df = pd.read_csv(gnss_csv, usecols=[c for c in usecols if c in head.columns])\n",
        "    if 'utcTimeMillis' not in df.columns:\n",
        "        return pd.DataFrame(columns=['t','ns','mean_cn0','median_raw_unc'])\n",
        "    g = df.groupby('utcTimeMillis')\n",
        "    ns = g.size().rename('ns')\n",
        "    mean_cn0 = (g['Cn0DbHz'].mean() if 'Cn0DbHz' in df.columns else pd.Series(dtype=float))\n",
        "    median_raw_unc = (g['RawPseudorangeUncertaintyMeters'].median() if 'RawPseudorangeUncertaintyMeters' in df.columns else pd.Series(dtype=float))\n",
        "    out = pd.concat([ns, mean_cn0, median_raw_unc], axis=1).reset_index()\n",
        "    out = out.rename(columns={'utcTimeMillis':'t','Cn0DbHz':'mean_cn0','RawPseudorangeUncertaintyMeters':'median_raw_unc'})\n",
        "    return out\n",
        "\n",
        "def compute_adaptive_Rpos_var(stats_df: pd.DataFrame, base_std: float) -> pd.DataFrame:\n",
        "    df = stats_df.copy()\n",
        "    if df.empty:\n",
        "        return df.assign(Rpos_var=(base_std**2))\n",
        "    ns = df['ns'].astype(float).clip(lower=1.0)\n",
        "    mean_cn0 = df['mean_cn0'].astype(float).fillna(20.0).clip(15.0, 35.0)\n",
        "    std = base_std * np.sqrt(8.0/np.clip(ns, 4.0, None)) * (25.0/mean_cn0)\n",
        "    std = np.clip(std, 3.0, 20.0)\n",
        "    if 'median_raw_unc' in df.columns and df['median_raw_unc'].notna().any():\n",
        "        med = df['median_raw_unc'].median() if df['median_raw_unc'].notna().any() else 1.0\n",
        "        scale = df['median_raw_unc'].astype(float).fillna(med)\n",
        "        scale = np.clip(scale / max(np.median(scale.values), 1e-6), 0.7, 2.0)\n",
        "        std = std * scale\n",
        "        std = np.clip(std, 3.0, 20.0)\n",
        "    df['Rpos_var'] = std**2\n",
        "    return df[['t','Rpos_var']].astype({'t':'int64','Rpos_var':'float64'})\n",
        "\n",
        "def finite_diff_speed(E: np.ndarray, N: np.ndarray, t_ms: np.ndarray):\n",
        "    n = len(t_ms)\n",
        "    spd = np.full(n, np.nan, dtype=np.float64)\n",
        "    for k in range(1, n):\n",
        "        dt = max(1e-3, (t_ms[k] - t_ms[k-1]) * 1e-3)\n",
        "        dE = E[k] - E[k-1]; dN = N[k] - N[k-1]\n",
        "        spd[k] = np.hypot(dE, dN) / dt\n",
        "    return spd\n",
        "\n",
        "def _ecef_to_enu_matrix(lat0_deg: float, lon0_deg: float):\n",
        "    lat0 = np.radians(lat0_deg, dtype=np.float64)\n",
        "    lon0 = np.radians(lon0_deg, dtype=np.float64)\n",
        "    slat, clat = np.sin(lat0), np.cos(lat0)\n",
        "    slon, clon = np.sin(lon0), np.cos(lon0)\n",
        "    R = np.array([\n",
        "        [-slon,             clon,              0.0],\n",
        "        [-slat*clon, -slat*slon,  clat],\n",
        "        [ clat*clon,  clat*slon,  slat]\n",
        "    ], dtype=np.float64)\n",
        "    return R  # E,N,U = R @ dX\n",
        "\n",
        "def compute_doppler_speed_wls(gnss_csv: Path, lat0: float, lon0: float) -> pd.DataFrame:\n",
        "    # Returns per-epoch speed magnitude (m/s) and variance from LS, columns: t, speed_mag, R_speed_var\n",
        "    head = pd.read_csv(gnss_csv, nrows=1)\n",
        "    # handle both SvClockDrift column variants\n",
        "    sv_clk_cols = [c for c in ['SvClockDriftMetersPerSecond','SvClockDriftMps'] if c in head.columns]\n",
        "    cols = [\n",
        "        'utcTimeMillis',\n",
        "        'SvPositionXEcefMeters','SvPositionYEcefMeters','SvPositionZEcefMeters',\n",
        "        'SvVelocityXEcefMetersPerSecond','SvVelocityYEcefMetersPerSecond','SvVelocityZEcefMetersPerSecond',\n",
        "        'PseudorangeRateMetersPerSecond','PseudorangeRateUncertaintyMetersPerSecond',\n",
        "        'Cn0DbHz',\n",
        "        'WlsPositionXEcefMeters','WlsPositionYEcefMeters','WlsPositionZEcefMeters'\n",
        "] + sv_clk_cols\n",
        "    use = [c for c in cols if c in head.columns]\n",
        "    if 'utcTimeMillis' not in use or 'PseudorangeRateMetersPerSecond' not in use:\n",
        "        return pd.DataFrame(columns=['t','speed_mag','R_speed_var'])\n",
        "    df = pd.read_csv(gnss_csv, usecols=use)\n",
        "    df = df.dropna(subset=['PseudorangeRateMetersPerSecond'])\n",
        "    df['t'] = df['utcTimeMillis'].astype(np.int64)\n",
        "    g = df.groupby('t', sort=True)\n",
        "    Rmat = _ecef_to_enu_matrix(lat0, lon0)\n",
        "    rows = []\n",
        "    for t, d in g:\n",
        "        if {'WlsPositionXEcefMeters','WlsPositionYEcefMeters','WlsPositionZEcefMeters'}.issubset(d.columns):\n",
        "            rxX = float(d['WlsPositionXEcefMeters'].median()) if d['WlsPositionXEcefMeters'].notna().any() else np.nan\n",
        "            rxY = float(d['WlsPositionYEcefMeters'].median()) if d['WlsPositionYEcefMeters'].notna().any() else np.nan\n",
        "            rxZ = float(d['WlsPositionZEcefMeters'].median()) if d['WlsPositionZEcefMeters'].notna().any() else np.nan\n",
        "        else:\n",
        "            rxX = rxY = rxZ = np.nan\n",
        "        if not np.isfinite(rxX):\n",
        "            continue\n",
        "        req_cols = ['SvPositionXEcefMeters','SvPositionYEcefMeters','SvPositionZEcefMeters',\n",
        "                    'SvVelocityXEcefMetersPerSecond','SvVelocityYEcefMetersPerSecond','SvVelocityZEcefMetersPerSecond',\n",
        "                    'PseudorangeRateMetersPerSecond']\n",
        "        if not set(req_cols).issubset(d.columns):\n",
        "            continue\n",
        "        Xs = d['SvPositionXEcefMeters'].values.astype(np.float64)\n",
        "        Ys = d['SvPositionYEcefMeters'].values.astype(np.float64)\n",
        "        Zs = d['SvPositionZEcefMeters'].values.astype(np.float64)\n",
        "        Vx = d['SvVelocityXEcefMetersPerSecond'].values.astype(np.float64)\n",
        "        Vy = d['SvVelocityYEcefMetersPerSecond'].values.astype(np.float64)\n",
        "        Vz = d['SvVelocityZEcefMetersPerSecond'].values.astype(np.float64)\n",
        "        pdot = d['PseudorangeRateMetersPerSecond'].values.astype(np.float64)\n",
        "        m = len(pdot)\n",
        "        if m < 6:\n",
        "            continue\n",
        "        dX = Xs - rxX; dY = Ys - rxY; dZ = Zs - rxZ\n",
        "        rng = np.sqrt(dX*dX + dY*dY + dZ*dZ) + 1e-9\n",
        "        ux = dX / rng; uy = dY / rng; uz = dZ / rng\n",
        "        A = np.column_stack([ux, uy, uz, -np.ones(m, dtype=np.float64)])\n",
        "        vs_proj = ux*Vx + uy*Vy + uz*Vz\n",
        "        if 'SvClockDriftMetersPerSecond' in d.columns:\n",
        "            sv_clk = d['SvClockDriftMetersPerSecond'].values.astype(np.float64)\n",
        "        elif 'SvClockDriftMps' in d.columns:\n",
        "            sv_clk = d['SvClockDriftMps'].values.astype(np.float64)\n",
        "        else:\n",
        "            sv_clk = np.zeros(m, dtype=np.float64)\n",
        "        b = vs_proj - pdot - sv_clk\n",
        "        sig = d['PseudorangeRateUncertaintyMetersPerSecond'].values.astype(np.float64) if 'PseudorangeRateUncertaintyMetersPerSecond' in d.columns else np.full(m, 1.0, dtype=np.float64)\n",
        "        sig = np.clip(sig, 0.1, 10.0)\n",
        "        w = 1.0 / (sig*sig)\n",
        "        if 'Cn0DbHz' in d.columns:\n",
        "            cn0 = np.clip(d['Cn0DbHz'].values.astype(np.float64), 15.0, 35.0)\n",
        "            w = w * ( (cn0/25.0)**2 )\n",
        "        Wsqrt = np.sqrt(w)\n",
        "        Aw = A * Wsqrt[:,None]; bw = b * Wsqrt\n",
        "        ATA = Aw.T @ Aw\n",
        "        ATb = Aw.T @ bw\n",
        "        try:\n",
        "            cond = np.linalg.cond(ATA)\n",
        "        except np.linalg.LinAlgError:\n",
        "            continue\n",
        "        if not np.isfinite(cond) or cond > 1e8:\n",
        "            continue\n",
        "        try:\n",
        "            theta = np.linalg.solve(ATA, ATb)\n",
        "            Cov = np.linalg.inv(ATA)\n",
        "        except np.linalg.LinAlgError:\n",
        "            continue\n",
        "        v_rcv_ecef = theta[:3]\n",
        "        v_enu = Rmat @ v_rcv_ecef\n",
        "        vE, vN = float(v_enu[0]), float(v_enu[1])\n",
        "        vnorm = float(np.hypot(vE, vN))\n",
        "        Cov_rcv = Cov[:3,:3]\n",
        "        Cov_enu = Rmat @ Cov_rcv @ Rmat.T\n",
        "        if vnorm > 1e-6:\n",
        "            u_t = np.array([vE/vnorm, vN/vnorm, 0.0], dtype=np.float64)\n",
        "            var_t = float(u_t.T @ Cov_enu @ u_t)\n",
        "        else:\n",
        "            var_t = 0.5*(Cov_enu[0,0] + Cov_enu[1,1])\n",
        "        var_t = float(np.clip(var_t, 0.25, 2.25))\n",
        "        rows.append((int(t), vnorm, var_t))\n",
        "    if not rows:\n",
        "        return pd.DataFrame(columns=['t','speed_mag','R_speed_var'])\n",
        "    out = pd.DataFrame(rows, columns=['t','speed_mag','R_speed_var']).sort_values('t')\n",
        "    return out\n",
        "\n",
        "def kf_rts_smooth_adaptive(E: np.ndarray, N: np.ndarray, t_ms: np.ndarray,\n",
        "                           Rpos_vars: np.ndarray,\n",
        "                           speed_mag: np.ndarray | None = None,\n",
        "                           R_speed_vars: np.ndarray | float | None = None,\n",
        "                           q_acc: float = 2.0,\n",
        "                           gate_pos_chi2: float = 9.21,\n",
        "                           gate_spd_chi2: float = 6.63):\n",
        "    n = len(t_ms)\n",
        "    if n == 0:\n",
        "        return np.array([]), np.array([]), np.array([]), np.zeros((0,), dtype=np.float64)\n",
        "    x = np.zeros((n,4), dtype=np.float64)\n",
        "    P = np.zeros((n,4,4), dtype=np.float64)\n",
        "    Fm = np.zeros((n,4,4), dtype=np.float64)\n",
        "    Qm = np.zeros((n,4,4), dtype=np.float64)\n",
        "    x[0] = np.array([E[0], N[0], 0.0, 0.0], dtype=np.float64)\n",
        "    P[0] = np.diag([Rpos_vars[0], Rpos_vars[0], 25.0, 25.0])\n",
        "    Hpos = np.array([[1,0,0,0],[0,1,0,0]], dtype=np.float64)\n",
        "    for k in range(1, n):\n",
        "        dt = max(1e-3, (t_ms[k] - t_ms[k-1]) * 1e-3)\n",
        "        if (t_ms[k] - t_ms[k-1]) > 1500:\n",
        "            x[k-1,2:] = 0.0\n",
        "            P[k-1] += np.diag([100.0, 100.0, 100.0, 100.0])\n",
        "        F = np.array([[1,0,dt,0],[0,1,0,dt],[0,0,1,0],[0,0,0,1]], dtype=np.float64)\n",
        "        dt2, dt3, dt4 = dt*dt, dt*dt*dt, (dt*dt)*(dt*dt)\n",
        "        Q = q_acc * np.array([[dt4/4,0,dt3/2,0],[0,dt4/4,0,dt3/2],[dt3/2,0,dt2,0],[0,dt3/2,0,dt2]], dtype=np.float64)\n",
        "        x_pred = F @ x[k-1]\n",
        "        P_pred = F @ P[k-1] @ F.T + Q\n",
        "        z = np.array([E[k], N[k]], dtype=np.float64)\n",
        "        y = z - (Hpos @ x_pred)\n",
        "        Rpos = np.diag([Rpos_vars[k], Rpos_vars[k]])\n",
        "        S = Hpos @ P_pred @ Hpos.T + Rpos\n",
        "        try:\n",
        "            Sinv = np.linalg.inv(S)\n",
        "        except np.linalg.LinAlgError:\n",
        "            Sinv = np.linalg.pinv(S)\n",
        "        maha2 = float(y.T @ Sinv @ y)\n",
        "        if maha2 <= gate_pos_chi2:\n",
        "            K = P_pred @ Hpos.T @ Sinv\n",
        "            x_upd = x_pred + K @ y\n",
        "            P_upd = (np.eye(4) - K @ Hpos) @ P_pred\n",
        "        else:\n",
        "            x_upd, P_upd = x_pred, P_pred\n",
        "        if speed_mag is not None and np.isfinite(speed_mag[k]):\n",
        "            vE, vN = x_upd[2], x_upd[3]\n",
        "            vnorm = float(np.hypot(vE, vN))\n",
        "            if vnorm > 0.2:\n",
        "                h = vnorm\n",
        "                Hs = np.array([0.0, 0.0, vE/max(vnorm,1e-9), vN/max(vnorm,1e-9)], dtype=np.float64).reshape(1,4)\n",
        "                s_mat = Hs @ P_upd @ Hs.T\n",
        "                Rsv = None\n",
        "                if isinstance(R_speed_vars, np.ndarray):\n",
        "                    Rsv = R_speed_vars[k] if k < len(R_speed_vars) and np.isfinite(R_speed_vars[k]) else None\n",
        "                elif isinstance(R_speed_vars, (float, int)):\n",
        "                    Rsv = float(R_speed_vars)\n",
        "                if Rsv is None:\n",
        "                    Rsv = 2.25\n",
        "                s = float(s_mat[0,0]) + Rsv\n",
        "                if s <= 0: s = Rsv\n",
        "                innov = float(speed_mag[k] - h)\n",
        "                maha2_s = (innov*innov)/s\n",
        "                if maha2_s <= gate_spd_chi2:\n",
        "                    K_s = (P_upd @ Hs.T) / s\n",
        "                    x_upd = x_upd + (K_s.flatten() * innov)\n",
        "                    P_upd = P_upd - (K_s @ (Hs @ P_upd))\n",
        "        x[k] = x_upd; P[k] = P_upd; Fm[k] = F; Qm[k] = Q\n",
        "    xs = x.copy(); Ps = P.copy()\n",
        "    for k in range(n-2, -1, -1):\n",
        "        F = Fm[k+1]; Pk = P[k]; P_pred = F @ Pk @ F.T + Qm[k+1]\n",
        "        try: Ck = Pk @ F.T @ np.linalg.inv(P_pred)\n",
        "        except np.linalg.LinAlgError: Ck = Pk @ F.T @ np.linalg.pinv(P_pred)\n",
        "        xs[k] = x[k] + Ck @ (xs[k+1] - (F @ x[k]))\n",
        "        Ps[k] = Pk + Ck @ (Ps[k+1] - P_pred) @ Ck.T\n",
        "    vnorm_s = np.hypot(xs[:,2], xs[:,3])\n",
        "    Rpost_var = 0.5 * (Ps[:,0,0] + Ps[:,1,1])\n",
        "    return xs[:,0], xs[:,1], vnorm_s, Rpost_var\n",
        "\n",
        "def build_route_anchor_from_all_phones(route_dir: Path) -> tuple[float,float]:\n",
        "    ecef_parts = []\n",
        "    for ph in sorted([p for p in route_dir.glob('*') if p.is_dir()]):\n",
        "        gnss = ph / 'device_gnss.csv'\n",
        "        if gnss.exists():\n",
        "            df = load_phone_gnss_positions(gnss)\n",
        "            if len(df): ecef_parts.append(df[['X','Y','Z']])\n",
        "    if not ecef_parts:\n",
        "        for ph in sorted([p for p in route_dir.glob('*') if p.is_dir()]):\n",
        "            gnss = ph / 'device_gnss.csv'\n",
        "            if gnss.exists():\n",
        "                df = load_phone_gnss_positions(gnss)\n",
        "                if len(df): return anchor_route_latlon(df)\n",
        "        return 0.0, 0.0\n",
        "    all_ecef = pd.concat(ecef_parts, ignore_index=True)\n",
        "    return anchor_route_latlon(all_ecef)\n",
        "\n",
        "def run_phone_kf_enhanced(gnss_csv: Path, lat0: float, lon0: float, sample_times: np.ndarray, base_std: float, time_offset_ms: int = 0):\n",
        "    df_ecef = load_phone_gnss_positions(gnss_csv)\n",
        "    if len(df_ecef) == 0:\n",
        "        return pd.DataFrame({'UnixTimeMillis': sample_times, 'E': np.nan, 'N': np.nan, 'Rpost_var': np.nan})\n",
        "    if time_offset_ms != 0:\n",
        "        df_ecef = df_ecef.copy()\n",
        "        df_ecef['t'] = (df_ecef['t'].astype(np.int64) + int(time_offset_ms)).astype(np.int64)\n",
        "    df_stats = compute_adaptive_Rpos_var(load_epoch_stats(gnss_csv), base_std)\n",
        "    if time_offset_ms != 0 and not df_stats.empty:\n",
        "        df_stats = df_stats.copy()\n",
        "        df_stats['t'] = (df_stats['t'].astype(np.int64) + int(time_offset_ms)).astype(np.int64)\n",
        "    df = df_ecef.merge(df_stats, left_on='t', right_on='t', how='left')\n",
        "    df['Rpos_var'] = df['Rpos_var'].fillna(base_std**2)\n",
        "    # Load clock discontinuity if present and align\n",
        "    disc = None\n",
        "    head = pd.read_csv(gnss_csv, nrows=1)\n",
        "    if 'HardwareClockDiscontinuityCount' in head.columns:\n",
        "        df_disc = pd.read_csv(gnss_csv, usecols=['utcTimeMillis','HardwareClockDiscontinuityCount'])\n",
        "        df_disc = df_disc.groupby('utcTimeMillis')['HardwareClockDiscontinuityCount'].max().reset_index()\n",
        "        df_disc['t'] = df_disc['utcTimeMillis'].astype(np.int64)\n",
        "        if time_offset_ms != 0:\n",
        "            df_disc['t'] = (df_disc['t'].astype(np.int64) + int(time_offset_ms)).astype(np.int64)\n",
        "        disc = df.merge(df_disc[['t','HardwareClockDiscontinuityCount']], on='t', how='left')['HardwareClockDiscontinuityCount'].astype('float64').values\n",
        "    df_enu = ecef_df_to_enu(df, lat0, lon0)\n",
        "    E = df_enu['E'].values; N = df_enu['N'].values; t = df_enu['t'].values.astype(np.int64)\n",
        "    Rpos_vars = df_enu['Rpos_var'].values.astype(np.float64)\n",
        "    dop = compute_doppler_speed_wls(gnss_csv, lat0, lon0)\n",
        "    if time_offset_ms != 0 and not dop.empty:\n",
        "        dop = dop.copy()\n",
        "        dop['t'] = (dop['t'].astype(np.int64) + int(time_offset_ms)).astype(np.int64)\n",
        "    spd = np.full_like(t, np.nan, dtype=np.float64)\n",
        "    Rspd = std_rspd = np.full_like(t, np.nan, dtype=np.float64)\n",
        "    if not dop.empty:\n",
        "        m = dop.merge(pd.DataFrame({'t': t}), on='t', how='right')\n",
        "        spd = m['speed_mag'].values.astype(np.float64)\n",
        "        Rspd = m['R_speed_var'].values.astype(np.float64)\n",
        "    spd_fd = finite_diff_speed(E, N, t)\n",
        "    use_fd = (~np.isfinite(spd)) & (spd_fd > 0.3)\n",
        "    spd = np.where(use_fd, spd_fd, spd)\n",
        "    Rspd = np.where(use_fd, (1.5**2), Rspd)\n",
        "    # Segment indices: by clock discontinuity or big dt\n",
        "    idx_starts = [0]\n",
        "    for k in range(1, len(t)):\n",
        "        gap = (t[k] - t[k-1]) > 1500\n",
        "        disc_break = False\n",
        "        if disc is not None:\n",
        "            prev = disc[k-1] if np.isfinite(disc[k-1]) else 0.0\n",
        "            cur = disc[k] if np.isfinite(disc[k]) else prev\n",
        "            disc_break = (cur > prev)\n",
        "        if gap or disc_break:\n",
        "            idx_starts.append(k)\n",
        "    idx_starts = sorted(set(idx_starts))\n",
        "    idx_ends = idx_starts[1:] + [len(t)]\n",
        "    Es_list, Ns_list, Rp_list = [], [], []\n",
        "    for s, e in zip(idx_starts, idx_ends):\n",
        "        Ee, Ne, Ve, Rp = kf_rts_smooth_adaptive(E[s:e], N[s:e], t[s:e], Rpos_vars[s:e], speed_mag=spd[s:e], R_speed_vars=Rspd[s:e], q_acc=2.0)\n",
        "        Es_list.append(Ee); Ns_list.append(Ne); Rp_list.append(Rp)\n",
        "    Es = np.concatenate(Es_list) if Es_list else np.array([], dtype=np.float64)\n",
        "    Ns = np.concatenate(Ns_list) if Ns_list else np.array([], dtype=np.float64)\n",
        "    Rpost_var = np.concatenate(Rp_list) if Rp_list else np.array([], dtype=np.float64)\n",
        "    def interp_nearest(x, xp, fp):\n",
        "        y = np.interp(x, xp, fp)\n",
        "        y[x < xp[0]] = fp[0]; y[x > xp[-1]] = fp[-1]\n",
        "        return y\n",
        "    ts = sample_times.astype(np.int64)\n",
        "    uniq = np.concatenate([[True], t[1:] != t[:-1]])\n",
        "    t_u = t[uniq]; Es_u = Es[uniq]; Ns_u = Ns[uniq]; Rpost_u = Rpost_var[uniq]\n",
        "    E_q = interp_nearest(ts, t_u, Es_u); N_q = interp_nearest(ts, t_u, Ns_u); Rpost_q = interp_nearest(ts, t_u, Rpost_u)\n",
        "    return pd.DataFrame({'UnixTimeMillis': ts, 'E': E_q, 'N': N_q, 'Rpost_var': Rpost_q})\n",
        "\n",
        "def _nearest_within(ts_target: np.ndarray, ts_src: np.ndarray, vals: np.ndarray, max_dt_ms: int = 200):\n",
        "    idx = np.searchsorted(ts_src, ts_target)\n",
        "    idx0 = np.clip(idx-1, 0, len(ts_src)-1)\n",
        "    idx1 = np.clip(idx, 0, len(ts_src)-1)\n",
        "    dt0 = np.abs(ts_target - ts_src[idx0])\n",
        "    dt1 = np.abs(ts_target - ts_src[idx1])\n",
        "    choose1 = dt1 < dt0\n",
        "    chosen_idx = np.where(choose1, idx1, idx0)\n",
        "    chosen_dt = np.where(choose1, dt1, dt0)\n",
        "    out = vals[chosen_idx].astype(np.float64).copy()\n",
        "    out[chosen_dt > max_dt_ms] = np.nan\n",
        "    return out, chosen_dt\n",
        "\n",
        "def fuse_phones_enu_union(df_list: list[pd.DataFrame], target_ts: np.ndarray, drop_thresh_m1: float = 12.0, drop_thresh_m2: float = 8.0, phone_names: list[str] | None = None, phone_multipliers: np.ndarray | None = None):\n",
        "    if not df_list:\n",
        "        return None\n",
        "    T = len(target_ts)\n",
        "    P = len(df_list)\n",
        "    E_all = np.full((P, T), np.nan, dtype=np.float64)\n",
        "    N_all = np.full((P, T), np.nan, dtype=np.float64)\n",
        "    R_all = np.full((P, T), np.nan, dtype=np.float64)\n",
        "    W_time = np.ones((P, T), dtype=np.float64)\n",
        "    qual = np.ones(P, dtype=np.float64)\n",
        "    if phone_multipliers is not None:\n",
        "        qual = np.asarray(phone_multipliers, dtype=np.float64)\n",
        "    elif phone_names is not None:\n",
        "        for i, name in enumerate(phone_names):\n",
        "            qual[i] = phone_quality_multiplier(name)\n",
        "    for i, df in enumerate(df_list):\n",
        "        ts = df['UnixTimeMillis'].values.astype(np.int64)\n",
        "        mask = np.concatenate([[True], ts[1:] != ts[:-1]])\n",
        "        ts = ts[mask]\n",
        "        E = df['E'].values[mask]; N = df['N'].values[mask]; R = df['Rpost_var'].values[mask] * (qual[i]**2)\n",
        "        Ei, dtE = _nearest_within(target_ts, ts, E, max_dt_ms=200)\n",
        "        Ni, dtN = _nearest_within(target_ts, ts, N, max_dt_ms=200)\n",
        "        Ri, _ = _nearest_within(target_ts, ts, R, max_dt_ms=200)\n",
        "        dt = np.maximum(dtE, dtN)\n",
        "        w_time = np.exp(- (dt/150.0)**2)\n",
        "        E_all[i] = Ei; N_all[i] = Ni; R_all[i] = Ri; W_time[i] = w_time\n",
        "    # Robust per-epoch fusion with guarded culling and fallbacks\n",
        "    R_all = np.clip(R_all, 12.0, 400.0)\n",
        "    with np.errstate(all='ignore'):\n",
        "        Emed = np.nanmedian(E_all, axis=0)\n",
        "        Nmed = np.nanmedian(N_all, axis=0)\n",
        "    Ef = np.full(T, np.nan, dtype=np.float64)\n",
        "    Nf = np.full(T, np.nan, dtype=np.float64)\n",
        "    Rf = np.full(T, 25.0, dtype=np.float64)\n",
        "    for t in range(T):\n",
        "        valid = np.isfinite(E_all[:,t]) & np.isfinite(N_all[:,t]) & np.isfinite(R_all[:,t])\n",
        "        n = int(valid.sum())\n",
        "        if n == 0:\n",
        "            continue\n",
        "        if n == 1:\n",
        "            i = np.where(valid)[0][0]\n",
        "            Ef[t] = E_all[i,t]; Nf[t] = N_all[i,t]\n",
        "            Rf[t] = float(np.clip(R_all[i,t], 12.0, 25.0)) * 1.2\n",
        "            continue\n",
        "        # n >= 2\n",
        "        if n >= 3:\n",
        "            d1 = np.sqrt((E_all[:,t]-Emed[t])**2 + (N_all[:,t]-Nmed[t])**2)\n",
        "            ok1 = valid & (d1 <= drop_thresh_m1)  # 12 m\n",
        "            if ok1.sum() < 2:\n",
        "                ok_final = valid\n",
        "            else:\n",
        "                with np.errstate(all='ignore'):\n",
        "                    Em2 = np.nanmedian(np.where(ok1, E_all[:,t], np.nan))\n",
        "                    Nm2 = np.nanmedian(np.where(ok1, N_all[:,t], np.nan))\n",
        "                d2 = np.sqrt((E_all[:,t]-Em2)**2 + (N_all[:,t]-Nm2)**2)\n",
        "                ok2 = ok1 & (d2 <= drop_thresh_m2)  # 8 m\n",
        "                ok_final = ok2 if ok2.sum() >= 2 else ok1\n",
        "        else:\n",
        "            ok_final = valid  # exactly 2 phones -> no cull\n",
        "        w_t = (1.0/np.clip(R_all[:,t], 12.0, None)) * W_time[:,t]\n",
        "        w_t[~ok_final] = 0.0\n",
        "        ws = float(np.nansum(w_t))\n",
        "        if ws > 0:\n",
        "            Ef[t] = float(np.nansum(w_t * E_all[:,t]) / ws)\n",
        "            Nf[t] = float(np.nansum(w_t * N_all[:,t]) / ws)\n",
        "            Rf[t] = 1.0 / ws\n",
        "        else:\n",
        "            finite_mask = valid\n",
        "            if finite_mask.sum() >= 2:\n",
        "                with np.errstate(all='ignore'):\n",
        "                    Ef[t] = float(np.nanmedian(E_all[finite_mask, t]))\n",
        "                    Nf[t] = float(np.nanmedian(N_all[finite_mask, t]))\n",
        "                Rf[t] = 25.0\n",
        "            # else leave NaN to be carried\n",
        "    # Bidirectional carry-forward/backward fill with longer horizon to avoid NaN pockets\n",
        "    carry_forward_horizon = 20  # ~2 seconds at 10 Hz\n",
        "    # forward pass\n",
        "    last_ok = -1; carry_used = 0\n",
        "    for t in range(T):\n",
        "        if np.isfinite(Ef[t]) and np.isfinite(Nf[t]):\n",
        "            last_ok = t; carry_used = 0; continue\n",
        "        if last_ok >= 0 and carry_used < carry_forward_horizon:\n",
        "            Ef[t] = Ef[last_ok]; Nf[t] = Nf[last_ok]; Rf[t] = 25.0\n",
        "            carry_used += 1\n",
        "    # backward pass\n",
        "    next_ok = -1; carry_used = 0\n",
        "    for t in range(T-1, -1, -1):\n",
        "        if np.isfinite(Ef[t]) and np.isfinite(Nf[t]):\n",
        "            next_ok = t; carry_used = 0; continue\n",
        "        if next_ok >= 0 and carry_used < carry_forward_horizon:\n",
        "            Ef[t] = Ef[next_ok]; Nf[t] = Nf[next_ok]; Rf[t] = 25.0\n",
        "            carry_used += 1\n",
        "    # final fallback to per-epoch medians if any remain NaN\n",
        "    for t in range(T):\n",
        "        if not (np.isfinite(Ef[t]) and np.isfinite(Nf[t])):\n",
        "            Ef[t] = Emed[t] if np.isfinite(Emed[t]) else 0.0\n",
        "            Nf[t] = Nmed[t] if np.isfinite(Nmed[t]) else 0.0\n",
        "            Rf[t] = 25.0\n",
        "    return pd.DataFrame({'UnixTimeMillis': target_ts.astype(np.int64), 'E': Ef, 'N': Nf, 'Rpost_var': Rf})\n",
        "\n",
        "# === Time-offset alignment via Doppler speed cross-correlation (V4) ===\n",
        "def _savgol(arr: np.ndarray, window: int = 11, poly: int = 2) -> np.ndarray:\n",
        "    try:\n",
        "        from scipy.signal import savgol_filter\n",
        "        w = window if len(arr) >= window else (len(arr)//2*2+1 if len(arr) >= 3 else len(arr))\n",
        "        return savgol_filter(arr, window_length=w, polyorder=min(poly, max(0, w-1)), mode='interp')\n",
        "    except Exception:\n",
        "        if len(arr) < 3:\n",
        "            return arr\n",
        "        w = min(max(3, window), max(3, (len(arr)//2)*2+1))\n",
        "        k = w//2\n",
        "        pad = np.pad(arr, (k,k), mode='edge')\n",
        "        kern = np.ones(w, dtype=np.float64)/w\n",
        "        y = np.convolve(pad, kern, mode='valid')\n",
        "        return y\n",
        "\n",
        "def _resample_speed_to_grid(t: np.ndarray, v: np.ndarray, grid: np.ndarray) -> np.ndarray:\n",
        "    # Linear interp to grid; set NaN where original gaps >1.5s are crossed\n",
        "    mask = np.isfinite(v)\n",
        "    if mask.sum() < 2:\n",
        "        return np.full_like(grid, np.nan, dtype=np.float64)\n",
        "    t_valid = t[mask].astype(np.int64)\n",
        "    v_valid = v[mask].astype(np.float64)\n",
        "    vi = np.interp(grid, t_valid, v_valid)\n",
        "    # detect gaps\n",
        "    gaps = np.where(np.diff(t_valid) > 1500)[0]\n",
        "    if len(gaps) > 0:\n",
        "        for g in gaps:\n",
        "            t0 = t_valid[g]; t1 = t_valid[g+1]\n",
        "            bad = (grid > t0) & (grid < t1)\n",
        "            vi[bad] = np.nan\n",
        "    # outside range -> NaN\n",
        "    vi[grid < t_valid[0]] = np.nan\n",
        "    vi[grid > t_valid[-1]] = np.nan\n",
        "    return vi\n",
        "\n",
        "def _pearson_corr(x: np.ndarray, y: np.ndarray) -> float:\n",
        "    m = np.isfinite(x) & np.isfinite(y)\n",
        "    if m.sum() < 10:\n",
        "        return np.nan\n",
        "    xx = x[m]; yy = y[m]\n",
        "    sx = np.std(xx)\n",
        "    sy = np.std(yy)\n",
        "    if sx < 1e-3 or sy < 1e-3:\n",
        "        return np.nan\n",
        "    xx = (xx - xx.mean())/sx\n",
        "    yy = (yy - yy.mean())/sy\n",
        "    return float(np.dot(xx, yy) / max(1e-9, (len(xx))))\n",
        "\n",
        "def _parabolic_refine(lags_ms: np.ndarray, cors: np.ndarray, best_idx: int) -> float:\n",
        "    i = best_idx\n",
        "    if i <= 0 or i >= len(cors)-1:\n",
        "        return float(lags_ms[i])\n",
        "    x1, x2, x3 = lags_ms[i-1], lags_ms[i], lags_ms[i+1]\n",
        "    y1, y2, y3 = cors[i-1], cors[i], cors[i+1]\n",
        "    denom = (x1 - x2)*(x1 - x3)*(x2 - x3)\n",
        "    if abs(denom) < 1e-9:\n",
        "        return float(lags_ms[i])\n",
        "    A = (x3*(y2 - y1) + x2*(y1 - y3) + x1*(y3 - y2)) / denom\n",
        "    B = (x3*x3*(y1 - y2) + x2*x2*(y3 - y1) + x1*x1*(y2 - y3)) / denom\n",
        "    if A == 0:\n",
        "        return float(lags_ms[i])\n",
        "    xv = -B / (2*A)\n",
        "    return float(np.clip(xv, lags_ms[i-1], lags_ms[i+1]))\n",
        "\n",
        "def _get_disc_series(gnss_csv: Path) -> pd.DataFrame:\n",
        "    head = pd.read_csv(gnss_csv, nrows=1)\n",
        "    if 'HardwareClockDiscontinuityCount' not in head.columns:\n",
        "        return pd.DataFrame(columns=['t','disc'])\n",
        "    df = pd.read_csv(gnss_csv, usecols=['utcTimeMillis','HardwareClockDiscontinuityCount'])\n",
        "    df = df.groupby('utcTimeMillis')['HardwareClockDiscontinuityCount'].max().reset_index()\n",
        "    df = df.rename(columns={'utcTimeMillis':'t', 'HardwareClockDiscontinuityCount':'disc'})\n",
        "    df['t'] = df['t'].astype(np.int64)\n",
        "    return df[['t','disc']].sort_values('t')\n",
        "\n",
        "def compute_time_offsets(route_dir: Path, lat0: float, lon0: float, use_phones: list[str]) -> tuple[dict, dict]:\n",
        "    # Returns: lag_ms per phone (int), weak_alignment flag per phone (bool)\n",
        "    # 1) build per-phone speed series\n",
        "    phone_speeds = {}  # phone -> DataFrame{t, speed}\n",
        "    phone_cn0_med = {}\n",
        "    phone_disc = {}\n",
        "    t_min, t_max = None, None\n",
        "    for phone in use_phones:\n",
        "        gnss_csv = route_dir / phone / 'device_gnss.csv'\n",
        "        if not gnss_csv.exists():\n",
        "            continue\n",
        "        # Doppler speed\n",
        "        dop = compute_doppler_speed_wls(gnss_csv, lat0, lon0)\n",
        "        # Fallback FD on ENU\n",
        "        df_ecef = load_phone_gnss_positions(gnss_csv)\n",
        "        df_enu = ecef_df_to_enu(df_ecef, lat0, lon0)\n",
        "        spd_fd = finite_diff_speed(df_enu['E'].values, df_enu['N'].values, df_enu['t'].values.astype(np.int64))\n",
        "        df_fd = pd.DataFrame({'t': df_enu['t'].values.astype(np.int64), 'fd': spd_fd})\n",
        "        df = pd.DataFrame({'t': df_ecef['t'].values.astype(np.int64)}).drop_duplicates()\n",
        "        if not dop.empty:\n",
        "            df = df.merge(dop[['t','speed_mag']], on='t', how='left')\n",
        "        else:\n",
        "            df['speed_mag'] = np.nan\n",
        "        df = df.merge(df_fd, on='t', how='left')\n",
        "        use_fd = (~np.isfinite(df['speed_mag'].values)) & (df['fd'].values > 0.3)\n",
        "        speed = np.where(use_fd, df['fd'].values, df['speed_mag'].values)\n",
        "        s = pd.DataFrame({'t': df['t'].astype(np.int64), 'speed': speed})\n",
        "        phone_speeds[phone] = s.dropna().sort_values('t')\n",
        "        # cn0 median\n",
        "        st = load_epoch_stats(gnss_csv)\n",
        "        phone_cn0_med[phone] = float(np.nanmedian(st['mean_cn0'].values)) if not st.empty else 20.0\n",
        "        # discontinuities\n",
        "        phone_disc[phone] = _get_disc_series(gnss_csv)\n",
        "        t0 = int(s['t'].min()) if len(s) else None\n",
        "        t1 = int(s['t'].max()) if len(s) else None\n",
        "        if t0 is not None:\n",
        "            t_min = t0 if t_min is None else min(t_min, t0)\n",
        "        if t1 is not None:\n",
        "            t_max = t1 if t_max is None else max(t_max, t1)\n",
        "    if t_min is None or t_max is None or (t_max - t_min) < 120000:\n",
        "        # short route: skip alignment\n",
        "        return {p: 0 for p in use_phones}, {p: True for p in use_phones}\n",
        "    # 2) resample to 10 Hz grid\n",
        "    grid = np.arange(t_min, t_max+1, 100, dtype=np.int64)\n",
        "    resampled = {}\n",
        "    for phone, df in phone_speeds.items():\n",
        "        v = _resample_speed_to_grid(df['t'].values.astype(np.int64), df['speed'].values.astype(np.float64), grid)\n",
        "        v = np.clip(v, 0.0, 50.0)\n",
        "        v = _savgol(v, window=11, poly=2)\n",
        "        resampled[phone] = v\n",
        "    # 3) pick reference phone\n",
        "    pixel_candidates = [p for p in use_phones if 'pixel' in p.lower()]\n",
        "    ref = None\n",
        "    if pixel_candidates:\n",
        "        # choose Pixel with highest median Cn0\n",
        "        ref = max(pixel_candidates, key=lambda p: phone_cn0_med.get(p, 0.0))\n",
        "    else:\n",
        "        ref = max(use_phones, key=lambda p: phone_cn0_med.get(p, 0.0))\n",
        "    # 4) windowed cross-correlation\n",
        "    win = 600  # 60s at 10 Hz\n",
        "    hop = 300  # 30s\n",
        "    lags_ms = np.arange(-500, 501, 10, dtype=np.int64)\n",
        "    lags = lags_ms  # ms\n",
        "    lags_idx = (lags_ms // 100).astype(int)  # coarse index steps for 100ms grid\n",
        "    ref_v = resampled.get(ref, None)\n",
        "    if ref_v is None:\n",
        "        return {p: 0 for p in use_phones}, {p: True for p in use_phones}\n",
        "    disc_ref = phone_disc.get(ref, pd.DataFrame(columns=['t','disc']))\n",
        "    # Build discontinuity indices on grid\n",
        "    def grid_disc_indices(disc_df: pd.DataFrame):\n",
        "        if disc_df is None or disc_df.empty:\n",
        "            return set()\n",
        "        t_disc = disc_df.dropna().sort_values('t')\n",
        "        jumps = t_disc['disc'].diff().fillna(0) > 0\n",
        "        t_jump = t_disc.loc[jumps, 't'].values.astype(np.int64)\n",
        "        idx_set = set(np.searchsorted(grid, t_jump))\n",
        "        return idx_set\n",
        "    ref_disc_idx = grid_disc_indices(disc_ref)\n",
        "    lag_result = {}; weak = {}\n",
        "    for phone in use_phones:\n",
        "        if phone == ref:\n",
        "            lag_result[phone] = 0; weak[phone] = False; continue\n",
        "        v = resampled.get(phone, None)\n",
        "        if v is None:\n",
        "            lag_result[phone] = 0; weak[phone] = True; continue\n",
        "        disc_idx = grid_disc_indices(phone_disc.get(phone, pd.DataFrame(columns=['t','disc'])))\n",
        "        lags_accepted = []; cors_accepted = []\n",
        "        for start in range(0, len(grid) - win + 1, hop):\n",
        "            end = start + win\n",
        "            # skip windows spanning discontinuities\n",
        "            if any((i > start and i < end) for i in ref_disc_idx) or any((i > start and i < end) for i in disc_idx):\n",
        "                continue\n",
        "            x = ref_v[start:end].copy()\n",
        "            y = v[start:end].copy()\n",
        "            # valid overlap check\n",
        "            m_valid = np.isfinite(x) & np.isfinite(y)\n",
        "            if m_valid.sum() < 300:  # >=30s\n",
        "                continue\n",
        "            if np.nanmedian(x[m_valid]) < 2.0:\n",
        "                continue\n",
        "            # build 10ms upsample within window\n",
        "            t0 = grid[start]; t1 = grid[end-1]\n",
        "            t_fine = np.arange(t0, t1+1, 10, dtype=np.int64)\n",
        "            # interpolate with NaNs preserved by masking\n",
        "            def upsample(seg, seg_mask):\n",
        "                tv = np.arange(t0, t1+1, 100, dtype=np.int64)\n",
        "                seg2 = seg.copy()\n",
        "                seg2[~seg_mask] = np.nan\n",
        "                # interpolate only over finite\n",
        "                mk = np.isfinite(seg2)\n",
        "                if mk.sum() < 10:\n",
        "                    return np.full_like(t_fine, np.nan, dtype=np.float64)\n",
        "                val = np.interp(t_fine, tv[mk], seg2[mk])\n",
        "                # invalidate regions between large gaps >1.5s (already handled at 100ms stage) -> keep as is\n",
        "                return val\n",
        "            x_f = upsample(x, np.isfinite(x))\n",
        "            y_f = upsample(y, np.isfinite(y))\n",
        "            # z-score within window\n",
        "            def zscore(a):\n",
        "                m = np.isfinite(a)\n",
        "                if m.sum() < 10:\n",
        "                    return a\n",
        "                mu = np.nanmean(a[m]); sd = np.nanstd(a[m])\n",
        "                if sd < 1e-3:\n",
        "                    return np.full_like(a, np.nan, dtype=np.float64)\n",
        "                out = (a - mu)/sd\n",
        "                out[~m] = np.nan\n",
        "                return out\n",
        "            xz = zscore(x_f); yz = zscore(y_f)\n",
        "            if not np.isfinite(xz).any() or not np.isfinite(yz).any():\n",
        "                continue\n",
        "            cors = []\n",
        "            for lag in lags:\n",
        "                # shift y by lag (ms)\n",
        "                if lag >= 0:\n",
        "                    # compare x[t0:t1-lag] with y[t0+lag:t1]\n",
        "                    idx_x0 = 0; idx_x1 = len(t_fine) - (lag//10)\n",
        "                    idx_y0 = (lag//10); idx_y1 = len(t_fine)\n",
        "                else:\n",
        "                    L = (-lag)//10\n",
        "                    idx_x0 = L; idx_x1 = len(t_fine)\n",
        "                    idx_y0 = 0; idx_y1 = len(t_fine) - L\n",
        "                if idx_x1 - idx_x0 < 300:\n",
        "                    cors.append(np.nan); continue\n",
        "                cx = xz[idx_x0:idx_x1]; cy = yz[idx_y0:idx_y1]\n",
        "                m = np.isfinite(cx) & np.isfinite(cy)\n",
        "                if m.sum() < 300:\n",
        "                    cors.append(np.nan); continue\n",
        "                val = _pearson_corr(cx[m], cy[m])\n",
        "                cors.append(val)\n",
        "            cors = np.array(cors, dtype=np.float64)\n",
        "            if not np.isfinite(cors).any():\n",
        "                continue\n",
        "            # acceptance with SNR\n",
        "            order = np.argsort(np.nan_to_num(cors, nan=-1.0))[::-1]\n",
        "            best = order[0]\n",
        "            max_corr = cors[best]\n",
        "            second = order[1] if len(order) > 1 else best\n",
        "            snr = (max_corr / max(1e-9, cors[second])) if second != best and np.isfinite(cors[second]) else np.inf\n",
        "            if not (np.isfinite(max_corr) and max_corr >= 0.75 and (np.isfinite(snr) and snr >= 1.15 or snr == np.inf)):\n",
        "                continue\n",
        "            # refine\n",
        "            lag_refined = _parabolic_refine(lags_ms, cors, best)\n",
        "            lags_accepted.append(lag_refined); cors_accepted.append(max_corr)\n",
        "        if len(lags_accepted) >= 3:\n",
        "            med_lag = float(np.median(lags_accepted))\n",
        "            med_corr = float(np.median(cors_accepted)) if cors_accepted else 0.0\n",
        "            med_lag = float(np.clip(med_lag, -300.0, 300.0))\n",
        "            lag_result[phone] = int(np.round(med_lag))\n",
        "            weak[phone] = (med_corr < 0.65)\n",
        "        else:\n",
        "            lag_result[phone] = 0\n",
        "            weak[phone] = True\n",
        "    return lag_result, weak\n",
        "\n",
        "def build_submission_with_fusion(sample_path: Path, test_root: Path) -> pd.DataFrame:\n",
        "    sub = pd.read_csv(sample_path)\n",
        "    sub['tripId'] = sub['tripId'].astype(str)\n",
        "    sub['route'] = sub['tripId'].str.rsplit('-', n=1).str[0]\n",
        "    out_rows = []\n",
        "    for route, sub_route in sub.groupby('route', sort=False):\n",
        "        route_dir = test_root / route\n",
        "        if not route_dir.exists():\n",
        "            for trip_id, grp in sub_route.groupby('tripId', sort=False):\n",
        "                phone = trip_id.rsplit('-',1)[-1]\n",
        "                gnss_csv = test_root / route / phone / 'device_gnss.csv'\n",
        "                pred_df = run_phone_kf(gnss_csv, grp['UnixTimeMillis'].values.astype(np.int64))\n",
        "                pred_df['tripId'] = trip_id\n",
        "                out_rows.append(pred_df[['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees']])\n",
        "            continue\n",
        "        lat0, lon0 = build_route_anchor_from_all_phones(route_dir)\n",
        "        phone_dirs = sorted([p for p in route_dir.glob('*') if p.is_dir()])\n",
        "        route_phones = [tid.rsplit('-',1)[-1] for tid in sub_route['tripId'].unique()]\n",
        "        # Compute per-phone time offsets (alignment)\n",
        "        lag_ms_map, weak_align = compute_time_offsets(route_dir, lat0, lon0, route_phones)\n",
        "        # Build per-phone tracks with time shift applied\n",
        "        times_by_phone = {tid.rsplit('-',1)[-1]: grp['UnixTimeMillis'].values.astype(np.int64) for tid, grp in sub_route.groupby('tripId', sort=False)}\n",
        "        per_phone_tracks = {}\n",
        "        for ph_dir in phone_dirs:\n",
        "            phone_name = ph_dir.name\n",
        "            if phone_name not in route_phones:\n",
        "                continue\n",
        "            gnss_csv = ph_dir / 'device_gnss.csv'\n",
        "            if not gnss_csv.exists():\n",
        "                continue\n",
        "            base_std = phone_base_std_from_name(phone_name)\n",
        "            ts = times_by_phone.get(phone_name, None)\n",
        "            if ts is None:\n",
        "                continue\n",
        "            t_offset = int(lag_ms_map.get(phone_name, 0))\n",
        "            trk = run_phone_kf_enhanced(gnss_csv, lat0, lon0, ts, base_std, time_offset_ms=t_offset)\n",
        "            per_phone_tracks[phone_name] = trk\n",
        "        if not per_phone_tracks:\n",
        "            for trip_id, grp in sub_route.groupby('tripId', sort=False):\n",
        "                phone = trip_id.rsplit('-',1)[-1]\n",
        "                gnss_csv = test_root / route / phone / 'device_gnss.csv'\n",
        "                pred_df = run_phone_kf(gnss_csv, grp['UnixTimeMillis'].values.astype(np.int64))\n",
        "                pred_df['tripId'] = trip_id\n",
        "                out_rows.append(pred_df[['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees']])\n",
        "            continue\n",
        "        # Per-phone ENU bias removal\n",
        "        all_E = np.concatenate([df['E'].values for df in per_phone_tracks.values()])\n",
        "        all_N = np.concatenate([df['N'].values for df in per_phone_tracks.values()])\n",
        "        route_E_med = np.nanmedian(all_E) if all_E.size else 0.0\n",
        "        route_N_med = np.nanmedian(all_N) if all_N.size else 0.0\n",
        "        for ph, df in per_phone_tracks.items():\n",
        "            dE = np.nanmedian(df['E'].values) - route_E_med\n",
        "            dN = np.nanmedian(df['N'].values) - route_N_med\n",
        "            per_phone_tracks[ph] = df.assign(E=df['E'].values - dE, N=df['N'].values - dN)\n",
        "        # Fusion inputs\n",
        "        target_ts = np.unique(np.sort(np.concatenate([df['UnixTimeMillis'].values.astype(np.int64) for df in per_phone_tracks.values()])))\n",
        "        fuse_inputs = [df[['UnixTimeMillis','E','N','Rpost_var']].copy() for df in per_phone_tracks.values()]\n",
        "        phone_names = list(per_phone_tracks.keys())\n",
        "        # Build phone multipliers and inflate if weak alignment\n",
        "        multipliers = []\n",
        "        for name in phone_names:\n",
        "            m = phone_quality_multiplier(name)\n",
        "            if weak_align.get(name, False):\n",
        "                m *= 1.2\n",
        "            multipliers.append(m)\n",
        "        fused_enu = fuse_phones_enu_union(fuse_inputs, target_ts=target_ts, drop_thresh_m1=12.0, drop_thresh_m2=8.0, phone_names=None, phone_multipliers=np.array(multipliers, dtype=np.float64))\n",
        "        if fused_enu is None or fused_enu.empty:\n",
        "            for trip_id, grp in sub_route.groupby('tripId', sort=False):\n",
        "                phone = trip_id.rsplit('-',1)[-1]\n",
        "                gnss_csv = test_root / route / phone / 'device_gnss.csv'\n",
        "                pred_df = run_phone_kf(gnss_csv, grp['UnixTimeMillis'].values.astype(np.int64))\n",
        "                pred_df['tripId'] = trip_id\n",
        "                out_rows.append(pred_df[['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees']])\n",
        "        else:\n",
        "            # Light RTS on fused with variable R: clip R in [12,25] m^2; q_acc=2.2\n",
        "            Rf = np.clip(fused_enu['Rpost_var'].values.astype(np.float64), 12.0, 25.0)\n",
        "            Ef_s, Nf_s, _, _ = kf_rts_smooth_adaptive(fused_enu['E'].values.astype(np.float64),\n",
        "                                                     fused_enu['N'].values.astype(np.float64),\n",
        "                                                     fused_enu['UnixTimeMillis'].values.astype(np.int64),\n",
        "                                                     Rpos_vars=Rf,\n",
        "                                                     speed_mag=None,\n",
        "                                                     R_speed_vars=None,\n",
        "                                                     q_acc=2.2)\n",
        "            # Optional SG smoothing (window=11, poly=2); fallback to moving average if SciPy unavailable\n",
        "            try:\n",
        "                from scipy.signal import savgol_filter\n",
        "                Ef_s = savgol_filter(Ef_s, window_length=11 if len(Ef_s) >= 11 else (len(Ef_s)//2*2+1), polyorder=2, mode='interp')\n",
        "                Nf_s = savgol_filter(Nf_s, window_length=11 if len(Nf_s) >= 11 else (len(Nf_s)//2*2+1), polyorder=2, mode='interp')\n",
        "            except Exception:\n",
        "                def movavg(x, w=9):\n",
        "                    w = int(min(max(3, w), max(3, (len(x)//2)*2+1)))\n",
        "                    k = w//2\n",
        "                    pad = np.pad(x, (k,k), mode='edge')\n",
        "                    kern = np.ones(w, dtype=np.float64)/w\n",
        "                    y = np.convolve(pad, kern, mode='valid')\n",
        "                    return y\n",
        "                Ef_s = movavg(np.asarray(Ef_s), w=9) if len(Ef_s) >= 3 else Ef_s\n",
        "                Nf_s = movavg(np.asarray(Nf_s), w=9) if len(Nf_s) >= 3 else Nf_s\n",
        "            lat_f, lon_f = enu_to_latlon_series(Ef_s, Nf_s, np.zeros_like(Ef_s), lat0, lon0)\n",
        "            fused_latlon = pd.DataFrame({'UnixTimeMillis': fused_enu['UnixTimeMillis'].values, 'LatitudeDegrees': lat_f, 'LongitudeDegrees': lon_f})\n",
        "            for trip_id, grp in sub_route.groupby('tripId', sort=False):\n",
        "                tmp = grp[['UnixTimeMillis']].merge(fused_latlon, on='UnixTimeMillis', how='left')\n",
        "                tmp['tripId'] = trip_id\n",
        "                out_rows.append(tmp[['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees']])\n",
        "    pred = pd.concat(out_rows, ignore_index=True)\n",
        "    pred = pred.merge(sub[['tripId','UnixTimeMillis']].assign(_ord=np.arange(len(sub))), on=['tripId','UnixTimeMillis'], how='right').sort_values('_ord').drop(columns=['_ord'])\n",
        "    pred['LatitudeDegrees'] = pred['LatitudeDegrees'].clip(-90, 90)\n",
        "    pred['LongitudeDegrees'] = ((pred['LongitudeDegrees'] + 180) % 360) - 180\n",
        "    return pred\n",
        "\n",
        "print('Enhanced KF (adaptive Rpos + Doppler speed + fusion + alignment + post-RTS) ready.', flush=True)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced KF (adaptive Rpos + Doppler speed + fusion + alignment + post-RTS) ready.\n"
          ]
        }
      ]
    },
    {
      "id": "faa8628d-f2b2-4d15-9072-fd500c111bbf",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "print('== Building fused submission (adaptive R + speed FD + multi-phone) ==', flush=True)\n",
        "pred_fused = build_submission_with_fusion(Path('sample_submission.csv'), Path('test'))\n",
        "print('Fused head:\\n', pred_fused.head(3))\n",
        "print('Ranges: lat[%.6f, %.6f] lon[%.6f, %.6f]' % (pred_fused.LatitudeDegrees.min(), pred_fused.LatitudeDegrees.max(), pred_fused.LongitudeDegrees.min(), pred_fused.LongitudeDegrees.max()))\n",
        "save_submission(pred_fused, Path('submission.csv'))\n",
        "print('== Done fused submission ==', flush=True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Building fused submission (adaptive R + speed FD + multi-phone) ==\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fused head:\n                              tripId  UnixTimeMillis  LatitudeDegrees  \\\n0  2020-06-04-US-MTV-1-GooglePixel4   1591304310441        37.310135   \n1  2020-06-04-US-MTV-1-GooglePixel4   1591304311441        37.310218   \n2  2020-06-04-US-MTV-1-GooglePixel4   1591304312441        37.310297   \n\n   LongitudeDegrees  \n0       -121.894196  \n1       -121.894343  \n2       -121.894484  \nRanges: lat[37.310053, 37.716598] lon[-122.539819, -121.894050]\nSaved submission: submission.csv shape: (37087, 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Done fused submission ==\n"
          ]
        }
      ]
    },
    {
      "id": "81b9105f-98ab-41e3-879b-9ccf746771b0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from collections import deque\n",
        "from pathlib import Path\n",
        "\n",
        "# V4.3+: Medal-driven patches: guarded fused-pass vel2D, tighter R/alpha/speed, stronger NHC & faster ZUPT, robust selection.\n",
        "\n",
        "def kf_rts_smooth_adaptive_v43(E: np.ndarray, N: np.ndarray, t_ms: np.ndarray,\n",
        "                               Rpos_vars: np.ndarray,\n",
        "                               speed_mag: np.ndarray | None = None,\n",
        "                               R_speed_vars: np.ndarray | float | None = None,\n",
        "                               nsat: np.ndarray | None = None,\n",
        "                               mean_cn0: np.ndarray | None = None,\n",
        "                               vE_obs: np.ndarray | None = None,\n",
        "                               vN_obs: np.ndarray | None = None,\n",
        "                               RvE_vars: np.ndarray | None = None,\n",
        "                               RvN_vars: np.ndarray | None = None,\n",
        "                               gate_pos_chi2: float = 7.38,\n",
        "                               gate_spd_chi2: float = 6.63,\n",
        "                               gate_vel_chi2: float = 6.63):\n",
        "    n = len(t_ms)\n",
        "    if n == 0:\n",
        "        return np.array([]), np.array([]), np.array([]), np.zeros((0,), dtype=np.float64)\n",
        "    R_raw = Rpos_vars.astype(np.float64).copy()\n",
        "    Rpos_vars = np.clip(R_raw, 9.0, 400.0)\n",
        "    if nsat is None: nsat = np.full(n, 8.0, dtype=np.float64)\n",
        "    if mean_cn0 is None: mean_cn0 = np.full(n, 22.0, dtype=np.float64)\n",
        "    nsat = nsat.astype(np.float64)\n",
        "    mean_cn0 = mean_cn0.astype(np.float64)\n",
        "\n",
        "    x = np.zeros((n,4), dtype=np.float64)\n",
        "    P = np.zeros((n,4,4), dtype=np.float64)\n",
        "    Fm = np.zeros((n,4,4), dtype=np.float64)\n",
        "    Qm = np.zeros((n,4,4), dtype=np.float64)\n",
        "    x[0] = np.array([E[0], N[0], 0.0, 0.0], dtype=np.float64)\n",
        "    P[0] = np.diag([Rpos_vars[0], Rpos_vars[0], 25.0, 25.0])\n",
        "    Hpos = np.array([[1,0,0,0],[0,1,0,0]], dtype=np.float64)\n",
        "    Hvel = np.array([[0,0,1,0],[0,0,0,1]], dtype=np.float64)\n",
        "\n",
        "    stopped = False\n",
        "    spd_buf = deque()\n",
        "    burst_steps = 0\n",
        "    use_vel2d = (vE_obs is not None and vN_obs is not None and RvE_vars is not None and RvN_vars is not None)\n",
        "\n",
        "    for k in range(1, n):\n",
        "        dt = max(1e-3, (t_ms[k] - t_ms[k-1]) * 1e-3)\n",
        "        if (t_ms[k] - t_ms[k-1]) > 1500:\n",
        "            stopped = False\n",
        "            spd_buf.clear()\n",
        "            burst_steps = 0\n",
        "        F = np.array([[1,0,dt,0],[0,1,0,dt],[0,0,1,0],[0,0,0,1]], dtype=np.float64)\n",
        "        x_pred = F @ x[k-1]\n",
        "        v_pred = float(np.hypot(x_pred[2], x_pred[3]))\n",
        "        dvE = x_pred[2] - x[k-1,2]; dvN = x_pred[3] - x[k-1,3]\n",
        "        acc = np.hypot(dvE, dvN) / dt\n",
        "        if burst_steps > 0:\n",
        "            q_acc = 3.5; burst_steps -= 1\n",
        "        elif v_pred < 0.5 and stopped:\n",
        "            q_acc = 0.5\n",
        "        elif acc > 2.5:\n",
        "            q_acc = 3.5; burst_steps = 3\n",
        "        else:\n",
        "            q_acc = 2.0\n",
        "        dt2, dt3, dt4 = dt*dt, dt*dt*dt, (dt*dt)*(dt*dt)\n",
        "        Q = q_acc * np.array([[dt4/4,0,dt3/2,0],[0,dt4/4,0,dt3/2],[dt3/2,0,dt2,0],[0,dt3/2,0,dt2]], dtype=np.float64)\n",
        "        P_pred = F @ P[k-1] @ F.T + Q\n",
        "\n",
        "        Rk_raw = R_raw[k]\n",
        "        Rk = Rpos_vars[k]\n",
        "        allow_pos = True\n",
        "        if (nsat[k] < 6) or (mean_cn0[k] < 20.0) or (Rk_raw > 400.0) or (v_pred > 55.0) or (acc > 12.0):\n",
        "            allow_pos = False\n",
        "        if v_pred > 40.0 and acc > 10.0:\n",
        "            allow_pos = False\n",
        "\n",
        "        x_upd, P_upd = x_pred, P_pred\n",
        "        if allow_pos:\n",
        "            z = np.array([E[k], N[k]], dtype=np.float64)\n",
        "            y = z - (Hpos @ x_pred)\n",
        "            Rpos = np.diag([Rk, Rk])\n",
        "            S = Hpos @ P_pred @ Hpos.T + Rpos\n",
        "            try: Sinv = np.linalg.inv(S)\n",
        "            except np.linalg.LinAlgError: Sinv = np.linalg.pinv(S)\n",
        "            maha2 = float(y.T @ Sinv @ y)\n",
        "            if maha2 <= gate_pos_chi2:\n",
        "                K = P_pred @ Hpos.T @ Sinv\n",
        "                x_upd = x_pred + K @ y\n",
        "                P_upd = (np.eye(4) - K @ Hpos) @ P_pred\n",
        "\n",
        "        did_vel2d = False\n",
        "        if use_vel2d and np.isfinite(vE_obs[k]) and np.isfinite(vN_obs[k]):\n",
        "            if R_raw[k] <= 100.0:\n",
        "                vobs = np.array([vE_obs[k], vN_obs[k]], dtype=np.float64)\n",
        "                if np.hypot(vobs[0], vobs[1]) <= 50.0:\n",
        "                    vpred_vec = x_upd[2:4]\n",
        "                    sp, so = np.hypot(vpred_vec[0], vpred_vec[1]), np.hypot(vobs[0], vobs[1])\n",
        "                    if not (so <= 1.0 or sp <= 0.5):\n",
        "                        cosang = float(np.dot(vpred_vec, vobs) / (sp*so + 1e-9)) if (sp > 1e-6 and so > 1e-6) else 1.0\n",
        "                        if not (np.isfinite(cosang) and cosang < -0.5):\n",
        "                            Rv = np.diag([float(np.clip(RvE_vars[k], 0.15**2, 1.5**2)), float(np.clip(RvN_vars[k], 0.15**2, 1.5**2))])\n",
        "                            if (nsat[k] < 8) or (mean_cn0[k] < 23.0):\n",
        "                                Rv = Rv * 1.25\n",
        "                            yv = vobs - (Hvel @ x_upd)\n",
        "                            S_v = Hvel @ P_upd @ Hvel.T + Rv\n",
        "                            try: S_v_inv = np.linalg.inv(S_v)\n",
        "                            except np.linalg.LinAlgError: S_v_inv = np.linalg.pinv(S_v)\n",
        "                            maha2_v = float(yv.T @ S_v_inv @ yv)\n",
        "                            if maha2_v <= gate_vel_chi2:\n",
        "                                K_v = P_upd @ Hvel.T @ S_v_inv\n",
        "                                x_upd = x_upd + K_v @ yv\n",
        "                                P_upd = (np.eye(4) - K_v @ Hvel) @ P_upd\n",
        "                                did_vel2d = True\n",
        "\n",
        "        if (not did_vel2d) and (speed_mag is not None) and np.isfinite(speed_mag[k]) and (nsat[k] >= 6) and (mean_cn0[k] >= 20.0):\n",
        "            vE, vN = x_upd[2], x_upd[3]\n",
        "            vnorm = float(np.hypot(vE, vN))\n",
        "            if vnorm > 0.2:\n",
        "                Hs = np.array([0.0, 0.0, vE/max(vnorm,1e-9), vN/max(vnorm,1e-9)], dtype=np.float64).reshape(1,4)\n",
        "                s_mat = Hs @ P_upd @ Hs.T\n",
        "                if isinstance(R_speed_vars, np.ndarray):\n",
        "                    Rsv = R_speed_vars[k] if k < len(R_speed_vars) and np.isfinite(R_speed_vars[k]) else 2.25\n",
        "                elif isinstance(R_speed_vars, (float, int)):\n",
        "                    Rsv = float(R_speed_vars)\n",
        "                else:\n",
        "                    Rsv = 2.25\n",
        "                s = float(s_mat[0,0]) + Rsv\n",
        "                innov = float(speed_mag[k] - vnorm)\n",
        "                maha2_s = (innov*innov)/max(s, 1e-9)\n",
        "                if maha2_s <= gate_spd_chi2:\n",
        "                    K_s = (P_upd @ Hs.T) / s\n",
        "                    x_upd = x_upd + (K_s.flatten() * innov)\n",
        "                    P_upd = P_upd - (K_s @ (Hs @ P_upd))\n",
        "\n",
        "        cur_t = t_ms[k]\n",
        "        spd_est = float(np.hypot(x_upd[2], x_upd[3]))\n",
        "        spd_buf.append((cur_t, spd_est))\n",
        "        while spd_buf and (cur_t - spd_buf[0][0]) > 1500:\n",
        "            spd_buf.popleft()\n",
        "        vals = [v for (tt, v) in spd_buf if (cur_t - tt) <= 1000]\n",
        "        ma = np.mean(vals) if len(vals) >= 5 else spd_est\n",
        "        duration = (spd_buf[-1][0] - spd_buf[0][0]) if len(spd_buf) > 1 else 0\n",
        "        if not stopped and ma < 0.18 and duration >= 1000:\n",
        "            stopped = True\n",
        "        if stopped and ma > 0.28:\n",
        "            stopped = False\n",
        "        if stopped and spd_est < 0.5:\n",
        "            H_v = np.array([[0,0,1,0],[0,0,0,1]], dtype=np.float64)\n",
        "            z_v = np.array([0.0, 0.0], dtype=np.float64)\n",
        "            R_v = np.diag([0.08**2, 0.08**2])\n",
        "            yv = z_v - (H_v @ x_upd)\n",
        "            S_v = H_v @ P_upd @ H_v.T + R_v\n",
        "            try: S_v_inv = np.linalg.inv(S_v)\n",
        "            except np.linalg.LinAlgError: S_v_inv = np.linalg.pinv(S_v)\n",
        "            maha2_v0 = float(yv.T @ S_v_inv @ yv)\n",
        "            if maha2_v0 <= 6.63:\n",
        "                K_v0 = P_upd @ H_v.T @ S_v_inv\n",
        "                x_upd = x_upd + K_v0 @ yv\n",
        "                P_upd = (np.eye(4) - K_v0 @ H_v) @ P_upd\n",
        "\n",
        "        vE_k, vN_k = float(x_upd[2]), float(x_upd[3])\n",
        "        spd_k = float(np.hypot(vE_k, vN_k))\n",
        "        if spd_k > 2.0:\n",
        "            if k > 0:\n",
        "                h_prev = np.arctan2(x[k-1,3], x[k-1,2])\n",
        "                h_cur = np.arctan2(vN_k, vE_k)\n",
        "                d = h_cur - h_prev\n",
        "                if d > np.pi: d -= 2*np.pi\n",
        "                if d < -np.pi: d += 2*np.pi\n",
        "                hdg_rate = abs(d) / dt\n",
        "            else:\n",
        "                hdg_rate = 1e9\n",
        "            thr = 0.15 if spd_k >= 12.0 else 0.12\n",
        "            if hdg_rate < thr:\n",
        "                psi = np.arctan2(vN_k, vE_k)\n",
        "                H_lat = np.array([[0.0, 0.0, -np.sin(psi), np.cos(psi)]], dtype=np.float64)\n",
        "                R_lat = 0.15**2\n",
        "                innov = -float((H_lat @ x_upd).item())\n",
        "                S_lat = float((H_lat @ P_upd @ H_lat.T).item()) + R_lat\n",
        "                if S_lat > 1e-9:\n",
        "                    maha2_lat = (innov*innov) / S_lat\n",
        "                    if maha2_lat <= 5.99:\n",
        "                        K_lat = (P_upd @ H_lat.T) / S_lat\n",
        "                        x_upd = x_upd + (K_lat.flatten() * innov)\n",
        "                        P_upd = P_upd - (K_lat @ (H_lat @ P_upd))\n",
        "\n",
        "        x[k] = x_upd; P[k] = P_upd; Fm[k] = F; Qm[k] = Q\n",
        "\n",
        "    xs = x.copy(); Ps = P.copy()\n",
        "    for k in range(n-2, -1, -1):\n",
        "        F = Fm[k+1]; Pk = P[k]; P_pred = F @ Pk @ F.T + Qm[k+1]\n",
        "        try: Ck = Pk @ F.T @ np.linalg.inv(P_pred)\n",
        "        except np.linalg.LinAlgError: Ck = Pk @ F.T @ np.linalg.pinv(P_pred)\n",
        "        xs[k] = x[k] + Ck @ (xs[k+1] - (F @ x[k]))\n",
        "        Ps[k] = Pk + Ck @ (Ps[k+1] - P_pred) @ Ck.T\n",
        "    vnorm_s = np.hypot(xs[:,2], xs[:,3])\n",
        "    Rpost_var = 0.5 * (Ps[:,0,0] + Ps[:,1,1])\n",
        "    return xs[:,0], xs[:,1], vnorm_s, Rpost_var\n",
        "\n",
        "def run_phone_kf_enhanced_v43(gnss_csv: Path, lat0: float, lon0: float, sample_times: np.ndarray, base_std: float, time_offset_ms: int = 0):\n",
        "    df_ecef = load_phone_gnss_positions(gnss_csv)\n",
        "    if len(df_ecef) == 0:\n",
        "        return pd.DataFrame({'UnixTimeMillis': sample_times, 'E': np.nan, 'N': np.nan, 'Rpost_var': np.nan})\n",
        "    if time_offset_ms != 0:\n",
        "        df_ecef = df_ecef.copy()\n",
        "        df_ecef['t'] = (df_ecef['t'].astype(np.int64) + int(time_offset_ms)).astype(np.int64)\n",
        "    stats_raw = load_epoch_stats(gnss_csv)\n",
        "    if not stats_raw.empty:\n",
        "        if time_offset_ms != 0:\n",
        "            stats_raw = stats_raw.copy(); stats_raw['t'] = (stats_raw['t'].astype(np.int64) + int(time_offset_ms)).astype(np.int64)\n",
        "        stats_use = stats_raw[['t','ns','mean_cn0']].copy()\n",
        "    else:\n",
        "        stats_use = pd.DataFrame({'t': df_ecef['t'].values.astype(np.int64), 'ns': 8.0, 'mean_cn0': 22.0})\n",
        "    df_r = pd.DataFrame({'t': df_ecef['t'].values.astype(np.int64), 'Rpos_var': (base_std**2)})\n",
        "    df = df_ecef.merge(df_r, on='t', how='left').merge(stats_use, on='t', how='left')\n",
        "    df['Rpos_var'] = np.clip(df['Rpos_var'].values.astype(np.float64), 9.0, 400.0)\n",
        "    try:\n",
        "        Rtmp = pd.Series(df['Rpos_var'].values).rolling(5,\" # Corrected: Added closing double quote for the string"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V4.3+ KF patched per expert: fused-pass vel2D guarded, R_upper=36, speed R tiers, alpha clamp [0.80,0.92], NHC strengthened, ZUPT faster, robust selection.\n"
          ]
        }
      ]
    },
    {
      "id": "80df0bcd-f2c5-4b23-8a59-96fcee82ac14",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "print('== Building V4.3 fused submission (epoch filtering + ZUPT + dynamic q_acc) ==', flush=True)\n",
        "pred_v43 = build_submission_with_fusion_v43(Path('sample_submission.csv'), Path('test'))\n",
        "print('V4.3 head:\\n', pred_v43.head(3))\n",
        "print('Ranges: lat[%.6f, %.6f] lon[%.6f, %.6f]' % (pred_v43.LatitudeDegrees.min(), pred_v43.LatitudeDegrees.max(), pred_v43.LongitudeDegrees.min(), pred_v43.LongitudeDegrees.max()))\n",
        "save_submission(pred_v43, Path('submission.csv'))\n",
        "print('== Done V4.3 fused submission ==', flush=True)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Building V4.3 fused submission (epoch filtering + ZUPT + dynamic q_acc) ==\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V4.3 head:\n                              tripId  UnixTimeMillis  LatitudeDegrees  \\\n0  2020-06-04-US-MTV-1-GooglePixel4   1591304310441        37.416319   \n1  2020-06-04-US-MTV-1-GooglePixel4   1591304311441        37.416317   \n2  2020-06-04-US-MTV-1-GooglePixel4   1591304312441        37.416316   \n\n   LongitudeDegrees  \n0       -122.080462  \n1       -122.080461  \n2       -122.080460  \nRanges: lat[37.352136, 37.655792] lon[-122.423840, -121.986868]\nSaved submission: submission.csv shape: (37087, 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Done V4.3 fused submission ==\n"
          ]
        }
      ]
    },
    {
      "id": "b1ed1e9b-46ca-4162-bb9e-1fa92e1875f3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "print('== Submission integrity checks ==', flush=True)\n",
        "sample = pd.read_csv('sample_submission.csv')\n",
        "sub = pd.read_csv('submission.csv')\n",
        "print('sample shape:', sample.shape, 'sub shape:', sub.shape)\n",
        "# Assert columns\n",
        "assert list(sub.columns) == ['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees'], f'Bad columns: {sub.columns}'\n",
        "# Assert shape\n",
        "assert sub.shape == sample.shape, f'Shape mismatch: {sub.shape} vs {sample.shape}'\n",
        "# Assert key equality and orderable join\n",
        "sample_keys = sample[['tripId','UnixTimeMillis']].copy()\n",
        "sub_keys = sub[['tripId','UnixTimeMillis']].copy()\n",
        "assert set(map(tuple, sample_keys.values)) == set(map(tuple, sub_keys.values)), 'Key set mismatch between sample and submission'\n",
        "# Check duplicates\n",
        "dups = sub.duplicated(subset=['tripId','UnixTimeMillis']).sum()\n",
        "assert dups == 0, f'Duplicate key rows in submission: {dups}'\n",
        "# NaN checks\n",
        "nna_lat = sub['LatitudeDegrees'].isna().sum()\n",
        "nna_lon = sub['LongitudeDegrees'].isna().sum()\n",
        "print('NaNs lat:', nna_lat, 'lon:', nna_lon)\n",
        "assert nna_lat == 0 and nna_lon == 0, 'NaNs present in lat/lon'\n",
        "# Range checks\n",
        "lat_min, lat_max = sub['LatitudeDegrees'].min(), sub['LatitudeDegrees'].max()\n",
        "lon_min, lon_max = sub['LongitudeDegrees'].min(), sub['LongitudeDegrees'].max()\n",
        "print('Ranges lat[%.6f, %.6f] lon[%.6f, %.6f]' % (lat_min, lat_max, lon_min, lon_max))\n",
        "assert lat_min >= -90 and lat_max <= 90, 'Latitude out of bounds'\n",
        "assert lon_min >= -180 and lon_max <= 180, 'Longitude out of bounds'\n",
        "# Route-wise quick NA/zero diagnostics\n",
        "sub['_route'] = sub['tripId'].str.rsplit('-', n=1).str[0]\n",
        "route_stats = sub.groupby('_route').agg(rows=('tripId','size'),\n",
        "                                        nan_lat=('LatitudeDegrees', lambda s: int(s.isna().sum())),\n",
        "                                        nan_lon=('LongitudeDegrees', lambda s: int(s.isna().sum())))\n",
        "print(route_stats)\n",
        "print('== Submission integrity OK ==', flush=True)"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Submission integrity checks ==\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample shape: (37087, 4) sub shape: (37087, 4)\nNaNs lat: 0 lon: 0\nRanges lat[37.352173, 37.771449] lon[-122.424205, -121.937547]\n                      rows  nan_lat  nan_lon\n_route                                      \n2020-06-04-US-MTV-1   3312        0        0\n2020-06-04-US-MTV-2   3298        0        0\n2020-07-08-US-MTV-1   4276        0        0\n2020-07-08-US-MTV-2   4235        0        0\n2021-04-08-US-MTV-1   2973        0        0\n2021-04-29-US-MTV-1   3115        0        0\n2021-04-29-US-MTV-2   3325        0        0\n2021-08-24-US-SVL-1  12553        0        0\n== Submission integrity OK ==\n"
          ]
        }
      ]
    },
    {
      "id": "75f4aada-7991-4ab5-9e81-0849a38add16",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "print('== Building sanity and fused-speed submissions (V4.3+) ==', flush=True)\n",
        "sample_path = Path('sample_submission.csv')\n",
        "test_root = Path('test')\n",
        "\n",
        "# A) Single-best-phone (Pixel-preferred) sanity submission\n",
        "pred_single = build_submission_single_best_phone_v43(sample_path, test_root)\n",
        "pred_single.to_csv('submission_single_best.csv', index=False)\n",
        "print('Saved submission_single_best.csv shape:', pred_single.shape, flush=True)\n",
        "\n",
        "# Quick integrity check for single-best\n",
        "sample = pd.read_csv(sample_path)\n",
        "assert list(pred_single.columns) == ['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees']\n",
        "assert pred_single.shape == sample.shape\n",
        "assert pred_single.duplicated(['tripId','UnixTimeMillis']).sum() == 0\n",
        "assert pred_single['LatitudeDegrees'].isna().sum() == 0 and pred_single['LongitudeDegrees'].isna().sum() == 0\n",
        "\n",
        "# B) Hardened fusion with fused-track speed pseudo-measurement\n",
        "pred_fused_speed = build_submission_with_fusion_v43(sample_path, test_root)\n",
        "pred_fused_speed.to_csv('submission_fused_speed.csv', index=False)\n",
        "print('Saved submission_fused_speed.csv shape:', pred_fused_speed.shape, flush=True)\n",
        "\n",
        "print('== Done building both submissions ==', flush=True)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Building sanity and fused-speed submissions (V4.3+) ==\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_single_best.csv shape: (37087, 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_fused_speed.csv shape: (37087, 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Done building both submissions ==\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n/usr/local/lib/python3.11/dist-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n/usr/local/lib/python3.11/dist-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n/usr/local/lib/python3.11/dist-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n/usr/local/lib/python3.11/dist-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n/usr/local/lib/python3.11/dist-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
          ]
        }
      ]
    },
    {
      "id": "cda3da9a-53d5-41bd-af2f-e0985a3c0759",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import shutil, pandas as pd\n",
        "print('== Setting submission.csv to single-best sanity version ==', flush=True)\n",
        "shutil.copyfile('submission_single_best.csv', 'submission.csv')\n",
        "print('Copied submission_single_best.csv -> submission.csv', flush=True)\n",
        "\n",
        "# Re-run integrity checks quickly\n",
        "sample = pd.read_csv('sample_submission.csv')\n",
        "sub = pd.read_csv('submission.csv')\n",
        "assert list(sub.columns) == ['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees']\n",
        "assert sub.shape == sample.shape\n",
        "assert sub.duplicated(['tripId','UnixTimeMillis']).sum() == 0\n",
        "assert sub['LatitudeDegrees'].isna().sum() == 0 and sub['LongitudeDegrees'].isna().sum() == 0\n",
        "print('Integrity OK. Ready to submit single-best.', flush=True)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Setting submission.csv to single-best sanity version ==\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied submission_single_best.csv -> submission.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Integrity OK. Ready to submit single-best.\n"
          ]
        }
      ]
    },
    {
      "id": "afa59dea-eb2d-40ae-bcbe-fa74e6055c63",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import shutil, pandas as pd\n",
        "print('== Setting submission.csv to hardened fusion + fused-speed version ==', flush=True)\n",
        "shutil.copyfile('submission_fused_speed.csv', 'submission.csv')\n",
        "print('Copied submission_fused_speed.csv -> submission.csv', flush=True)\n",
        "\n",
        "# Integrity checks\n",
        "sample = pd.read_csv('sample_submission.csv')\n",
        "sub = pd.read_csv('submission.csv')\n",
        "assert list(sub.columns) == ['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees']\n",
        "assert sub.shape == sample.shape\n",
        "assert sub.duplicated(['tripId','UnixTimeMillis']).sum() == 0\n",
        "assert sub['LatitudeDegrees'].isna().sum() == 0 and sub['LongitudeDegrees'].isna().sum() == 0\n",
        "print('Integrity OK. Ready to submit fused-speed.', flush=True)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Setting submission.csv to hardened fusion + fused-speed version ==\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied submission_fused_speed.csv -> submission.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sub.shape == sample.shape\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sub.duplicated([\u001b[33m'\u001b[39m\u001b[33mtripId\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mUnixTimeMillis\u001b[39m\u001b[33m'\u001b[39m]).sum() == \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sub[\u001b[33m'\u001b[39m\u001b[33mLatitudeDegrees\u001b[39m\u001b[33m'\u001b[39m].isna().sum() == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m sub[\u001b[33m'\u001b[39m\u001b[33mLongitudeDegrees\u001b[39m\u001b[33m'\u001b[39m].isna().sum() == \u001b[32m0\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mIntegrity OK. Ready to submit fused-speed.\u001b[39m\u001b[33m'\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[31mAssertionError\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "3c98f5ca-cbc4-4f1c-bee3-a7b5d993c618",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd, numpy as np, shutil\n",
        "print('== Cleaning fused-speed submission to remove NaNs ==', flush=True)\n",
        "pred = pd.read_csv('submission_fused_speed.csv')\n",
        "before_nan_lat = int(pred['LatitudeDegrees'].isna().sum())\n",
        "before_nan_lon = int(pred['LongitudeDegrees'].isna().sum())\n",
        "print('NaNs before: lat', before_nan_lat, 'lon', before_nan_lon, flush=True)\n",
        "pred['_route'] = pred['tripId'].str.rsplit('-', n=1).str[0]\n",
        "# Per-trip ffill/bfill then median fill for any remaining gaps\n",
        "pred['LatitudeDegrees'] = pred.groupby('tripId')['LatitudeDegrees'].transform(lambda s: s.ffill().bfill())\n",
        "pred['LongitudeDegrees'] = pred.groupby('tripId')['LongitudeDegrees'].transform(lambda s: s.ffill().bfill())\n",
        "pred['LatitudeDegrees'] = pred.groupby('tripId')['LatitudeDegrees'].transform(lambda s: s.fillna(s.median()))\n",
        "pred['LongitudeDegrees'] = pred.groupby('tripId')['LongitudeDegrees'].transform(lambda s: s.fillna(s.median()))\n",
        "# Clip ranges\n",
        "pred['LatitudeDegrees'] = pred['LatitudeDegrees'].clip(-90, 90)\n",
        "pred['LongitudeDegrees'] = ((pred['LongitudeDegrees'] + 180) % 360) - 180\n",
        "after_nan_lat = int(pred['LatitudeDegrees'].isna().sum())\n",
        "after_nan_lon = int(pred['LongitudeDegrees'].isna().sum())\n",
        "print('NaNs after: lat', after_nan_lat, 'lon', after_nan_lon, flush=True)\n",
        "pred = pred[['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees']]\n",
        "pred.to_csv('submission_fused_speed_clean.csv', index=False)\n",
        "print('Wrote submission_fused_speed_clean.csv shape:', pred.shape, flush=True)\n",
        "shutil.copyfile('submission_fused_speed_clean.csv', 'submission.csv')\n",
        "print('Copied cleaned fused-speed -> submission.csv', flush=True)\n",
        "# Final integrity check\n",
        "sample = pd.read_csv('sample_submission.csv')\n",
        "sub = pd.read_csv('submission.csv')\n",
        "assert list(sub.columns) == ['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees']\n",
        "assert sub.shape == sample.shape\n",
        "assert sub.duplicated(['tripId','UnixTimeMillis']).sum() == 0\n",
        "assert sub['LatitudeDegrees'].isna().sum() == 0 and sub['LongitudeDegrees'].isna().sum() == 0\n",
        "print('Integrity OK after cleaning.', flush=True)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Cleaning fused-speed submission to remove NaNs ==\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaNs before: lat 7270 lon 7270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaNs after: lat 7270 lon 7270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_fused_speed_clean.csv shape: (37087, 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied cleaned fused-speed -> submission.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n/usr/local/lib/python3.11/dist-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n/usr/local/lib/python3.11/dist-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n/usr/local/lib/python3.11/dist-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n/usr/local/lib/python3.11/dist-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n/usr/local/lib/python3.11/dist-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sub.shape == sample.shape\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sub.duplicated([\u001b[33m'\u001b[39m\u001b[33mtripId\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mUnixTimeMillis\u001b[39m\u001b[33m'\u001b[39m]).sum() == \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sub[\u001b[33m'\u001b[39m\u001b[33mLatitudeDegrees\u001b[39m\u001b[33m'\u001b[39m].isna().sum() == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m sub[\u001b[33m'\u001b[39m\u001b[33mLongitudeDegrees\u001b[39m\u001b[33m'\u001b[39m].isna().sum() == \u001b[32m0\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mIntegrity OK after cleaning.\u001b[39m\u001b[33m'\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[31mAssertionError\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "bf7ed6d0-3ece-42f7-90aa-f22e7129a630",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd, numpy as np, shutil\n",
        "print('== Filling NaNs in fused-speed using single-best fallback ==', flush=True)\n",
        "fused = pd.read_csv('submission_fused_speed.csv')\n",
        "single = pd.read_csv('submission_single_best.csv')\n",
        "print('Fused NaNs before:', int(fused['LatitudeDegrees'].isna().sum()), int(fused['LongitudeDegrees'].isna().sum()), flush=True)\n",
        "m = fused.merge(single, on=['tripId','UnixTimeMillis'], how='left', suffixes=('', '_sb'))\n",
        "na_lat = m['LatitudeDegrees'].isna()\n",
        "na_lon = m['LongitudeDegrees'].isna()\n",
        "m.loc[na_lat, 'LatitudeDegrees'] = m.loc[na_lat, 'LatitudeDegrees_sb']\n",
        "m.loc[na_lon, 'LongitudeDegrees'] = m.loc[na_lon, 'LongitudeDegrees_sb']\n",
        "pred = m[['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees']].copy()\n",
        "# Per-trip safety fill and clipping\n",
        "pred['LatitudeDegrees'] = pred.groupby('tripId')['LatitudeDegrees'].transform(lambda s: s.ffill().bfill())\n",
        "pred['LongitudeDegrees'] = pred.groupby('tripId')['LongitudeDegrees'].transform(lambda s: s.ffill().bfill())\n",
        "pred['LatitudeDegrees'] = pred.groupby('tripId')['LatitudeDegrees'].transform(lambda s: s.fillna(s.median()))\n",
        "pred['LongitudeDegrees'] = pred.groupby('tripId')['LongitudeDegrees'].transform(lambda s: s.fillna(s.median()))\n",
        "pred['LatitudeDegrees'] = pred['LatitudeDegrees'].clip(-90, 90)\n",
        "pred['LongitudeDegrees'] = ((pred['LongitudeDegrees'] + 180) % 360) - 180\n",
        "after_lat = int(pred['LatitudeDegrees'].isna().sum()); after_lon = int(pred['LongitudeDegrees'].isna().sum())\n",
        "print('NaNs after fallback fill:', after_lat, after_lon, flush=True)\n",
        "pred.to_csv('submission_fused_speed_filled.csv', index=False)\n",
        "print('Wrote submission_fused_speed_filled.csv shape:', pred.shape, flush=True)\n",
        "shutil.copyfile('submission_fused_speed_filled.csv', 'submission.csv')\n",
        "print('Copied fused+fallback -> submission.csv', flush=True)\n",
        "# Final integrity\n",
        "sample = pd.read_csv('sample_submission.csv')\n",
        "sub = pd.read_csv('submission.csv')\n",
        "assert list(sub.columns) == ['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees']\n",
        "assert sub.shape == sample.shape\n",
        "assert sub.duplicated(['tripId','UnixTimeMillis']).sum() == 0\n",
        "assert sub['LatitudeDegrees'].isna().sum() == 0 and sub['LongitudeDegrees'].isna().sum() == 0\n",
        "print('Integrity OK after fallback fill.', flush=True)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Filling NaNs in fused-speed using single-best fallback ==\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fused NaNs before: 7270 7270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaNs after fallback fill: 0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_fused_speed_filled.csv shape: (37087, 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied fused+fallback -> submission.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Integrity OK after fallback fill.\n"
          ]
        }
      ]
    },
    {
      "id": "e9e85341-e4e5-4044-b618-6c3b4e94ea66",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === Minimal raw-WLS (position + velocity) per-epoch solver (ECEF) with RAIM, per expert plan ===\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def _ecef_to_enu_rot(lat0_deg: float, lon0_deg: float) -> np.ndarray:\n",
        "    lat0 = np.radians(lat0_deg, dtype=np.float64)\n",
        "    lon0 = np.radians(lon0_deg, dtype=np.float64)\n",
        "    slat, clat = np.sin(lat0), np.cos(lat0)\n",
        "    slon, clon = np.sin(lon0), np.cos(lon0)\n",
        "    # E,N,U rows\n",
        "    return np.array([\n",
        "        [-slon,             clon,              0.0],\n",
        "        [-slat*clon, -slat*slon,  clat],\n",
        "        [ clat*clon,  clat*slon,  slat]\n",
        "    ], dtype=np.float64)\n",
        "\n",
        "def _prep_epoch(df_epoch: pd.DataFrame, gps_only: bool = True) -> pd.DataFrame:\n",
        "    d = df_epoch.copy()\n",
        "    if gps_only and 'ConstellationType' in d.columns:\n",
        "        d = d[d['ConstellationType'] == 1]\n",
        "    # Elevation mask (robustness)\n",
        "    if 'SvElevationDegrees' in d.columns:\n",
        "        try:\n",
        "            d = d[d['SvElevationDegrees'].astype(float) >= 10.0]\n",
        "        except Exception:\n",
        "            pass\n",
        "    # Required columns presence guard\n",
        "    req = ['SvPositionXEcefMeters','SvPositionYEcefMeters','SvPositionZEcefMeters',\n",
        "           'RawPseudorangeMeters','RawPseudorangeUncertaintyMeters','Cn0DbHz']\n",
        "    for c in req:\n",
        "        if c not in d.columns:\n",
        "            return pd.DataFrame()\n",
        "    # Filters\n",
        "    d = d.dropna(subset=['SvPositionXEcefMeters','SvPositionYEcefMeters','SvPositionZEcefMeters','RawPseudorangeMeters'])\n",
        "    if len(d) < 6:\n",
        "        return pd.DataFrame()\n",
        "    d['Cn0DbHz'] = d['Cn0DbHz'].astype(np.float64).fillna(0.0)\n",
        "    d['RawPseudorangeUncertaintyMeters'] = d['RawPseudorangeUncertaintyMeters'].astype(np.float64).fillna(1e6)\n",
        "    d = d[(d['Cn0DbHz'] >= 15.0) & (d['RawPseudorangeUncertaintyMeters'] <= 50.0)]\n",
        "    if len(d) < 6:\n",
        "        return pd.DataFrame()\n",
        "    # Corrections (fill missing with 0)\n",
        "    for c in ['SvClockBiasMeters','IonosphericDelayMeters','TroposphericDelayMeters','IsrbMeters']:\n",
        "        if c not in d.columns:\n",
        "            d[c] = 0.0\n",
        "        else:\n",
        "            d[c] = d[c].astype(np.float64).fillna(0.0)\n",
        "    return d\n",
        "\n",
        "def _weights_code_sigma(unc_m: np.ndarray, cn0: np.ndarray) -> np.ndarray:\n",
        "    unc = np.clip(unc_m.astype(np.float64), 1.0, 30.0)\n",
        "    cn = np.clip(cn0.astype(np.float64), 15.0, 35.0)\n",
        "    sigma = unc * (25.0 / cn)\n",
        "    return sigma\n",
        "\n",
        "def _init_x0_from_wls(df_epoch: pd.DataFrame) -> np.ndarray | None:\n",
        "    cols = ['WlsPositionXEcefMeters','WlsPositionYEcefMeters','WlsPositionZEcefMeters']\n",
        "    if all(c in df_epoch.columns for c in cols):\n",
        "        x = df_epoch[cols[0]].median() if df_epoch[cols[0]].notna().any() else np.nan\n",
        "        y = df_epoch[cols[1]].median() if df_epoch[cols[1]].notna().any() else np.nan\n",
        "        z = df_epoch[cols[2]].median() if df_epoch[cols[2]].notna().any() else np.nan\n",
        "        if np.isfinite(x) and np.isfinite(y) and np.isfinite(z):\n",
        "            return np.array([float(x), float(y), float(z)], dtype=np.float64)\n",
        "    return None\n",
        "\n",
        "def wls_position_epoch(df_epoch: pd.DataFrame, x0_ecef: np.ndarray | None = None, max_drops: int = 2) -> tuple[np.ndarray, float, np.ndarray, np.ndarray, bool]:\n",
        "    d = _prep_epoch(df_epoch, gps_only=True)\n",
        "    if d.empty:\n",
        "        return np.full(3, np.nan), np.nan, np.full((4,4), np.nan), np.array([]), False\n",
        "    Xs = d['SvPositionXEcefMeters'].values.astype(np.float64)\n",
        "    Ys = d['SvPositionYEcefMeters'].values.astype(np.float64)\n",
        "    Zs = d['SvPositionZEcefMeters'].values.astype(np.float64)\n",
        "    pr = d['RawPseudorangeMeters'].values.astype(np.float64)\n",
        "    unc = d['RawPseudorangeUncertaintyMeters'].values.astype(np.float64)\n",
        "    cn0 = d['Cn0DbHz'].values.astype(np.float64)\n",
        "    clk = d['SvClockBiasMeters'].values.astype(np.float64)\n",
        "    ion = d['IonosphericDelayMeters'].values.astype(np.float64)\n",
        "    tro = d['TroposphericDelayMeters'].values.astype(np.float64)\n",
        "    isrb = d['IsrbMeters'].values.astype(np.float64)\n",
        "    pr_corr = pr + clk - ion - tro - isrb\n",
        "    sigma = _weights_code_sigma(unc, cn0)\n",
        "    W = 1.0 / (sigma*sigma)\n",
        "    # Init from WlsPosition if available; else fallback to satellite barycenter or provided x0\n",
        "    x0_wls = _init_x0_from_wls(df_epoch)\n",
        "    if x0_wls is not None:\n",
        "        x = x0_wls\n",
        "    elif x0_ecef is not None and np.all(np.isfinite(x0_ecef)):\n",
        "        x = x0_ecef.astype(np.float64).copy()\n",
        "    else:\n",
        "        x = np.array([np.median(Xs), np.median(Ys), np.median(Zs)], dtype=np.float64)\n",
        "    cdt = 0.0\n",
        "    keep = np.ones(len(pr_corr), dtype=bool)\n",
        "    ok = False\n",
        "    for it in range(5):\n",
        "        dX = Xs[keep] - x[0]; dY = Ys[keep] - x[1]; dZ = Zs[keep] - x[2]\n",
        "        rho = np.sqrt(dX*dX + dY*dY + dZ*dZ) + 1e-9\n",
        "        ux, uy, uz = dX/rho, dY/rho, dZ/rho\n",
        "        H = np.column_stack([-ux, -uy, -uz, np.ones(np.sum(keep), dtype=np.float64)])\n",
        "        h = rho + cdt\n",
        "        r = pr_corr[keep] - h\n",
        "        w = W[keep]\n",
        "        Wsqrt = np.sqrt(w)\n",
        "        Hw = H * Wsqrt[:,None]; rw = r * Wsqrt\n",
        "        ATA = Hw.T @ Hw\n",
        "        ATb = Hw.T @ rw\n",
        "        try:\n",
        "            cond = np.linalg.cond(ATA)\n",
        "            if not np.isfinite(cond) or cond > 1e8:\n",
        "                break\n",
        "            delta = np.linalg.solve(ATA, ATb)\n",
        "        except np.linalg.LinAlgError:\n",
        "            break\n",
        "        # Cap step to avoid divergence from bad init\n",
        "        step = delta[:3]\n",
        "        step_norm = float(np.linalg.norm(step))\n",
        "        if step_norm > 100.0:\n",
        "            step = step * (100.0 / step_norm)\n",
        "        x = x + step\n",
        "        cdt = cdt + float(delta[3])\n",
        "        if np.linalg.norm(step) < 1e-3 and abs(delta[3]) < 0.1:\n",
        "            ok = True\n",
        "            break\n",
        "        ok = True\n",
        "        # RAIM after first iter\n",
        "        if it == 0 and max_drops > 0:\n",
        "            dX_all = Xs - x[0]; dY_all = Ys - x[1]; dZ_all = Zs - x[2]\n",
        "            rho_all = np.sqrt(dX_all*dX_all + dY_all*dY_all + dZ_all*dZ_all) + 1e-9\n",
        "            r_all = pr_corr - (rho_all + cdt)\n",
        "            z = r_all / sigma\n",
        "            drops = 0\n",
        "            while drops < max_drops and keep.sum() > 6:\n",
        "                idx = np.argmax(np.abs(z))\n",
        "                if abs(z[idx]) > 3.5:\n",
        "                    keep[idx] = False\n",
        "                    drops += 1\n",
        "                    z[idx] = 0.0\n",
        "                else:\n",
        "                    break\n",
        "    if not ok or keep.sum() < 6:\n",
        "        return np.full(3, np.nan), np.nan, np.full((4,4), np.nan), np.array([]), False\n",
        "    # Covariance\n",
        "    dX = Xs[keep] - x[0]; dY = Ys[keep] - x[1]; dZ = Zs[keep] - x[2]\n",
        "    rho = np.sqrt(dX*dX + dY*dY + dZ*dZ) + 1e-9\n",
        "    ux, uy, uz = dX/rho, dY/rho, dZ/rho\n",
        "    H = np.column_stack([-ux, -uy, -uz, np.ones(np.sum(keep), dtype=np.float64)])\n",
        "    r = pr_corr[keep] - (rho + cdt)\n",
        "    w = W[keep]\n",
        "    Wsqrt = np.sqrt(w)\n",
        "    Hw = H * Wsqrt[:,None]; rw = r * Wsqrt\n",
        "    ATA = Hw.T @ Hw\n",
        "    try:\n",
        "        Cov = np.linalg.inv(ATA)\n",
        "    except np.linalg.LinAlgError:\n",
        "        Cov = np.full((4,4), np.nan)\n",
        "    rms = float(np.sqrt(np.nanmean(r*r))) if np.isfinite(r).any() else 1.0\n",
        "    sigma0_2 = max(1.0, rms)**2\n",
        "    Cov = Cov * sigma0_2\n",
        "    return x, cdt, Cov, keep, True\n",
        "\n",
        "def wls_velocity_epoch(df_epoch: pd.DataFrame, x_ecef: np.ndarray) -> tuple[np.ndarray, float, np.ndarray, bool]:\n",
        "    d = _prep_epoch(df_epoch, gps_only=True)\n",
        "    if d.empty or (not np.all(np.isfinite(x_ecef))):\n",
        "        return np.full(3, np.nan), np.nan, np.full((4,4), np.nan), False\n",
        "    reqv = ['SvVelocityXEcefMetersPerSecond','SvVelocityYEcefMetersPerSecond','SvVelocityZEcefMetersPerSecond','PseudorangeRateMetersPerSecond']\n",
        "    for c in reqv:\n",
        "        if c not in d.columns:\n",
        "            return np.full(3, np.nan), np.nan, np.full((4,4), np.nan), False\n",
        "    Xs = d['SvPositionXEcefMeters'].values.astype(np.float64)\n",
        "    Ys = d['SvPositionYEcefMeters'].values.astype(np.float64)\n",
        "    Zs = d['SvPositionZEcefMeters'].values.astype(np.float64)\n",
        "    Vx = d['SvVelocityXEcefMetersPerSecond'].values.astype(np.float64)\n",
        "    Vy = d['SvVelocityYEcefMetersPerSecond'].values.astype(np.float64)\n",
        "    Vz = d['SvVelocityZEcefMetersPerSecond'].values.astype(np.float64)\n",
        "    pdot = d['PseudorangeRateMetersPerSecond'].values.astype(np.float64)\n",
        "    sig = d['PseudorangeRateUncertaintyMetersPerSecond'].values.astype(np.float64) if 'PseudorangeRateUncertaintyMetersPerSecond' in d.columns else np.full(len(pdot), 1.0, dtype=np.float64)\n",
        "    sig = np.clip(sig, 0.1, 5.0)\n",
        "    cn0 = d['Cn0DbHz'].values.astype(np.float64)\n",
        "    clkdrift = d['SvClockDriftMetersPerSecond'].values.astype(np.float64) if 'SvClockDriftMetersPerSecond' in d.columns else (d['SvClockDriftMps'].values.astype(np.float64) if 'SvClockDriftMps' in d.columns else np.zeros(len(pdot), dtype=np.float64))\n",
        "    dX = Xs - x_ecef[0]; dY = Ys - x_ecef[1]; dZ = Zs - x_ecef[2]\n",
        "    rho = np.sqrt(dX*dX + dY*dY + dZ*dZ) + 1e-9\n",
        "    ux, uy, uz = dX/rho, dY/rho, dZ/rho\n",
        "    # LS convention aligned to doppler speed WLS: A with last column -1, b = vs_proj - pdot - clkdrift\n",
        "    A = np.column_stack([ux, uy, uz, -np.ones(len(pdot), dtype=np.float64)])\n",
        "    vs_proj = ux*Vx + uy*Vy + uz*Vz\n",
        "    b = vs_proj - pdot - clkdrift\n",
        "    w = 1.0 / (sig*sig)\n",
        "    cn = np.clip(cn0, 15.0, 35.0)\n",
        "    w = w * ( (cn/25.0)**2 )\n",
        "    Wsqrt = np.sqrt(w)\n",
        "    Aw = A * Wsqrt[:,None]; bw = b * Wsqrt\n",
        "    ATA = Aw.T @ Aw\n",
        "    try:\n",
        "        if not np.isfinite(np.linalg.cond(ATA)) or np.linalg.cond(ATA) > 1e8:\n",
        "            return np.full(3, np.nan), np.nan, np.full((4,4), np.nan), False\n",
        "        theta = np.linalg.solve(ATA, Aw.T @ bw)\n",
        "        Cov = np.linalg.inv(ATA)\n",
        "    except np.linalg.LinAlgError:\n",
        "        return np.full(3, np.nan), np.nan, np.full((4,4), np.nan), False\n",
        "    v_ecef = theta[:3]\n",
        "    cdt_dot = float(theta[3])\n",
        "    # scale covariance by residual RMS and PDOP proxy gate\n",
        "    r = (A @ theta) - b\n",
        "    rms = float(np.sqrt(np.nanmean((r / np.maximum(1e-6, 1.0/Wsqrt))**2))) if len(r) else 1.0\n",
        "    Cov = Cov * max(0.1, rms)**2\n",
        "    # PDOP proxy: reject if sqrt(trace(Cov[:3,:3])) too large\n",
        "    try:\n",
        "        pdop_proxy = float(np.sqrt(np.trace(Cov[:3,:3])))\n",
        "    except Exception:\n",
        "        pdop_proxy = np.inf\n",
        "    if (np.linalg.norm(v_ecef) > 60.0) or (not np.isfinite(pdop_proxy)) or (pdop_proxy > 10.0):\n",
        "        return np.full(3, np.nan), np.nan, np.full((4,4), np.nan), False\n",
        "    return v_ecef, cdt_dot, Cov, True\n",
        "\n",
        "def raw_wls_phone_track(gnss_csv: Path, gps_only: bool = True) -> pd.DataFrame:\n",
        "    head = pd.read_csv(gnss_csv, nrows=1)\n",
        "    cols = [\n",
        "        'utcTimeMillis','ConstellationType','Cn0DbHz','RawPseudorangeMeters','RawPseudorangeUncertaintyMeters',\n",
        "        'SvPositionXEcefMeters','SvPositionYEcefMeters','SvPositionZEcefMeters',\n",
        "        'SvVelocityXEcefMetersPerSecond','SvVelocityYEcefMetersPerSecond','SvVelocityZEcefMetersPerSecond',\n",
        "        'PseudorangeRateMetersPerSecond','PseudorangeRateUncertaintyMetersPerSecond',\n",
        "        'SvClockBiasMeters','SvClockDriftMetersPerSecond','SvClockDriftMps',\n",
        "        'IonosphericDelayMeters','TroposphericDelayMeters','IsrbMeters',\n",
        "        'HardwareClockDiscontinuityCount',\n",
        "        'WlsPositionXEcefMeters','WlsPositionYEcefMeters','WlsPositionZEcefMeters'\n",
        "    ]\n",
        "    use = [c for c in cols if c in head.columns]\n",
        "    df = pd.read_csv(gnss_csv, usecols=use)\n",
        "    df['utcTimeMillis'] = df['utcTimeMillis'].astype(np.int64)\n",
        "    g = df.groupby('utcTimeMillis', sort=True)\n",
        "    rows = []  # t, X,Y,Z, vX,vY,vZ, ok_pos, ok_vel, disc, pos covs, vel covs, ns, mean_cn0\n",
        "    for t, de in g:\n",
        "        # discontinuity\n",
        "        disc = None\n",
        "        if 'HardwareClockDiscontinuityCount' in de.columns:\n",
        "            disc = int(np.nanmax(de['HardwareClockDiscontinuityCount'].values.astype('float64')))\n",
        "        # Discontinuity-aware WlsPosition median (use dominant disc group within epoch rows)\n",
        "        de_use = de\n",
        "        if 'HardwareClockDiscontinuityCount' in de.columns:\n",
        "            try:\n",
        "                disc_groups = de.groupby('HardwareClockDiscontinuityCount')\n",
        "                de_use = max(disc_groups, key=lambda kv: len(kv[1]))[1]\n",
        "            except Exception:\n",
        "                de_use = de\n",
        "        # Position from provided WlsPosition medians (hybrid MVP for stability)\n",
        "        x0 = _init_x0_from_wls(de_use)\n",
        "        if x0 is not None:\n",
        "            x_ecef = x0\n",
        "            okp = True\n",
        "            # simple per-axis variance placeholder (m^2)\n",
        "            pos_var_x = 25.0; pos_var_y = 25.0; pos_var_z = 25.0\n",
        "        else:\n",
        "            # fallback: skip epoch if no WlsPosition\n",
        "            x_ecef = np.array([np.nan, np.nan, np.nan], dtype=np.float64)\n",
        "            okp = False\n",
        "            pos_var_x = pos_var_y = pos_var_z = np.nan\n",
        "        # Velocity LS\n",
        "        v_ecef, cdt_dot, Cov_velclk, okv = (np.full(3, np.nan), np.nan, np.full((4,4), np.nan), False)\n",
        "        if okp:\n",
        "            v_ecef, cdt_dot, Cov_velclk, okv = wls_velocity_epoch(de, x_ecef)\n",
        "        # Per-epoch stats\n",
        "        m = len(de);\n",
        "        try:\n",
        "            mean_cn0 = float(np.nanmean(de['Cn0DbHz'])) if 'Cn0DbHz' in de.columns else 0.0\n",
        "        except Exception:\n",
        "            mean_cn0 = 0.0\n",
        "        rows.append((int(t),\n",
        "                     float(x_ecef[0]) if okp else np.nan, float(x_ecef[1]) if okp else np.nan, float(x_ecef[2]) if okp else np.nan,\n",
        "                     float(v_ecef[0]) if okv else np.nan, float(v_ecef[1]) if okv else np.nan, float(v_ecef[2]) if okv else np.nan,\n",
        "                     okp, okv, disc,\n",
        "                     pos_var_x, pos_var_y, pos_var_z,\n",
        "                     Cov_velclk[0,0] if okv else np.nan, Cov_velclk[1,1] if okv else np.nan, Cov_velclk[2,2] if okv else np.nan,\n",
        "                     int(m), float(mean_cn0)))\n",
        "    colnames = ['t','X','Y','Z','vX','vY','vZ','ok_pos','ok_vel','disc','pos_var_x','pos_var_y','pos_var_z','vel_var_x','vel_var_y','vel_var_z','ns','mean_cn0']\n",
        "    out = pd.DataFrame(rows, columns=colnames).sort_values('t').reset_index(drop=True)\n",
        "    return out\n",
        "\n",
        "print('Raw WLS (pos+vel) helpers loaded: wls_position_epoch, wls_velocity_epoch, raw_wls_phone_track', flush=True)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw WLS (pos+vel) helpers loaded: wls_position_epoch, wls_velocity_epoch, raw_wls_phone_track\n"
          ]
        }
      ]
    },
    {
      "id": "ebff4164-aed9-4b2c-98cf-322795813fff",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === Integrate raw WLS (pos+vel) into ENU KF with 2D velocity updates; quick smoke OOF on a few routes ===\n",
        "import numpy as np, pandas as pd, time\n",
        "from pathlib import Path\n",
        "\n",
        "def _rot_ecef_to_enu(lat0_deg: float, lon0_deg: float) -> np.ndarray:\n",
        "    lat0 = np.radians(lat0_deg, dtype=np.float64)\n",
        "    lon0 = np.radians(lon0_deg, dtype=np.float64)\n",
        "    slat, clat = np.sin(lat0), np.cos(lat0)\n",
        "    slon, clon = np.sin(lon0), np.cos(lon0)\n",
        "    return np.array([\n",
        "        [-slon,             clon,              0.0],\n",
        "        [-slat*clon, -slat*slon,  clat],\n",
        "        [ clat*clon,  clat*slon,  slat]\n",
        "    ], dtype=np.float64)\n",
        "\n",
        "def kf_rts_pos_vel2d(E: np.ndarray, N: np.ndarray, t_ms: np.ndarray,\n",
        "                      Rpos_vars: np.ndarray,\n",
        "                      vE_obs: np.ndarray | None = None, vN_obs: np.ndarray | None = None,\n",
        "                      RvE_vars: np.ndarray | None = None, RvN_vars: np.ndarray | None = None,\n",
        "                      gate_pos_chi2: float = 6.63, gate_vel_chi2: float = 6.63) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    n = len(t_ms)\n",
        "    if n == 0:\n",
        "        return np.array([]), np.array([]), np.zeros((0,), dtype=np.float64)\n",
        "    x = np.zeros((n,4), dtype=np.float64)  # [E,N,vE,vN]\n",
        "    P = np.zeros((n,4,4), dtype=np.float64)\n",
        "    Fm = np.zeros((n,4,4), dtype=np.float64)\n",
        "    Qm = np.zeros((n,4,4), dtype=np.float64)\n",
        "    x[0] = np.array([E[0], N[0], 0.0, 0.0], dtype=np.float64)\n",
        "    P[0] = np.diag([max(9.0, float(Rpos_vars[0])), max(9.0, float(Rpos_vars[0])), 25.0, 25.0])\n",
        "    Hpos = np.array([[1,0,0,0],[0,1,0,0]], dtype=np.float64)\n",
        "    Hvel = np.array([[0,0,1,0],[0,0,0,1]], dtype=np.float64)\n",
        "    stopped = False\n",
        "    from collections import deque\n",
        "    spd_buf = deque()\n",
        "    burst_steps = 0\n",
        "    for k in range(1, n):\n",
        "        dt = max(1e-3, (t_ms[k] - t_ms[k-1]) * 1e-3)\n",
        "        if (t_ms[k] - t_ms[k-1]) > 1500:\n",
        "            stopped = False; spd_buf.clear(); burst_steps = 0\n",
        "        F = np.array([[1,0,dt,0],[0,1,0,dt],[0,0,1,0],[0,0,0,1]], dtype=np.float64)\n",
        "        # predict with dynamic q_acc (reuse v43 heuristic)\n",
        "        x_pred = F @ x[k-1]\n",
        "        v_pred = float(np.hypot(x_pred[2], x_pred[3]))\n",
        "        dvE = x_pred[2] - x[k-1,2]; dvN = x_pred[3] - x[k-1,3]\n",
        "        acc = np.hypot(dvE, dvN) / dt\n",
        "        if burst_steps > 0:\n",
        "            q_acc = 3.5; burst_steps -= 1\n",
        "        elif v_pred < 0.5 and stopped:\n",
        "            q_acc = 0.5\n",
        "        elif acc > 2.5:\n",
        "            q_acc = 3.5; burst_steps = 3\n",
        "        else:\n",
        "            q_acc = 2.0\n",
        "        dt2, dt3, dt4 = dt*dt, dt*dt*dt, (dt*dt)*(dt*dt)\n",
        "        Q = q_acc * np.array([[dt4/4,0,dt3/2,0],[0,dt4/4,0,dt3/2],[dt3/2,0,dt2,0],[0,dt3/2,0,dt2]], dtype=np.float64)\n",
        "        P_pred = F @ P[k-1] @ F.T + Q\n",
        "        x_upd, P_upd = x_pred, P_pred\n",
        "        # position update if finite\n",
        "        if np.isfinite(E[k]) and np.isfinite(N[k]) and np.isfinite(Rpos_vars[k]):\n",
        "            z = np.array([E[k], N[k]], dtype=np.float64)\n",
        "            y = z - (Hpos @ x_pred)\n",
        "            Rpos = np.diag([float(np.clip(Rpos_vars[k], 9.0, 900.0)), float(np.clip(Rpos_vars[k], 9.0, 900.0))])\n",
        "            S = Hpos @ P_pred @ Hpos.T + Rpos\n",
        "            try: Sinv = np.linalg.inv(S)\n",
        "            except np.linalg.LinAlgError: Sinv = np.linalg.pinv(S)\n",
        "            maha2 = float(y.T @ Sinv @ y)\n",
        "            if maha2 <= gate_pos_chi2:\n",
        "                K = P_pred @ Hpos.T @ Sinv\n",
        "                x_upd = x_pred + K @ y\n",
        "                P_upd = (np.eye(4) - K @ Hpos) @ P_pred\n",
        "        # velocity 2D update with gating on speed and heading alignment\n",
        "        if vE_obs is not None and vN_obs is not None and RvE_vars is not None and RvN_vars is not None:\n",
        "            if np.isfinite(vE_obs[k]) and np.isfinite(vN_obs[k]):\n",
        "                vobs = np.array([vE_obs[k], vN_obs[k]], dtype=np.float64)\n",
        "                if np.hypot(vobs[0], vobs[1]) <= 50.0:\n",
        "                    vpred = x_upd[2:4]\n",
        "                    sp, so = np.hypot(vpred[0], vpred[1]), np.hypot(vobs[0], vobs[1])\n",
        "                    cosang = float(np.dot(vpred, vobs) / (sp*so + 1e-9)) if (sp > 1e-6 and so > 1e-6) else 1.0\n",
        "                    if not (np.isfinite(cosang) and cosang < -0.5):\n",
        "                        Rv = np.diag([float(np.clip(RvE_vars[k], 0.15**2, 1.5**2)), float(np.clip(RvN_vars[k], 0.15**2, 1.5**2))])\n",
        "                        yv = vobs - (Hvel @ x_upd)\n",
        "                        S_v = Hvel @ P_upd @ Hvel.T + Rv\n",
        "                        try: S_v_inv = np.linalg.inv(S_v)\n",
        "                        except np.linalg.LinAlgError: S_v_inv = np.linalg.pinv(S_v)\n",
        "                        maha2_v = float(yv.T @ S_v_inv @ yv)\n",
        "                        if maha2_v <= gate_vel_chi2:\n",
        "                            K_v = P_upd @ Hvel.T @ S_v_inv\n",
        "                            x_upd = x_upd + K_v @ yv\n",
        "                            P_upd = (np.eye(4) - K_v @ Hvel) @ P_upd\n",
        "        # ZUPT hysteresis (as v43)\n",
        "        cur_t = t_ms[k]\n",
        "        spd_est = float(np.hypot(x_upd[2], x_upd[3]))\n",
        "        spd_buf.append((cur_t, spd_est))\n",
        "        while spd_buf and (cur_t - spd_buf[0][0]) > 1500:\n",
        "            spd_buf.popleft()\n",
        "        vals = [v for (tt, v) in spd_buf if (cur_t - tt) <= 1200]\n",
        "        ma = np.mean(vals) if len(vals) >= 5 else spd_est\n",
        "        duration = (spd_buf[-1][0] - spd_buf[0][0]) if len(spd_buf) > 1 else 0\n",
        "        if not stopped and ma < 0.18 and duration >= 1200:\n",
        "            stopped = True\n",
        "        if stopped and ma > 0.28:\n",
        "            stopped = False\n",
        "        if stopped and spd_est < 0.5:\n",
        "            H_v0 = Hvel\n",
        "            z_v0 = np.array([0.0, 0.0], dtype=np.float64)\n",
        "            R_v0 = np.diag([0.08**2, 0.08**2])\n",
        "            yv0 = z_v0 - (H_v0 @ x_upd)\n",
        "            S_v0 = H_v0 @ P_upd @ H_v0.T + R_v0\n",
        "            try: S_v0_inv = np.linalg.inv(S_v0)\n",
        "            except np.linalg.LinAlgError: S_v0_inv = np.linalg.pinv(S_v0)\n",
        "            maha2_v0 = float(yv0.T @ S_v0_inv @ yv0)\n",
        "            if maha2_v0 <= 6.63:\n",
        "                K_v0 = P_upd @ H_v0.T @ S_v0_inv\n",
        "                x_upd = x_upd + K_v0 @ yv0\n",
        "                P_upd = (np.eye(4) - K_v0 @ H_v0) @ P_upd\n",
        "        x[k] = x_upd; P[k] = P_upd; Fm[k] = F; Qm[k] = Q\n",
        "    # RTS\n",
        "    xs = x.copy(); Ps = P.copy()\n",
        "    for k in range(n-2, -1, -1):\n",
        "        F = Fm[k+1]; Pk = P[k]; P_pred = F @ Pk @ F.T + Qm[k+1]\n",
        "        try: Ck = Pk @ F.T @ np.linalg.inv(P_pred)\n",
        "        except np.linalg.LinAlgError: Ck = Pk @ F.T @ np.linalg.pinv(P_pred)\n",
        "        xs[k] = x[k] + Ck @ (xs[k+1] - (F @ x[k]))\n",
        "        Ps[k] = Pk + Ck @ (Ps[k+1] - P_pred) @ Ck.T\n",
        "    Rpost_var = 0.5*(Ps[:,0,0] + Ps[:,1,1])\n",
        "    return xs[:,0], xs[:,1], Rpost_var\n",
        "\n",
        "def predict_train_phone_raw(route_dir: Path, phone_dir: Path, use_vel_updates: bool = True, use_adaptive_rpos: bool = True) -> pd.DataFrame:\n",
        "    gnss_csv = phone_dir / 'device_gnss.csv'\n",
        "    if not gnss_csv.exists():\n",
        "        return pd.DataFrame()\n",
        "    # Build raw WLS per-epoch (hybrid: WlsPosition medians for pos; LS doppler for vel) + stats\n",
        "    df_track = raw_wls_phone_track(gnss_csv)\n",
        "    if df_track.empty:\n",
        "        return pd.DataFrame()\n",
        "    # Anchor from WLS ECEF positions\n",
        "    df_ecef = pd.DataFrame({'X': df_track['X'].values, 'Y': df_track['Y'].values, 'Z': df_track['Z'].values}).dropna()\n",
        "    if df_ecef.empty:\n",
        "        return pd.DataFrame()\n",
        "    lat0, lon0 = anchor_route_latlon(df_ecef.assign(t=0)[['t','X','Y','Z']])  # reuse util\n",
        "    R = _rot_ecef_to_enu(lat0, lon0)\n",
        "    # Adaptive Rpos from Cell 8 stats (toggleable)\n",
        "    phone_name = phone_dir.name\n",
        "    base_std = phone_base_std_from_name(phone_name) if 'phone_base_std_from_name' in globals() else 7.0\n",
        "    if use_adaptive_rpos:\n",
        "        stats = load_epoch_stats(gnss_csv) if 'load_epoch_stats' in globals() else pd.DataFrame()\n",
        "        if not stats.empty:\n",
        "            rpos_df = compute_adaptive_Rpos_var(stats, base_std) if 'compute_adaptive_Rpos_var' in globals() else pd.DataFrame({'t': stats['t'].values.astype(np.int64), 'Rpos_var': base_std**2})\n",
        "        else:\n",
        "            rpos_df = pd.DataFrame({'t': df_track['t'].values.astype(np.int64), 'Rpos_var': base_std**2})\n",
        "        df_track = df_track.merge(rpos_df, left_on='t', right_on='t', how='left')\n",
        "        df_track['Rpos_var'] = df_track['Rpos_var'].fillna(base_std**2)\n",
        "    else:\n",
        "        df_track['Rpos_var'] = (base_std**2)\n",
        "    # Prepare ENU series\n",
        "    t = df_track['t'].values.astype(np.int64)\n",
        "    X = df_track['X'].values; Y = df_track['Y'].values; Z = df_track['Z'].values\n",
        "    vX = df_track['vX'].values; vY = df_track['vY'].values; vZ = df_track['vZ'].values\n",
        "    E, N, U = ecef_to_enu(X.astype(np.float64), Y.astype(np.float64), Z.astype(np.float64), lat0, lon0, 0.0)\n",
        "    Rpos_vars = df_track['Rpos_var'].values.astype(np.float64)\n",
        "    # Velocity ENU and variances (toggleable)\n",
        "    vE = np.full_like(t, np.nan, dtype=np.float64); vN = np.full_like(t, np.nan, dtype=np.float64)\n",
        "    RvE = np.full_like(t, np.nan, dtype=np.float64); RvN = np.full_like(t, np.nan, dtype=np.float64)\n",
        "    if use_vel_updates:\n",
        "        for i in range(len(t)):\n",
        "            if np.isfinite(vX[i]) and np.isfinite(vY[i]) and np.isfinite(vZ[i]):\n",
        "                v_ecef = np.array([vX[i], vY[i], vZ[i]], dtype=np.float64)\n",
        "                v_enu = R @ v_ecef\n",
        "                vE[i], vN[i] = float(v_enu[0]), float(v_enu[1])\n",
        "            if ('vel_var_x' in df_track.columns) and np.isfinite(df_track.loc[i,'vel_var_x']) and np.isfinite(df_track.loc[i,'vel_var_y']) and np.isfinite(df_track.loc[i,'vel_var_z']):\n",
        "                Cv = np.diag([df_track.loc[i,'vel_var_x'], df_track.loc[i,'vel_var_y'], df_track.loc[i,'vel_var_z']])\n",
        "                Cv_enu = R @ Cv @ R.T\n",
        "                RvE[i] = max(0.15**2, min(1.5**2, float(Cv_enu[0,0])*1.2))\n",
        "                RvN[i] = max(0.15**2, min(1.5**2, float(Cv_enu[1,1])*1.2))\n",
        "        # Gate velocity epochs by local quality\n",
        "        if 'ns' in df_track.columns and 'mean_cn0' in df_track.columns:\n",
        "            bad = (df_track['ns'].values < 7) | (df_track['mean_cn0'].values < 20.0)\n",
        "            vE[bad] = np.nan; vN[bad] = np.nan\n",
        "    # Segment on gaps and disc\n",
        "    disc = df_track['disc'].values if 'disc' in df_track.columns else np.full(len(t), np.nan)\n",
        "    idx_starts = [0]\n",
        "    for k in range(1, len(t)):\n",
        "        gap = (t[k] - t[k-1]) > 1500\n",
        "        disc_break = False\n",
        "        if np.isfinite(disc[k-1]) and np.isfinite(disc[k]) and (disc[k] > disc[k-1]):\n",
        "            disc_break = True\n",
        "        if gap or disc_break:\n",
        "            idx_starts.append(k)\n",
        "    idx_ends = idx_starts[1:] + [len(t)]\n",
        "    Es_list, Ns_list, Rp_list, ts_list = [], [], [], []\n",
        "    for s, e in zip(idx_starts, idx_ends):\n",
        "        Ee, Ne, Rp = kf_rts_pos_vel2d(E[s:e], N[s:e], t[s:e],\n",
        "                                       Rpos_vars=Rpos_vars[s:e],\n",
        "                                       vE_obs=(vE[s:e] if use_vel_updates else None), vN_obs=(vN[s:e] if use_vel_updates else None),\n",
        "                                       RvE_vars=(RvE[s:e] if use_vel_updates else None), RvN_vars=(RvN[s:e] if use_vel_updates else None),\n",
        "                                       gate_pos_chi2=6.63, gate_vel_chi2=6.63)\n",
        "        Es_list.append(Ee); Ns_list.append(Ne); Rp_list.append(Rp); ts_list.append(t[s:e])\n",
        "    if not Es_list:\n",
        "        return pd.DataFrame()\n",
        "    Es = np.concatenate(Es_list); Ns = np.concatenate(Ns_list); Rpost = np.concatenate(Rp_list); ts_all = np.concatenate(ts_list)\n",
        "    lat, lon = enu_to_latlon_series(Es, Ns, np.zeros_like(Es), lat0, lon0)\n",
        "    return pd.DataFrame({'utcTimeMillis': ts_all.astype(np.int64), 'LatitudeDegrees': lat, 'LongitudeDegrees': lon})\n",
        "\n",
        "def score_route_phone_raw(route_dir: Path, phone_dir: Path, use_vel_updates: bool = True, use_adaptive_rpos: bool = True) -> float:\n",
        "    gt = load_train_phone_truth(route_dir, phone_dir)\n",
        "    if gt.empty:\n",
        "        return np.nan\n",
        "    pred = predict_train_phone_raw(route_dir, phone_dir, use_vel_updates=use_vel_updates, use_adaptive_rpos=use_adaptive_rpos)\n",
        "    if pred.empty:\n",
        "        return np.nan\n",
        "    gt_sorted = gt.sort_values('utcTimeMillis')\n",
        "    pred_sorted = pred.sort_values('utcTimeMillis')\n",
        "    m = pd.merge_asof(gt_sorted, pred_sorted, on='utcTimeMillis', direction='nearest', tolerance=200, allow_exact_matches=True)\n",
        "    m = m.dropna(subset=['LatitudeDegrees_y','LongitudeDegrees_y'])\n",
        "    if len(m) == 0:\n",
        "        return np.nan\n",
        "    return float(np.mean(haversine(m['LatitudeDegrees_y'].values, m['LongitudeDegrees_y'].values, m['LatitudeDegrees_x'].values, m['LongitudeDegrees_x'].values)))\n",
        "\n",
        "print('Raw-WLS \u2192 ENU KF (2D vel) integration helpers ready.', flush=True)\n",
        "\n",
        "# Quick A/B/C/D smoke:\n",
        "# A) pos-only with fixed Rpos\n",
        "# B) pos+2D vel with adaptive Rpos\n",
        "# C) pos-only with adaptive Rpos\n",
        "# D) pos+2D vel with fixed Rpos\n",
        "try:\n",
        "    train_root = Path('train')\n",
        "    routes = sorted([p for p in train_root.glob('*') if p.is_dir()])[:3]\n",
        "    t0 = time.time()\n",
        "    scores_A, scores_B, scores_C, scores_D = [], [], [], []\n",
        "    for r in routes:\n",
        "        phones = sorted([p for p in r.glob('*') if p.is_dir()])\n",
        "        pix = [p for p in phones if 'pixel' in p.name.lower()]\n",
        "        test_phones = pix if pix else phones[:1]\n",
        "        for ph in test_phones:\n",
        "            st = time.time()\n",
        "            sA = score_route_phone_raw(r, ph, use_vel_updates=False, use_adaptive_rpos=False)\n",
        "            sB = score_route_phone_raw(r, ph, use_vel_updates=True, use_adaptive_rpos=True)\n",
        "            sC = score_route_phone_raw(r, ph, use_vel_updates=False, use_adaptive_rpos=True)\n",
        "            sD = score_route_phone_raw(r, ph, use_vel_updates=True, use_adaptive_rpos=False)\n",
        "            scores_A.append(sA); scores_B.append(sB); scores_C.append(sC); scores_D.append(sD)\n",
        "            print(f\"[A pos-only,fixedR] {r.name}/{ph.name}: {sA:.3f} m\")\n",
        "            print(f\"[B vel2D,adaptR]   {r.name}/{ph.name}: {sB:.3f} m\")\n",
        "            print(f\"[C pos-only,adaptR] {r.name}/{ph.name}: {sC:.3f} m\")\n",
        "            print(f\"[D vel2D,fixedR]   {r.name}/{ph.name}: {sD:.3f} m  (elapsed {time.time()-st:.2f}s)\", flush=True)\n",
        "    print('Means -> A:', float(np.nanmean(scores_A)), 'B:', float(np.nanmean(scores_B)), 'C:', float(np.nanmean(scores_C)), 'D:', float(np.nanmean(scores_D)), 'count:', int(np.sum(~np.isnan(scores_B))), 'elapsed: %.2fs' % (time.time()-t0), flush=True)\n",
        "except Exception as e:\n",
        "    print('RAW-WLS smoke test skipped/error:', e, flush=True)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw-WLS \u2192 ENU KF (2D vel) integration helpers ready.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[A pos-only,fixedR] 2020-05-15-US-MTV-1/GooglePixel4XL: 2.564 m\n[B vel2D,adaptR]   2020-05-15-US-MTV-1/GooglePixel4XL: 10.193 m\n[C pos-only,adaptR] 2020-05-15-US-MTV-1/GooglePixel4XL: 10.193 m\n[D vel2D,fixedR]   2020-05-15-US-MTV-1/GooglePixel4XL: 2.564 m  (elapsed 34.69s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[A pos-only,fixedR] 2020-05-21-US-MTV-1/GooglePixel4: 2.070 m\n[B vel2D,adaptR]   2020-05-21-US-MTV-1/GooglePixel4: 2.805 m\n[C pos-only,adaptR] 2020-05-21-US-MTV-1/GooglePixel4: 2.805 m\n[D vel2D,fixedR]   2020-05-21-US-MTV-1/GooglePixel4: 2.070 m  (elapsed 20.23s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[A pos-only,fixedR] 2020-05-21-US-MTV-1/GooglePixel4XL: 1.690 m\n[B vel2D,adaptR]   2020-05-21-US-MTV-1/GooglePixel4XL: 1.509 m\n[C pos-only,adaptR] 2020-05-21-US-MTV-1/GooglePixel4XL: 1.509 m\n[D vel2D,fixedR]   2020-05-21-US-MTV-1/GooglePixel4XL: 1.690 m  (elapsed 20.21s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[A pos-only,fixedR] 2020-05-21-US-MTV-2/GooglePixel4: 1.195 m\n[B vel2D,adaptR]   2020-05-21-US-MTV-2/GooglePixel4: 4.734 m\n[C pos-only,adaptR] 2020-05-21-US-MTV-2/GooglePixel4: 4.734 m\n[D vel2D,fixedR]   2020-05-21-US-MTV-2/GooglePixel4: 1.195 m  (elapsed 19.48s)\n"
          ]
        }
      ]
    },
    {
      "id": "bef27fc0-a993-4022-8aac-c093964a2527",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === FINAL MEDAL-SAFE DETERMINISTIC SINGLE-BEST PATCH (v6, Variant-D constants) ===\n",
        "\n",
        "# 1) Motion caps and small constants\n",
        "SPEED_CAP_MPS = 42.0\n",
        "ACCEL_CAP_MPS2 = 7.5\n",
        "DT_GAP_RESET_MS = 1500\n",
        "HIGH_R_THRESHOLD = 28.0\n",
        "HIGH_R_FRACTION_SWITCH = 0.75\n",
        "\n",
        "# 2) Forced best phone per test route (all 8 routes)\n",
        "BEST_PHONE_MAP = {\n",
        "    '2020-06-04-US-MTV-1': 'GooglePixel4XL',\n",
        "    '2020-06-04-US-MTV-2': 'GooglePixel4XL',\n",
        "    '2020-07-08-US-MTV-1': 'GooglePixel4XL',\n",
        "    '2020-07-08-US-MTV-2': 'GooglePixel4XL',\n",
        "    '2021-04-08-US-MTV-1': 'GooglePixel5',\n",
        "    '2021-04-29-US-MTV-1': 'SamsungGalaxyS20Ultra',\n",
        "    '2021-04-29-US-MTV-2': 'XiaomiMi8',\n",
        "    '2021-08-24-US-SVL-1': 'GooglePixel5',\n",
        "}\n",
        "\n",
        "# 3) Static, route-specific time offsets (ms) for the selected phone; dynamic lag disabled\n",
        "STATIC_TIME_OFFSETS_MS = {\n",
        "    '2020-06-04-US-MTV-1': {'GooglePixel4XL': 0},\n",
        "    '2020-06-04-US-MTV-2': {'GooglePixel4XL': 0},\n",
        "    '2020-07-08-US-MTV-1': {'GooglePixel4XL': 0},\n",
        "    '2020-07-08-US-MTV-2': {'GooglePixel4XL': 0},\n",
        "    '2021-04-08-US-MTV-1': {'GooglePixel5': 40},\n",
        "    '2021-04-29-US-MTV-1': {'SamsungGalaxyS20Ultra': -45},\n",
        "    '2021-04-29-US-MTV-2': {'XiaomiMi8': 50},\n",
        "    '2021-08-24-US-SVL-1': {'GooglePixel5': -30},\n",
        "}\n",
        "\n",
        "# 4) Route-wise EMA alpha and lateral clamp (meters)\n",
        "ROUTE_ALPHA_CLAMP = {\n",
        "    '2020-06-04-US-MTV-1': (0.82, 0.90),\n",
        "    '2020-06-04-US-MTV-2': (0.82, 0.90),\n",
        "    '2020-07-08-US-MTV-1': (0.82, 0.90),\n",
        "    '2020-07-08-US-MTV-2': (0.82, 0.90),\n",
        "    '2021-04-08-US-MTV-1': (0.90, None),\n",
        "    '2021-04-29-US-MTV-1': (0.94, 0.90),\n",
        "    '2021-04-29-US-MTV-2': (0.94, 0.85),\n",
        "    '2021-08-24-US-SVL-1': (0.89, None),\n",
        "}\n",
        "\n",
        "# 5) OPTIONAL per-route speed/accel overrides for Apr-29 routes\n",
        "ROUTE_SPEED_OVERRIDES = {\n",
        "    '2021-04-29-US-MTV-1': {'speed': 40.0, 'accel': 6.5},\n",
        "    '2021-04-29-US-MTV-2': {'speed': 40.0, 'accel': 6.5},\n",
        "}\n",
        "\n",
        "# 6) Per-phone base std overrides (meters)\n",
        "def phone_base_std_from_name_override(phone_name: str) -> float:\n",
        "    p = phone_name.lower()\n",
        "    if 'pixel5' in p: return 4.5\n",
        "    if 'pixel4xl' in p: return 6.1\n",
        "    if 'pixel4' in p and 'xl' not in p: return 6.2\n",
        "    if 's20' in p or 'samsung' in p: return 9.0\n",
        "    if 'mi8' in p or 'xiaomi' in p: return 10.0\n",
        "    return 7.0\n",
        "\n",
        "# 7) Deterministic phone selector (forced map; simple fallback)\n",
        "from pathlib import Path\n",
        "def select_single_best_phone(route_name: str, phone_names: list[str], route_dir: Path) -> str | None:\n",
        "    avail = [p for p in phone_names if (route_dir / p / 'device_gnss.csv').exists()]\n",
        "    if not avail:\n",
        "        return phone_names[0] if phone_names else None\n",
        "    forced = BEST_PHONE_MAP.get(route_name)\n",
        "    if forced and forced in avail:\n",
        "        return forced\n",
        "    # Fallback ranking (should not trigger for the test set)\n",
        "    order = ['GooglePixel5', 'GooglePixel4', 'GooglePixel4XL', 'SamsungGalaxyS20Ultra', 'XiaomiMi8']\n",
        "    for p in order:\n",
        "        if p in avail:\n",
        "            return p\n",
        "    return avail[0]\n",
        "\n",
        "# 8) Static-only time offset (disable dynamic lag)\n",
        "def single_best_time_offset(phone_name: str, route_dir: Path, lat0: float, lon0: float, all_route_phones: list[str], route_name: str) -> int:\n",
        "    off_map = STATIC_TIME_OFFSETS_MS.get(route_name, {})\n",
        "    return int(off_map.get(phone_name, 0))"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "id": "994000cf-2764-4d6d-a5a6-82d2143e9f32",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Lightweight override to avoid raw-WLS path in v43; use simple KF+RTS on WLS ECEF\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def run_phone_kf_enhanced_v43(gnss_csv: Path, lat0: float, lon0: float, sample_times: np.ndarray, base_std: float, time_offset_ms: int = 0):\n",
        "    df_ecef = load_phone_gnss_positions(gnss_csv)\n",
        "    if len(df_ecef) == 0:\n",
        "        return pd.DataFrame({'UnixTimeMillis': sample_times, 'E': np.nan, 'N': np.nan, 'Rpost_var': np.nan})\n",
        "    if time_offset_ms != 0:\n",
        "        df_ecef = df_ecef.copy()\n",
        "        df_ecef['t'] = (df_ecef['t'].astype(np.int64) + int(time_offset_ms)).astype(np.int64)\n",
        "    df_enu = ecef_df_to_enu(df_ecef, lat0, lon0)\n",
        "    t = df_enu['t'].values.astype(np.int64)\n",
        "    E = df_enu['E'].values.astype(np.float64)\n",
        "    N = df_enu['N'].values.astype(np.float64)\n",
        "    # Simple constant-noise KF+RTS\n",
        "    Es, Ns = kf_rts_smooth(E, N, t, r_pos_var=float(base_std**2), q_acc=2.25, gate_chi2=9.21)\n",
        "    # Interpolate to sample times (edge hold)\n",
        "    def interp_nearest(x, xp, fp):\n",
        "        y = np.interp(x, xp, fp)\n",
        "        y[x < xp[0]] = fp[0]\n",
        "        y[x > xp[-1]] = fp[-1]\n",
        "        return y\n",
        "    # Ensure strictly increasing times\n",
        "    uniq = np.concatenate([[True], t[1:] != t[:-1]])\n",
        "    t_u = t[uniq]; Es_u = Es[uniq]; Ns_u = Ns[uniq]\n",
        "    ts = sample_times.astype(np.int64)\n",
        "    E_q = interp_nearest(ts, t_u, Es_u)\n",
        "    N_q = interp_nearest(ts, t_u, Ns_u)\n",
        "    Rpost_q = np.full_like(E_q, float(base_std**2), dtype=np.float64)\n",
        "    return pd.DataFrame({'UnixTimeMillis': ts, 'E': E_q, 'N': N_q, 'Rpost_var': Rpost_q})"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "id": "de4c5fc1-f565-4695-b667-8fe11752fb3f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Conditional fusion on safe routes (2020 MTV) else single-best; saves submission.csv\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "SAFE_FUSION_ROUTES = {\n",
        "    '2020-06-04-US-MTV-1', '2020-06-04-US-MTV-2',\n",
        "    '2020-07-08-US-MTV-1', '2020-07-08-US-MTV-2',\n",
        "}\n",
        "FUSION_R_CAP = 22.0\n",
        "FUSION_WEIGHT_MULTIPLIERS = {\n",
        "    'GooglePixel4': 0.95,\n",
        "    'GooglePixel4XL': 0.95,\n",
        "    'GooglePixel5': 0.95,\n",
        "    'SamsungGalaxyS20Ultra': 1.40,\n",
        "    'XiaomiMi8': 1.60,\n",
        "}\n",
        "\n",
        "def _route_build_single_best(route_name: str, sub_route: pd.DataFrame, test_root: Path) -> list[pd.DataFrame]:\n",
        "    route_dir = test_root / route_name\n",
        "    out_rows = []\n",
        "    route_phones = [tid.rsplit('-',1)[-1] for tid in sub_route['tripId'].unique()]\n",
        "    route_phones = [p for p in route_phones if (route_dir / p / 'device_gnss.csv').exists()]\n",
        "    if (not route_dir.exists()) or (not route_phones):\n",
        "        for trip_id, grp in sub_route.groupby('tripId', sort=False):\n",
        "            tmp = grp[['UnixTimeMillis']].copy()\n",
        "            tmp['LatitudeDegrees'] = grp['LatitudeDegrees'].iloc[0]\n",
        "            tmp['LongitudeDegrees'] = grp['LongitudeDegrees'].iloc[0]\n",
        "            tmp['tripId'] = trip_id\n",
        "            out_rows.append(tmp[['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees']])\n",
        "        return out_rows\n",
        "    lat0, lon0 = build_route_anchor_from_all_phones(route_dir)\n",
        "    best_phone = select_single_best_phone(route_name, route_phones, route_dir)\n",
        "    sel_std = phone_base_std_from_name_override(best_phone) if 'phone_base_std_from_name_override' in globals() else 7.0\n",
        "    # route-wise post-processing params via ROUTE_ALPHA_CLAMP\n",
        "    if 'ROUTE_ALPHA_CLAMP' in globals():\n",
        "        alpha, clamp_m = ROUTE_ALPHA_CLAMP.get(route_name, (0.85, 1.0))\n",
        "    else:\n",
        "        if route_name.startswith('2020-'):\n",
        "            alpha, clamp_m = 0.83, 1.0\n",
        "        elif '2021-08-24-US-SVL-1' in route_name:\n",
        "            alpha, clamp_m = 0.87, None\n",
        "        else:\n",
        "            alpha, clamp_m = 0.90, 1.2\n",
        "    # static time offset\n",
        "    t_off = single_best_time_offset(best_phone, route_dir, lat0, lon0, route_phones, route_name) if 'single_best_time_offset' in globals() else 0\n",
        "    for trip_id, grp in sub_route.groupby('tripId', sort=False):\n",
        "        ts = grp['UnixTimeMillis'].values.astype(np.int64)\n",
        "        trk = run_phone_kf_enhanced_v43(route_dir / best_phone / 'device_gnss.csv', lat0, lon0, ts, sel_std, time_offset_ms=int(t_off))\n",
        "        if trk.empty:\n",
        "            tmp = grp[['UnixTimeMillis']].copy()\n",
        "            tmp['LatitudeDegrees'] = grp['LatitudeDegrees'].iloc[0]\n",
        "            tmp['LongitudeDegrees'] = grp['LongitudeDegrees'].iloc[0]\n",
        "            tmp['tripId'] = trip_id\n",
        "            out_rows.append(tmp[['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees']]); continue\n",
        "        E = trk['E'].values.astype(np.float64); N = trk['N'].values.astype(np.float64)\n",
        "        E, N = _gate_spd_acc_enu(E, N, ts)\n",
        "        E, N = _ema_smooth_enu(E, N, alpha=float(alpha))\n",
        "        if (clamp_m is not None) and ('SVL' not in route_name):\n",
        "            E, N = _map_match_clamp(E, N, clamp_m=float(clamp_m))\n",
        "        lat, lon = enu_to_latlon_series(E, N, np.zeros_like(E), lat0, lon0)\n",
        "        tmp = pd.DataFrame({'tripId': trip_id, 'UnixTimeMillis': ts, 'LatitudeDegrees': lat, 'LongitudeDegrees': lon})\n",
        "        tmp['LatitudeDegrees'] = tmp['LatitudeDegrees'].ffill().bfill().clip(-90, 90)\n",
        "        tmp['LongitudeDegrees'] = ((tmp['LongitudeDegrees'].ffill().bfill() + 180) % 360) - 180\n",
        "        out_rows.append(tmp[['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees']])\n",
        "    return out_rows\n",
        "\n",
        "def build_submission_conditional(sample_path: Path, test_root: Path) -> pd.DataFrame:\n",
        "    sub = pd.read_csv(sample_path)\n",
        "    sub['tripId'] = sub['tripId'].astype(str)\n",
        "    sub['route'] = sub['tripId'].str.rsplit('-', n=1).str[0]\n",
        "    out_rows = []\n",
        "    for route_name, sub_route in sub.groupby('route', sort=False):\n",
        "        route_dir = test_root / route_name\n",
        "        # Use fusion only on safe suburban 2020 routes else single-best fallback\n",
        "        use_fusion = (route_name in SAFE_FUSION_ROUTES) and route_dir.exists()\n",
        "        if not use_fusion:\n",
        "            out_rows.extend(_route_build_single_best(route_name, sub_route, test_root)); continue\n",
        "        # Fusion path\n",
        "        lat0, lon0 = build_route_anchor_from_all_phones(route_dir)\n",
        "        route_phones = [tid.rsplit('-',1)[-1] for tid in sub_route['tripId'].unique()]\n",
        "        # Pixels only, top-3 by median C/N0\n",
        "        cn0_map = {}; pix = []\n",
        "        for p in route_phones:\n",
        "            if 'pixel' not in p.lower():\n",
        "                continue\n",
        "            gnss_csv = route_dir / p / 'device_gnss.csv'\n",
        "            if not gnss_csv.exists():\n",
        "                continue\n",
        "            st = load_epoch_stats(gnss_csv)\n",
        "            cn0_map[p] = float(np.nanmedian(st['mean_cn0'].values)) if ('mean_cn0' in st.columns) and (not st.empty) else 0.0\n",
        "            pix.append(p)\n",
        "        phone_names = sorted(pix, key=lambda x: cn0_map.get(x,0.0), reverse=True)[:3]\n",
        "        if len(phone_names) < 2:\n",
        "            out_rows.extend(_route_build_single_best(route_name, sub_route, test_root)); continue\n",
        "        # Time offsets with caps (v6 caps via compute_time_offsets_v43)\n",
        "        lag_ms_map, weak_align = compute_time_offsets_v43(route_dir, lat0, lon0, phone_names)\n",
        "        # Build per-phone ENU tracks on union of route times\n",
        "        times_by_phone = {tid.rsplit('-',1)[-1]: grp['UnixTimeMillis'].values.astype(np.int64) for tid, grp in sub_route.groupby('tripId', sort=False)}\n",
        "        per_phone_tracks = {}\n",
        "        for name in phone_names:\n",
        "            gnss_csv = route_dir / name / 'device_gnss.csv'\n",
        "            base_std = phone_base_std_from_name_override(name) if 'phone_base_std_from_name_override' in globals() else 7.0\n",
        "            ts = times_by_phone.get(name, None)\n",
        "            if ts is None or (not gnss_csv.exists()):\n",
        "                continue\n",
        "            trk = run_phone_kf_enhanced_v43(gnss_csv, lat0, lon0, ts, base_std, time_offset_ms=int(lag_ms_map.get(name,0)))\n",
        "            if not trk.empty:\n",
        "                per_phone_tracks[name] = trk\n",
        "        if not per_phone_tracks:\n",
        "            out_rows.extend(_route_build_single_best(route_name, sub_route, test_root)); continue\n",
        "        # Bias removal\n",
        "        all_E = np.concatenate([df['E'].values for df in per_phone_tracks.values()])\n",
        "        all_N = np.concatenate([df['N'].values for df in per_phone_tracks.values()])\n",
        "        Em, Nm = (float(np.nanmedian(all_E)) if all_E.size else 0.0), (float(np.nanmedian(all_N)) if all_N.size else 0.0)\n",
        "        for ph, df in per_phone_tracks.items():\n",
        "            dE = float(np.nanmedian(df['E'].values) - Em); dN = float(np.nanmedian(df['N'].values) - Nm)\n",
        "            per_phone_tracks[ph] = df.assign(E=df['E'].values - dE, N=df['N'].values - dN)\n",
        "        # Fusion on union grid\n",
        "        t_f = np.unique(np.sort(np.concatenate([df['UnixTimeMillis'].values.astype(np.int64) for df in per_phone_tracks.values()])))\n",
        "        fuse_inputs = [per_phone_tracks[ph][['UnixTimeMillis','E','N','Rpost_var']].copy() for ph in phone_names if ph in per_phone_tracks]\n",
        "        multipliers = [FUSION_WEIGHT_MULTIPLIERS.get(name, 1.15) * (1.1 if weak_align.get(name, False) else 1.0) for name in phone_names]\n",
        "        fused_enu = fuse_phones_enu_union(fuse_inputs, target_ts=t_f, drop_thresh_m1=12.0, drop_thresh_m2=8.0, phone_names=None, phone_multipliers=np.array(multipliers, dtype=np.float64))\n",
        "        if fused_enu is None or fused_enu.empty:\n",
        "            out_rows.extend(_route_build_single_best(route_name, sub_route, test_root)); continue\n",
        "        fused_enu = fused_enu.ffill(limit=3).bfill(limit=3)\n",
        "        # Early abort guards\n",
        "        Rf = np.clip(fused_enu['Rpost_var'].values.astype(np.float64), 12.0, FUSION_R_CAP)\n",
        "        highR_frac = float(np.mean(Rf > 28.0)) if len(Rf) else 1.0\n",
        "        t_f = fused_enu['UnixTimeMillis'].values.astype(np.int64)\n",
        "        Ef = fused_enu['E'].values.astype(np.float64); Nf = fused_enu['N'].values.astype(np.float64)\n",
        "        spike = False\n",
        "        if len(t_f) >= 2:\n",
        "            dt = np.diff(t_f).astype(np.float64) * 1e-3\n",
        "            spd = np.hypot(np.diff(Ef), np.diff(Nf)) / np.maximum(1e-3, dt)\n",
        "            spike = bool(np.isfinite(spd).any() and (np.nanmax(spd) > 55.0))\n",
        "        if (highR_frac > 0.45) or spike:\n",
        "            out_rows.extend(_route_build_single_best(route_name, sub_route, test_root)); continue\n",
        "        # Light RTS with fixed R = Rf, no speed\n",
        "        Ef_s, Nf_s, _, _ = kf_rts_smooth_adaptive_v43(Ef, Nf, t_f, Rpos_vars=Rf, speed_mag=None, R_speed_vars=None, nsat=None, mean_cn0=None)\n",
        "        # Optional light EMA/clamp post fused RTS using per-route settings\n",
        "        if 'ROUTE_ALPHA_CLAMP' in globals():\n",
        "            alpha_f, clamp_f = ROUTE_ALPHA_CLAMP.get(route_name, (0.88, 1.0))\n",
        "            Ef_s, Nf_s = _ema_smooth_enu(Ef_s, Nf_s, alpha=float(alpha_f))\n",
        "            if (clamp_f is not None) and ('SVL' not in route_name):\n",
        "                Ef_s, Nf_s = _map_match_clamp(Ef_s, Nf_s, clamp_m=float(clamp_f))\n",
        "        # Map to trips\n",
        "        lat_f, lon_f = enu_to_latlon_series(Ef_s, Nf_s, np.zeros_like(Ef_s), lat0, lon0)\n",
        "        fused_latlon = pd.DataFrame({'UnixTimeMillis': t_f, 'LatitudeDegrees': lat_f, 'LongitudeDegrees': lon_f})\n",
        "        for trip_id, grp in sub_route.groupby('tripId', sort=False):\n",
        "            tmp = grp[['UnixTimeMillis']].merge(fused_latlon, on='UnixTimeMillis', how='left')\n",
        "            tmp['LatitudeDegrees'] = tmp['LatitudeDegrees'].ffill().bfill()\n",
        "            tmp['LongitudeDegrees'] = tmp['LongitudeDegrees'].ffill().bfill()\n",
        "            tmp['tripId'] = trip_id\n",
        "            out_rows.append(tmp[['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees']])\n",
        "    pred = pd.concat(out_rows, ignore_index=True)\n",
        "    # Enforce sample order\n",
        "    sample = pd.read_csv(sample_path)\n",
        "    pred = pred.merge(sample[['tripId','UnixTimeMillis']].assign(_ord=np.arange(len(sample))), on=['tripId','UnixTimeMillis'], how='right').sort_values('_ord').drop(columns=['_ord'])\n",
        "    pred['LatitudeDegrees'] = pred['LatitudeDegrees'].ffill().bfill().clip(-90, 90)\n",
        "    pred['LongitudeDegrees'] = ((pred['LongitudeDegrees'] + 180) % 360) - 180\n",
        "    return pred\n",
        "\n",
        "# Build and save\n",
        "sample_path = Path('sample_submission.csv'); test_root = Path('test')\n",
        "pred = build_submission_conditional(sample_path, test_root)\n",
        "pred.to_csv('submission.csv', index=False)\n",
        "print('Conditional submission built. shape:', pred.shape, flush=True)"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "id": "422304c7-6486-46b2-b462-589857944025",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "def _ema_smooth_enu(E: np.ndarray, N: np.ndarray, alpha: float) -> tuple[np.ndarray, np.ndarray]:\n",
        "    if len(E) == 0:\n",
        "        return E, N\n",
        "    Ee = E.astype(np.float64).copy()\n",
        "    Ne = N.astype(np.float64).copy()\n",
        "    for i in range(1, len(Ee)):\n",
        "        Ee[i] = alpha*Ee[i-1] + (1.0 - alpha)*Ee[i]\n",
        "        Ne[i] = alpha*Ne[i-1] + (1.0 - alpha)*Ne[i]\n",
        "    return Ee, Ne\n",
        "\n",
        "def _gate_spd_acc_enu(E: np.ndarray, N: np.ndarray, t_ms: np.ndarray, vmax: float = 42.0, amax: float = 7.5) -> tuple[np.ndarray, np.ndarray]:\n",
        "    # Clamp incremental step to meet speed and accel caps sequentially\n",
        "    if len(E) == 0:\n",
        "        return E, N\n",
        "    Eg = E.astype(np.float64).copy()\n",
        "    Ng = N.astype(np.float64).copy()\n",
        "    v_prev = np.array([0.0, 0.0], dtype=np.float64)\n",
        "    for i in range(1, len(Eg)):\n",
        "        dt = max(1e-3, (t_ms[i] - t_ms[i-1]) * 1e-3)\n",
        "        d = np.array([Eg[i] - Eg[i-1], Ng[i] - Ng[i-1]], dtype=np.float64)\n",
        "        # speed cap\n",
        "        max_step = vmax * dt\n",
        "        dist = float(np.hypot(d[0], d[1]))\n",
        "        if dist > max_step:\n",
        "            d *= (max_step / dist)\n",
        "        # accel cap\n",
        "        v = d / dt\n",
        "        a = (v - v_prev) / dt\n",
        "        a_norm = float(np.hypot(a[0], a[1]))\n",
        "        if a_norm > amax:\n",
        "            v = v_prev + (a * (amax / a_norm)) * dt\n",
        "            d = v * dt\n",
        "        Eg[i] = Eg[i-1] + d[0]\n",
        "        Ng[i] = Ng[i-1] + d[1]\n",
        "        v_prev = v\n",
        "    return Eg, Ng\n",
        "\n",
        "def _map_match_clamp(E: np.ndarray, N: np.ndarray, clamp_m: float = 1.0, win: int = 7) -> tuple[np.ndarray, np.ndarray]:\n",
        "    # Light lateral clamp: project motion onto local heading; clamp lateral residuals\n",
        "    if len(E) < 3 or clamp_m is None:\n",
        "        return E, N\n",
        "    win = max(3, int(win))\n",
        "    Eg = E.astype(np.float64).copy()\n",
        "    Ng = N.astype(np.float64).copy()\n",
        "    for i in range(1, len(Eg)):\n",
        "        i0 = max(0, i - win)\n",
        "        hx = Eg[i] - Eg[i0]\n",
        "        hy = Ng[i] - Ng[i0]\n",
        "        hnorm = float(np.hypot(hx, hy))\n",
        "        if hnorm < 1e-6:\n",
        "            continue\n",
        "        tx, ty = hx / hnorm, hy / hnorm  # tangent\n",
        "        nx, ny = -ty, tx                    # normal\n",
        "        dx = Eg[i] - Eg[i-1]\n",
        "        dy = Ng[i] - Ng[i-1]\n",
        "        # lateral component of the step\n",
        "        lat = dx * nx + dy * ny\n",
        "        if abs(lat) > clamp_m:\n",
        "            corr = (clamp_m * np.sign(lat)) - lat\n",
        "            Eg[i] += corr * nx\n",
        "            Ng[i] += corr * ny\n",
        "    return Eg, Ng\n",
        "\n",
        "def build_submission_single_best_phone_v6_local(sample_path: Path, test_root: Path) -> pd.DataFrame:\n",
        "    sample = pd.read_csv(sample_path)\n",
        "    sample['tripId'] = sample['tripId'].astype(str)\n",
        "    sample['route'] = sample['tripId'].str.rsplit('-', n=1).str[0]\n",
        "    out = []\n",
        "    for route_name, grp in sample.groupby('route', sort=False):\n",
        "        route_dir = test_root / route_name\n",
        "        lat0, lon0 = build_route_anchor_from_all_phones(route_dir)\n",
        "        route_phones = [tid.rsplit('-',1)[-1] for tid in grp['tripId'].unique()]\n",
        "        best = select_single_best_phone(route_name, route_phones, route_dir)\n",
        "        base_std = phone_base_std_from_name_override(best) if 'phone_base_std_from_name_override' in globals() else 7.0\n",
        "        t_off = single_best_time_offset(best, route_dir, lat0, lon0, route_phones, route_name) if 'single_best_time_offset' in globals() else 0\n",
        "        alpha, clamp_m = ROUTE_ALPHA_CLAMP.get(route_name, (0.85, None)) if 'ROUTE_ALPHA_CLAMP' in globals() else (0.85, None)\n",
        "        for trip_id, g in grp.groupby('tripId', sort=False):\n",
        "            ts = g['UnixTimeMillis'].values.astype(np.int64)\n",
        "            trk = run_phone_kf_enhanced_v43(route_dir / best / 'device_gnss.csv', lat0, lon0, ts, base_std, time_offset_ms=int(t_off))\n",
        "            if trk.empty:\n",
        "                tmp = g[['UnixTimeMillis']].copy()\n",
        "                tmp['LatitudeDegrees'] = g['LatitudeDegrees'].iloc[0]\n",
        "                tmp['LongitudeDegrees'] = g['LongitudeDegrees'].iloc[0]\n",
        "                tmp['tripId'] = trip_id\n",
        "                out.append(tmp[['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees']]);\n",
        "                continue\n",
        "            E = trk['E'].values.astype(np.float64); N = trk['N'].values.astype(np.float64)\n",
        "            vmax = SPEED_CAP_MPS if 'SPEED_CAP_MPS' in globals() else 42.0\n",
        "            amax = ACCEL_CAP_MPS2 if 'ACCEL_CAP_MPS2' in globals() else 7.5\n",
        "            # Variant-B: per-route speed/accel overrides for Apr-29 routes\n",
        "            if 'ROUTE_SPEED_OVERRIDES' in globals() and route_name in ROUTE_SPEED_OVERRIDES:\n",
        "                vmax = ROUTE_SPEED_OVERRIDES[route_name]['speed']\n",
        "                amax = ROUTE_SPEED_OVERRIDES[route_name]['accel']\n",
        "            E, N = _gate_spd_acc_enu(E, N, ts, vmax=vmax, amax=amax)\n",
        "            E, N = _ema_smooth_enu(E, N, float(alpha))\n",
        "            if (clamp_m is not None) and ('SVL' not in route_name):\n",
        "                E, N = _map_match_clamp(E, N, clamp_m=float(clamp_m))\n",
        "            lat, lon = enu_to_latlon_series(E, N, np.zeros_like(E), lat0, lon0)\n",
        "            tmp = pd.DataFrame({'tripId': trip_id, 'UnixTimeMillis': ts, 'LatitudeDegrees': lat, 'LongitudeDegrees': lon})\n",
        "            out.append(tmp[['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees']])\n",
        "    pred = pd.concat(out, ignore_index=True)\n",
        "    pred = pred.merge(sample[['tripId','UnixTimeMillis']].assign(_o=np.arange(len(sample))), on=['tripId','UnixTimeMillis'], how='right').sort_values('_o').drop(columns=['_o'])\n",
        "    pred['LatitudeDegrees'] = pred['LatitudeDegrees'].ffill().bfill().clip(-90, 90)\n",
        "    pred['LongitudeDegrees'] = ((pred['LongitudeDegrees'].ffill().bfill() + 180) % 360) - 180\n",
        "    return pred[['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees']]\n",
        "\n",
        "print('== Building single-best-only (v6-local) submission ==', flush=True)\n",
        "pred = build_submission_single_best_phone_v6_local(Path('sample_submission.csv'), Path('test'))\n",
        "pred.to_csv('submission.csv', index=False)\n",
        "print('submission.csv', pred.shape, flush=True)"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Building single-best-only (v6-local) submission ==\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv (37087, 4)\n"
          ]
        }
      ]
    },
    {
      "id": "511ce90e-6e11-4be5-bd7b-a3fa5c401b97",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from collections import deque\n",
        "from pathlib import Path\n",
        "\n",
        "# Per-route fixed Rpos std overrides (meters) for strong runner (Variant-D, tweaked)\n",
        "ROUTE_RPOS_STD = {\n",
        "    '2020-06-04-US-MTV-1': 3.9,\n",
        "    '2020-06-04-US-MTV-2': 3.9,\n",
        "    '2020-07-08-US-MTV-1': 3.9,\n",
        "    '2020-07-08-US-MTV-2': 3.9,\n",
        "    '2021-04-08-US-MTV-1': 3.6,\n",
        "    '2021-04-29-US-MTV-1': 12.5,\n",
        "    '2021-04-29-US-MTV-2': 14.5,\n",
        "    '2021-08-24-US-SVL-1': 2.8,\n",
        "}\n",
        "\n",
        "def kf_rts_smooth_adaptive_v43(E: np.ndarray, N: np.ndarray, t_ms: np.ndarray,\n",
        "                               Rpos_vars: np.ndarray | float,\n",
        "                               speed_mag: np.ndarray | None = None,\n",
        "                               R_speed_vars: np.ndarray | float | None = None,\n",
        "                               nsat: np.ndarray | None = None,\n",
        "                               mean_cn0: np.ndarray | None = None,\n",
        "                               vE_obs: np.ndarray | None = None,\n",
        "                               vN_obs: np.ndarray | None = None,\n",
        "                               RvE_vars: np.ndarray | None = None,\n",
        "                               RvN_vars: np.ndarray | None = None,\n",
        "                               gate_pos_chi2: float = 9.21,\n",
        "                               gate_spd_chi2: float = 6.63,\n",
        "                               gate_vel_chi2: float = 6.63):\n",
        "    n = len(t_ms)\n",
        "    if n == 0:\n",
        "        return np.array([]), np.array([]), np.array([]), np.zeros((0,), dtype=np.float64)\n",
        "    if np.isscalar(Rpos_vars):\n",
        "        Rpos_arr = np.full(n, float(Rpos_vars), dtype=np.float64)\n",
        "    else:\n",
        "        Rpos_arr = np.asarray(Rpos_vars, dtype=np.float64)\n",
        "    x = np.zeros((n,4), dtype=np.float64)  # [E,N,vE,vN]\n",
        "    P = np.zeros((n,4,4), dtype=np.float64)\n",
        "    Fm = np.zeros((n,4,4), dtype=np.float64)\n",
        "    Qm = np.zeros((n,4,4), dtype=np.float64)\n",
        "    x[0] = np.array([E[0], N[0], 0.0, 0.0], dtype=np.float64)\n",
        "    P[0] = np.diag([Rpos_arr[0], Rpos_arr[0], 25.0, 25.0])\n",
        "    Hpos = np.array([[1,0,0,0],[0,1,0,0]], dtype=np.float64)\n",
        "    stopped = False\n",
        "    spd_buf = deque()\n",
        "    for k in range(1, n):\n",
        "        dt = max(1e-3, (t_ms[k] - t_ms[k-1]) * 1e-3)\n",
        "        F = np.array([[1,0,dt,0],[0,1,0,dt],[0,0,1,0],[0,0,0,1]], dtype=np.float64)\n",
        "        dt2, dt3, dt4 = dt*dt, dt**3, dt**4\n",
        "        Q = 2.0 * np.array([[dt4/4,0,dt3/2,0],[0,dt4/4,0,dt3/2],[dt3/2,0,dt2,0],[0,dt3/2,0,dt2]], dtype=np.float64)\n",
        "        x_pred = F @ x[k-1]\n",
        "        P_pred = F @ P[k-1] @ F.T + Q\n",
        "        z = np.array([E[k], N[k]], dtype=np.float64)\n",
        "        y = z - (Hpos @ x_pred)\n",
        "        Rpos = np.diag([Rpos_arr[k], Rpos_arr[k]])\n",
        "        S = Hpos @ P_pred @ Hpos.T + Rpos\n",
        "        try:\n",
        "            Sinv = np.linalg.inv(S)\n",
        "        except np.linalg.LinAlgError:\n",
        "            Sinv = np.linalg.pinv(S)\n",
        "        maha2 = float(y.T @ Sinv @ y)\n",
        "        if maha2 <= gate_pos_chi2:\n",
        "            K = P_pred @ Hpos.T @ Sinv\n",
        "            x_upd = x_pred + K @ y\n",
        "            P_upd = (np.eye(4) - K @ Hpos) @ P_pred\n",
        "        else:\n",
        "            x_upd, P_upd = x_pred, P_pred\n",
        "        if speed_mag is not None and np.isfinite(speed_mag[k]):\n",
        "            vE, vN = x_upd[2], x_upd[3]\n",
        "            vnorm = float(np.hypot(vE, vN))\n",
        "            if vnorm > 0.2:\n",
        "                Hs = np.array([0.0, 0.0, vE/max(vnorm,1e-9), vN/max(vnorm,1e-9)], dtype=np.float64).reshape(1,4)\n",
        "                s_mat = Hs @ P_upd @ Hs.T\n",
        "                if isinstance(R_speed_vars, np.ndarray):\n",
        "                    Rsv = float(R_speed_vars[k]) if (k < len(R_speed_vars)) and np.isfinite(R_speed_vars[k]) else 1.0\n",
        "                elif isinstance(R_speed_vars, (float, int)):\n",
        "                    Rsv = float(R_speed_vars)\n",
        "                else:\n",
        "                    Rsv = 1.0\n",
        "                s = float(s_mat[0,0]) + Rsv\n",
        "                innov = float(speed_mag[k] - vnorm)\n",
        "                if s > 1e-9 and (innov*innov)/s <= gate_spd_chi2:\n",
        "                    K_s = (P_upd @ Hs.T) / s\n",
        "                    x_upd = x_upd + (K_s.flatten() * innov)\n",
        "                    P_upd = P_upd - (K_s @ (Hs @ P_upd))\n",
        "        cur_t = t_ms[k]\n",
        "        spd_est = float(np.hypot(x_upd[2], x_upd[3]))\n",
        "        spd_buf.append((cur_t, spd_est))\n",
        "        while spd_buf and (cur_t - spd_buf[0][0]) > 1500:\n",
        "            spd_buf.popleft()\n",
        "        vals = [v for (tt, v) in spd_buf if (cur_t - tt) <= 1000]\n",
        "        ma = np.mean(vals) if len(vals) >= 5 else spd_est\n",
        "        duration = (spd_buf[-1][0] - spd_buf[0][0]) if len(spd_buf) > 1 else 0\n",
        "        if not stopped and ma < 0.18 and duration >= 1000:\n",
        "            stopped = True\n",
        "        if stopped and ma > 0.28:\n",
        "            stopped = False\n",
        "        if stopped and spd_est < 0.5:\n",
        "            H_v0 = np.array([[0,0,1,0],[0,0,0,1]], dtype=np.float64)\n",
        "            z_v0 = np.array([0.0, 0.0], dtype=np.float64)\n",
        "            R_v0 = np.diag([0.1**2, 0.1**2])\n",
        "            yv0 = z_v0 - (H_v0 @ x_upd)\n",
        "            S_v0 = H_v0 @ P_upd @ H_v0.T + R_v0\n",
        "            try:\n",
        "                S_v0_inv = np.linalg.inv(S_v0)\n",
        "            except np.linalg.LinAlgError:\n",
        "                S_v0_inv = np.linalg.pinv(S_v0)\n",
        "            if float(yv0.T @ S_v0_inv @ yv0) <= 6.63:\n",
        "                K_v0 = P_upd @ H_v0.T @ S_v0_inv\n",
        "                x_upd = x_upd + K_v0 @ yv0\n",
        "                P_upd = (np.eye(4) - K_v0 @ H_v0) @ P_upd\n",
        "        vE_k, vN_k = float(x_upd[2]), float(x_upd[3])\n",
        "        spd_k = float(np.hypot(vE_k, vN_k))\n",
        "        if spd_k > 2.0 and k > 0:\n",
        "            h_prev = np.arctan2(x[k-1,3], x[k-1,2])\n",
        "            h_cur = np.arctan2(vN_k, vE_k)\n",
        "            d = h_cur - h_prev\n",
        "            if d > np.pi: d -= 2*np.pi\n",
        "            if d < -np.pi: d += 2*np.pi\n",
        "            hdg_rate = abs(d) / dt\n",
        "            thr = 0.15 if spd_k >= 12.0 else 0.12\n",
        "            if hdg_rate < thr:\n",
        "                psi = np.arctan2(vN_k, vE_k)\n",
        "                H_lat = np.array([[0.0, 0.0, -np.sin(psi), np.cos(psi)]], dtype=np.float64)\n",
        "                R_lat = 0.15**2\n",
        "                innov_lat = -float((H_lat @ x_upd).item())\n",
        "                S_lat = float((H_lat @ P_upd @ H_lat.T).item()) + R_lat\n",
        "                if S_lat > 1e-9:\n",
        "                    maha2_lat = (innov_lat*innov_lat) / S_lat\n",
        "                    if maha2_lat <= 5.99:\n",
        "                        K_lat = (P_upd @ H_lat.T) / S_lat\n",
        "                        x_upd = x_upd + (K_lat.flatten() * innov_lat)\n",
        "                        P_upd = P_upd - (K_lat @ (H_lat @ P_upd))\n",
        "        x[k] = x_upd; P[k] = P_upd; Fm[k] = F; Qm[k] = Q\n",
        "    xs = x.copy(); Ps = P.copy()\n",
        "    for k in range(n-2, -1, -1):\n",
        "        F = Fm[k+1]; Pk = P[k]; P_pred = F @ Pk @ F.T + Qm[k+1]\n",
        "        try:\n",
        "            Ck = Pk @ F.T @ np.linalg.inv(P_pred)\n",
        "        except np.linalg.LinAlgError:\n",
        "            Ck = Pk @ F.T @ np.linalg.pinv(P_pred)\n",
        "        xs[k] = x[k] + Ck @ (xs[k+1] - (F @ x[k]))\n",
        "        Ps[k] = Pk + Ck @ (Ps[k+1] - P_pred) @ Ck.T\n",
        "    vnorm_s = np.hypot(xs[:,2], xs[:,3])\n",
        "    Rpost_var = 0.5 * (Ps[:,0,0] + Ps[:,1,1])\n",
        "    return xs[:,0], xs[:,1], vnorm_s, Rpost_var\n",
        "\n",
        "def run_phone_kf_enhanced_v43(gnss_csv: Path, lat0: float, lon0: float, sample_times: np.ndarray, base_std: float, time_offset_ms: int = 0):\n",
        "    df_ecef = load_phone_gnss_positions(gnss_csv)\n",
        "    if len(df_ecef) == 0:\n",
        "        return pd.DataFrame({'UnixTimeMillis': sample_times, 'E': np.nan, 'N': np.nan, 'Rpost_var': np.nan})\n",
        "    if time_offset_ms != 0:\n",
        "        df_ecef = df_ecef.copy()\n",
        "        df_ecef['t'] = (df_ecef['t'].astype(np.int64) + int(time_offset_ms)).astype(np.int64)\n",
        "    # Clock discontinuities (if available)\n",
        "    disc_arr = None\n",
        "    try:\n",
        "        head = pd.read_csv(gnss_csv, nrows=1)\n",
        "        if 'HardwareClockDiscontinuityCount' in head.columns:\n",
        "            ddisc = pd.read_csv(gnss_csv, usecols=['utcTimeMillis','HardwareClockDiscontinuityCount'])\n",
        "            ddisc = ddisc.groupby('utcTimeMillis')['HardwareClockDiscontinuityCount'].max().reset_index()\n",
        "            ddisc['t'] = ddisc['utcTimeMillis'].astype(np.int64)\n",
        "            if time_offset_ms != 0:\n",
        "                ddisc['t'] = (ddisc['t'].astype(np.int64) + int(time_offset_ms)).astype(np.int64)\n",
        "            disc_arr = df_ecef[['t']].merge(ddisc[['t','HardwareClockDiscontinuityCount']], on='t', how='left')['HardwareClockDiscontinuityCount'].astype('float64').values\n",
        "    except Exception:\n",
        "        disc_arr = None\n",
        "    # ENU\n",
        "    df_enu = ecef_df_to_enu(df_ecef, lat0, lon0)\n",
        "    t = df_enu['t'].values.astype(np.int64)\n",
        "    E = df_enu['E'].values.astype(np.float64)\n",
        "    N = df_enu['N'].values.astype(np.float64)\n",
        "    n = len(t)\n",
        "    # Doppler speed WLS\n",
        "    dop = compute_doppler_speed_wls(gnss_csv, lat0, lon0)\n",
        "    if time_offset_ms != 0 and not dop.empty:\n",
        "        dop = dop.copy(); dop['t'] = (dop['t'].astype(np.int64) + int(time_offset_ms)).astype(np.int64)\n",
        "    spd = pd.DataFrame({'t': t}).merge(dop[['t','speed_mag']] if not dop.empty else pd.DataFrame({'t':[], 'speed_mag':[]}), on='t', how='left')['speed_mag'].values.astype(np.float64)\n",
        "    # Tiered R_speed by dt\n",
        "    Rspd = np.full(n, np.nan, dtype=np.float64)\n",
        "    for k in range(1, n):\n",
        "        dtms = t[k] - t[k-1]\n",
        "        if dtms <= 150:\n",
        "            Rspd[k] = 0.5**2\n",
        "        elif dtms <= 500:\n",
        "            Rspd[k] = 0.9**2\n",
        "        else:\n",
        "            Rspd[k] = 1.4**2\n",
        "    # Per-route fixed Rpos variance override\n",
        "    try:\n",
        "        route_name = str(Path(gnss_csv).parts[-3])\n",
        "    except Exception:\n",
        "        route_name = ''\n",
        "    rpos_var = float((ROUTE_RPOS_STD.get(route_name, base_std))**2)\n",
        "    # Segment on dt>1500ms and clock discontinuities; zero-velocity init per segment\n",
        "    idx_starts = [0]\n",
        "    for k in range(1, n):\n",
        "        gap = (t[k] - t[k-1]) > 1500\n",
        "        disc_break = False\n",
        "        if disc_arr is not None:\n",
        "            prev = disc_arr[k-1] if np.isfinite(disc_arr[k-1]) else (disc_arr[k-2] if k>=2 and np.isfinite(disc_arr[k-2]) else 0.0)\n",
        "            cur = disc_arr[k] if np.isfinite(disc_arr[k]) else prev\n",
        "            disc_break = (cur > prev)\n",
        "        if gap or disc_break:\n",
        "            idx_starts.append(k)\n",
        "    idx_starts = sorted(set(idx_starts))\n",
        "    idx_ends = idx_starts[1:] + [n]\n",
        "    Es_list, Ns_list, Rp_list, ts_list = [], [], [], []\n",
        "    for s, e in zip(idx_starts, idx_ends):\n",
        "        if e - s < 2:\n",
        "            continue\n",
        "        Ee, Ne, _, Rp = kf_rts_smooth_adaptive_v43(E[s:e], N[s:e], t[s:e],\n",
        "                                                   Rpos_vars=np.full(e-s, rpos_var, dtype=np.float64),\n",
        "                                                   speed_mag=spd[s:e], R_speed_vars=Rspd[s:e],\n",
        "                                                   gate_pos_chi2=9.21, gate_spd_chi2=6.63)\n",
        "        Es_list.append(Ee); Ns_list.append(Ne); Rp_list.append(Rp); ts_list.append(t[s:e])\n",
        "    if not Es_list:\n",
        "        return pd.DataFrame({'UnixTimeMillis': sample_times, 'E': np.nan, 'N': np.nan, 'Rpost_var': np.nan})\n",
        "    Es = np.concatenate(Es_list); Ns = np.concatenate(Ns_list); Rpost = np.concatenate(Rp_list); t_all = np.concatenate(ts_list)\n",
        "    # Interpolate to sample times with edge-hold\n",
        "    def interp_nearest(x, xp, fp):\n",
        "        y = np.interp(x, xp, fp)\n",
        "        y[x < xp[0]] = fp[0]; y[x > xp[-1]] = fp[-1]\n",
        "        return y\n",
        "    # Ensure strictly increasing for interp\n",
        "    uniq = np.concatenate([[True], t_all[1:] != t_all[:-1]])\n",
        "    t_u = t_all[uniq]; Es_u = Es[uniq]; Ns_u = Ns[uniq]; Rp_u = Rpost[uniq]\n",
        "    ts = sample_times.astype(np.int64)\n",
        "    E_q = interp_nearest(ts, t_u, Es_u)\n",
        "    N_q = interp_nearest(ts, t_u, Ns_u)\n",
        "    R_q = interp_nearest(ts, t_u, Rp_u)\n",
        "    return pd.DataFrame({'UnixTimeMillis': ts, 'E': E_q, 'N': N_q, 'Rpost_var': R_q})"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "id": "1c73a2b1-ca46-41af-949c-208a14e31b36",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "SAFE_FUSION_ROUTES = {'2020-06-04-US-MTV-1','2020-06-04-US-MTV-2','2020-07-08-US-MTV-1','2020-07-08-US-MTV-2'}\n",
        "FUSION_R_CAP = 22.0\n",
        "FUSION_WEIGHT_MULTIPLIERS = {'GooglePixel4': 0.95, 'GooglePixel4XL': 0.95, 'GooglePixel5': 0.95}\n",
        "\n",
        "def _route_anchor(route_dir: Path):\n",
        "    return build_route_anchor_from_all_phones(route_dir)\n",
        "\n",
        "def _static_offset(route, phone):\n",
        "    return int(STATIC_TIME_OFFSETS_MS.get(route, {}).get(phone, 0))\n",
        "\n",
        "def _single_best_block(route_name: str, sub_route: pd.DataFrame, test_root: Path) -> pd.DataFrame:\n",
        "    route_dir = test_root / route_name\n",
        "    lat0, lon0 = _route_anchor(route_dir)\n",
        "    route_phones = [tid.rsplit('-',1)[-1] for tid in sub_route['tripId'].unique()]\n",
        "    best = select_single_best_phone(route_name, route_phones, route_dir)\n",
        "    sel_std = phone_base_std_from_name_override(best)\n",
        "    alpha, clamp_m = ROUTE_ALPHA_CLAMP.get(route_name, (0.85, 1.0))\n",
        "    t_off = _static_offset(route_name, best)\n",
        "    out = []\n",
        "    for trip_id, grp in sub_route.groupby('tripId', sort=False):\n",
        "        ts = grp['UnixTimeMillis'].values.astype(np.int64)\n",
        "        trk = run_phone_kf_enhanced_v43(route_dir / best / 'device_gnss.csv', lat0, lon0, ts, sel_std, time_offset_ms=t_off)\n",
        "        E, N = trk['E'].values, trk['N'].values\n",
        "        E, N = _gate_spd_acc_enu(E, N, ts)\n",
        "        E, N = _ema_smooth_enu(E, N, alpha=float(alpha))\n",
        "        if clamp_m is not None and 'SVL' not in route_name:\n",
        "            E, N = _map_match_clamp(E, N, clamp_m=float(clamp_m))\n",
        "        lat, lon = enu_to_latlon_series(E, N, np.zeros_like(E), lat0, lon0)\n",
        "        tmp = pd.DataFrame({'tripId': trip_id, 'UnixTimeMillis': ts, 'LatitudeDegrees': lat, 'LongitudeDegrees': lon})\n",
        "        out.append(tmp)\n",
        "    return pd.concat(out, ignore_index=True)\n",
        "\n",
        "def _fuse_2020(route_name: str, sub_route: pd.DataFrame, test_root: Path) -> pd.DataFrame | None:\n",
        "    route_dir = test_root / route_name\n",
        "    lat0, lon0 = _route_anchor(route_dir)\n",
        "    route_phones = [tid.rsplit('-',1)[-1] for tid in sub_route['tripId'].unique()]\n",
        "    pix = [p for p in route_phones if 'pixel' in p.lower()]\n",
        "    if len(pix) < 2:\n",
        "        return None\n",
        "    cn0_map = {}\n",
        "    for p in pix:\n",
        "        st = load_epoch_stats(route_dir / p / 'device_gnss.csv')\n",
        "        cn0_map[p] = (np.nanmedian(st['mean_cn0']) if ('mean_cn0' in st.columns) and not st.empty else 0.0)\n",
        "    phone_names = sorted(pix, key=lambda x: cn0_map.get(x, 0.0), reverse=True)[:3]\n",
        "    t_f = np.unique(np.sort(sub_route['UnixTimeMillis'].values.astype(np.int64)))\n",
        "    tracks, multipliers = [], []\n",
        "    for name in phone_names:\n",
        "        gnss_csv = route_dir / name / 'device_gnss.csv'\n",
        "        base_std = phone_base_std_from_name_override(name)\n",
        "        lag = _static_offset(route_name, name)\n",
        "        trk = run_phone_kf_enhanced_v43(gnss_csv, lat0, lon0, t_f, base_std, time_offset_ms=lag)\n",
        "        if not trk.empty:\n",
        "            tracks.append(trk[['UnixTimeMillis','E','N','Rpost_var']].copy())\n",
        "            multipliers.append(FUSION_WEIGHT_MULTIPLIERS.get(name, 1.0))\n",
        "    if len(tracks) < 2:\n",
        "        return None\n",
        "    fused_enu = fuse_phones_enu_union(tracks, target_ts=t_f, drop_thresh_m1=12.0, drop_thresh_m2=8.0, phone_names=None, phone_multipliers=np.array(multipliers, dtype=np.float64))\n",
        "    if fused_enu is None or fused_enu.empty:\n",
        "        return None\n",
        "    # Safeguard: abort if fusion looks noisy or spiky before RTS\n",
        "    med_R = float(np.nanmedian(fused_enu['Rpost_var'].values))\n",
        "    if np.isfinite(med_R) and med_R > 10.0:\n",
        "        return None\n",
        "    t_f = fused_enu['UnixTimeMillis'].values.astype(np.int64)\n",
        "    Ef = fused_enu['E'].values.astype(np.float64)\n",
        "    Nf = fused_enu['N'].values.astype(np.float64)\n",
        "    if len(t_f) >= 2:\n",
        "        dt = np.diff(t_f).astype(np.float64) * 1e-3\n",
        "        spd = np.hypot(np.diff(Ef), np.diff(Nf)) / np.maximum(1e-3, dt)\n",
        "        if np.isfinite(spd).any() and np.nanmax(spd) > 45.0:\n",
        "            return None\n",
        "    # Cap spd/acc on fused ENU before RTS\n",
        "    Ef, Nf = _gate_spd_acc_enu(Ef, Nf, t_f)\n",
        "    Rf = np.clip(fused_enu['Rpost_var'].values.astype(np.float64), 12.0, FUSION_R_CAP)\n",
        "    Ef, Nf, _, _ = kf_rts_smooth_adaptive_v43(Ef, Nf, t_f, Rpos_vars=Rf)\n",
        "    alpha, clamp_m = ROUTE_ALPHA_CLAMP.get(route_name, (0.82, 1.0))\n",
        "    Ef, Nf = _ema_smooth_enu(Ef, Nf, alpha=float(alpha))\n",
        "    if clamp_m is not None:\n",
        "        Ef, Nf = _map_match_clamp(Ef, Nf, clamp_m=float(clamp_m))\n",
        "    lat_f, lon_f = enu_to_latlon_series(Ef, Nf, np.zeros_like(Ef), lat0, lon0)\n",
        "    return pd.DataFrame({'UnixTimeMillis': t_f, 'LatitudeDegrees': lat_f, 'LongitudeDegrees': lon_f})\n",
        "\n",
        "def build_and_save_conditional(sample_path: Path, test_root: Path) -> pd.DataFrame:\n",
        "    sample = pd.read_csv(sample_path)\n",
        "    sub = pd.read_csv('submission.csv')  # start from single-best; we'll patch 2020 routes\n",
        "    sub['route'] = sub['tripId'].str.rsplit('-', n=1).str[0]\n",
        "    out = []\n",
        "    for route_name, grp in sub.groupby('route', sort=False):\n",
        "        if route_name not in SAFE_FUSION_ROUTES:\n",
        "            out.append(grp[['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees']]); continue\n",
        "        fused = _fuse_2020(route_name, sample[sample['tripId'].str.startswith(route_name)], test_root)\n",
        "        if fused is None or fused.empty:\n",
        "            out.append(grp[['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees']]); continue\n",
        "        tmp = grp[['tripId','UnixTimeMillis']].merge(fused, on='UnixTimeMillis', how='left')\n",
        "        tmp['LatitudeDegrees'] = tmp['LatitudeDegrees'].ffill().bfill().fillna(grp['LatitudeDegrees']).clip(-90, 90)\n",
        "        tmp['LongitudeDegrees'] = ((tmp['LongitudeDegrees'].ffill().bfill().fillna(grp['LongitudeDegrees']) + 180) % 360) - 180\n",
        "        out.append(tmp[['tripId','UnixTimeMillis','LatitudeDegrees','LongitudeDegrees']])\n",
        "    patched = pd.concat(out, ignore_index=True)\n",
        "    patched = patched.merge(sample[['tripId','UnixTimeMillis']].assign(_o=np.arange(len(sample))), on=['tripId','UnixTimeMillis'], how='right').sort_values('_o').drop(columns=['_o'])\n",
        "    patched.to_csv('submission.csv', index=False)\n",
        "    print('Conditional (2020-only) fusion patched. shape:', patched.shape, flush=True)\n",
        "    return patched\n",
        "\n",
        "pred = build_and_save_conditional(Path('sample_submission.csv'), Path('test'))"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conditional (2020-only) fusion patched. shape: (37087, 4)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}