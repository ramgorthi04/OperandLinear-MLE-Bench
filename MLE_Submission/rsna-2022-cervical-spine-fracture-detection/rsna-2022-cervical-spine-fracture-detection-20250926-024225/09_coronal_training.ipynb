{
  "cells": [
    {
      "id": "d50ab40c-6d26-4df8-b5f3-4949b29a5b7a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from scipy.special import expit as sigmoid\n",
        "import gc\n",
        "from itertools import chain\n",
        "import torch.amp as amp\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Coronal MIP files already generated from full_mip[1].\n",
        "train_df = pd.read_csv('train.csv')\n",
        "coronal_dir = 'data/mips_coronal/train'\n",
        "\n",
        "# Labels\n",
        "label_cols = ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7']\n",
        "y = train_df[label_cols].values\n",
        "y_overall = y.max(axis=1).astype(int)\n",
        "groups = train_df['StudyInstanceUID'].values\n",
        "\n",
        "# ImageNet-style normalization for replicated channels (average RGB means/stds)\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "# Fixed augmentations: anatomy-safe, no VerticalFlip/RandomRotate90\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(384, 384),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.GaussianBlur(blur_limit=3, p=0.1),\n",
        "    A.Normalize(mean=mean, std=std),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(384, 384),\n",
        "    A.Normalize(mean=mean, std=std),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "class CoronalDataset(Dataset):\n",
        "    def __init__(self, df, mip_dir, transform):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.mip_dir = mip_dir\n",
        "        self.transform = transform\n",
        "        self.label_cols = label_cols\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        uid = row['StudyInstanceUID']\n",
        "        img_path = os.path.join(self.mip_dir, f'{uid}.npy')\n",
        "        img = np.load(img_path).astype(np.float32)  # (384, 384)\n",
        "        img = np.stack([img] * 3, axis=-1)  # replicate to (384, 384, 3)\n",
        "        augmented = self.transform(image=img)['image']\n",
        "        labels = torch.tensor(row[self.label_cols].values.astype(np.float32))\n",
        "        return augmented, labels\n",
        "\n",
        "# Training parameters - BATCH_SIZE=1, ACCUM_STEPS=8 to avoid OOM\n",
        "N_FOLDS = 5\n",
        "BATCH_SIZE = 1\n",
        "VAL_BATCH_SIZE = 1\n",
        "NUM_EPOCHS = 15\n",
        "LR = 1e-4\n",
        "PATIENCE = 4\n",
        "SEED = 789\n",
        "ACCUM_STEPS = 8\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# CV splitter - FIXED: Use StratifiedGroupKFold to prevent patient leakage\n",
        "skf = StratifiedGroupKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "# OOF logits collection (will include HFlip TTA averaging)\n",
        "oof_logits = np.zeros((len(train_df), 7), dtype=np.float32)\n",
        "fold_scores = []\n",
        "\n",
        "for fold in range(1, N_FOLDS + 1):\n",
        "    print(f'\\n=== Fold {fold}/{N_FOLDS} ===')\n",
        "    train_idx, val_idx = list(skf.split(train_df, y_overall, groups))[fold-1]\n",
        "    train_ds = CoronalDataset(train_df.iloc[train_idx], coronal_dir, train_transform)\n",
        "    val_ds = CoronalDataset(train_df.iloc[val_idx], coronal_dir, val_transform)\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    model = timm.create_model('regnety_004', pretrained=True, num_classes=7, in_chans=3,\n",
        "                              drop_rate=0.3, drop_path_rate=0.15)\n",
        "    try:\n",
        "        model.set_grad_checkpointing(True)\n",
        "    except:\n",
        "        pass\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    scaler = amp.GradScaler('cuda')\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        # Train with AMP, float16, channels_last, grad accum\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        for batch_idx, (imgs, labels) in enumerate(train_loader):\n",
        "            imgs = imgs.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "                logits = model(imgs)\n",
        "                loss = criterion(logits, labels) / ACCUM_STEPS\n",
        "            scaler.scale(loss).backward()\n",
        "            if (batch_idx + 1) % ACCUM_STEPS == 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "            train_loss += loss.item() * ACCUM_STEPS\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # Val (BCE loss only, no TTA here)\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for imgs, lbls in val_loader:\n",
        "                imgs = imgs.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "                lbls = lbls.to(device, non_blocking=True)\n",
        "                with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "                    logits = model(imgs)\n",
        "                    loss = criterion(logits, lbls)\n",
        "                val_loss += loss.item()\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        scheduler.step()\n",
        "        print(f'Epoch {epoch+1}: Train {train_loss:.4f}, Val {val_loss:.4f}')\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict().copy()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= PATIENCE:\n",
        "                print(f'Early stopping at epoch {epoch+1}')\n",
        "                break\n",
        "\n",
        "    # Load best model\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "        torch.save(model.state_dict(), f'fold_{fold}_regnet_coronal_fixed.pth')\n",
        "    else:\n",
        "        print(f'No best model for fold {fold}')\n",
        "        continue\n",
        "\n",
        "    # Collect true OOF logits with HFlip TTA for this fold's val\n",
        "    model.eval()\n",
        "    fold_oof_logits = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, _ in val_loader:\n",
        "            imgs = imgs.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "                logits_orig = model(imgs)\n",
        "                imgs_flip = torch.flip(imgs, dims=[3])  # HFlip\n",
        "                logits_flip = model(imgs_flip)\n",
        "                logits_avg = 0.5 * (logits_orig + logits_flip)\n",
        "            fold_oof_logits.append(logits_avg.cpu().numpy())\n",
        "    fold_oof = np.concatenate(fold_oof_logits, axis=0)\n",
        "    oof_logits[val_idx] = fold_oof\n",
        "\n",
        "    # Fold score (full WLL with patient_overall=max(p7)) - FIXED: Add labels=[0,1] to handle all-0 val sets\n",
        "    p7 = sigmoid(fold_oof)\n",
        "    fold_y7 = y[val_idx]\n",
        "    fold_y_overall = fold_y7.max(axis=1).astype(int)\n",
        "    p_overall = p7.max(axis=1)\n",
        "    vert_losses = [log_loss(fold_y7[:, i], p7[:, i], labels=[0,1]) for i in range(7)]\n",
        "    overall_loss = log_loss(fold_y_overall, p_overall, labels=[0,1])\n",
        "    fold_score = np.average(vert_losses + [overall_loss], weights=[1]*7 + [2])\n",
        "    fold_scores.append(fold_score)\n",
        "    print(f'Fold {fold} full WLL: {fold_score:.4f}')\n",
        "\n",
        "    # Cleanup\n",
        "    del model, train_loader, val_loader, train_ds, val_ds, scaler\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Save OOF logits (with TTA)\n",
        "np.save('oof_logits_coronal_fixed.npy', oof_logits)\n",
        "\n",
        "# Overall CV full WLL\n",
        "cv_wll = np.mean(fold_scores)\n",
        "print(f'5-fold CV full WLL: {cv_wll:.4f} (target ~0.45-0.48 standalone, leakage-free)')\n",
        "\n",
        "# Also vertebrae-only for reference - FIXED: Add labels=[0,1]\n",
        "p7_oof = sigmoid(oof_logits)\n",
        "vert_losses = [log_loss(y[:, i], p7_oof[:, i], labels=[0,1]) for i in range(7)]\n",
        "vert_wll = np.mean(vert_losses)\n",
        "print(f'Vertebrae-only OOF WLL: {vert_wll:.4f}')\n",
        "\n",
        "print('Leakage-fixed coronal training complete. OOF logits (HFlip TTA) saved as oof_logits_coronal_fixed.npy. CV scores now reliable (expect higher than leaky 0.4680). Next: similarly fix 10_axial_training.ipynb and 11_soft_tissue_axial_training.ipynb, retrain all three for new leakage-free OOF, then re-gate in 03_inference_tta.ipynb cell 6, establish reliable baseline, proceed to 6-channel training after mip_prep completes.')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n\n=== Fold 1/5 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n  original_init(self, **validated_kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_369466/3957355412.py:122: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(dtype=torch.float16):\n/app/.pip-target/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_369466/3957355412.py:142: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train 0.5982, Val 0.5517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train 0.4120, Val 0.4788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train 0.3542, Val 0.3646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train 0.3482, Val 0.3706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train 0.3441, Val 0.4074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train 0.3442, Val 0.3927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train 0.3477, Val 0.4275\nEarly stopping at epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_369466/3957355412.py:175: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 full WLL: 0.4812\n\n=== Fold 2/5 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_369466/3957355412.py:122: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(dtype=torch.float16):\n/app/.pip-target/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_369466/3957355412.py:142: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train 0.5999, Val 0.5734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train 0.4182, Val 0.3791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train 0.3655, Val 0.3156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train 0.3526, Val 0.3366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train 0.3513, Val 0.3513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train 0.3540, Val 0.3232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train 0.3540, Val 0.3262\nEarly stopping at epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_369466/3957355412.py:175: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 full WLL: 0.4189\n\n=== Fold 3/5 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_369466/3957355412.py:122: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(dtype=torch.float16):\n/app/.pip-target/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_369466/3957355412.py:142: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train 0.5723, Val 0.5143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train 0.4083, Val 0.4358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train 0.3509, Val 0.3852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train 0.3384, Val 0.3754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train 0.3413, Val 0.3816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train 0.3386, Val 0.3746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train 0.3408, Val 0.3896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train 0.3385, Val 0.3669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train 0.3366, Val 0.3903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train 0.3389, Val 0.3865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Train 0.3411, Val 0.3662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: Train 0.3351, Val 0.3721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13: Train 0.3333, Val 0.3817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: Train 0.3362, Val 0.3768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: Train 0.3348, Val 0.3753\nEarly stopping at epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_369466/3957355412.py:175: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 full WLL: 0.4784\n\n=== Fold 4/5 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_369466/3957355412.py:122: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(dtype=torch.float16):\n/app/.pip-target/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_369466/3957355412.py:142: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train 0.6387, Val 0.6021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train 0.4325, Val 0.5117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train 0.3362, Val 0.4594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train 0.3315, Val 0.4059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train 0.3297, Val 0.5498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train 0.3261, Val 0.4828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train 0.3248, Val 0.4109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train 0.3244, Val 0.4259\nEarly stopping at epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_369466/3957355412.py:175: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 full WLL: 0.5227\n\n=== Fold 5/5 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_369466/3957355412.py:122: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(dtype=torch.float16):\n/app/.pip-target/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_369466/3957355412.py:142: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train 0.6007, Val 0.5311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train 0.4201, Val 0.3605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train 0.3676, Val 0.3658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train 0.3550, Val 0.3247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train 0.3604, Val 0.3092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train 0.3554, Val 0.3333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train 0.3527, Val 0.3077\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train 0.3600, Val 0.3095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train 0.3527, Val 0.3575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train 0.3559, Val 0.3312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Train 0.3520, Val 0.3344\nEarly stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_369466/3957355412.py:175: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 full WLL: 0.4173\n5-fold CV full WLL: 0.4637 (target ~0.45-0.48 standalone, leakage-free)\nVertebrae-only OOF WLL: 0.3738\nLeakage-fixed coronal training complete. OOF logits (HFlip TTA) saved as oof_logits_coronal_fixed.npy. CV scores now reliable (expect higher than leaky 0.4680). Next: similarly fix 10_axial_training.ipynb and 11_soft_tissue_axial_training.ipynb, retrain all three for new leakage-free OOF, then re-gate in 03_inference_tta.ipynb cell 6, establish reliable baseline, proceed to 6-channel training after mip_prep completes.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}