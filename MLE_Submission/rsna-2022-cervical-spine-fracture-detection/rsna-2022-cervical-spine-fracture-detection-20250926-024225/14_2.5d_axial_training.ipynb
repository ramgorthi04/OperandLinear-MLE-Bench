{
  "cells": [
    {
      "id": "822cb6e1-a424-4b6c-aaf5-3274887685e2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "import gc, numpy as np, pandas as pd, torch, torch.nn as nn, timm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "    free, total = torch.cuda.mem_get_info()\n",
        "    print(f'VRAM Free/Total: {free/1e9:.2f}/{total/1e9:.2f} GB')\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "SEED=793; N_FOLDS=5; EPOCHS=20; PATIENCE=5; BATCH=4; LR=1e-4\n",
        "np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "label_cols = ['C1','C2','C3','C4','C5','C6','C7']\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "class Axial3ch(Dataset):\n",
        "    def __init__(self, df, is_train=True, cache_dir='temp_3d_vols'):\n",
        "        self.df=df.reset_index(drop=True); self.is_train=is_train; self.cache_dir=cache_dir\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self,i):\n",
        "        r=self.df.iloc[i]; uid=r.StudyInstanceUID\n",
        "        vol=np.load(os.path.join(self.cache_dir,f'{uid}.npy')).astype(np.float32)  # (2,Z,H,W)\n",
        "        bone=(vol[1]>0.2).astype(np.float32); zsum=bone.sum(axis=(1,2))+1e-6\n",
        "        zc=int(np.average(np.arange(vol.shape[1]), weights=zsum))\n",
        "        z0=np.clip(zc,0,vol.shape[1]-1); zb=np.clip(zc-1,0,vol.shape[1]-1); za=np.clip(zc+1,0,vol.shape[1]-1)\n",
        "        soft_c=vol[0,z0]; bone_c=vol[1,z0]; bone_ctx=(vol[1,zb]+vol[1,za])/2.0\n",
        "        img=np.stack([soft_c,bone_c,bone_ctx],0).astype(np.float32)\n",
        "        if self.is_train:\n",
        "            if np.random.rand()<0.5: img=img[:, :, ::-1].copy()\n",
        "            if np.random.rand()<0.5: img=img[:, ::-1, :].copy()\n",
        "            if np.random.rand()<0.8:\n",
        "                a=1+np.random.uniform(-0.1,0.1); b=np.random.uniform(-0.05,0.05); img=np.clip(img*a+b,0,1)\n",
        "        x=torch.from_numpy(img).float(); y7=torch.from_numpy(r[label_cols].values.astype(np.float32))\n",
        "        return x,y7\n",
        "\n",
        "def build_model():\n",
        "    m=timm.create_model('efficientnet_b3', pretrained=True, in_chans=3, num_classes=7)\n",
        "    m=m.to(device).to(memory_format=torch.channels_last)\n",
        "    if hasattr(m,'set_grad_checkpointing'): m.set_grad_checkpointing(True)\n",
        "    return m\n",
        "\n",
        "df = df[df['StudyInstanceUID'].apply(lambda u: os.path.exists(os.path.join('temp_3d_vols', f'{u}.npy')))].reset_index(drop=True)\n",
        "y = df[label_cols].values.astype(np.float32)\n",
        "y_overall = y.max(1).astype(int); groups=df['StudyInstanceUID'].values\n",
        "\n",
        "# Compute pos_weight\n",
        "col_sums = y.sum(axis=0)\n",
        "pos_weight = (len(df) - col_sums) / np.clip(col_sums, 1, None)\n",
        "pos_weight = np.clip(pos_weight, 1, 4)\n",
        "pos_weight = torch.tensor(pos_weight, dtype=torch.float32).to(device)\n",
        "\n",
        "skf=StratifiedGroupKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "oof=np.zeros((len(df),7),np.float32); scores=[]\n",
        "for fold,(tr,va) in enumerate(skf.split(df,y_overall,groups),1):\n",
        "    print(f'Starting Fold {fold}')\n",
        "    tr_ds=Axial3ch(df.iloc[tr],True); va_ds=Axial3ch(df.iloc[va],False)\n",
        "    tr_dl=DataLoader(tr_ds,batch_size=BATCH,shuffle=True,num_workers=0,pin_memory=True)\n",
        "    va_dl=DataLoader(va_ds,batch_size=BATCH,shuffle=False,num_workers=0,pin_memory=True)\n",
        "    model=build_model()\n",
        "    opt=torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "    sch=torch.optim.lr_scheduler.CosineAnnealingLR(opt,T_max=EPOCHS)\n",
        "    crit=nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    best=1e9; bad=0\n",
        "    for ep in range(EPOCHS):\n",
        "        model.train(); tr_loss=0\n",
        "        for x,yb in tr_dl:\n",
        "            x = x.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            yb = yb.to(device, non_blocking=True)\n",
        "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "                logits=model(x); loss=crit(logits,yb)\n",
        "            opt.zero_grad(set_to_none=True); loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
        "            opt.step(); tr_loss+=loss.item()\n",
        "        tr_loss/=max(1,len(tr_dl))\n",
        "        model.eval(); va_loss=0; preds=[]\n",
        "        with torch.no_grad():\n",
        "            for x,yb in va_dl:\n",
        "                x = x.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "                yb = yb.to(device, non_blocking=True)\n",
        "                with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "                    logits=model(x); loss=crit(logits,yb)\n",
        "                va_loss+=loss.item(); preds.append(torch.sigmoid(logits).cpu().numpy())\n",
        "        va_loss/=max(1,len(va_dl)); sch.step()\n",
        "        print(f'Fold {fold} Ep{ep+1}: tr {tr_loss:.4f} va {va_loss:.4f}')\n",
        "        if va_loss<best: best=va_loss; bad=0; best_state={k:v.detach().cpu() for k,v in model.state_dict().items()}\n",
        "        else:\n",
        "            bad+=1\n",
        "            if bad>=PATIENCE: break\n",
        "    model.load_state_dict(best_state, strict=True)\n",
        "    model.eval(); pred=[]\n",
        "    with torch.no_grad():\n",
        "        for x,_ in va_dl:\n",
        "            x = x.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "                pred.append(model(x).sigmoid().cpu().numpy())\n",
        "    oof[va]=np.concatenate(pred)\n",
        "    yv=y[va]; anyv=yv.max(1)\n",
        "    vl=[log_loss(yv[:,i],oof[va][:,i],labels=[0,1]) for i in range(7)]\n",
        "    ol=log_loss(anyv,oof[va].max(1),labels=[0,1])\n",
        "    wll=np.average(vl+[ol],weights=[1]*7+[2]); scores.append(wll)\n",
        "    torch.save(best_state, f'fold_{fold}_2p5d_efficientnetb3.pth')\n",
        "    del model,tr_dl,va_dl; gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "np.save('oof_logits_2p5d_axial.npy', oof)\n",
        "print('CV WLL:', np.mean(scores))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\nGPU: NVIDIA A10-24Q\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available():\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mGPU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.cuda.get_device_name(\u001b[32m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     free, total = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmem_get_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mVRAM Free/Total: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfree/\u001b[32m1e9\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal/\u001b[32m1e9\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m GB\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m torch.backends.cudnn.benchmark = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/cuda/memory.py:685\u001b[39m, in \u001b[36mmem_get_info\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m    683\u001b[39m     device = torch.cuda.current_device()\n\u001b[32m    684\u001b[39m device = _get_device_index(device)\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudaMemGetInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mRuntimeError\u001b[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}