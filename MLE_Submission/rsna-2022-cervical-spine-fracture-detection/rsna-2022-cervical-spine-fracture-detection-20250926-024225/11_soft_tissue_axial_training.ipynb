{
  "cells": [
    {
      "id": "390ddc08-2faf-42df-ba03-8156f747c2b4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from scipy.special import expit as sigmoid\n",
        "import gc\n",
        "from itertools import chain\n",
        "import torch.amp as amp\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Soft-tissue axial MIP files generated with C=40 W=400 window.\n",
        "train_df = pd.read_csv('train.csv')\n",
        "axial_dir = 'data/mips_axial_soft/train'\n",
        "\n",
        "# Labels\n",
        "label_cols = ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7']\n",
        "y = train_df[label_cols].values\n",
        "y_overall = y.max(axis=1).astype(int)\n",
        "groups = train_df['StudyInstanceUID'].values\n",
        "\n",
        "# ImageNet-style normalization for replicated channels (average RGB means/stds)\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "# Fixed augmentations: anatomy-safe, no VerticalFlip/RandomRotate90\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(384, 384),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.GaussianBlur(blur_limit=3, p=0.1),\n",
        "    A.Normalize(mean=mean, std=std),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(384, 384),\n",
        "    A.Normalize(mean=mean, std=std),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "class AxialDataset(Dataset):\n",
        "    def __init__(self, df, mip_dir, transform):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.mip_dir = mip_dir\n",
        "        self.transform = transform\n",
        "        self.label_cols = label_cols\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        uid = row['StudyInstanceUID']\n",
        "        img_path = os.path.join(self.mip_dir, f'{uid}.npy')\n",
        "        img = np.load(img_path).astype(np.float32)  # (384, 384)\n",
        "        img = np.stack([img] * 3, axis=-1)  # replicate to (384, 384, 3)\n",
        "        augmented = self.transform(image=img)['image']\n",
        "        labels = torch.tensor(row[self.label_cols].values.astype(np.float32))\n",
        "        return augmented, labels\n",
        "\n",
        "# Training parameters - BATCH_SIZE=1, ACCUM_STEPS=8 to avoid OOM\n",
        "N_FOLDS = 5\n",
        "BATCH_SIZE = 1\n",
        "VAL_BATCH_SIZE = 1\n",
        "NUM_EPOCHS = 15\n",
        "LR = 1e-4\n",
        "PATIENCE = 4\n",
        "SEED = 790\n",
        "ACCUM_STEPS = 8\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# CV splitter - FIXED: Use StratifiedGroupKFold to prevent patient leakage\n",
        "skf = StratifiedGroupKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "# OOF logits collection (will include HFlip TTA averaging)\n",
        "oof_logits = np.zeros((len(train_df), 7), dtype=np.float32)\n",
        "fold_scores = []\n",
        "\n",
        "for fold in range(1, N_FOLDS + 1):\n",
        "    print(f'\\n=== Fold {fold}/{N_FOLDS} ===')\n",
        "    train_idx, val_idx = list(skf.split(train_df, y_overall, groups))[fold-1]\n",
        "    train_ds = AxialDataset(train_df.iloc[train_idx], axial_dir, train_transform)\n",
        "    val_ds = AxialDataset(train_df.iloc[val_idx], axial_dir, val_transform)\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    model = timm.create_model('regnety_004', pretrained=True, num_classes=7, in_chans=3,\n",
        "                              drop_rate=0.3, drop_path_rate=0.15)\n",
        "    try:\n",
        "        model.set_grad_checkpointing(True)\n",
        "    except:\n",
        "        pass\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    scaler = amp.GradScaler('cuda')\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        # Train with AMP, float16, channels_last, grad accum\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        for batch_idx, (imgs, labels) in enumerate(train_loader):\n",
        "            imgs = imgs.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "                logits = model(imgs)\n",
        "                loss = criterion(logits, labels) / ACCUM_STEPS\n",
        "            scaler.scale(loss).backward()\n",
        "            if (batch_idx + 1) % ACCUM_STEPS == 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "            train_loss += loss.item() * ACCUM_STEPS\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # Val (BCE loss only, no TTA here)\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for imgs, lbls in val_loader:\n",
        "                imgs = imgs.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "                lbls = lbls.to(device, non_blocking=True)\n",
        "                with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "                    logits = model(imgs)\n",
        "                    loss = criterion(logits, lbls)\n",
        "                val_loss += loss.item()\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        scheduler.step()\n",
        "        print(f'Epoch {epoch+1}: Train {train_loss:.4f}, Val {val_loss:.4f}')\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict().copy()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= PATIENCE:\n",
        "                print(f'Early stopping at epoch {epoch+1}')\n",
        "                break\n",
        "\n",
        "    # Load best model\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "        torch.save(model.state_dict(), f'fold_{fold}_regnet_axial_soft_fixed.pth')\n",
        "    else:\n",
        "        print(f'No best model for fold {fold}')\n",
        "        continue\n",
        "\n",
        "    # Collect true OOF logits with HFlip TTA for this fold's val\n",
        "    model.eval()\n",
        "    fold_oof_logits = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, _ in val_loader:\n",
        "            imgs = imgs.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "                logits_orig = model(imgs)\n",
        "                imgs_flip = torch.flip(imgs, dims=[3])  # HFlip\n",
        "                logits_flip = model(imgs_flip)\n",
        "                logits_avg = 0.5 * (logits_orig + logits_flip)\n",
        "            fold_oof_logits.append(logits_avg.cpu().numpy())\n",
        "    fold_oof = np.concatenate(fold_oof_logits, axis=0)\n",
        "    oof_logits[val_idx] = fold_oof\n",
        "\n",
        "    # Fold score (full WLL with patient_overall=max(p7)) - FIXED: Add labels=[0,1] to handle all-0 val sets\n",
        "    p7 = sigmoid(fold_oof)\n",
        "    fold_y7 = y[val_idx]\n",
        "    fold_y_overall = fold_y7.max(axis=1).astype(int)\n",
        "    p_overall = p7.max(axis=1)\n",
        "    vert_losses = [log_loss(fold_y7[:, i], p7[:, i], labels=[0,1]) for i in range(7)]\n",
        "    overall_loss = log_loss(fold_y_overall, p_overall, labels=[0,1])\n",
        "    fold_score = np.average(vert_losses + [overall_loss], weights=[1]*7 + [2])\n",
        "    fold_scores.append(fold_score)\n",
        "    print(f'Fold {fold} full WLL: {fold_score:.4f}')\n",
        "\n",
        "    # Cleanup\n",
        "    del model, train_loader, val_loader, train_ds, val_ds, scaler\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Save OOF logits (with TTA)\n",
        "np.save('oof_logits_axial_soft_fixed.npy', oof_logits)\n",
        "\n",
        "# Overall CV full WLL\n",
        "cv_wll = np.mean(fold_scores)\n",
        "print(f'5-fold CV full WLL: {cv_wll:.4f} (target ~0.45-0.48 standalone, leakage-free)')\n",
        "\n",
        "# Also vertebrae-only for reference - FIXED: Add labels=[0,1]\n",
        "p7_oof = sigmoid(oof_logits)\n",
        "vert_losses = [log_loss(y[:, i], p7_oof[:, i], labels=[0,1]) for i in range(7)]\n",
        "vert_wll = np.mean(vert_losses)\n",
        "print(f'Vertebrae-only OOF WLL: {vert_wll:.4f}')\n",
        "\n",
        "print('Leakage-fixed soft-tissue axial training complete. OOF logits (HFlip TTA) saved as oof_logits_axial_soft_fixed.npy. CV scores now reliable (expect higher than leaky 0.4759). Next: re-execute cell 6 in 03_inference_tta.ipynb with all fixed OOFs for true gating (baseline ~0.42 leakage-free, Swin accepted, check if axial/soft now >0.001 gain), await mip_prep finish, then edit 12_multi_channel_training.ipynb cell 0 (in_chans=6 load (6,H,W).npy transpose (H,W,6) norm [0.485]*6 B=1 VAL=1 ACCUM=8 checkpointing float16 channels_last SEED=791 fixed skf HFlip TTA dims=[3] save fold_regnet_multi_fixed.pth oof_logits_multi_fixed.npy ~2h), execute run_all, gate Multi, if passes update cell 1 in 03 for 5-way inference (add predict_multi in_chans=6 TTA X5 blend refit postproc), execute cell 1, submit.')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n\n=== Fold 1/5 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n  original_init(self, **validated_kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train 0.6049, Val 0.6117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train 0.4475, Val 0.5296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train 0.3755, Val 0.4277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train 0.3565, Val 0.4229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train 0.3821, Val 0.3593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train 0.3614, Val 0.3492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train 0.3296, Val 0.3502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train 0.3439, Val 0.3533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train 0.3341, Val 0.3452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train 0.3417, Val 0.3485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Train 0.3319, Val 0.3461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: Train 0.3294, Val 0.3473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13: Train 0.3421, Val 0.3490\nEarly stopping at epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 full WLL: 0.5134\n\n=== Fold 2/5 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train 0.6767, Val 0.6465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train 0.4824, Val 0.5237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train 0.4222, Val 0.4197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train 0.3875, Val 0.3618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train 0.3571, Val 0.3191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train 0.3509, Val 0.3741\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train 0.3356, Val 0.3565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train 0.3470, Val 0.3021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train 0.3319, Val 0.3170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train 0.3637, Val 0.3059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Train 0.3463, Val 0.3024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: Train 0.3587, Val 0.2992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13: Train 0.3311, Val 0.3039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: Train 0.3405, Val 0.3034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: Train 0.3485, Val 0.3000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 full WLL: 0.4558\n\n=== Fold 3/5 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train 0.6382, Val 0.5957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train 0.4746, Val 0.5163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train 0.3883, Val 0.4677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train 0.3757, Val 0.3972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train 0.3479, Val 0.3717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train 0.3301, Val 0.3698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train 0.3494, Val 0.3701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train 0.3395, Val 0.3695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train 0.3376, Val 0.3737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train 0.3390, Val 0.3805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Train 0.3397, Val 0.3774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: Train 0.3323, Val 0.3715\nEarly stopping at epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 full WLL: 0.4771\n\n=== Fold 4/5 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train 0.6504, Val 0.6075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train 0.4949, Val 0.4939\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train 0.4324, Val 0.4497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train 0.3752, Val 0.3388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train 0.3483, Val 0.3202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train 0.3469, Val 0.2965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train 0.3528, Val 0.2985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train 0.3756, Val 0.2995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train 0.3338, Val 0.2965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train 0.3523, Val 0.2895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Train 0.3435, Val 0.2899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: Train 0.3549, Val 0.2847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13: Train 0.3673, Val 0.2892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: Train 0.3348, Val 0.2926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: Train 0.3350, Val 0.2913\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "y_true contains only one label (0). Please provide the list of all expected class labels explicitly through the labels argument.",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 180\u001b[39m\n\u001b[32m    178\u001b[39m fold_y_overall = fold_y7.max(axis=\u001b[32m1\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m    179\u001b[39m p_overall = p7.max(axis=\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m vert_losses = \u001b[43m[\u001b[49m\u001b[43mlog_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold_y7\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp7\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m7\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    181\u001b[39m overall_loss = log_loss(fold_y_overall, p_overall)\n\u001b[32m    182\u001b[39m fold_score = np.average(vert_losses + [overall_loss], weights=[\u001b[32m1\u001b[39m]*\u001b[32m7\u001b[39m + [\u001b[32m2\u001b[39m])\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 180\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    178\u001b[39m fold_y_overall = fold_y7.max(axis=\u001b[32m1\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m    179\u001b[39m p_overall = p7.max(axis=\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m vert_losses = [\u001b[43mlog_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold_y7\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp7\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m7\u001b[39m)]\n\u001b[32m    181\u001b[39m overall_loss = log_loss(fold_y_overall, p_overall)\n\u001b[32m    182\u001b[39m fold_score = np.average(vert_losses + [overall_loss], weights=[\u001b[32m1\u001b[39m]*\u001b[32m7\u001b[39m + [\u001b[32m2\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/metrics/_classification.py:3240\u001b[39m, in \u001b[36mlog_loss\u001b[39m\u001b[34m(y_true, y_pred, normalize, sample_weight, labels)\u001b[39m\n\u001b[32m   3162\u001b[39m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[32m   3163\u001b[39m     {\n\u001b[32m   3164\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33marray-like\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   3171\u001b[39m )\n\u001b[32m   3172\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_loss\u001b[39m(y_true, y_pred, *, normalize=\u001b[38;5;28;01mTrue\u001b[39;00m, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, labels=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   3173\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Log loss, aka logistic loss or cross-entropy loss.\u001b[39;00m\n\u001b[32m   3174\u001b[39m \n\u001b[32m   3175\u001b[39m \u001b[33;03m    This is the loss function used in (multinomial) logistic regression\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3238\u001b[39m \u001b[33;03m    0.21616\u001b[39;00m\n\u001b[32m   3239\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3240\u001b[39m     transformed_labels, y_pred = \u001b[43m_validate_multiclass_probabilistic_prediction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3241\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\n\u001b[32m   3242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3244\u001b[39m     \u001b[38;5;66;03m# Clipping\u001b[39;00m\n\u001b[32m   3245\u001b[39m     eps = np.finfo(y_pred.dtype).eps\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/metrics/_classification.py:229\u001b[39m, in \u001b[36m_validate_multiclass_probabilistic_prediction\u001b[39m\u001b[34m(y_true, y_prob, sample_weight, labels)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lb.classes_) == \u001b[32m1\u001b[39m:\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33my_true contains only one label (\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m). Please \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    231\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprovide the list of all expected class labels explicitly through the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    232\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlabels argument.\u001b[39m\u001b[33m\"\u001b[39m.format(lb.classes_[\u001b[32m0\u001b[39m])\n\u001b[32m    233\u001b[39m         )\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    235\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    236\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe labels array needs to contain at least two \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    237\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlabels, got \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m.format(lb.classes_)\n\u001b[32m    238\u001b[39m         )\n",
            "\u001b[31mValueError\u001b[39m: y_true contains only one label (0). Please provide the list of all expected class labels explicitly through the labels argument."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}