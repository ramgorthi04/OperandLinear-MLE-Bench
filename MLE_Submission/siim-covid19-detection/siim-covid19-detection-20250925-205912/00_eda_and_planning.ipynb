{
  "cells": [
    {
      "id": "776102d7-a24a-4e4b-8682-96f8e3add7cd",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SIIM-COVID19 Detection: Plan and EDA\n",
        "\n",
        "## 1. Problem Understanding\n",
        "\n",
        "This is a medical imaging competition with two related tasks:\n",
        "1.  **Image-level:** Detect and localize COVID-19 related opacities with bounding boxes for a single class: `opacity`.\n",
        "2.  **Study-level:** Classify each study into one of four classes: `Negative for Pneumonia`, `Typical Appearance`, `Indeterminate Appearance`, `Atypical Appearance`.\n",
        "\n",
        "The evaluation metric is a blended **mean Average Precision (mAP)**, averaging the image-level and study-level mAP scores. The submission format requires predictions for both tasks.\n",
        "\n",
        "## 2. Revised Plan (Based on Expert Advice)\n",
        "\n",
        "### Core Strategy: Two-Model Pipeline\n",
        "Following expert advice, I will build two separate, specialized models:\n",
        "1.  **Object Detector:** A YOLOv5 model trained to detect a single `opacity` class.\n",
        "2.  **Image Classifier:** An EfficientNet model (via `timm`) trained to classify images into the four study-level categories.\n",
        "3.  **Fusion:** At inference, predictions will be fused. Study-level predictions will be an aggregation of the classifier's outputs across all images in a study. Detector meta-features (e.g., max confidence score) can be used to refine this.\n",
        "\n",
        "### Phase 1: Setup, EDA & Preprocessing (Hours 0-3)\n",
        "1.  **Environment Setup:** Install necessary packages: `pandas`, `pydicom`, `scikit-learn`, `timm`, `albumentations`, and clone the YOLOv5 repository.\n",
        "2.  **Metadata Exploration:** Analyze `train_study_level.csv` and `train_image_level.csv`. Merge them to create a unified dataframe for training.\n",
        "3.  **Validation Strategy:** Implement a `StratifiedGroupKFold` split on `StudyInstanceUID`. The stratification will be based on a combination of the study-level class and a `has_opacity` flag to ensure balanced folds for both tasks.\n",
        "4.  **DICOM Preprocessing Pipeline:** This is a critical, make-or-break step.\n",
        "    *   Load DICOM files using `pydicom`.\n",
        "    *   Apply `Rescale Slope/Intercept` and `VOI LUT`.\n",
        "    *   Handle `Photometric Interpretation` (invert `MONOCHROME1` images).\n",
        "    *   Resize images (e.g., to 640x640 or 1024x1024) using letterboxing to preserve aspect ratio.\n",
        "    *   Normalize pixel values.\n",
        "    *   **Cache processed images as PNGs** to accelerate training experiments.\n",
        "\n",
        "### Phase 2: Baseline Model Training (Hours 3-10)\n",
        "1.  **Detector Training (YOLOv5):**\n",
        "    *   Prepare data in YOLOv5 format (`.txt` label files).\n",
        "    *   Train a `YOLOv5s` or `YOLOv5m` model on a single fold to establish a working pipeline.\n",
        "    *   Use augmentations from `albumentations` (H-flip, small rotations, brightness/contrast).\n",
        "2.  **Classifier Training (EfficientNet):**\n",
        "    *   Create a PyTorch `Dataset` for the cached PNG images.\n",
        "    *   Train an `EfficientNet-B3` model on a single fold.\n",
        "3.  **Inference & Submission:**\n",
        "    *   Build an inference script that runs both models.\n",
        "    *   Implement the logic for the specific submission format: `opacity conf x y w h` for detections and `none 1 0 0 1 1` for images with no predicted boxes.\n",
        "    *   Aggregate image-level classifications to the study level (e.g., by taking the `max` of probabilities).\n",
        "    *   Generate a baseline `submission.csv` and verify its format.\n",
        "\n",
        "### Phase 3: Full CV Training & Ensembling (Hours 10-18)\n",
        "1.  **Full CV Training:** Train both the detector and classifier on all folds.\n",
        "2.  **Test-Time Augmentation (TTA):** Implement horizontal flip TTA for both models during inference.\n",
        "3.  **Ensembling:**\n",
        "    *   **Detector:** Use **Weighted Boxes Fusion (WBF)** to ensemble predictions from different fold models.\n",
        "    *   **Classifier:** Average the logits/probabilities from the fold models.\n",
        "4.  **Threshold Tuning:** Optimize the detector's confidence threshold and the WBF IoU threshold on the out-of-fold validation sets to maximize the CV mAP score.\n",
        "\n",
        "### Phase 4: Final Submission (Hours 18-24)\n",
        "1.  **Final Inference:** Run the full, ensembled pipeline with TTA on the test set.\n",
        "2.  **Sanity Checks:** Perform final checks on the `submission.csv` file, ensuring all study/image IDs are present and the format is perfect.\n",
        "3.  **Submit**."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "2c797f7a-b20e-4295-9973-722fd79132cc",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 1: Setup, EDA & Preprocessing\n",
        "\n",
        "## 1.1 Environment Setup\n",
        "\n",
        "First, let's install the necessary libraries and clone the YOLOv5 repository."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "3447c03f-5f21-4629-9ea3-b75827f4fc72",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "print(\"--- Installing packages ---\")\n",
        "packages = [\n",
        "    \"pydicom\", \"timm\", \"albumentations\", \"scikit-image\", \"pycocotools\",\n",
        "    \"pylibjpeg\", \"pylibjpeg-libjpeg\", \"python-gdcm\"\n",
        "]\n",
        "try:\n",
        "    # Removed -q flag for verbose output\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install'] + packages, check=True)\n",
        "    print(\"Packages installed successfully.\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Pip install failed: {e}\")\n",
        "\n",
        "print(\"\\n--- Cloning yolov5 repository ---\")\n",
        "if not os.path.exists('yolov5'):\n",
        "    try:\n",
        "        subprocess.run(['git', 'clone', 'https://github.com/ultralytics/yolov5.git'], check=True)\n",
        "        print(\"YOLOv5 cloned successfully.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Git clone failed: {e}\")\n",
        "else:\n",
        "    print(\"YOLOv5 directory already exists.\")\n",
        "\n",
        "print(\"\\n--- Installing yolov5 requirements ---\")\n",
        "yolov5_req_path = os.path.join('yolov5', 'requirements.txt')\n",
        "if os.path.exists(yolov5_req_path):\n",
        "    try:\n",
        "        # Removed -q flag for verbose output\n",
        "        subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', yolov5_req_path], check=True)\n",
        "        print(\"YOLOv5 requirements installed successfully.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"YOLOv5 requirements install failed: {e}\")\n",
        "else:\n",
        "    print(f\"Could not find {yolov5_req_path}\")\n",
        "\n",
        "print(\"\\nSetup cell finished.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "61add011-3ccc-4f08-b1ee-75c66675b8eb",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Metadata Exploration\n",
        "\n",
        "Now that the environment is set up, let's load the training metadata and inspect its structure. We have two main files: `train_study_level.csv` and `train_image_level.csv`."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "acfbc3aa-3c77-4daa-a5cb-bc78b0d069a6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "\n",
        "DATA_DIR = './'\n",
        "\n",
        "df_study = pd.read_csv(os.path.join(DATA_DIR, 'train_study_level.csv'))\n",
        "df_image = pd.read_csv(os.path.join(DATA_DIR, 'train_image_level.csv'))\n",
        "\n",
        "print(\"Study Level Data:\")\n",
        "display(df_study.head())\n",
        "print(f\"Shape: {df_study.shape}\")\n",
        "print(\"\\nImage Level Data:\")\n",
        "display(df_image.head())\n",
        "print(f\"Shape: {df_image.shape}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Study Level Data:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "                   id  Negative for Pneumonia  Typical Appearance  \\\n0  00086460a852_study                       0                   1   \n1  00292f8c37bd_study                       1                   0   \n2  005057b3f880_study                       1                   0   \n3  0051d9b12e72_study                       0                   0   \n4  00792b5c8852_study                       1                   0   \n\n   Indeterminate Appearance  Atypical Appearance  \n0                         0                    0  \n1                         0                    0  \n2                         0                    0  \n3                         0                    1  \n4                         0                    0  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Negative for Pneumonia</th>\n      <th>Typical Appearance</th>\n      <th>Indeterminate Appearance</th>\n      <th>Atypical Appearance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00086460a852_study</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00292f8c37bd_study</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>005057b3f880_study</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0051d9b12e72_study</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00792b5c8852_study</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (5448, 5)\n\nImage Level Data:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "                   id                                              boxes  \\\n0  000a312787f2_image  [{'x': 789.28836, 'y': 582.43035, 'width': 102...   \n1  000c3a3f293f_image                                                NaN   \n2  0012ff7358bc_image  [{'x': 677.42216, 'y': 197.97662, 'width': 867...   \n3  001398f4ff4f_image  [{'x': 2729, 'y': 2181.33331, 'width': 948.000...   \n4  001bd15d1891_image  [{'x': 623.23328, 'y': 1050, 'width': 714, 'he...   \n\n                                               label StudyInstanceUID  \n0  opacity 1 789.28836 582.43035 1815.94498 2499....     5776db0cec75  \n1                                     none 1 0 0 1 1     ff0879eb20ed  \n2  opacity 1 677.42216 197.97662 1545.21983 1197....     9d514ce429a7  \n3    opacity 1 2729 2181.33331 3677.00012 2785.33331     28dddc8559b2  \n4  opacity 1 623.23328 1050 1337.23328 2156 opaci...     dfd9fdd85a3e  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>boxes</th>\n      <th>label</th>\n      <th>StudyInstanceUID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000a312787f2_image</td>\n      <td>[{'x': 789.28836, 'y': 582.43035, 'width': 102...</td>\n      <td>opacity 1 789.28836 582.43035 1815.94498 2499....</td>\n      <td>5776db0cec75</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000c3a3f293f_image</td>\n      <td>NaN</td>\n      <td>none 1 0 0 1 1</td>\n      <td>ff0879eb20ed</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0012ff7358bc_image</td>\n      <td>[{'x': 677.42216, 'y': 197.97662, 'width': 867...</td>\n      <td>opacity 1 677.42216 197.97662 1545.21983 1197....</td>\n      <td>9d514ce429a7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>001398f4ff4f_image</td>\n      <td>[{'x': 2729, 'y': 2181.33331, 'width': 948.000...</td>\n      <td>opacity 1 2729 2181.33331 3677.00012 2785.33331</td>\n      <td>28dddc8559b2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>001bd15d1891_image</td>\n      <td>[{'x': 623.23328, 'y': 1050, 'width': 714, 'he...</td>\n      <td>opacity 1 623.23328 1050 1337.23328 2156 opaci...</td>\n      <td>dfd9fdd85a3e</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (5696, 4)\nError in callback <function _enable_matplotlib_integration.<locals>.configure_once at 0x70127d9c1e40> (for post_run_cell), with arguments args (<ExecutionResult object at 70115813cc10, execution_count=62 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 70115813fa10, raw_cell=\"import pandas as pd\nimport numpy as np\nimport os\n\n..\" transformed_cell=\"import pandas as pd\nimport numpy as np\nimport os\n\n..\" store_history=True silent=False shell_futures=True cell_id=None> result=None>,),kwargs {}:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'matplotlib' has no attribute 'backend_bases'",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/matplotlib_inline/backend_inline.py:222\u001b[39m, in \u001b[36m_enable_matplotlib_integration.<locals>.configure_once\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconfigure_once\u001b[39m(*args):\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     \u001b[43mactivate_matplotlib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m     configure_inline_support(ip, backend)\n\u001b[32m    224\u001b[39m     ip.events.unregister(\u001b[33m'\u001b[39m\u001b[33mpost_run_cell\u001b[39m\u001b[33m'\u001b[39m, configure_once)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:410\u001b[39m, in \u001b[36mactivate_matplotlib\u001b[39m\u001b[34m(backend)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;66;03m# Due to circular imports, pyplot may be only partially initialised\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[38;5;66;03m# when this function runs.\u001b[39;00m\n\u001b[32m    407\u001b[39m \u001b[38;5;66;03m# So avoid needing matplotlib attribute-lookup to access pyplot.\u001b[39;00m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mswitch_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m plt.show._needmain = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    413\u001b[39m \u001b[38;5;66;03m# We need to detect at runtime whether show() is called by the user.\u001b[39;00m\n\u001b[32m    414\u001b[39m \u001b[38;5;66;03m# For this, we wrap it into a decorator which adds a 'called' flag.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/matplotlib/pyplot.py:449\u001b[39m, in \u001b[36mswitch_backend\u001b[39m\u001b[34m(newbackend)\u001b[39m\n\u001b[32m    443\u001b[39m show = \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33mshow\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m# In that classical approach, backends are implemented as modules, but\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;66;03m# \"inherit\" default method implementations from backend_bases._Backend.\u001b[39;00m\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# This is achieved by creating a \"class\" that inherits from\u001b[39;00m\n\u001b[32m    448\u001b[39m \u001b[38;5;66;03m# backend_bases._Backend and whose body is filled with the module globals.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbackend_mod\u001b[39;00m(\u001b[43mmatplotlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackend_bases\u001b[49m._Backend):\n\u001b[32m    450\u001b[39m     \u001b[38;5;28mlocals\u001b[39m().update(\u001b[38;5;28mvars\u001b[39m(module))\n\u001b[32m    452\u001b[39m \u001b[38;5;66;03m# However, the newer approach for defining new_figure_manager and\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[38;5;66;03m# show is to derive them from canvas methods.  In that case, also\u001b[39;00m\n\u001b[32m    454\u001b[39m \u001b[38;5;66;03m# update backend_mod accordingly; also, per-backend customization of\u001b[39;00m\n\u001b[32m    455\u001b[39m \u001b[38;5;66;03m# draw_if_interactive is disabled.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/matplotlib/_api/__init__.py:218\u001b[39m, in \u001b[36mcaching_module_getattr.<locals>.__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m props:\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m props[name].\u001b[34m__get__\u001b[39m(instance)\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m    219\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__module__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mAttributeError\u001b[39m: module 'matplotlib' has no attribute 'backend_bases'"
          ]
        }
      ]
    },
    {
      "id": "ef9e8269-b60d-449b-a9d8-193e95821146",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Clean up IDs and merge the dataframes\n",
        "df_study['StudyInstanceUID'] = df_study['id'].apply(lambda x: x.replace('_study', ''))\n",
        "df_image['image_id'] = df_image['id'].apply(lambda x: x.replace('_image', ''))\n",
        "\n",
        "# Merge the two dataframes\n",
        "df_merged = df_image.merge(df_study, on='StudyInstanceUID', how='left')\n",
        "\n",
        "# Create a 'has_opacity' flag for easier analysis\n",
        "df_merged['has_opacity'] = df_merged['boxes'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
        "\n",
        "print(\"Merged Dataframe:\")\n",
        "display(df_merged.head())\n",
        "print(f\"Shape: {df_merged.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "bb7e6b34-8c01-4081-97b0-3bfd33890697",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# The plotting code is failing due to an environment issue with matplotlib.\n",
        "# For now, I will just print the numerical summaries to understand the distribution.\n",
        "\n",
        "print(\"--- Analyzing Data Distribution ---\")\n",
        "\n",
        "# Study-level classification distribution\n",
        "print(\"\\nDistribution of Study-Level Classes:\")\n",
        "df_study_labels = df_merged.drop_duplicates('StudyInstanceUID')\n",
        "study_counts = df_study_labels[['Negative for Pneumonia', 'Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance']].sum()\n",
        "print(study_counts)\n",
        "\n",
        "# Image-level opacity distribution\n",
        "print(\"\\nDistribution of Images with/without Opacity:\")\n",
        "opacity_counts = df_merged['has_opacity'].value_counts()\n",
        "print(opacity_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "d9343186-e0c9-4558-9d6e-87fc9a597a89",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Validation Strategy\n",
        "\n",
        "As recommended by the experts, I will use `StratifiedGroupKFold` to create the cross-validation folds. \n",
        "\n",
        "*   **Groups:** Splitting will be grouped by `StudyInstanceUID` to ensure that all images from a single study belong to the same fold (either train or validation), preventing data leakage.\n",
        "*   **Stratification:** The stratification will be based on the study-level classification labels to ensure that each fold has a similar distribution of the four classes."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "0cbe1334-d9d9-46da-b4d9-a1c9767d2795",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "\n",
        "N_SPLITS = 5\n",
        "\n",
        "# Prepare data for splitting. We need one row per study.\n",
        "# .reset_index(drop=True) is crucial to align integer indices from k-fold split with dataframe rows.\n",
        "df_folds = df_merged.drop_duplicates('StudyInstanceUID').reset_index(drop=True)\n",
        "\n",
        "# Create a single target column for stratification\n",
        "df_folds['stratify_col'] = df_folds[['Negative for Pneumonia', 'Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance']].idxmax(axis=1)\n",
        "\n",
        "# Get groups and stratification targets\n",
        "groups = df_folds['StudyInstanceUID']\n",
        "y_stratify = df_folds['stratify_col']\n",
        "\n",
        "df_folds['fold'] = -1\n",
        "\n",
        "sgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
        "\n",
        "# Use .loc with the integer indices from split() on the reset-index df_folds\n",
        "for fold, (train_idx, val_idx) in enumerate(sgkf.split(df_folds, y_stratify, groups)):\n",
        "    df_folds.loc[val_idx, 'fold'] = fold\n",
        "\n",
        "# Merge the fold and stratification info back into the main dataframe\n",
        "df_merged = df_merged.merge(df_folds[['StudyInstanceUID', 'fold', 'stratify_col']], on='StudyInstanceUID', how='left')\n",
        "\n",
        "print(\"Fold distribution:\")\n",
        "print(df_merged['fold'].value_counts())\n",
        "\n",
        "print(\"\\nValidation set stratification check (normalized counts per fold):\")\n",
        "# Use print instead of display to avoid matplotlib errors\n",
        "print(df_merged.groupby('fold')['stratify_col'].value_counts(normalize=True).unstack())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "a93ed8b6-24ba-491f-9583-b0a8b62082eb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Save the dataframe with fold information ---\n",
        "print(\"Saving the merged dataframe with fold information to 'df_train_folds.csv'...\")\n",
        "df_merged.to_csv('df_train_folds.csv', index=False)\n",
        "print(\"File saved successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "ccd262fb-9ee1-4f19-ba3d-908532356e1f",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 DICOM Preprocessing and Caching\n",
        "\n",
        "This is a critical step. I will now process the raw DICOM files into a more usable format (PNG) and cache them to disk. This will significantly speed up training and experimentation.\n",
        "\n",
        "The preprocessing pipeline will:\n",
        "1.  Read the DICOM file.\n",
        "2.  Apply the VOI (Value of Interest) LUT if available, which is crucial for correct windowing.\n",
        "3.  Invert the image if `Photometric Interpretation` is `MONOCHROME1`.\n",
        "4.  Convert the pixel data to a standard 8-bit format (0-255).\n",
        "5.  Save the processed image as a PNG file.\n",
        "\n",
        "I will process all images and save them to a new `train_png` directory."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "9884b67b-33e2-4d49-b1a8-73674b32b2cd",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# --- Fix 1: Add pip target directory to Python path ---\n",
        "pip_target_path = '/app/.pip-target'\n",
        "if pip_target_path not in sys.path:\n",
        "    print(f\"Adding '{pip_target_path}' to sys.path.\")\n",
        "    sys.path.insert(0, pip_target_path)\n",
        "else:\n",
        "    print(f\"'{pip_target_path}' is already in sys.path.\")\n",
        "\n",
        "# --- Fix 2: Add GDCM's C++ library path to LD_LIBRARY_PATH ---\n",
        "# This is crucial for pydicom to find the underlying decompression codecs.\n",
        "gdcm_lib_path = '/app/.pip-target/python_gdcm.libs'\n",
        "if os.path.exists(gdcm_lib_path):\n",
        "    print(f\"Found GDCM library path: {gdcm_lib_path}\")\n",
        "    current_ld_path = os.environ.get('LD_LIBRARY_PATH', '')\n",
        "    if gdcm_lib_path not in current_ld_path:\n",
        "        print(\"Adding GDCM library path to LD_LIBRARY_PATH.\")\n",
        "        os.environ['LD_LIBRARY_PATH'] = f\"{gdcm_lib_path}:{current_ld_path}\"\n",
        "    else:\n",
        "        print(\"GDCM library path already in LD_LIBRARY_PATH.\")\n",
        "    print(f\"Current LD_LIBRARY_PATH: {os.environ.get('LD_LIBRARY_PATH', '')}\")\n",
        "else:\n",
        "    print(f\"Warning: GDCM library path '{gdcm_lib_path}' not found.\")\n",
        "\n",
        "print(\"\\nEnvironment setup cell complete. A kernel restart is required for LD_LIBRARY_PATH changes to take full effect.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "9d8ead23-c3c1-4b9d-a339-d3cf9833aae0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pydicom\n",
        "import sys\n",
        "import os\n",
        "import importlib.metadata\n",
        "\n",
        "print(\"--- Environment Diagnostics ---\")\n",
        "\n",
        "# 1. Check LD_LIBRARY_PATH\n",
        "print(f\"LD_LIBRARY_PATH: {os.environ.get('LD_LIBRARY_PATH', 'Not Set')}\")\n",
        "\n",
        "# 2. Check package versions\n",
        "print(\"\\nPackage Versions:\")\n",
        "# Check for 'python-gdcm' which is the correct pip package name\n",
        "packages_to_check = ['pydicom', 'python-gdcm', 'pylibjpeg', 'pylibjpeg-libjpeg', 'packaging', 'setuptools']\n",
        "for package in packages_to_check:\n",
        "    try:\n",
        "        version = importlib.metadata.version(package)\n",
        "        print(f\"  - {package}: {version}\")\n",
        "    except importlib.metadata.PackageNotFoundError:\n",
        "        print(f\"  - {package}: NOT FOUND\")\n",
        "\n",
        "# 3. Check pydicom's available handlers\n",
        "print(\"\\n--- pydicom.config.pixel_data_handlers ---\")\n",
        "from pydicom import config\n",
        "print(config.pixel_data_handlers)\n",
        "\n",
        "# 4. Try importing gdcm directly to verify it's accessible\n",
        "print(\"\\n--- Attempting to import gdcm ---\")\n",
        "try:\n",
        "    import gdcm\n",
        "    print(\"Successfully imported gdcm\")\n",
        "    # Check the version of the underlying GDCM library\n",
        "    print(f\"gdcm library version: {gdcm.Version.GetVersion()}\")\n",
        "except ImportError as e:\n",
        "    print(f\"Failed to import gdcm: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while importing gdcm: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "15774e82-b2dc-409d-aef5-c01a025054d9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Pivoting to SimpleITK as per expert's fallback advice due to persistent pydicom errors.\n",
        "print(\"--- Installing SimpleITK ---\")\n",
        "try:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'SimpleITK'], check=True)\n",
        "    print(\"SimpleITK installed successfully.\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Pip install failed: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "ee1eecc9-d8d6-47fe-bfc9-cccdadef1438",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Pivoting to SimpleITK for DICOM Preprocessing ---\n",
        "import os, glob, cv2, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "import SimpleITK as sitk\n",
        "import pydicom\n",
        "\n",
        "print(\"--- Preprocessing with SimpleITK Fallback ---\")\n",
        "\n",
        "# --- 1. Map image IDs to their file paths (already done, but good to have here) ---\n",
        "if 'dcm_path' not in df_merged.columns or df_merged['dcm_path'].isna().all():\n",
        "    print(\"Re-building file path map for training images...\")\n",
        "    all_dcm_files = glob.glob('train/*/*/*.dcm')\n",
        "    image_id_to_path = {os.path.basename(p).replace('.dcm', ''): p for p in all_dcm_files}\n",
        "    df_merged['dcm_path'] = df_merged['image_id'].map(image_id_to_path)\n",
        "    print(f\"Found {len(all_dcm_files)} DICOM files.\")\n",
        "    print(f\"Mapped {df_merged['dcm_path'].notna().sum()} of {len(df_merged)} image IDs to paths.\")\n",
        "\n",
        "# --- 2. Define the processing function using SimpleITK ---\n",
        "def process_dicom_with_sitk(row, output_dir):\n",
        "    image_id = row['image_id']\n",
        "    dcm_path = row['dcm_path']\n",
        "    \n",
        "    if pd.isna(dcm_path):\n",
        "        return\n",
        "        \n",
        "    save_path = os.path.join(output_dir, f\"{image_id}.png\")\n",
        "    \n",
        "    if os.path.exists(save_path):\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Read metadata with pydicom (fast, avoids pixel data)\n",
        "        dicom_meta = pydicom.dcmread(dcm_path, stop_before_pixels=True)\n",
        "        \n",
        "        # Read image data with SimpleITK (robust)\n",
        "        sitk_image = sitk.ReadImage(dcm_path)\n",
        "        data = sitk.GetArrayFromImage(sitk_image).squeeze()\n",
        "        \n",
        "        # Apply Rescale Slope/Intercept from metadata\n",
        "        if 'RescaleSlope' in dicom_meta and 'RescaleIntercept' in dicom_meta:\n",
        "            slope = float(dicom_meta.RescaleSlope)\n",
        "            intercept = float(dicom_meta.RescaleIntercept)\n",
        "            data = data * slope + intercept\n",
        "            \n",
        "        # Invert MONOCHROME1 images\n",
        "        if dicom_meta.PhotometricInterpretation == \"MONOCHROME1\":\n",
        "            data = np.amax(data) - data\n",
        "        \n",
        "        # Apply lung windowing (as suggested by expert)\n",
        "        center = -600\n",
        "        width = 1500\n",
        "        min_val = center - width // 2\n",
        "        max_val = center + width // 2\n",
        "        data = np.clip(data, min_val, max_val)\n",
        "        \n",
        "        # Normalize to 8-bit (0-255)\n",
        "        data = data - np.min(data)\n",
        "        data = data / (np.max(data) + 1e-6)\n",
        "        data = (data * 255).astype(np.uint8)\n",
        "        \n",
        "        cv2.imwrite(save_path, data)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {dcm_path} (Image ID: {image_id}): {e}\")\n",
        "\n",
        "# --- 3. Process and save all training images ---\n",
        "output_dir = 'train_png'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"\\nProcessing DICOMs and saving to '{output_dir}'...\")\n",
        "\n",
        "for _, row in tqdm(df_merged.iterrows(), total=len(df_merged), desc=\"Processing All Train DICOMs with SITK\"):\n",
        "    process_dicom_with_sitk(row, output_dir)\n",
        "\n",
        "print(f\"\\nFinished processing and caching training images. Check '{output_dir}' for PNG files.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "9da23cd1-6b4c-44e8-974c-5b9adfa35aed",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 2: Baseline Model Training\n",
        "\n",
        "With the EDA and preprocessing underway, I'll now prepare the data for the first model: the YOLOv5 object detector.\n",
        "\n",
        "## 2.1 Prepare Data for YOLOv5\n",
        "\n",
        "YOLOv5 requires a specific directory structure and label format:\n",
        "1.  **Directory Structure:** A root directory containing `images` and `labels` subdirectories. Each of these will have `train` and `val` splits.\n",
        "2.  **Label Format:** For each image, there must be a corresponding `.txt` file with the same name. Each line in the file represents one bounding box in the format: `<class_index> <x_center_norm> <y_center_norm> <width_norm> <height_norm>`.\n",
        "\n",
        "I will now create these files and directories based on the folds defined earlier."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "c3fabc98-b251-4c32-acde-15f9f3ac2b83",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import ast\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "\n",
        "# --- Configuration ---\n",
        "YOLO_DATA_DIR = 'yolov5_data'\n",
        "FOLD_TO_VALIDATE = 0 # Use fold 0 for the validation set\n",
        "\n",
        "# --- Create YOLOv5 directory structure ---\n",
        "print(f\"Creating directory structure under '{YOLO_DATA_DIR}'...\")\n",
        "os.makedirs(os.path.join(YOLO_DATA_DIR, 'images/train'), exist_ok=True)\n",
        "os.makedirs(os.path.join(YOLO_DATA_DIR, 'images/val'), exist_ok=True)\n",
        "os.makedirs(os.path.join(YOLO_DATA_DIR, 'labels/train'), exist_ok=True)\n",
        "os.makedirs(os.path.join(YOLO_DATA_DIR, 'labels/val'), exist_ok=True)\n",
        "print(\"Directory structure created.\")\n",
        "\n",
        "# --- Get image dimensions if not already present ---\n",
        "if 'img_height' not in df_merged.columns or df_merged['img_height'].sum() == 0:\n",
        "    print(\"Reading DICOM metadata to get image dimensions...\")\n",
        "    df_merged['img_height'] = 0\n",
        "    df_merged['img_width'] = 0\n",
        "    \n",
        "    for index, row in tqdm(df_merged.iterrows(), total=len(df_merged), desc=\"Reading DICOM metadata\"):\n",
        "        try:\n",
        "            dicom_meta = pydicom.dcmread(row['dcm_path'], stop_before_pixels=True)\n",
        "            df_merged.loc[index, 'img_height'] = dicom_meta.Rows\n",
        "            df_merged.loc[index, 'img_width'] = dicom_meta.Columns\n",
        "        except Exception as e:\n",
        "            print(f\"Could not read dimensions for {row['image_id']}: {e}\")\n",
        "    print(\"Finished getting image dimensions.\")\n",
        "else:\n",
        "    print(\"Image dimensions already present in dataframe.\")\n",
        "\n",
        "# --- Create symlinks and label files ---\n",
        "print(\"Creating YOLO symlinks and label files...\")\n",
        "for _, row in tqdm(df_merged.iterrows(), total=len(df_merged), desc=\"Creating YOLO files\"):\n",
        "    split = 'val' if row['fold'] == FOLD_TO_VALIDATE else 'train'\n",
        "    image_id = row['image_id']\n",
        "    \n",
        "    # 1. Create symlink to the cached PNG image\n",
        "    src_png_path = os.path.abspath(os.path.join('train_png', f\"{image_id}.png\"))\n",
        "    dst_img_path = os.path.join(YOLO_DATA_DIR, 'images', split, f\"{image_id}.png\")\n",
        "    \n",
        "    if os.path.exists(src_png_path):\n",
        "        if not os.path.lexists(dst_img_path):\n",
        "            os.symlink(src_png_path, dst_img_path)\n",
        "    else:\n",
        "        # Skip if the source PNG doesn't exist (might have failed preprocessing)\n",
        "        continue\n",
        "\n",
        "    # 2. Create the label file\n",
        "    label_path = os.path.join(YOLO_DATA_DIR, 'labels', split, f\"{image_id}.txt\")\n",
        "    \n",
        "    with open(label_path, 'w') as f:\n",
        "        if row['has_opacity'] == 1 and isinstance(row['boxes'], str):\n",
        "            try:\n",
        "                boxes = ast.literal_eval(row['boxes'])\n",
        "            except (ValueError, SyntaxError):\n",
        "                continue\n",
        "\n",
        "            img_h = row['img_height']\n",
        "            img_w = row['img_width']\n",
        "            \n",
        "            if img_h == 0 or img_w == 0:\n",
        "                continue # Skip if dimensions are invalid\n",
        "\n",
        "            for box in boxes:\n",
        "                class_id = 0 # single class 'opacity'\n",
        "                \n",
        "                x = box['x']\n",
        "                y = box['y']\n",
        "                w = box['width']\n",
        "                h = box['height']\n",
        "                \n",
        "                x_center_norm = (x + w / 2) / img_w\n",
        "                y_center_norm = (y + h / 2) / img_h\n",
        "                width_norm = w / img_w\n",
        "                height_norm = h / img_h\n",
        "                \n",
        "                f.write(f\"{class_id} {x_center_norm:.6f} {y_center_norm:.6f} {width_norm:.6f} {height_norm:.6f}\\n\")\n",
        "\n",
        "print(\"YOLOv5 data preparation finished.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "1ed24d79-70f4-48a2-8d58-c0ee86bf41c3",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Create YOLOv5 Dataset Config\n",
        "\n",
        "Before training, I need to create a YAML file that tells the YOLOv5 training script where to find the images and what the class names are."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "23c7927c-926d-4814-9479-1679af005e27",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "YOLO_DATA_DIR = 'yolov5_data'\n",
        "config = {\n",
        "    'path': os.path.abspath(YOLO_DATA_DIR), # dataset root dir\n",
        "    'train': 'images/train',  # train images (relative to 'path')\n",
        "    'val': 'images/val',  # val images (relative to 'path')\n",
        "    'nc': 1,  # number of classes\n",
        "    'names': ['opacity']  # class names\n",
        "}\n",
        "\n",
        "config_path = os.path.join('yolov5', 'data', 'siim_covid19.yaml')\n",
        "os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
        "\n",
        "with open(config_path, 'w') as f:\n",
        "    yaml.dump(config, f, default_flow_style=False)\n",
        "\n",
        "print(f\"YOLOv5 config file created at: {config_path}\")\n",
        "print(\"\\n--- Config Content ---\")\n",
        "with open(config_path, 'r') as f:\n",
        "    print(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "89040aed-8357-48db-91f6-43f898125c23",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Train YOLOv5 Detector\n",
        "\n",
        "All data preparation is complete. I will now train the YOLOv5s model on `fold 0` for a few epochs to establish a baseline. I'll use pretrained weights to speed up convergence.\n",
        "\n",
        "The training command will be executed directly in the notebook using `!python`. The key parameters are:\n",
        "- `--img 640`: Image size.\n",
        "- `--batch 16`: Batch size, chosen to fit on the available GPU.\n",
        "- `--epochs 15`: Number of training epochs for this baseline run.\n",
        "- `--data yolov5/data/siim_covid19.yaml`: The dataset configuration file.\n",
        "- `--weights yolov5s.pt`: Pretrained weights to start from.\n",
        "- `--project yolov5_runs/train`: The output directory for training results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "54604470-1306-4325-9abb-9f2bccec5bb2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "command = [\n",
        "    sys.executable,\n",
        "    'yolov5/train.py',\n",
        "    '--img', '640',\n",
        "    '--batch', '16',\n",
        "    '--epochs', '15',\n",
        "    '--data', 'yolov5/data/siim_covid19.yaml',\n",
        "    '--weights', 'yolov5s.pt',\n",
        "    '--project', 'yolov5_runs/train',\n",
        "    '--name', 'baseline_fold0',\n",
        "    '--exist-ok'\n",
        "]\n",
        "\n",
        "print(f\"Running command: {' '.join(command)}\")\n",
        "try:\n",
        "    # Using subprocess.run to avoid issues with IPython's '!' magic\n",
        "    subprocess.run(command, check=True)\n",
        "    print(\"\\n--- YOLOv5 training finished successfully. ---\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"\\n--- YOLOv5 training failed with exit code {e.returncode}. ---\")\n",
        "    # The output from the command will be printed to stderr/stdout automatically."
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}