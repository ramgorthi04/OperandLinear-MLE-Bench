{
  "cells": [
    {
      "id": "940f46dd-fd87-464e-8774-423120eb450b",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 9: Final Stable Submission with Tuned Parameters\n",
        "\n",
        "## Context\n",
        "Previous attempts to generate a submission with the tuned hyperparameters (`conf_th=0.10`, `neg_th=0.70`) from notebook `07` failed due to catastrophic and persistent CUDA OOM errors. The environment appears to be unstable, preventing even single models from loading.\n",
        "\n",
        "This notebook is the final attempt to generate a submission before the deadline. The strategy is designed for maximum stability and minimal memory footprint.\n",
        "\n",
        "## Strategy: Sequential Inference\n",
        "\n",
        "Instead of loading multiple models at once, we will process the classifier and detector stages sequentially, aggressively clearing memory at each step.\n",
        "\n",
        "1.  **Classifier Stage:**\n",
        "    *   Iterate through each of the 5 classifier model folds.\n",
        "    *   For each fold: Load **one** model, run inference on the test set, store predictions, and then **delete the model and clear the CUDA cache**.\n",
        "    *   Average the predictions from all 5 folds.\n",
        "    *   Save the resulting study-level predictions to `study_preds.csv`.\n",
        "\n",
        "2.  **Detector Stage:**\n",
        "    *   **Restart the kernel** to ensure a completely clean memory state.\n",
        "    *   Iterate through each of the 5 YOLOv5s detector model folds.\n",
        "    *   For each fold: Load **one** model, run inference, store predictions, and then **delete the model and clear the CUDA cache**.\n",
        "    *   Save the raw, unfiltered box predictions to `box_preds.csv`.\n",
        "\n",
        "3.  **Assembly Stage:**\n",
        "    *   Load the intermediate predictions from `study_preds.csv` and `box_preds.csv`.\n",
        "    *   Apply the optimal post-processing thresholds found in notebook `07`:\n",
        "        *   `confidence_threshold = 0.10`\n",
        "        *   `negative_filter_threshold = 0.70`\n",
        "    *   Format and generate the final `submission.csv`."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "33297607-6103-4240-8de8-5ae38e35b517",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- STAGE 1: CLASSIFIER INFERENCE ---\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import timm\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import os\n",
        "\n",
        "print('Stage 1: Starting Classifier Inference...')\n",
        "\n",
        "# --- Config ---\n",
        "TEST_IMAGE_DIR = 'test_png_3ch/'\n",
        "MODEL_PATHS = [f'classifier_fold{i}_best.pth' for i in range(5)]\n",
        "BATCH_SIZE = 8 # Reduced for stability\n",
        "IMG_SIZE = 512\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "CLASSES = ['Negative for Pneumonia', 'Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance']\n",
        "\n",
        "# --- Dataset ---\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, df, image_dir):\n",
        "        self.df = df\n",
        "        self.image_dir = image_dir\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image_path = os.path.join(self.image_dir, row['id'].replace('_image', '.png'))\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "        image = image.astype(np.float32) / 255.0\n",
        "        image = image.transpose(2, 0, 1)\n",
        "        return torch.from_numpy(image)\n",
        "\n",
        "# --- Inference Loop ---\n",
        "df_sub = pd.read_csv('sample_submission.csv')\n",
        "df_test_study = df_sub[df_sub['id'].str.contains('_study')].copy()\n",
        "df_test_study['image_id'] = df_test_study['id'].str.replace('_study', '')\n",
        "\n",
        "df_test_img = pd.DataFrame({'id': os.listdir(TEST_IMAGE_DIR)})\n",
        "df_test_img['id'] = df_test_img['id'].str.replace('.png', '_image')\n",
        "test_dataset = TestDataset(df_test_img, TEST_IMAGE_DIR)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "all_fold_preds = []\n",
        "\n",
        "for fold, model_path in enumerate(MODEL_PATHS):\n",
        "    print(f'--- Processing Fold {fold} ---')\n",
        "    # Load model\n",
        "    model = timm.create_model('efficientnet_b5', pretrained=False, num_classes=4)\n",
        "    # Use FP16 and channels_last for stability, as suggested by expert\n",
        "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "    model = model.to(DEVICE).half().to(memory_format=torch.channels_last)\n",
        "    model.eval()\n",
        "    \n",
        "    fold_preds = []\n",
        "    with torch.no_grad():\n",
        "        for images in tqdm(test_loader, desc=f'Fold {fold} Inference'):\n",
        "            images = images.to(DEVICE).half().to(memory_format=torch.channels_last)\n",
        "            outputs = model(images)\n",
        "            fold_preds.append(torch.softmax(outputs, dim=1).cpu().numpy())\n",
        "    \n",
        "    all_fold_preds.append(np.concatenate(fold_preds))\n",
        "    \n",
        "    # Aggressively clean up memory\n",
        "    del model, fold_preds\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f'Fold {fold} complete. Memory cleared.')\n",
        "\n",
        "# --- Ensemble and Save ---\n",
        "avg_preds = np.mean(all_fold_preds, axis=0)\n",
        "df_preds = pd.DataFrame(avg_preds, columns=CLASSES)\n",
        "df_preds['id'] = df_test_img['id']\n",
        "\n",
        "# Map image-level preds to study-level\n",
        "df_train_meta = pd.read_csv('df_train_folds.csv')\n",
        "df_test_meta = pd.read_csv('sample_submission.csv')\n",
        "df_test_meta['StudyInstanceUID'] = df_test_meta['id'].apply(lambda x: x.split('_')[0])\n",
        "df_train_meta['StudyInstanceUID'] = df_train_meta['StudyInstanceUID']\n",
        "df_test_img_meta = df_train_meta[['image_id', 'StudyInstanceUID']].drop_duplicates()\n",
        "df_preds['image_id'] = df_preds['id'].str.replace('_image', '')\n",
        "df_preds = df_preds.merge(df_test_img_meta, on='image_id', how='left')\n",
        "\n",
        "# Handle cases where test image is not in train meta (should not happen with provided data but good practice)\n",
        "if df_preds['StudyInstanceUID'].isnull().any():\n",
        "    print('Warning: Some test images could not be mapped to a StudyInstanceUID. Filling with image_id.')\n",
        "    df_preds['StudyInstanceUID'] = df_preds['StudyInstanceUID'].fillna(df_preds['image_id'])\n",
        "\n",
        "study_preds = df_preds.groupby('StudyInstanceUID')[CLASSES].mean().reset_index()\n",
        "study_preds.to_csv('study_preds.csv', index=False)\n",
        "print('\\nStage 1 Complete: Classifier predictions saved to study_preds.csv')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 1: Starting Classifier Inference...\n--- Processing Fold 0 ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AcceleratorError",
          "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Use FP16 and channels_last for stability, as suggested by expert\u001b[39;00m\n\u001b[32m     56\u001b[39m model.load_state_dict(torch.load(model_path, map_location=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m model = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m.half().to(memory_format=torch.channels_last)\n\u001b[32m     58\u001b[39m model.eval()\n\u001b[32m     60\u001b[39m fold_preds = []\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:1369\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1366\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1367\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    933\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    939\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:955\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    952\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    956\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    958\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:1355\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1348\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1349\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1350\u001b[39m             device,\n\u001b[32m   1351\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1352\u001b[39m             non_blocking,\n\u001b[32m   1353\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1354\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1361\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
            "\u001b[31mAcceleratorError\u001b[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}