{
  "cells": [
    {
      "id": "60257ce2-3016-490d-82c6-27fb12808d0b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- 1. Setup: Imports and Configuration ---\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import timm\n",
        "from tqdm import tqdm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "import glob\n",
        "from torchvision.ops import nms\n",
        "\n",
        "# --- Configuration ---\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Classifier Config\n",
        "CLF_MODEL_NAME = 'tf_efficientnet_b5_ns'\n",
        "CLF_IMG_SIZE = 512\n",
        "CLF_BATCH_SIZE = 4 # Reduced from 8 to prevent OOM\n",
        "CLF_MODEL_PATHS = sorted(glob.glob('classifier_fold*_best.pth'))\n",
        "CLASSES = ['Negative for Pneumonia', 'Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance']\n",
        "CLASS_MAP_LOWER = {\n",
        "    'Negative for Pneumonia': 'negative',\n",
        "    'Typical Appearance': 'typical',\n",
        "    'Indeterminate Appearance': 'indeterminate',\n",
        "    'Atypical Appearance': 'atypical'\n",
        "}\n",
        "\n",
        "# Detector Config\n",
        "DET_IMG_SIZE = 640\n",
        "DET_BATCH_SIZE = 16\n",
        "DET_MODEL_PATHS = sorted(glob.glob('yolov5_runs/train_cv/yolov5s_fold*/weights/best.pt'))\n",
        "\n",
        "# Post-Processing & Ensemble Config\n",
        "# NEW TUNED PARAMS from notebook 07:\n",
        "TUNED_CONF_THRESHOLD = 0.10\n",
        "TUNED_NEG_FILTER_THRESHOLD = 0.70\n",
        "DETECTOR_RAW_CONF_THRESHOLD = 0.001 # Keep this low to get all boxes for subsequent filtering\n",
        "NMS_IOU_THRESHOLD = 0.5 # This was not tuned, keep as is\n",
        "\n",
        "# Data Paths\n",
        "TEST_IMAGE_DIR_3CH = 'test_png_3ch/' # For classifier\n",
        "TEST_IMAGE_DIR_1CH = 'test_png/'     # For detector\n",
        "SAMPLE_SUB_PATH = 'sample_submission.csv'\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "print(f\"Found {len(CLF_MODEL_PATHS)} classifier models.\")\n",
        "print(f\"Found {len(DET_MODEL_PATHS)} detector models.\")\n",
        "print(f\"Using TUNED parameters: conf_th={TUNED_CONF_THRESHOLD}, neg_filter_th={TUNED_NEG_FILTER_THRESHOLD}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\nFound 5 classifier models.\nFound 5 detector models.\nUsing TUNED parameters: conf_th=0.1, neg_filter_th=0.7\n"
          ]
        }
      ]
    },
    {
      "id": "812dbce0-3db1-45a5-9af3-3e3b3b771d47",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- 2. Classifier Inference (5-Fold Ensemble) ---\n",
        "\n",
        "def get_transforms(img_size):\n",
        "    return A.Compose([\n",
        "        A.Resize(img_size, img_size),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "class SIIMCOVIDTestDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image)\n",
        "            image = augmented['image']\n",
        "            \n",
        "        return image, os.path.basename(image_path).replace('.png', '')\n",
        "\n",
        "print(\"--- Starting Classifier Inference ---\")\n",
        "\n",
        "# Load test file list\n",
        "df_test_imgs = pd.DataFrame({'id': os.listdir(TEST_IMAGE_DIR_3CH)})\n",
        "df_test_imgs['image_id'] = df_test_imgs['id'].str.replace('.png', '')\n",
        "df_test_imgs['path'] = TEST_IMAGE_DIR_3CH + df_test_imgs['id']\n",
        "\n",
        "# Create dataset and dataloader\n",
        "# FIX: Set num_workers=0 to prevent hanging issue\n",
        "test_dataset = SIIMCOVIDTestDataset(df_test_imgs['path'].values, transform=get_transforms(CLF_IMG_SIZE))\n",
        "test_loader = DataLoader(test_dataset, batch_size=CLF_BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "# Load all 5 classifier models\n",
        "models = []\n",
        "for path in CLF_MODEL_PATHS:\n",
        "    model = timm.create_model(CLF_MODEL_NAME, pretrained=False, num_classes=len(CLASSES))\n",
        "    model.load_state_dict(torch.load(path))\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    models.append(model)\n",
        "print(f\"Loaded {len(models)} classifier models onto {DEVICE}\")\n",
        "\n",
        "# Run inference\n",
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "    for images, image_ids in tqdm(test_loader, desc=\"Classifier Inference\"):\n",
        "        images = images.to(DEVICE)\n",
        "        \n",
        "        # Get predictions from all models and average them\n",
        "        batch_preds = torch.zeros((images.size(0), len(CLASSES)), device=DEVICE)\n",
        "        for model in models:\n",
        "            batch_preds += torch.softmax(model(images), dim=1)\n",
        "        batch_preds /= len(models)\n",
        "        \n",
        "        for i, image_id in enumerate(image_ids):\n",
        "            preds = {f'pred_{cls}': batch_preds[i, j].item() for j, cls in enumerate(CLASSES)}\n",
        "            preds['image_id'] = image_id\n",
        "            all_preds.append(preds)\n",
        "\n",
        "df_image_preds = pd.DataFrame(all_preds)\n",
        "\n",
        "# Aggregate to study level\n",
        "test_dcm_paths = glob.glob('test/*/*/*.dcm')\n",
        "test_map_data = []\n",
        "for path in test_dcm_paths:\n",
        "    parts = path.split('/')\n",
        "    study_id = parts[1]\n",
        "    image_id = parts[-1].replace('.dcm', '')\n",
        "    test_map_data.append({'image_id': image_id, 'StudyInstanceUID': study_id})\n",
        "study_id_map = pd.DataFrame(test_map_data).drop_duplicates()\n",
        "\n",
        "df_image_preds_with_study = df_image_preds.merge(study_id_map, on='image_id', how='left')\n",
        "\n",
        "pred_cols = [f'pred_{c}' for c in CLASSES]\n",
        "df_study_preds = df_image_preds_with_study.groupby('StudyInstanceUID')[pred_cols].mean().reset_index()\n",
        "\n",
        "print(\"Classifier inference complete. Study-level predictions created.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Classifier Inference ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name tf_efficientnet_b5_ns to current tf_efficientnet_b5.ns_jft_in1k.\n  model = create_fn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 5 classifier models onto cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifier Inference:   0%|          | 0/160 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifier Inference:   0%|          | 0/160 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 23.72 GiB of which 263.12 MiB is free. Process 22789 has 227.00 MiB memory in use. Process 43368 has 599.00 MiB memory in use. Process 54134 has 3.35 GiB memory in use. Process 342427 has 1.80 GiB memory in use. Process 362585 has 11.13 GiB memory in use. Process 369303 has 933.00 MiB memory in use. Process 408019 has 1.75 GiB memory in use. Process 413696 has 1.39 GiB memory in use. Of the allocated memory 1.12 GiB is allocated by PyTorch, and 10.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     58\u001b[39m batch_preds = torch.zeros((images.size(\u001b[32m0\u001b[39m), \u001b[38;5;28mlen\u001b[39m(CLASSES)), device=DEVICE)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     batch_preds += torch.softmax(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m, dim=\u001b[32m1\u001b[39m)\n\u001b[32m     61\u001b[39m batch_preds /= \u001b[38;5;28mlen\u001b[39m(models)\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, image_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(image_ids):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/timm/models/efficientnet.py:339\u001b[39m, in \u001b[36mEfficientNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Forward pass.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    340\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.forward_head(x)\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/timm/models/efficientnet.py:312\u001b[39m, in \u001b[36mEfficientNet.forward_features\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m    311\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Forward pass through feature extraction layers.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv_stem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    313\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.bn1(x)\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.grad_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting():\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/timm/layers/conv2d_same.py:53\u001b[39m, in \u001b[36mConv2dSame.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconv2d_same\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/timm/layers/conv2d_same.py:28\u001b[39m, in \u001b[36mconv2d_same\u001b[39m\u001b[34m(x, weight, bias, stride, padding, dilation, groups)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconv2d_same\u001b[39m(\n\u001b[32m     19\u001b[39m         x,\n\u001b[32m     20\u001b[39m         weight: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m         groups: \u001b[38;5;28mint\u001b[39m = \u001b[32m1\u001b[39m,\n\u001b[32m     26\u001b[39m ):\n\u001b[32m     27\u001b[39m     x = pad_same(x, weight.shape[-\u001b[32m2\u001b[39m:], stride, dilation)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 23.72 GiB of which 263.12 MiB is free. Process 22789 has 227.00 MiB memory in use. Process 43368 has 599.00 MiB memory in use. Process 54134 has 3.35 GiB memory in use. Process 342427 has 1.80 GiB memory in use. Process 362585 has 11.13 GiB memory in use. Process 369303 has 933.00 MiB memory in use. Process 408019 has 1.75 GiB memory in use. Process 413696 has 1.39 GiB memory in use. Of the allocated memory 1.12 GiB is allocated by PyTorch, and 10.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "id": "586acd94-2c3b-4a5c-a57f-fce8b6155d41",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- 3. Detector Inference (5-Fold Ensemble) ---\n",
        "\n",
        "print(\"--- Starting Detector Inference ---\")\n",
        "\n",
        "# Load all 5 detector models\n",
        "det_models = []\n",
        "for path in tqdm(DET_MODEL_PATHS, desc=\"Loading detector models\"):\n",
        "    model = torch.hub.load(\n",
        "        'ultralytics/yolov5',\n",
        "        'custom',\n",
        "        path=path,\n",
        "        force_reload=True,\n",
        "        _verbose=False\n",
        "    )\n",
        "    model.to(DEVICE).eval()\n",
        "    model.conf = DETECTOR_RAW_CONF_THRESHOLD # Use a very low threshold to get all possible boxes\n",
        "    model.iou = NMS_IOU_THRESHOLD\n",
        "    det_models.append(model)\n",
        "print(f\"Loaded {len(det_models)} detector models onto {DEVICE}\")\n",
        "\n",
        "# Get test image paths\n",
        "test_image_paths = sorted(glob.glob(f'{TEST_IMAGE_DIR_1CH}/*.png'))\n",
        "test_image_ids = [os.path.basename(p).replace('.png', '') for p in test_image_paths]\n",
        "print(f\"Found {len(test_image_paths)} test images for detection.\")\n",
        "\n",
        "# Run inference and collect all raw predictions from all models\n",
        "all_det_preds = []\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(0, len(test_image_paths), DET_BATCH_SIZE), desc=\"Detector Inference\"):\n",
        "        batch_paths = test_image_paths[i:i+DET_BATCH_SIZE]\n",
        "        batch_image_ids = test_image_ids[i:i+DET_BATCH_SIZE]\n",
        "\n",
        "        batch_model_preds = [[] for _ in range(len(batch_paths))]\n",
        "\n",
        "        for model in det_models:\n",
        "            results = model(batch_paths, size=DET_IMG_SIZE)\n",
        "            preds_df_list = results.pandas().xyxy\n",
        "            \n",
        "            for j, preds_df in enumerate(preds_df_list):\n",
        "                if not preds_df.empty:\n",
        "                    batch_model_preds[j].append(preds_df[['xmin', 'ymin', 'xmax', 'ymax', 'confidence']].values)\n",
        "\n",
        "        for j, image_id in enumerate(batch_image_ids):\n",
        "            if batch_model_preds[j]:\n",
        "                combined_preds = np.vstack(batch_model_preds[j])\n",
        "                for pred in combined_preds:\n",
        "                    all_det_preds.append({\n",
        "                        'image_id': image_id,\n",
        "                        'x_min': pred[0],\n",
        "                        'y_min': pred[1],\n",
        "                        'x_max': pred[2],\n",
        "                        'y_max': pred[3],\n",
        "                        'confidence': pred[4]\n",
        "                    })\n",
        "\n",
        "df_det_preds_raw = pd.DataFrame(all_det_preds)\n",
        "print(\"Detector inference complete. Raw box predictions created.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Detector Inference ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading detector models:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /app/.cache/torch/hub/master.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLoading detector models:   0%|          | 0/5 [00:01<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.72 GiB of which 251.12 MiB is free. Process 22789 has 227.00 MiB memory in use. Process 43368 has 599.00 MiB memory in use. Process 54134 has 3.35 GiB memory in use. Process 342427 has 1.80 GiB memory in use. Process 362585 has 11.13 GiB memory in use. Process 369303 has 933.00 MiB memory in use. Process 408019 has 1.75 GiB memory in use. Process 413696 has 1.40 GiB memory in use. Of the allocated memory 1.14 GiB is allocated by PyTorch, and 56.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Cache may be out of date, try `force_reload=True` or see https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading for help.",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:70\u001b[39m, in \u001b[36m_create\u001b[39m\u001b[34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     model = \u001b[43mDetectMultiBackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoshape\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# detection model\u001b[39;00m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m autoshape:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:489\u001b[39m, in \u001b[36mDetectMultiBackend.__init__\u001b[39m\u001b[34m(self, weights, device, dnn, data, fp16, fuse)\u001b[39m\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pt:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     model = \u001b[43mattempt_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfuse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    490\u001b[39m     stride = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mint\u001b[39m(model.stride.max()), \u001b[32m32\u001b[39m)  \u001b[38;5;66;03m# model stride\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/ultralytics_yolov5_master/models/experimental.py:100\u001b[39m, in \u001b[36mattempt_load\u001b[39m\u001b[34m(weights, device, inplace, fuse)\u001b[39m\n\u001b[32m     99\u001b[39m ckpt = torch_load(attempt_download(w), map_location=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# load\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m ckpt = \u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# Model compatibility updates\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:1175\u001b[39m, in \u001b[36mModule.float\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1167\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Casts all floating point parameters and buffers to ``float`` datatype.\u001b[39;00m\n\u001b[32m   1168\u001b[39m \n\u001b[32m   1169\u001b[39m \u001b[33;03m.. note::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1173\u001b[39m \u001b[33;03m    Module: self\u001b[39;00m\n\u001b[32m   1174\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/ultralytics_yolov5_master/models/yolo.py:208\u001b[39m, in \u001b[36mBaseModel._apply\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Applies transformations like to(), cpu(), cuda(), half() to model tensors excluding parameters or registered\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[33;03mbuffers.\u001b[39;00m\n\u001b[32m    207\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m m = \u001b[38;5;28mself\u001b[39m.model[-\u001b[32m1\u001b[39m]  \u001b[38;5;66;03m# Detect()\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:955\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    956\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:1175\u001b[39m, in \u001b[36mModule.float.<locals>.<lambda>\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1167\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Casts all floating point parameters and buffers to ``float`` datatype.\u001b[39;00m\n\u001b[32m   1168\u001b[39m \n\u001b[32m   1169\u001b[39m \u001b[33;03m.. note::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1173\u001b[39m \u001b[33;03m    Module: self\u001b[39;00m\n\u001b[32m   1174\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;28;01melse\u001b[39;00m t)\n",
            "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.72 GiB of which 253.12 MiB is free. Process 22789 has 227.00 MiB memory in use. Process 43368 has 599.00 MiB memory in use. Process 54134 has 3.35 GiB memory in use. Process 342427 has 1.80 GiB memory in use. Process 362585 has 11.13 GiB memory in use. Process 369303 has 933.00 MiB memory in use. Process 408019 has 1.75 GiB memory in use. Process 413696 has 1.40 GiB memory in use. Of the allocated memory 1.13 GiB is allocated by PyTorch, and 3.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:85\u001b[39m, in \u001b[36m_create\u001b[39m\u001b[34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[39m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m         model = \u001b[43mattempt_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# arbitrary model\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/ultralytics_yolov5_master/models/experimental.py:100\u001b[39m, in \u001b[36mattempt_load\u001b[39m\u001b[34m(weights, device, inplace, fuse)\u001b[39m\n\u001b[32m     99\u001b[39m ckpt = torch_load(attempt_download(w), map_location=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# load\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m ckpt = \u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m.float()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# Model compatibility updates\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:1369\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1367\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/ultralytics_yolov5_master/models/yolo.py:208\u001b[39m, in \u001b[36mBaseModel._apply\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Applies transformations like to(), cpu(), cuda(), half() to model tensors excluding parameters or registered\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[33;03mbuffers.\u001b[39;00m\n\u001b[32m    207\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m m = \u001b[38;5;28mself\u001b[39m.model[-\u001b[32m1\u001b[39m]  \u001b[38;5;66;03m# Detect()\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
            "    \u001b[31m[... skipping similar frames: Module._apply at line 928 (3 times)]\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:955\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    956\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:1355\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1349\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1350\u001b[39m             device,\n\u001b[32m   1351\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1352\u001b[39m             non_blocking,\n\u001b[32m   1353\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1354\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.72 GiB of which 251.12 MiB is free. Process 22789 has 227.00 MiB memory in use. Process 43368 has 599.00 MiB memory in use. Process 54134 has 3.35 GiB memory in use. Process 342427 has 1.80 GiB memory in use. Process 362585 has 11.13 GiB memory in use. Process 369303 has 933.00 MiB memory in use. Process 408019 has 1.75 GiB memory in use. Process 413696 has 1.40 GiB memory in use. Of the allocated memory 1.14 GiB is allocated by PyTorch, and 56.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m det_models = []\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m tqdm(DET_MODEL_PATHS, desc=\u001b[33m\"\u001b[39m\u001b[33mLoading detector models\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     model = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43multralytics/yolov5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcustom\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_verbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     model.to(DEVICE).eval()\n\u001b[32m     16\u001b[39m     model.conf = DETECTOR_RAW_CONF_THRESHOLD \u001b[38;5;66;03m# Use a very low threshold to get all possible boxes\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/hub.py:647\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source == \u001b[33m\"\u001b[39m\u001b[33mgithub\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    638\u001b[39m     repo_or_dir = _get_cache_or_reload(\n\u001b[32m    639\u001b[39m         repo_or_dir,\n\u001b[32m    640\u001b[39m         force_reload,\n\u001b[32m   (...)\u001b[39m\u001b[32m    644\u001b[39m         skip_validation=skip_validation,\n\u001b[32m    645\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m model = \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/hub.py:680\u001b[39m, in \u001b[36m_load_local\u001b[39m\u001b[34m(hubconf_dir, model, *args, **kwargs)\u001b[39m\n\u001b[32m    677\u001b[39m     hub_module = _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[32m    679\u001b[39m     entry = _load_entry_from_hubconf(hub_module, model)\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m     model = \u001b[43mentry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:135\u001b[39m, in \u001b[36mcustom\u001b[39m\u001b[34m(path, autoshape, _verbose, device)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcustom\u001b[39m(path=\u001b[33m\"\u001b[39m\u001b[33mpath/to/model.pt\u001b[39m\u001b[33m\"\u001b[39m, autoshape=\u001b[38;5;28;01mTrue\u001b[39;00m, _verbose=\u001b[38;5;28;01mTrue\u001b[39;00m, device=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    107\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[33;03m    Loads a custom or local YOLOv5 model from a given path with optional autoshaping and device specification.\u001b[39;00m\n\u001b[32m    109\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    133\u001b[39m \u001b[33;03m        ```\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoshape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_verbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:103\u001b[39m, in \u001b[36m_create\u001b[39m\u001b[34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[39m\n\u001b[32m    101\u001b[39m help_url = \u001b[33m\"\u001b[39m\u001b[33mhttps://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    102\u001b[39m s = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Cache may be out of date, try `force_reload=True` or see \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhelp_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for help.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(s) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[31mException\u001b[39m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.72 GiB of which 251.12 MiB is free. Process 22789 has 227.00 MiB memory in use. Process 43368 has 599.00 MiB memory in use. Process 54134 has 3.35 GiB memory in use. Process 342427 has 1.80 GiB memory in use. Process 362585 has 11.13 GiB memory in use. Process 369303 has 933.00 MiB memory in use. Process 408019 has 1.75 GiB memory in use. Process 413696 has 1.40 GiB memory in use. Of the allocated memory 1.14 GiB is allocated by PyTorch, and 56.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Cache may be out of date, try `force_reload=True` or see https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading for help."
          ]
        }
      ]
    },
    {
      "id": "5783d528-60f5-46b3-bfdd-a43a6c68d3b8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- 4. Combine, Post-Process, and Format for Submission ---\n",
        "\n",
        "print(\"--- Combining, Post-Processing, and Formatting with Tuned Parameters ---\")\n",
        "\n",
        "# --- Step 4.1: Apply Tuned Filtering and NMS ---\n",
        "\n",
        "# First, filter raw detector predictions by the tuned confidence threshold\n",
        "print(f\"Raw detector predictions from all models: {len(df_det_preds_raw) if 'df_det_preds_raw' in locals() else 'Not generated yet'}\")\n",
        "df_det_conf_filtered = df_det_preds_raw[df_det_preds_raw['confidence'] > TUNED_CONF_THRESHOLD].copy()\n",
        "print(f\"Boxes after confidence filtering (conf > {TUNED_CONF_THRESHOLD}): {len(df_det_conf_filtered)}\")\n",
        "\n",
        "# Merge with study info to get StudyInstanceUID and classifier predictions\n",
        "df_det_conf_filtered_with_study = df_det_conf_filtered.merge(study_id_map, on='image_id', how='left')\n",
        "df_det_merged = df_det_conf_filtered_with_study.merge(df_study_preds, on='StudyInstanceUID', how='left')\n",
        "\n",
        "# Apply classifier-guided filtering using the tuned threshold\n",
        "print(f\"Boxes before classifier filtering: {len(df_det_merged)}\")\n",
        "df_det_filtered = df_det_merged[df_det_merged['pred_Negative for Pneumonia'] < TUNED_NEG_FILTER_THRESHOLD]\n",
        "print(f\"Boxes after classifier filtering (neg_pred < {TUNED_NEG_FILTER_THRESHOLD}): {len(df_det_filtered)}\")\n",
        "\n",
        "final_image_preds = []\n",
        "# Group by image to apply NMS\n",
        "for image_id, group in tqdm(df_det_filtered.groupby('image_id'), desc=\"Applying NMS\"):\n",
        "    boxes = torch.tensor(group[['x_min', 'y_min', 'x_max', 'y_max']].values, dtype=torch.float32)\n",
        "    scores = torch.tensor(group['confidence'].values, dtype=torch.float32)\n",
        "    \n",
        "    # Apply NMS\n",
        "    keep_indices = nms(boxes, scores, NMS_IOU_THRESHOLD)\n",
        "    \n",
        "    final_boxes = boxes[keep_indices].cpu().numpy()\n",
        "    final_scores = scores[keep_indices].cpu().numpy()\n",
        "    \n",
        "    # Format for submission string\n",
        "    pred_string_parts = []\n",
        "    for box, score in zip(final_boxes, final_scores):\n",
        "        pred_string_parts.append(f\"opacity {score:.4f} {box[0]:.1f} {box[1]:.1f} {box[2]:.1f} {box[3]:.1f}\")\n",
        "    \n",
        "    if pred_string_parts:\n",
        "        final_image_preds.append({\n",
        "            'id': f\"{image_id}_image\",\n",
        "            'PredictionString': \" \".join(pred_string_parts)\n",
        "        })\n",
        "\n",
        "df_image_sub = pd.DataFrame(final_image_preds)\n",
        "\n",
        "# --- Step 4.2: Format Study-Level Predictions ---\n",
        "study_sub_list = []\n",
        "for _, row in df_study_preds.iterrows():\n",
        "    study_id = row['StudyInstanceUID']\n",
        "    pred_strings = []\n",
        "    for cls in CLASSES:\n",
        "        pred_strings.append(f\"{CLASS_MAP_LOWER[cls]} {row[f'pred_{cls}']:.4f} 0 0 1 1\")\n",
        "    \n",
        "    study_sub_list.append({\n",
        "        'id': f\"{study_id}_study\",\n",
        "        'PredictionString': \" \".join(pred_strings)\n",
        "    })\n",
        "\n",
        "df_study_sub = pd.DataFrame(study_sub_list)\n",
        "\n",
        "# --- Step 4.3: Combine and Create Final Submission File ---\n",
        "df_submission = pd.concat([df_study_sub, df_image_sub], ignore_index=True)\n",
        "\n",
        "# Add entries for images with no predicted boxes (submit 'none 1 0 0 1 1')\n",
        "all_test_image_ids = {f\"{img_id}_image\" for img_id in test_image_ids}\n",
        "predicted_image_ids = set(df_image_sub['id'].unique())\n",
        "missing_image_ids = all_test_image_ids - predicted_image_ids\n",
        "\n",
        "missing_df = pd.DataFrame([{'id': img_id, 'PredictionString': 'none 1 0 0 1 1'} for img_id in missing_image_ids])\n",
        "df_submission = pd.concat([df_submission, missing_df], ignore_index=True)\n",
        "\n",
        "# Reorder to match sample submission\n",
        "sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
        "df_submission = df_submission.set_index('id').reindex(sample_sub['id']).reset_index()\n",
        "\n",
        "# Save to file\n",
        "df_submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"\\nFinal submission.csv created successfully with tuned parameters!\")\n",
        "display(df_submission.head())\n",
        "display(df_submission.tail())\n",
        "print(f\"Total rows in submission: {len(df_submission)}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Combining, Post-Processing, and Formatting with Tuned Parameters ---\nRaw detector predictions from all models: Not generated yet\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_det_preds_raw' is not defined",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# --- Step 4.1: Apply Tuned Filtering and NMS ---\u001b[39;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# First, filter raw detector predictions by the tuned confidence threshold\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRaw detector predictions from all models: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_det_preds_raw)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdf_det_preds_raw\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlocals\u001b[39m()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mNot generated yet\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m df_det_conf_filtered = \u001b[43mdf_det_preds_raw\u001b[49m[df_det_preds_raw[\u001b[33m'\u001b[39m\u001b[33mconfidence\u001b[39m\u001b[33m'\u001b[39m] > TUNED_CONF_THRESHOLD].copy()\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBoxes after confidence filtering (conf > \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTUNED_CONF_THRESHOLD\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_det_conf_filtered)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Merge with study info to get StudyInstanceUID and classifier predictions\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'df_det_preds_raw' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}