{
  "cells": [
    {
      "id": "1ac34cf3-f6fc-46cc-a3a8-d9cbef5fd8b0",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 8: Final Stable Submission with Tuned Parameters\n",
        "\n",
        "## Objective\n",
        "Generate a final submission using the best performing models and hyperparameters in a memory-stable manner. The previous attempt (`08_final_submission_tuned.ipynb`) failed due to CUDA OOM errors and kernel hangs, likely caused by loading all 10 models (5x classifier, 5x detector) into memory simultaneously.\n",
        "\n",
        "## Strategy: Sequential Inference with Memory Management\n",
        "This notebook breaks the inference process into discrete, sequential steps. After each major step, models are deleted and CUDA memory is cleared to prevent memory fragmentation and crashes.\n",
        "\n",
        "### Workflow\n",
        "1.  **Setup & Configuration:**\n",
        "    *   Load all necessary libraries.\n",
        "    *   Define constants, including the optimal hyperparameters found in `07_yolov5m_wbf_tuning.ipynb`:\n",
        "        *   `CONF_THRESHOLD = 0.10`\n",
        "        *   `NEGATIVE_FILTER_THRESHOLD = 0.70`\n",
        "    *   Prepare the test dataframes.\n",
        "\n",
        "2.  **Part 1: Classifier Inference:**\n",
        "    *   Load the 5 `EfficientNet-B5` classifier models one by one.\n",
        "    *   Run inference on the test set for each model.\n",
        "    *   Average the predictions across the 5 folds.\n",
        "    *   Save the final ensembled classifier predictions to `test_preds_classifier.csv`.\n",
        "    *   **Crucially: Delete all classifier models and clear CUDA cache (`torch.cuda.empty_cache()`).**\n",
        "\n",
        "3.  **Part 2: Detector Inference:**\n",
        "    *   Load the 5 `YOLOv5s` detector models one by one.\n",
        "    *   Run inference on the test set for each model.\n",
        "    *   Aggregate all raw box predictions.\n",
        "    *   Save the raw detector predictions to `test_preds_detector.csv`.\n",
        "    *   **Crucially: Delete all detector models and clear CUDA cache.**\n",
        "\n",
        "4.  **Part 3: Post-Processing & Submission File Generation:**\n",
        "    *   Load the intermediate prediction files (`test_preds_classifier.csv`, `test_preds_detector.csv`).\n",
        "    *   Apply the post-processing pipeline:\n",
        "        *   Filter boxes by the tuned confidence threshold (`0.10`).\n",
        "        *   Filter boxes based on the classifier's 'Negative' prediction using the tuned threshold (`0.70`).\n",
        "    *   Format the results into the required `id,PredictionString` format.\n",
        "    *   Generate the final `submission.csv`."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "f1197327-4a8f-43dc-94dc-25af11df373a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Part 1: Setup & Configuration ---\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import glob\n",
        "\n",
        "# Add timm to path if not installed in the standard location\n",
        "sys.path.append('/app/.pip-target/lib/python3.11/site-packages')\n",
        "import timm\n",
        "\n",
        "# --- Configuration ---\n",
        "DATA_DIR = '.'\n",
        "TEST_IMAGE_DIR = 'test_png_3ch' # Using 3-channel images for classifier\n",
        "CLASSIFIER_MODEL_DIR = '.'\n",
        "DETECTOR_MODEL_DIR = 'yolov5_runs/train_cv'\n",
        "\n",
        "IMG_SIZE = 512 # For classifier\n",
        "BATCH_SIZE = 1 # LAST ATTEMPT: Set to 1 to minimize memory footprint\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Tuned hyperparameters from notebook 07\n",
        "TUNED_CONF_THRESHOLD = 0.10\n",
        "TUNED_NEG_FILTER_THRESHOLD = 0.70\n",
        "\n",
        "CLASSES = ['Negative for Pneumonia', 'Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance']\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "NUM_FOLDS = 5\n",
        "\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"Tuned Confidence Threshold: {TUNED_CONF_THRESHOLD}\")\n",
        "print(f\"Tuned Negative Filter Threshold: {TUNED_NEG_FILTER_THRESHOLD}\")\n",
        "\n",
        "# --- Prepare Test DataFrames ---\n",
        "df_sub = pd.read_csv(os.path.join(DATA_DIR, 'sample_submission.csv'))\n",
        "\n",
        "# Create image-level test dataframe\n",
        "df_sub['StudyInstanceUID'] = df_sub['id'].apply(lambda x: x.split('_')[0])\n",
        "df_sub['image_id'] = df_sub['id'].apply(lambda x: x.split('_')[0])\n",
        "df_test_img = df_sub[df_sub['id'].str.contains('_image')].copy()\n",
        "df_test_img['image_path'] = df_test_img['id'].apply(lambda x: os.path.join(TEST_IMAGE_DIR, x.replace('_image', '.png')))\n",
        "\n",
        "# Create study-level test dataframe\n",
        "df_test_study = df_sub[df_sub['id'].str.contains('_study')].copy()\n",
        "\n",
        "print(f\"Found {len(df_test_img)} test images.\")\n",
        "print(f\"Found {len(df_test_study)} test studies.\")\n",
        "display(df_test_img.head())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\nTuned Confidence Threshold: 0.1\nTuned Negative Filter Threshold: 0.7\nFound 638 test images.\nFound 606 test studies.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "                     id PredictionString StudyInstanceUID      image_id  \\\n606  004cbd797cd1_image   none 1 0 0 1 1     004cbd797cd1  004cbd797cd1   \n607  008ca392cff3_image   none 1 0 0 1 1     008ca392cff3  008ca392cff3   \n608  00b8180bd3a8_image   none 1 0 0 1 1     00b8180bd3a8  00b8180bd3a8   \n609  00e3a7e91a34_image   none 1 0 0 1 1     00e3a7e91a34  00e3a7e91a34   \n610  0124f624dacb_image   none 1 0 0 1 1     0124f624dacb  0124f624dacb   \n\n                        image_path  \n606  test_png_3ch/004cbd797cd1.png  \n607  test_png_3ch/008ca392cff3.png  \n608  test_png_3ch/00b8180bd3a8.png  \n609  test_png_3ch/00e3a7e91a34.png  \n610  test_png_3ch/0124f624dacb.png  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>PredictionString</th>\n      <th>StudyInstanceUID</th>\n      <th>image_id</th>\n      <th>image_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>606</th>\n      <td>004cbd797cd1_image</td>\n      <td>none 1 0 0 1 1</td>\n      <td>004cbd797cd1</td>\n      <td>004cbd797cd1</td>\n      <td>test_png_3ch/004cbd797cd1.png</td>\n    </tr>\n    <tr>\n      <th>607</th>\n      <td>008ca392cff3_image</td>\n      <td>none 1 0 0 1 1</td>\n      <td>008ca392cff3</td>\n      <td>008ca392cff3</td>\n      <td>test_png_3ch/008ca392cff3.png</td>\n    </tr>\n    <tr>\n      <th>608</th>\n      <td>00b8180bd3a8_image</td>\n      <td>none 1 0 0 1 1</td>\n      <td>00b8180bd3a8</td>\n      <td>00b8180bd3a8</td>\n      <td>test_png_3ch/00b8180bd3a8.png</td>\n    </tr>\n    <tr>\n      <th>609</th>\n      <td>00e3a7e91a34_image</td>\n      <td>none 1 0 0 1 1</td>\n      <td>00e3a7e91a34</td>\n      <td>00e3a7e91a34</td>\n      <td>test_png_3ch/00e3a7e91a34.png</td>\n    </tr>\n    <tr>\n      <th>610</th>\n      <td>0124f624dacb_image</td>\n      <td>none 1 0 0 1 1</td>\n      <td>0124f624dacb</td>\n      <td>0124f624dacb</td>\n      <td>test_png_3ch/0124f624dacb.png</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "54b203e6-0653-4c37-acef-235967484231",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# --- Part 2: Classifier Inference (ABANDONED) ---\n",
        "\n",
        "**ACTION:** The `EfficientNet-B5` models are causing persistent, unrecoverable CUDA OOM errors, even with FP16 and a batch size of 1. With less than 15 minutes remaining, debugging this is not feasible.\n",
        "\n",
        "**PIVOT:** Following expert advice, I am abandoning the classifier for test-time inference. I will proceed with the following plan:\n",
        "1.  **Detector Inference:** Use the 5-fold ensembled `YOLOv5s` models to generate bounding box predictions. This component is stable.\n",
        "2.  **Heuristic Study Predictions:** Instead of using a classifier, generate study-level predictions using a simple heuristic based on the number of bounding boxes found for each study.\n",
        "3.  **Post-Processing & Submission:** Combine the detector predictions and heuristic-based study predictions into a final `submission.csv`."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "678dbef5-106b-4f30-8c07-e04d77dd4411",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Part 3: Detector Inference (CPU FALLBACK) ---\n",
        "\n",
        "print(\"--- Starting Detector Inference (CPU FALLBACK, 1-FOLD ONLY) ---\")\n",
        "\n",
        "# Config for detector\n",
        "DET_MODEL_PATHS = sorted(glob.glob('yolov5_runs/train_cv/yolov5s_fold*/weights/best.pt'))\n",
        "DET_IMG_SIZE = 640\n",
        "DET_BATCH_SIZE = 16\n",
        "TEST_IMAGE_DIR_1CH = 'test_png/'\n",
        "CPU_DEVICE = 'cpu' # Explicitly use CPU as GPU is unstable\n",
        "\n",
        "# Get test image paths\n",
        "test_image_paths = sorted(glob.glob(f'{TEST_IMAGE_DIR_1CH}/*.png'))\n",
        "print(f\"Found {len(test_image_paths)} test images for detection.\")\n",
        "\n",
        "all_det_preds = []\n",
        "\n",
        "# ONLY RUN 1 FOLD ON CPU DUE TO TIME CONSTRAINTS\n",
        "for fold, path in enumerate(DET_MODEL_PATHS[:1]):\n",
        "    print(f\"\\n--- Processing Fold {fold} with model {path} on CPU ---\")\n",
        "    \n",
        "    model = torch.hub.load(\n",
        "        'ultralytics/yolov5',\n",
        "        'custom',\n",
        "        path=path,\n",
        "        force_reload=True, # Try to clear cache issues\n",
        "        _verbose=False\n",
        "    )\n",
        "    model.to(CPU_DEVICE).eval()\n",
        "    model.conf = TUNED_CONF_THRESHOLD \n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(test_image_paths), DET_BATCH_SIZE), desc=f\"Fold {fold} Detection (CPU)\"):\n",
        "            batch_paths = test_image_paths[i:i+DET_BATCH_SIZE]\n",
        "            results = model(batch_paths, size=DET_IMG_SIZE)\n",
        "            preds_df_list = results.pandas().xyxy\n",
        "            \n",
        "            for j, preds_df in enumerate(preds_df_list):\n",
        "                if not preds_df.empty:\n",
        "                    image_id = os.path.basename(batch_paths[j]).replace('.png', '')\n",
        "                    for _, row in preds_df.iterrows():\n",
        "                        all_det_preds.append({\n",
        "                            'image_id': image_id,\n",
        "                            'x_min': row['xmin'],\n",
        "                            'y_min': row['ymin'],\n",
        "                            'x_max': row['xmax'],\n",
        "                            'y_max': row['ymax'],\n",
        "                            'confidence': row['confidence']\n",
        "                        })\n",
        "\n",
        "    del model\n",
        "    gc.collect()\n",
        "\n",
        "df_det_preds_raw = pd.DataFrame(all_det_preds if all_det_preds else [])\n",
        "df_det_preds_raw.to_csv('test_preds_detector.csv', index=False)\n",
        "\n",
        "print(\"\\nDetector inference complete. Raw box predictions saved.\")\n",
        "display(df_det_preds_raw.head())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Detector Inference (CPU FALLBACK, 1-FOLD ONLY) ---\nFound 638 test images for detection.\n\n--- Processing Fold 0 with model yolov5_runs/train_cv/yolov5s_fold0/weights/best.pt on CPU ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /app/.cache/torch/hub/master.zip\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n. Cache may be out of date, try `force_reload=True` or see https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading for help.",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:70\u001b[39m, in \u001b[36m_create\u001b[39m\u001b[34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     model = \u001b[43mDetectMultiBackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoshape\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# detection model\u001b[39;00m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m autoshape:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:489\u001b[39m, in \u001b[36mDetectMultiBackend.__init__\u001b[39m\u001b[34m(self, weights, device, dnn, data, fp16, fuse)\u001b[39m\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pt:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     model = \u001b[43mattempt_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfuse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    490\u001b[39m     stride = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mint\u001b[39m(model.stride.max()), \u001b[32m32\u001b[39m)  \u001b[38;5;66;03m# model stride\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/ultralytics_yolov5_master/models/experimental.py:100\u001b[39m, in \u001b[36mattempt_load\u001b[39m\u001b[34m(weights, device, inplace, fuse)\u001b[39m\n\u001b[32m     99\u001b[39m ckpt = torch_load(attempt_download(w), map_location=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# load\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m ckpt = \u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m.float()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# Model compatibility updates\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:1369\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1367\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/ultralytics_yolov5_master/models/yolo.py:208\u001b[39m, in \u001b[36mBaseModel._apply\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Applies transformations like to(), cpu(), cuda(), half() to model tensors excluding parameters or registered\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[33;03mbuffers.\u001b[39;00m\n\u001b[32m    207\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m m = \u001b[38;5;28mself\u001b[39m.model[-\u001b[32m1\u001b[39m]  \u001b[38;5;66;03m# Detect()\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:955\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    956\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:1355\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1349\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1350\u001b[39m             device,\n\u001b[32m   1351\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1352\u001b[39m             non_blocking,\n\u001b[32m   1353\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1354\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[31mAcceleratorError\u001b[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:85\u001b[39m, in \u001b[36m_create\u001b[39m\u001b[34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[39m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m         model = \u001b[43mattempt_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# arbitrary model\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/ultralytics_yolov5_master/models/experimental.py:100\u001b[39m, in \u001b[36mattempt_load\u001b[39m\u001b[34m(weights, device, inplace, fuse)\u001b[39m\n\u001b[32m     99\u001b[39m ckpt = torch_load(attempt_download(w), map_location=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# load\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m ckpt = \u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m.float()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# Model compatibility updates\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:1369\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1367\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/ultralytics_yolov5_master/models/yolo.py:208\u001b[39m, in \u001b[36mBaseModel._apply\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Applies transformations like to(), cpu(), cuda(), half() to model tensors excluding parameters or registered\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[33;03mbuffers.\u001b[39;00m\n\u001b[32m    207\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m m = \u001b[38;5;28mself\u001b[39m.model[-\u001b[32m1\u001b[39m]  \u001b[38;5;66;03m# Detect()\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:955\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    956\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:1355\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1349\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1350\u001b[39m             device,\n\u001b[32m   1351\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1352\u001b[39m             non_blocking,\n\u001b[32m   1353\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1354\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[31mAcceleratorError\u001b[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fold, path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(DET_MODEL_PATHS[:\u001b[32m1\u001b[39m]):\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Processing Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m on CPU ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     model = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43multralytics/yolov5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcustom\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Try to clear cache issues\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_verbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     model.to(CPU_DEVICE).eval()\n\u001b[32m     30\u001b[39m     model.conf = TUNED_CONF_THRESHOLD \n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/hub.py:647\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source == \u001b[33m\"\u001b[39m\u001b[33mgithub\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    638\u001b[39m     repo_or_dir = _get_cache_or_reload(\n\u001b[32m    639\u001b[39m         repo_or_dir,\n\u001b[32m    640\u001b[39m         force_reload,\n\u001b[32m   (...)\u001b[39m\u001b[32m    644\u001b[39m         skip_validation=skip_validation,\n\u001b[32m    645\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m model = \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/hub.py:680\u001b[39m, in \u001b[36m_load_local\u001b[39m\u001b[34m(hubconf_dir, model, *args, **kwargs)\u001b[39m\n\u001b[32m    677\u001b[39m     hub_module = _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[32m    679\u001b[39m     entry = _load_entry_from_hubconf(hub_module, model)\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m     model = \u001b[43mentry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:135\u001b[39m, in \u001b[36mcustom\u001b[39m\u001b[34m(path, autoshape, _verbose, device)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcustom\u001b[39m(path=\u001b[33m\"\u001b[39m\u001b[33mpath/to/model.pt\u001b[39m\u001b[33m\"\u001b[39m, autoshape=\u001b[38;5;28;01mTrue\u001b[39;00m, _verbose=\u001b[38;5;28;01mTrue\u001b[39;00m, device=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    107\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[33;03m    Loads a custom or local YOLOv5 model from a given path with optional autoshaping and device specification.\u001b[39;00m\n\u001b[32m    109\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    133\u001b[39m \u001b[33;03m        ```\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoshape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautoshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_verbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:103\u001b[39m, in \u001b[36m_create\u001b[39m\u001b[34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[39m\n\u001b[32m    101\u001b[39m help_url = \u001b[33m\"\u001b[39m\u001b[33mhttps://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    102\u001b[39m s = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Cache may be out of date, try `force_reload=True` or see \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhelp_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for help.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(s) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[31mException\u001b[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n. Cache may be out of date, try `force_reload=True` or see https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading for help."
          ]
        }
      ]
    },
    {
      "id": "80dae4f1-b9e2-4b02-a779-bcdd56b1d385",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Part 4: Final Fallback - No-Inference Submission ---\n",
        "\n",
        "print(\"CRITICAL: All inference attempts failed due to persistent OOM. Pivoting to no-inference submission.\")\n",
        "\n",
        "# --- Step 1: Generate Heuristic Study Predictions (from training set priors) ---\n",
        "df_train_study = pd.read_csv('train_study_level.csv')\n",
        "df_train_study['id'] = df_train_study['id'].str.replace('_study', '')\n",
        "\n",
        "cols = ['Negative for Pneumonia', 'Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance']\n",
        "priors = df_train_study[cols].mean().values\n",
        "\n",
        "print(f\"Calculated Priors from Training Data:\")\n",
        "for cls, p in zip(cols, priors):\n",
        "    print(f\"  - {cls}: {p:.4f}\")\n",
        "\n",
        "study_preds_list = []\n",
        "for study_id in df_test_study['StudyInstanceUID'].unique():\n",
        "    pred_strings = []\n",
        "    # Format: lowercase class name + probability\n",
        "    pred_strings.append(f\"negative {priors[0]:.4f} 0 0 1 1\")\n",
        "    pred_strings.append(f\"typical {priors[1]:.4f} 0 0 1 1\")\n",
        "    pred_strings.append(f\"indeterminate {priors[2]:.4f} 0 0 1 1\")\n",
        "    pred_strings.append(f\"atypical {priors[3]:.4f} 0 0 1 1\")\n",
        "    \n",
        "    study_preds_list.append({\n",
        "        'id': f\"{study_id}_study\",\n",
        "        'PredictionString': \" \".join(pred_strings)\n",
        "    })\n",
        "df_study_sub = pd.DataFrame(study_preds_list)\n",
        "\n",
        "# --- Step 2: Generate 'None' Image Predictions ---\n",
        "image_preds_list = []\n",
        "for image_id in df_test_img['image_id'].unique():\n",
        "    image_preds_list.append({\n",
        "        'id': f\"{image_id}_image\",\n",
        "        'PredictionString': 'none 1 0 0 1 1'\n",
        "    })\n",
        "df_image_sub = pd.DataFrame(image_preds_list)\n",
        "\n",
        "# --- Step 3: Combine and Create Final Submission File ---\n",
        "df_submission = pd.concat([df_study_sub, df_image_sub], ignore_index=True)\n",
        "\n",
        "# Reorder to match sample submission\n",
        "sample_sub = pd.read_csv('sample_submission.csv')\n",
        "df_submission = df_submission.set_index('id').reindex(sample_sub['id']).reset_index()\n",
        "\n",
        "df_submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"\\nFinal submission.csv created using training priors and no bounding boxes.\")\n",
        "display(df_submission.head())\n",
        "display(df_submission.tail())\n",
        "print(f\"Total rows in submission: {len(df_submission)}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CRITICAL: All inference attempts failed due to persistent OOM. Pivoting to no-inference submission.\nCalculated Priors from Training Data:\n  - Negative for Pneumonia: 0.2740\n  - Typical Appearance: 0.4708\n  - Indeterminate Appearance: 0.1747\n  - Atypical Appearance: 0.0804\n\nFinal submission.csv created using training priors and no bounding boxes.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "                   id                                   PredictionString\n0  000c9c05fd14_study  negative 0.2740 0 0 1 1 typical 0.4708 0 0 1 1...\n1  00c74279c5b7_study  negative 0.2740 0 0 1 1 typical 0.4708 0 0 1 1...\n2  00ccd633fb0e_study  negative 0.2740 0 0 1 1 typical 0.4708 0 0 1 1...\n3  00e936c58da6_study  negative 0.2740 0 0 1 1 typical 0.4708 0 0 1 1...\n4  01206a422293_study  negative 0.2740 0 0 1 1 typical 0.4708 0 0 1 1...",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>PredictionString</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000c9c05fd14_study</td>\n      <td>negative 0.2740 0 0 1 1 typical 0.4708 0 0 1 1...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00c74279c5b7_study</td>\n      <td>negative 0.2740 0 0 1 1 typical 0.4708 0 0 1 1...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00ccd633fb0e_study</td>\n      <td>negative 0.2740 0 0 1 1 typical 0.4708 0 0 1 1...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00e936c58da6_study</td>\n      <td>negative 0.2740 0 0 1 1 typical 0.4708 0 0 1 1...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>01206a422293_study</td>\n      <td>negative 0.2740 0 0 1 1 typical 0.4708 0 0 1 1...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "                      id PredictionString\n1239  ff03d1d41968_image   none 1 0 0 1 1\n1240  ff0743bee789_image   none 1 0 0 1 1\n1241  ffab0f8f27f0_image   none 1 0 0 1 1\n1242  ffbeafe30b77_image   none 1 0 0 1 1\n1243  ffe942c8655f_image   none 1 0 0 1 1",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>PredictionString</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1239</th>\n      <td>ff03d1d41968_image</td>\n      <td>none 1 0 0 1 1</td>\n    </tr>\n    <tr>\n      <th>1240</th>\n      <td>ff0743bee789_image</td>\n      <td>none 1 0 0 1 1</td>\n    </tr>\n    <tr>\n      <th>1241</th>\n      <td>ffab0f8f27f0_image</td>\n      <td>none 1 0 0 1 1</td>\n    </tr>\n    <tr>\n      <th>1242</th>\n      <td>ffbeafe30b77_image</td>\n      <td>none 1 0 0 1 1</td>\n    </tr>\n    <tr>\n      <th>1243</th>\n      <td>ffe942c8655f_image</td>\n      <td>none 1 0 0 1 1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows in submission: 1244\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}