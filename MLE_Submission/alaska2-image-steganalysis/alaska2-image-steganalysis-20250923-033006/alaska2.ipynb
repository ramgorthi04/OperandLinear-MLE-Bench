{
  "cells": [
    {
      "id": "5fdaed7d-a421-4eb0-b799-e8f46170cfef",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ALASKA2 Image Steganalysis \u2014 Plan (updated per expert advice)\n",
        "\n",
        "Goal: 4-class training (Cover, JMiPOD, JUNIWARD, UERD). Submission Label = 1 - p_cover (probability of stego). Target weighted-AUROC medal.\n",
        "\n",
        "Protocol:\n",
        "- Env check first (nvidia-smi); install PyTorch cu121 stack only after confirming GPU.\n",
        "- Data audit: build manifest with columns [path, label_4c, is_stego, jpeg_qf, source_id, fold].\n",
        "- CV: StratifiedGroupKFold(n_splits=5, stratify=4-class label, groups=source_id parsed from filename) to avoid leakage across images from same base cover.\n",
        "- Preprocessing:\n",
        "  - Use YCbCr (or Y-only) inputs. Start with 256 for smoke, move to 512 (8\u00d78 JPEG grid aligned center-crop).\n",
        "  - Augmentations: only flips/transpose (no rotations, no color jitter, no random-resized-crop).\n",
        "  - Scale to [0,1]; avoid per-image z-scoring; keep JPEG artifacts intact; consistent decoder.\n",
        "\n",
        "Baseline (hours 0\u20133):\n",
        "- tf_efficientnet_b2 (timm), 4-class head, AMP, AdamW (lr=2e-4, wd=1e-4), cosine w/ warmup, label smoothing 0.1.\n",
        "- Input 256 (RGB for fastest bring-up if YCbCr loader not ready), 3 epochs/fold, balanced batches (~50% cover, 50% stego split across 3).\n",
        "- HFlip TTA at inference. Save OOF logits and per-fold weights.\n",
        "\n",
        "Push to medal (hours 3\u201320):\n",
        "- Switch to YCbCr/Y-only at 512 with grid-aligned center-crop.\n",
        "- Stronger backbones: tf_efficientnet_b3/b4 or convnext_tiny.\n",
        "- Add SRM residual channels (few fixed kernels or ~30 maps with TLU) concatenated to Y; adapt first conv (1\u00d71) to pretrained backbone. Expect +0.01\u20130.03 CV.\n",
        "- Train 2\u20133 diverse models/seeds/resolutions.\n",
        "\n",
        "Ensembling (hours 20\u201324):\n",
        "- Logit-average using OOF to set simple weights; if rushed, equal-weight logit avg.\n",
        "- Diversity: SRM vs no-SRM, Y-only vs YCbCr, different backbones/resolutions/seeds.\n",
        "\n",
        "Milestones & reviews:\n",
        "1) After env check + manifest/folds (request expert).\n",
        "2) After baseline 1\u20132 folds sanity and OOF.\n",
        "3) After full 5-fold YCbCr/512 run.\n",
        "4) After second backbone and/or SRM variant; then ensemble.\n",
        "\n",
        "Next: add env/GPU check code cell, then implement manifest builder + folds."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "1f078138-2ded-4ee9-af61-fb8228e74946",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Env/GPU check and quick dataset audit\n",
        "import os, subprocess, glob, time, shutil\n",
        "print('=== nvidia-smi ===', flush=True)\n",
        "subprocess.run(['bash','-lc','nvidia-smi || true'], check=False)\n",
        "print('\\n=== Dataset counts ===', flush=True)\n",
        "base='.'\n",
        "cls_dirs=['Cover','JMiPOD','JUNIWARD','UERD','Test']\n",
        "for d in cls_dirs:\n",
        "    t0=time.time()\n",
        "    files=glob.glob(os.path.join(base,d,'*.jpg'))\n",
        "    print(f'{d}: {len(files)} files (scan {time.time()-t0:.2f}s)', flush=True)\n",
        "print('\\nSample files:', flush=True)\n",
        "for d in ['Cover','JMiPOD','JUNIWARD','UERD','Test']:\n",
        "    files=glob.glob(os.path.join(base,d,'*.jpg'))[:3]\n",
        "    print(d, [os.path.basename(x) for x in files], flush=True)\n",
        "print('\\nDisk usage of repo (du -h -d1):', flush=True)\n",
        "subprocess.run(['bash','-lc','du -h -d1 . | sort -h | tail -n 10'], check=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "1636604c-a744-4da0-b08c-ffe37f1d8d72",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build manifest and 5-fold StratifiedGroupKFold\n",
        "import os, glob, pandas as pd\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "\n",
        "label_map = {'Cover':0, 'JMiPOD':1, 'JUNIWARD':2, 'UERD':3}\n",
        "rows = []\n",
        "for cls in ['Cover','JMiPOD','JUNIWARD','UERD']:\n",
        "    for p in glob.glob(os.path.join(cls,'*.jpg')):\n",
        "        fname = os.path.basename(p)\n",
        "        source_id = os.path.splitext(fname)[0]  # group by base id\n",
        "        rows.append({\n",
        "            'path': p,\n",
        "            'filename': fname,\n",
        "            'label_4c': label_map[cls],\n",
        "            'is_stego': 0 if cls=='Cover' else 1,\n",
        "            'jpeg_qf': -1,  # placeholder; optional extraction later\n",
        "            'source_id': source_id\n",
        "        })\n",
        "df = pd.DataFrame(rows)\n",
        "print('Manifest size:', df.shape, 'cover:', (df.label_4c==0).sum(), 'stego:', (df.label_4c!=0).sum(), flush=True)\n",
        "\n",
        "# StratifiedGroupKFold by 4-class with grouping by source_id\n",
        "n_splits = 5\n",
        "skf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "df['fold'] = -1\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(df, y=df['label_4c'], groups=df['source_id'])):\n",
        "    df.loc[val_idx, 'fold'] = fold\n",
        "print('Fold distribution (counts by label per fold):', flush=True)\n",
        "print(df.groupby(['fold','label_4c']).size().unstack(1), flush=True)\n",
        "\n",
        "out_csv = 'manifest.csv'\n",
        "df.to_csv(out_csv, index=False)\n",
        "print('Saved', out_csv, 'with columns:', list(df.columns), flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "9c2f16a6-499b-40d8-984c-346c0a8f8d64",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Clean reinstall PyTorch cu124 and purge shadowed .pip-target dirs (install only; no torch import here)\n",
        "import sys, subprocess, time, os, shutil\n",
        "t0=time.time()\n",
        "print('Uninstalling any existing torch stack...', flush=True)\n",
        "subprocess.run([sys.executable,'-m','pip','uninstall','-y','torch','torchvision','torchaudio'], check=False)\n",
        "print('Purging shadowing dirs under /app/.pip-target ...', flush=True)\n",
        "purge_dirs = [\n",
        "    '/app/.pip-target/torch',\n",
        "    '/app/.pip-target/torch-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torch-2.5.1.dist-info',\n",
        "    '/app/.pip-target/torchvision',\n",
        "    '/app/.pip-target/torchvision-0.19.1.dist-info',\n",
        "    '/app/.pip-target/torchvision-0.20.1.dist-info',\n",
        "    '/app/.pip-target/torchaudio',\n",
        "    '/app/.pip-target/torchaudio-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torchaudio-2.5.1.dist-info'\n",
        "]\n",
        "for d in purge_dirs:\n",
        "    if os.path.exists(d):\n",
        "        print('Removing', d, flush=True)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "print('Installing torch/cu124...', flush=True)\n",
        "subprocess.run([sys.executable,'-m','pip','install','--force-reinstall','--no-cache-dir',\n",
        "                '--index-url','https://download.pytorch.org/whl/cu124',\n",
        "                'torch==2.5.1','torchvision==0.20.1','torchaudio==2.5.1'], check=True)\n",
        "print('Install done in %.1fs' % (time.time()-t0), flush=True)\n",
        "print('NOTE: After this cell, restart kernel then run the verify cell (idx 5).', flush=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uninstalling any existing torch stack...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.4.1+cu121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uninstalling torch-2.4.1+cu121:\n  Successfully uninstalled torch-2.4.1+cu121\nFound existing installation: torchvision 0.19.1+cu121\nUninstalling torchvision-0.19.1+cu121:\n  Successfully uninstalled torchvision-0.19.1+cu121\nFound existing installation: torchaudio 2.4.1+cu121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uninstalling torchaudio-2.4.1+cu121:\n  Successfully uninstalled torchaudio-2.4.1+cu121\nInstalling torch/cu124...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.5.1\n  Downloading https://download.pytorch.org/whl/cu124/torch-2.5.1%2Bcu124-cp311-cp311-linux_x86_64.whl (908.3 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 908.3/908.3 MB 536.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.20.1\n  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.20.1%2Bcu124-cp311-cp311-linux_x86_64.whl (7.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.3/7.3 MB 125.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==2.5.1\n  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.5.1%2Bcu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.4/3.4 MB 257.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jinja2\n  Downloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl (133 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 133.3/133.3 KB 418.1 MB/s eta 0:00:00\nCollecting typing-extensions>=4.8.0\n  Downloading https://download.pytorch.org/whl/typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusparse-cu12==12.3.1.170\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 207.5/207.5 MB 522.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton==3.1.0\n  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.5/209.5 MB 557.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-cupti-cu12==12.4.127\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13.8/13.8 MB 454.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvjitlink-cu12==12.4.127\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 21.1/21.1 MB 343.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvtx-cu12==12.4.127\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 433.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 330.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting filelock\n  Downloading https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 24.6/24.6 MB 313.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.4.127\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 883.7/883.7 KB 462.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fsspec\n  Downloading https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl (177 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 177.6/177.6 KB 479.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nccl-cu12==2.21.5\n  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 188.7/188.7 MB 553.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting networkx\n  Downloading https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl (1.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.7/1.7 MB 498.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy==1.13.1\n  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.2/6.2 MB 345.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusolver-cu12==11.6.1.9\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 127.9/127.9 MB 544.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cublas-cu12==12.4.5.8\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 363.4/363.4 MB 524.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-curand-cu12==10.3.5.147\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.3/56.3 MB 457.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cufft-cu12==11.2.1.3\n  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 211.5/211.5 MB 341.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading https://download.pytorch.org/whl/numpy-1.26.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 293.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow!=8.3.*,>=5.3.0\n  Downloading https://download.pytorch.org/whl/pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4.4/4.4 MB 421.2 MB/s eta 0:00:00\nCollecting mpmath<1.4,>=1.1.0\n  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 492.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-2.1.5 filelock-3.13.1 fsspec-2024.6.1 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 numpy-1.26.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pillow-11.0.0 sympy-1.13.1 torch-2.5.1+cu124 torchaudio-2.5.1+cu124 torchvision-0.20.1+cu124 triton-3.1.0 typing-extensions-4.12.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/jinja2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cudnn_cu12-9.1.0.70.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/PIL already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/isympy.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath-1.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/share already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Install done in 53.9s\n"
          ]
        }
      ]
    },
    {
      "id": "8ef6449b-abd3-4e1d-bb10-74aa00e047b5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CUDA diagnostics via subprocess with LD_LIBRARY_PATH set\n",
        "import os, sys, subprocess, textwrap\n",
        "env = os.environ.copy()\n",
        "ld = env.get('LD_LIBRARY_PATH','')\n",
        "prefix = '/usr/local/nvidia/lib:/usr/local/nvidia/lib64'\n",
        "env['LD_LIBRARY_PATH'] = f\"{prefix}:{ld}\" if ld else prefix\n",
        "print('LD_LIBRARY_PATH =', env['LD_LIBRARY_PATH'], flush=True)\n",
        "code = textwrap.dedent('''\n",
        "import os, torch, traceback\n",
        "print('torch:', torch.__version__, 'built CUDA:', getattr(torch.version,'cuda',None))\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "try:\n",
        "    import ctypes, sys\n",
        "    for p in ('/usr/local/nvidia/lib','/usr/local/nvidia/lib64'):\n",
        "        try:\n",
        "            ctypes.CDLL(p+'/libcudart.so', mode=ctypes.RTLD_GLOBAL)\n",
        "        except Exception:\n",
        "            pass\n",
        "    if torch.cuda.is_available():\n",
        "        print('GPU:', torch.cuda.get_device_name(0))\n",
        "        print('Device count:', torch.cuda.device_count())\n",
        "except Exception as e:\n",
        "    print('Exception during CUDA check:', e)\n",
        "    traceback.print_exc()\n",
        "''')\n",
        "print('--- Subprocess torch CUDA check ---', flush=True)\n",
        "res = subprocess.run([sys.executable, '-c', code], env=env, text=True, capture_output=True)\n",
        "print(res.stdout, flush=True)\n",
        "print(res.stderr, flush=True)\n",
        "print('Return code:', res.returncode, flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "cc4775a1-5719-4385-a827-df3de5c3cb9d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Verify Torch CUDA after clean install (set env before import) + diagnostics (hard purge .pip-target shadowing)\n",
        "import os, subprocess, sys, shutil\n",
        "print('Python:', sys.executable)\n",
        "print('Initial CUDA_VISIBLE_DEVICES=', os.getenv('CUDA_VISIBLE_DEVICES'))\n",
        "print('Initial NVIDIA_VISIBLE_DEVICES=', os.getenv('NVIDIA_VISIBLE_DEVICES'))\n",
        "if os.getenv('CUDA_VISIBLE_DEVICES') in (None, '', 'void'):\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'; print('Set CUDA_VISIBLE_DEVICES=0')\n",
        "if os.getenv('NVIDIA_VISIBLE_DEVICES') in (None, '', 'void'):\n",
        "    os.environ['NVIDIA_VISIBLE_DEVICES'] = 'all'; print('Set NVIDIA_VISIBLE_DEVICES=all')\n",
        "os.environ['PYTHONNOUSERSITE'] = '1'\n",
        "print('Set PYTHONNOUSERSITE=1 to ignore user site')\n",
        "print('Effective CUDA_VISIBLE_DEVICES=', os.getenv('CUDA_VISIBLE_DEVICES'))\n",
        "print('Effective NVIDIA_VISIBLE_DEVICES=', os.getenv('NVIDIA_VISIBLE_DEVICES'))\n",
        "subprocess.run(['bash','-lc','ldconfig -p | grep libcuda || true'], check=False)\n",
        "\n",
        "# Hard purge entire /app/.pip-target to avoid any shadowing\n",
        "pip_target = '/app/.pip-target'\n",
        "if os.path.exists(pip_target):\n",
        "    print('Removing entire', pip_target)\n",
        "    shutil.rmtree(pip_target, ignore_errors=True)\n",
        "\n",
        "# Clean sys.modules if torch was previously imported in this kernel\n",
        "for mod in list(sys.modules.keys()):\n",
        "    if mod == 'torch' or mod.startswith('torch.'):\n",
        "        del sys.modules[mod]\n",
        "\n",
        "# Clean sys.path of any user directories and .pip-target entries\n",
        "sys.path = [p for p in sys.path if p and '/.pip-target' not in p and '/app/.local' not in p]\n",
        "print('sys.path cleaned (first 8):', sys.path[:8])\n",
        "\n",
        "try:\n",
        "    import importlib\n",
        "    torch = importlib.import_module('torch')\n",
        "    print('Torch:', torch.__version__, 'built CUDA:', getattr(torch.version,'cuda',None), flush=True)\n",
        "    print('Torch file:', getattr(torch, '__file__', 'unknown'))\n",
        "    try:\n",
        "        from importlib.metadata import version\n",
        "        print('importlib.metadata torch version:', version('torch'))\n",
        "    except Exception as e:\n",
        "        print('metadata read error:', e)\n",
        "    print('CUDA available:', torch.cuda.is_available(), flush=True)\n",
        "    if torch.cuda.is_available():\n",
        "        print('GPU:', torch.cuda.get_device_name(0), 'Count:', torch.cuda.device_count(), flush=True)\n",
        "    else:\n",
        "        try:\n",
        "            torch.zeros(1).cuda()\n",
        "        except Exception as e:\n",
        "            print('CUDA init error:', e)\n",
        "except Exception as e:\n",
        "    print('Torch import failed:', e)\n",
        "print('Done verify.')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: /usr/bin/python3.11\nInitial CUDA_VISIBLE_DEVICES= 0\nInitial NVIDIA_VISIBLE_DEVICES= all\nSet PYTHONNOUSERSITE=1 to ignore user site\nEffective CUDA_VISIBLE_DEVICES= 0\nEffective NVIDIA_VISIBLE_DEVICES= all\n\tlibcudart.so.12 (libc6,x86-64) => /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so.12\n\tlibcudart.so (libc6,x86-64) => /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so\n\tlibcudadebugger.so.1 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcudadebugger.so.1\n\tlibcuda.so.1 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcuda.so.1\nRemoving shadowed torch dir: /app/.pip-target/torch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing /app/.pip-target/torch-2.4.1.dist-info\nsys.path cleaned (first 8): ['/app', '/app/src', '/usr/lib/python311.zip', '/usr/lib/python3.11', '/usr/lib/python3.11/lib-dynload', '', '/usr/local/lib/python3.11/dist-packages', '/usr/lib/python3/dist-packages']\nTorch: 2.4.1+cu121 built CUDA: 12.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch file: /app/.pip-target/torch/__init__.py\nmetadata read error: No package metadata was found for torch\nCUDA available: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA init error: No CUDA GPUs are available\nDone verify.\n"
          ]
        }
      ]
    },
    {
      "id": "5c9f1239-9dc6-4811-8ee5-30bfb788445a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CPU fallback: DCT histogram features + LightGBM multiclass baseline\n",
        "import os, gc, time, glob, numpy as np, pandas as pd\n",
        "from joblib import Parallel, delayed\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import lightgbm as lgb\n",
        "\n",
        "def load_Y(path):\n",
        "    im = Image.open(path).convert('YCbCr')\n",
        "    y, _, _ = im.split()\n",
        "    return np.asarray(y, dtype=np.float32)\n",
        "\n",
        "# Precompute 8x8 DCT matrix (orthonormal)\n",
        "def dct_matrix_8():\n",
        "    N = 8\n",
        "    C = np.zeros((N,N), dtype=np.float32)\n",
        "    for k in range(N):\n",
        "        for n in range(N):\n",
        "            alpha = np.sqrt(1/N) if k==0 else np.sqrt(2/N)\n",
        "            C[k,n] = alpha * np.cos((np.pi*(2*n+1)*k)/(2*N))\n",
        "    return C\n",
        "C8 = dct_matrix_8()\n",
        "\n",
        "# Zigzag indices for 8x8\n",
        "def zigzag_indices(n=8):\n",
        "    idx = []\n",
        "    for s in range(2*n-1):\n",
        "        if s%2==0:\n",
        "            for i in range(s, -1, -1):\n",
        "                j = s - i\n",
        "                if i<n and j<n: idx.append((i,j))\n",
        "        else:\n",
        "            for j in range(s, -1, -1):\n",
        "                i = s - j\n",
        "                if i<n and j<n: idx.append((i,j))\n",
        "    return idx\n",
        "zz = zigzag_indices(8)\n",
        "ac_positions = zz[1:21]  # first 20 AC coefficients (skip DC at [0,0])\n",
        "\n",
        "def dct_hist_features(y, clip=10, bins=21):\n",
        "    H, W = y.shape\n",
        "    H8, W8 = (H//8)*8, (W//8)*8\n",
        "    y = y[:H8, :W8]\n",
        "    # blockify to (nb, 8, 8)\n",
        "    y_blocks = y.reshape(H8//8, 8, W8//8, 8).transpose(0,2,1,3).reshape(-1,8,8)\n",
        "    # DCT for all blocks: C @ B @ C.T\n",
        "    tmp = np.einsum('ab,nbc->nac', C8, y_blocks, optimize=True)\n",
        "    dcts = np.einsum('nab,cb->nac', tmp, C8, optimize=True)  # (nb,8,8)\n",
        "    # hist per selected AC\n",
        "    feats = []\n",
        "    for (i,j) in ac_positions:\n",
        "        v = np.rint(dcts[:, i, j]).astype(np.int32)\n",
        "        v = np.clip(v, -clip, clip)\n",
        "        # bins centered at integers from -clip..clip\n",
        "        hist = np.bincount(v + clip, minlength=bins).astype(np.float32)\n",
        "        # L1 normalize\n",
        "        s = hist.sum()\n",
        "        feats.append(hist / (s if s>0 else 1.0))\n",
        "    return np.concatenate(feats, axis=0)  # 20*21 = 420\n",
        "\n",
        "def extract_one(path):\n",
        "    try:\n",
        "        y = load_Y(path)\n",
        "        return dct_hist_features(y)\n",
        "    except Exception as e:\n",
        "        return np.full(420, np.nan, dtype=np.float32)\n",
        "\n",
        "def _extract_in_chunks(paths, chunk=5000, n_jobs=36, tag='train'):\n",
        "    all_parts = []\n",
        "    t0 = time.time()\n",
        "    for i in range(0, len(paths), chunk):\n",
        "        t1 = time.time()\n",
        "        part_paths = paths[i:i+chunk]\n",
        "        part = Parallel(n_jobs=n_jobs, batch_size=64, prefer='threads')(delayed(extract_one)(p) for p in part_paths)\n",
        "        part = np.vstack(part).astype(np.float32)\n",
        "        all_parts.append(part)\n",
        "        done = i + len(part_paths)\n",
        "        print(f'[{tag}] processed {done}/{len(paths)} in chunk {i//chunk+1}, elapsed {time.time()-t1:.1f}s total {time.time()-t0:.1f}s', flush=True)\n",
        "    return np.vstack(all_parts) if all_parts else np.zeros((0,420), dtype=np.float32)\n",
        "\n",
        "def _save_paths(paths, fname):\n",
        "    with open(fname, 'w') as f:\n",
        "        for p in paths:\n",
        "            f.write(str(p)+'\\n')\n",
        "\n",
        "def _load_paths(fname):\n",
        "    if not os.path.exists(fname):\n",
        "        return None\n",
        "    with open(fname, 'r') as f:\n",
        "        return [line.strip() for line in f.readlines()]\n",
        "\n",
        "def build_features(manifest_csv='manifest.csv', cache_train='train_feats.npy', cache_test='test_feats.npy',\n",
        "                   subsample_n=None, chunk=5000, n_jobs=36, seed=42):\n",
        "    df = pd.read_csv(manifest_csv)\n",
        "    # Optional stratified subsample for smoke runs\n",
        "    if subsample_n is not None and subsample_n < len(df):\n",
        "        per = max(1, subsample_n // 4)\n",
        "        dfs = []\n",
        "        for c in [0,1,2,3]:\n",
        "            sub = df[df['label_4c']==c].sample(n=per, random_state=seed)\n",
        "            dfs.append(sub)\n",
        "        df = pd.concat(dfs, axis=0).sort_values('path').reset_index(drop=True)  # deterministic order\n",
        "        print('Subsampled manifest to', len(df), 'rows (approx stratified by label)', flush=True)\n",
        "        cache_train = f'train_feats_{len(df)}.npy'\n",
        "    else:\n",
        "        df = df.sort_values('path').reset_index(drop=True)\n",
        "    train_paths = df['path'].tolist()\n",
        "    train_paths_txt = cache_train + '.paths.txt'\n",
        "    test_files = sorted(glob.glob('Test/*.jpg'))\n",
        "    test_paths_txt = cache_test + '.paths.txt'\n",
        "    # Train features with cache validation\n",
        "    need_train = True\n",
        "    if os.path.exists(cache_train) and os.path.exists(train_paths_txt):\n",
        "        cached = _load_paths(train_paths_txt)\n",
        "        if cached == train_paths:\n",
        "            need_train = False\n",
        "    if need_train:\n",
        "        t0=time.time()\n",
        "        X = _extract_in_chunks(train_paths, chunk=chunk, n_jobs=n_jobs, tag='train')\n",
        "        np.save(cache_train, X)\n",
        "        _save_paths(train_paths, train_paths_txt)\n",
        "        print('Saved', cache_train, X.shape, 'in %.1fs' % (time.time()-t0), flush=True)\n",
        "    else:\n",
        "        X = np.load(cache_train); print('Loaded', cache_train, X.shape)\n",
        "    # Test features with cache validation\n",
        "    need_test = True\n",
        "    if os.path.exists(cache_test) and os.path.exists(test_paths_txt):\n",
        "        cached_t = _load_paths(test_paths_txt)\n",
        "        if cached_t == test_files:\n",
        "            need_test = False\n",
        "    if need_test:\n",
        "        t0=time.time()\n",
        "        XT = _extract_in_chunks(test_files, chunk=chunk, n_jobs=n_jobs, tag='test')\n",
        "        np.save(cache_test, XT)\n",
        "        _save_paths(test_files, test_paths_txt)\n",
        "        print('Saved', cache_test, XT.shape, 'in %.1fs' % (time.time()-t0), flush=True)\n",
        "    else:\n",
        "        XT = np.load(cache_test); print('Loaded', cache_test, XT.shape)\n",
        "    return df, X, test_files, XT\n",
        "\n",
        "def train_lgbm(df, X, n_splits=5, seed=42):\n",
        "    params = dict(objective='multiclass', num_class=4, learning_rate=0.05,\n",
        "                  max_depth=-1, num_leaves=255, feature_fraction=0.9, bagging_fraction=0.9,\n",
        "                  bagging_freq=1, min_data_in_leaf=50, lambda_l1=0.0, lambda_l2=0.0,\n",
        "                  n_jobs=36, verbose=-1)\n",
        "    oof = np.zeros((len(df), 4), dtype=np.float32)\n",
        "    skf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "    for fold, (trn_idx, val_idx) in enumerate(skf.split(X, y=df['label_4c'], groups=df['source_id'])):\n",
        "        t0=time.time()\n",
        "        Xtr, Xva = X[trn_idx], X[val_idx]\n",
        "        ytr, yva = df['label_4c'].values[trn_idx], df['label_4c'].values[val_idx]\n",
        "        dtr = lgb.Dataset(Xtr, label=ytr)\n",
        "        dva = lgb.Dataset(Xva, label=yva)\n",
        "        print(f'[Fold {fold}] train {Xtr.shape} val {Xva.shape}', flush=True)\n",
        "        # Use fixed rounds to avoid early_stopping API mismatch\n",
        "        bst = lgb.train(params, dtr, num_boost_round=600, valid_sets=[dtr, dva], valid_names=['tr','va'])\n",
        "        oof[val_idx] = bst.predict(Xva)\n",
        "        # AUROC over stego vs cover\n",
        "        p_cover = oof[val_idx, 0]\n",
        "        y_bin = (df.iloc[val_idx]['label_4c'].values!=0).astype(np.int32)\n",
        "        try:\n",
        "            auc = roc_auc_score(y_bin, 1.0 - p_cover)\n",
        "            print(f'[Fold {fold}] bin AUC: {auc:.4f} | time {time.time()-t0:.1f}s', flush=True)\n",
        "        except Exception:\n",
        "            print(f'[Fold {fold}] AUC failed | time {time.time()-t0:.1f}s', flush=True)\n",
        "        del Xtr, Xva, ytr, yva, dtr, dva, bst; gc.collect()\n",
        "    return oof\n",
        "\n",
        "def fit_and_predict(subsample_n=None, n_splits=5, chunk=5000, n_jobs=36):\n",
        "    df, X, test_files, XT = build_features(subsample_n=subsample_n, chunk=chunk, n_jobs=n_jobs)\n",
        "    oof = train_lgbm(df, X, n_splits=n_splits, seed=42)\n",
        "    # Train final model on full (subsampled) data\n",
        "    params = dict(objective='multiclass', num_class=4, learning_rate=0.05,\n",
        "                  max_depth=-1, num_leaves=255, feature_fraction=0.9, bagging_fraction=0.9,\n",
        "                  bagging_freq=1, min_data_in_leaf=50, lambda_l1=0.0, lambda_l2=0.0,\n",
        "                  n_jobs=36, verbose=-1)\n",
        "    dfull = lgb.Dataset(X, label=df['label_4c'].values)\n",
        "    print('Training final model on full data...', flush=True)\n",
        "    bst = lgb.train(params, dfull, num_boost_round=800)\n",
        "    Ptest = bst.predict(XT)\n",
        "    # Submission: Label = 1 - p_cover\n",
        "    p_stego = 1.0 - Ptest[:,0]\n",
        "    sub = pd.DataFrame({'Id': [os.path.basename(p) for p in test_files], 'Label': p_stego})\n",
        "    sub = sub.sort_values('Id')\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv', sub.shape, flush=True)\n",
        "\n",
        "# To run: fit_and_predict(subsample_n=20000, n_splits=3)  # smoke; then scale up\n",
        "print('CPU DCT-hist pipeline cell ready. Call fit_and_predict(subsample_n=..., n_splits=...) to start.', flush=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU DCT-hist pipeline cell ready. Call fit_and_predict(subsample_n=..., n_splits=...) to start.\n"
          ]
        }
      ]
    },
    {
      "id": "38da2c35-5954-4162-b22f-c48f70a3c41a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Start CPU feature extraction/training/submission (smoke run first)\n",
        "import os\n",
        "# Avoid BLAS over-subscription when using joblib threads\n",
        "os.environ['OMP_NUM_THREADS'] = '1'\n",
        "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
        "os.environ['MKL_NUM_THREADS'] = '1'\n",
        "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
        "print('Launching fit_and_predict(subsample_n=20000, n_splits=3) ...', flush=True)\n",
        "fit_and_predict(subsample_n=20000, n_splits=3, chunk=5000, n_jobs=36)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching fit_and_predict(subsample_n=20000, n_splits=3) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subsampled manifest to 20000 rows (approx stratified by label)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] processed 5000/20000 in chunk 1, elapsed 34.5s total 34.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] processed 10000/20000 in chunk 2, elapsed 34.5s total 69.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] processed 15000/20000 in chunk 3, elapsed 34.6s total 103.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] processed 20000/20000 in chunk 4, elapsed 34.4s total 138.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved train_feats_20000.npy (20000, 420) in 138.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded test_feats.npy (5000, 420)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] train (13332, 420) val (6668, 420)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] bin AUC: 0.4974 | time 81.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] train (13336, 420) val (6664, 420)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 1] bin AUC: 0.5147 | time 78.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] train (13332, 420) val (6668, 420)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] bin AUC: 0.5180 | time 79.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training final model on full data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (5000, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "1b60c8d4-2c7a-4a84-a0ca-80e528dfa7f6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sanity checks: alignment, splits, and cache validity for subsample_n=20000\n",
        "import os, glob, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "\n",
        "manifest_csv = 'manifest.csv'\n",
        "cache_train = 'train_feats_20000.npy'\n",
        "paths_txt = cache_train + '.paths.txt'\n",
        "\n",
        "df = pd.read_csv(manifest_csv)\n",
        "# Recreate the deterministic 20k subsample logic (per cell 6): per-class sample then sort by path\n",
        "per = max(1, 20000 // 4)\n",
        "dfs = []\n",
        "for c in [0,1,2,3]:\n",
        "    sub = df[df['label_4c']==c].sample(n=per, random_state=42)\n",
        "    dfs.append(sub)\n",
        "df_sub = pd.concat(dfs, axis=0).sort_values('path').reset_index(drop=True)\n",
        "train_paths_expected = df_sub['path'].tolist()\n",
        "\n",
        "print('Subsampled df_sub:', df_sub.shape, 'cover:', (df_sub.label_4c==0).sum(), flush=True)\n",
        "\n",
        "# Compare cached paths (if exists)\n",
        "cached_ok = False\n",
        "if os.path.exists(paths_txt):\n",
        "    with open(paths_txt, 'r') as f:\n",
        "        cached_paths = [line.strip() for line in f.readlines()]\n",
        "    mism = sum(a!=b for a,b in zip(cached_paths, train_paths_expected)) if len(cached_paths)==len(train_paths_expected) else -1\n",
        "    cached_ok = (mism == 0 and len(cached_paths)==len(train_paths_expected))\n",
        "    print('Cache paths exists:', True, '| same length:', len(cached_paths)==len(train_paths_expected), '| mismatches:', mism, flush=True)\n",
        "else:\n",
        "    print('Cache paths file missing:', paths_txt, flush=True)\n",
        "\n",
        "# Basic split/group sanity on df_sub\n",
        "skf = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "fold_ids = np.full(len(df_sub), -1, dtype=int)\n",
        "for k, (tr, va) in enumerate(skf.split(df_sub, y=df_sub['label_4c'], groups=df_sub['source_id'])):\n",
        "    fold_ids[va] = k\n",
        "df_sub['fold'] = fold_ids\n",
        "print('Fold/label counts:\\n', df_sub.groupby(['fold','label_4c']).size().unstack(1), flush=True)\n",
        "# Ensure no source_id in both train and val per fold\n",
        "for k in range(3):\n",
        "    tr_sid = set(df_sub.loc[df_sub['fold']!=k, 'source_id'])\n",
        "    va_sid = set(df_sub.loc[df_sub['fold']==k, 'source_id'])\n",
        "    inter = tr_sid.intersection(va_sid)\n",
        "    print(f'Fold {k} group leakage count:', len(inter), flush=True)\n",
        "\n",
        "# Feature health if cache present\n",
        "if os.path.exists(cache_train):\n",
        "    X = np.load(cache_train)\n",
        "    print('Loaded X:', X.shape, flush=True)\n",
        "    if X.shape[0] != len(df_sub):\n",
        "        print('ROW COUNT MISMATCH: X rows vs df_sub rows ->', X.shape[0], len(df_sub), flush=True)\n",
        "    n_nan = int(np.isnan(X).sum())\n",
        "    n_zero_var = int((X.std(0)==0).sum())\n",
        "    print('NaNs:', n_nan, '| zero-variance feats:', n_zero_var, flush=True)\n",
        "else:\n",
        "    print('Cache feature file missing:', cache_train, flush=True)\n",
        "\n",
        "# If cache paths missing or mismatch, advise deletion\n",
        "if (not os.path.exists(paths_txt)) or (not cached_ok):\n",
        "    print('Recommendation: delete stale cache to force regeneration:', cache_train, 'and', paths_txt, flush=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subsampled df_sub: (20000, 7) cover: 5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cache paths file missing: train_feats_20000.npy.paths.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold/label counts:\n label_4c     0     1     2     3\nfold                            \n0         1667  1667  1667  1667\n1         1666  1666  1666  1666\n2         1667  1667  1667  1667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0 group leakage count: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 group leakage count: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 group leakage count: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded X: (20000, 420)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaNs: 0 | zero-variance feats: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommendation: delete stale cache to force regeneration: train_feats_20000.npy and train_feats_20000.npy.paths.txt\n"
          ]
        }
      ]
    },
    {
      "id": "6228a697-550a-4661-a48f-980ddb0ce7ad",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Force-regenerate cached features for 20k subset\n",
        "import os\n",
        "targets = ['train_feats_20000.npy', 'train_feats_20000.npy.paths.txt']\n",
        "for t in targets:\n",
        "    if os.path.exists(t):\n",
        "        try:\n",
        "            os.remove(t)\n",
        "            print('Deleted', t, flush=True)\n",
        "        except Exception as e:\n",
        "            print('Failed to delete', t, e, flush=True)\n",
        "    else:\n",
        "        print('Not found (ok):', t, flush=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted train_feats_20000.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not found (ok): train_feats_20000.npy.paths.txt\n"
          ]
        }
      ]
    },
    {
      "id": "024167cf-8125-4d12-9118-31267ef699b6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# SRM-lite residuals + co-occurrences (CPU) and LightGBM baseline\n",
        "import os, time, glob, numpy as np, pandas as pd, gc\n",
        "from PIL import Image\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import lightgbm as lgb\n",
        "from scipy.signal import convolve2d\n",
        "\n",
        "def load_Y(path):\n",
        "    im = Image.open(path).convert('YCbCr')\n",
        "    y, _, _ = im.split()\n",
        "    return np.asarray(y, dtype=np.float32)\n",
        "\n",
        "# 6 SRM-lite kernels (as per expert advice) with symmetric padding\n",
        "K3_1 = np.array([[0,1,0],[1,-4,1],[0,1,0]], dtype=np.float32)\n",
        "K3_2 = np.array([[-1,2,-1],[2,-4,2],[-1,2,-1]], dtype=np.float32)\n",
        "K3_H = np.array([[-1,2,-1],[-1,2,-1],[-1,2,-1]], dtype=np.float32)\n",
        "K3_V = K3_H.T.copy()\n",
        "K3_D = np.array([[ 2,-1, 0],[-1, 0, 1],[ 0, 1,-2]], dtype=np.float32)\n",
        "K5_S = (1.0/12.0) * np.array([[-1, 2,-2, 2,-1],\n",
        "                               [ 2,-6, 8,-6, 2],\n",
        "                               [-2, 8,-12, 8,-2],\n",
        "                               [ 2,-6, 8,-6, 2],\n",
        "                               [-1, 2,-2, 2,-1]], dtype=np.float32)\n",
        "SRM_KERNELS = [K3_1, K3_2, K3_H, K3_V, K3_D, K5_S]\n",
        "\n",
        "def _quantize(res, T=3):\n",
        "    q = np.rint(res)\n",
        "    return np.clip(q, -T, T).astype(np.int8)  # values in [-T..T]\n",
        "\n",
        "def _hist7(v):\n",
        "    # v int in [-3..3] -> bins 0..6\n",
        "    idx = (v + 3).ravel().astype(np.int32)\n",
        "    h = np.bincount(idx, minlength=7).astype(np.float32)\n",
        "    s = h.sum();\n",
        "    return h / (s if s>0 else 1.0)\n",
        "\n",
        "def _cooc2(v, axis=1):\n",
        "    # 2nd-order co-occurrence along axis: returns 49-dim (7x7) L1-normalized\n",
        "    if axis==1:\n",
        "        a = v[:, :-1].astype(np.int32) + 3\n",
        "        b = v[:,  1: ].astype(np.int32) + 3\n",
        "    else:\n",
        "        a = v[:-1, :].astype(np.int32) + 3\n",
        "        b = v[ 1:, :].astype(np.int32) + 3\n",
        "    idx = (a * 7 + b).ravel()\n",
        "    h = np.bincount(idx, minlength=49).astype(np.float32)\n",
        "    s = h.sum();\n",
        "    return h / (s if s>0 else 1.0)\n",
        "\n",
        "def srm_features_from_Y(y, T=3):\n",
        "    feats = []\n",
        "    for k in SRM_KERNELS:\n",
        "        r = convolve2d(y, k, mode='same', boundary='symm')\n",
        "        q = _quantize(r, T=T)\n",
        "        feats.append(_hist7(q))           # 7\n",
        "        feats.append(_cooc2(q, axis=1))   # 49 horizontal\n",
        "        feats.append(_cooc2(q, axis=0))   # 49 vertical\n",
        "    return np.concatenate(feats).astype(np.float32)  # 6*(7+49+49) = 630\n",
        "\n",
        "def srm_extract_one(path):\n",
        "    try:\n",
        "        y = load_Y(path)\n",
        "        return srm_features_from_Y(y, T=3)\n",
        "    except Exception:\n",
        "        return np.full(630, np.nan, dtype=np.float32)\n",
        "\n",
        "def _paths_save(paths, fname):\n",
        "    with open(fname, 'w') as f:\n",
        "        for p in paths: f.write(str(p)+'\\n')\n",
        "def _paths_load(fname):\n",
        "    if not os.path.exists(fname): return None\n",
        "    with open(fname,'r') as f: return [line.strip() for line in f.readlines()]\n",
        "\n",
        "def _extract_in_chunks_srm(paths, chunk=4000, n_jobs=36, tag='train'):\n",
        "    from joblib import Parallel, delayed\n",
        "    all_parts = []; t0=time.time()\n",
        "    for i in range(0, len(paths), chunk):\n",
        "        t1=time.time(); part_paths = paths[i:i+chunk]\n",
        "        part = Parallel(n_jobs=n_jobs, batch_size=64, prefer='threads')(delayed(srm_extract_one)(p) for p in part_paths)\n",
        "        part = np.vstack(part).astype(np.float32); all_parts.append(part)\n",
        "        print(f'[{tag}-srm] processed {i+len(part_paths)}/{len(paths)} chunk {i//chunk+1} elapsed {time.time()-t1:.1f}s total {time.time()-t0:.1f}s', flush=True)\n",
        "    return np.vstack(all_parts) if all_parts else np.zeros((0,630), dtype=np.float32)\n",
        "\n",
        "def build_features_srm(manifest_csv='manifest.csv', subsample_n=None, seed=42, n_jobs=36, chunk=4000):\n",
        "    df = pd.read_csv(manifest_csv)\n",
        "    if subsample_n is not None and subsample_n < len(df):\n",
        "        per = max(1, subsample_n // 4)\n",
        "        dfs = []\n",
        "        for c in [0,1,2,3]:\n",
        "            dfs.append(df[df['label_4c']==c].sample(n=per, random_state=seed))\n",
        "        df = pd.concat(dfs, axis=0).sort_values('path').reset_index(drop=True)\n",
        "        cache_train = f'train_feats_srm_{len(df)}.npy'\n",
        "    else:\n",
        "        df = df.sort_values('path').reset_index(drop=True)\n",
        "        cache_train = 'train_feats_srm.npy'\n",
        "    train_paths = df['path'].tolist()\n",
        "    train_paths_txt = cache_train + '.paths.txt'\n",
        "    test_paths = sorted(glob.glob('Test/*.jpg'))\n",
        "    cache_test = 'test_feats_srm.npy'\n",
        "    test_paths_txt = cache_test + '.paths.txt'\n",
        "    # Train cache validate\n",
        "    need_train = True\n",
        "    if os.path.exists(cache_train) and os.path.exists(train_paths_txt):\n",
        "        if _paths_load(train_paths_txt) == train_paths:\n",
        "            need_train = False\n",
        "    if need_train:\n",
        "        X = _extract_in_chunks_srm(train_paths, chunk=chunk, n_jobs=n_jobs, tag='train')\n",
        "        np.save(cache_train, X); _paths_save(train_paths, train_paths_txt)\n",
        "        print('Saved', cache_train, X.shape, flush=True)\n",
        "    else:\n",
        "        X = np.load(cache_train); print('Loaded', cache_train, X.shape, flush=True)\n",
        "    # Test cache validate\n",
        "    need_test = True\n",
        "    if os.path.exists(cache_test) and os.path.exists(test_paths_txt):\n",
        "        if _paths_load(test_paths_txt) == test_paths:\n",
        "            need_test = False\n",
        "    if need_test:\n",
        "        XT = _extract_in_chunks_srm(test_paths, chunk=chunk, n_jobs=n_jobs, tag='test')\n",
        "        np.save(cache_test, XT); _paths_save(test_paths, test_paths_txt)\n",
        "        print('Saved', cache_test, XT.shape, flush=True)\n",
        "    else:\n",
        "        XT = np.load(cache_test); print('Loaded', cache_test, XT.shape, flush=True)\n",
        "    return df, X, test_paths, XT, cache_train, cache_test\n",
        "\n",
        "def train_lgbm_srm(df, X, n_splits=5, seed=42):\n",
        "    params = dict(objective='multiclass', num_class=4, learning_rate=0.03,\n",
        "                  max_depth=8, num_leaves=128, min_data_in_leaf=100,\n",
        "                  feature_fraction=0.6, bagging_fraction=0.7, bagging_freq=1,\n",
        "                  lambda_l1=1.0, lambda_l2=1.0, n_jobs=36, verbose=-1)\n",
        "    oof = np.zeros((len(df), 4), dtype=np.float32)\n",
        "    skf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "    for fold, (tr, va) in enumerate(skf.split(X, y=df['label_4c'], groups=df['source_id'])):\n",
        "        t0=time.time()\n",
        "        dtr = lgb.Dataset(X[tr], label=df['label_4c'].values[tr])\n",
        "        dva = lgb.Dataset(X[va], label=df['label_4c'].values[va])\n",
        "        print(f'[SRM Fold {fold}] train {len(tr)} val {len(va)}', flush=True)\n",
        "        bst = lgb.train(params, dtr, num_boost_round=1200, valid_sets=[dtr, dva], valid_names=['tr','va'])\n",
        "        oof[va] = bst.predict(X[va])\n",
        "        y_bin = (df.iloc[va]['label_4c'].values!=0).astype(np.int32)\n",
        "        try:\n",
        "            auc = roc_auc_score(y_bin, 1.0 - oof[va,0])\n",
        "            print(f'[SRM Fold {fold}] bin AUC: {auc:.4f} | time {time.time()-t0:.1f}s', flush=True)\n",
        "        except Exception:\n",
        "            print(f'[SRM Fold {fold}] AUC failed | time {time.time()-t0:.1f}s', flush=True)\n",
        "        del dtr, dva; gc.collect()\n",
        "    return oof\n",
        "\n",
        "def fit_and_predict_srm(subsample_n=20000, n_splits=3, chunk=4000, n_jobs=36):\n",
        "    print('Launching SRM-lite pipeline...', flush=True)\n",
        "    df, X, test_paths, XT, ctr, cte = build_features_srm(subsample_n=subsample_n, chunk=chunk, n_jobs=n_jobs)\n",
        "    print('Features SRM:', X.shape, XT.shape, flush=True)\n",
        "    oof = train_lgbm_srm(df, X, n_splits=n_splits, seed=42)\n",
        "    # Final model\n",
        "    params = dict(objective='multiclass', num_class=4, learning_rate=0.03,\n",
        "                  max_depth=8, num_leaves=128, min_data_in_leaf=100,\n",
        "                  feature_fraction=0.6, bagging_fraction=0.7, bagging_freq=1,\n",
        "                  lambda_l1=1.0, lambda_l2=1.0, n_jobs=36, verbose=-1)\n",
        "    print('Training final SRM model...', flush=True)\n",
        "    dfull = lgb.Dataset(X, label=df['label_4c'].values)\n",
        "    bst = lgb.train(params, dfull, num_boost_round=1500)\n",
        "    P = bst.predict(XT)\n",
        "    p_stego = 1.0 - P[:,0]\n",
        "    sub = pd.DataFrame({'Id': [os.path.basename(p) for p in test_paths], 'Label': p_stego}).sort_values('Id')\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv', sub.shape, flush=True)\n",
        "    return oof"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "id": "6a3a6cf1-25f8-4f10-af12-c59f70764e34",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run SRM-lite smoke test (smaller subset for quick signal check)\n",
        "import os\n",
        "os.environ['OMP_NUM_THREADS'] = '1'\n",
        "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
        "os.environ['MKL_NUM_THREADS'] = '1'\n",
        "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
        "print('Launching fit_and_predict_srm(subsample_n=5000, n_splits=3) ...', flush=True)\n",
        "oof_srm = fit_and_predict_srm(subsample_n=5000, n_splits=3, chunk=4000, n_jobs=36)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching fit_and_predict_srm(subsample_n=5000, n_splits=3) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching SRM-lite pipeline...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train-srm] processed 4000/5000 chunk 1 elapsed 217.8s total 217.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train-srm] processed 5000/5000 chunk 2 elapsed 53.7s total 271.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved train_feats_srm_5000.npy (5000, 630)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[test-srm] processed 4000/5000 chunk 1 elapsed 216.9s total 216.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[test-srm] processed 5000/5000 chunk 2 elapsed 54.2s total 271.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved test_feats_srm.npy (5000, 630)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features SRM: (5000, 630) (5000, 630)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SRM Fold 0] train 3332 val 1668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SRM Fold 0] bin AUC: 0.5379 | time 10.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SRM Fold 1] train 3332 val 1668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SRM Fold 1] bin AUC: 0.5368 | time 7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SRM Fold 2] train 3336 val 1664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SRM Fold 2] bin AUC: 0.5183 | time 9.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training final SRM model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (5000, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "48e0bed3-124e-4a2f-a588-f17d0b7e1e48",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DCTR-lite JPEG-domain features (CPU) + LightGBM\n",
        "import os, time, glob, numpy as np, pandas as pd, gc, subprocess, sys\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Ensure jpeg2dct installed\n",
        "try:\n",
        "    import jpeg2dct\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--no-cache-dir', 'jpeg2dct'], check=True)\n",
        "    import jpeg2dct\n",
        "\n",
        "# First 12 AC zigzag positions (skip DC (0,0))\n",
        "def zigzag_8x8():\n",
        "    n=8; idx=[]\n",
        "    for s in range(2*n-1):\n",
        "        if s%2==0:\n",
        "            for i in range(s, -1, -1):\n",
        "                j=s-i\n",
        "                if i<n and j<n: idx.append((i,j))\n",
        "        else:\n",
        "            for j in range(s, -1, -1):\n",
        "                i=s-j\n",
        "                if i<n and j<n: idx.append((i,j))\n",
        "    return idx\n",
        "zz = zigzag_8x8()[1:13]  # 12 AC subbands\n",
        "\n",
        "def _ternary(x, thr=1):\n",
        "    # map to {-1,0,1} with dead-zone\n",
        "    t = np.zeros_like(x, dtype=np.int8)\n",
        "    t[x >  thr] = 1\n",
        "    t[x < -thr] = -1\n",
        "    return t\n",
        "\n",
        "def _cooc3_1d(arr, order=3, base=3):\n",
        "    # arr values in {-1,0,1} -> map to {0,1,2}; 3^order bins\n",
        "    v = (arr + 1).astype(np.int32)\n",
        "    if order == 3:\n",
        "        a = v[:, :-2]; b = v[:, 1:-1]; c = v[:, 2:]\n",
        "        idx = (a * (base*base) + b * base + c).ravel()\n",
        "        hist = np.bincount(idx, minlength=base**order).astype(np.float32)\n",
        "    else:\n",
        "        raise ValueError('order!=3 not supported')\n",
        "    s = hist.sum()\n",
        "    return hist / (s if s>0 else 1.0)\n",
        "\n",
        "def dctr_lite_one(path, subbands=zz, thr=1):\n",
        "    try:\n",
        "        coefs = jpeg2dct.load(path)\n",
        "        # coefs.y shape: (H/8, W/8, 8, 8) of quantized DCT ints\n",
        "        Y = coefs.y  # int16/32\n",
        "        feats = []\n",
        "        for (i,j) in subbands:\n",
        "            sub = Y[..., i, j].astype(np.int32)\n",
        "            # inter-block horizontal/vertical diffs\n",
        "            Dx = sub[:, 1:] - sub[:, :-1]\n",
        "            Dy = sub[1:, :] - sub[:-1, :]\n",
        "            tx = _ternary(Dx, thr=thr)\n",
        "            ty = _ternary(Dy, thr=thr)\n",
        "            # 3rd-order co-occ along rows for Dx and along cols for Dy\n",
        "            hx = _cooc3_1d(tx, order=3, base=3)  # 27\n",
        "            hy = _cooc3_1d(ty.T, order=3, base=3)  # also along rows after transpose -> 27\n",
        "            feats.append(hx); feats.append(hy)\n",
        "        return np.concatenate(feats).astype(np.float32)  # 12 * (27+27) = 648\n",
        "    except Exception:\n",
        "        return np.full(12*54, np.nan, dtype=np.float32)\n",
        "\n",
        "def _paths_save(paths, fname):\n",
        "    with open(fname, 'w') as f:\n",
        "        for p in paths: f.write(str(p)+'\\n')\n",
        "def _paths_load(fname):\n",
        "    if not os.path.exists(fname): return None\n",
        "    with open(fname,'r') as f: return [line.strip() for line in f.readlines()]\n",
        "\n",
        "def _extract_in_chunks_dctr(paths, chunk=6000, n_jobs=36, tag='train'):\n",
        "    from joblib import Parallel, delayed\n",
        "    all_parts = []; t0=time.time()\n",
        "    for i in range(0, len(paths), chunk):\n",
        "        t1=time.time(); part_paths = paths[i:i+chunk]\n",
        "        part = Parallel(n_jobs=n_jobs, batch_size=64, prefer='threads')(delayed(dctr_lite_one)(p) for p in part_paths)\n",
        "        part = np.vstack(part).astype(np.float32); all_parts.append(part)\n",
        "        print(f'[{tag}-dctr] processed {i+len(part_paths)}/{len(paths)} chunk {i//chunk+1} elapsed {time.time()-t1:.1f}s total {time.time()-t0:.1f}s', flush=True)\n",
        "    return np.vstack(all_parts) if all_parts else np.zeros((0,648), dtype=np.float32)\n",
        "\n",
        "def build_features_dctr(manifest_csv='manifest.csv', subsample_n=None, seed=42, n_jobs=36, chunk=6000):\n",
        "    df = pd.read_csv(manifest_csv)\n",
        "    if subsample_n is not None and subsample_n < len(df):\n",
        "        per = max(1, subsample_n // 4)\n",
        "        dfs = [df[df['label_4c']==c].sample(n=per, random_state=seed) for c in [0,1,2,3]]\n",
        "        df = pd.concat(dfs, axis=0).sort_values('path').reset_index(drop=True)\n",
        "        cache_train = f'train_feats_dctr_{len(df)}.npy'\n",
        "    else:\n",
        "        df = df.sort_values('path').reset_index(drop=True)\n",
        "        cache_train = 'train_feats_dctr.npy'\n",
        "    train_paths = df['path'].tolist()\n",
        "    train_paths_txt = cache_train + '.paths.txt'\n",
        "    test_paths = sorted(glob.glob('Test/*.jpg'))\n",
        "    cache_test = 'test_feats_dctr.npy'\n",
        "    test_paths_txt = cache_test + '.paths.txt'\n",
        "    # Train cache validate\n",
        "    need_train = True\n",
        "    if os.path.exists(cache_train) and os.path.exists(train_paths_txt):\n",
        "        if _paths_load(train_paths_txt) == train_paths:\n",
        "            need_train = False\n",
        "    if need_train:\n",
        "        X = _extract_in_chunks_dctr(train_paths, chunk=chunk, n_jobs=n_jobs, tag='train')\n",
        "        np.save(cache_train, X); _paths_save(train_paths, train_paths_txt)\n",
        "        print('Saved', cache_train, X.shape, flush=True)\n",
        "    else:\n",
        "        X = np.load(cache_train); print('Loaded', cache_train, X.shape, flush=True)\n",
        "    # Test cache validate\n",
        "    need_test = True\n",
        "    if os.path.exists(cache_test) and os.path.exists(test_paths_txt):\n",
        "        if _paths_load(test_paths_txt) == test_paths:\n",
        "            need_test = False\n",
        "    if need_test:\n",
        "        XT = _extract_in_chunks_dctr(test_paths, chunk=chunk, n_jobs=n_jobs, tag='test')\n",
        "        np.save(cache_test, XT); _paths_save(test_paths, test_paths_txt)\n",
        "        print('Saved', cache_test, XT.shape, flush=True)\n",
        "    else:\n",
        "        XT = np.load(cache_test); print('Loaded', cache_test, XT.shape, flush=True)\n",
        "    return df, X, test_paths, XT, cache_train, cache_test\n",
        "\n",
        "def train_lgbm_dctr(df, X, n_splits=3, seed=42):\n",
        "    params = dict(objective='multiclass', num_class=4, learning_rate=0.03,\n",
        "                  max_depth=8, num_leaves=128, min_data_in_leaf=100,\n",
        "                  feature_fraction=0.6, bagging_fraction=0.7, bagging_freq=1,\n",
        "                  lambda_l1=1.0, lambda_l2=1.0, n_jobs=36, verbose=-1)\n",
        "    oof = np.zeros((len(df), 4), dtype=np.float32)\n",
        "    skf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "    for fold, (tr, va) in enumerate(skf.split(X, y=df['label_4c'], groups=df['source_id'])):\n",
        "        t0=time.time()\n",
        "        dtr = lgb.Dataset(X[tr], label=df['label_4c'].values[tr])\n",
        "        dva = lgb.Dataset(X[va], label=df['label_4c'].values[va])\n",
        "        print(f'[DCTR Fold {fold}] train {len(tr)} val {len(va)}', flush=True)\n",
        "        bst = lgb.train(params, dtr, num_boost_round=1500, valid_sets=[dtr, dva], valid_names=['tr','va'])\n",
        "        oof[va] = bst.predict(X[va])\n",
        "        y_bin = (df.iloc[va]['label_4c'].values!=0).astype(np.int32)\n",
        "        try:\n",
        "            auc = roc_auc_score(y_bin, 1.0 - oof[va,0])\n",
        "            print(f'[DCTR Fold {fold}] bin AUC: {auc:.4f} | time {time.time()-t0:.1f}s', flush=True)\n",
        "        except Exception:\n",
        "            print(f'[DCTR Fold {fold}] AUC failed | time {time.time()-t0:.1f}s', flush=True)\n",
        "        del dtr, dva; gc.collect()\n",
        "    return oof\n",
        "\n",
        "def fit_and_predict_dctr(subsample_n=5000, n_splits=3, chunk=6000, n_jobs=36):\n",
        "    print('Launching DCTR-lite pipeline...', flush=True)\n",
        "    df, X, test_paths, XT, ctr, cte = build_features_dctr(subsample_n=subsample_n, chunk=chunk, n_jobs=n_jobs)\n",
        "    print('Features DCTR:', X.shape, XT.shape, flush=True)\n",
        "    oof = train_lgbm_dctr(df, X, n_splits=n_splits, seed=42)\n",
        "    # Final model\n",
        "    params = dict(objective='multiclass', num_class=4, learning_rate=0.03,\n",
        "                  max_depth=8, num_leaves=128, min_data_in_leaf=100,\n",
        "                  feature_fraction=0.6, bagging_fraction=0.7, bagging_freq=1,\n",
        "                  lambda_l1=1.0, lambda_l2=1.0, n_jobs=36, verbose=-1)\n",
        "    print('Training final DCTR model...', flush=True)\n",
        "    dfull = lgb.Dataset(X, label=df['label_4c'].values)\n",
        "    bst = lgb.train(params, dfull, num_boost_round=1800)\n",
        "    P = bst.predict(XT)\n",
        "    p_stego = 1.0 - P[:,0]\n",
        "    sub = pd.DataFrame({'Id': [os.path.basename(p) for p in test_paths], 'Label': p_stego}).sort_values('Id')\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv', sub.shape, flush=True)\n",
        "    return oof"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jpeg2dct\n  Downloading jpeg2dct-0.2.4.tar.gz (106 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 106.5/106.5 KB 4.7 MB/s eta 0:00:00\n  Preparing metadata (setup.py): started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py): finished with status 'done'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 196.9 MB/s eta 0:00:00\nBuilding wheels for collected packages: jpeg2dct\n  Building wheel for jpeg2dct (setup.py): started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for jpeg2dct (setup.py): finished with status 'error'\n  Running setup.py clean for jpeg2dct\nFailed to build jpeg2dct\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  error: subprocess-exited-with-error\n  \n  \u00d7 python setup.py bdist_wheel did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [63 lines of output]\n      running bdist_wheel\n      running build\n      running build_py\n      creating build\n      creating build/lib.linux-x86_64-3.11\n      creating build/lib.linux-x86_64-3.11/test\n      copying test/__init__.py -> build/lib.linux-x86_64-3.11/test\n      creating build/lib.linux-x86_64-3.11/jpeg2dct\n      copying jpeg2dct/__init__.py -> build/lib.linux-x86_64-3.11/jpeg2dct\n      creating build/lib.linux-x86_64-3.11/test/tensorflow\n      copying test/tensorflow/__init__.py -> build/lib.linux-x86_64-3.11/test/tensorflow\n      copying test/tensorflow/test_decode.py -> build/lib.linux-x86_64-3.11/test/tensorflow\n      creating build/lib.linux-x86_64-3.11/test/numpy\n      copying test/numpy/test_load.py -> build/lib.linux-x86_64-3.11/test/numpy\n      copying test/numpy/__init__.py -> build/lib.linux-x86_64-3.11/test/numpy\n      creating build/lib.linux-x86_64-3.11/jpeg2dct/petastorm\n      copying jpeg2dct/petastorm/ex.py -> build/lib.linux-x86_64-3.11/jpeg2dct/petastorm\n      copying jpeg2dct/petastorm/__init__.py -> build/lib.linux-x86_64-3.11/jpeg2dct/petastorm\n      copying jpeg2dct/petastorm/codecs.py -> build/lib.linux-x86_64-3.11/jpeg2dct/petastorm\n      creating build/lib.linux-x86_64-3.11/jpeg2dct/tensorflow\n      copying jpeg2dct/tensorflow/__init__.py -> build/lib.linux-x86_64-3.11/jpeg2dct/tensorflow\n      creating build/lib.linux-x86_64-3.11/jpeg2dct/common\n      copying jpeg2dct/common/__init__.py -> build/lib.linux-x86_64-3.11/jpeg2dct/common\n      creating build/lib.linux-x86_64-3.11/jpeg2dct/numpy\n      copying jpeg2dct/numpy/__init__.py -> build/lib.linux-x86_64-3.11/jpeg2dct/numpy\n      copying jpeg2dct/numpy/dctfromjpg_wrapper.py -> build/lib.linux-x86_64-3.11/jpeg2dct/numpy\n      running build_ext\n      x86_64-linux-gnu-gcc -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -std=c++11 -fPIC -O2 -I/usr/include/python3.11 -c build/temp.linux-x86_64-3.11/test_compile/test_cpp_flags.cc -o build/temp.linux-x86_64-3.11/test_compile/test_cpp_flags.o\n      x86_64-linux-gnu-gcc -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.11/test_compile/test_cpp_flags.o -o build/temp.linux-x86_64-3.11/test_compile/test_cpp_flags.so\n      INFO: Unable to build TensorFlow plugin, will skip it.\n      \n      Traceback (most recent call last):\n        File \"/tmp/pip-install-5mvle_21/jpeg2dct_5e35bc6d5d114dd68b890176027080cd/setup.py\", line 29, in check_tf_version\n          import tensorflow as tf\n      ModuleNotFoundError: No module named 'tensorflow'\n      \n      During handling of the above exception, another exception occurred:\n      \n      Traceback (most recent call last):\n        File \"/tmp/pip-install-5mvle_21/jpeg2dct_5e35bc6d5d114dd68b890176027080cd/setup.py\", line 270, in build_extensions\n          abi_compile_flags = build_tf_extension(self, options)\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"/tmp/pip-install-5mvle_21/jpeg2dct_5e35bc6d5d114dd68b890176027080cd/setup.py\", line 242, in build_tf_extension\n          check_tf_version()\n        File \"/tmp/pip-install-5mvle_21/jpeg2dct_5e35bc6d5d114dd68b890176027080cd/setup.py\", line 35, in check_tf_version\n          raise DistutilsPlatformError(\n      distutils.errors.DistutilsPlatformError: import tensorflow failed, is it installed?\n      \n      Traceback (most recent call last):\n        File \"/tmp/pip-install-5mvle_21/jpeg2dct_5e35bc6d5d114dd68b890176027080cd/setup.py\", line 29, in check_tf_version\n          import tensorflow as tf\n      ModuleNotFoundError: No module named 'tensorflow'\n      \n      \n      building 'jpeg2dct.common.common_lib' extension\n      creating build/temp.linux-x86_64-3.11/jpeg2dct\n      creating build/temp.linux-x86_64-3.11/jpeg2dct/common\n      x86_64-linux-gnu-gcc -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I./include -I/usr/include/python3.11 -c jpeg2dct/common/dctfromjpg.cc -o build/temp.linux-x86_64-3.11/jpeg2dct/common/dctfromjpg.o -std=c++11 -fPIC -O2\n      jpeg2dct/common/dctfromjpg.cc:16:10: fatal error: jpeglib.h: No such file or directory\n         16 | #include <jpeglib.h>\n            |          ^~~~~~~~~~~\n      compilation terminated.\n      error: command '/usr/bin/x86_64-linux-gnu-gcc' failed with exit code 1\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for jpeg2dct\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: numpy, jpeg2dct\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Running setup.py install for jpeg2dct: started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Running setup.py install for jpeg2dct: finished with status 'error'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  error: subprocess-exited-with-error\n  \n  \u00d7 Running setup.py install for jpeg2dct did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [65 lines of output]\n      running install\n      /usr/lib/python3/dist-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n        warnings.warn(\n      running build\n      running build_py\n      creating build\n      creating build/lib.linux-x86_64-3.11\n      creating build/lib.linux-x86_64-3.11/test\n      copying test/__init__.py -> build/lib.linux-x86_64-3.11/test\n      creating build/lib.linux-x86_64-3.11/jpeg2dct\n      copying jpeg2dct/__init__.py -> build/lib.linux-x86_64-3.11/jpeg2dct\n      creating build/lib.linux-x86_64-3.11/test/tensorflow\n      copying test/tensorflow/__init__.py -> build/lib.linux-x86_64-3.11/test/tensorflow\n      copying test/tensorflow/test_decode.py -> build/lib.linux-x86_64-3.11/test/tensorflow\n      creating build/lib.linux-x86_64-3.11/test/numpy\n      copying test/numpy/test_load.py -> build/lib.linux-x86_64-3.11/test/numpy\n      copying test/numpy/__init__.py -> build/lib.linux-x86_64-3.11/test/numpy\n      creating build/lib.linux-x86_64-3.11/jpeg2dct/petastorm\n      copying jpeg2dct/petastorm/ex.py -> build/lib.linux-x86_64-3.11/jpeg2dct/petastorm\n      copying jpeg2dct/petastorm/__init__.py -> build/lib.linux-x86_64-3.11/jpeg2dct/petastorm\n      copying jpeg2dct/petastorm/codecs.py -> build/lib.linux-x86_64-3.11/jpeg2dct/petastorm\n      creating build/lib.linux-x86_64-3.11/jpeg2dct/tensorflow\n      copying jpeg2dct/tensorflow/__init__.py -> build/lib.linux-x86_64-3.11/jpeg2dct/tensorflow\n      creating build/lib.linux-x86_64-3.11/jpeg2dct/common\n      copying jpeg2dct/common/__init__.py -> build/lib.linux-x86_64-3.11/jpeg2dct/common\n      creating build/lib.linux-x86_64-3.11/jpeg2dct/numpy\n      copying jpeg2dct/numpy/__init__.py -> build/lib.linux-x86_64-3.11/jpeg2dct/numpy\n      copying jpeg2dct/numpy/dctfromjpg_wrapper.py -> build/lib.linux-x86_64-3.11/jpeg2dct/numpy\n      running build_ext\n      x86_64-linux-gnu-gcc -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -std=c++11 -fPIC -O2 -I/usr/include/python3.11 -c build/temp.linux-x86_64-3.11/test_compile/test_cpp_flags.cc -o build/temp.linux-x86_64-3.11/test_compile/test_cpp_flags.o\n      x86_64-linux-gnu-gcc -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.11/test_compile/test_cpp_flags.o -o build/temp.linux-x86_64-3.11/test_compile/test_cpp_flags.so\n      INFO: Unable to build TensorFlow plugin, will skip it.\n      \n      Traceback (most recent call last):\n        File \"/tmp/pip-install-5mvle_21/jpeg2dct_5e35bc6d5d114dd68b890176027080cd/setup.py\", line 29, in check_tf_version\n          import tensorflow as tf\n      ModuleNotFoundError: No module named 'tensorflow'\n      \n      During handling of the above exception, another exception occurred:\n      \n      Traceback (most recent call last):\n        File \"/tmp/pip-install-5mvle_21/jpeg2dct_5e35bc6d5d114dd68b890176027080cd/setup.py\", line 270, in build_extensions\n          abi_compile_flags = build_tf_extension(self, options)\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"/tmp/pip-install-5mvle_21/jpeg2dct_5e35bc6d5d114dd68b890176027080cd/setup.py\", line 242, in build_tf_extension\n          check_tf_version()\n        File \"/tmp/pip-install-5mvle_21/jpeg2dct_5e35bc6d5d114dd68b890176027080cd/setup.py\", line 35, in check_tf_version\n          raise DistutilsPlatformError(\n      distutils.errors.DistutilsPlatformError: import tensorflow failed, is it installed?\n      \n      Traceback (most recent call last):\n        File \"/tmp/pip-install-5mvle_21/jpeg2dct_5e35bc6d5d114dd68b890176027080cd/setup.py\", line 29, in check_tf_version\n          import tensorflow as tf\n      ModuleNotFoundError: No module named 'tensorflow'\n      \n      \n      building 'jpeg2dct.common.common_lib' extension\n      creating build/temp.linux-x86_64-3.11/jpeg2dct\n      creating build/temp.linux-x86_64-3.11/jpeg2dct/common\n      x86_64-linux-gnu-gcc -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I./include -I/usr/include/python3.11 -c jpeg2dct/common/dctfromjpg.cc -o build/temp.linux-x86_64-3.11/jpeg2dct/common/dctfromjpg.o -std=c++11 -fPIC -O2\n      jpeg2dct/common/dctfromjpg.cc:16:10: fatal error: jpeglib.h: No such file or directory\n         16 | #include <jpeglib.h>\n            |          ^~~~~~~~~~~\n      compilation terminated.\n      error: command '/usr/bin/x86_64-linux-gnu-gcc' failed with exit code 1\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: legacy-install-failure\n\n\u00d7 Encountered error while trying to install package.\n\u2570\u2500> jpeg2dct\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for output from the failure.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "Command '['/usr/bin/python3.11', '-m', 'pip', 'install', '--no-cache-dir', 'jpeg2dct']' returned non-zero exit status 1.",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjpeg2dct\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'jpeg2dct'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjpeg2dct\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-m\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpip\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minstall\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m--no-cache-dir\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mjpeg2dct\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjpeg2dct\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# First 12 AC zigzag positions (skip DC (0,0))\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/subprocess.py:569\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m     retcode = process.poll()\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    570\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
            "\u001b[31mCalledProcessError\u001b[39m: Command '['/usr/bin/python3.11', '-m', 'pip', 'install', '--no-cache-dir', 'jpeg2dct']' returned non-zero exit status 1."
          ]
        }
      ]
    },
    {
      "id": "e463e03c-2669-4bc2-9b57-f23f41110aee",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# SRM-lite v2: more kernels + diagonal co-occurrences (CPU) + LightGBM\n",
        "import os, time, glob, numpy as np, pandas as pd, gc\n",
        "from PIL import Image\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import lightgbm as lgb\n",
        "from scipy.signal import convolve2d\n",
        "\n",
        "def load_Y(path):\n",
        "    im = Image.open(path).convert('YCbCr')\n",
        "    y, _, _ = im.split()\n",
        "    return np.asarray(y, dtype=np.float32)\n",
        "\n",
        "# Expanded kernel set (12):\n",
        "# basic Laplacians\n",
        "K3_L4 = np.array([[0,1,0],[1,-4,1],[0,1,0]], dtype=np.float32)\n",
        "K3_L8 = np.array([[1,1,1],[1,-8,1],[1,1,1]], dtype=np.float32)\n",
        "# directional (second-derivative-like)\n",
        "K3_H2 = np.array([[-1,2,-1],[-1,2,-1],[-1,2,-1]], dtype=np.float32)\n",
        "K3_V2 = K3_H2.T.copy()\n",
        "K3_D1 = np.array([[ 2,-1, 0],[-1, 0, 1],[ 0, 1,-2]], dtype=np.float32)\n",
        "K3_D2 = np.array([[ 0,-1, 2],[-1, 0, 1],[ 2, 1, 0]], dtype=np.float32)\n",
        "# Sobel-like\n",
        "K3_SX = np.array([[ 1, 2, 1],[ 0, 0, 0],[-1,-2,-1]], dtype=np.float32)\n",
        "K3_SY = K3_SX.T.copy()\n",
        "# Prewitt-like\n",
        "K3_PX = np.array([[ 1, 1, 1],[ 0, 0, 0],[-1,-1,-1]], dtype=np.float32)\n",
        "K3_PY = K3_PX.T.copy()\n",
        "# strong 5x5\n",
        "K5_S = (1.0/12.0) * np.array([[-1, 2,-2, 2,-1],[ 2,-6, 8,-6, 2],[-2, 8,-12, 8,-2],[ 2,-6, 8,-6, 2],[-1, 2,-2, 2,-1]], dtype=np.float32)\n",
        "# small sharpening\n",
        "K3_SH = np.array([[0,-1,0],[-1,5,-1],[0,-1,0]], dtype=np.float32)\n",
        "\n",
        "SRM_KERNELS_V2 = [K3_L4, K3_L8, K3_H2, K3_V2, K3_D1, K3_D2, K3_SX, K3_SY, K3_PX, K3_PY, K5_S, K3_SH]\n",
        "\n",
        "def _quantize(res, T=2):\n",
        "    q = np.rint(res)\n",
        "    return np.clip(q, -T, T).astype(np.int8)\n",
        "\n",
        "def _hist_levels(v, T=2):\n",
        "    # bins: [-T..T] -> 2T+1\n",
        "    offset = T\n",
        "    idx = (v + offset).ravel().astype(np.int32)\n",
        "    h = np.bincount(idx, minlength=2*T+1).astype(np.float32)\n",
        "    s = h.sum();\n",
        "    return h / (s if s>0 else 1.0)\n",
        "\n",
        "def _cooc2_axis(v, T=2, axis=1):\n",
        "    Q = 2*T+1\n",
        "    off = T\n",
        "    if axis==1:\n",
        "        a = v[:, :-1].astype(np.int32) + off\n",
        "        b = v[:,  1: ].astype(np.int32) + off\n",
        "    else:\n",
        "        a = v[:-1, :].astype(np.int32) + off\n",
        "        b = v[ 1:, :].astype(np.int32) + off\n",
        "    idx = (a * Q + b).ravel()\n",
        "    h = np.bincount(idx, minlength=Q*Q).astype(np.float32)\n",
        "    s = h.sum();\n",
        "    return h / (s if s>0 else 1.0)\n",
        "\n",
        "def _cooc2_diag(v, T=2, diag_type='main'):\n",
        "    # diag_type: 'main' for i,j with (i+1,j+1); 'anti' for (i+1,j-1)\n",
        "    Q = 2*T+1; off=T\n",
        "    if diag_type=='main':\n",
        "        a = v[:-1, :-1].astype(np.int32) + off\n",
        "        b = v[ 1:,  1:].astype(np.int32) + off\n",
        "    else:\n",
        "        a = v[:-1, 1:].astype(np.int32) + off\n",
        "        b = v[ 1:, :-1].astype(np.int32) + off\n",
        "    idx = (a * Q + b).ravel()\n",
        "    h = np.bincount(idx, minlength=Q*Q).astype(np.float32)\n",
        "    s = h.sum()\n",
        "    return h / (s if s>0 else 1.0)\n",
        "\n",
        "def srm_v2_features_from_Y(y, T=2):\n",
        "    feats = []\n",
        "    for k in SRM_KERNELS_V2:\n",
        "        r = convolve2d(y, k, mode='same', boundary='symm')\n",
        "        q = _quantize(r, T=T)\n",
        "        # 1D hist\n",
        "        feats.append(_hist_levels(q, T=T))                    # (2T+1)=5\n",
        "        # 2nd-order co-occ: H, V, diag, anti-diag\n",
        "        feats.append(_cooc2_axis(q, T=T, axis=1))             # 25\n",
        "        feats.append(_cooc2_axis(q, T=T, axis=0))             # 25\n",
        "        feats.append(_cooc2_diag(q, T=T, diag_type='main'))   # 25\n",
        "        feats.append(_cooc2_diag(q, T=T, diag_type='anti'))   # 25\n",
        "    return np.concatenate(feats).astype(np.float32)  # per map: 5+25*4=105; 12 maps -> 1260 dims\n",
        "\n",
        "def srm_v2_extract_one(path):\n",
        "    try:\n",
        "        y = load_Y(path)\n",
        "        return srm_v2_features_from_Y(y, T=2)\n",
        "    except Exception:\n",
        "        return np.full(1260, np.nan, dtype=np.float32)\n",
        "\n",
        "def _paths_save(paths, fname):\n",
        "    with open(fname, 'w') as f:\n",
        "        for p in paths: f.write(str(p)+'\\n')\n",
        "def _paths_load(fname):\n",
        "    if not os.path.exists(fname): return None\n",
        "    with open(fname,'r') as f: return [line.strip() for line in f.readlines()]\n",
        "\n",
        "def _extract_in_chunks_srm_v2(paths, chunk=3000, n_jobs=36, tag='train'):\n",
        "    all_parts = []; t0=time.time()\n",
        "    for i in range(0, len(paths), chunk):\n",
        "        t1=time.time(); part_paths = paths[i:i+chunk]\n",
        "        part = Parallel(n_jobs=n_jobs, batch_size=64, prefer='threads')(delayed(srm_v2_extract_one)(p) for p in part_paths)\n",
        "        part = np.vstack(part).astype(np.float32); all_parts.append(part)\n",
        "        print(f'[{tag}-srmv2] processed {i+len(part_paths)}/{len(paths)} chunk {i//chunk+1} elapsed {time.time()-t1:.1f}s total {time.time()-t0:.1f}s', flush=True)\n",
        "    return np.vstack(all_parts) if all_parts else np.zeros((0,1260), dtype=np.float32)\n",
        "\n",
        "def build_features_srm_v2(manifest_csv='manifest.csv', subsample_n=None, seed=42, n_jobs=36, chunk=3000):\n",
        "    df = pd.read_csv(manifest_csv)\n",
        "    if subsample_n is not None and subsample_n < len(df):\n",
        "        per = max(1, subsample_n // 4)\n",
        "        dfs = [df[df['label_4c']==c].sample(n=per, random_state=seed) for c in [0,1,2,3]]\n",
        "        df = pd.concat(dfs, axis=0).sort_values('path').reset_index(drop=True)\n",
        "        cache_train = f'train_feats_srmv2_{len(df)}.npy'\n",
        "    else:\n",
        "        df = df.sort_values('path').reset_index(drop=True)\n",
        "        cache_train = 'train_feats_srmv2.npy'\n",
        "    train_paths = df['path'].tolist()\n",
        "    train_paths_txt = cache_train + '.paths.txt'\n",
        "    test_paths = sorted(glob.glob('Test/*.jpg'))\n",
        "    cache_test = 'test_feats_srmv2.npy'\n",
        "    test_paths_txt = cache_test + '.paths.txt'\n",
        "    need_train = True\n",
        "    if os.path.exists(cache_train) and os.path.exists(train_paths_txt):\n",
        "        if _paths_load(train_paths_txt) == train_paths: need_train = False\n",
        "    if need_train:\n",
        "        X = _extract_in_chunks_srm_v2(train_paths, chunk=chunk, n_jobs=n_jobs, tag='train')\n",
        "        np.save(cache_train, X); _paths_save(train_paths, train_paths_txt)\n",
        "        print('Saved', cache_train, X.shape, flush=True)\n",
        "    else:\n",
        "        X = np.load(cache_train); print('Loaded', cache_train, X.shape, flush=True)\n",
        "    need_test = True\n",
        "    if os.path.exists(cache_test) and os.path.exists(test_paths_txt):\n",
        "        if _paths_load(test_paths_txt) == test_paths: need_test = False\n",
        "    if need_test:\n",
        "        XT = _extract_in_chunks_srm_v2(test_paths, chunk=chunk, n_jobs=n_jobs, tag='test')\n",
        "        np.save(cache_test, XT); _paths_save(test_paths, test_paths_txt)\n",
        "        print('Saved', cache_test, XT.shape, flush=True)\n",
        "    else:\n",
        "        XT = np.load(cache_test); print('Loaded', cache_test, XT.shape, flush=True)\n",
        "    return df, X, test_paths, XT, cache_train, cache_test\n",
        "\n",
        "def train_lgbm_srm_v2(df, X, n_splits=3, seed=42):\n",
        "    params = dict(objective='multiclass', num_class=4, learning_rate=0.03,\n",
        "                  max_depth=8, num_leaves=128, min_data_in_leaf=100,\n",
        "                  feature_fraction=0.6, bagging_fraction=0.7, bagging_freq=1,\n",
        "                  lambda_l1=1.0, lambda_l2=1.0, n_jobs=36, verbose=-1)\n",
        "    oof = np.zeros((len(df), 4), dtype=np.float32)\n",
        "    skf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "    for fold, (tr, va) in enumerate(skf.split(X, y=df['label_4c'], groups=df['source_id'])):\n",
        "        t0=time.time()\n",
        "        dtr = lgb.Dataset(X[tr], label=df['label_4c'].values[tr])\n",
        "        dva = lgb.Dataset(X[va], label=df['label_4c'].values[va])\n",
        "        print(f'[SRMv2 Fold {fold}] train {len(tr)} val {len(va)}', flush=True)\n",
        "        bst = lgb.train(params, dtr, num_boost_round=1500, valid_sets=[dtr, dva], valid_names=['tr','va'])\n",
        "        oof[va] = bst.predict(X[va])\n",
        "        y_bin = (df.iloc[va]['label_4c'].values!=0).astype(np.int32)\n",
        "        try:\n",
        "            auc = roc_auc_score(y_bin, 1.0 - oof[va,0])\n",
        "            print(f'[SRMv2 Fold {fold}] bin AUC: {auc:.4f} | time {time.time()-t0:.1f}s', flush=True)\n",
        "        except Exception:\n",
        "            print(f'[SRMv2 Fold {fold}] AUC failed | time {time.time()-t0:.1f}s', flush=True)\n",
        "        del dtr, dva; gc.collect()\n",
        "    return oof\n",
        "\n",
        "def fit_and_predict_srm_v2(subsample_n=5000, n_splits=3, chunk=3000, n_jobs=36):\n",
        "    print('Launching SRM-lite v2 pipeline...', flush=True)\n",
        "    df, X, test_paths, XT, ctr, cte = build_features_srm_v2(subsample_n=subsample_n, chunk=chunk, n_jobs=n_jobs)\n",
        "    print('Features SRMv2:', X.shape, XT.shape, flush=True)\n",
        "    oof = train_lgbm_srm_v2(df, X, n_splits=n_splits, seed=42)\n",
        "    params = dict(objective='multiclass', num_class=4, learning_rate=0.03,\n",
        "                  max_depth=8, num_leaves=128, min_data_in_leaf=100,\n",
        "                  feature_fraction=0.6, bagging_fraction=0.7, bagging_freq=1,\n",
        "                  lambda_l1=1.0, lambda_l2=1.0, n_jobs=36, verbose=-1)\n",
        "    print('Training final SRMv2 model...', flush=True)\n",
        "    dfull = lgb.Dataset(X, label=df['label_4c'].values)\n",
        "    bst = lgb.train(params, dfull, num_boost_round=1800)\n",
        "    P = bst.predict(XT)\n",
        "    p_stego = 1.0 - P[:,0]\n",
        "    sub = pd.DataFrame({'Id': [os.path.basename(p) for p in test_paths], 'Label': p_stego}).sort_values('Id')\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv', sub.shape, flush=True)\n",
        "    return oof"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "id": "0457fa63-5013-4c7f-a7ac-7498fa776e2a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run SRM-lite v2 smoke test (5k)\n",
        "import os\n",
        "os.environ['OMP_NUM_THREADS'] = '1'\n",
        "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
        "os.environ['MKL_NUM_THREADS'] = '1'\n",
        "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
        "print('Launching fit_and_predict_srm_v2(subsample_n=5000, n_splits=3) ...', flush=True)\n",
        "oof_srm_v2 = fit_and_predict_srm_v2(subsample_n=5000, n_splits=3, chunk=3000, n_jobs=36)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching fit_and_predict_srm_v2(subsample_n=5000, n_splits=3) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching SRM-lite v2 pipeline...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train-srmv2] processed 3000/5000 chunk 1 elapsed 327.9s total 327.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train-srmv2] processed 5000/5000 chunk 2 elapsed 218.0s total 545.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved train_feats_srmv2_5000.npy (5000, 1260)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[test-srmv2] processed 3000/5000 chunk 1 elapsed 327.0s total 327.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[test-srmv2] processed 5000/5000 chunk 2 elapsed 217.2s total 544.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved test_feats_srmv2.npy (5000, 1260)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features SRMv2: (5000, 1260) (5000, 1260)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SRMv2 Fold 0] train 3332 val 1668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SRMv2 Fold 0] bin AUC: 0.5288 | time 17.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SRMv2 Fold 1] train 3332 val 1668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SRMv2 Fold 1] bin AUC: 0.5240 | time 19.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SRMv2 Fold 2] train 3336 val 1664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SRMv2 Fold 2] bin AUC: 0.5346 | time 19.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training final SRMv2 model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (5000, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "2fddbcca-b82c-4ca7-ac1b-cd2ef7c66328",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install JPEG dev libs and jpeg2dct (for DCTR-lite)\n",
        "import subprocess, sys\n",
        "print('Installing system deps...', flush=True)\n",
        "subprocess.run(['bash','-lc','apt-get update && apt-get install -y --no-install-recommends libjpeg-dev zlib1g-dev build-essential'], check=True)\n",
        "print('Installing jpeg2dct...', flush=True)\n",
        "subprocess.run([sys.executable, '-m', 'pip', 'install', '--no-cache-dir', 'jpeg2dct==0.2.4'], check=True)\n",
        "import jpeg2dct, numpy as np\n",
        "print('jpeg2dct version OK, import success.')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing system deps...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E: List directory /var/lib/apt/lists/partial is missing. - Acquire (30: Read-only file system)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "Command '['bash', '-lc', 'apt-get update && apt-get install -y --no-install-recommends libjpeg-dev zlib1g-dev build-essential']' returned non-zero exit status 100.",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msubprocess\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mInstalling system deps...\u001b[39m\u001b[33m'\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbash\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-lc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mapt-get update && apt-get install -y --no-install-recommends libjpeg-dev zlib1g-dev build-essential\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mInstalling jpeg2dct...\u001b[39m\u001b[33m'\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      6\u001b[39m subprocess.run([sys.executable, \u001b[33m'\u001b[39m\u001b[33m-m\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m--no-cache-dir\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mjpeg2dct==0.2.4\u001b[39m\u001b[33m'\u001b[39m], check=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/subprocess.py:569\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m     retcode = process.poll()\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    570\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
            "\u001b[31mCalledProcessError\u001b[39m: Command '['bash', '-lc', 'apt-get update && apt-get install -y --no-install-recommends libjpeg-dev zlib1g-dev build-essential']' returned non-zero exit status 100."
          ]
        }
      ]
    },
    {
      "id": "2bc88348-f097-447c-980f-eb68e16b5524",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Combine SRM (630) + SRM-v2 (1260) features and train LGBM (5k subset)\n",
        "import numpy as np, pandas as pd, os, time, gc\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import lightgbm as lgb\n",
        "\n",
        "subsample_n = 5000\n",
        "seed = 42\n",
        "print('Loading cached SRM/SRMv2 features for 5k...', flush=True)\n",
        "X1 = np.load('train_feats_srm_5000.npy') if os.path.exists('train_feats_srm_5000.npy') else None\n",
        "X2 = np.load('train_feats_srmv2_5000.npy') if os.path.exists('train_feats_srmv2_5000.npy') else None\n",
        "XT1 = np.load('test_feats_srm.npy') if os.path.exists('test_feats_srm.npy') else None\n",
        "XT2 = np.load('test_feats_srmv2.npy') if os.path.exists('test_feats_srmv2.npy') else None\n",
        "assert X1 is not None and X2 is not None and XT1 is not None and XT2 is not None, 'Missing cached features; run cells 11 and 14 first.'\n",
        "assert X1.shape[0] == subsample_n and X2.shape[0] == subsample_n, f'Row mismatch: {X1.shape} {X2.shape}'\n",
        "assert XT1.shape[0] == XT2.shape[0] == 5000, f'Test rows mismatch: {XT1.shape} {XT2.shape}'\n",
        "X = np.concatenate([X1, X2], axis=1).astype(np.float32)\n",
        "XT = np.concatenate([XT1, XT2], axis=1).astype(np.float32)\n",
        "print('Combined shapes:', X.shape, XT.shape, flush=True)\n",
        "\n",
        "# Rebuild the deterministic 5k df to align labels/folds\n",
        "df_all = pd.read_csv('manifest.csv')\n",
        "per = subsample_n // 4\n",
        "dfs = []\n",
        "for c in [0,1,2,3]:\n",
        "    dfs.append(df_all[df_all['label_4c']==c].sample(n=per, random_state=seed))\n",
        "df = pd.concat(dfs, axis=0).sort_values('path').reset_index(drop=True)\n",
        "print('df shape:', df.shape, '| label counts:', df['label_4c'].value_counts().to_dict(), flush=True)\n",
        "\n",
        "# CV train\n",
        "params = dict(objective='multiclass', num_class=4, learning_rate=0.03,\n",
        "              max_depth=8, num_leaves=128, min_data_in_leaf=100,\n",
        "              feature_fraction=0.6, bagging_fraction=0.7, bagging_freq=1,\n",
        "              lambda_l1=1.0, lambda_l2=1.0, n_jobs=36, verbose=-1)\n",
        "skf = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=seed)\n",
        "oof = np.zeros((len(df), 4), dtype=np.float32)\n",
        "t0 = time.time()\n",
        "for fold, (tr, va) in enumerate(skf.split(X, y=df['label_4c'], groups=df['source_id'])):\n",
        "    dtr = lgb.Dataset(X[tr], label=df['label_4c'].values[tr])\n",
        "    dva = lgb.Dataset(X[va], label=df['label_4c'].values[va])\n",
        "    print(f'[COMB Fold {fold}] train {len(tr)} val {len(va)}', flush=True)\n",
        "    bst = lgb.train(params, dtr, num_boost_round=1500, valid_sets=[dtr, dva], valid_names=['tr','va'])\n",
        "    oof[va] = bst.predict(X[va])\n",
        "    y_bin = (df.iloc[va]['label_4c'].values!=0).astype(np.int32)\n",
        "    auc = roc_auc_score(y_bin, 1.0 - oof[va,0])\n",
        "    print(f'[COMB Fold {fold}] bin AUC: {auc:.4f}', flush=True)\n",
        "print('CV done in %.1fs' % (time.time()-t0), flush=True)\n",
        "\n",
        "# Final model and submission\n",
        "print('Training final combined model...', flush=True)\n",
        "dfull = lgb.Dataset(X, label=df['label_4c'].values)\n",
        "bst = lgb.train(params, dfull, num_boost_round=1800)\n",
        "P = bst.predict(XT)\n",
        "p_stego = 1.0 - P[:,0]\n",
        "import glob\n",
        "test_paths = sorted(glob.glob('Test/*.jpg'))\n",
        "import pandas as pd, os\n",
        "sub = pd.DataFrame({'Id': [os.path.basename(p) for p in test_paths], 'Label': p_stego}).sort_values('Id')\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv', sub.shape, flush=True)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached SRM/SRMv2 features for 5k...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined shapes: (5000, 1890) (5000, 1890)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df shape: (5000, 7) | label counts: {0: 1250, 1: 1250, 2: 1250, 3: 1250}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[COMB Fold 0] train 3332 val 1668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[COMB Fold 0] bin AUC: 0.5350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[COMB Fold 1] train 3332 val 1668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[COMB Fold 1] bin AUC: 0.5259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[COMB Fold 2] train 3336 val 1664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[COMB Fold 2] bin AUC: 0.5331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV done in 76.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training final combined model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (5000, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "e71e989a-64cb-4f84-a7f9-806fa070787c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Rich SRM (per expert) + JPEG proxy (block DCT + boundaries + Qtable) features; LGBM trainer\n",
        "import os, glob, time, numpy as np, pandas as pd, gc\n",
        "from PIL import Image\n",
        "from joblib import Parallel, delayed\n",
        "from scipy.signal import convolve2d\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import lightgbm as lgb\n",
        "\n",
        "# --- Utils ---\n",
        "def load_Y(path):\n",
        "    im = Image.open(path).convert('YCbCr')\n",
        "    y, _, _ = im.split()\n",
        "    return np.asarray(y, dtype=np.float32)\n",
        "\n",
        "def zigzag_indices(n=8):\n",
        "    idx=[]\n",
        "    for s in range(2*n-1):\n",
        "        if s%2==0:\n",
        "            for i in range(s,-1,-1):\n",
        "                j=s-i\n",
        "                if i<n and j<n: idx.append((i,j))\n",
        "        else:\n",
        "            for j in range(s,-1,-1):\n",
        "                i=s-j\n",
        "                if i<n and j<n: idx.append((i,j))\n",
        "    return idx\n",
        "ZZ_FULL = zigzag_indices(8)\n",
        "\n",
        "def dct_matrix_8():\n",
        "    N=8; C=np.zeros((N,N), dtype=np.float32)\n",
        "    for k in range(N):\n",
        "        for n in range(N):\n",
        "            alpha = np.sqrt(1/N) if k==0 else np.sqrt(2/N)\n",
        "            C[k,n] = alpha * np.cos((np.pi*(2*n+1)*k)/(2*N))\n",
        "    return C\n",
        "C8 = dct_matrix_8()\n",
        "\n",
        "# --- Rich SRM subset (8 kernels), T=3, 2nd-order 4 dirs + 3rd-order (H,V,D,A) for top-4 maps ---\n",
        "K3_L4 = np.array([[0,1,0],[1,-4,1],[0,1,0]], dtype=np.float32)\n",
        "K3_L8 = np.array([[1,1,1],[1,-8,1],[1,1,1]], dtype=np.float32)\n",
        "K3_H2 = np.array([[-1,2,-1],[-1,2,-1],[-1,2,-1]], dtype=np.float32)\n",
        "K3_V2 = K3_H2.T.copy()\n",
        "K3_D1 = np.array([[ 2,-1, 0],[-1, 0, 1],[ 0, 1,-2]], dtype=np.float32)\n",
        "# Zero-sum diagonal kernel (fixed) to avoid saturation\n",
        "K3_D2 = np.array([[ 0,-1, 2],\n",
        "                  [-1, 0, 1],\n",
        "                  [-2, 1, 0]], dtype=np.float32)\n",
        "K5_S  = (1.0/12.0)*np.array([[-1,2,-2,2,-1],[2,-6,8,-6,2],[-2,8,-12,8,-2],[2,-6,8,-6,2],[-1,2,-2,2,-1]], dtype=np.float32)\n",
        "K3_2  = np.array([[-1,2,-1],[ 2,-4, 2],[-1,2,-1]], dtype=np.float32)\n",
        "SRM8 = [K3_L4, K3_L8, K3_H2, K3_V2, K3_D1, K3_D2, K5_S, K3_2]\n",
        "TOP4_IDX = {0,1,2,3}  # indices in SRM8\n",
        "# Expert-refined per-kernel scaling\n",
        "SRM_SCALE = [2,8,2,2,3,5,1,2]\n",
        "\n",
        "def _tlu_q(v, T=3):\n",
        "    q = np.rint(v)\n",
        "    return np.clip(q, -T, T).astype(np.int8)\n",
        "\n",
        "def _hist_levels(q, T=3):\n",
        "    Q = 2*T+1; off=T\n",
        "    idx = (q + off).ravel().astype(np.int32)\n",
        "    h = np.bincount(idx, minlength=Q).astype(np.float32)\n",
        "    s = h.sum();\n",
        "    return h / (s if s>0 else 1.0)\n",
        "\n",
        "def _cooc2_dir(q, T=3, mode='H'):\n",
        "    Q = 2*T+1; off=T\n",
        "    if mode=='H':\n",
        "        a = q[:, :-1].astype(np.int32)+off; b = q[:, 1:].astype(np.int32)+off\n",
        "    elif mode=='V':\n",
        "        a = q[:-1, :].astype(np.int32)+off; b = q[1:, :].astype(np.int32)+off\n",
        "    elif mode=='D':\n",
        "        a = q[:-1, :-1].astype(np.int32)+off; b = q[1:, 1:].astype(np.int32)+off\n",
        "    else:  # 'A' anti-diag\n",
        "        a = q[:-1, 1:].astype(np.int32)+off; b = q[1:, :-1].astype(np.int32)+off\n",
        "    idx = (a*Q + b).ravel()\n",
        "    h = np.bincount(idx, minlength=Q*Q).astype(np.float32)\n",
        "    s = h.sum();\n",
        "    return h / (s if s>0 else 1.0)\n",
        "\n",
        "def _cooc3_axis(q, T=3, axis='H'):\n",
        "    Q = 2*T+1; off=T\n",
        "    if axis=='H':\n",
        "        v = (q[:, :-2].astype(np.int32)+off, q[:, 1:-1].astype(np.int32)+off, q[:, 2:].astype(np.int32)+off)\n",
        "    else:\n",
        "        v = (q[:-2, :].astype(np.int32)+off, q[1:-1, :].astype(np.int32)+off, q[2:, :].astype(np.int32)+off)\n",
        "    a,b,c = v\n",
        "    idx = (a*(Q*Q) + b*Q + c).ravel()\n",
        "    h = np.bincount(idx, minlength=Q**3).astype(np.float32)\n",
        "    s = h.sum();\n",
        "    return h / (s if s>0 else 1.0)\n",
        "\n",
        "def _cooc3_diag(q, T=3, kind='D'):\n",
        "    # 3rd-order along diagonals on q in [-T..T]; returns 343 dims\n",
        "    Q = 2*T+1; off=T\n",
        "    v = q.astype(np.int32) + off\n",
        "    H, W = v.shape\n",
        "    acc = np.zeros(Q**3, dtype=np.float64); total = 0\n",
        "    if kind == 'D':\n",
        "        # main diagonals\n",
        "        for k in range(-(H-1), W):\n",
        "            d = np.diag(v, k=k)\n",
        "            if d.size >= 3:\n",
        "                a, b, c = d[:-2], d[1:-1], d[2:]\n",
        "                idx = (a*(Q*Q) + b*Q + c).ravel()\n",
        "                h = np.bincount(idx, minlength=Q**3)\n",
        "                acc += h; total += h.sum()\n",
        "    else:\n",
        "        # anti-diagonals: flip left-right then reuse main\n",
        "        vf = np.fliplr(v)\n",
        "        for k in range(-(H-1), W):\n",
        "            d = np.diag(vf, k=k)\n",
        "            if d.size >= 3:\n",
        "                a, b, c = d[:-2], d[1:-1], d[2:]\n",
        "                idx = (a*(Q*Q) + b*Q + c).ravel()\n",
        "                h = np.bincount(idx, minlength=Q**3)\n",
        "                acc += h; total += h.sum()\n",
        "    if total == 0:\n",
        "        out = np.zeros(Q**3, dtype=np.float32); out[0] = 1.0; return out\n",
        "    return (acc / total).astype(np.float32)\n",
        "\n",
        "def srm_rich_from_Y(y):\n",
        "    feats = []\n",
        "    for ki, (k, s) in enumerate(zip(SRM8, SRM_SCALE)):\n",
        "        r = convolve2d(y, k, mode='same', boundary='symm')\n",
        "        r = r / float(s)\n",
        "        q = _tlu_q(r, T=3)\n",
        "        feats.append(_hist_levels(q, T=3))             # 7\n",
        "        feats.append(_cooc2_dir(q, T=3, mode='H'))     # 49\n",
        "        feats.append(_cooc2_dir(q, T=3, mode='V'))     # 49\n",
        "        feats.append(_cooc2_dir(q, T=3, mode='D'))     # 49\n",
        "        feats.append(_cooc2_dir(q, T=3, mode='A'))     # 49\n",
        "        if ki in TOP4_IDX:\n",
        "            feats.append(_cooc3_axis(q, T=3, axis='H'))  # 343\n",
        "            feats.append(_cooc3_axis(q, T=3, axis='V'))  # 343\n",
        "            feats.append(_cooc3_diag(q, T=3, kind='D'))  # 343\n",
        "            feats.append(_cooc3_diag(q, T=3, kind='A'))  # 343\n",
        "    return np.concatenate(feats).astype(np.float32)  # 8*(7+4*49)+4*(4*343)=7112\n",
        "\n",
        "# --- JPEG proxy from decoded Y: block DCT + interblock co-occ; boundaries; qtable stats ---\n",
        "QTABLE_FALLBACKS = 0\n",
        "def get_luma_qtable(path):\n",
        "    global QTABLE_FALLBACKS\n",
        "    try:\n",
        "        im = Image.open(path)\n",
        "        qd = getattr(im, 'quantization', None)\n",
        "        if isinstance(qd, dict) and len(qd)>0:\n",
        "            key = 0 if 0 in qd else sorted(qd.keys())[0]\n",
        "            q = qd[key]\n",
        "            if isinstance(q, list) and len(q)==64:\n",
        "                t = np.zeros((8,8), dtype=np.float32)\n",
        "                for k,(i,j) in enumerate(ZZ_FULL): t[i,j] = q[k]\n",
        "                return t\n",
        "    except Exception:\n",
        "        pass\n",
        "    # Fallback to standard luminance QF75 table\n",
        "    QTABLE_FALLBACKS += 1\n",
        "    tbl = [16,11,10,16,24,40,51,61,12,12,14,19,26,58,60,55,14,13,16,24,40,57,69,56,14,17,22,29,51,87,80,62,18,22,37,56,68,109,103,77,24,35,55,64,81,104,113,92,49,64,78,87,103,121,120,101,72,92,95,98,112,100,103,99]\n",
        "    t = np.zeros((8,8), dtype=np.float32)\n",
        "    for k,(i,j) in enumerate(ZZ_FULL): t[i,j] = tbl[k]\n",
        "    return t\n",
        "\n",
        "def _ternary_sign(x, thr):\n",
        "    t = np.zeros_like(x, dtype=np.int8)\n",
        "    t[x >  thr] = 1\n",
        "    t[x < -thr] = -1\n",
        "    return t\n",
        "\n",
        "def _cooc3_1d(arr):\n",
        "    # arr in {-1,0,1}, map to {0,1,2}, 3^3=27 bins along rows\n",
        "    v = (arr + 1).astype(np.int32)\n",
        "    if v.shape[1] < 3:\n",
        "        h = np.zeros(27, dtype=np.float32); h[0]=1.0; return h\n",
        "    a = v[:, :-2]; b = v[:, 1:-1]; c = v[:, 2:]\n",
        "    idx = (a * 9 + b * 3 + c).ravel()\n",
        "    h = np.bincount(idx, minlength=27).astype(np.float32)\n",
        "    s = h.sum(); return h / (s if s>0 else 1.0)\n",
        "\n",
        "def _cooc3_1d_cols(arr):\n",
        "    # along columns -> operate on transposed to reuse 1D rows logic\n",
        "    return _cooc3_1d(arr.T)\n",
        "\n",
        "def _cooc3_diag_main(arr):\n",
        "    # along main diagonal sequences (top-left to bottom-right)\n",
        "    H, W = arr.shape\n",
        "    if H < 3 or W < 3:\n",
        "        h = np.zeros(27, dtype=np.float32); h[0]=1.0; return h\n",
        "    feats = []\n",
        "    for off in range(-(H-1), W):\n",
        "        diag = np.diag(arr, k=off)\n",
        "        if diag.size >= 3:\n",
        "            feats.append(_cooc3_1d(diag[None, :]))\n",
        "    if len(feats)==0:\n",
        "        h = np.zeros(27, dtype=np.float32); h[0]=1.0; return h\n",
        "    return np.mean(np.stack(feats, axis=0), axis=0)\n",
        "\n",
        "def _cooc3_diag_anti(arr):\n",
        "    # along anti-diagonal sequences (top-right to bottom-left)\n",
        "    return _cooc3_diag_main(np.fliplr(arr))\n",
        "\n",
        "def block_dct_qproxy_feats(y, tY, ac_count=29):\n",
        "    # Includes DC band (zz[0]) + first 29 AC -> 30 bands total\n",
        "    H8,W8 = y.shape\n",
        "    Yb = y.reshape(H8//8,8,W8//8,8).transpose(0,2,1,3).reshape(-1,8,8)\n",
        "    # JPEG level shift\n",
        "    Yb = Yb - 128.0\n",
        "    tmp = np.einsum('ab,nbc->nac', C8, Yb, optimize=True)\n",
        "    D = np.einsum('nab,cb->nac', tmp, C8, optimize=True)  # (nb,8,8)\n",
        "    h8=H8//8; w8=W8//8\n",
        "    D = D.reshape(h8,w8,8,8)\n",
        "    # Quantize by luminance table\n",
        "    Q = np.rint(D / (tY + 1e-9)).astype(np.int32)\n",
        "    feats = []\n",
        "    bands = ZZ_FULL[0:1+ac_count]  # include DC\n",
        "    for (i,j) in bands:\n",
        "        S = Q[..., i,j]\n",
        "        # Inter-block differences\n",
        "        Dx = S[:,1:] - S[:,:-1] if S.shape[1] > 1 else np.zeros_like(S[:, :0])\n",
        "        Dy = S[1:,:] - S[:-1,:] if S.shape[0] > 1 else np.zeros_like(S[:0, :])\n",
        "        # Branch A: dead-zone |>1|\n",
        "        tx1 = _ternary_sign(Dx, thr=1); ty1 = _ternary_sign(Dy, thr=1)\n",
        "        hx1 = _cooc3_1d(tx1); hy1 = _cooc3_1d_cols(ty1)\n",
        "        hd1 = _cooc3_diag_main(tx1)  # approximate diagonals on H diffs\n",
        "        ha1 = _cooc3_diag_anti(tx1)\n",
        "        feats.extend([hx1, hy1, hd1, ha1])\n",
        "        # Branch B: no-deadzone (>0)\n",
        "        tx0 = _ternary_sign(Dx, thr=0); ty0 = _ternary_sign(Dy, thr=0)\n",
        "        hx0 = _cooc3_1d(tx0); hy0 = _cooc3_1d_cols(ty0)\n",
        "        hd0 = _cooc3_diag_main(tx0)\n",
        "        ha0 = _cooc3_diag_anti(tx0)\n",
        "        feats.extend([hx0, hy0, hd0, ha0])\n",
        "    return np.concatenate(feats).astype(np.float32)  # 30 bands * (2 branches * 4 dirs * 27) = 6480\n",
        "\n",
        "def boundary_feats(y):\n",
        "    H8,W8 = y.shape\n",
        "    def hist_and_cooc_1d(arr, clip=5, bins=11):\n",
        "        if arr.size==0:\n",
        "            h = np.zeros(bins, dtype=np.float32); h[0]=1.0\n",
        "            c = np.zeros(bins*bins, dtype=np.float32); c[0]=1.0\n",
        "            return np.concatenate([h,c])\n",
        "        v = np.clip(np.rint(arr).astype(np.int32), -clip, clip)\n",
        "        h = np.bincount(v+clip, minlength=bins).astype(np.float32);\n",
        "        h /= (h.sum() if h.sum()>0 else 1.0)\n",
        "        if v.size>1:\n",
        "            a = v[:-1]+clip; b = v[1:]+clip; idx=(a*bins+b).ravel()\n",
        "            c = np.bincount(idx, minlength=bins*bins).astype(np.float32);\n",
        "            c /= (c.sum() if c.sum()>0 else 1.0)\n",
        "        else:\n",
        "            c = np.zeros(bins*bins, dtype=np.float32); c[0]=1.0\n",
        "        return np.concatenate([h,c])\n",
        "    # Per-line vertical boundaries\n",
        "    v_lines = []\n",
        "    for k in range(7, W8-1, 8):\n",
        "        diff = y[:,k] - y[:,k+1]\n",
        "        v_lines.append(hist_and_cooc_1d(diff))\n",
        "    if len(v_lines)==0:\n",
        "        fv = np.concatenate([np.eye(1,11,0,dtype=np.float32).sum(0), np.eye(1,121,0,dtype=np.float32).sum(0)])\n",
        "    else:\n",
        "        fv = np.mean(np.stack(v_lines, axis=0), axis=0)\n",
        "    # Per-line horizontal boundaries\n",
        "    h_lines = []\n",
        "    for k in range(7, H8-1, 8):\n",
        "        diff = y[k,:] - y[k+1,:]\n",
        "        h_lines.append(hist_and_cooc_1d(diff))\n",
        "    if len(h_lines)==0:\n",
        "        fh = np.concatenate([np.eye(1,11,0,dtype=np.float32).sum(0), np.eye(1,121,0,dtype=np.float32).sum(0)])\n",
        "    else:\n",
        "        fh = np.mean(np.stack(h_lines, axis=0), axis=0)\n",
        "    return np.concatenate([fv, fh]).astype(np.float32)  # 2*(11+121)=264\n",
        "\n",
        "def qtable_stats(tY):\n",
        "    vals = tY.ravel().astype(np.float32)\n",
        "    feats = [vals.min(), vals.max(), vals.mean(), np.median(vals), vals.std(), np.log1p(vals).sum()]\n",
        "    m = vals.mean()\n",
        "    bucket = [1.0 if m<=5 else 0.0, 1.0 if (m>5 and m<=10) else 0.0, 1.0 if m>10 else 0.0]\n",
        "    return np.array(feats + bucket, dtype=np.float32)  # 9\n",
        "\n",
        "# Updated lengths after adding SRM 3rd-order diagonals for TOP4\n",
        "SRM_LEN = 7112\n",
        "JPEG_LEN = 6480\n",
        "BOUND_LEN = 264\n",
        "QT_LEN = 9\n",
        "TOTAL_LEN = SRM_LEN + JPEG_LEN + BOUND_LEN + QT_LEN  # 13865\n",
        "\n",
        "def rich_features_one(path):\n",
        "    try:\n",
        "        y = load_Y(path)\n",
        "        # Align all branches to same 8x8 grid\n",
        "        H,W = y.shape; H8,W8 = (H//8)*8, (W//8)*8; y = y[:H8,:W8]\n",
        "        # Rich SRM\n",
        "        f_srm = srm_rich_from_Y(y)\n",
        "        # JPEG proxy\n",
        "        tY = get_luma_qtable(path)\n",
        "        f_dct = block_dct_qproxy_feats(y, tY, ac_count=29)\n",
        "        f_bnd = boundary_feats(y)\n",
        "        f_qt  = qtable_stats(tY)\n",
        "        return np.concatenate([f_srm, f_dct, f_bnd, f_qt]).astype(np.float32)\n",
        "    except Exception:\n",
        "        return np.full(TOTAL_LEN, np.nan, dtype=np.float32)\n",
        "\n",
        "def _paths_save(paths, fname):\n",
        "    with open(fname,'w') as f:\n",
        "        for p in paths: f.write(str(p)+'\\n')\n",
        "def _paths_load(fname):\n",
        "    if not os.path.exists(fname): return None\n",
        "    with open(fname,'r') as f: return [line.strip() for line in f.readlines()]\n",
        "\n",
        "def build_features_rich(manifest_csv='manifest.csv', subsample_n=5000, seed=42, n_jobs=36, chunk=1000, compute_test=True):\n",
        "    df_all = pd.read_csv(manifest_csv)\n",
        "    per = subsample_n//4\n",
        "    df = pd.concat([df_all[df_all['label_4c']==c].sample(n=per, random_state=seed) for c in [0,1,2,3]], axis=0)\n",
        "    df = df.sort_values('path').reset_index(drop=True)\n",
        "    train_paths = df['path'].tolist()\n",
        "    cache_train = f'train_feats_rich_{len(df)}.npy'\n",
        "    train_paths_txt = cache_train + '.paths.txt'\n",
        "    test_paths = sorted(glob.glob('Test/*.jpg'))\n",
        "    cache_test = 'test_feats_rich.npy'\n",
        "    test_paths_txt = cache_test + '.paths.txt'\n",
        "    # Train cache\n",
        "    need_train = True\n",
        "    if os.path.exists(cache_train) and os.path.exists(train_paths_txt):\n",
        "        if _paths_load(train_paths_txt) == train_paths: need_train=False\n",
        "    if need_train:\n",
        "        all_parts=[]; t0=time.time()\n",
        "        for i in range(0, len(train_paths), chunk):\n",
        "            t1=time.time(); part=train_paths[i:i+chunk]\n",
        "            feats = Parallel(n_jobs=n_jobs, batch_size=32, prefer='threads')(delayed(rich_features_one)(p) for p in part)\n",
        "            feats = np.vstack(feats).astype(np.float32); all_parts.append(feats)\n",
        "            print(f'[train-rich] {i+len(part)}/{len(train_paths)} elapsed {time.time()-t1:.1f}s total {time.time()-t0:.1f}s', flush=True)\n",
        "        X = np.vstack(all_parts) if all_parts else np.zeros((0,TOTAL_LEN), dtype=np.float32)\n",
        "        np.save(cache_train, X); _paths_save(train_paths, train_paths_txt); print('Saved', cache_train, X.shape, flush=True)\n",
        "    else:\n",
        "        X = np.load(cache_train); print('Loaded', cache_train, X.shape, flush=True)\n",
        "    # Test cache (optional)\n",
        "    XT = None\n",
        "    if compute_test:\n",
        "        need_test = True\n",
        "        if os.path.exists(cache_test) and os.path.exists(test_paths_txt):\n",
        "            if _paths_load(test_paths_txt) == test_paths: need_test=False\n",
        "        if need_test:\n",
        "            all_parts=[]; t0=time.time()\n",
        "            for i in range(0, len(test_paths), chunk):\n",
        "                t1=time.time(); part=test_paths[i:i+chunk]\n",
        "                feats = Parallel(n_jobs=n_jobs, batch_size=32, prefer='threads')(delayed(rich_features_one)(p) for p in part)\n",
        "                feats = np.vstack(feats).astype(np.float32); all_parts.append(feats)\n",
        "                print(f'[test-rich]  {i+len(part)}/{len(test_paths)} elapsed {time.time()-t1:.1f}s total {time.time()-t0:.1f}s', flush=True)\n",
        "            XT = np.vstack(all_parts) if all_parts else np.zeros((0,TOTAL_LEN), dtype=np.float32)\n",
        "            np.save(cache_test, XT); _paths_save(test_paths, test_paths_txt); print('Saved', cache_test, XT.shape, flush=True)\n",
        "        else:\n",
        "            XT = np.load(cache_test); print('Loaded', cache_test, XT.shape, flush=True)\n",
        "    print('QTable fallbacks so far:', QTABLE_FALLBACKS, flush=True)\n",
        "    return df, X, test_paths if compute_test else [], XT\n",
        "\n",
        "def train_lgbm_rich(df, X, n_splits=3, seed=42):\n",
        "    params = dict(objective='multiclass', num_class=4, learning_rate=0.02,\n",
        "                  max_depth=-1, num_leaves=256, min_data_in_leaf=50,\n",
        "                  feature_fraction=0.45, bagging_fraction=0.75, bagging_freq=1,\n",
        "                  lambda_l1=1.5, lambda_l2=1.5, n_jobs=36, verbose=-1)\n",
        "    oof = np.zeros((len(df),4), dtype=np.float32)\n",
        "    skf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "    for fold, (tr, va) in enumerate(skf.split(X, y=df['label_4c'], groups=df['source_id'])):\n",
        "        t0=time.time()\n",
        "        dtr = lgb.Dataset(X[tr], label=df['label_4c'].values[tr])\n",
        "        dva = lgb.Dataset(X[va], label=df['label_4c'].values[va])\n",
        "        print(f'[RICH Fold {fold}] train {len(tr)} val {len(va)}', flush=True)\n",
        "        bst = lgb.train(params, dtr, num_boost_round=2600, valid_sets=[dtr, dva], valid_names=['tr','va'])\n",
        "        oof[va] = bst.predict(X[va])\n",
        "        yb = (df.iloc[va]['label_4c'].values!=0).astype(np.int32)\n",
        "        try:\n",
        "            auc = roc_auc_score(yb, 1.0 - oof[va,0]);\n",
        "            print(f'[RICH Fold {fold}] bin AUC: {auc:.4f} | time {time.time()-t0:.1f}s', flush=True)\n",
        "        except Exception:\n",
        "            print(f'[RICH Fold {fold}] AUC failed | time {time.time()-t0:.1f}s', flush=True)\n",
        "        del dtr, dva; gc.collect()\n",
        "    return oof\n",
        "\n",
        "def fit_and_predict_rich(subsample_n=5000, n_splits=3, n_jobs=36, chunk=1000, compute_test=True):\n",
        "    print('Launching RICH features pipeline...', flush=True)\n",
        "    df, X, test_paths, XT = build_features_rich(subsample_n=subsample_n, n_jobs=n_jobs, chunk=chunk, compute_test=compute_test)\n",
        "    print('Features RICH:', X.shape, (None if XT is None else XT.shape), flush=True)\n",
        "    oof = train_lgbm_rich(df, X, n_splits=n_splits, seed=42)\n",
        "    if compute_test:\n",
        "        # Final model\n",
        "        params = dict(objective='multiclass', num_class=4, learning_rate=0.02,\n",
        "                      max_depth=-1, num_leaves=256, min_data_in_leaf=50,\n",
        "                      feature_fraction=0.45, bagging_fraction=0.75, bagging_freq=1,\n",
        "                      lambda_l1=1.5, lambda_l2=1.5, n_jobs=36, verbose=-1)\n",
        "        print('Training final RICH model...', flush=True)\n",
        "        dfull = lgb.Dataset(X, label=df['label_4c'].values)\n",
        "        bst = lgb.train(params, dfull, num_boost_round=2600)\n",
        "        P = bst.predict(XT); p_stego = 1.0 - P[:,0]\n",
        "        sub = pd.DataFrame({'Id':[os.path.basename(p) for p in test_paths], 'Label':p_stego}).sort_values('Id')\n",
        "        sub.to_csv('submission.csv', index=False); print('Saved submission.csv', sub.shape, flush=True)\n",
        "    return oof\n",
        "\n",
        "print('Rich SRM + JPEG proxy cell ready. Call fit_and_predict_rich(subsample_n=5000, n_splits=3).', flush=True)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rich SRM + JPEG proxy cell ready. Call fit_and_predict_rich(subsample_n=5000, n_splits=3).\n"
          ]
        }
      ]
    },
    {
      "id": "261ff04d-a4f1-4098-98d5-09bd85b43138",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Patch: fix K3_D2 to zero-sum to prevent saturation; rebuild SRM8\n",
        "import numpy as np\n",
        "\n",
        "# Zero-sum diagonal kernel variant (corrected bottom-left to -2)\n",
        "K3_D2 = np.array([[ 0,-1, 2],\n",
        "                  [-1, 0, 1],\n",
        "                  [-2, 1, 0]], dtype=np.float32)\n",
        "\n",
        "# Rebuild SRM8 with corrected K3_D2\n",
        "SRM8 = [K3_L4, K3_L8, K3_H2, K3_V2, K3_D1, K3_D2, K5_S, K3_2]\n",
        "\n",
        "print('Patched K3_D2 sum =', float(K3_D2.sum()))\n",
        "print('SRM8[5] updated; SRM_SCALE =', SRM_SCALE)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patched K3_D2 sum = 0.0\nSRM8[5] updated; SRM_SCALE = [2, 6, 2, 2, 4, 4, 1, 2]\n"
          ]
        }
      ]
    },
    {
      "id": "b971ce2a-8f07-4f81-9d57-928a7d5cc5e6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run Rich features smoke test (5k, 3 folds)\n",
        "import os, time\n",
        "os.environ['OMP_NUM_THREADS'] = '1'\n",
        "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
        "os.environ['MKL_NUM_THREADS'] = '1'\n",
        "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
        "sub_n = 5000; folds = 3; jobs = 36; chunk = 300\n",
        "print(f'Launching fit_and_predict_rich(subsample_n={sub_n}, n_splits={folds}, n_jobs={jobs}, chunk={chunk}, compute_test=False) ...', flush=True)\n",
        "t0=time.time()\n",
        "oof_rich = fit_and_predict_rich(subsample_n=sub_n, n_splits=folds, n_jobs=jobs, chunk=chunk, compute_test=False)\n",
        "print('Rich smoke test done in %.1fs' % (time.time()-t0), flush=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching fit_and_predict_rich(subsample_n=5000, n_splits=3, n_jobs=36, chunk=300, compute_test=False) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching RICH features pipeline...\n"
          ]
        }
      ]
    },
    {
      "id": "7cd1adc8-2889-4833-b717-13d0ec59fc2c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Diagnostics for RICH features (run after 5k smoke or standalone on small subset)\n",
        "import numpy as np, pandas as pd, glob, os, time\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "def rich_feature_batch(paths, n_jobs=36):\n",
        "    from joblib import Parallel, delayed\n",
        "    return np.vstack(Parallel(n_jobs=n_jobs, batch_size=32, prefer='threads')(delayed(rich_features_one)(p) for p in paths)).astype(np.float32)\n",
        "\n",
        "def single_pair_diff_probe():\n",
        "    # pick a source present in all four classes\n",
        "    ids = set(os.path.splitext(os.path.basename(p))[0] for p in glob.glob('Cover/*.jpg'))\n",
        "    for cls in ['JMiPOD','JUNIWARD','UERD']:\n",
        "        ids = ids.intersection(set(os.path.splitext(os.path.basename(p))[0] for p in glob.glob(f'{cls}/*.jpg')))\n",
        "    sid = sorted(list(ids))[0]\n",
        "    paths = {c: f\"{c}/{sid}.jpg\" for c in ['Cover','JMiPOD','JUNIWARD','UERD']}\n",
        "    feats = {k: rich_features_one(v) for k,v in paths.items()}\n",
        "    cover = feats['Cover']\n",
        "    parts = {'SRM':(0,4368), 'JPEG':(4368,4368+1080), 'BOUND':(4368+1080, 4368+1080+264), 'QTABLE':(4368+1080+264, 4368+1080+264+9)}\n",
        "    print('Probe source_id:', sid)\n",
        "    for k in ['JMiPOD','JUNIWARD','UERD']:\n",
        "        d = feats[k] - cover\n",
        "        print(f'-- {k} -- L1:{np.sum(np.abs(d)):.2f} max|d|:{np.max(np.abs(d)):.2f} nz>{(np.abs(d)>1e-6).sum()}')\n",
        "        for name,(a,b) in parts.items():\n",
        "            dd = np.abs(d[a:b]); m = dd.mean();\n",
        "            top = np.argsort(-dd)[:10] + a\n",
        "            print(f'   {name}: mean|d|={m:.6f} top10_idx={top.tolist()}')\n",
        "\n",
        "def per_feature_signal_probe(cache='train_feats_rich_5000.npy', manifest_csv='manifest.csv', subsample_n=5000, seed=42):\n",
        "    if os.path.exists(cache):\n",
        "        X = np.load(cache);\n",
        "        df_all = pd.read_csv(manifest_csv)\n",
        "        per = subsample_n//4\n",
        "        df = pd.concat([df_all[df_all.label_4c==c].sample(n=per, random_state=seed) for c in [0,1,2,3]]).sort_values('path').reset_index(drop=True)\n",
        "    else:\n",
        "        print('Cache not found; building small subset (200 per class)...')\n",
        "        df_all = pd.read_csv(manifest_csv)\n",
        "        per = 200\n",
        "        df = pd.concat([df_all[df_all.label_4c==c].sample(n=per, random_state=seed) for c in [0,1,2,3]]).sort_values('path').reset_index(drop=True)\n",
        "        X = rich_feature_batch(df.path.tolist())\n",
        "    y_bin = (df.label_4c.values!=0).astype(np.int32)\n",
        "    Xc = X[y_bin==0]; Xs = X[y_bin==1]\n",
        "    ks_stats = []; mad = []\n",
        "    for j in range(X.shape[1]):\n",
        "        try:\n",
        "            ks = ks_2samp(Xc[:,j], Xs[:,j]).statistic\n",
        "        except Exception:\n",
        "            ks = 0.0\n",
        "        ks_stats.append(ks);\n",
        "        mad.append(float(np.abs(Xs[:,j].mean() - Xc[:,j].mean())))\n",
        "    ks_stats = np.array(ks_stats); mad = np.array(mad)\n",
        "    def topk(arr, k=20):\n",
        "        idx = np.argsort(-arr)[:k];\n",
        "        return list(zip(idx.tolist(), arr[idx].round(6).tolist()))\n",
        "    print('Top20 KS:', topk(ks_stats))\n",
        "    print('Top20 |mean diff|:', topk(mad))\n",
        "    # block ownership\n",
        "    blocks = np.array(['SRM']*4368 + ['JPEG']*1080 + ['BOUND']*264 + ['QTABLE']*9)\n",
        "    for name in ['SRM','JPEG','BOUND','QTABLE']:\n",
        "        mks = ks_stats[blocks==name].mean(); mmad = mad[blocks==name].mean()\n",
        "        print(f'{name} mean KS={mks:.6f} mean |mean diff|={mmad:.6f}')\n",
        "\n",
        "def shuffle_label_check(cache='train_feats_rich_5000.npy', manifest_csv='manifest.csv', subsample_n=5000, seed=42):\n",
        "    import lightgbm as lgb\n",
        "    from sklearn.model_selection import StratifiedGroupKFold\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    if os.path.exists(cache):\n",
        "        X = np.load(cache)\n",
        "        df_all = pd.read_csv(manifest_csv)\n",
        "        per = subsample_n//4\n",
        "        df = pd.concat([df_all[df_all.label_4c==c].sample(n=per, random_state=seed) for c in [0,1,2,3]]).sort_values('path').reset_index(drop=True)\n",
        "    else:\n",
        "        print('Cache not found; building small subset (200 per class)...')\n",
        "        df_all = pd.read_csv(manifest_csv)\n",
        "        per = 200\n",
        "        df = pd.concat([df_all[df_all.label_4c==c].sample(n=per, random_state=seed) for c in [0,1,2,3]]).sort_values('path').reset_index(drop=True)\n",
        "        X = rich_feature_batch(df.path.tolist())\n",
        "    y4 = df.label_4c.values.copy()\n",
        "    grp = df.source_id.values\n",
        "    # true labels AUC\n",
        "    skf = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    oof = np.zeros((len(df),4), np.float32)\n",
        "    params = dict(objective='multiclass', num_class=4, learning_rate=0.03, max_depth=8, num_leaves=128, min_data_in_leaf=50, feature_fraction=0.55, bagging_fraction=0.75, bagging_freq=1, lambda_l1=1.5, lambda_l2=1.5, n_jobs=36, verbose=-1)\n",
        "    for f,(tr,va) in enumerate(skf.split(X, y=y4, groups=grp)):\n",
        "        dtr=lgb.Dataset(X[tr], label=y4[tr]); dva=lgb.Dataset(X[va], label=y4[va])\n",
        "        bst=lgb.train(params, dtr, num_boost_round=600)\n",
        "        oof[va]=bst.predict(X[va])\n",
        "    auc_true = roc_auc_score((y4!=0).astype(int), 1.0 - oof[:,0])\n",
        "    # shuffled labels AUC\n",
        "    rng = np.random.RandomState(123); y4_shuf = y4.copy(); rng.shuffle(y4_shuf)\n",
        "    oof_s = np.zeros_like(oof)\n",
        "    for f,(tr,va) in enumerate(skf.split(X, y=y4_shuf, groups=grp)):\n",
        "        dtr=lgb.Dataset(X[tr], label=y4_shuf[tr]); dva=lgb.Dataset(X[va], label=y4_shuf[va])\n",
        "        bst=lgb.train(params, dtr, num_boost_round=600)\n",
        "        oof_s[va]=bst.predict(X[va])\n",
        "    auc_shuf = roc_auc_score((y4_shuf!=0).astype(int), 1.0 - oof_s[:,0])\n",
        "    print(f'Shuffle check: true AUC={auc_true:.4f} | shuffled AUC={auc_shuf:.4f}')\n",
        "\n",
        "print('Diagnostics ready: single_pair_diff_probe(); per_feature_signal_probe(); shuffle_label_check()')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diagnostics ready: single_pair_diff_probe(); per_feature_signal_probe(); shuffle_label_check()\n"
          ]
        }
      ]
    },
    {
      "id": "16bf534c-1952-42d2-ae65-c24fde158f6a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run RICH diagnostics (expects train_feats_rich_5000.npy cache present)\n",
        "import os\n",
        "os.environ['OMP_NUM_THREADS'] = '1'\n",
        "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
        "os.environ['MKL_NUM_THREADS'] = '1'\n",
        "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
        "print('=== Single cover-stego pair diff probe ===', flush=True)\n",
        "single_pair_diff_probe()\n",
        "print('\\n=== Per-feature signal probe (KS, |mean diff|) ===', flush=True)\n",
        "per_feature_signal_probe(cache='train_feats_rich_5000.npy', manifest_csv='manifest.csv', subsample_n=5000, seed=42)\n",
        "print('\\n=== Shuffle-label check ===', flush=True)\n",
        "shuffle_label_check(cache='train_feats_rich_5000.npy', manifest_csv='manifest.csv', subsample_n=5000, seed=42)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Single cover-stego pair diff probe ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probe source_id: 00001\n-- JMiPOD -- L1:1.46 max|d|:0.01 nz>4933\n   SRM: mean|d|=0.000094 top10_idx=[3965, 3968, 3962, 4011, 3966, 4024, 111, 4164, 4060, 4091]\n   JPEG: mean|d|=0.000869 top10_idx=[5434, 5353, 4894, 4867, 5056, 5164, 4766, 4892, 4858, 4921]\n   BOUND: mean|d|=0.000413 top10_idx=[5580, 5458, 5585, 5457, 5589, 5588, 5700, 5587, 5591, 5449]\n   QTABLE: mean|d|=0.000000 top10_idx=[5712, 5713, 5714, 5715, 5716, 5717, 5718, 5719, 5720]\n-- JUNIWARD -- L1:1.72 max|d|:0.01 nz>4992\n   SRM: mean|d|=0.000101 top10_idx=[3962, 3968, 3966, 3965, 4164, 4060, 4099, 4011, 4024, 3975]\n   JPEG: mean|d|=0.001053 top10_idx=[5083, 5407, 5353, 4900, 5319, 5297, 4894, 5110, 5333, 4867]\n   BOUND: mean|d|=0.000535 top10_idx=[5580, 5458, 5457, 5591, 5585, 5448, 5586, 5579, 5455, 5587]\n   QTABLE: mean|d|=0.000000 top10_idx=[5712, 5713, 5714, 5715, 5716, 5717, 5718, 5719, 5720]\n-- UERD -- L1:2.39 max|d|:0.02 nz>5007\n   SRM: mean|d|=0.000228 top10_idx=[3968, 2765, 3965, 2729, 3962, 2814, 2778, 2863, 2827, 1791]\n   JPEG: mean|d|=0.001158 top10_idx=[5407, 5110, 5353, 5164, 5434, 4867, 4678, 5083, 4894, 4651]\n   BOUND: mean|d|=0.000534 top10_idx=[5458, 5448, 5580, 5590, 5453, 5469, 5601, 5457, 5519, 5701]\n   QTABLE: mean|d|=0.000000 top10_idx=[5712, 5713, 5714, 5715, 5716, 5717, 5718, 5719, 5720]\n\n=== Per-feature signal probe (KS, |mean diff|) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top20 KS: [(2525, 0.0344), (4308, 0.029067), (4319, 0.028267), (2572, 0.027467), (3128, 0.026133), (4270, 0.025867), (3127, 0.025333), (2540, 0.025333), (2475, 0.025067), (2467, 0.025067), (4340, 0.024533), (4302, 0.024533), (4122, 0.024267), (4158, 0.024), (2624, 0.024), (2571, 0.024), (5500, 0.023733), (2409, 0.023733), (5499, 0.023733), (2912, 0.0232)]\nTop20 |mean diff|: [(5407, 0.003737), (5434, 0.003558), (5110, 0.002681), (5380, 0.002678), (5353, 0.002649), (5164, 0.002599), (5083, 0.002551), (5137, 0.002548), (5458, 0.00248), (5448, 0.002468), (4867, 0.002417), (5580, 0.0024), (5590, 0.002376), (5299, 0.002333), (5326, 0.002322), (5029, 0.002318), (3962, 0.002254), (3968, 0.002244), (4678, 0.002242), (5002, 0.002228)]\nSRM mean KS=0.011115 mean |mean diff|=0.000049\nJPEG mean KS=0.008387 mean |mean diff|=0.000148\nBOUND mean KS=0.015695 mean |mean diff|=0.000156\nQTABLE mean KS=0.000000 mean |mean diff|=0.000000\n\n=== Shuffle-label check ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shuffle check: true AUC=0.5293 | shuffled AUC=0.4995\n"
          ]
        }
      ]
    },
    {
      "id": "93327a23-9e7e-4a16-b2f0-0e7e61766ffa",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Clear RICH caches to recompute with fixes\n",
        "import os\n",
        "targets = [\n",
        "    'train_feats_rich_5000.npy',\n",
        "    'train_feats_rich_5000.npy.paths.txt',\n",
        "    'test_feats_rich.npy',\n",
        "    'test_feats_rich.npy.paths.txt'\n",
        "]\n",
        "for t in targets:\n",
        "    if os.path.exists(t):\n",
        "        try:\n",
        "            os.remove(t); print('Deleted', t, flush=True)\n",
        "        except Exception as e:\n",
        "            print('Failed to delete', t, e, flush=True)\n",
        "    else:\n",
        "        print('Not found (ok):', t, flush=True)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted train_feats_rich_5000.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted train_feats_rich_5000.npy.paths.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not found (ok): test_feats_rich.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not found (ok): test_feats_rich.npy.paths.txt\n"
          ]
        }
      ]
    },
    {
      "id": "42ff715a-5870-4544-b24c-72e7023d03f5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Updated diagnostics for RICH v2 dims (6261) + SRM bin-use check\n",
        "import numpy as np, pandas as pd, glob, os\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "SRM_LEN = 4368\n",
        "JPEG_LEN = 1620\n",
        "BOUND_LEN = 264\n",
        "QT_LEN = 9\n",
        "TOTAL_LEN = SRM_LEN + JPEG_LEN + BOUND_LEN + QT_LEN\n",
        "\n",
        "def single_pair_diff_probe_v2():\n",
        "    ids = set(os.path.splitext(os.path.basename(p))[0] for p in glob.glob('Cover/*.jpg'))\n",
        "    for cls in ['JMiPOD','JUNIWARD','UERD']:\n",
        "        ids &= set(os.path.splitext(os.path.basename(p))[0] for p in glob.glob(f'{cls}/*.jpg'))\n",
        "    sid = sorted(list(ids))[0]\n",
        "    paths = {c: f\"{c}/{sid}.jpg\" for c in ['Cover','JMiPOD','JUNIWARD','UERD']}\n",
        "    feats = {k: rich_features_one(v) for k,v in paths.items()}\n",
        "    cover = feats['Cover']\n",
        "    parts = {\n",
        "        'SRM': (0, SRM_LEN),\n",
        "        'JPEG': (SRM_LEN, SRM_LEN+JPEG_LEN),\n",
        "        'BOUND': (SRM_LEN+JPEG_LEN, SRM_LEN+JPEG_LEN+BOUND_LEN),\n",
        "        'QTABLE': (SRM_LEN+JPEG_LEN+BOUND_LEN, TOTAL_LEN)\n",
        "    }\n",
        "    print('Probe source_id:', sid)\n",
        "    for k in ['JMiPOD','JUNIWARD','UERD']:\n",
        "        d = feats[k] - cover\n",
        "        print(f'-- {k} -- L1:{np.sum(np.abs(d)):.4f} max|d|:{np.max(np.abs(d)):.4f} nz>{(np.abs(d)>1e-6).sum()}')\n",
        "        for name,(a,b) in parts.items():\n",
        "            dd = np.abs(d[a:b]);\n",
        "            m = float(dd.mean());\n",
        "            top = np.argsort(-dd)[:10] + a\n",
        "            print(f'   {name}: mean|d|={m:.6f} top10_idx={top.tolist()}')\n",
        "\n",
        "def per_feature_signal_probe_v2(cache='train_feats_rich_5000.npy', manifest_csv='manifest.csv', subsample_n=5000, seed=42):\n",
        "    if os.path.exists(cache):\n",
        "        X = np.load(cache)\n",
        "        assert X.shape[1] == TOTAL_LEN, f'Unexpected feature dim: {X.shape}'\n",
        "        df_all = pd.read_csv(manifest_csv)\n",
        "        per = subsample_n//4\n",
        "        df = pd.concat([df_all[df_all.label_4c==c].sample(n=per, random_state=seed) for c in [0,1,2,3]]).sort_values('path').reset_index(drop=True)\n",
        "    else:\n",
        "        print('Cache not found; exiting.')\n",
        "        return\n",
        "    y_bin = (df.label_4c.values!=0).astype(np.int32)\n",
        "    Xc = X[y_bin==0]; Xs = X[y_bin==1]\n",
        "    ks_stats = np.zeros(X.shape[1], dtype=np.float32)\n",
        "    mad = np.zeros(X.shape[1], dtype=np.float32)\n",
        "    for j in range(X.shape[1]):\n",
        "        try:\n",
        "            ks_stats[j] = ks_2samp(Xc[:,j], Xs[:,j]).statistic\n",
        "        except Exception:\n",
        "            ks_stats[j] = 0.0\n",
        "        mad[j] = float(abs(Xs[:,j].mean() - Xc[:,j].mean()))\n",
        "    def topk(arr, k=20):\n",
        "        idx = np.argsort(-arr)[:k]\n",
        "        return list(zip(idx.tolist(), np.round(arr[idx],6).tolist()))\n",
        "    print('Top20 KS:', topk(ks_stats))\n",
        "    print('Top20 |mean diff|:', topk(mad))\n",
        "    blocks = np.array(['SRM']*SRM_LEN + ['JPEG']*JPEG_LEN + ['BOUND']*BOUND_LEN + ['QTABLE']*QT_LEN)\n",
        "    for name in ['SRM','JPEG','BOUND','QTABLE']:\n",
        "        mks = ks_stats[blocks==name].mean(); mmad = mad[blocks==name].mean()\n",
        "        print(f'{name} mean KS={mks:.6f} mean |mean diff|={mmad:.6f}')\n",
        "\n",
        "def srm_bin_use_check(sample_paths=None, max_show=1):\n",
        "    # Inspect q-bin usage for key kernels to tune SRM_SCALE quickly\n",
        "    if sample_paths is None:\n",
        "        df = pd.read_csv('manifest.csv')\n",
        "        # choose first few cover images for stability\n",
        "        sample_paths = df[df.label_4c==0].sort_values('path').path.tolist()[:max_show]\n",
        "    for p in sample_paths:\n",
        "        y = load_Y(p); H,W=y.shape; y = y[:(H//8)*8, :(W//8)*8]\n",
        "        print('Path:', p)\n",
        "        for ki,(k,s) in enumerate(zip(SRM8, SRM_SCALE)):\n",
        "            r = convolve2d(y, k, mode='same', boundary='symm')/float(s)\n",
        "            q = np.clip(np.rint(r), -3, 3).astype(np.int8)\n",
        "            vals, cnts = np.unique(q, return_counts=True)\n",
        "            frac = (cnts / cnts.sum()).round(4)\n",
        "            print(f'  Kernel {ki} scale {s}: bins {vals.tolist()} frac {frac.tolist()}')\n",
        "        break\n",
        "\n",
        "print('Updated diagnostics ready: single_pair_diff_probe_v2(); per_feature_signal_probe_v2(); srm_bin_use_check()')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated diagnostics ready: single_pair_diff_probe_v2(); per_feature_signal_probe_v2(); srm_bin_use_check()\n"
          ]
        }
      ]
    },
    {
      "id": "320af775-8c17-4bb4-af7d-5393a5f090a8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Execute updated diagnostics for RICH v2\n",
        "import os\n",
        "os.environ['OMP_NUM_THREADS'] = '1'\n",
        "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
        "os.environ['MKL_NUM_THREADS'] = '1'\n",
        "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
        "print('=== SRM bin-use check (1 cover) ===', flush=True)\n",
        "srm_bin_use_check(max_show=1)\n",
        "print('\\n=== Per-feature signal probe v2 (KS, |mean diff|) ===', flush=True)\n",
        "per_feature_signal_probe_v2(cache='train_feats_rich_5000.npy', manifest_csv='manifest.csv', subsample_n=5000, seed=42)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== SRM bin-use check (1 cover) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path: Cover/00001.jpg\n  Kernel 0 scale 2: bins [-3, -2, -1, 0, 1, 2, 3] frac [0.299, 0.1093, 0.0402, 0.1232, 0.0387, 0.1022, 0.2874]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Kernel 1 scale 6: bins [-3, -2, -1, 0, 1, 2, 3] frac [0.2859, 0.094, 0.0766, 0.1096, 0.0732, 0.0875, 0.2731]\n  Kernel 2 scale 2: bins [-3, -2, -1, 0, 1, 2, 3] frac [0.3425, 0.0726, 0.0259, 0.0822, 0.0272, 0.0769, 0.3727]\n  Kernel 3 scale 2: bins [-3, -2, -1, 0, 1, 2, 3] frac [0.3276, 0.0836, 0.0304, 0.0937, 0.031, 0.0871, 0.3466]\n  Kernel 4 scale 4: bins [-3, -2, -1, 0, 1, 2, 3] frac [0.2536, 0.097, 0.0739, 0.1355, 0.0739, 0.0995, 0.2666]\n  Kernel 5 scale 4: bins [-3, -2, -1, 0, 1, 2, 3] frac [0.3757, 0.0533, 0.0346, 0.0597, 0.0352, 0.055, 0.3866]\n  Kernel 6 scale 1: bins [-3, -2, -1, 0, 1, 2, 3] frac [0.0858, 0.0981, 0.1858, 0.2415, 0.1941, 0.1043, 0.0904]\n  Kernel 7 scale 2: bins [-3, -2, -1, 0, 1, 2, 3] frac [0.1897, 0.1431, 0.0641, 0.2076, 0.0632, 0.1443, 0.188]\n\n=== Per-feature signal probe v2 (KS, |mean diff|) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top20 KS: [(3170, 0.041600000113248825), (2912, 0.04106700047850609), (2366, 0.03946699947118759), (2624, 0.03840000182390213), (2961, 0.031199999153614044), (3169, 0.02826699987053871), (2575, 0.02826699987053871), (2623, 0.02800000086426735), (3121, 0.027732999995350838), (2415, 0.02693299949169159), (2359, 0.026667000725865364), (3177, 0.026667000725865364), (3010, 0.026399999856948853), (6040, 0.026399999856948853), (2554, 0.026399999856948853), (2574, 0.02613300085067749), (2424, 0.025867000222206116), (3084, 0.025333000347018242), (2367, 0.025333000347018242), (2905, 0.025066999718546867)]\nTop20 |mean diff|: [(5866, 0.0037640000227838755), (5407, 0.003737000050023198), (5839, 0.0036410000175237656), (5434, 0.00355900009162724), (5461, 0.0032679999712854624), (5488, 0.0029430000577121973), (5785, 0.002747999969869852), (5812, 0.0027340000960975885), (5110, 0.002718999981880188), (5380, 0.00267699989490211), (5353, 0.0026489999145269394), (5164, 0.002597999991849065), (5083, 0.002561999950557947), (5137, 0.0025480000767856836), (5998, 0.0024800000246614218), (4867, 0.0024759999942034483), (5988, 0.0024679999332875013), (5542, 0.0024319998919963837), (6120, 0.002400000113993883), (6130, 0.0023759999312460423)]\nSRM mean KS=0.011123 mean |mean diff|=0.000069\nJPEG mean KS=0.008189 mean |mean diff|=0.000155\nBOUND mean KS=0.016157 mean |mean diff|=0.000156\nQTABLE mean KS=0.000000 mean |mean diff|=0.000000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}