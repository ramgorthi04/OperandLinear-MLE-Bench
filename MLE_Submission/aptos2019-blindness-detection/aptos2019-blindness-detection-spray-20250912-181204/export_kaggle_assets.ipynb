{
  "cells": [
    {
      "id": "65981622-18d6-443e-8145-87a25faedf78",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Export assets for Kaggle GPU pivot: package cache768 and small metadata bundle\n",
        "import os, sys, tarfile, hashlib, math, time, shutil, json\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path('.').resolve()\n",
        "CACHE_DIR = BASE / 'cache768'\n",
        "OUT_DIR = BASE / 'kaggle_export'\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def dir_size_bytes(path: Path) -> int:\n",
        "    total = 0\n",
        "    for p in path.rglob('*'):\n",
        "        if p.is_file():\n",
        "            try:\n",
        "                total += p.stat().st_size\n",
        "            except Exception:\n",
        "                pass\n",
        "    return total\n",
        "\n",
        "def human(n):\n",
        "    for u in ['B','KB','MB','GB','TB']:\n",
        "        if n < 1024 or u=='TB':\n",
        "            return f\"{n:.2f} {u}\"\n",
        "        n /= 1024\n",
        "\n",
        "def sha256_file(p: Path) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with p.open('rb') as f:\n",
        "        for chunk in iter(lambda: f.read(1024*1024), b''):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "print('== Kaggle Export Start ==')\n",
        "print('CWD:', BASE)\n",
        "assert CACHE_DIR.exists(), f'Missing {CACHE_DIR}'\n",
        "\n",
        "# 1) Report sizes\n",
        "size_cache = dir_size_bytes(CACHE_DIR)\n",
        "print('cache768 size:', human(size_cache))\n",
        "\n",
        "# 2) Create a tar (uncompressed) to split safely in Kaggle UI\n",
        "tar_path = OUT_DIR / 'cache768.tar'\n",
        "if not tar_path.exists():\n",
        "    t0 = time.time()\n",
        "    with tarfile.open(tar_path, mode='w') as tar:\n",
        "        tar.add(CACHE_DIR, arcname='cache768')\n",
        "    print('Created', tar_path, 'size:', human(tar_path.stat().st_size), 'in', f'{time.time()-t0:.1f}s')\n",
        "else:\n",
        "    print('Exists:', tar_path, 'size:', human(tar_path.stat().st_size))\n",
        "\n",
        "# 3) Split into ~1.9GB parts\n",
        "PART_BYTES = 1_900_000_000  # ~1.9 GB per part\n",
        "parts = []\n",
        "with tar_path.open('rb') as fin:\n",
        "    idx = 0\n",
        "    while True:\n",
        "        chunk = fin.read(PART_BYTES)\n",
        "        if not chunk:\n",
        "            break\n",
        "        part_path = OUT_DIR / f'cache768.tar.part{idx:02d}'\n",
        "        with part_path.open('wb') as fout:\n",
        "            fout.write(chunk)\n",
        "        parts.append(part_path)\n",
        "        print('Wrote', part_path, 'size:', human(part_path.stat().st_size))\n",
        "        idx += 1\n",
        "\n",
        "# Optional: remove original tar to save space after splitting\n",
        "try:\n",
        "    tar_bytes = tar_path.stat().st_size\n",
        "    if len(parts) >= 1 and all(p.exists() for p in parts):\n",
        "        tar_path.unlink(missing_ok=True)\n",
        "        print('Removed original tar to save space (', human(tar_bytes), ')')\n",
        "except Exception as e:\n",
        "    print('Could not remove tar:', e)\n",
        "\n",
        "# 4) Compute checksums manifest\n",
        "manifest = {\n",
        "    'created_at': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'parts': []\n",
        "}\n",
        "for p in parts:\n",
        "    h = sha256_file(p)\n",
        "    manifest['parts'].append({'file': p.name, 'bytes': p.stat().st_size, 'sha256': h})\n",
        "man_path = OUT_DIR / 'cache768_parts_manifest.json'\n",
        "man_path.write_text(json.dumps(manifest, indent=2))\n",
        "print('Wrote manifest:', man_path)\n",
        "\n",
        "# 5) Small metadata bundle (upload as a separate small dataset or alongside parts)\n",
        "small_files = [\n",
        "    'folds.csv',\n",
        "    'next24h_plan.ipynb',\n",
        "    'competition_best_practices.md',\n",
        "    'requirements.txt',\n",
        "    'kaggle_train_tfefnv2l_768.ipynb',\n",
        "    'kaggle_gpu_pivot_checklist.ipynb'\n",
        "]\n",
        "SMALL_OUT = OUT_DIR / 'aptos_kaggle_small_bundle'\n",
        "SMALL_OUT.mkdir(parents=True, exist_ok=True)\n",
        "readme = (\n",
        "    'README.txt',\n",
        "    'Kaggle GPU Pivot Instructions\\n'\n",
        "    '- Upload all cache768.tar.part** files together as one Kaggle Dataset (aptos-cache768).\\n'\n",
        "    '- In Kaggle Notebook, reassemble: cat cache768.tar.part* > cache768.tar; then: tar -xf cache768.tar -C /kaggle/working\\n'\n",
        "    '- CACHE_DIR = \"/kaggle/working/cache768\"\\n'\n",
        "    '- Add competition data as input; ensure torch.cuda.is_available() is True.\\n'\n",
        "    '- Open kaggle_train_tfefnv2l_768.ipynb and run: run_all_folds(); then build submissions (CDF5 and thresholds).\\n'\n",
        "    '- See kaggle_gpu_pivot_checklist.ipynb for step-by-step run/submit sequence.\\n'\n",
        ")\n",
        "Path(SMALL_OUT / readme[0]).write_text(readme[1])\n",
        "for fn in small_files:\n",
        "    p = BASE / fn\n",
        "    if p.exists():\n",
        "        shutil.copy2(p, SMALL_OUT / p.name)\n",
        "        print('Added to small bundle:', p.name)\n",
        "    else:\n",
        "        print('Missing (skipped):', p)\n",
        "\n",
        "# Zip small bundle\n",
        "small_zip = shutil.make_archive(str(OUT_DIR / 'aptos_kaggle_small_bundle'), 'zip', root_dir=SMALL_OUT)\n",
        "print('Wrote small bundle zip:', small_zip)\n",
        "\n",
        "print('== Export complete ==')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Kaggle Export Start ==\nCWD: /app/agent_run_states/aptos2019-blindness-detection-spray-20250912-181204\ncache768 size: 4.01 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created /app/agent_run_states/aptos2019-blindness-detection-spray-20250912-181204/kaggle_export/cache768.tar size: 4.02 GB in 4.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote /app/agent_run_states/aptos2019-blindness-detection-spray-20250912-181204/kaggle_export/cache768.tar.part00 size: 1.77 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote /app/agent_run_states/aptos2019-blindness-detection-spray-20250912-181204/kaggle_export/cache768.tar.part01 size: 1.77 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote /app/agent_run_states/aptos2019-blindness-detection-spray-20250912-181204/kaggle_export/cache768.tar.part02 size: 492.07 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed original tar to save space ( 4.02 GB )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote manifest: /app/agent_run_states/aptos2019-blindness-detection-spray-20250912-181204/kaggle_export/cache768_parts_manifest.json\nAdded to small bundle: folds.csv\nAdded to small bundle: next24h_plan.ipynb\nMissing (skipped): /app/agent_run_states/aptos2019-blindness-detection-spray-20250912-181204/competition_best_practices.md\nAdded to small bundle: requirements.txt\nAdded to small bundle: kaggle_train_tfefnv2l_768.ipynb\nAdded to small bundle: kaggle_gpu_pivot_checklist.ipynb\nWrote small bundle zip: /app/agent_run_states/aptos2019-blindness-detection-spray-20250912-181204/kaggle_export/aptos_kaggle_small_bundle.zip\n== Export complete ==\n"
          ]
        }
      ]
    },
    {
      "id": "e6d2d1db-b27d-45db-bb9e-c0a4017c5ba4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sync latest Kaggle notebooks into small bundle and rebuild zip\n",
        "import os, shutil, zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "root = Path('.').resolve()\n",
        "bundle_dir = root / 'kaggle_export' / 'aptos_kaggle_small_bundle'\n",
        "bundle_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "src_train_nb = root / 'kaggle_train_tfefnv2l_768.ipynb'\n",
        "src_check_nb = root / 'kaggle_gpu_pivot_checklist.ipynb'\n",
        "src_folds    = root / 'folds.csv'\n",
        "\n",
        "assert src_train_nb.exists(), f'Missing {src_train_nb}'\n",
        "assert src_check_nb.exists(), f'Missing {src_check_nb}'\n",
        "assert src_folds.exists(), f'Missing {src_folds}'\n",
        "\n",
        "# Copy updated notebooks and folds.csv into bundle\n",
        "shutil.copy2(src_train_nb, bundle_dir / src_train_nb.name)\n",
        "shutil.copy2(src_check_nb, bundle_dir / src_check_nb.name)\n",
        "shutil.copy2(src_folds,    bundle_dir / src_folds.name)\n",
        "print('Synced files into bundle:', [p.name for p in [src_train_nb, src_check_nb, src_folds]])\n",
        "\n",
        "# Rebuild zip\n",
        "zip_path = root / 'kaggle_export' / 'aptos_kaggle_small_bundle.zip'\n",
        "if zip_path.exists():\n",
        "    zip_path.unlink()\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'w', compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "    for p in ['README.txt', 'folds.csv', 'kaggle_train_tfefnv2l_768.ipynb', 'kaggle_gpu_pivot_checklist.ipynb', 'requirements.txt']:\n",
        "        fp = bundle_dir / p\n",
        "        if not fp.exists():\n",
        "            print('Warning: missing in bundle:', fp)\n",
        "            continue\n",
        "        zf.write(fp, arcname=f'aptos_kaggle_small_bundle/{p}')\n",
        "print('Rebuilt:', zip_path, 'size:', round(zip_path.stat().st_size/1024**2, 2), 'MB')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synced files into bundle: ['kaggle_train_tfefnv2l_768.ipynb', 'kaggle_gpu_pivot_checklist.ipynb', 'folds.csv']\nRebuilt: /app/agent_run_states/aptos2019-blindness-detection-spray-20250912-181204/kaggle_export/aptos_kaggle_small_bundle.zip size: 0.05 MB\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}