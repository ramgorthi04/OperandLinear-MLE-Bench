{
  "cells": [
    {
      "id": "02b0dc8e-0d97-45c6-b61a-72c4c0146ff8",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan to Medal: OpenVaccine (stanford-covid-vaccine)\n",
        "\n",
        "Objectives:\n",
        "- Establish GPU availability and stable environment\n",
        "- Build a robust CV mirroring test conditions\n",
        "- Ship a fast baseline ASAP, then iterate with feature engineering and stronger models\n",
        "- Use expert reviews at milestones\n",
        "\n",
        "Initial Steps (Milestone 1):\n",
        "1) Environment check:\n",
        "- Verify GPU via nvidia-smi; if unavailable, exit competition\n",
        "- Install CUDA 12.1-compatible PyTorch only if/when needed\n",
        "\n",
        "2) Data audit:\n",
        "- Load train.json/test.json; inspect schema, sizes, and sample rows\n",
        "- Inspect sample_submission.csv to confirm expected output format and row count\n",
        "\n",
        "3) Validation plan:\n",
        "- Reproduce competition split logic (sequence-level, per-base targets)\n",
        "- Deterministic KFold/GroupKFold as appropriate; ensure transforms fit inside folds\n",
        "\n",
        "4) Baseline model (Milestone 2):\n",
        "- Start with a simple per-base model (e.g., token features + lightweight model) to verify pipeline\n",
        "- Generate submission.csv; validate format and sanity-check predictions\n",
        "\n",
        "5) Feature engineering (Milestone 3):\n",
        "- Encodings: sequence tokens, base pairing structure, loop types if provided\n",
        "- Positional features: index, length-normalized, GC-content windows\n",
        "- Optionally augment with predicted structure if available/feasible\n",
        "\n",
        "6) Stronger models (Milestone 4):\n",
        "- Sequence models: BiGRU/LSTM or lightweight Transformer\n",
        "- Tune with early stopping; log progress per fold\n",
        "- Cache OOF/test predictions for blends\n",
        "\n",
        "7) Ensembling (Milestone 5):\n",
        "- Blend diverse seeds/architectures; weighted average on OOF\n",
        "\n",
        "Checkpoints for Expert Review:\n",
        "- After this plan\n",
        "- After data audit + CV finalized\n",
        "- After baseline submission\n",
        "- After FE + stronger models\n",
        "- Before final blend\n",
        "\n",
        "Risk controls:\n",
        "- Subsample for smoke tests\n",
        "- Print elapsed time per fold; interrupt long/stuck jobs\n",
        "- Verify submission file before scoring"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "b5c90184-35fa-4347-bc63-5b175edf7890",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment check and quick data audit\n",
        "import os, sys, json, time, subprocess, math, statistics as stats\n",
        "import pandas as pd\n",
        "\n",
        "print('=== GPU CHECK: nvidia-smi ===', flush=True)\n",
        "try:\n",
        "    out = subprocess.run(['bash','-lc','nvidia-smi || true'], capture_output=True, text=True)\n",
        "    print(out.stdout)\n",
        "except Exception as e:\n",
        "    print('nvidia-smi failed:', e)\n",
        "\n",
        "print('=== Files present ===', flush=True)\n",
        "print(os.listdir('.'))\n",
        "\n",
        "print('=== sample_submission.csv audit ===')\n",
        "sub = pd.read_csv('sample_submission.csv')\n",
        "print('sample_submission shape:', sub.shape)\n",
        "print('submission columns:', list(sub.columns))\n",
        "print(sub.head(3))\n",
        "\n",
        "def read_json_robust(path):\n",
        "    # Many Kaggle datasets here are JSON lines; try lines=True first\n",
        "    try:\n",
        "        df = pd.read_json(path, lines=True)\n",
        "        return df\n",
        "    except Exception as e1:\n",
        "        print(f'lines=True failed for {path}:', e1)\n",
        "        try:\n",
        "            df = pd.read_json(path)\n",
        "            return df\n",
        "        except Exception as e2:\n",
        "            print(f'standard read_json failed for {path}:', e2)\n",
        "            raise\n",
        "\n",
        "print('=== train.json audit ===')\n",
        "train = read_json_robust('train.json')\n",
        "print('train rows (sequences):', len(train))\n",
        "print('train columns:', list(train.columns))\n",
        "print(train.head(1).T.head(30))\n",
        "\n",
        "# Infer key fields commonly present\n",
        "expected_cols = ['id','sequence','structure','predicted_loop_type','signal_to_noise','SN_filter','seq_scored']\n",
        "present = [c for c in expected_cols if c in train.columns]\n",
        "print('present expected columns:', present)\n",
        "\n",
        "# Derive lengths and scored lengths if possible\n",
        "def safe_len(x):\n",
        "    try:\n",
        "        return len(x) if isinstance(x, (list, str)) else int(x) if pd.notnull(x) else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "lengths = train['sequence'].map(safe_len) if 'sequence' in train.columns else pd.Series([None]*len(train))\n",
        "if 'seq_scored' in train.columns:\n",
        "    scored = train['seq_scored']\n",
        "else:\n",
        "    target_like = [c for c in train.columns if c.startswith('react') or c.startswith('deg') or c.endswith('_error')]\n",
        "    scored = train[target_like[0]].map(safe_len) if target_like else pd.Series([None]*len(train))\n",
        "print('length value counts (top):')\n",
        "print(lengths.value_counts().head(10))\n",
        "print('seq_scored value counts (top):')\n",
        "print(scored.value_counts().head(10))\n",
        "\n",
        "print('=== test.json audit ===')\n",
        "test = read_json_robust('test.json')\n",
        "print('test rows (sequences):', len(test))\n",
        "print('test columns:', list(test.columns))\n",
        "print(test.head(1).T.head(30))\n",
        "\n",
        "# Confirm target columns from sample_submission\n",
        "target_cols = [c for c in sub.columns if c != 'id_seqpos']\n",
        "print('Targets per base (from submission):', target_cols, ' | count:', len(target_cols))\n",
        "\n",
        "print('=== QUICK SANITY: id/seqpos mapping example ===')\n",
        "print(sub.head(5)['id_seqpos'])\n",
        "\n",
        "print('AUDIT COMPLETE')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== GPU CHECK: nvidia-smi ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Sep 25 01:27:18 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.06             Driver Version: 550.144.06     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10-24Q                 On  |   00000002:00:00.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |     182MiB /  24512MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\n=== Files present ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sample_submission.csv', '.00_eda_and_planning_kernel_state.json', 'docker_run.log', 'requirements.txt', 'submission.csv', 'train.json', 'agent_metadata', 'task.txt', 'test.json', '00_eda_and_planning.ipynb', 'description.md']\n=== sample_submission.csv audit ===\nsample_submission shape: (25680, 6)\nsubmission columns: ['id_seqpos', 'reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n        id_seqpos  reactivity  deg_Mg_pH10  deg_pH10  deg_Mg_50C  deg_50C\n0  id_00b436dec_0         0.0          0.0       0.0         0.0      0.0\n1  id_00b436dec_1         0.0          0.0       0.0         0.0      0.0\n2  id_00b436dec_2         0.0          0.0       0.0         0.0      0.0\n=== train.json audit ===\ntrain rows (sequences): 2160\ntrain columns: ['index', 'id', 'sequence', 'structure', 'predicted_loop_type', 'signal_to_noise', 'SN_filter', 'seq_length', 'seq_scored', 'reactivity_error', 'deg_error_Mg_pH10', 'deg_error_pH10', 'deg_error_Mg_50C', 'deg_error_50C', 'reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n                                                                     0\nindex                                                                0\nid                                                        id_001f94081\nsequence             GGAAAAGCUCUAAUAACAGGAGACUAGGACUACGUAUUUCUAGGUA...\nstructure            .....((((((.......)))).)).((.....((..((((((......\npredicted_loop_type  EEEEESSSSSSHHHHHHHSSSSBSSXSSIIIIISSIISSSSSSHHH...\nsignal_to_noise                                                  6.894\nSN_filter                                                            1\nseq_length                                                         107\nseq_scored                                                          68\nreactivity_error     [0.1359, 0.20700000000000002, 0.1633, 0.1452, ...\ndeg_error_Mg_pH10    [0.26130000000000003, 0.38420000000000004, 0.1...\ndeg_error_pH10       [0.2631, 0.28600000000000003, 0.0964, 0.1574, ...\ndeg_error_Mg_50C     [0.1501, 0.275, 0.0947, 0.18660000000000002, 0...\ndeg_error_50C        [0.2167, 0.34750000000000003, 0.188, 0.2124, 0...\nreactivity           [0.3297, 1.5693000000000001, 1.1227, 0.8686, 0...\ndeg_Mg_pH10          [0.7556, 2.983, 0.2526, 1.3789, 0.637600000000...\ndeg_pH10             [2.3375, 3.5060000000000002, 0.3008, 1.0108, 0...\ndeg_Mg_50C           [0.35810000000000003, 2.9683, 0.2589, 1.4552, ...\ndeg_50C              [0.6382, 3.4773, 0.9988, 1.3228, 0.78770000000...\npresent expected columns: ['id', 'sequence', 'structure', 'predicted_loop_type', 'signal_to_noise', 'SN_filter', 'seq_scored']\nlength value counts (top):\nsequence\n107    2160\nName: count, dtype: int64\nseq_scored value counts (top):\nseq_scored\n68    2160\nName: count, dtype: int64\n=== test.json audit ===\ntest rows (sequences): 240\ntest columns: ['index', 'id', 'sequence', 'structure', 'predicted_loop_type', 'seq_length', 'seq_scored']\n                                                                     0\nindex                                                                0\nid                                                        id_00b436dec\nsequence             GGAAAUCAUCGAGGACGGGUCCGUUCAGCACGCGAAAGCGUCGUGA...\nstructure            .....(((((((((((..(((((((((..((((....))))..)))...\npredicted_loop_type  EEEEESSSSSSSSSSSIISSSSSSSSSIISSSSHHHHSSSSIISSS...\nseq_length                                                         107\nseq_scored                                                          68\nTargets per base (from submission): ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']  | count: 5\n=== QUICK SANITY: id/seqpos mapping example ===\n0    id_00b436dec_0\n1    id_00b436dec_1\n2    id_00b436dec_2\n3    id_00b436dec_3\n4    id_00b436dec_4\nName: id_seqpos, dtype: object\nAUDIT COMPLETE\n"
          ]
        }
      ]
    },
    {
      "id": "d1f1dd55-0625-44cf-8bef-413b789b57e0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install CUDA 12.1 PyTorch stack and verify GPU\n",
        "import os, sys, subprocess, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "def pip(*args):\n",
        "    print('> pip', *args, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "# Uninstall any preinstalled torch variants to avoid conflicts\n",
        "for pkg in ('torch','torchvision','torchaudio'):\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', pkg], check=False)\n",
        "\n",
        "# Clean stray site dirs that might shadow correct wheels\n",
        "for d in (\n",
        "    '/app/.pip-target/torch',\n",
        "    '/app/.pip-target/torchvision',\n",
        "    '/app/.pip-target/torchaudio',\n",
        "    '/app/.pip-target/torchgen',\n",
        "    '/app/.pip-target/functorch',\n",
        "):\n",
        "    if os.path.exists(d):\n",
        "        print('Removing', d)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "# Install exact cu121 torch stack\n",
        "pip('install',\n",
        "    '--index-url', 'https://download.pytorch.org/whl/cu121',\n",
        "    '--extra-index-url', 'https://pypi.org/simple',\n",
        "    'torch==2.4.1', 'torchvision==0.19.1', 'torchaudio==2.4.1')\n",
        "\n",
        "# Freeze versions for later installs\n",
        "Path('constraints.txt').write_text('torch==2.4.1\\ntorchvision==0.19.1\\ntorchaudio==2.4.1\\n')\n",
        "\n",
        "import torch\n",
        "print('torch:', torch.__version__, 'built CUDA:', getattr(torch.version, 'cuda', None))\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "assert str(getattr(torch.version,'cuda','')).startswith('12.1'), f'Wrong CUDA build: {torch.version.cuda}'\n",
        "assert torch.cuda.is_available(), 'CUDA not available'\n",
        "print('GPU:', torch.cuda.get_device_name(0))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torch as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torchvision as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> pip install --index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://pypi.org/simple torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torchaudio as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121, https://pypi.org/simple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (799.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 MB 390.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.19.1\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.1/7.1 MB 220.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.4/3.4 MB 316.8 MB/s eta 0:00:00\nCollecting filelock\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\nCollecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 85.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 418.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fsspec\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 513.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 508.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 226.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 173.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 223.5 MB/s eta 0:00:00\nCollecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 47.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 349.0 MB/s eta 0:00:00\nCollecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 477.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 51.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing-extensions>=4.8.0\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 420.9 MB/s eta 0:00:00\nCollecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 354.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 205.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 220.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 501.6 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 106.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow!=8.3.*,>=5.3.0\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 172.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 483.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 195.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 488.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.2 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pillow-11.3.0 sympy-1.14.0 torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121 triton-3.0.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.4.1+cu121 built CUDA: 12.1\nCUDA available: True\nGPU: NVIDIA A10-24Q\n"
          ]
        }
      ]
    },
    {
      "id": "33649adf-03cf-4904-b84f-7cedd83275a1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Data pipeline: encoding, pairing parse, Dataset/Collate\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "SEQ_VOCAB = {'A':0,'C':1,'G':2,'U':3}\n",
        "STR_VOCAB = {'.':0,'(':1,')':2}\n",
        "LOOP_VOCAB = {'B':0,'E':1,'H':2,'I':3,'M':4,'S':5,'X':6}\n",
        "\n",
        "def encode_string(s, vocab, unk_val=0):\n",
        "    return np.array([vocab.get(ch, unk_val) for ch in s], dtype=np.int64)\n",
        "\n",
        "def parse_dot_bracket(struct):\n",
        "    stack = []\n",
        "    pair_idx = np.full(len(struct), -1, dtype=np.int32)\n",
        "    for i, ch in enumerate(struct):\n",
        "        if ch == '(':\n",
        "            stack.append(i)\n",
        "        elif ch == ')':\n",
        "            if not stack:\n",
        "                continue\n",
        "            j = stack.pop()\n",
        "            pair_idx[i] = j\n",
        "            pair_idx[j] = i\n",
        "    is_paired = (pair_idx != -1).astype(np.int8)\n",
        "    pair_dist = np.where(pair_idx!=-1, np.abs(np.arange(len(struct)) - pair_idx), 0).astype(np.int16)\n",
        "    return pair_idx, is_paired, pair_dist\n",
        "\n",
        "TARGET_COLS = ['reactivity','deg_Mg_pH10','deg_pH10','deg_Mg_50C','deg_50C']\n",
        "\n",
        "class RNADataset(Dataset):\n",
        "    def __init__(self, df, train_mode=True):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.train_mode = train_mode\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        seq = row['sequence']\n",
        "        struct = row['structure']\n",
        "        loop = row['predicted_loop_type']\n",
        "        L = len(seq)\n",
        "        seq_ids = encode_string(seq, SEQ_VOCAB)\n",
        "        str_ids = encode_string(struct, STR_VOCAB)\n",
        "        loop_ids = encode_string(loop, LOOP_VOCAB)\n",
        "        pair_idx, is_paired, pair_dist = parse_dot_bracket(struct)\n",
        "        pos_idx = np.arange(L, dtype=np.int32)\n",
        "        pos_norm = pos_idx / max(L-1, 1)\n",
        "        snr = float(row.get('signal_to_noise', 1.0))\n",
        "        snr_feat = np.full(L, snr, dtype=np.float32)\n",
        "        # features per position\n",
        "        feats = np.stack([\n",
        "            pos_idx.astype(np.float32),\n",
        "            pos_norm.astype(np.float32),\n",
        "            is_paired.astype(np.float32),\n",
        "            pair_dist.astype(np.float32),\n",
        "            snr_feat,\n",
        "        ], axis=1)  # [L, F]\n",
        "        seq_scored = int(row['seq_scored']) if 'seq_scored' in row else L\n",
        "        mask_scored = np.zeros(L, dtype=np.float32)\n",
        "        mask_scored[:seq_scored] = 1.0\n",
        "        item = {\n",
        "            'seq_ids': torch.from_numpy(seq_ids),\n",
        "            'str_ids': torch.from_numpy(str_ids),\n",
        "            'loop_ids': torch.from_numpy(loop_ids),\n",
        "            'feats': torch.from_numpy(feats),\n",
        "            'mask_scored': torch.from_numpy(mask_scored),\n",
        "            'id': row['id'],\n",
        "        }\n",
        "        if self.train_mode:\n",
        "            # Targets are length seq_scored (68). Pad to full sequence length L.\n",
        "            T = len(TARGET_COLS)\n",
        "            tar = np.zeros((L, T), dtype=np.float32)\n",
        "            for ti, c in enumerate(TARGET_COLS):\n",
        "                arr = np.array(row[c], dtype=np.float32)\n",
        "                take = min(len(arr), L)\n",
        "                tar[:take, ti] = arr[:take]\n",
        "            item['targets'] = torch.from_numpy(tar)\n",
        "        return item\n",
        "\n",
        "def collate_pad(batch):\n",
        "    # pad to max len in batch; guard against any per-item length mismatch by slicing to min length\n",
        "    lens_seq = [len(b['seq_ids']) for b in batch]\n",
        "    maxL = max(lens_seq)\n",
        "    T = len(TARGET_COLS)\n",
        "    B = len(batch)\n",
        "    seq_ids = torch.full((B, maxL), 0, dtype=torch.long)\n",
        "    str_ids = torch.full((B, maxL), 0, dtype=torch.long)\n",
        "    loop_ids = torch.full((B, maxL), 0, dtype=torch.long)\n",
        "    feats = torch.zeros((B, maxL, 5), dtype=torch.float32)\n",
        "    mask_pad = torch.zeros((B, maxL), dtype=torch.float32)\n",
        "    mask_scored = torch.zeros((B, maxL), dtype=torch.float32)\n",
        "    ids = []\n",
        "    targets = None\n",
        "    has_targets = 'targets' in batch[0]\n",
        "    if has_targets:\n",
        "        targets = torch.zeros((B, maxL, T), dtype=torch.float32)\n",
        "    for i, b in enumerate(batch):\n",
        "        Ls = len(b['seq_ids'])\n",
        "        Lt = b['targets'].shape[0] if has_targets else Ls\n",
        "        L = min(Ls, Lt)\n",
        "        seq_ids[i,:L] = b['seq_ids'][:L]\n",
        "        str_ids[i,:L] = b['str_ids'][:L]\n",
        "        loop_ids[i,:L] = b['loop_ids'][:L]\n",
        "        feats[i,:L] = b['feats'][:L]\n",
        "        mask_pad[i,:L] = 1.0\n",
        "        mask_scored[i,:L] = b['mask_scored'][:L]\n",
        "        ids.append(b['id'])\n",
        "        if has_targets:\n",
        "            targets[i,:L] = b['targets'][:L]\n",
        "    out = {'seq_ids':seq_ids, 'str_ids':str_ids, 'loop_ids':loop_ids, 'feats':feats,\n",
        "           'mask_pad':mask_pad, 'mask_scored':mask_scored, 'ids':ids}\n",
        "    if has_targets:\n",
        "        out['targets'] = targets\n",
        "    return out\n",
        "\n",
        "print('Dataset utilities defined.')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset utilities defined.\n"
          ]
        }
      ]
    },
    {
      "id": "6c54bbaf-7a58-4c73-9014-8f9b9ae55ce4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# BiGRU baseline: model, CV training (3-fold smoke), inference, submission\n",
        "import math, time, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "TARGET_COLS = ['reactivity','deg_Mg_pH10','deg_pH10','deg_Mg_50C','deg_50C']\n",
        "\n",
        "class BiGRUModel(nn.Module):\n",
        "    def __init__(self, emb_dim=48, feat_dim=5, hidden=256, layers=2, dropout=0.3, n_targets=5):\n",
        "        super().__init__()\n",
        "        self.seq_emb = nn.Embedding(len(SEQ_VOCAB), emb_dim, padding_idx=0)\n",
        "        self.str_emb = nn.Embedding(len(STR_VOCAB), emb_dim, padding_idx=0)\n",
        "        self.loop_emb = nn.Embedding(len(LOOP_VOCAB), emb_dim, padding_idx=0)\n",
        "        in_dim = emb_dim*3 + feat_dim\n",
        "        self.proj = nn.Linear(in_dim, hidden*2)\n",
        "        self.gru = nn.GRU(input_size=hidden*2, hidden_size=hidden, num_layers=layers, batch_first=True, bidirectional=True, dropout=dropout if layers>1 else 0.0)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.head = nn.Linear(hidden*2, n_targets)\n",
        "\n",
        "    def forward(self, seq_ids, str_ids, loop_ids, feats, lengths):\n",
        "        x = torch.cat([\n",
        "            self.seq_emb(seq_ids),\n",
        "            self.str_emb(str_ids),\n",
        "            self.loop_emb(loop_ids),\n",
        "            feats\n",
        "        ], dim=-1)  # [B,L,3E+F]\n",
        "        x = self.proj(x)\n",
        "        # pack for efficiency\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_out, _ = self.gru(packed)\n",
        "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
        "        out = self.dropout(out)\n",
        "        preds = self.head(out)\n",
        "        return preds\n",
        "\n",
        "def masked_mse(preds, targets, mask):\n",
        "    # mask: [B,L] 1=include\n",
        "    diff = (preds - targets)**2\n",
        "    mask3 = mask.unsqueeze(-1).expand_as(diff)\n",
        "    num = (diff * mask3).sum(dim=(0,1))  # sum over B,L per target\n",
        "    den = mask3.sum(dim=(0,1)).clamp_min(1.0)\n",
        "    mse_t = num / den  # per target\n",
        "    return mse_t, torch.sqrt(mse_t.mean())  # per-target and MCRMSE\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, scaler, scheduler=None):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    n_batches = 0\n",
        "    t0 = time.time()\n",
        "    for it, batch in enumerate(loader):\n",
        "        seq_ids = batch['seq_ids'].to(device)\n",
        "        str_ids = batch['str_ids'].to(device)\n",
        "        loop_ids = batch['loop_ids'].to(device)\n",
        "        feats = batch['feats'].to(device)\n",
        "        mask_scored = (batch['mask_scored'] * batch['mask_pad']).to(device)\n",
        "        targets = batch['targets'].to(device)\n",
        "        lengths = batch['mask_pad'].sum(dim=1).to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=True):\n",
        "            preds = model(seq_ids, str_ids, loop_ids, feats, lengths)\n",
        "            mse_t, mcrmse = masked_mse(preds, targets, mask_scored)\n",
        "            loss = mse_t.mean()\n",
        "        scaler.scale(loss).backward()\n",
        "        # unscale for clipping then step\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "        n_batches += 1\n",
        "        if (it+1) % 50 == 0:\n",
        "            print(f'  iter {it+1}/{len(loader)} loss {total_loss/n_batches:.5f} elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "    return total_loss / max(n_batches,1)\n",
        "\n",
        "def validate(model, loader):\n",
        "    model.eval()\n",
        "    t_mse_sum = torch.zeros(len(TARGET_COLS), device=device)\n",
        "    t_den = torch.zeros(len(TARGET_COLS), device=device)\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            seq_ids = batch['seq_ids'].to(device)\n",
        "            str_ids = batch['str_ids'].to(device)\n",
        "            loop_ids = batch['loop_ids'].to(device)\n",
        "            feats = batch['feats'].to(device)\n",
        "            mask_scored = (batch['mask_scored'] * batch['mask_pad']).to(device)\n",
        "            targets = batch['targets'].to(device)\n",
        "            lengths = batch['mask_pad'].sum(dim=1).to(device)\n",
        "            preds = model(seq_ids, str_ids, loop_ids, feats, lengths)\n",
        "            diff = (preds - targets)**2\n",
        "            mask3 = mask_scored.unsqueeze(-1).expand_as(diff)\n",
        "            t_mse_sum += (diff * mask3).sum(dim=(0,1))\n",
        "            t_den += mask3.sum(dim=(0,1))\n",
        "    mse_t = (t_mse_sum / t_den.clamp_min(1.0)).detach().cpu().numpy()\n",
        "    mcrmse = float(np.sqrt(mse_t.mean()))\n",
        "    return mcrmse, mse_t\n",
        "\n",
        "def predict_model(model, loader):\n",
        "    model.eval()\n",
        "    preds_all = []\n",
        "    ids_all = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            seq_ids = batch['seq_ids'].to(device)\n",
        "            str_ids = batch['str_ids'].to(device)\n",
        "            loop_ids = batch['loop_ids'].to(device)\n",
        "            feats = batch['feats'].to(device)\n",
        "            lengths = batch['mask_pad'].sum(dim=1).to(device)\n",
        "            preds = model(seq_ids, str_ids, loop_ids, feats, lengths)  # [B,L,T]\n",
        "            preds_all.append(preds.detach().cpu().numpy())\n",
        "            ids_all.extend(batch['ids'])\n",
        "    return np.concatenate(preds_all, axis=0), ids_all  # [N,L,T], ids list\n",
        "\n",
        "# Build dataframes\n",
        "train_df = read_json_robust('train.json')\n",
        "test_df = read_json_robust('test.json')\n",
        "target_cols = TARGET_COLS.copy()\n",
        "\n",
        "# Filter training by quality as advised\n",
        "if 'SN_filter' in train_df.columns:\n",
        "    train_df = train_df[train_df['SN_filter']==1].reset_index(drop=True)\n",
        "print('Train after SN_filter==1:', train_df.shape, flush=True)\n",
        "\n",
        "# Config\n",
        "FOLDS = 3  # smoke test; later switch to 5\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "LR = 1e-3\n",
        "WD = 1e-4\n",
        "DROPOUT = 0.3\n",
        "HIDDEN = 256\n",
        "EMB = 48\n",
        "\n",
        "# CV split\n",
        "groups = train_df['id'].values\n",
        "gkf = GroupKFold(n_splits=FOLDS)\n",
        "fold_indices = list(gkf.split(train_df, groups=groups))\n",
        "print('Fold sizes:', [ (len(tr), len(va)) for tr,va in fold_indices ])\n",
        "\n",
        "oof_preds = np.zeros((len(train_df), train_df['seq_length'].iloc[0], len(TARGET_COLS)), dtype=np.float32)\n",
        "test_preds_folds = []\n",
        "\n",
        "for fi, (tr_idx, va_idx) in enumerate(fold_indices):\n",
        "    print(f'===== Fold {fi+1}/{FOLDS} =====', flush=True)\n",
        "    tr_df = train_df.iloc[tr_idx].reset_index(drop=True)\n",
        "    va_df = train_df.iloc[va_idx].reset_index(drop=True)\n",
        "    tr_ds = RNADataset(tr_df, train_mode=True)\n",
        "    va_ds = RNADataset(va_df, train_mode=True)\n",
        "    te_ds = RNADataset(test_df, train_mode=False)\n",
        "    tr_loader = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, collate_fn=collate_pad)\n",
        "    va_loader = DataLoader(va_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate_pad)\n",
        "    te_loader = DataLoader(te_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate_pad)\n",
        "\n",
        "    model = BiGRUModel(emb_dim=EMB, feat_dim=5, hidden=HIDDEN, layers=2, dropout=DROPOUT, n_targets=len(TARGET_COLS)).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    total_steps = EPOCHS * len(tr_loader)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
        "\n",
        "    best_mcrmse = 1e9\n",
        "    best_state = None\n",
        "    patience = 5\n",
        "    bad = 0\n",
        "    t0 = time.time()\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "        ep_t0 = time.time()\n",
        "        tr_loss = train_one_epoch(model, tr_loader, optimizer, scaler, scheduler)\n",
        "        val_mcrmse, val_mse_t = validate(model, va_loader)\n",
        "        print(f'Epoch {epoch}/{EPOCHS} fold {fi} tr_loss {tr_loss:.5f} val_mcrmse {val_mcrmse:.5f} per-target {np.sqrt(val_mse_t)} time {time.time()-ep_t0:.1f}s', flush=True)\n",
        "        if val_mcrmse < best_mcrmse - 1e-4:\n",
        "            best_mcrmse = val_mcrmse\n",
        "            best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n",
        "            bad = 0\n",
        "        else:\n",
        "            bad += 1\n",
        "        if bad >= patience:\n",
        "            print('Early stopping.', flush=True)\n",
        "            break\n",
        "    print(f'Fold {fi} best mcrmse: {best_mcrmse:.5f} elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict({k:v.to(device) for k,v in best_state.items()})\n",
        "\n",
        "    # OOF preds\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        va_loader2 = DataLoader(va_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate_pad)\n",
        "        ptr = 0\n",
        "        for batch in va_loader2:\n",
        "            B = len(batch['ids'])\n",
        "            seq_ids = batch['seq_ids'].to(device)\n",
        "            str_ids = batch['str_ids'].to(device)\n",
        "            loop_ids = batch['loop_ids'].to(device)\n",
        "            feats = batch['feats'].to(device)\n",
        "            lengths = batch['mask_pad'].sum(dim=1).to(device)\n",
        "            preds = model(seq_ids, str_ids, loop_ids, feats, lengths).detach().cpu().numpy()\n",
        "            oof_preds[va_idx[ptr:ptr+B]] = preds\n",
        "            ptr += B\n",
        "\n",
        "    # Test preds\n",
        "    fold_test_preds, _ = predict_model(model, te_loader)\n",
        "    test_preds_folds.append(fold_test_preds)\n",
        "    del model, optimizer, scheduler, scaler, tr_loader, va_loader, te_loader\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# OOF metric on train (scored positions only) with padded y_true\n",
        "seq_len = int(train_df['seq_length'].iloc[0])\n",
        "seq_scored = int(train_df['seq_scored'].iloc[0])\n",
        "y_true = np.zeros((len(train_df), seq_len, len(TARGET_COLS)), dtype=np.float32)\n",
        "for ti, col in enumerate(TARGET_COLS):\n",
        "    arrs = train_df[col].tolist()\n",
        "    for i, arr in enumerate(arrs):\n",
        "        a = np.asarray(arr, dtype=np.float32)\n",
        "        take = min(len(a), seq_len, seq_scored)\n",
        "        y_true[i, :take, ti] = a[:take]\n",
        "mask = np.zeros((len(train_df), seq_len), dtype=np.float32)\n",
        "mask[:, :seq_scored] = 1.0\n",
        "mse_t = ((oof_preds - y_true)**2 * mask[...,None]).sum(axis=(0,1)) / mask.sum(axis=(0,1)).clip(1)\n",
        "oof_mcrmse = float(np.sqrt(mse_t.mean()))\n",
        "print('OOF MCRMSE:', oof_mcrmse, ' per-target RMSE:', np.sqrt(mse_t))\n",
        "\n",
        "# Ensemble test preds over folds\n",
        "test_preds = np.mean(np.stack(test_preds_folds, axis=0), axis=0)  # [Ntest, L, T]\n",
        "\n",
        "# Clip predictions per target (hardcoded safe bounds) and slice first 68 positions\n",
        "seq_scored_test = int(test_df['seq_scored'].iloc[0])\n",
        "bounds = {'reactivity':3.0, 'deg_Mg_pH10':5.0, 'deg_pH10':5.0, 'deg_Mg_50C':5.0, 'deg_50C':5.0}\n",
        "for ti, col in enumerate(TARGET_COLS):\n",
        "    test_preds[..., ti] = np.clip(test_preds[..., ti], 0.0, bounds[col])\n",
        "\n",
        "# Build submission from sample order\n",
        "sub = pd.read_csv('sample_submission.csv')\n",
        "id_to_row = {rid:i for i,rid in enumerate(test_df['id'].values)}\n",
        "records = []\n",
        "for i in range(len(test_df)):\n",
        "    rid = test_df['id'].iloc[i]\n",
        "    for pos in range(seq_scored_test):\n",
        "        rec = {'id_seqpos': f'{rid}_{pos}'}\n",
        "        for ti, col in enumerate(TARGET_COLS):\n",
        "            rec[col] = float(test_preds[i, pos, ti])\n",
        "        records.append(rec)\n",
        "pred_df = pd.DataFrame(records)\n",
        "sub_out = sub[['id_seqpos']].merge(pred_df, on='id_seqpos', how='left')\n",
        "assert sub_out.shape == sub.shape, f'Wrong submission shape: {sub_out.shape} vs {sub.shape}'\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with shape', sub_out.shape, 'OOF MCRMSE', oof_mcrmse)\n",
        "print(sub_out.head())\n",
        "print('Done.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train after SN_filter==1: (1349, 19)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold sizes: [(899, 450), (899, 450), (900, 449)]\n===== Fold 1/3 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_235/1675138758.py:165: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_235/1675138758.py:65: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 fold 0 tr_loss 0.61789 val_mcrmse 0.46464 per-target [0.42203888 0.49103627 0.42863777 0.43786088 0.53363615] time 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10 fold 0 tr_loss 0.21553 val_mcrmse 0.40631 per-target [0.38557398 0.45640785 0.41188267 0.42247215 0.34689045] time 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10 fold 0 tr_loss 0.17475 val_mcrmse 0.38148 per-target [0.3467144  0.44628134 0.38573438 0.38672563 0.33151618] time 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10 fold 0 tr_loss 0.15714 val_mcrmse 0.36894 per-target [0.35675916 0.4192098  0.36312023 0.3765078  0.3223946 ] time 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10 fold 0 tr_loss 0.14136 val_mcrmse 0.35654 per-target [0.31844038 0.41604698 0.35648796 0.35778314 0.32558483] time 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10 fold 0 tr_loss 0.13102 val_mcrmse 0.34075 per-target [0.31065324 0.39456546 0.34727368 0.34295106 0.30025217] time 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10 fold 0 tr_loss 0.12575 val_mcrmse 0.33628 per-target [0.3085154  0.38822353 0.34262368 0.3374293  0.2971208 ] time 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10 fold 0 tr_loss 0.12272 val_mcrmse 0.33370 per-target [0.3072244  0.38512233 0.34006754 0.33471873 0.29393208] time 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10 fold 0 tr_loss 0.12104 val_mcrmse 0.33302 per-target [0.30614346 0.38426918 0.33965087 0.33402365 0.29358512] time 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10 fold 0 tr_loss 0.12131 val_mcrmse 0.33333 per-target [0.3075565  0.38434333 0.33943585 0.33448124 0.29348603] time 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0 best mcrmse: 0.33302 elapsed 4.8s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}