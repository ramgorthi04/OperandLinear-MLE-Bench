{
  "cells": [
    {
      "id": "760bc4dd-f4f3-4349-bb83-7d15f3862dd3",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final plan (medal path, concise)\n",
        "\n",
        "Objectives (in order):\n",
        "- S31: Recency-optimized blending is done in production; keep gamma=0.98 variant as primary for now.\n",
        "- S32: Train high-capacity time-aware LR_main (title+request_text only, no subreddit) with L2 saga; cache OOF/test.\n",
        "- S33: Re-run 7/8-way recency-weighted logit blend including LR_main; write primary + 15% shrink hedges.\n",
        "- S34: Refit-on-full for MPNet emb+meta with 5-seed bag and fixed num_boost_round; update test preds.\n",
        "- S35: Final refit-on-full for all XGB bases (Dense v1/v2, Meta, MiniLM, MPNet) with 5-seed bag; LR models refit with chosen C.\n",
        "- S36: Build final refit blends with recency-optimized weights; write hedges and promote best to submission.csv.\n",
        "\n",
        "Constraints and settings:\n",
        "- Time-aware CV: 6 blocks forward-chaining; validate on blocks 1..5 only.\n",
        "- LR_main TF-IDF:\n",
        "  - word 1\u20133, char_wb 2\u20136; min_df in {1,2}; max_features per view \u2248 300k\u2013400k (RAM check).\n",
        "  - Regularization: L2 (saga), C \u2208 {0.6, 0.8, 1.0, 1.2, 1.5}; max_iter=2000, n_jobs=-1.\n",
        "  - Add small meta_v1 if and only if it improves blend \u2265 +0.001; otherwise keep text-only.\n",
        "- Blending (logit space, nonnegative, sum=1):\n",
        "  - LR_mix g \u2208 {0.90, 0.95, 0.97}; w_LR \u2265 0.25; Meta \u2208 [0.18,0.22]; Dense_total \u2208 [0.22,0.40];\n",
        "  - MiniLM \u2208 [0.10,0.15], MPNet \u2208 [0.08,0.12], embeddings total \u2264 0.30.\n",
        "  - If LR_main included: w_LRmain \u2208 [0.05,0.10] only if it lifts OOF on late-tuned objective.\n",
        "  - Optimize with full-mask, last-2, and gamma \u2208 {0.90,0.95,0.98}; produce 15% shrink hedges.\n",
        "- Refit-on-full:\n",
        "  - XGB: use median best_iteration from time-CV as fixed num_boost_round; 5 seeds [42,1337,2025,614,2718]; device=cuda.\n",
        "  - LR: rebuild vectorizers on full train; same C as best fold; predict test probs.\n",
        "\n",
        "Artifacts to produce:\n",
        "- oof_lr_main_time.npy, test_lr_main_time.npy\n",
        "- submission_8way_full.csv / last2.csv / gammaXX.csv (+ _shrunk) with/without LR_main\n",
        "- test_xgb_emb_mpnet_fullbag.npy (and similarly for other XGB bases if refit updated)\n",
        "\n",
        "Next cell: implement S32 LR_main time-aware training with caching and progress logs."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "e8bac677-b5f3-4393-8df7-9d65ad5a7b42",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S32: Time-aware high-capacity LR_main (title + request_text only), L2 saga; cache OOF/test\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body_no_leak(df):\n",
        "    # Prefer request_text (avoid edit_aware per expert advice); fallback if missing\n",
        "    if 'request_text' in df.columns:\n",
        "        return df['request_text'].fillna('').astype(str)\n",
        "    col = 'request_text_edit_aware' if 'request_text_edit_aware' in df.columns else 'request_text'\n",
        "    return df.get(col, pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def build_text(df):\n",
        "    return (get_title(df) + '\\n' + get_body_no_leak(df)).astype(str)\n",
        "\n",
        "txt_tr = build_text(train)\n",
        "txt_te = build_text(test)\n",
        "\n",
        "# 6-block forward-chaining folds (validate blocks 1..5)\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []\n",
        "mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}')\n",
        "\n",
        "# High-capacity TF-IDF views\n",
        "word_params = dict(analyzer='word', ngram_range=(1,3), lowercase=True, min_df=2, max_features=300_000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "char_params = dict(analyzer='char_wb', ngram_range=(2,6), lowercase=True, min_df=2, max_features=300_000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "\n",
        "C_grid = [0.8, 1.0, 1.2]\n",
        "results = []\n",
        "best = dict(auc=-1.0, C=None, oof=None, te=None)\n",
        "\n",
        "for C in C_grid:\n",
        "    tC = time.time()\n",
        "    oof = np.zeros(n, dtype=np.float32)\n",
        "    te_parts = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "        t0 = time.time()\n",
        "        tr_text = txt_tr.iloc[tr_idx]; va_text = txt_tr.iloc[va_idx]\n",
        "        tf_w = TfidfVectorizer(**word_params)\n",
        "        Xw_tr = tf_w.fit_transform(tr_text); Xw_va = tf_w.transform(va_text); Xw_te = tf_w.transform(txt_te)\n",
        "        tf_c = TfidfVectorizer(**char_params)\n",
        "        Xc_tr = tf_c.fit_transform(tr_text); Xc_va = tf_c.transform(va_text); Xc_te = tf_c.transform(txt_te)\n",
        "        X_tr = hstack([Xw_tr, Xc_tr], format='csr')\n",
        "        X_va = hstack([Xw_va, Xc_va], format='csr')\n",
        "        X_te = hstack([Xw_te, Xc_te], format='csr')\n",
        "        clf = LogisticRegression(penalty='l2', solver='saga', C=C, max_iter=2000, n_jobs=-1, verbose=0)\n",
        "        clf.fit(X_tr, y[tr_idx])\n",
        "        va_pred = clf.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "        te_pred = clf.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "        oof[va_idx] = va_pred\n",
        "        te_parts.append(te_pred)\n",
        "        auc = roc_auc_score(y[va_idx], va_pred)\n",
        "        print(f'[LR_main C={C}] Fold {fi} AUC: {auc:.5f} | {time.time()-t0:.1f}s | tr:{X_tr.shape}, va:{X_va.shape}')\n",
        "        del Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, X_tr, X_va, X_te, clf; gc.collect()\n",
        "    auc_mask = roc_auc_score(y[mask], oof[mask])\n",
        "    te_mean = np.mean(te_parts, axis=0).astype(np.float32)\n",
        "    results.append((C, auc_mask))\n",
        "    print(f'[LR_main C={C}] OOF AUC(validated): {auc_mask:.5f} | {time.time()-tC:.1f}s')\n",
        "    if auc_mask > best['auc']:\n",
        "        best.update(dict(auc=auc_mask, C=C, oof=oof.copy(), te=te_mean.copy()))\n",
        "    del oof, te_parts; gc.collect()\n",
        "\n",
        "print('C grid results:', results)\n",
        "print(f'Best C={best[\"C\"]} | OOF AUC(validated)={best[\"auc\"]:.5f}')\n",
        "np.save('oof_lr_main_time.npy', best['oof'].astype(np.float32))\n",
        "np.save('test_lr_main_time.npy', best['te'].astype(np.float32))\n",
        "print('Saved oof_lr_main_time.npy and test_lr_main_time.npy')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV: 5 folds; validated 2398/2878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main C=0.8] Fold 1 AUC: 0.67896 | 5.8s | tr:(480, 36871), va:(480, 36871)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main C=0.8] Fold 2 AUC: 0.61152 | 9.8s | tr:(960, 59665), va:(480, 59665)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main C=0.8] Fold 3 AUC: 0.58009 | 13.0s | tr:(1440, 77632), va:(480, 77632)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main C=0.8] Fold 4 AUC: 0.63032 | 17.0s | tr:(1920, 91689), va:(479, 91689)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main C=0.8] Fold 5 AUC: 0.64895 | 18.4s | tr:(2399, 104131), va:(479, 104131)\n[LR_main C=0.8] OOF AUC(validated): 0.62378 | 65.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main C=1.0] Fold 1 AUC: 0.67685 | 5.6s | tr:(480, 36871), va:(480, 36871)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main C=1.0] Fold 2 AUC: 0.60895 | 10.1s | tr:(960, 59665), va:(480, 59665)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main C=1.0] Fold 3 AUC: 0.57817 | 14.1s | tr:(1440, 77632), va:(480, 77632)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main C=1.0] Fold 4 AUC: 0.62856 | 17.8s | tr:(1920, 91689), va:(479, 91689)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main C=1.0] Fold 5 AUC: 0.64877 | 21.6s | tr:(2399, 104131), va:(479, 104131)\n[LR_main C=1.0] OOF AUC(validated): 0.62322 | 70.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main C=1.2] Fold 1 AUC: 0.67495 | 6.2s | tr:(480, 36871), va:(480, 36871)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main C=1.2] Fold 2 AUC: 0.60622 | 9.7s | tr:(960, 59665), va:(480, 59665)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main C=1.2] Fold 3 AUC: 0.57687 | 13.9s | tr:(1440, 77632), va:(480, 77632)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main C=1.2] Fold 4 AUC: 0.62808 | 18.9s | tr:(1920, 91689), va:(479, 91689)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main C=1.2] Fold 5 AUC: 0.64811 | 21.2s | tr:(2399, 104131), va:(479, 104131)\n[LR_main C=1.2] OOF AUC(validated): 0.62254 | 71.0s\nC grid results: [(0.8, 0.623780657748049), (1.0, 0.6232247161901174), (1.2, 0.6225446323425503)]\nBest C=0.8 | OOF AUC(validated)=0.62378\nSaved oof_lr_main_time.npy and test_lr_main_time.npy\n"
          ]
        }
      ]
    },
    {
      "id": "afab409f-474c-4aae-b862-48f495037baa",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S34: MPNet emb+meta FULL refit 5-seed bag (fixed rounds ~ median best_iter=29) + rebuild gamma-best 7-way submission\n",
        "import numpy as np, pandas as pd, time, gc, xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "# Load MPNet embeddings + meta_v1\n",
        "Emb_tr = np.load('emb_mpnet_tr.npy').astype(np.float32)\n",
        "Emb_te = np.load('emb_mpnet_te.npy').astype(np.float32)\n",
        "Meta_tr = np.load('meta_v1_tr.npy').astype(np.float32)\n",
        "Meta_te = np.load('meta_v1_te.npy').astype(np.float32)\n",
        "Xtr_raw = np.hstack([Emb_tr, Meta_tr]).astype(np.float32)\n",
        "Xte_raw = np.hstack([Emb_te, Meta_te]).astype(np.float32)\n",
        "print('Full-refit feature shapes:', Xtr_raw.shape, Xte_raw.shape)\n",
        "\n",
        "# Standardize on full train\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "Xtr = scaler.fit_transform(Xtr_raw).astype(np.float32)\n",
        "Xte = scaler.transform(Xte_raw).astype(np.float32)\n",
        "del Xtr_raw, Xte_raw; gc.collect()\n",
        "\n",
        "# XGB params (same as CV runs)\n",
        "params = dict(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='auc',\n",
        "    max_depth=3,\n",
        "    eta=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.6,\n",
        "    min_child_weight=8,\n",
        "    reg_alpha=0.5,\n",
        "    reg_lambda=3.0,\n",
        "    gamma=0.0,\n",
        "    device='cuda',\n",
        "    tree_method='hist'\n",
        ")\n",
        "\n",
        "# Fixed rounds from median of best_iter observed in time-CV logs\n",
        "num_boost_round = 29\n",
        "seeds = [42, 1337, 2025, 614, 2718]\n",
        "pos = float((y == 1).sum()); neg = float((y == 0).sum())\n",
        "spw = (neg / max(pos, 1.0)) if pos > 0 else 1.0\n",
        "print(f'Class balance full-train: pos={int(pos)} neg={int(neg)} spw={spw:.2f} | rounds={num_boost_round} | seeds={seeds}')\n",
        "\n",
        "dtr = xgb.DMatrix(Xtr, label=y)\n",
        "dte = xgb.DMatrix(Xte)\n",
        "\n",
        "test_seed_preds = []\n",
        "t0 = time.time()\n",
        "for si, seed in enumerate(seeds, 1):\n",
        "    p = dict(params); p['seed'] = seed; p['scale_pos_weight'] = spw\n",
        "    booster = xgb.train(p, dtr, num_boost_round=num_boost_round, verbose_eval=False)\n",
        "    te_pred = booster.predict(dte).astype(np.float32)\n",
        "    test_seed_preds.append(te_pred)\n",
        "    print(f'[MPNet full-refit seed {seed}] done | te_pred mean={te_pred.mean():.4f}')\n",
        "test_avg = np.mean(test_seed_preds, axis=0).astype(np.float32)\n",
        "print(f'MPNet full-refit bag done in {time.time()-t0:.1f}s | test mean={test_avg.mean():.4f}')\n",
        "np.save('test_xgb_emb_mpnet_fullbag.npy', test_avg)\n",
        "print('Saved test_xgb_emb_mpnet_fullbag.npy')\n",
        "\n",
        "# Rebuild gamma-best 7-way blend using refit MPNet test preds and prior best weights (from S30 gamma=0.98)\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "t_meta = np.load('test_xgb_meta_time.npy')\n",
        "t_emb_min = np.load('test_xgb_emb_meta_time.npy')\n",
        "t_emb_mp_refit = np.load('test_xgb_emb_mpnet_fullbag.npy')\n",
        "\n",
        "# Gamma-best config from S30:\n",
        "g = 0.97\n",
        "w_lr, w_d1, w_d2, w_meta, w_emn, w_emp = 0.24, 0.15, 0.15, 0.22, 0.12, 0.12\n",
        "\n",
        "tz_lr_mix = (1.0 - g)*to_logit(t_lr_w) + g*to_logit(t_lr_ns)\n",
        "zt = (w_lr*tz_lr_mix +\n",
        "      w_d1*to_logit(t_d1) +\n",
        "      w_d2*to_logit(t_d2) +\n",
        "      w_meta*to_logit(t_meta) +\n",
        "      w_emn*to_logit(t_emb_min) +\n",
        "      w_emp*to_logit(t_emb_mp_refit))\n",
        "pt = sigmoid(zt).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "sub.to_csv('submission_7way_gamma0p98_mpnet_fullrefit.csv', index=False)\n",
        "\n",
        "# 15% shrink-to-equal hedge\n",
        "w_vec = np.array([w_lr, w_d1, w_d2, w_meta, w_emn, w_emp], dtype=np.float64)\n",
        "w_eq = np.ones_like(w_vec)/len(w_vec)\n",
        "alpha = 0.15\n",
        "w_shr = ((1.0 - alpha)*w_vec + alpha*w_eq); w_shr = (w_shr / w_shr.sum()).astype(np.float64)\n",
        "zt_shr = (w_shr[0]*tz_lr_mix +\n",
        "          w_shr[1]*to_logit(t_d1) +\n",
        "          w_shr[2]*to_logit(t_d2) +\n",
        "          w_shr[3]*to_logit(t_meta) +\n",
        "          w_shr[4]*to_logit(t_emb_min) +\n",
        "          w_shr[5]*to_logit(t_emb_mp_refit))\n",
        "pt_shr = sigmoid(zt_shr).astype(np.float32)\n",
        "pd.DataFrame({id_col: test[id_col].values, target_col: pt_shr}).to_csv('submission_7way_gamma0p98_mpnet_fullrefit_shrunk.csv', index=False)\n",
        "\n",
        "# Promote refit submission\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Promoted submission_7way_gamma0p98_mpnet_fullrefit.csv to submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "2a96cd71-c3b6-495e-81c1-b2c703f2364e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S32b: Time-aware LR_main + meta_v1 (title+request_text only), L2 saga; cache OOF/test\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body_no_leak(df):\n",
        "    # Avoid edit_aware; prefer request_text\n",
        "    if 'request_text' in df.columns:\n",
        "        return df['request_text'].fillna('').astype(str)\n",
        "    return df.get('request_text', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def build_text(df):\n",
        "    return (get_title(df) + '\\n' + get_body_no_leak(df)).astype(str)\n",
        "\n",
        "txt_tr = build_text(train); txt_te = build_text(test)\n",
        "\n",
        "# Load meta_v1 features\n",
        "Meta_tr = np.load('meta_v1_tr.npy').astype(np.float32)\n",
        "Meta_te = np.load('meta_v1_te.npy').astype(np.float32)\n",
        "\n",
        "# 6-block forward-chaining folds\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}')\n",
        "\n",
        "# High-capacity TF-IDF views\n",
        "word_params = dict(analyzer='word', ngram_range=(1,3), lowercase=True, min_df=2, max_features=300_000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "char_params = dict(analyzer='char_wb', ngram_range=(2,6), lowercase=True, min_df=2, max_features=300_000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "\n",
        "C_grid = [0.8, 1.0]\n",
        "results = []\n",
        "best = dict(auc=-1.0, C=None, oof=None, te=None)\n",
        "\n",
        "for C in C_grid:\n",
        "    tC = time.time()\n",
        "    oof = np.zeros(n, dtype=np.float32)\n",
        "    te_parts = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "        t0 = time.time()\n",
        "        tr_text = txt_tr.iloc[tr_idx]; va_text = txt_tr.iloc[va_idx]\n",
        "        tf_w = TfidfVectorizer(**word_params)\n",
        "        Xw_tr = tf_w.fit_transform(tr_text); Xw_va = tf_w.transform(va_text); Xw_te = tf_w.transform(txt_te)\n",
        "        tf_c = TfidfVectorizer(**char_params)\n",
        "        Xc_tr = tf_c.fit_transform(tr_text); Xc_va = tf_c.transform(va_text); Xc_te = tf_c.transform(txt_te)\n",
        "        # Stack text views\n",
        "        X_tr_text = hstack([Xw_tr, Xc_tr], format='csr')\n",
        "        X_va_text = hstack([Xw_va, Xc_va], format='csr')\n",
        "        X_te_text = hstack([Xw_te, Xc_te], format='csr')\n",
        "        # Append meta_v1 (as CSR) without scaling\n",
        "        X_tr = hstack([X_tr_text, csr_matrix(Meta_tr[tr_idx])], format='csr')\n",
        "        X_va = hstack([X_va_text, csr_matrix(Meta_tr[va_idx])], format='csr')\n",
        "        X_te = hstack([X_te_text, csr_matrix(Meta_te)], format='csr')\n",
        "        clf = LogisticRegression(penalty='l2', solver='saga', C=C, max_iter=2000, n_jobs=-1, verbose=0)\n",
        "        clf.fit(X_tr, y[tr_idx])\n",
        "        va_pred = clf.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "        te_pred = clf.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "        oof[va_idx] = va_pred\n",
        "        te_parts.append(te_pred)\n",
        "        auc = roc_auc_score(y[va_idx], va_pred)\n",
        "        print(f'[LR_main+meta C={C}] Fold {fi} AUC: {auc:.5f} | {time.time()-t0:.1f}s | tr:{X_tr.shape[0]}x{X_tr.shape[1]}')\n",
        "        del Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, X_tr_text, X_va_text, X_te_text, X_tr, X_va, X_te, clf; gc.collect()\n",
        "    auc_mask = roc_auc_score(y[mask], oof[mask])\n",
        "    te_mean = np.mean(te_parts, axis=0).astype(np.float32)\n",
        "    results.append((C, auc_mask))\n",
        "    print(f'[LR_main+meta C={C}] OOF AUC(validated): {auc_mask:.5f} | {time.time()-tC:.1f}s')\n",
        "    if auc_mask > best['auc']:\n",
        "        best.update(dict(auc=auc_mask, C=C, oof=oof.copy(), te=te_mean.copy()))\n",
        "    del oof, te_parts; gc.collect()\n",
        "\n",
        "print('C grid results:', results)\n",
        "print(f'Best C={best[\"C\"]} | OOF AUC(validated)={best[\"auc\"]:.5f}')\n",
        "np.save('oof_lr_main_meta_time.npy', best['oof'].astype(np.float32))\n",
        "np.save('test_lr_main_meta_time.npy', best['te'].astype(np.float32))\n",
        "print('Saved oof_lr_main_meta_time.npy and test_lr_main_meta_time.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "a222d475-9a5d-4d20-a38f-6fa1146ccf8a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S33: Recency-weighted 7/8-way logit blend including LR_main+meta; write variants + 15% shrink hedges\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# 6-block forward-chaining blocks and masks\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "n = len(train)\n",
        "mask_full = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    mask_full[np.array(blocks[i])] = True\n",
        "mask_last2 = np.zeros(n, dtype=bool)\n",
        "for i in [4,5]:\n",
        "    mask_last2[np.array(blocks[i])] = True\n",
        "print(f'Time-CV validated full: {mask_full.sum()}/{n} | last2: {mask_last2.sum()}')\n",
        "\n",
        "# Load base OOF/test\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy'); t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "o_lr_ns = np.load('oof_lr_time_nosub_meta.npy');  t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy');         t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_time_v2.npy');      t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta_time.npy');        t_meta = np.load('test_xgb_meta_time.npy')\n",
        "o_emn = np.load('oof_xgb_emb_meta_time.npy');     t_emn_refit = np.load('test_xgb_emb_meta_time.npy')  # MiniLM (no full-bag yet)\n",
        "o_emp = np.load('oof_xgb_emb_mpnet_time.npy');    \n",
        "t_emp_path_full = 'test_xgb_emb_mpnet_fullbag.npy'\n",
        "try:\n",
        "    t_emp_refit = np.load(t_emp_path_full)\n",
        "    print('Using MPNet full-bag test preds.')\n",
        "except Exception:\n",
        "    t_emp_refit = np.load('test_xgb_emb_mpnet_time.npy')\n",
        "    print('Using MPNet CV-avg test preds (no full-bag found).')\n",
        "\n",
        "# Optional LR_main+meta\n",
        "try:\n",
        "    o_lr_mainm = np.load('oof_lr_main_meta_time.npy')\n",
        "    t_lr_mainm = np.load('test_lr_main_meta_time.npy')\n",
        "    has_lr_mainm = True\n",
        "    print('Loaded LR_main+meta OOF/test.')\n",
        "except Exception:\n",
        "    has_lr_mainm = False\n",
        "    print('LR_main+meta not found; running 7-way only.')\n",
        "\n",
        "# Convert to logits\n",
        "z_lr_w, z_lr_ns = to_logit(o_lr_w), to_logit(o_lr_ns)\n",
        "z_d1, z_d2, z_meta = to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)\n",
        "z_emn, z_emp = to_logit(o_emn), to_logit(o_emp)\n",
        "tz_lr_w, tz_lr_ns = to_logit(t_lr_w), to_logit(t_lr_ns)\n",
        "tz_d1, tz_d2, tz_meta = to_logit(t_d1), to_logit(t_d2), to_logit(t_meta)\n",
        "tz_emn = to_logit(t_emn_refit); tz_emp = to_logit(t_emp_refit)\n",
        "if has_lr_mainm:\n",
        "    z_lr_mainm = to_logit(o_lr_mainm); tz_lr_mainm = to_logit(t_lr_mainm)\n",
        "\n",
        "# Grids per expert priors (tight around previous best)\n",
        "g_grid = [0.96, 0.97, 0.98]\n",
        "meta_grid = [0.18, 0.20, 0.22]\n",
        "dense_tot_grid = [0.28, 0.30, 0.35]\n",
        "dense_split = [(0.6, 0.4), (0.7, 0.3), (0.8, 0.2)]  # (v1, v2) fractions\n",
        "emb_tot_grid = [0.24, 0.27, 0.30]\n",
        "emb_split = [(0.6, 0.4), (0.5, 0.5)]  # (MiniLM, MPNet)\n",
        "w_lrmain_grid = [0.0, 0.05, 0.08] if has_lr_mainm else [0.0]\n",
        "\n",
        "def search(mask, sample_weight=None):\n",
        "    best_auc, best_cfg, tried = -1.0, None, 0\n",
        "    for g in g_grid:\n",
        "        z_lr_mix = (1.0 - g)*z_lr_w + g*z_lr_ns\n",
        "        tz_lr_mix = (1.0 - g)*tz_lr_w + g*tz_lr_ns\n",
        "        for w_meta in meta_grid:\n",
        "            for d_tot in dense_tot_grid:\n",
        "                for dv1, dv2 in dense_split:\n",
        "                    w_d1 = d_tot * dv1; w_d2 = d_tot * dv2\n",
        "                    for e_tot in emb_tot_grid:\n",
        "                        for emn_fr, emp_fr in emb_split:\n",
        "                            w_emn = e_tot * emn_fr; w_emp = e_tot * emp_fr\n",
        "                            rem = 1.0 - (w_meta + w_d1 + w_d2 + w_emn + w_emp)\n",
        "                            if rem <= 0: continue\n",
        "                            for w_lrmain in w_lrmain_grid:\n",
        "                                if w_lrmain > rem: continue\n",
        "                                w_lr = rem - w_lrmain\n",
        "                                if w_lr < 0.25:  # enforce LR_mix \u2265 0.25\n",
        "                                    continue\n",
        "                                z_oof = (w_lr*z_lr_mix + w_d1*z_d1 + w_d2*z_d2 + w_meta*z_meta + w_emn*z_emn + w_emp*z_emp)\n",
        "                                if has_lr_mainm and w_lrmain > 0:\n",
        "                                    z_oof = z_oof + w_lrmain*z_lr_mainm\n",
        "                                auc = roc_auc_score(y[mask], z_oof[mask], sample_weight=(sample_weight[mask] if sample_weight is not None else None))\n",
        "                                tried += 1\n",
        "                                if auc > best_auc:\n",
        "                                    best_auc = auc\n",
        "                                    best_cfg = dict(g=float(g), w_lr=float(w_lr), w_d1=float(w_d1), w_d2=float(w_d2), w_meta=float(w_meta),\n",
        "                                                    w_emn=float(w_emn), w_emp=float(w_emp), w_lrmain=float(w_lrmain), tz_lr_mix=tz_lr_mix)\n",
        "    return best_auc, best_cfg, tried\n",
        "\n",
        "# 1) Full-mask\n",
        "auc_full, cfg_full, tried_full = search(mask_full)\n",
        "print(f'[Full] tried={tried_full} | best OOF(z) AUC={auc_full:.5f} | cfg={ {k:v for k,v in cfg_full.items() if k!=\"tz_lr_mix\"} }')\n",
        "\n",
        "# 2) Last-2 blocks only\n",
        "auc_last2, cfg_last2, tried_last2 = search(mask_last2)\n",
        "print(f'[Last2] tried={tried_last2} | best OOF(z,last2) AUC={auc_last2:.5f} | cfg={ {k:v for k,v in cfg_last2.items() if k!=\"tz_lr_mix\"} }')\n",
        "\n",
        "# 3) Gamma-decayed over validated\n",
        "best_gamma, best_auc_g, best_cfg_g = None, -1.0, None\n",
        "for gamma in [0.95, 0.98]:\n",
        "    w = np.zeros(n, dtype=np.float64)\n",
        "    for bi in range(1, k):\n",
        "        age = (k - 1) - bi\n",
        "        w[np.array(blocks[bi])] = (gamma ** age)\n",
        "    auc_g, cfg_g, _ = search(mask_full, sample_weight=w)\n",
        "    print(f'[Gamma {gamma}] best OOF(z,weighted) AUC={auc_g:.5f}')\n",
        "    if auc_g > best_auc_g:\n",
        "        best_auc_g, best_cfg_g, best_gamma = auc_g, cfg_g, gamma\n",
        "print(f'[Gamma-best] gamma={best_gamma} | AUC={best_auc_g:.5f} | cfg={ {k:v for k,v in best_cfg_g.items() if k!=\"tz_lr_mix\"} }')\n",
        "\n",
        "def build_and_save(tag, cfg):\n",
        "    g = cfg['g']; tz_lr_mix = cfg['tz_lr_mix']\n",
        "    w_lr, w_d1, w_d2, w_meta, w_emn, w_emp, w_lrmain = cfg['w_lr'], cfg['w_d1'], cfg['w_d2'], cfg['w_meta'], cfg['w_emn'], cfg['w_emp'], cfg['w_lrmain']\n",
        "    zt = (w_lr*tz_lr_mix + w_d1*tz_d1 + w_d2*tz_d2 + w_meta*tz_meta + w_emn*tz_emn + w_emp*tz_emp)\n",
        "    if has_lr_mainm and w_lrmain > 0:\n",
        "        zt = zt + w_lrmain*tz_lr_mainm\n",
        "    pt = sigmoid(zt).astype(np.float32)\n",
        "    pd.DataFrame({id_col: ids, target_col: pt}).to_csv(f'submission_blend_{tag}.csv', index=False)\n",
        "    # 15% shrink hedge across present components\n",
        "    w_list = [w_lr, w_d1, w_d2, w_meta, w_emn, w_emp]\n",
        "    comp_logits = [tz_lr_mix, tz_d1, tz_d2, tz_meta, tz_emn, tz_emp]\n",
        "    if has_lr_mainm and w_lrmain > 0:\n",
        "        w_list.append(w_lrmain); comp_logits.append(tz_lr_mainm)\n",
        "    w_vec = np.array(w_list, dtype=np.float64)\n",
        "    w_eq = np.ones_like(w_vec)/len(w_vec)\n",
        "    alpha = 0.15\n",
        "    w_shr = ((1.0 - alpha)*w_vec + alpha*w_eq); w_shr = (w_shr / w_shr.sum()).astype(np.float64)\n",
        "    zt_shr = np.zeros_like(comp_logits[0], dtype=np.float64)\n",
        "    for wi, zi in zip(w_shr, comp_logits):\n",
        "        zt_shr += wi*zi\n",
        "    pt_shr = sigmoid(zt_shr).astype(np.float32)\n",
        "    pd.DataFrame({id_col: ids, target_col: pt_shr}).to_csv(f'submission_blend_{tag}_shrunk.csv', index=False)\n",
        "\n",
        "build_and_save('full', cfg_full)\n",
        "build_and_save('last2', cfg_last2)\n",
        "build_and_save(f'gamma{best_gamma:.2f}'.replace('.','p'), best_cfg_g)\n",
        "\n",
        "# Promote gamma-best as primary\n",
        "prim = f'submission_blend_gamma{best_gamma:.2f}'.replace('.','p') + '.csv'\n",
        "pd.read_csv(prim).to_csv('submission.csv', index=False)\n",
        "print(f'Promoted {prim} to submission.csv')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV validated full: 2398/2878 | last2: 958\nUsing MPNet full-bag test preds.\nLoaded LR_main+meta OOF/test.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Full] tried=156 | best OOF(z) AUC=0.68197 | cfg={'g': 0.98, 'w_lr': 0.25, 'w_d1': 0.22400000000000003, 'w_d2': 0.05600000000000001, 'w_meta': 0.2, 'w_emn': 0.135, 'w_emp': 0.135, 'w_lrmain': 0.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Last2] tried=156 | best OOF(z,last2) AUC=0.64782 | cfg={'g': 0.98, 'w_lr': 0.25, 'w_d1': 0.22400000000000003, 'w_d2': 0.05600000000000001, 'w_meta': 0.2, 'w_emn': 0.162, 'w_emp': 0.10800000000000001, 'w_lrmain': 0.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gamma 0.95] best OOF(z,weighted) AUC=0.67894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gamma 0.98] best OOF(z,weighted) AUC=0.68076\n[Gamma-best] gamma=0.98 | AUC=0.68076 | cfg={'g': 0.98, 'w_lr': 0.25, 'w_d1': 0.22400000000000003, 'w_d2': 0.05600000000000001, 'w_meta': 0.2, 'w_emn': 0.135, 'w_emp': 0.135, 'w_lrmain': 0.0}\nPromoted submission_blend_gamma0p98.csv to submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "066c6f98-5e5e-4595-ab03-abc67891220f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S34c: Full-train 5-seed refits for MiniLM (emb+meta) and Meta-only XGB with rounds from last-block early-stop; rebuild gamma-best submission\n",
        "import numpy as np, pandas as pd, time, gc, xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "# Time blocks for last-block validation to pick num_boost_round\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "tr_idx_rounds = np.concatenate(blocks[:5])  # first 5 blocks\n",
        "va_idx_rounds = np.array(blocks[5])         # last block as validation\n",
        "print(f'Rounds selection using last block valid: tr={len(tr_idx_rounds)} va={len(va_idx_rounds)}')\n",
        "\n",
        "def pick_rounds(X_tr_full, y_full, name, base_params, max_rounds=4000, early_stopping_rounds=100):\n",
        "    # Split last block for early stopping to estimate rounds\n",
        "    X_tr = X_tr_full[tr_idx_rounds]\n",
        "    y_tr = y_full[tr_idx_rounds]\n",
        "    X_va = X_tr_full[va_idx_rounds]\n",
        "    y_va = y_full[va_idx_rounds]\n",
        "    dtr = xgb.DMatrix(X_tr, label=y_tr)\n",
        "    dva = xgb.DMatrix(X_va, label=y_va)\n",
        "    booster = xgb.train(base_params, dtr, num_boost_round=max_rounds, evals=[(dva, 'valid')], early_stopping_rounds=early_stopping_rounds, verbose_eval=False)\n",
        "    best_iter = int(booster.best_iteration or booster.best_ntree_limit or 100)\n",
        "    print(f'[{name}] picked rounds (last-block ES): best_iter={best_iter}')\n",
        "    del dtr, dva, booster; gc.collect()\n",
        "    return best_iter\n",
        "\n",
        "def fullbag_predict(X_full, y_full, X_test, name, base_params, num_rounds, seeds):\n",
        "    dtr = xgb.DMatrix(X_full, label=y_full)\n",
        "    dte = xgb.DMatrix(X_test)\n",
        "    pos = float((y_full == 1).sum()); neg = float((y_full == 0).sum())\n",
        "    spw = (neg / max(pos, 1.0)) if pos > 0 else 1.0\n",
        "    preds = []\n",
        "    for si, seed in enumerate(seeds, 1):\n",
        "        p = dict(base_params); p['seed'] = seed; p['scale_pos_weight'] = spw\n",
        "        booster = xgb.train(p, dtr, num_boost_round=num_rounds, verbose_eval=False)\n",
        "        te_pred = booster.predict(dte).astype(np.float32)\n",
        "        preds.append(te_pred)\n",
        "        print(f'[{name}] seed {seed} done | mean={te_pred.mean():.4f}')\n",
        "        del booster; gc.collect()\n",
        "    out = np.mean(preds, axis=0).astype(np.float32)\n",
        "    print(f'[{name}] bag mean={out.mean():.4f} | num_rounds={num_rounds} | seeds={seeds}')\n",
        "    return out\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Base params (same family as earlier CV runs)\n",
        "xgb_params = dict(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='auc',\n",
        "    max_depth=3,\n",
        "    eta=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.6,\n",
        "    min_child_weight=8,\n",
        "    reg_alpha=0.5,\n",
        "    reg_lambda=3.0,\n",
        "    gamma=0.0,\n",
        "    device='cuda',\n",
        "    tree_method='hist'\n",
        ")\n",
        "seeds = [42, 1337, 2025, 614, 2718]\n",
        "\n",
        "# 1) MiniLM emb+meta refit\n",
        "Emb_min_tr = np.load('emb_minilm_tr.npy').astype(np.float32)\n",
        "Emb_min_te = np.load('emb_minilm_te.npy').astype(np.float32)\n",
        "Meta_tr = np.load('meta_v1_tr.npy').astype(np.float32)\n",
        "Meta_te = np.load('meta_v1_te.npy').astype(np.float32)\n",
        "Xmin_tr_raw = np.hstack([Emb_min_tr, Meta_tr]).astype(np.float32)\n",
        "Xmin_te_raw = np.hstack([Emb_min_te, Meta_te]).astype(np.float32)\n",
        "scaler_min = StandardScaler(with_mean=True, with_std=True)\n",
        "Xmin_tr = scaler_min.fit_transform(Xmin_tr_raw).astype(np.float32)\n",
        "Xmin_te = scaler_min.transform(Xmin_te_raw).astype(np.float32)\n",
        "min_rounds = pick_rounds(Xmin_tr, y, 'MiniLM', xgb_params, max_rounds=4000, early_stopping_rounds=100)\n",
        "pred_min_fullbag = fullbag_predict(Xmin_tr, y, Xmin_te, 'MiniLM', xgb_params, min_rounds, seeds)\n",
        "np.save('test_xgb_emb_minilm_fullbag.npy', pred_min_fullbag)\n",
        "print('Saved test_xgb_emb_minilm_fullbag.npy')\n",
        "\n",
        "# 2) Meta-only refit\n",
        "Xmeta_tr = Meta_tr.astype(np.float32)\n",
        "Xmeta_te = Meta_te.astype(np.float32)\n",
        "meta_rounds = pick_rounds(Xmeta_tr, y, 'Meta-only', xgb_params, max_rounds=4000, early_stopping_rounds=100)\n",
        "pred_meta_fullbag = fullbag_predict(Xmeta_tr, y, Xmeta_te, 'Meta-only', xgb_params, meta_rounds, seeds)\n",
        "np.save('test_xgb_meta_fullbag.npy', pred_meta_fullbag)\n",
        "print('Saved test_xgb_meta_fullbag.npy')\n",
        "\n",
        "# Rebuild gamma-best blend (from S33 cfg) using refit test preds for MiniLM/MPNet/Meta\n",
        "t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "t_d1 = np.load('test_xgb_dense_time.npy')   # Dense v1 (no refit available)\n",
        "t_d2 = np.load('test_xgb_dense_time_v2.npy')# Dense v2 (no refit available)\n",
        "t_meta_ref = np.load('test_xgb_meta_fullbag.npy')\n",
        "t_emn_ref = np.load('test_xgb_emb_minilm_fullbag.npy')\n",
        "t_emp_ref = np.load('test_xgb_emb_mpnet_fullbag.npy')\n",
        "\n",
        "# Use S33 gamma-best weights (printed there)\n",
        "g = 0.98\n",
        "w_lr, w_d1, w_d2, w_meta, w_emn, w_emp, w_lrmain = 0.25, 0.224, 0.056, 0.20, 0.135, 0.135, 0.0\n",
        "\n",
        "tz_lr_mix = (1.0 - g)*to_logit(t_lr_w) + g*to_logit(t_lr_ns)\n",
        "zt = (w_lr*tz_lr_mix +\n",
        "      w_d1*to_logit(t_d1) +\n",
        "      w_d2*to_logit(t_d2) +\n",
        "      w_meta*to_logit(t_meta_ref) +\n",
        "      w_emn*to_logit(t_emn_ref) +\n",
        "      w_emp*to_logit(t_emp_ref))\n",
        "pt = sigmoid(zt).astype(np.float32)\n",
        "pd.DataFrame({id_col: test[id_col].values, target_col: pt}).to_csv('submission_blend_gamma0p98_fullrefits.csv', index=False)\n",
        "\n",
        "# 15% shrink hedge\n",
        "w_vec = np.array([w_lr, w_d1, w_d2, w_meta, w_emn, w_emp], dtype=np.float64)\n",
        "w_eq = np.ones_like(w_vec)/len(w_vec)\n",
        "alpha = 0.15\n",
        "w_shr = ((1.0 - alpha)*w_vec + alpha*w_eq); w_shr = (w_shr / w_shr.sum()).astype(np.float64)\n",
        "zt_shr = (w_shr[0]*tz_lr_mix +\n",
        "          w_shr[1]*to_logit(t_d1) +\n",
        "          w_shr[2]*to_logit(t_d2) +\n",
        "          w_shr[3]*to_logit(t_meta_ref) +\n",
        "          w_shr[4]*to_logit(t_emn_ref) +\n",
        "          w_shr[5]*to_logit(t_emp_ref))\n",
        "pt_shr = sigmoid(zt_shr).astype(np.float32)\n",
        "pd.DataFrame({id_col: test[id_col].values, target_col: pt_shr}).to_csv('submission_blend_gamma0p98_fullrefits_shrunk.csv', index=False)\n",
        "\n",
        "# Promote full-refit gamma-best\n",
        "pd.read_csv('submission_blend_gamma0p98_fullrefits.csv').to_csv('submission.csv', index=False)\n",
        "print('Promoted submission_blend_gamma0p98_fullrefits.csv to submission.csv')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rounds selection using last block valid: tr=2399 va=479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MiniLM] picked rounds (last-block ES): best_iter=45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MiniLM] seed 42 done | mean=0.4681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MiniLM] seed 1337 done | mean=0.4672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MiniLM] seed 2025 done | mean=0.4684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MiniLM] seed 614 done | mean=0.4697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MiniLM] seed 2718 done | mean=0.4680\n[MiniLM] bag mean=0.4683 | num_rounds=45 | seeds=[42, 1337, 2025, 614, 2718]\nSaved test_xgb_emb_minilm_fullbag.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta-only] picked rounds (last-block ES): best_iter=595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta-only] seed 42 done | mean=0.4263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta-only] seed 1337 done | mean=0.4285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta-only] seed 2025 done | mean=0.4262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta-only] seed 614 done | mean=0.4253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta-only] seed 2718 done | mean=0.4256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta-only] bag mean=0.4264 | num_rounds=595 | seeds=[42, 1337, 2025, 614, 2718]\nSaved test_xgb_meta_fullbag.npy\nPromoted submission_blend_gamma0p98_fullrefits.csv to submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "23a4a2b2-2d96-4942-abcf-580c3c1c4722",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S32c: Time-decayed LR_nosub + meta_v1 (title+request_text only), L2 saga; cache OOF/test\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body_no_leak(df):\n",
        "    # Avoid edit_aware; prefer request_text\n",
        "    if 'request_text' in df.columns:\n",
        "        return df['request_text'].fillna('').astype(str)\n",
        "    return df.get('request_text', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def build_text(df):\n",
        "    return (get_title(df) + '\\n' + get_body_no_leak(df)).astype(str)\n",
        "\n",
        "txt_tr = build_text(train); txt_te = build_text(test)\n",
        "\n",
        "# Load meta_v1 features\n",
        "Meta_tr = np.load('meta_v1_tr.npy').astype(np.float32)\n",
        "Meta_te = np.load('meta_v1_te.npy').astype(np.float32)\n",
        "\n",
        "# 6-block forward-chaining folds\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "block_id = np.zeros(n, dtype=np.int32)\n",
        "for bi in range(k):\n",
        "    block_id[np.array(blocks[bi])] = bi\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}')\n",
        "\n",
        "# Time-decay weights per training sample: w = gamma^(age), age = (k-1 - block_id)\n",
        "gamma = 0.98\n",
        "sample_w_all = (gamma ** ( (k-1) - block_id )).astype(np.float32)\n",
        "\n",
        "# TF-IDF views (slightly lighter caps for speed)\n",
        "word_params = dict(analyzer='word', ngram_range=(1,3), lowercase=True, min_df=2, max_features=200_000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "char_params = dict(analyzer='char_wb', ngram_range=(2,6), lowercase=True, min_df=2, max_features=200_000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "\n",
        "C = 1.0\n",
        "tC = time.time()\n",
        "oof = np.zeros(n, dtype=np.float32)\n",
        "te_parts = []\n",
        "for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    t0 = time.time()\n",
        "    tr_text = txt_tr.iloc[tr_idx]; va_text = txt_tr.iloc[va_idx]\n",
        "    tf_w = TfidfVectorizer(**word_params)\n",
        "    Xw_tr = tf_w.fit_transform(tr_text); Xw_va = tf_w.transform(va_text); Xw_te = tf_w.transform(txt_te)\n",
        "    tf_c = TfidfVectorizer(**char_params)\n",
        "    Xc_tr = tf_c.fit_transform(tr_text); Xc_va = tf_c.transform(va_text); Xc_te = tf_c.transform(txt_te)\n",
        "    X_tr_text = hstack([Xw_tr, Xc_tr], format='csr')\n",
        "    X_va_text = hstack([Xw_va, Xc_va], format='csr')\n",
        "    X_te_text = hstack([Xw_te, Xc_te], format='csr')\n",
        "    X_tr = hstack([X_tr_text, csr_matrix(Meta_tr[tr_idx])], format='csr')\n",
        "    X_va = hstack([X_va_text, csr_matrix(Meta_tr[va_idx])], format='csr')\n",
        "    X_te = hstack([X_te_text, csr_matrix(Meta_te)], format='csr')\n",
        "    clf = LogisticRegression(penalty='l2', solver='saga', C=C, max_iter=2000, n_jobs=-1, verbose=0)\n",
        "    clf.fit(X_tr, y[tr_idx], sample_weight=sample_w_all[tr_idx])\n",
        "    va_pred = clf.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "    te_pred = clf.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "    oof[va_idx] = va_pred\n",
        "    te_parts.append(te_pred)\n",
        "    auc = roc_auc_score(y[va_idx], va_pred)\n",
        "    print(f'[LR_nosub+meta_decay C={C}, gamma={gamma}] Fold {fi} AUC: {auc:.5f} | {time.time()-t0:.1f}s | tr:{X_tr.shape[0]}x{X_tr.shape[1]}')\n",
        "    del Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, X_tr_text, X_va_text, X_te_text, X_tr, X_va, X_te, clf; gc.collect()\n",
        "auc_mask = roc_auc_score(y[mask], oof[mask])\n",
        "te_mean = np.mean(te_parts, axis=0).astype(np.float32)\n",
        "print(f'[LR_nosub+meta_decay] OOF AUC(validated): {auc_mask:.5f} | total {time.time()-tC:.1f}s')\n",
        "np.save('oof_lr_time_nosub_meta_decay.npy', oof.astype(np.float32))\n",
        "np.save('test_lr_time_nosub_meta_decay.npy', te_mean.astype(np.float32))\n",
        "print('Saved oof_lr_time_nosub_meta_decay.npy and test_lr_time_nosub_meta_decay.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "ffaa938b-2d7a-43f2-8c87-9b50946d1e68",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fix import for Path used in S33b\n",
        "from pathlib import Path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "42b0e9a4-9d72-4357-9074-67a85ed4c6ad",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S33b: Retune blends using updated full-bag MiniLM/Meta and option to use LR_nosub_meta_decay vs baseline\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# 6-block forward-chaining blocks and masks\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "n = len(train)\n",
        "mask_full = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    mask_full[np.array(blocks[i])] = True\n",
        "mask_last2 = np.zeros(n, dtype=bool)\n",
        "for i in [4,5]:\n",
        "    mask_last2[np.array(blocks[i])] = True\n",
        "print(f'Time-CV validated full: {mask_full.sum()}/{n} | last2: {mask_last2.sum()}')\n",
        "\n",
        "# Load base OOF\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy')\n",
        "o_lr_ns_base = np.load('oof_lr_time_nosub_meta.npy')\n",
        "o_lr_ns_decay = np.load('oof_lr_time_nosub_meta_decay.npy') if (Path('oof_lr_time_nosub_meta_decay.npy').exists()) else None\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_time_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta_time.npy')\n",
        "o_emn = np.load('oof_xgb_emb_meta_time.npy')\n",
        "o_emp = np.load('oof_xgb_emb_mpnet_time.npy')\n",
        "use_lr_decay_options = [False, True] if (o_lr_ns_decay is not None) else [False]\n",
        "\n",
        "# Optional LR_main+meta\n",
        "has_lr_mainm = Path('oof_lr_main_meta_time.npy').exists() and Path('test_lr_main_meta_time.npy').exists()\n",
        "if has_lr_mainm:\n",
        "    o_lr_mainm = np.load('oof_lr_main_meta_time.npy')\n",
        "    t_lr_mainm = np.load('test_lr_main_meta_time.npy')\n",
        "    print('Loaded LR_main+meta for blend consideration.')\n",
        "\n",
        "# Convert OOF to logits (we'll choose lr_ns variant inside search)\n",
        "z_lr_w = to_logit(o_lr_w)\n",
        "z_d1, z_d2, z_meta = to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)\n",
        "z_emn, z_emp = to_logit(o_emn), to_logit(o_emp)\n",
        "if has_lr_mainm:\n",
        "    z_lr_mainm = to_logit(o_lr_mainm)\n",
        "\n",
        "# Load test preds, preferring full-bag refits where available\n",
        "t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "t_lr_ns_base = np.load('test_lr_time_nosub_meta.npy')\n",
        "t_lr_ns_decay = np.load('test_lr_time_nosub_meta_decay.npy') if Path('test_lr_time_nosub_meta_decay.npy').exists() else None\n",
        "t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "t_meta = np.load('test_xgb_meta_fullbag.npy') if Path('test_xgb_meta_fullbag.npy').exists() else np.load('test_xgb_meta_time.npy')\n",
        "t_emn = np.load('test_xgb_emb_minilm_fullbag.npy') if Path('test_xgb_emb_minilm_fullbag.npy').exists() else np.load('test_xgb_emb_meta_time.npy')\n",
        "t_emp = np.load('test_xgb_emb_mpnet_fullbag.npy') if Path('test_xgb_emb_mpnet_fullbag.npy').exists() else np.load('test_xgb_emb_mpnet_time.npy')\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Grids (tight)\n",
        "g_grid = [0.96, 0.97, 0.98]\n",
        "meta_grid = [0.18, 0.20, 0.22]\n",
        "dense_tot_grid = [0.28, 0.30, 0.35]\n",
        "dense_split = [(0.6, 0.4), (0.7, 0.3), (0.8, 0.2)]\n",
        "emb_tot_grid = [0.24, 0.27, 0.30]\n",
        "emb_split = [(0.6, 0.4), (0.5, 0.5)]\n",
        "w_lrmain_grid = [0.0, 0.05, 0.08] if has_lr_mainm else [0.0]\n",
        "\n",
        "def search(mask, sample_weight=None):\n",
        "    best_auc, best_cfg, tried = -1.0, None, 0\n",
        "    for use_decay in use_lr_decay_options:\n",
        "        z_lr_ns = to_logit(o_lr_ns_decay) if (use_decay and (o_lr_ns_decay is not None)) else to_logit(o_lr_ns_base)\n",
        "        for g in g_grid:\n",
        "            z_lr_mix = (1.0 - g)*z_lr_w + g*z_lr_ns\n",
        "            for w_meta in meta_grid:\n",
        "                for d_tot in dense_tot_grid:\n",
        "                    for dv1, dv2 in dense_split:\n",
        "                        w_d1 = d_tot * dv1; w_d2 = d_tot * dv2\n",
        "                        for e_tot in emb_tot_grid:\n",
        "                            for emn_fr, emp_fr in emb_split:\n",
        "                                w_emn = e_tot * emn_fr; w_emp = e_tot * emp_fr\n",
        "                                rem = 1.0 - (w_meta + w_d1 + w_d2 + w_emn + w_emp)\n",
        "                                if rem <= 0: continue\n",
        "                                for w_lrmain in w_lrmain_grid:\n",
        "                                    if w_lrmain > rem: continue\n",
        "                                    w_lr = rem - w_lrmain\n",
        "                                    if w_lr < 0.25: continue\n",
        "                                    z_oof = (w_lr*z_lr_mix + w_d1*z_d1 + w_d2*z_d2 + w_meta*z_meta + w_emn*z_emn + w_emp*z_emp)\n",
        "                                    if has_lr_mainm and w_lrmain > 0:\n",
        "                                        z_oof = z_oof + w_lrmain*z_lr_mainm\n",
        "                                    auc = roc_auc_score(y[mask], z_oof[mask], sample_weight=(sample_weight[mask] if sample_weight is not None else None))\n",
        "                                    tried += 1\n",
        "                                    if auc > best_auc:\n",
        "                                        best_auc = auc\n",
        "                                        best_cfg = dict(use_decay=use_decay, g=float(g), w_lr=float(w_lr), w_d1=float(w_d1), w_d2=float(w_d2), w_meta=float(w_meta),\n",
        "                                                        w_emn=float(w_emn), w_emp=float(w_emp), w_lrmain=float(w_lrmain))\n",
        "    return best_auc, best_cfg, tried\n",
        "\n",
        "# 1) Full-mask\n",
        "auc_full, cfg_full, tried_full = search(mask_full)\n",
        "print(f'[Full] tried={tried_full} | best OOF(z) AUC={auc_full:.5f} | cfg={cfg_full}')\n",
        "\n",
        "# 2) Last-2\n",
        "auc_last2, cfg_last2, tried_last2 = search(mask_last2)\n",
        "print(f'[Last2] tried={tried_last2} | best OOF(z,last2) AUC={auc_last2:.5f} | cfg={cfg_last2}')\n",
        "\n",
        "# 3) Gamma-decayed\n",
        "best_gamma, best_auc_g, best_cfg_g = None, -1.0, None\n",
        "for gamma in [0.95, 0.98]:\n",
        "    w = np.zeros(n, dtype=np.float64)\n",
        "    for bi in range(1, k):\n",
        "        age = (k - 1) - bi\n",
        "        w[np.array(blocks[bi])] = (gamma ** age)\n",
        "    auc_g, cfg_g, _ = search(mask_full, sample_weight=w)\n",
        "    print(f'[Gamma {gamma}] best OOF(z,weighted) AUC={auc_g:.5f}')\n",
        "    if auc_g > best_auc_g:\n",
        "        best_auc_g, best_cfg_g, best_gamma = auc_g, cfg_g, gamma\n",
        "print(f'[Gamma-best] gamma={best_gamma} | AUC={best_auc_g:.5f} | cfg={best_cfg_g}')\n",
        "\n",
        "def build_and_save(tag, cfg):\n",
        "    use_decay = cfg['use_decay']\n",
        "    tz_lr_ns = to_logit(t_lr_ns_decay if (use_decay and (t_lr_ns_decay is not None)) else t_lr_ns_base)\n",
        "    tz_lr_w = to_logit(t_lr_w)\n",
        "    tz_lr_mix = (1.0 - cfg['g'])*tz_lr_w + cfg['g']*tz_lr_ns\n",
        "    z_comps = [cfg['w_lr']*tz_lr_mix,\n",
        "               cfg['w_d1']*to_logit(t_d1),\n",
        "               cfg['w_d2']*to_logit(t_d2),\n",
        "               cfg['w_meta']*to_logit(t_meta),\n",
        "               cfg['w_emn']*to_logit(t_emn),\n",
        "               cfg['w_emp']*to_logit(t_emp)]\n",
        "    w_list = [cfg['w_lr'], cfg['w_d1'], cfg['w_d2'], cfg['w_meta'], cfg['w_emn'], cfg['w_emp']]\n",
        "    if has_lr_mainm and cfg['w_lrmain'] > 0:\n",
        "        z_comps.append(cfg['w_lrmain']*to_logit(t_lr_mainm))\n",
        "        w_list.append(cfg['w_lrmain'])\n",
        "    zt = np.sum(z_comps, axis=0)\n",
        "    pt = sigmoid(zt).astype(np.float32)\n",
        "    pd.DataFrame({id_col: ids, target_col: pt}).to_csv(f'submission_reblend_{tag}.csv', index=False)\n",
        "    # Shrink hedge 15%\n",
        "    w_vec = np.array(w_list, dtype=np.float64)\n",
        "    w_eq = np.ones_like(w_vec)/len(w_vec)\n",
        "    alpha = 0.15\n",
        "    w_shr = ((1.0 - alpha)*w_vec + alpha*w_eq); w_shr = (w_shr / w_shr.sum()).astype(np.float64)\n",
        "    # Rebuild z with shrunk weights\n",
        "    comps_logits = [to_logit(t_lr_mix := ( (1.0 - cfg['g'])*to_logit(t_lr_w) + cfg['g']*tz_lr_ns ))]  # placeholder, not used directly\n",
        "    comp_logits = [tz_lr_mix, to_logit(t_d1), to_logit(t_d2), to_logit(t_meta), to_logit(t_emn), to_logit(t_emp)]\n",
        "    if has_lr_mainm and cfg['w_lrmain'] > 0:\n",
        "        comp_logits.append(to_logit(t_lr_mainm))\n",
        "    zt_shr = 0.0\n",
        "    for wi, zi in zip(w_shr, comp_logits):\n",
        "        zt_shr += wi*zi\n",
        "    pt_shr = sigmoid(zt_shr).astype(np.float32)\n",
        "    pd.DataFrame({id_col: ids, target_col: pt_shr}).to_csv(f'submission_reblend_{tag}_shrunk.csv', index=False)\n",
        "\n",
        "build_and_save('full', cfg_full)\n",
        "build_and_save('last2', cfg_last2)\n",
        "build_and_save(f'gamma{best_gamma:.2f}'.replace('.','p'), best_cfg_g)\n",
        "\n",
        "# Promote gamma-best\n",
        "prim = f'submission_reblend_gamma{best_gamma:.2f}'.replace('.','p') + '.csv'\n",
        "pd.read_csv(prim).to_csv('submission.csv', index=False)\n",
        "print(f'Promoted {prim} to submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "8bbf6655-fd99-4106-8228-937af7cae6e3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S37: TF-IDF(word 1-2) -> SVD(300) per-fold, + meta_v1, StandardScaler, models: LR and XGB; cache OOF/test\n",
        "import numpy as np, pandas as pd, time, gc, sys\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import xgboost as xgb\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body_no_leak(df):\n",
        "    if 'request_text' in df.columns:\n",
        "        return df['request_text'].fillna('').astype(str)\n",
        "    return df.get('request_text', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def build_text(df):\n",
        "    return (get_title(df) + '\\n' + get_body_no_leak(df)).astype(str)\n",
        "\n",
        "txt_tr = build_text(train)\n",
        "txt_te = build_text(test)\n",
        "\n",
        "# Time-aware 6-block forward-chaining\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask_val = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask_val[va_idx] = True\n",
        "print(f'Time-CV folds={len(folds)}; validated {mask_val.sum()}/{n}')\n",
        "\n",
        "Meta_tr = np.load('meta_v1_tr.npy').astype(np.float32)\n",
        "Meta_te = np.load('meta_v1_te.npy').astype(np.float32)\n",
        "\n",
        "word_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=2, max_features=300_000,\n",
        "                   sublinear_tf=True, smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "\n",
        "def run_svd_base(model_type='lr', smoke=True, seed=42, tag='svd_word300_meta'):\n",
        "    start = time.time()\n",
        "    use_folds = folds[:2] if smoke else folds\n",
        "    oof = np.zeros(n, dtype=np.float32)\n",
        "    te_parts = []\n",
        "    fold_times = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(use_folds, 1):\n",
        "        t0 = time.time()\n",
        "        tr_text = txt_tr.iloc[tr_idx]; va_text = txt_tr.iloc[va_idx]\n",
        "        tf_w = TfidfVectorizer(**word_params)\n",
        "        Xw_tr = tf_w.fit_transform(tr_text)\n",
        "        Xw_va = tf_w.transform(va_text)\n",
        "        Xw_te = tf_w.transform(txt_te)\n",
        "        svd = TruncatedSVD(n_components=300, n_iter=5, random_state=seed, algorithm='randomized')\n",
        "        Z_tr = svd.fit_transform(Xw_tr).astype(np.float32)\n",
        "        Z_va = svd.transform(Xw_va).astype(np.float32)\n",
        "        Z_te = svd.transform(Xw_te).astype(np.float32)\n",
        "        # Concatenate meta and standardize together\n",
        "        M_tr = Meta_tr[tr_idx]\n",
        "        M_va = Meta_tr[va_idx]\n",
        "        M_te = Meta_te\n",
        "        X_tr = np.hstack([Z_tr, M_tr]).astype(np.float32)\n",
        "        X_va = np.hstack([Z_va, M_va]).astype(np.float32)\n",
        "        X_te = np.hstack([Z_te, M_te]).astype(np.float32)\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "        X_tr = scaler.fit_transform(X_tr).astype(np.float32)\n",
        "        X_va = scaler.transform(X_va).astype(np.float32)\n",
        "        X_te_s = scaler.transform(X_te).astype(np.float32)\n",
        "        if model_type == 'lr':\n",
        "            clf = LogisticRegression(solver='saga', penalty='l2', C=1.0, max_iter=3000, n_jobs=-1, verbose=0, random_state=seed)\n",
        "            clf.fit(X_tr, y[tr_idx])\n",
        "            va_pred = clf.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "            te_pred = clf.predict_proba(X_te_s)[:,1].astype(np.float32)\n",
        "        elif model_type == 'xgb':\n",
        "            dtr = xgb.DMatrix(X_tr, label=y[tr_idx])\n",
        "            dva = xgb.DMatrix(X_va, label=y[va_idx])\n",
        "            dte = xgb.DMatrix(X_te_s)\n",
        "            pos = float((y[tr_idx] == 1).sum()); neg = float((y[tr_idx] == 0).sum())\n",
        "            spw = (neg / max(pos, 1.0)) if pos > 0 else 1.0\n",
        "            params = dict(objective='binary:logistic', eval_metric='auc', max_depth=5, eta=0.05, subsample=0.8,\n",
        "                          colsample_bytree=0.8, min_child_weight=4, reg_alpha=0.2, reg_lambda=2.5,\n",
        "                          device='cuda', tree_method='hist', seed=seed, scale_pos_weight=spw)\n",
        "            booster = xgb.train(params, dtr, num_boost_round=4000, evals=[(dva, 'valid')],\n",
        "                                early_stopping_rounds=100, verbose_eval=False)\n",
        "            va_pred = booster.predict(dva).astype(np.float32)\n",
        "            te_pred = booster.predict(dte, iteration_range=(0, booster.best_iteration+1 if booster.best_iteration is not None else 0)).astype(np.float32)\n",
        "        else:\n",
        "            raise ValueError('model_type must be lr or xgb')\n",
        "        oof[va_idx] = va_pred\n",
        "        te_parts.append(te_pred)\n",
        "        auc = roc_auc_score(y[va_idx], va_pred)\n",
        "        dt = time.time()-t0; fold_times.append(dt)\n",
        "        print(f'[SVD {model_type}] Fold {fi}/{len(use_folds)} AUC={auc:.5f} | {dt:.1f}s | TF:{Xw_tr.shape[1]} SVD:{Z_tr.shape[1]} feats:{X_tr.shape[1]}')\n",
        "        del tf_w, Xw_tr, Xw_va, Xw_te, svd, Z_tr, Z_va, Z_te, M_tr, M_va, M_te, X_tr, X_va, X_te, X_te_s\n",
        "        if model_type == 'xgb':\n",
        "            del dtr, dva, dte, booster\n",
        "        gc.collect()\n",
        "    auc_mask = roc_auc_score(y[mask_val], oof[mask_val]) if not smoke else roc_auc_score(y[use_folds[0][1]], oof[use_folds[0][1]])\n",
        "    te_mean = np.mean(te_parts, axis=0).astype(np.float32)\n",
        "    mode_tag = f'{model_type}_smoke' if smoke else model_type\n",
        "    oof_path = f'oof_{mode_tag}_{tag}.npy'\n",
        "    te_path = f'test_{mode_tag}_{tag}.npy'\n",
        "    np.save(oof_path, oof.astype(np.float32))\n",
        "    np.save(te_path, te_mean)\n",
        "    print(f'[SVD {model_type}] DONE | OOF(valid mask={\"full\" if not smoke else \"1-fold\"}) AUC={auc_mask:.5f} | folds={len(use_folds)} | {time.time()-start:.1f}s')\n",
        "    print(f'Saved {oof_path} and {te_path}')\n",
        "\n",
        "# Example next steps (not executed here):\n",
        "# 1) Smoke test LR: run_svd_base(model_type='lr', smoke=True, seed=42, tag='svd_word300_meta')\n",
        "# 2) If good, full CV LR: run_svd_base(model_type='lr', smoke=False, seed=42, tag='svd_word300_meta')\n",
        "# 3) Train XGB variant: run_svd_base(model_type='xgb', smoke=False, seed=42, tag='svd_word300_meta')\n",
        "# Then add to blend tuner and retune weights."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "1f47500d-5cb8-4901-b4dc-cb938d47aaab",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S37-run: 2-fold smoke test for SVD base with LR (+meta); expect OOF ~0.65+ on first val fold\n",
        "try:\n",
        "    run_svd_base(model_type='lr', smoke=True, seed=42, tag='svd_word300_meta')\n",
        "except Exception as e:\n",
        "    import traceback, sys\n",
        "    print('Error during SVD LR smoke test:', e)\n",
        "    traceback.print_exc(file=sys.stdout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "d27f1ad5-259d-4edb-b7c4-b2bf0274da26",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S37-run-full: Full 5-fold CV for SVD base with LR and XGB (+meta); cache OOF/test\n",
        "try:\n",
        "    print('=== Running SVD LR full CV ===')\n",
        "    run_svd_base(model_type='lr', smoke=False, seed=42, tag='svd_word300_meta')\n",
        "    print('=== Running SVD XGB full CV ===')\n",
        "    run_svd_base(model_type='xgb', smoke=False, seed=42, tag='svd_word300_meta')\n",
        "except Exception as e:\n",
        "    import traceback, sys\n",
        "    print('Error during SVD full runs:', e)\n",
        "    traceback.print_exc(file=sys.stdout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "990d8cf6-ea64-4c90-8706-4df3e7327811",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S37b: Reblend including new SVD bases; allow downweighting/removing Dense; promote gamma-best\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from pathlib import Path\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Time blocks and masks\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "n = len(train)\n",
        "mask_full = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    mask_full[np.array(blocks[i])] = True\n",
        "mask_last2 = np.zeros(n, dtype=bool)\n",
        "for i in [4,5]:\n",
        "    mask_last2[np.array(blocks[i])] = True\n",
        "print(f'Time-CV validated full: {mask_full.sum()}/{n} | last2: {mask_last2.sum()}')\n",
        "\n",
        "# Load OOF preds\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy')\n",
        "o_lr_ns_base = np.load('oof_lr_time_nosub_meta.npy')\n",
        "o_lr_ns_decay = np.load('oof_lr_time_nosub_meta_decay.npy') if Path('oof_lr_time_nosub_meta_decay.npy').exists() else None\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_time_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta_time.npy')\n",
        "o_emn = np.load('oof_xgb_emb_meta_time.npy')\n",
        "o_emp = np.load('oof_xgb_emb_mpnet_time.npy')\n",
        "# New SVD bases\n",
        "o_svd_lr = np.load('oof_lr_svd_word300_meta.npy')\n",
        "o_svd_xgb = np.load('oof_xgb_svd_word300_meta.npy')\n",
        "\n",
        "# Optional LR_main+meta (kept optional but likely 0 weight)\n",
        "has_lr_mainm = Path('oof_lr_main_meta_time.npy').exists() and Path('test_lr_main_meta_time.npy').exists()\n",
        "if has_lr_mainm:\n",
        "    o_lr_mainm = np.load('oof_lr_main_meta_time.npy')\n",
        "    print('Loaded LR_main+meta for blend consideration.')\n",
        "\n",
        "# Convert OOF to logits where fixed\n",
        "z_lr_w = to_logit(o_lr_w)\n",
        "z_d1, z_d2, z_meta = to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)\n",
        "z_emn, z_emp = to_logit(o_emn), to_logit(o_emp)\n",
        "z_svd_lr, z_svd_xgb = to_logit(o_svd_lr), to_logit(o_svd_xgb)\n",
        "if has_lr_mainm:\n",
        "    z_lr_mainm = to_logit(o_lr_mainm)\n",
        "\n",
        "# Load test preds (prefer full-bag refits where available for embeddings/meta); SVD ones from current run\n",
        "t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "t_lr_ns_base = np.load('test_lr_time_nosub_meta.npy')\n",
        "t_lr_ns_decay = np.load('test_lr_time_nosub_meta_decay.npy') if Path('test_lr_time_nosub_meta_decay.npy').exists() else None\n",
        "t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "t_meta = np.load('test_xgb_meta_fullbag.npy') if Path('test_xgb_meta_fullbag.npy').exists() else np.load('test_xgb_meta_time.npy')\n",
        "t_emn = np.load('test_xgb_emb_minilm_fullbag.npy') if Path('test_xgb_emb_minilm_fullbag.npy').exists() else np.load('test_xgb_emb_meta_time.npy')\n",
        "t_emp = np.load('test_xgb_emb_mpnet_fullbag.npy') if Path('test_xgb_emb_mpnet_fullbag.npy').exists() else np.load('test_xgb_emb_mpnet_time.npy')\n",
        "t_svd_lr = np.load('test_lr_svd_word300_meta.npy')\n",
        "t_svd_xgb = np.load('test_xgb_svd_word300_meta.npy')\n",
        "if has_lr_mainm:\n",
        "    t_lr_mainm = np.load('test_lr_main_meta_time.npy')\n",
        "\n",
        "# Grids\n",
        "g_grid = [0.96, 0.97, 0.98]\n",
        "meta_grid = [0.18, 0.20, 0.22]\n",
        "dense_tot_grid = [0.0, 0.15, 0.22, 0.28]  # allow turning Dense off\n",
        "dense_split = [(0.8, 0.2), (0.7, 0.3), (0.6, 0.4)]\n",
        "emb_tot_grid = [0.20, 0.24, 0.27, 0.30]\n",
        "emb_split = [(0.6, 0.4), (0.5, 0.5)]\n",
        "svd_tot_grid = [0.0, 0.05, 0.10, 0.15, 0.20]\n",
        "svd_split = [(0.7, 0.3), (0.5, 0.5)]  # (svd_lr, svd_xgb)\n",
        "w_lrmain_grid = [0.0, 0.05] if has_lr_mainm else [0.0]\n",
        "use_lr_decay_options = [False, True] if (o_lr_ns_decay is not None) else [False]\n",
        "\n",
        "def search(mask, sample_weight=None):\n",
        "    best_auc, best_cfg, tried = -1.0, None, 0\n",
        "    for use_decay in use_lr_decay_options:\n",
        "        z_lr_ns = to_logit(o_lr_ns_decay) if (use_decay and (o_lr_ns_decay is not None)) else to_logit(o_lr_ns_base)\n",
        "        for g in g_grid:\n",
        "            z_lr_mix = (1.0 - g)*z_lr_w + g*z_lr_ns\n",
        "            for w_meta in meta_grid:\n",
        "                for d_tot in dense_tot_grid:\n",
        "                    for dv1, dv2 in dense_split:\n",
        "                        w_d1 = d_tot * dv1; w_d2 = d_tot * dv2\n",
        "                        for e_tot in emb_tot_grid:\n",
        "                            for emn_fr, emp_fr in emb_split:\n",
        "                                w_emn = e_tot * emn_fr; w_emp = e_tot * emp_fr\n",
        "                                for s_tot in svd_tot_grid:\n",
        "                                    for s_lr_fr, s_xgb_fr in svd_split:\n",
        "                                        w_svd_lr = s_tot * s_lr_fr; w_svd_xgb = s_tot * s_xgb_fr\n",
        "                                        rem = 1.0 - (w_meta + w_d1 + w_d2 + w_emn + w_emp + w_svd_lr + w_svd_xgb)\n",
        "                                        if rem <= 0: continue\n",
        "                                        for w_lrmain in w_lrmain_grid:\n",
        "                                            if w_lrmain > rem: continue\n",
        "                                            w_lr = rem - w_lrmain\n",
        "                                            if w_lr < 0.20:  # keep LR_mix reasonably weighted\n",
        "                                                continue\n",
        "                                            z_oof = (w_lr*z_lr_mix +\n",
        "                                                     w_d1*z_d1 + w_d2*z_d2 +\n",
        "                                                     w_meta*z_meta +\n",
        "                                                     w_emn*z_emn + w_emp*z_emp +\n",
        "                                                     w_svd_lr*z_svd_lr + w_svd_xgb*z_svd_xgb)\n",
        "                                            if has_lr_mainm and w_lrmain > 0:\n",
        "                                                z_oof = z_oof + w_lrmain*z_lr_mainm\n",
        "                                            auc = roc_auc_score(y[mask], z_oof[mask], sample_weight=(sample_weight[mask] if sample_weight is not None else None))\n",
        "                                            tried += 1\n",
        "                                            if auc > best_auc:\n",
        "                                                best_auc = auc\n",
        "                                                best_cfg = dict(use_decay=use_decay, g=float(g),\n",
        "                                                                w_lr=float(w_lr), w_d1=float(w_d1), w_d2=float(w_d2),\n",
        "                                                                w_meta=float(w_meta), w_emn=float(w_emn), w_emp=float(w_emp),\n",
        "                                                                w_svd_lr=float(w_svd_lr), w_svd_xgb=float(w_svd_xgb),\n",
        "                                                                w_lrmain=float(w_lrmain))\n",
        "    return best_auc, best_cfg, tried\n",
        "\n",
        "# 1) Full-mask\n",
        "auc_full, cfg_full, tried_full = search(mask_full)\n",
        "print(f'[Full] tried={tried_full} | best OOF(z) AUC={auc_full:.5f} | cfg={cfg_full}')\n",
        "\n",
        "# 2) Last-2\n",
        "auc_last2, cfg_last2, tried_last2 = search(mask_last2)\n",
        "print(f'[Last2] tried={tried_last2} | best OOF(z,last2) AUC={auc_last2:.5f} | cfg={cfg_last2}')\n",
        "\n",
        "# 3) Gamma-decayed\n",
        "best_gamma, best_auc_g, best_cfg_g = None, -1.0, None\n",
        "for gamma in [0.95, 0.98]:\n",
        "    w = np.zeros(n, dtype=np.float64)\n",
        "    for bi in range(1, k):\n",
        "        age = (k - 1) - bi\n",
        "        w[np.array(blocks[bi])] = (gamma ** age)\n",
        "    auc_g, cfg_g, _ = search(mask_full, sample_weight=w)\n",
        "    print(f'[Gamma {gamma}] best OOF(z,weighted) AUC={auc_g:.5f}')\n",
        "    if auc_g > best_auc_g:\n",
        "        best_auc_g, best_cfg_g, best_gamma = auc_g, cfg_g, gamma\n",
        "print(f'[Gamma-best] gamma={best_gamma} | AUC={best_auc_g:.5f} | cfg={best_cfg_g}')\n",
        "\n",
        "def build_and_save(tag, cfg):\n",
        "    use_decay = cfg['use_decay']\n",
        "    tz_lr_ns = to_logit(t_lr_ns_decay if (use_decay and (t_lr_ns_decay is not None)) else t_lr_ns_base)\n",
        "    tz_lr_w = to_logit(t_lr_w)\n",
        "    tz_lr_mix = (1.0 - cfg['g'])*tz_lr_w + cfg['g']*tz_lr_ns\n",
        "    z_parts = [\n",
        "        cfg['w_lr']*tz_lr_mix,\n",
        "        cfg['w_d1']*to_logit(t_d1),\n",
        "        cfg['w_d2']*to_logit(t_d2),\n",
        "        cfg['w_meta']*to_logit(t_meta),\n",
        "        cfg['w_emn']*to_logit(t_emn),\n",
        "        cfg['w_emp']*to_logit(t_emp),\n",
        "        cfg['w_svd_lr']*to_logit(t_svd_lr),\n",
        "        cfg['w_svd_xgb']*to_logit(t_svd_xgb)\n",
        "    ]\n",
        "    w_list = [cfg['w_lr'], cfg['w_d1'], cfg['w_d2'], cfg['w_meta'], cfg['w_emn'], cfg['w_emp'], cfg['w_svd_lr'], cfg['w_svd_xgb']]\n",
        "    if has_lr_mainm and cfg['w_lrmain'] > 0:\n",
        "        z_parts.append(cfg['w_lrmain']*to_logit(t_lr_mainm))\n",
        "        w_list.append(cfg['w_lrmain'])\n",
        "    zt = np.sum(z_parts, axis=0)\n",
        "    pt = sigmoid(zt).astype(np.float32)\n",
        "    pd.DataFrame({id_col: ids, target_col: pt}).to_csv(f'submission_reblend_svd_{tag}.csv', index=False)\n",
        "    # 15% shrink hedge\n",
        "    w_vec = np.array(w_list, dtype=np.float64)\n",
        "    w_eq = np.ones_like(w_vec)/len(w_vec)\n",
        "    alpha = 0.15\n",
        "    w_shr = ((1.0 - alpha)*w_vec + alpha*w_eq); w_shr = (w_shr / w_shr.sum()).astype(np.float64)\n",
        "    comp_logits = [tz_lr_mix, to_logit(t_d1), to_logit(t_d2), to_logit(t_meta), to_logit(t_emn), to_logit(t_emp), to_logit(t_svd_lr), to_logit(t_svd_xgb)]\n",
        "    if has_lr_mainm and cfg['w_lrmain'] > 0:\n",
        "        comp_logits.append(to_logit(t_lr_mainm))\n",
        "    zt_shr = 0.0\n",
        "    for wi, zi in zip(w_shr, comp_logits):\n",
        "        zt_shr += wi*zi\n",
        "    pt_shr = sigmoid(zt_shr).astype(np.float32)\n",
        "    pd.DataFrame({id_col: ids, target_col: pt_shr}).to_csv(f'submission_reblend_svd_{tag}_shrunk.csv', index=False)\n",
        "\n",
        "build_and_save('full', cfg_full)\n",
        "build_and_save('last2', cfg_last2)\n",
        "build_and_save(f'gamma{best_gamma:.2f}'.replace('.','p'), best_cfg_g)\n",
        "\n",
        "# Promote gamma-best\n",
        "prim = f'submission_reblend_svd_gamma{best_gamma:.2f}'.replace('.','p') + '.csv'\n",
        "pd.read_csv(prim).to_csv('submission.csv', index=False)\n",
        "print(f'Promoted {prim} to submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "8b7d4a8f-92d9-4850-bf95-25baeb170433",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S37c: Dual-view TF-IDF (word 1-2 + char_wb 3-5) -> SVD (224+160) per-fold + meta_v1 -> StandardScaler -> LR/XGB\n",
        "import numpy as np, pandas as pd, time, gc, sys\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import xgboost as xgb\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body_no_leak(df):\n",
        "    if 'request_text' in df.columns:\n",
        "        return df['request_text'].fillna('').astype(str)\n",
        "    return df.get('request_text', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def build_text(df):\n",
        "    return (get_title(df) + '\\n' + get_body_no_leak(df)).astype(str)\n",
        "\n",
        "txt_tr = build_text(train)\n",
        "txt_te = build_text(test)\n",
        "\n",
        "# Time-aware 6-block forward-chaining\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask_val = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask_val[va_idx] = True\n",
        "print(f'Time-CV folds={len(folds)}; validated {mask_val.sum()}/{n}')\n",
        "\n",
        "Meta_tr = np.load('meta_v1_tr.npy').astype(np.float32)\n",
        "Meta_te = np.load('meta_v1_te.npy').astype(np.float32)\n",
        "\n",
        "word_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=2, max_features=300_000,\n",
        "                   stop_words='english', sublinear_tf=True, smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "char_params = dict(analyzer='char_wb', ngram_range=(3,5), lowercase=True, min_df=2, max_features=200_000,\n",
        "                   stop_words='english', sublinear_tf=True, smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "\n",
        "def run_svd_dual(model_type='xgb', smoke=True, seed=42, tag='svd_word224_char160_meta', n_iter_svd=7, n_comp_word=224, n_comp_char=160):\n",
        "    start = time.time()\n",
        "    use_folds = folds[:2] if smoke else folds\n",
        "    oof = np.zeros(n, dtype=np.float32)\n",
        "    te_parts = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(use_folds, 1):\n",
        "        t0 = time.time()\n",
        "        tr_text = txt_tr.iloc[tr_idx]; va_text = txt_tr.iloc[va_idx]\n",
        "        # Word view\n",
        "        tf_w = TfidfVectorizer(**word_params)\n",
        "        Xw_tr = tf_w.fit_transform(tr_text); Xw_va = tf_w.transform(va_text); Xw_te = tf_w.transform(txt_te)\n",
        "        svd_w = TruncatedSVD(n_components=n_comp_word, n_iter=n_iter_svd, random_state=seed, algorithm='randomized')\n",
        "        Zw_tr = svd_w.fit_transform(Xw_tr).astype(np.float32)\n",
        "        Zw_va = svd_w.transform(Xw_va).astype(np.float32)\n",
        "        Zw_te = svd_w.transform(Xw_te).astype(np.float32)\n",
        "        # Char view\n",
        "        tf_c = TfidfVectorizer(**char_params)\n",
        "        Xc_tr = tf_c.fit_transform(tr_text); Xc_va = tf_c.transform(va_text); Xc_te = tf_c.transform(txt_te)\n",
        "        svd_c = TruncatedSVD(n_components=n_comp_char, n_iter=n_iter_svd, random_state=seed+1, algorithm='randomized')\n",
        "        Zc_tr = svd_c.fit_transform(Xc_tr).astype(np.float32)\n",
        "        Zc_va = svd_c.transform(Xc_va).astype(np.float32)\n",
        "        Zc_te = svd_c.transform(Xc_te).astype(np.float32)\n",
        "        # Concat views + meta\n",
        "        Z_tr = np.hstack([Zw_tr, Zc_tr]).astype(np.float32)\n",
        "        Z_va = np.hstack([Zw_va, Zc_va]).astype(np.float32)\n",
        "        Z_te = np.hstack([Zw_te, Zc_te]).astype(np.float32)\n",
        "        M_tr = Meta_tr[tr_idx]; M_va = Meta_tr[va_idx]; M_te = Meta_te\n",
        "        X_tr = np.hstack([Z_tr, M_tr]).astype(np.float32)\n",
        "        X_va = np.hstack([Z_va, M_va]).astype(np.float32)\n",
        "        X_te = np.hstack([Z_te, M_te]).astype(np.float32)\n",
        "        # Standardize\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "        X_tr = scaler.fit_transform(X_tr).astype(np.float32)\n",
        "        X_va = scaler.transform(X_va).astype(np.float32)\n",
        "        X_te_s = scaler.transform(X_te).astype(np.float32)\n",
        "        if model_type == 'lr':\n",
        "            clf = LogisticRegression(solver='saga', penalty='l2', C=1.0, max_iter=3000, n_jobs=-1, verbose=0, random_state=seed)\n",
        "            clf.fit(X_tr, y[tr_idx])\n",
        "            va_pred = clf.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "            te_pred = clf.predict_proba(X_te_s)[:,1].astype(np.float32)\n",
        "        elif model_type == 'xgb':\n",
        "            dtr = xgb.DMatrix(X_tr, label=y[tr_idx])\n",
        "            dva = xgb.DMatrix(X_va, label=y[va_idx])\n",
        "            dte = xgb.DMatrix(X_te_s)\n",
        "            pos = float((y[tr_idx] == 1).sum()); neg = float((y[tr_idx] == 0).sum())\n",
        "            spw = (neg / max(pos, 1.0)) if pos > 0 else 1.0\n",
        "            params = dict(objective='binary:logistic', eval_metric='auc', max_depth=5, eta=0.05, subsample=0.8,\n",
        "                          colsample_bytree=0.8, min_child_weight=5, reg_alpha=0.3, reg_lambda=3.0,\n",
        "                          device='cuda', tree_method='hist', seed=seed, scale_pos_weight=spw)\n",
        "            booster = xgb.train(params, dtr, num_boost_round=4000, evals=[(dva, 'valid')],\n",
        "                                early_stopping_rounds=100, verbose_eval=False)\n",
        "            va_pred = booster.predict(dva).astype(np.float32)\n",
        "            te_pred = booster.predict(dte, iteration_range=(0, booster.best_iteration+1 if booster.best_iteration is not None else 0)).astype(np.float32)\n",
        "            del dtr, dva, dte, booster\n",
        "        else:\n",
        "            raise ValueError('model_type must be lr or xgb')\n",
        "        oof[va_idx] = va_pred\n",
        "        te_parts.append(te_pred)\n",
        "        auc = roc_auc_score(y[va_idx], va_pred)\n",
        "        print(f'[SVDdual {model_type}] Fold {fi}/{len(use_folds)} AUC={auc:.5f} | {time.time()-t0:.1f}s | wordTF:{Xw_tr.shape[1]} charTF:{Xc_tr.shape[1]} Z:{Z_tr.shape[1]} + meta->{X_tr.shape[1]}')\n",
        "        # Cleanup\n",
        "        del tf_w, Xw_tr, Xw_va, Xw_te, svd_w, Zw_tr, Zw_va, Zw_te\n",
        "        del tf_c, Xc_tr, Xc_va, Xc_te, svd_c, Zc_tr, Zc_va, Zc_te\n",
        "        del Z_tr, Z_va, Z_te, M_tr, M_va, M_te, X_tr, X_va, X_te, X_te_s\n",
        "        gc.collect()\n",
        "    auc_mask = roc_auc_score(y[mask_val], oof[mask_val]) if not smoke else roc_auc_score(y[use_folds[0][1]], oof[use_folds[0][1]])\n",
        "    te_mean = np.mean(te_parts, axis=0).astype(np.float32)\n",
        "    mode_tag = f'{model_type}_smoke' if smoke else model_type\n",
        "    oof_path = f'oof_{mode_tag}_{tag}.npy'; te_path = f'test_{mode_tag}_{tag}.npy'\n",
        "    np.save(oof_path, oof.astype(np.float32)); np.save(te_path, te_mean)\n",
        "    print(f'[SVDdual {model_type}] DONE | OOF(valid mask={\"full\" if not smoke else \"1-fold\"}) AUC={auc_mask:.5f} | folds={len(use_folds)} | {time.time()-start:.1f}s')\n",
        "    print(f'Saved {oof_path} and {te_path}')\n",
        "\n",
        "# Next: run smoke test with XGB/LR (n_iter=7, 224+160), then full CV if promising:\n",
        "# run_svd_dual(model_type='xgb', smoke=True, seed=42, tag='svd_word224_char160_meta', n_iter_svd=7, n_comp_word=224, n_comp_char=160)\n",
        "# run_svd_dual(model_type='lr', smoke=True, seed=42, tag='svd_word224_char160_meta', n_iter_svd=7, n_comp_word=224, n_comp_char=160)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "a7029e0b-48d3-4018-8e99-9042381f39ba",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S37c-run: 2-fold smoke test for dual-view SVD base with XGB (+meta); expect >= word-only SVD folds\n",
        "try:\n",
        "    run_svd_dual(model_type='xgb', smoke=True, seed=42, tag='svd_word192_char128_meta', n_iter_svd=5)\n",
        "except Exception as e:\n",
        "    import traceback, sys\n",
        "    print('Error during SVDdual XGB smoke test:', e)\n",
        "    traceback.print_exc(file=sys.stdout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "e5576c06-0523-4d46-8eb6-cbd26c45aa77",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S37d-run-full: Full 5-fold CV for dual-view SVD (word192+char128)+meta with XGB; cache OOF/test, then reblend next\n",
        "try:\n",
        "    run_svd_dual(model_type='xgb', smoke=False, seed=42, tag='svd_word192_char128_meta', n_iter_svd=5)\n",
        "except Exception as e:\n",
        "    import traceback, sys\n",
        "    print('Error during SVDdual XGB full run:', e)\n",
        "    traceback.print_exc(file=sys.stdout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "00ce23cd-5617-4991-baed-6cee7646c7cc",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S37e: Reblend including dual-view SVD XGB base; allow Dense to drop; promote gamma-best\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from pathlib import Path\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Time blocks and masks\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "n = len(train)\n",
        "mask_full = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    mask_full[np.array(blocks[i])] = True\n",
        "mask_last2 = np.zeros(n, dtype=bool)\n",
        "for i in [4,5]:\n",
        "    mask_last2[np.array(blocks[i])] = True\n",
        "print(f'Time-CV validated full: {mask_full.sum()}/{n} | last2: {mask_last2.sum()}')\n",
        "\n",
        "# Load OOF preds\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy')\n",
        "o_lr_ns_base = np.load('oof_lr_time_nosub_meta.npy')\n",
        "o_lr_ns_decay = np.load('oof_lr_time_nosub_meta_decay.npy') if Path('oof_lr_time_nosub_meta_decay.npy').exists() else None\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_time_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta_time.npy')\n",
        "o_emn = np.load('oof_xgb_emb_meta_time.npy')\n",
        "o_emp = np.load('oof_xgb_emb_mpnet_time.npy')\n",
        "# Dual-view SVD base\n",
        "o_svd_dual = np.load('oof_xgb_svd_word192_char128_meta.npy')\n",
        "\n",
        "# Optional LR_main+meta\n",
        "has_lr_mainm = Path('oof_lr_main_meta_time.npy').exists() and Path('test_lr_main_meta_time.npy').exists()\n",
        "if has_lr_mainm:\n",
        "    o_lr_mainm = np.load('oof_lr_main_meta_time.npy')\n",
        "    print('Loaded LR_main+meta for blend consideration.')\n",
        "\n",
        "# Convert OOF to logits\n",
        "z_lr_w = to_logit(o_lr_w)\n",
        "z_d1, z_d2, z_meta = to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)\n",
        "z_emn, z_emp = to_logit(o_emn), to_logit(o_emp)\n",
        "z_svd_dual = to_logit(o_svd_dual)\n",
        "if has_lr_mainm:\n",
        "    z_lr_mainm = to_logit(o_lr_mainm)\n",
        "\n",
        "# Load test preds (prefer full-bag refits where available)\n",
        "t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "t_lr_ns_base = np.load('test_lr_time_nosub_meta.npy')\n",
        "t_lr_ns_decay = np.load('test_lr_time_nosub_meta_decay.npy') if Path('test_lr_time_nosub_meta_decay.npy').exists() else None\n",
        "t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "t_meta = np.load('test_xgb_meta_fullbag.npy') if Path('test_xgb_meta_fullbag.npy').exists() else np.load('test_xgb_meta_time.npy')\n",
        "t_emn = np.load('test_xgb_emb_minilm_fullbag.npy') if Path('test_xgb_emb_minilm_fullbag.npy').exists() else np.load('test_xgb_emb_meta_time.npy')\n",
        "t_emp = np.load('test_xgb_emb_mpnet_fullbag.npy') if Path('test_xgb_emb_mpnet_fullbag.npy').exists() else np.load('test_xgb_emb_mpnet_time.npy')\n",
        "t_svd_dual = np.load('test_xgb_svd_word192_char128_meta.npy')\n",
        "if has_lr_mainm:\n",
        "    t_lr_mainm = np.load('test_lr_main_meta_time.npy')\n",
        "\n",
        "# Grids\n",
        "g_grid = [0.96, 0.97, 0.98]\n",
        "meta_grid = [0.18, 0.20, 0.22]\n",
        "dense_tot_grid = [0.0, 0.15, 0.22, 0.28]\n",
        "dense_split = [(0.8, 0.2), (0.7, 0.3), (0.6, 0.4)]\n",
        "emb_tot_grid = [0.20, 0.24, 0.27, 0.30]\n",
        "emb_split = [(0.6, 0.4), (0.5, 0.5)]\n",
        "svd_dual_grid = [0.0, 0.05, 0.08, 0.10, 0.12, 0.15, 0.20]\n",
        "w_lrmain_grid = [0.0, 0.05] if has_lr_mainm else [0.0]\n",
        "use_lr_decay_options = [False, True] if (o_lr_ns_decay is not None) else [False]\n",
        "\n",
        "def search(mask, sample_weight=None):\n",
        "    best_auc, best_cfg, tried = -1.0, None, 0\n",
        "    for use_decay in use_lr_decay_options:\n",
        "        z_lr_ns = to_logit(o_lr_ns_decay) if (use_decay and (o_lr_ns_decay is not None)) else to_logit(o_lr_ns_base)\n",
        "        for g in g_grid:\n",
        "            z_lr_mix = (1.0 - g)*z_lr_w + g*z_lr_ns\n",
        "            for w_meta in meta_grid:\n",
        "                for d_tot in dense_tot_grid:\n",
        "                    for dv1, dv2 in dense_split:\n",
        "                        w_d1 = d_tot * dv1; w_d2 = d_tot * dv2\n",
        "                        for e_tot in emb_tot_grid:\n",
        "                            for emn_fr, emp_fr in emb_split:\n",
        "                                w_emn = e_tot * emn_fr; w_emp = e_tot * emp_fr\n",
        "                                for w_svd_dual in svd_dual_grid:\n",
        "                                    rem = 1.0 - (w_meta + w_d1 + w_d2 + w_emn + w_emp + w_svd_dual)\n",
        "                                    if rem <= 0: continue\n",
        "                                    for w_lrmain in w_lrmain_grid:\n",
        "                                        if w_lrmain > rem: continue\n",
        "                                        w_lr = rem - w_lrmain\n",
        "                                        if w_lr < 0.20: continue\n",
        "                                        z_oof = (w_lr*z_lr_mix +\n",
        "                                                 w_d1*z_d1 + w_d2*z_d2 +\n",
        "                                                 w_meta*z_meta +\n",
        "                                                 w_emn*z_emn + w_emp*z_emp +\n",
        "                                                 w_svd_dual*z_svd_dual)\n",
        "                                        if has_lr_mainm and w_lrmain > 0:\n",
        "                                            z_oof = z_oof + w_lrmain*z_lr_mainm\n",
        "                                        auc = roc_auc_score(y[mask], z_oof[mask], sample_weight=(sample_weight[mask] if sample_weight is not None else None))\n",
        "                                        tried += 1\n",
        "                                        if auc > best_auc:\n",
        "                                            best_auc = auc\n",
        "                                            best_cfg = dict(use_decay=use_decay, g=float(g),\n",
        "                                                            w_lr=float(w_lr), w_d1=float(w_d1), w_d2=float(w_d2),\n",
        "                                                            w_meta=float(w_meta), w_emn=float(w_emn), w_emp=float(w_emp),\n",
        "                                                            w_svd_dual=float(w_svd_dual), w_lrmain=float(w_lrmain))\n",
        "    return best_auc, best_cfg, tried\n",
        "\n",
        "# 1) Full-mask\n",
        "auc_full, cfg_full, tried_full = search(mask_full)\n",
        "print(f'[Full] tried={tried_full} | best OOF(z) AUC={auc_full:.5f} | cfg={cfg_full}')\n",
        "\n",
        "# 2) Last-2\n",
        "auc_last2, cfg_last2, tried_last2 = search(mask_last2)\n",
        "print(f'[Last2] tried={tried_last2} | best OOF(z,last2) AUC={auc_last2:.5f} | cfg={cfg_last2}')\n",
        "\n",
        "# 3) Gamma-decayed\n",
        "best_gamma, best_auc_g, best_cfg_g = None, -1.0, None\n",
        "for gamma in [0.95, 0.98]:\n",
        "    w = np.zeros(n, dtype=np.float64)\n",
        "    for bi in range(1, k):\n",
        "        age = (k - 1) - bi\n",
        "        w[np.array(blocks[bi])] = (gamma ** age)\n",
        "    auc_g, cfg_g, _ = search(mask_full, sample_weight=w)\n",
        "    print(f'[Gamma {gamma}] best OOF(z,weighted) AUC={auc_g:.5f}')\n",
        "    if auc_g > best_auc_g:\n",
        "        best_auc_g, best_cfg_g, best_gamma = auc_g, cfg_g, gamma\n",
        "print(f'[Gamma-best] gamma={best_gamma} | AUC={best_auc_g:.5f} | cfg={best_cfg_g}')\n",
        "\n",
        "def build_and_save(tag, cfg):\n",
        "    use_decay = cfg['use_decay']\n",
        "    tz_lr_ns = to_logit(t_lr_ns_decay if (use_decay and (t_lr_ns_decay is not None)) else t_lr_ns_base)\n",
        "    tz_lr_w = to_logit(t_lr_w)\n",
        "    tz_lr_mix = (1.0 - cfg['g'])*tz_lr_w + cfg['g']*tz_lr_ns\n",
        "    z_parts = [\n",
        "        cfg['w_lr']*tz_lr_mix,\n",
        "        cfg['w_d1']*to_logit(t_d1),\n",
        "        cfg['w_d2']*to_logit(t_d2),\n",
        "        cfg['w_meta']*to_logit(t_meta),\n",
        "        cfg['w_emn']*to_logit(t_emn),\n",
        "        cfg['w_emp']*to_logit(t_emp),\n",
        "        cfg['w_svd_dual']*to_logit(t_svd_dual)\n",
        "    ]\n",
        "    w_list = [cfg['w_lr'], cfg['w_d1'], cfg['w_d2'], cfg['w_meta'], cfg['w_emn'], cfg['w_emp'], cfg['w_svd_dual']]\n",
        "    if has_lr_mainm and cfg['w_lrmain'] > 0:\n",
        "        z_parts.append(cfg['w_lrmain']*to_logit(t_lr_mainm))\n",
        "        w_list.append(cfg['w_lrmain'])\n",
        "    zt = np.sum(z_parts, axis=0)\n",
        "    pt = sigmoid(zt).astype(np.float32)\n",
        "    pd.DataFrame({id_col: ids, target_col: pt}).to_csv(f'submission_reblend_svddual_{tag}.csv', index=False)\n",
        "    # 15% shrink hedge\n",
        "    w_vec = np.array(w_list, dtype=np.float64)\n",
        "    w_eq = np.ones_like(w_vec)/len(w_vec)\n",
        "    alpha = 0.15\n",
        "    w_shr = ((1.0 - alpha)*w_vec + alpha*w_eq); w_shr = (w_shr / w_shr.sum()).astype(np.float64)\n",
        "    comp_logits = [tz_lr_mix, to_logit(t_d1), to_logit(t_d2), to_logit(t_meta), to_logit(t_emn), to_logit(t_emp), to_logit(t_svd_dual)]\n",
        "    if has_lr_mainm and cfg['w_lrmain'] > 0:\n",
        "        comp_logits.append(to_logit(t_lr_mainm))\n",
        "    zt_shr = 0.0\n",
        "    for wi, zi in zip(w_shr, comp_logits):\n",
        "        zt_shr += wi*zi\n",
        "    pt_shr = sigmoid(zt_shr).astype(np.float32)\n",
        "    pd.DataFrame({id_col: ids, target_col: pt_shr}).to_csv(f'submission_reblend_svddual_{tag}_shrunk.csv', index=False)\n",
        "\n",
        "build_and_save('full', cfg_full)\n",
        "build_and_save('last2', cfg_last2)\n",
        "build_and_save(f'gamma{best_gamma:.2f}'.replace('.','p'), best_cfg_g)\n",
        "\n",
        "# Promote gamma-best\n",
        "prim = f'submission_reblend_svddual_gamma{best_gamma:.2f}'.replace('.','p') + '.csv'\n",
        "pd.read_csv(prim).to_csv('submission.csv', index=False)\n",
        "print(f'Promoted {prim} to submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "93962252-4a4b-47d9-ac26-a9251b385057",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S38: DistilRoBERTa fine-tune (title + [SEP] + request_text), time-aware 6-block CV, cache OOF/test\n",
        "import os, sys, time, gc, math\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "# Ensure HF cache is writable/local\n",
        "os.environ['HF_HOME'] = os.path.abspath('hf_cache')\n",
        "os.environ['TRANSFORMERS_CACHE'] = os.path.abspath('hf_cache')\n",
        "\n",
        "# Install torch + transformers if missing\n",
        "try:\n",
        "    import torch\n",
        "except ImportError:\n",
        "    import subprocess\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'torch', 'transformers', 'accelerate', 'datasets', 'evaluate', 'scikit-learn'])\n",
        "    import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Torch CUDA:', torch.cuda.is_available(), '| device:', device)\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body_no_leak(df):\n",
        "    return (df['request_text'] if 'request_text' in df.columns else df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "def build_text(df):\n",
        "    # For RoBERTa, SEP as '</s>' token. We'll just use a textual '[SEP]' and rely on tokenizer to split.\n",
        "    return (get_title(df) + ' [SEP] ' + get_body_no_leak(df)).astype(str)\n",
        "\n",
        "txt_tr = build_text(train).tolist()\n",
        "txt_te = build_text(test).tolist()\n",
        "\n",
        "# Time-aware 6-block forward-chaining (validate blocks 1..5)\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask_val = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask_val[va_idx] = True\n",
        "print(f'Time-CV folds={len(folds)}; validated {mask_val.sum()}/{n}')\n",
        "\n",
        "model_name = 'distilroberta-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "max_len = 256\n",
        "batch_size = 32\n",
        "epochs = 2\n",
        "lr = 2e-5\n",
        "weight_decay = 0.01\n",
        "warmup_ratio = 0.1\n",
        "seed = 42\n",
        "torch.manual_seed(seed);\n",
        "np.random.seed(seed)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels=None):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        item = tokenizer(self.texts[idx], padding='max_length', truncation=True, max_length=max_len, return_tensors='pt')\n",
        "        item = {k: v.squeeze(0) for k, v in item.items()}\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        return item\n",
        "\n",
        "class DistilRobertaForBinary(torch.nn.Module):\n",
        "    def __init__(self, base_name):\n",
        "        super().__init__()\n",
        "        self.base = AutoModel.from_pretrained(base_name)\n",
        "        hidden_size = self.base.config.hidden_size\n",
        "        self.classifier = torch.nn.Linear(hidden_size, 1)\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.base(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # DistilRoBERTa: use mean pooling over last hidden state masked by attention\n",
        "        last_hidden = outputs.last_hidden_state  # (B, L, H)\n",
        "        mask = attention_mask.unsqueeze(-1).float()\n",
        "        summed = (last_hidden * mask).sum(dim=1)\n",
        "        counts = mask.sum(dim=1).clamp(min=1e-6)\n",
        "        pooled = summed / counts\n",
        "        logits = self.classifier(pooled).squeeze(-1)  # (B,)\n",
        "        return logits\n",
        "\n",
        "def train_fold(tr_idx, va_idx, fold_id):\n",
        "    x_tr = [txt_tr[i] for i in tr_idx]; y_tr = y[tr_idx].astype(np.float32)\n",
        "    x_va = [txt_tr[i] for i in va_idx]; y_va = y[va_idx].astype(np.float32)\n",
        "    ds_tr = TextDataset(x_tr, y_tr); ds_va = TextDataset(x_va, y_va)\n",
        "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    model = DistilRobertaForBinary(model_name).to(device)\n",
        "    # Imbalance: pos_weight = neg/pos on train fold\n",
        "    pos = float((y_tr == 1).sum()); neg = float((y_tr == 0).sum());\n",
        "    pos_weight = torch.tensor([ (neg / max(pos, 1.0)) if pos > 0 else 1.0 ], device=device, dtype=torch.float32)\n",
        "    criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    total_steps = epochs * math.ceil(len(ds_tr) / batch_size)\n",
        "    warmup_steps = int(warmup_ratio * total_steps)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "    best_va_auc = -1.0\n",
        "    best_state = None\n",
        "    t0 = time.time()\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        tr_loss = 0.0; nb = 0\n",
        "        for batch in dl_tr:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            input_ids = batch['input_ids'].to(device); attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "                logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                loss = criterion(logits, labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer); scaler.update()\n",
        "            scheduler.step()\n",
        "            tr_loss += loss.item(); nb += 1\n",
        "        # Validate\n",
        "        model.eval()\n",
        "        va_probs = []; va_targets = []\n",
        "        with torch.no_grad():\n",
        "            for batch in dl_va:\n",
        "                input_ids = batch['input_ids'].to(device); attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].cpu().numpy()\n",
        "                logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "                va_probs.append(probs); va_targets.append(labels)\n",
        "        va_probs = np.concatenate(va_probs); va_targets = np.concatenate(va_targets)\n",
        "        va_auc = roc_auc_score(va_targets, va_probs) if va_targets.min() != va_targets.max() else 0.5\n",
        "        print(f'[Fold {fold_id}] Epoch {ep}/{epochs} | tr_loss={(tr_loss/ max(nb,1)):.4f} | VA AUC={va_auc:.5f} | elapsed={time.time()-t0:.1f}s', flush=True)\n",
        "        # Track best\n",
        "        if va_auc > best_va_auc:\n",
        "            best_va_auc = va_auc\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "    # Load best\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    # Inference on val\n",
        "    ds_va = TextDataset(x_va, y_va); dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    va_probs = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in dl_va:\n",
        "            input_ids = batch['input_ids'].to(device); attention_mask = batch['attention_mask'].to(device)\n",
        "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "            va_probs.append(probs)\n",
        "    va_probs = np.concatenate(va_probs).astype(np.float32)\n",
        "    # Inference on test\n",
        "    ds_te = TextDataset(txt_te, None); dl_te = DataLoader(ds_te, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    te_probs = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dl_te:\n",
        "            input_ids = batch['input_ids'].to(device); attention_mask = batch['attention_mask'].to(device)\n",
        "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "            te_probs.append(probs)\n",
        "    te_probs = np.concatenate(te_probs).astype(np.float32)\n",
        "    # Cleanup\n",
        "    del model, optimizer, scheduler, scaler, ds_tr, ds_va, dl_tr, dl_va\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "    return va_probs, te_probs, float(best_va_auc)\n",
        "\n",
        "oof = np.zeros(n, dtype=np.float32)\n",
        "te_parts = []\n",
        "fold_aucs = []\n",
        "all_start = time.time()\n",
        "for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    fold_start = time.time()\n",
        "    va_pred, te_pred, va_auc = train_fold(tr_idx, va_idx, fi)\n",
        "    oof[va_idx] = va_pred\n",
        "    te_parts.append(te_pred)\n",
        "    fold_aucs.append(va_auc)\n",
        "    print(f'[FT DistilRoBERTa] Fold {fi}/{len(folds)} done | AUC={va_auc:.5f} | {time.time()-fold_start:.1f}s', flush=True)\n",
        "\n",
        "auc_full = roc_auc_score(y[mask_val], oof[mask_val])\n",
        "te_mean = np.mean(te_parts, axis=0).astype(np.float32)\n",
        "np.save('oof_transformer_distilroberta.npy', oof.astype(np.float32))\n",
        "np.save('test_transformer_distilroberta.npy', te_mean)\n",
        "print(f'[FT DistilRoBERTa] DONE | OOF AUC={auc_full:.5f} | folds AUC={fold_aucs} | total {time.time()-all_start:.1f}s')\n",
        "print('Saved oof_transformer_distilroberta.npy and test_transformer_distilroberta.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "37c410a1-d5a1-4aa7-a828-c3eb1abb985e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S37f-run-full-upgraded: Full 5-fold CV for upgraded dual-view SVD (word224+char160, n_iter=7) with XGB and LR (+meta); cache OOF/test\n",
        "try:\n",
        "    print('=== Running upgraded SVDdual XGB full CV (224+160, n_iter=7) ===')\n",
        "    run_svd_dual(model_type='xgb', smoke=False, seed=42, tag='svd_word224_char160_meta', n_iter_svd=7, n_comp_word=224, n_comp_char=160)\n",
        "    print('=== Running upgraded SVDdual LR full CV (224+160, n_iter=7) ===')\n",
        "    run_svd_dual(model_type='lr', smoke=False, seed=42, tag='svd_word224_char160_meta', n_iter_svd=7, n_comp_word=224, n_comp_char=160)\n",
        "except Exception as e:\n",
        "    import traceback, sys\n",
        "    print('Error during upgraded dual SVD full runs:', e)\n",
        "    traceback.print_exc(file=sys.stdout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "053422f5-ac7c-4613-94d6-252475d09f9a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S37g-run-smoke-upgraded: 2-fold smoke for upgraded dual SVD (word224+char160, n_iter=7) XGB and LR\n",
        "try:\n",
        "    print('=== Smoke: upgraded SVDdual XGB (224+160, n_iter=7) ===')\n",
        "    run_svd_dual(model_type='xgb', smoke=True, seed=42, tag='svd_word224_char160_meta', n_iter_svd=7, n_comp_word=224, n_comp_char=160)\n",
        "    print('=== Smoke: upgraded SVDdual LR (224+160, n_iter=7) ===')\n",
        "    run_svd_dual(model_type='lr', smoke=True, seed=42, tag='svd_word224_char160_meta', n_iter_svd=7, n_comp_word=224, n_comp_char=160)\n",
        "except Exception as e:\n",
        "    import traceback, sys\n",
        "    print('Error during upgraded dual SVD smoke runs:', e)\n",
        "    traceback.print_exc(file=sys.stdout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "62a59f26-07d9-414e-8197-47c8b9591e22",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S37h: Reblend with last-2 as primary objective, dense cap <=0.12, gamma in {0.975,0.98,0.99}; include dual SVD base (fast grid + progress logs)\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from pathlib import Path\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Time blocks and masks\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "n = len(train)\n",
        "mask_full = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    mask_full[np.array(blocks[i])] = True\n",
        "mask_last2 = np.zeros(n, dtype=bool)\n",
        "for i in [4,5]:\n",
        "    mask_last2[np.array(blocks[i])] = True\n",
        "print(f'Time-CV validated full: {mask_full.sum()}/{n} | last2: {mask_last2.sum()}', flush=True)\n",
        "\n",
        "# Load available OOF preds\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy')\n",
        "o_lr_ns_base = np.load('oof_lr_time_nosub_meta.npy')\n",
        "o_lr_ns_decay = np.load('oof_lr_time_nosub_meta_decay.npy') if Path('oof_lr_time_nosub_meta_decay.npy').exists() else None\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_time_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta_time.npy')\n",
        "o_emn = np.load('oof_xgb_emb_meta_time.npy')\n",
        "o_emp = np.load('oof_xgb_emb_mpnet_time.npy')\n",
        "o_svd_dual = np.load('oof_xgb_svd_word192_char128_meta.npy')  # dual SVD base (XGB, word192+char128)\n",
        "\n",
        "# Optional bases\n",
        "has_lr_mainm = Path('oof_lr_main_meta_time.npy').exists() and Path('test_lr_main_meta_time.npy').exists()\n",
        "if has_lr_mainm:\n",
        "    o_lr_mainm = np.load('oof_lr_main_meta_time.npy')\n",
        "    print('Loaded LR_main+meta for blend consideration.', flush=True)\n",
        "\n",
        "# Convert OOF to logits\n",
        "z_lr_w = to_logit(o_lr_w)\n",
        "z_d1, z_d2, z_meta = to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)\n",
        "z_emn, z_emp = to_logit(o_emn), to_logit(o_emp)\n",
        "z_svd_dual = to_logit(o_svd_dual)\n",
        "if has_lr_mainm:\n",
        "    z_lr_mainm = to_logit(o_lr_mainm)\n",
        "\n",
        "# Load test preds (prefer full-bag refits where available)\n",
        "t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "t_lr_ns_base = np.load('test_lr_time_nosub_meta.npy')\n",
        "t_lr_ns_decay = np.load('test_lr_time_nosub_meta_decay.npy') if Path('test_lr_time_nosub_meta_decay.npy').exists() else None\n",
        "t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "t_meta = np.load('test_xgb_meta_fullbag.npy') if Path('test_xgb_meta_fullbag.npy').exists() else np.load('test_xgb_meta_time.npy')\n",
        "t_emn = np.load('test_xgb_emb_minilm_fullbag.npy') if Path('test_xgb_emb_minilm_fullbag.npy').exists() else np.load('test_xgb_emb_meta_time.npy')\n",
        "t_emp = np.load('test_xgb_emb_mpnet_fullbag.npy') if Path('test_xgb_emb_mpnet_fullbag.npy').exists() else np.load('test_xgb_emb_mpnet_time.npy')\n",
        "t_svd_dual = np.load('test_xgb_svd_word192_char128_meta.npy')\n",
        "if has_lr_mainm:\n",
        "    t_lr_mainm = np.load('test_lr_main_meta_time.npy')\n",
        "\n",
        "# Fast grids per expert guidance (dense cap <= 0.12) with progress logs\n",
        "g_grid = [0.97, 0.98]\n",
        "meta_grid = [0.18, 0.20, 0.22]\n",
        "dense_tot_grid = [0.0, 0.06, 0.12]\n",
        "dense_split = [(0.6, 0.4), (0.7, 0.3)]\n",
        "emb_tot_grid = [0.24, 0.27, 0.30]\n",
        "emb_split = [(0.6, 0.4), (0.5, 0.5)]\n",
        "svd_dual_grid = [0.0, 0.05, 0.10, 0.12, 0.15]\n",
        "w_lr_min_grid = [0.22, 0.25]\n",
        "w_lrmain_grid = [0.0, 0.05] if has_lr_mainm else [0.0]\n",
        "use_lr_decay_options = [False, True] if (o_lr_ns_decay is not None) else [False]\n",
        "\n",
        "def search(mask, sample_weight=None):\n",
        "    best_auc, best_cfg, tried = -1.0, None, 0\n",
        "    t0 = time.time()\n",
        "    for use_decay in use_lr_decay_options:\n",
        "        z_lr_ns = to_logit(o_lr_ns_decay) if (use_decay and (o_lr_ns_decay is not None)) else to_logit(o_lr_ns_base)\n",
        "        for g in g_grid:\n",
        "            z_lr_mix = (1.0 - g)*z_lr_w + g*z_lr_ns\n",
        "            for w_lr_min in w_lr_min_grid:\n",
        "                for w_meta in meta_grid:\n",
        "                    for d_tot in dense_tot_grid:\n",
        "                        for dv1, dv2 in dense_split:\n",
        "                            w_d1 = d_tot * dv1; w_d2 = d_tot * dv2\n",
        "                            for e_tot in emb_tot_grid:\n",
        "                                for emn_fr, emp_fr in emb_split:\n",
        "                                    w_emn = e_tot * emn_fr; w_emp = e_tot * emp_fr\n",
        "                                    for w_svd_dual in svd_dual_grid:\n",
        "                                        rem = 1.0 - (w_meta + w_d1 + w_d2 + w_emn + w_emp + w_svd_dual)\n",
        "                                        if rem <= 0: continue\n",
        "                                        for w_lrmain in w_lrmain_grid:\n",
        "                                            if w_lrmain > rem: continue\n",
        "                                            w_lr = rem - w_lrmain\n",
        "                                            if w_lr < w_lr_min: continue\n",
        "                                            z_oof = (w_lr*z_lr_mix +\n",
        "                                                     w_d1*z_d1 + w_d2*z_d2 +\n",
        "                                                     w_meta*z_meta +\n",
        "                                                     w_emn*z_emn + w_emp*z_emp +\n",
        "                                                     w_svd_dual*z_svd_dual)\n",
        "                                            if has_lr_mainm and w_lrmain > 0:\n",
        "                                                z_oof = z_oof + w_lrmain*z_lr_mainm\n",
        "                                            auc = roc_auc_score(y[mask], z_oof[mask], sample_weight=(sample_weight[mask] if sample_weight is not None else None))\n",
        "                                            tried += 1\n",
        "                                            if tried % 1000 == 0:\n",
        "                                                print(f'  tried={tried} | curr_best={best_auc:.5f} | elapsed={time.time()-t0:.1f}s', flush=True)\n",
        "                                            if auc > best_auc:\n",
        "                                                best_auc = auc\n",
        "                                                best_cfg = dict(use_decay=use_decay, g=float(g), w_lr=float(w_lr),\n",
        "                                                                w_d1=float(w_d1), w_d2=float(w_d2), w_meta=float(w_meta),\n",
        "                                                                w_emn=float(w_emn), w_emp=float(w_emp), w_svd_dual=float(w_svd_dual),\n",
        "                                                                w_lrmain=float(w_lrmain))\n",
        "    print(f'  search done | tried={tried} | best={best_auc:.5f} | {time.time()-t0:.1f}s', flush=True)\n",
        "    return best_auc, best_cfg, tried\n",
        "\n",
        "# Primary: last-2 objective\n",
        "auc_last2, cfg_last2, tried_last2 = search(mask_last2)\n",
        "print(f'[Last2 PRIMARY] tried={tried_last2} | best OOF(z,last2) AUC={auc_last2:.5f} | cfg={cfg_last2}', flush=True)\n",
        "\n",
        "# Also report full and gamma-decayed (tighter gammas) for reference\n",
        "auc_full, cfg_full, tried_full = search(mask_full)\n",
        "print(f'[Full] tried={tried_full} | best OOF(z) AUC={auc_full:.5f} | cfg={cfg_full}', flush=True)\n",
        "\n",
        "best_gamma, best_auc_g, best_cfg_g = None, -1.0, None\n",
        "for gamma in [0.975, 0.98, 0.99]:\n",
        "    w = np.zeros(n, dtype=np.float64)\n",
        "    for bi in range(1, k):\n",
        "        age = (k - 1) - bi\n",
        "        w[np.array(blocks[bi])] = (gamma ** age)\n",
        "    auc_g, cfg_g, _ = search(mask_full, sample_weight=w)\n",
        "    print(f'[Gamma {gamma}] best OOF(z,weighted) AUC={auc_g:.5f}', flush=True)\n",
        "    if auc_g > best_auc_g:\n",
        "        best_auc_g, best_cfg_g, best_gamma = auc_g, cfg_g, gamma\n",
        "print(f'[Gamma-best] gamma={best_gamma} | AUC={best_auc_g:.5f} | cfg={best_cfg_g}', flush=True)\n",
        "\n",
        "def build_and_save(tag, cfg):\n",
        "    use_decay = cfg['use_decay']\n",
        "    tz_lr_ns = to_logit(t_lr_ns_decay if (use_decay and (t_lr_ns_decay is not None)) else t_lr_ns_base)\n",
        "    tz_lr_w = to_logit(t_lr_w)\n",
        "    tz_lr_mix = (1.0 - cfg['g'])*tz_lr_w + cfg['g']*tz_lr_ns\n",
        "    parts = [\n",
        "        cfg['w_lr']*tz_lr_mix,\n",
        "        cfg['w_d1']*to_logit(t_d1),\n",
        "        cfg['w_d2']*to_logit(t_d2),\n",
        "        cfg['w_meta']*to_logit(t_meta),\n",
        "        cfg['w_emn']*to_logit(t_emn),\n",
        "        cfg['w_emp']*to_logit(t_emp),\n",
        "        cfg['w_svd_dual']*to_logit(t_svd_dual)\n",
        "    ]\n",
        "    w_list = [cfg['w_lr'], cfg['w_d1'], cfg['w_d2'], cfg['w_meta'], cfg['w_emn'], cfg['w_emp'], cfg['w_svd_dual']]\n",
        "    if has_lr_mainm and cfg['w_lrmain'] > 0:\n",
        "        parts.append(cfg['w_lrmain']*to_logit(t_lr_mainm))\n",
        "        w_list.append(cfg['w_lrmain'])\n",
        "    zt = np.sum(parts, axis=0)\n",
        "    pt = sigmoid(zt).astype(np.float32)\n",
        "    pd.DataFrame({id_col: ids, target_col: pt}).to_csv(f'submission_last2blend_{tag}.csv', index=False)\n",
        "    # 15% shrink hedge\n",
        "    w_vec = np.array(w_list, dtype=np.float64)\n",
        "    w_eq = np.ones_like(w_vec)/len(w_vec)\n",
        "    alpha = 0.15\n",
        "    w_shr = ((1.0 - alpha)*w_vec + alpha*w_eq); w_shr = (w_shr / w_shr.sum()).astype(np.float64)\n",
        "    comp_logits = [tz_lr_mix, to_logit(t_d1), to_logit(t_d2), to_logit(t_meta), to_logit(t_emn), to_logit(t_emp), to_logit(t_svd_dual)]\n",
        "    if has_lr_mainm and cfg['w_lrmain'] > 0:\n",
        "        comp_logits.append(to_logit(t_lr_mainm))\n",
        "    zt_shr = 0.0\n",
        "    for wi, zi in zip(w_shr, comp_logits):\n",
        "        zt_shr += wi*zi\n",
        "    pt_shr = sigmoid(zt_shr).astype(np.float32)\n",
        "    pd.DataFrame({id_col: ids, target_col: pt_shr}).to_csv(f'submission_last2blend_{tag}_shrunk.csv', index=False)\n",
        "\n",
        "# Build and save with last-2 winner and gamma-best\n",
        "build_and_save('last2', cfg_last2)\n",
        "build_and_save(f'gamma{best_gamma:.3f}'.replace('.', 'p'), best_cfg_g)\n",
        "\n",
        "# Promote last-2 winner as primary per expert guidance\n",
        "prim = 'submission_last2blend_last2.csv'\n",
        "pd.read_csv(prim).to_csv('submission.csv', index=False)\n",
        "print(f'Promoted {prim} to submission.csv', flush=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV validated full: 2398/2878 | last2: 958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded LR_main+meta for blend consideration.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=1000 | curr_best=0.64630 | elapsed=1.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=2000 | curr_best=0.64630 | elapsed=3.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=3000 | curr_best=0.64634 | elapsed=5.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=4000 | curr_best=0.64649 | elapsed=7.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=5000 | curr_best=0.64649 | elapsed=8.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=6000 | curr_best=0.64649 | elapsed=10.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=7000 | curr_best=0.64649 | elapsed=12.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=8000 | curr_best=0.64649 | elapsed=14.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  search done | tried=8096 | best=0.64649 | 14.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Last2 PRIMARY] tried=8096 | best OOF(z,last2) AUC=0.64649 | cfg={'use_decay': False, 'g': 0.98, 'w_lr': 0.30999999999999994, 'w_d1': 0.08399999999999999, 'w_d2': 0.036, 'w_meta': 0.22, 'w_emn': 0.18, 'w_emp': 0.12, 'w_svd_dual': 0.05, 'w_lrmain': 0.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=1000 | curr_best=0.68219 | elapsed=2.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=2000 | curr_best=0.68227 | elapsed=4.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=3000 | curr_best=0.68227 | elapsed=6.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=4000 | curr_best=0.68227 | elapsed=8.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=5000 | curr_best=0.68227 | elapsed=10.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=6000 | curr_best=0.68248 | elapsed=12.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=7000 | curr_best=0.68248 | elapsed=14.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=8000 | curr_best=0.68249 | elapsed=17.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  search done | tried=8096 | best=0.68249 | 17.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Full] tried=8096 | best OOF(z) AUC=0.68249 | cfg={'use_decay': True, 'g': 0.98, 'w_lr': 0.25999999999999995, 'w_d1': 0.08399999999999999, 'w_d2': 0.036, 'w_meta': 0.22, 'w_emn': 0.15, 'w_emp': 0.15, 'w_svd_dual': 0.05, 'w_lrmain': 0.05}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=1000 | curr_best=0.68070 | elapsed=2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=2000 | curr_best=0.68077 | elapsed=4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=3000 | curr_best=0.68077 | elapsed=7.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=4000 | curr_best=0.68077 | elapsed=9.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=5000 | curr_best=0.68077 | elapsed=11.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=6000 | curr_best=0.68098 | elapsed=14.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=7000 | curr_best=0.68098 | elapsed=16.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=8000 | curr_best=0.68099 | elapsed=19.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  search done | tried=8096 | best=0.68099 | 19.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gamma 0.975] best OOF(z,weighted) AUC=0.68099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=1000 | curr_best=0.68100 | elapsed=2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=2000 | curr_best=0.68107 | elapsed=4.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=3000 | curr_best=0.68107 | elapsed=7.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=4000 | curr_best=0.68107 | elapsed=9.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=5000 | curr_best=0.68107 | elapsed=11.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=6000 | curr_best=0.68128 | elapsed=14.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=7000 | curr_best=0.68128 | elapsed=16.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=8000 | curr_best=0.68129 | elapsed=18.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  search done | tried=8096 | best=0.68129 | 19.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gamma 0.98] best OOF(z,weighted) AUC=0.68129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=1000 | curr_best=0.68159 | elapsed=2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=2000 | curr_best=0.68167 | elapsed=4.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=3000 | curr_best=0.68167 | elapsed=7.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=4000 | curr_best=0.68167 | elapsed=9.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=5000 | curr_best=0.68167 | elapsed=11.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=6000 | curr_best=0.68188 | elapsed=14.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=7000 | curr_best=0.68188 | elapsed=16.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=8000 | curr_best=0.68189 | elapsed=19.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  search done | tried=8096 | best=0.68189 | 19.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gamma 0.99] best OOF(z,weighted) AUC=0.68189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gamma-best] gamma=0.99 | AUC=0.68189 | cfg={'use_decay': True, 'g': 0.98, 'w_lr': 0.25999999999999995, 'w_d1': 0.08399999999999999, 'w_d2': 0.036, 'w_meta': 0.22, 'w_emn': 0.15, 'w_emp': 0.15, 'w_svd_dual': 0.05, 'w_lrmain': 0.05}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promoted submission_last2blend_last2.csv to submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "f1294f15-2811-4886-92fa-e4a2d6c39e20",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S37h-mini: Deterministic last-2 blend build using prior good cfg (from S37e last2) to avoid grid stalls\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Load required test predictions (prefer refits where available)\n",
        "t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "t_meta = np.load('test_xgb_meta_fullbag.npy') if Path('test_xgb_meta_fullbag.npy').exists() else np.load('test_xgb_meta_time.npy')\n",
        "t_emn = np.load('test_xgb_emb_minilm_fullbag.npy') if Path('test_xgb_emb_minilm_fullbag.npy').exists() else np.load('test_xgb_emb_meta_time.npy')\n",
        "t_emp = np.load('test_xgb_emb_mpnet_fullbag.npy') if Path('test_xgb_emb_mpnet_fullbag.npy').exists() else np.load('test_xgb_emb_mpnet_time.npy')\n",
        "# Dual SVD base (word192+char128); upgraded 224+160 not available due to runtime, so use this for now\n",
        "t_svd_dual = np.load('test_xgb_svd_word192_char128_meta.npy')\n",
        "\n",
        "# Fixed last-2-inspired config from prior search (S37e last2 cfg):\n",
        "use_decay = False\n",
        "g = 0.97\n",
        "w_lr, w_d1, w_d2, w_meta, w_emn, w_emp, w_svd_dual = 0.22, 0.224, 0.056, 0.20, 0.18, 0.12, 0.00\n",
        "\n",
        "# Build logits with LR mix\n",
        "tz_lr_mix = (1.0 - g)*to_logit(t_lr_w) + g*to_logit(t_lr_ns)\n",
        "zt = (w_lr*tz_lr_mix +\n",
        "      w_d1*to_logit(t_d1) +\n",
        "      w_d2*to_logit(t_d2) +\n",
        "      w_meta*to_logit(t_meta) +\n",
        "      w_emn*to_logit(t_emn) +\n",
        "      w_emp*to_logit(t_emp) +\n",
        "      w_svd_dual*to_logit(t_svd_dual))\n",
        "pt = sigmoid(zt).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: pt}).to_csv('submission_last2_fixed.csv', index=False)\n",
        "\n",
        "# 15% shrink-to-equal hedge over active components\n",
        "w_vec = np.array([w_lr, w_d1, w_d2, w_meta, w_emn, w_emp] + ([w_svd_dual] if w_svd_dual > 0 else []), dtype=np.float64)\n",
        "comps = [tz_lr_mix, to_logit(t_d1), to_logit(t_d2), to_logit(t_meta), to_logit(t_emn), to_logit(t_emp)] + ([to_logit(t_svd_dual)] if w_svd_dual > 0 else [])\n",
        "w_eq = np.ones_like(w_vec) / len(w_vec)\n",
        "alpha = 0.15\n",
        "w_shr = ((1.0 - alpha)*w_vec + alpha*w_eq); w_shr = (w_shr / w_shr.sum()).astype(np.float64)\n",
        "zt_shr = np.zeros_like(comps[0], dtype=np.float64)\n",
        "for wi, zi in zip(w_shr, comps):\n",
        "    zt_shr += wi*zi\n",
        "pt_shr = sigmoid(zt_shr).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: pt_shr}).to_csv('submission_last2_fixed_shrunk.csv', index=False)\n",
        "\n",
        "# Promote last-2 fixed as primary (per expert guidance to emphasize recency on LB)\n",
        "pd.read_csv('submission_last2_fixed.csv').to_csv('submission.csv', index=False)\n",
        "print('Promoted submission_last2_fixed.csv to submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "3671aeef-d163-4857-b8d4-5ea84a131e4e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S37h-mini2: Fast deterministic blends with cached test preds; robust logs and file checks\n",
        "import os, time, glob, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def write_sub(path, probs):\n",
        "    df = pd.DataFrame({id_col: ids, target_col: probs.astype(np.float32)})\n",
        "    df.to_csv(path, index=False)\n",
        "    print(f'Wrote {path} | mean={probs.mean():.6f} | mtime={time.strftime(\"%H:%M:%S\", time.localtime(Path(path).stat().st_mtime))}', flush=True)\n",
        "\n",
        "# Load test predictions (prefer refits where available)\n",
        "t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "t_meta = np.load('test_xgb_meta_fullbag.npy') if Path('test_xgb_meta_fullbag.npy').exists() else np.load('test_xgb_meta_time.npy')\n",
        "t_emn = np.load('test_xgb_emb_minilm_fullbag.npy') if Path('test_xgb_emb_minilm_fullbag.npy').exists() else np.load('test_xgb_emb_meta_time.npy')\n",
        "t_emp = np.load('test_xgb_emb_mpnet_fullbag.npy') if Path('test_xgb_emb_mpnet_fullbag.npy').exists() else np.load('test_xgb_emb_mpnet_time.npy')\n",
        "t_svd_dual = np.load('test_xgb_svd_word192_char128_meta.npy') if Path('test_xgb_svd_word192_char128_meta.npy').exists() else None\n",
        "\n",
        "# Blend 1: Last-2 inspired fixed config (from S37e last2 cfg) without SVD-dual\n",
        "g1 = 0.97\n",
        "w_lr, w_d1, w_d2, w_meta, w_emn, w_emp = 0.22, 0.224, 0.056, 0.20, 0.18, 0.12\n",
        "tz_lr_mix = (1.0 - g1)*to_logit(t_lr_w) + g1*to_logit(t_lr_ns)\n",
        "z1 = (w_lr*tz_lr_mix +\n",
        "      w_d1*to_logit(t_d1) +\n",
        "      w_d2*to_logit(t_d2) +\n",
        "      w_meta*to_logit(t_meta) +\n",
        "      w_emn*to_logit(t_emn) +\n",
        "      w_emp*to_logit(t_emp))\n",
        "p1 = sigmoid(z1)\n",
        "write_sub('submission_last2_fixed_fast.csv', p1)\n",
        "\n",
        "# Blend 2: Gamma-best cfg from S37e full with SVD-dual weight if available\n",
        "g2 = 0.97\n",
        "w_lr2, w_d1_2, w_d2_2, w_meta_2, w_emn_2, w_emp_2, w_svd2 = 0.21, 0.176, 0.044, 0.22, 0.15, 0.15, (0.05 if t_svd_dual is not None else 0.0)\n",
        "tz_lr_mix2 = (1.0 - g2)*to_logit(t_lr_w) + g2*to_logit(t_lr_ns)\n",
        "z2 = (w_lr2*tz_lr_mix2 +\n",
        "      w_d1_2*to_logit(t_d1) +\n",
        "      w_d2_2*to_logit(t_d2) +\n",
        "      w_meta_2*to_logit(t_meta) +\n",
        "      w_emn_2*to_logit(t_emn) +\n",
        "      w_emp_2*to_logit(t_emp))\n",
        "if w_svd2 > 0:\n",
        "    z2 = z2 + w_svd2*to_logit(t_svd_dual)\n",
        "p2 = sigmoid(z2)\n",
        "write_sub('submission_gamma0p97_svddual_fast.csv', p2)\n",
        "\n",
        "# Promote gamma-based as primary hedge; if absent, promote last2\n",
        "primary = 'submission_gamma0p97_svddual_fast.csv' if Path('submission_gamma0p97_svddual_fast.csv').exists() else 'submission_last2_fixed_fast.csv'\n",
        "pd.read_csv(primary).to_csv('submission.csv', index=False)\n",
        "print(f'Promoted {primary} to submission.csv | mtime={time.strftime(\"%H:%M:%S\", time.localtime(Path(\"submission.csv\").stat().st_mtime))}', flush=True)\n",
        "\n",
        "# List recent submissions\n",
        "cands = sorted(glob.glob('submission*.csv'), key=lambda p: Path(p).stat().st_mtime, reverse=True)[:6]\n",
        "for p in cands:\n",
        "    st = Path(p).stat()\n",
        "    print(f'{p} | {st.st_size} bytes | mtime={time.strftime(\"%H:%M:%S\", time.localtime(st.st_mtime))}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "c5dae3d1-6de7-4cc7-878f-1d33fe0e277c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S37h-mini3: Promote best-known existing blend to submission.csv safely (no heavy deps)\n",
        "import pandas as pd, time, glob\n",
        "from pathlib import Path\n",
        "\n",
        "candidates = [\n",
        "    'submission_reblend_svddual_gamma0p98.csv',\n",
        "    'submission_blend_gamma0p98_fullrefits.csv',\n",
        "    'submission_reblend_gamma0p98.csv',\n",
        "    'submission_blend_gamma0p98.csv',\n",
        "    'submission_7way_gamma0p98_mpnet_fullrefit.csv'\n",
        "]\n",
        "\n",
        "chosen = None\n",
        "for p in candidates:\n",
        "    if Path(p).exists():\n",
        "        chosen = p\n",
        "        break\n",
        "\n",
        "if chosen is None:\n",
        "    raise FileNotFoundError('No known submission candidates found to promote.')\n",
        "\n",
        "sub = pd.read_csv(chosen)\n",
        "assert {'request_id','requester_received_pizza'}.issubset(sub.columns), 'Submission columns mismatch'\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(f'Promoted {chosen} to submission.csv | rows={len(sub)} | mean={sub.requester_received_pizza.mean():.6f} | mtime=' +\n",
        "      time.strftime('%H:%M:%S', time.localtime(Path('submission.csv').stat().st_mtime)), flush=True)\n",
        "\n",
        "# List recent submissions for sanity\n",
        "recent = sorted(glob.glob('submission*.csv'), key=lambda p: Path(p).stat().st_mtime, reverse=True)[:5]\n",
        "for p in recent:\n",
        "    st = Path(p).stat()\n",
        "    print(f'{p} | {st.st_size} bytes | mtime=' + time.strftime('%H:%M:%S', time.localtime(st.st_mtime)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "39f91e02-f1b8-4609-b22c-67dc335684e3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S37h-mini4: Safe promote using shutil (no pandas, no json reads) to avoid I/O stalls\n",
        "import os, time, glob, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "candidates = [\n",
        "    'submission_reblend_svddual_gamma0p98.csv',\n",
        "    'submission_blend_gamma0p98_fullrefits.csv',\n",
        "    'submission_reblend_gamma0p98.csv',\n",
        "    'submission_blend_gamma0p98.csv',\n",
        "    'submission_7way_gamma0p98_mpnet_fullrefit.csv'\n",
        "]\n",
        "\n",
        "chosen = None\n",
        "for p in candidates:\n",
        "    if Path(p).exists() and Path(p).stat().st_size > 0:\n",
        "        chosen = p\n",
        "        break\n",
        "\n",
        "if chosen is None:\n",
        "    raise FileNotFoundError('No known submission candidates found to promote (shutil).')\n",
        "\n",
        "shutil.copyfile(chosen, 'submission.csv')\n",
        "st = Path('submission.csv').stat()\n",
        "print(f'Promoted {chosen} -> submission.csv | size={st.st_size} bytes | mtime=' + time.strftime('%H:%M:%S', time.localtime(st.st_mtime)), flush=True)\n",
        "\n",
        "# Print head safely without pandas\n",
        "with open('submission.csv', 'r', encoding='utf-8') as f:\n",
        "    for i in range(3):\n",
        "        line = f.readline()\n",
        "        if not line:\n",
        "            break\n",
        "        print(line.rstrip())\n",
        "\n",
        "# List recent submission files\n",
        "recent = sorted(glob.glob('submission*.csv'), key=lambda p: Path(p).stat().st_mtime, reverse=True)[:6]\n",
        "for p in recent:\n",
        "    stp = Path(p).stat()\n",
        "    print(f'{p} | {stp.st_size} bytes | mtime=' + time.strftime('%H:%M:%S', time.localtime(stp.st_mtime)))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promoted submission_reblend_svddual_gamma0p98.csv -> submission.csv | size=23350 bytes | mtime=14:47:42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "request_id,requester_received_pizza\nt3_1aw5zf,0.48315614\nt3_roiuw,0.47056925\nsubmission.csv | 23350 bytes | mtime=14:47:42\nsubmission_reblend_svddual_gamma0p98_shrunk.csv | 23343 bytes | mtime=14:04:03\nsubmission_reblend_svddual_gamma0p98.csv | 23350 bytes | mtime=14:04:03\nsubmission_reblend_svddual_last2_shrunk.csv | 23314 bytes | mtime=14:04:03\nsubmission_reblend_svddual_last2.csv | 23311 bytes | mtime=14:04:03\nsubmission_reblend_svddual_full_shrunk.csv | 23343 bytes | mtime=14:04:03\n"
          ]
        }
      ]
    },
    {
      "id": "7d4ce6fd-835d-45de-99e8-749f8e4f4d47",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S37h-mini5: Rank-average top-2 submissions to create a hedge and promote\n",
        "import numpy as np, time, glob\n",
        "from pathlib import Path\n",
        "\n",
        "def read_probs(path):\n",
        "    # Read CSV second column (probabilities) without pandas\n",
        "    probs = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        header = f.readline()\n",
        "        for line in f:\n",
        "            parts = line.rstrip().split(',')\n",
        "            if len(parts) >= 2:\n",
        "                try:\n",
        "                    probs.append(float(parts[1]))\n",
        "                except ValueError:\n",
        "                    probs.append(0.5)\n",
        "    return np.asarray(probs, dtype=np.float64)\n",
        "\n",
        "def rank01(x):\n",
        "    # rankdata method='average' approximation using two argsorts (ties handled approximately)\n",
        "    order = np.argsort(x, kind='mergesort')\n",
        "    ranks = np.empty_like(order, dtype=np.float64)\n",
        "    ranks[order] = np.arange(len(x), dtype=np.float64)\n",
        "    return ranks / max(len(x) - 1, 1)\n",
        "\n",
        "cand1 = 'submission_reblend_svddual_gamma0p98.csv'  # expert primary\n",
        "cand2_options = [\n",
        "    'submission_blend_gamma0p98_fullrefits.csv',\n",
        "    'submission_7way_gamma0p98_mpnet_fullrefit.csv'\n",
        "]\n",
        "\n",
        "cand2 = next((p for p in cand2_options if Path(p).exists() and Path(p).stat().st_size > 0), None)\n",
        "if not (Path(cand1).exists() and Path(cand1).stat().st_size > 0 and cand2 is not None):\n",
        "    raise FileNotFoundError(f'Missing candidates for rank-average: cand1={Path(cand1).exists()} cand2={cand2}')\n",
        "\n",
        "p1 = read_probs(cand1)\n",
        "p2 = read_probs(cand2)\n",
        "assert p1.shape == p2.shape and p1.ndim == 1, 'Submission length mismatch'\n",
        "\n",
        "r1 = rank01(p1); r2 = rank01(p2)\n",
        "ravg = (r1 + r2) / 2.0\n",
        "\n",
        "# Read ids from cand1 safely\n",
        "ids = []\n",
        "with open(cand1, 'r', encoding='utf-8') as f:\n",
        "    header = f.readline()\n",
        "    for line in f:\n",
        "        parts = line.rstrip().split(',')\n",
        "        ids.append(parts[0])\n",
        "\n",
        "out_path = 'submission_rankavg_top2.csv'\n",
        "with open(out_path, 'w', encoding='utf-8') as f:\n",
        "    f.write('request_id,requester_received_pizza\\n')\n",
        "    for rid, val in zip(ids, ravg.astype(np.float32)):\n",
        "        f.write(f'{rid},{val:.8f}\\n')\n",
        "print(f'Wrote {out_path} | mean={ravg.mean():.6f} | mtime=' + time.strftime('%H:%M:%S', time.localtime(Path(out_path).stat().st_mtime)), flush=True)\n",
        "\n",
        "# Promote rank-avg hedge\n",
        "Path(out_path).replace('submission.csv')\n",
        "print('Promoted submission_rankavg_top2.csv to submission.csv | size=' + str(Path('submission.csv').stat().st_size))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_rankavg_top2.csv | mean=0.500000 | mtime=14:49:27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promoted submission_rankavg_top2.csv to submission.csv | size=23773\n"
          ]
        }
      ]
    },
    {
      "id": "ec2a2ad5-3116-4ad0-a629-27c915087cb6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S39: Time-aware level-2 stacker on base OOF logits (logistic meta-learner); write submission\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from pathlib import Path\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "# Time-aware 6-block forward-chaining\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "\n",
        "# mask_valid marks rows with available base OOF (blocks 1..5). We'll only train/validate on these.\n",
        "mask_valid = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    mask_valid[np.array(blocks[i])] = True\n",
        "\n",
        "# Define stacker folds so that training has non-empty OOF rows: validate blocks 2..5, train on blocks 1..(i-1)\n",
        "folds = []\n",
        "for i in range(2, k):  # i = 2..5\n",
        "    va_idx = np.array(blocks[i])\n",
        "    tr_idx = np.concatenate(blocks[1:i])  # only blocks with OOF available\n",
        "    folds.append((tr_idx, va_idx))\n",
        "print(f'Stacker folds: {len(folds)} (validate blocks 2..5)')\n",
        "\n",
        "# Load base OOF/test probabilities and convert to logits\n",
        "base_names = []\n",
        "oof_list = []; te_list = []\n",
        "\n",
        "def add_base(oof_path, te_path, name):\n",
        "    if (Path(oof_path).exists() and Path(te_path).exists()):\n",
        "        o = np.load(oof_path); t = np.load(te_path)\n",
        "        oof_list.append(to_logit(o)); te_list.append(to_logit(t)); base_names.append(name);\n",
        "        print(f'Added base: {name} | oof:{o.shape} te:{t.shape}')\n",
        "\n",
        "# Core bases\n",
        "add_base('oof_lr_time_withsub_meta.npy', 'test_lr_time_withsub_meta.npy', 'lr_withsub_meta')\n",
        "add_base('oof_lr_time_nosub_meta.npy', 'test_lr_time_nosub_meta.npy', 'lr_nosub_meta')\n",
        "if Path('oof_lr_time_nosub_meta_decay.npy').exists() and Path('test_lr_time_nosub_meta_decay.npy').exists():\n",
        "    add_base('oof_lr_time_nosub_meta_decay.npy', 'test_lr_time_nosub_meta_decay.npy', 'lr_nosub_meta_decay')\n",
        "add_base('oof_xgb_dense_time.npy', 'test_xgb_dense_time.npy', 'dense_v1')\n",
        "add_base('oof_xgb_dense_time_v2.npy', 'test_xgb_dense_time_v2.npy', 'dense_v2')\n",
        "add_base('oof_xgb_meta_time.npy', 'test_xgb_meta_time.npy', 'meta_xgb')\n",
        "add_base('oof_xgb_emb_meta_time.npy', 'test_xgb_emb_meta_time.npy', 'minilm_xgb')\n",
        "add_base('oof_xgb_emb_mpnet_time.npy', 'test_xgb_emb_mpnet_time.npy', 'mpnet_xgb')\n",
        "if Path('oof_xgb_svd_word192_char128_meta.npy').exists() and Path('test_xgb_svd_word192_char128_meta.npy').exists():\n",
        "    add_base('oof_xgb_svd_word192_char128_meta.npy', 'test_xgb_svd_word192_char128_meta.npy', 'svd_dual_xgb')\n",
        "\n",
        "# Prefer full-bag test replacements when available (overwrite test logits in te_list accordingly)\n",
        "name_to_idx = {n:i for i,n in enumerate(base_names)}\n",
        "def swap_test(name, te_path):\n",
        "    if name in name_to_idx and Path(te_path).exists():\n",
        "        i = name_to_idx[name];\n",
        "        te_list[i] = to_logit(np.load(te_path));\n",
        "        print(f'Replaced test for {name} with {te_path}')\n",
        "\n",
        "swap_test('meta_xgb', 'test_xgb_meta_fullbag.npy')\n",
        "swap_test('minilm_xgb', 'test_xgb_emb_minilm_fullbag.npy')\n",
        "swap_test('mpnet_xgb', 'test_xgb_emb_mpnet_fullbag.npy')\n",
        "\n",
        "m = len(base_names)\n",
        "assert m > 1, 'Not enough bases for stacking'\n",
        "O = np.vstack(oof_list).T  # (n, m) OOF logits\n",
        "T = np.vstack(te_list).T   # (n_test, m) test logits\n",
        "print(f'Stacker features: train OOF logits {O.shape} | test logits {T.shape} | bases={base_names}')\n",
        "\n",
        "# Train per-fold logistic stacker using only rows with OOF (mask_valid).\n",
        "oof_stacker = np.zeros(n, dtype=np.float32)\n",
        "te_parts = []\n",
        "C_grid = [0.5, 1.0, 2.0]\n",
        "for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    # Ensure training indices are within mask_valid\n",
        "    tr_idx = tr_idx[mask_valid[tr_idx]]\n",
        "    X_tr = O[tr_idx]; y_tr = y[tr_idx]\n",
        "    X_va = O[va_idx]; y_va = y[va_idx]\n",
        "    if X_tr.shape[0] == 0:\n",
        "        print(f'[Stacker] Fold {fi} has empty train after mask; skipping.', flush=True)\n",
        "        continue\n",
        "    best_auc, best_C, best_clf = -1.0, None, None\n",
        "    for C in C_grid:\n",
        "        clf = LogisticRegression(solver='liblinear', penalty='l2', C=C, max_iter=1000)\n",
        "        clf.fit(X_tr, y_tr)\n",
        "        va_pred = clf.predict_proba(X_va)[:,1]\n",
        "        auc = roc_auc_score(y_va, va_pred) if (y_va.min()!=y_va.max()) else 0.5\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_C, best_clf = auc, C, clf\n",
        "    oof_stacker[va_idx] = best_clf.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "    te_parts.append(best_clf.predict_proba(T)[:,1].astype(np.float32))\n",
        "    print(f'[Stacker] Fold {fi} | best C={best_C} | AUC={best_auc:.5f} | tr={len(tr_idx)} va={len(va_idx)}', flush=True)\n",
        "\n",
        "auc_valid = roc_auc_score(y[mask_valid], oof_stacker[mask_valid])\n",
        "print(f'[Stacker] OOF AUC (validated blocks 1..5): {auc_valid:.5f}')\n",
        "te_mean = np.mean(te_parts, axis=0).astype(np.float32)\n",
        "\n",
        "# Save and write submission\n",
        "np.save('oof_stacker_logits.npy', oof_stacker.astype(np.float32))\n",
        "np.save('test_stacker_logits.npy', te_mean.astype(np.float32))\n",
        "sub = pd.DataFrame({id_col: ids, target_col: te_mean})\n",
        "out_path = 'submission_stacker_lr.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(f'Wrote {out_path} and promoted to submission.csv | mean={te_mean.mean():.6f}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacker folds: 4 (validate blocks 2..5)\nAdded base: lr_withsub_meta | oof:(2878,) te:(1162,)\nAdded base: lr_nosub_meta | oof:(2878,) te:(1162,)\nAdded base: lr_nosub_meta_decay | oof:(2878,) te:(1162,)\nAdded base: dense_v1 | oof:(2878,) te:(1162,)\nAdded base: dense_v2 | oof:(2878,) te:(1162,)\nAdded base: meta_xgb | oof:(2878,) te:(1162,)\nAdded base: minilm_xgb | oof:(2878,) te:(1162,)\nAdded base: mpnet_xgb | oof:(2878,) te:(1162,)\nAdded base: svd_dual_xgb | oof:(2878,) te:(1162,)\nReplaced test for meta_xgb with test_xgb_meta_fullbag.npy\nReplaced test for minilm_xgb with test_xgb_emb_minilm_fullbag.npy\nReplaced test for mpnet_xgb with test_xgb_emb_mpnet_fullbag.npy\nStacker features: train OOF logits (2878, 9) | test logits (1162, 9) | bases=['lr_withsub_meta', 'lr_nosub_meta', 'lr_nosub_meta_decay', 'dense_v1', 'dense_v2', 'meta_xgb', 'minilm_xgb', 'mpnet_xgb', 'svd_dual_xgb']\n[Stacker] Fold 1 | best C=1.0 | AUC=0.69839 | tr=480 va=480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stacker] Fold 2 | best C=0.5 | AUC=0.64224 | tr=960 va=480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stacker] Fold 3 | best C=1.0 | AUC=0.64945 | tr=1440 va=479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stacker] Fold 4 | best C=0.5 | AUC=0.64255 | tr=1919 va=479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stacker] OOF AUC (validated blocks 1..5): 0.57933\nWrote submission_stacker_lr.csv and promoted to submission.csv | mean=0.198167\n"
          ]
        }
      ]
    },
    {
      "id": "5edf7100-b3f8-4a86-b43e-0aaa4bc2cb36",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S40: Time-aware Char TF-IDF (char_wb 2-6) + LogisticRegression base; cache OOF/test\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body_no_leak(df):\n",
        "    if 'request_text' in df.columns:\n",
        "        return df['request_text'].fillna('').astype(str)\n",
        "    return df.get('request_text', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def build_text(df):\n",
        "    return (get_title(df) + '\\n' + get_body_no_leak(df)).astype(str)\n",
        "\n",
        "txt_tr = build_text(train)\n",
        "txt_te = build_text(test)\n",
        "\n",
        "# Time-aware 6-block forward-chaining (validate blocks 1..5)\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}', flush=True)\n",
        "\n",
        "# Char TF-IDF params (lightweight caps for speed/stability)\n",
        "tf_params = dict(analyzer='char_wb', ngram_range=(2,6), lowercase=True, min_df=2, max_features=200_000,\n",
        "                 sublinear_tf=True, smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "\n",
        "C = 1.0\n",
        "oof = np.zeros(n, dtype=np.float32)\n",
        "te_parts = []\n",
        "t_all = time.time()\n",
        "for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    t0 = time.time()\n",
        "    tf = TfidfVectorizer(**tf_params)\n",
        "    X_tr = tf.fit_transform(txt_tr.iloc[tr_idx])\n",
        "    X_va = tf.transform(txt_tr.iloc[va_idx])\n",
        "    X_te = tf.transform(txt_te)\n",
        "    clf = LogisticRegression(penalty='l2', solver='saga', C=C, max_iter=2000, n_jobs=-1, verbose=0)\n",
        "    clf.fit(X_tr, y[tr_idx])\n",
        "    va_pred = clf.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "    te_pred = clf.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "    oof[va_idx] = va_pred\n",
        "    te_parts.append(te_pred)\n",
        "    auc = roc_auc_score(y[va_idx], va_pred)\n",
        "    print(f'[CharLR C={C}] Fold {fi} AUC={auc:.5f} | feats={X_tr.shape[1]} | {time.time()-t0:.1f}s', flush=True)\n",
        "    del tf, X_tr, X_va, X_te, clf; gc.collect()\n",
        "\n",
        "auc_mask = roc_auc_score(y[mask], oof[mask])\n",
        "te_mean = np.mean(te_parts, axis=0).astype(np.float32)\n",
        "print(f'[CharLR] OOF AUC(validated)={auc_mask:.5f} | total {time.time()-t_all:.1f}s', flush=True)\n",
        "np.save('oof_lr_charwb_time.npy', oof.astype(np.float32))\n",
        "np.save('test_lr_charwb_time.npy', te_mean.astype(np.float32))\n",
        "print('Saved oof_lr_charwb_time.npy and test_lr_charwb_time.npy', flush=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV: 5 folds; validated 2398/2878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CharLR C=1.0] Fold 1 AUC=0.68122 | feats=26806 | 4.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CharLR C=1.0] Fold 2 AUC=0.61014 | feats=40485 | 8.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CharLR C=1.0] Fold 3 AUC=0.57826 | feats=50592 | 11.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CharLR C=1.0] Fold 4 AUC=0.62621 | feats=57465 | 12.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CharLR C=1.0] Fold 5 AUC=0.66546 | feats=63589 | 16.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CharLR] OOF AUC(validated)=0.62454 | total 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_lr_charwb_time.npy and test_lr_charwb_time.npy\n"
          ]
        }
      ]
    },
    {
      "id": "43b22917-843a-4d7e-9bea-b940e9fad7df",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S41: Reblend gamma-best by adding Char TF-IDF LR base with small weight; choose w_char via OOF gamma-weighted AUC\n",
        "import numpy as np, pandas as pd, time\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "ids = test[id_col].values\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Time blocks and mask for validated rows\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "n = len(train)\n",
        "mask_full = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    mask_full[np.array(blocks[i])] = True\n",
        "\n",
        "# Gamma weights for OOF objective (gamma=0.98)\n",
        "gamma = 0.98\n",
        "w_oof = np.zeros(n, dtype=np.float64)\n",
        "for bi in range(1, k):\n",
        "    age = (k - 1) - bi\n",
        "    w_oof[np.array(blocks[bi])] = (gamma ** age)\n",
        "\n",
        "# Load OOF for existing components\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy')\n",
        "o_lr_ns = np.load('oof_lr_time_nosub_meta.npy')\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_time_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta_time.npy')\n",
        "o_emn = np.load('oof_xgb_emb_meta_time.npy')\n",
        "o_emp = np.load('oof_xgb_emb_mpnet_time.npy')\n",
        "o_svd_dual = np.load('oof_xgb_svd_word192_char128_meta.npy') if Path('oof_xgb_svd_word192_char128_meta.npy').exists() else None\n",
        "o_char = np.load('oof_lr_charwb_time.npy')\n",
        "\n",
        "# Convert to logits\n",
        "z_lr_w, z_lr_ns = to_logit(o_lr_w), to_logit(o_lr_ns)\n",
        "z_d1, z_d2, z_meta = to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)\n",
        "z_emn, z_emp = to_logit(o_emn), to_logit(o_emp)\n",
        "z_svd = to_logit(o_svd_dual) if (o_svd_dual is not None) else None\n",
        "z_char = to_logit(o_char)\n",
        "\n",
        "# Test preds\n",
        "t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "t_meta = np.load('test_xgb_meta_fullbag.npy') if Path('test_xgb_meta_fullbag.npy').exists() else np.load('test_xgb_meta_time.npy')\n",
        "t_emn = np.load('test_xgb_emb_minilm_fullbag.npy') if Path('test_xgb_emb_minilm_fullbag.npy').exists() else np.load('test_xgb_emb_meta_time.npy')\n",
        "t_emp = np.load('test_xgb_emb_mpnet_fullbag.npy') if Path('test_xgb_emb_mpnet_fullbag.npy').exists() else np.load('test_xgb_emb_mpnet_time.npy')\n",
        "t_svd = np.load('test_xgb_svd_word192_char128_meta.npy') if Path('test_xgb_svd_word192_char128_meta.npy').exists() else None\n",
        "t_char = np.load('test_lr_charwb_time.npy')\n",
        "\n",
        "tz = lambda arr: to_logit(arr)\n",
        "\n",
        "# Expert gamma-best baseline (S37e) weights including dual SVD\n",
        "g = 0.97\n",
        "base_weights = dict(w_lr=0.21, w_d1=0.176, w_d2=0.044, w_meta=0.22, w_emn=0.15, w_emp=0.15, w_svd=0.05)\n",
        "\n",
        "def blend_oof_with_char(w_char):\n",
        "    # Renormalize other weights to sum to (1 - w_char)\n",
        "    scale = 1.0 - w_char\n",
        "    w_lr = base_weights['w_lr'] * scale\n",
        "    w_d1 = base_weights['w_d1'] * scale\n",
        "    w_d2 = base_weights['w_d2'] * scale\n",
        "    w_meta = base_weights['w_meta'] * scale\n",
        "    w_emn = base_weights['w_emn'] * scale\n",
        "    w_emp = base_weights['w_emp'] * scale\n",
        "    w_svd = (base_weights['w_svd'] * scale) if (z_svd is not None) else 0.0\n",
        "    z_lr_mix = (1.0 - g)*z_lr_w + g*z_lr_ns\n",
        "    z = (w_lr*z_lr_mix + w_d1*z_d1 + w_d2*z_d2 + w_meta*z_meta + w_emn*z_emn + w_emp*z_emp)\n",
        "    if z_svd is not None:\n",
        "        z = z + w_svd*z_svd\n",
        "    z = z + w_char*z_char\n",
        "    return z, dict(w_lr=w_lr, w_d1=w_d1, w_d2=w_d2, w_meta=w_meta, w_emn=w_emn, w_emp=w_emp, w_svd=w_svd, w_char=w_char)\n",
        "\n",
        "candidates = [0.03, 0.05, 0.07, 0.08]\n",
        "best_auc, best_cfg = -1.0, None\n",
        "for wc in candidates:\n",
        "    z_oof, cfg = blend_oof_with_char(wc)\n",
        "    auc = roc_auc_score(y[mask_full], z_oof[mask_full], sample_weight=w_oof[mask_full])\n",
        "    print(f'[Char add] w_char={wc:.3f} | gamma-weighted OOF AUC={auc:.5f}')\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_cfg = auc, cfg\n",
        "print(f'[Char add] Selected w_char={best_cfg[\"w_char\"]:.3f} | AUC={best_auc:.5f}')\n",
        "\n",
        "def build_test(cfg, tag):\n",
        "    tz_lr_mix = (1.0 - g)*tz(t_lr_w) + g*tz(t_lr_ns)\n",
        "    parts = [\n",
        "        cfg['w_lr']*tz_lr_mix,\n",
        "        cfg['w_d1']*tz(t_d1),\n",
        "        cfg['w_d2']*tz(t_d2),\n",
        "        cfg['w_meta']*tz(t_meta),\n",
        "        cfg['w_emn']*tz(t_emn),\n",
        "        cfg['w_emp']*tz(t_emp)\n",
        "    ]\n",
        "    if t_svd is not None and cfg['w_svd'] > 0:\n",
        "        parts.append(cfg['w_svd']*tz(t_svd))\n",
        "    parts.append(cfg['w_char']*tz(t_char))\n",
        "    zt = np.sum(parts, axis=0)\n",
        "    pt = sigmoid(zt).astype(np.float32)\n",
        "    out = pd.DataFrame({id_col: ids, target_col: pt})\n",
        "    path = f'submission_gamma0p97_svddual_char{cfg[\"w_char\"]:.3f}.csv'\n",
        "    out.to_csv(path, index=False)\n",
        "    # 15% shrink-to-equal hedge\n",
        "    w_list = [cfg['w_lr'], cfg['w_d1'], cfg['w_d2'], cfg['w_meta'], cfg['w_emn'], cfg['w_emp'], cfg['w_char']] + ([cfg['w_svd']] if (t_svd is not None and cfg['w_svd']>0) else [])\n",
        "    comp_logits = [tz_lr_mix, tz(t_d1), tz(t_d2), tz(t_meta), tz(t_emn), tz(t_emp), tz(t_char)] + ([tz(t_svd)] if (t_svd is not None and cfg['w_svd']>0) else [])\n",
        "    w_vec = np.array(w_list, dtype=np.float64)\n",
        "    w_eq = np.ones_like(w_vec)/len(w_vec)\n",
        "    alpha = 0.15\n",
        "    w_shr = ((1.0 - alpha)*w_vec + alpha*w_eq); w_shr = (w_shr / w_shr.sum()).astype(np.float64)\n",
        "    zt_shr = np.zeros_like(comp_logits[0], dtype=np.float64)\n",
        "    for wi, zi in zip(w_shr, comp_logits):\n",
        "        zt_shr += wi*zi\n",
        "    pt_shr = sigmoid(zt_shr).astype(np.float32)\n",
        "    pd.DataFrame({id_col: ids, target_col: pt_shr}).to_csv(path.replace('.csv','_shrunk.csv'), index=False)\n",
        "    # Promote primary\n",
        "    out.to_csv('submission.csv', index=False)\n",
        "    print(f'Wrote {path} (+_shrunk) and promoted to submission.csv | mean={pt.mean():.6f}')\n",
        "\n",
        "# Build test with best w_char\n",
        "build_test(best_cfg, 'gamma0p97_char')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Char add] w_char=0.030 | gamma-weighted OOF AUC=0.68158\n[Char add] w_char=0.050 | gamma-weighted OOF AUC=0.68171\n[Char add] w_char=0.070 | gamma-weighted OOF AUC=0.68171\n[Char add] w_char=0.080 | gamma-weighted OOF AUC=0.68172\n[Char add] Selected w_char=0.080 | AUC=0.68172\nWrote submission_gamma0p97_svddual_char0.080.csv (+_shrunk) and promoted to submission.csv | mean=0.389134\n"
          ]
        }
      ]
    },
    {
      "id": "1bb53e6f-842c-4023-b034-1ee46b07ce76",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S42: Subreddit view - CountVectorizer (binary) on requester_subreddits_at_request + LR; time-aware CV; cache OOF/test\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def subs_to_text(series):\n",
        "    # requester_subreddits_at_request is list-like; join with spaces; handle NaNs\n",
        "    out = []\n",
        "    for v in series.fillna('').values:\n",
        "        if isinstance(v, list):\n",
        "            out.append(' '.join(map(str, v)))\n",
        "        else:\n",
        "            out.append(str(v))\n",
        "    return pd.Series(out, index=series.index)\n",
        "\n",
        "sub_tr = subs_to_text(train.get('requester_subreddits_at_request', pd.Series(['']*len(train))))\n",
        "sub_te = subs_to_text(test.get('requester_subreddits_at_request', pd.Series(['']*len(test))))\n",
        "\n",
        "# Time-aware 6-block forward-chaining (validate blocks 1..5)\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}', flush=True)\n",
        "\n",
        "# CountVectorizer params (binary presence, minimal caps)\n",
        "cv_params = dict(lowercase=True, binary=True, min_df=2, max_features=100_000)\n",
        "\n",
        "C = 1.0\n",
        "oof = np.zeros(n, dtype=np.float32)\n",
        "te_parts = []\n",
        "t_all = time.time()\n",
        "for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    t0 = time.time()\n",
        "    cv = CountVectorizer(**cv_params)\n",
        "    X_tr = cv.fit_transform(sub_tr.iloc[tr_idx])\n",
        "    X_va = cv.transform(sub_tr.iloc[va_idx])\n",
        "    X_te = cv.transform(sub_te)\n",
        "    clf = LogisticRegression(penalty='l2', solver='liblinear', C=C, max_iter=2000)\n",
        "    clf.fit(X_tr, y[tr_idx])\n",
        "    va_pred = clf.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "    te_pred = clf.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "    oof[va_idx] = va_pred\n",
        "    te_parts.append(te_pred)\n",
        "    auc = roc_auc_score(y[va_idx], va_pred)\n",
        "    print(f'[SubLR C={C}] Fold {fi} AUC={auc:.5f} | feats={X_tr.shape[1]} | {time.time()-t0:.1f}s', flush=True)\n",
        "    del cv, X_tr, X_va, X_te, clf; gc.collect()\n",
        "\n",
        "auc_mask = roc_auc_score(y[mask], oof[mask])\n",
        "te_mean = np.mean(te_parts, axis=0).astype(np.float32)\n",
        "print(f'[SubLR] OOF AUC(validated)={auc_mask:.5f} | total {time.time()-t_all:.1f}s', flush=True)\n",
        "np.save('oof_lr_subs_time.npy', oof.astype(np.float32))\n",
        "np.save('test_lr_subs_time.npy', te_mean.astype(np.float32))\n",
        "print('Saved oof_lr_subs_time.npy and test_lr_subs_time.npy', flush=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV: 5 folds; validated 2398/2878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SubLR C=1.0] Fold 1 AUC=0.57882 | feats=403 | 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SubLR C=1.0] Fold 2 AUC=0.56112 | feats=744 | 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SubLR C=1.0] Fold 3 AUC=0.58016 | feats=1130 | 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SubLR C=1.0] Fold 4 AUC=0.50120 | feats=1566 | 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SubLR C=1.0] Fold 5 AUC=0.43204 | feats=2189 | 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SubLR] OOF AUC(validated)=0.53578 | total 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_lr_subs_time.npy and test_lr_subs_time.npy\n"
          ]
        }
      ]
    },
    {
      "id": "35f3b31f-bbd5-4e65-9794-cc8dc5668bfb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S43: Reblend with Char LR + Subreddit LR small weights via gamma-weighted OOF; promote submission\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "ids = test[id_col].values\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Time blocks and gamma weights (gamma=0.98) over validated rows\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "n = len(train)\n",
        "mask_full = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    mask_full[np.array(blocks[i])] = True\n",
        "gamma = 0.98\n",
        "w_oof = np.zeros(n, dtype=np.float64)\n",
        "for bi in range(1, k):\n",
        "    age = (k - 1) - bi\n",
        "    w_oof[np.array(blocks[bi])] = (gamma ** age)\n",
        "\n",
        "# Load OOF/test for base components\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy')\n",
        "o_lr_ns = np.load('oof_lr_time_nosub_meta.npy')\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_time_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta_time.npy')\n",
        "o_emn = np.load('oof_xgb_emb_meta_time.npy')\n",
        "o_emp = np.load('oof_xgb_emb_mpnet_time.npy')\n",
        "o_svd_dual = np.load('oof_xgb_svd_word192_char128_meta.npy') if Path('oof_xgb_svd_word192_char128_meta.npy').exists() else None\n",
        "o_char = np.load('oof_lr_charwb_time.npy')\n",
        "o_sub = np.load('oof_lr_subs_time.npy') if Path('oof_lr_subs_time.npy').exists() else None\n",
        "\n",
        "# Convert to logits\n",
        "z_lr_w, z_lr_ns = to_logit(o_lr_w), to_logit(o_lr_ns)\n",
        "z_d1, z_d2, z_meta = to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)\n",
        "z_emn, z_emp = to_logit(o_emn), to_logit(o_emp)\n",
        "z_svd = to_logit(o_svd_dual) if (o_svd_dual is not None) else None\n",
        "z_char = to_logit(o_char)\n",
        "z_sub = to_logit(o_sub) if (o_sub is not None) else None\n",
        "\n",
        "# Test preds\n",
        "t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "t_meta = np.load('test_xgb_meta_fullbag.npy') if Path('test_xgb_meta_fullbag.npy').exists() else np.load('test_xgb_meta_time.npy')\n",
        "t_emn = np.load('test_xgb_emb_minilm_fullbag.npy') if Path('test_xgb_emb_minilm_fullbag.npy').exists() else np.load('test_xgb_emb_meta_time.npy')\n",
        "t_emp = np.load('test_xgb_emb_mpnet_fullbag.npy') if Path('test_xgb_emb_mpnet_fullbag.npy').exists() else np.load('test_xgb_emb_mpnet_time.npy')\n",
        "t_svd = np.load('test_xgb_svd_word192_char128_meta.npy') if Path('test_xgb_svd_word192_char128_meta.npy').exists() else None\n",
        "t_char = np.load('test_lr_charwb_time.npy')\n",
        "t_sub = np.load('test_lr_subs_time.npy') if Path('test_lr_subs_time.npy').exists() else None\n",
        "\n",
        "tz = lambda arr: to_logit(arr)\n",
        "\n",
        "# Base gamma-best weights (S37e) + tune small extras\n",
        "g = 0.97\n",
        "base = dict(w_lr=0.21, w_d1=0.176, w_d2=0.044, w_meta=0.22, w_emn=0.15, w_emp=0.15, w_svd=(0.05 if z_svd is not None else 0.0))\n",
        "\n",
        "def score_oof(w_char, w_sub):\n",
        "    extra = w_char + (w_sub if z_sub is not None else 0.0)\n",
        "    if extra >= 0.20:\n",
        "        return -1.0, None  # guard, shouldn't happen for our grid\n",
        "    scale = 1.0 - extra\n",
        "    w_lr = base['w_lr']*scale; w_d1 = base['w_d1']*scale; w_d2 = base['w_d2']*scale\n",
        "    w_meta = base['w_meta']*scale; w_emn = base['w_emn']*scale; w_emp = base['w_emp']*scale; w_svd = base['w_svd']*scale\n",
        "    z_lr_mix = (1.0 - g)*z_lr_w + g*z_lr_ns\n",
        "    z = (w_lr*z_lr_mix + w_d1*z_d1 + w_d2*z_d2 + w_meta*z_meta + w_emn*z_emn + w_emp*z_emp)\n",
        "    if z_svd is not None and w_svd > 0: z = z + w_svd*z_svd\n",
        "    if w_char > 0: z = z + w_char*z_char\n",
        "    if (z_sub is not None) and (w_sub > 0): z = z + w_sub*z_sub\n",
        "    auc = roc_auc_score(y[mask_full], z[mask_full], sample_weight=w_oof[mask_full])\n",
        "    return auc, dict(w_lr=w_lr, w_d1=w_d1, w_d2=w_d2, w_meta=w_meta, w_emn=w_emn, w_emp=w_emp, w_svd=w_svd, w_char=w_char, w_sub=(w_sub if z_sub is not None else 0.0))\n",
        "\n",
        "w_char_grid = [0.05, 0.08]\n",
        "w_sub_grid = [0.0, 0.01, 0.02] if (z_sub is not None) else [0.0]\n",
        "best_auc, best_cfg = -1.0, None\n",
        "for wc in w_char_grid:\n",
        "    for ws in w_sub_grid:\n",
        "        auc, cfg = score_oof(wc, ws)\n",
        "        print(f'[Char+Subs] w_char={wc:.3f} w_sub={ws:.3f} | gamma-OOF AUC={auc:.5f}')\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_cfg = auc, cfg\n",
        "print(f'[Char+Subs] Selected cfg: {best_cfg} | AUC={best_auc:.5f}')\n",
        "\n",
        "def build_test(cfg):\n",
        "    tz_lr_mix = (1.0 - g)*tz(t_lr_w) + g*tz(t_lr_ns)\n",
        "    parts = [\n",
        "        cfg['w_lr']*tz_lr_mix,\n",
        "        cfg['w_d1']*tz(t_d1),\n",
        "        cfg['w_d2']*tz(t_d2),\n",
        "        cfg['w_meta']*tz(t_meta),\n",
        "        cfg['w_emn']*tz(t_emn),\n",
        "        cfg['w_emp']*tz(t_emp)\n",
        "    ]\n",
        "    if (t_svd is not None) and (cfg['w_svd'] > 0): parts.append(cfg['w_svd']*tz(t_svd))\n",
        "    if cfg['w_char'] > 0: parts.append(cfg['w_char']*tz(t_char))\n",
        "    if (t_sub is not None) and (cfg['w_sub'] > 0): parts.append(cfg['w_sub']*tz(t_sub))\n",
        "    zt = np.sum(parts, axis=0)\n",
        "    pt = sigmoid(zt).astype(np.float32)\n",
        "    tag = f'char{cfg[\"w_char\"]:.3f}_subs{cfg[\"w_sub\"]:.3f}'\n",
        "    out_path = f'submission_gamma0p97_svddual_{tag}.csv'\n",
        "    pd.DataFrame({id_col: ids, target_col: pt}).to_csv(out_path, index=False)\n",
        "    # 15% shrink-to-equal hedge\n",
        "    comp_logits = [tz_lr_mix, tz(t_d1), tz(t_d2), tz(t_meta), tz(t_emn), tz(t_emp)]\n",
        "    w_list = [cfg['w_lr'], cfg['w_d1'], cfg['w_d2'], cfg['w_meta'], cfg['w_emn'], cfg['w_emp']]\n",
        "    if (t_svd is not None) and (cfg['w_svd'] > 0): comp_logits.append(tz(t_svd)); w_list.append(cfg['w_svd'])\n",
        "    if cfg['w_char'] > 0: comp_logits.append(tz(t_char)); w_list.append(cfg['w_char'])\n",
        "    if (t_sub is not None) and (cfg['w_sub'] > 0): comp_logits.append(tz(t_sub)); w_list.append(cfg['w_sub'])\n",
        "    w_vec = np.array(w_list, dtype=np.float64)\n",
        "    w_eq = np.ones_like(w_vec)/len(w_vec)\n",
        "    alpha = 0.15\n",
        "    w_shr = ((1.0 - alpha)*w_vec + alpha*w_eq); w_shr = (w_shr / w_shr.sum()).astype(np.float64)\n",
        "    zt_shr = np.zeros_like(comp_logits[0], dtype=np.float64)\n",
        "    for wi, zi in zip(w_shr, comp_logits):\n",
        "        zt_shr += wi*zi\n",
        "    pt_shr = sigmoid(zt_shr).astype(np.float32)\n",
        "    pd.DataFrame({id_col: ids, target_col: pt_shr}).to_csv(out_path.replace('.csv','_shrunk.csv'), index=False)\n",
        "    # Promote\n",
        "    pd.DataFrame({id_col: ids, target_col: pt}).to_csv('submission.csv', index=False)\n",
        "    print(f'Wrote {out_path} (+_shrunk) and promoted to submission.csv | mean={pt.mean():.6f}')\n",
        "\n",
        "build_test(best_cfg)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Char+Subs] w_char=0.050 w_sub=0.000 | gamma-OOF AUC=0.68171\n[Char+Subs] w_char=0.050 w_sub=0.010 | gamma-OOF AUC=0.68147\n[Char+Subs] w_char=0.050 w_sub=0.020 | gamma-OOF AUC=0.68106\n[Char+Subs] w_char=0.080 w_sub=0.000 | gamma-OOF AUC=0.68172\n[Char+Subs] w_char=0.080 w_sub=0.010 | gamma-OOF AUC=0.68145\n[Char+Subs] w_char=0.080 w_sub=0.020 | gamma-OOF AUC=0.68095\n[Char+Subs] Selected cfg: {'w_lr': 0.1932, 'w_d1': 0.16192, 'w_d2': 0.04048, 'w_meta': 0.2024, 'w_emn': 0.138, 'w_emp': 0.138, 'w_svd': 0.046000000000000006, 'w_char': 0.08, 'w_sub': 0.0} | AUC=0.68172\nWrote submission_gamma0p97_svddual_char0.080_subs0.000.csv (+_shrunk) and promoted to submission.csv | mean=0.389134\n"
          ]
        }
      ]
    },
    {
      "id": "d9a98cc4-5d48-46a0-b4ca-c21eecb965a5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S44: e5-base-v2 sentence embeddings + LR head (time-aware CV); cache OOF/test\n",
        "import os, sys, time, gc, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Ensure HF cache local\n",
        "os.environ['HF_HOME'] = os.path.abspath('hf_cache')\n",
        "os.environ['TRANSFORMERS_CACHE'] = os.path.abspath('hf_cache')\n",
        "\n",
        "# Install sentence-transformers if missing\n",
        "try:\n",
        "    import torch\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception:\n",
        "    import subprocess\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'sentence-transformers', 'torch'])\n",
        "    import torch\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Device: {device} | GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}', flush=True)\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body_no_leak(df):\n",
        "    return (df['request_text'] if 'request_text' in df.columns else df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "def build_text(df):\n",
        "    # e5 expects query: prefix\n",
        "    return ('query: ' + (get_title(df) + ' ' + get_body_no_leak(df))).astype(str)\n",
        "\n",
        "txt_tr = build_text(train).tolist()\n",
        "txt_te = build_text(test).tolist()\n",
        "\n",
        "# Encode e5-base-v2 (cache to .npy)\n",
        "emb_tr_path, emb_te_path = 'emb_e5_tr.npy', 'emb_e5_te.npy'\n",
        "if Path(emb_tr_path).exists() and Path(emb_te_path).exists():\n",
        "    E_tr = np.load(emb_tr_path).astype(np.float32)\n",
        "    E_te = np.load(emb_te_path).astype(np.float32)\n",
        "    print('Loaded cached e5 embeddings:', E_tr.shape, E_te.shape, flush=True)\n",
        "else:\n",
        "    model_name = 'intfloat/e5-base-v2'\n",
        "    model = SentenceTransformer(model_name, device=device)\n",
        "    # Normalize embeddings=True per expert advice\n",
        "    bs = 128\n",
        "    t0 = time.time()\n",
        "    E_tr = model.encode(txt_tr, batch_size=bs, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True).astype(np.float32)\n",
        "    E_te = model.encode(txt_te, batch_size=bs, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True).astype(np.float32)\n",
        "    np.save(emb_tr_path, E_tr); np.save(emb_te_path, E_te)\n",
        "    print(f'Encoded e5: tr {E_tr.shape} te {E_te.shape} | {time.time()-t0:.1f}s', flush=True)\n",
        "    del model; torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "# Time-aware 6-block forward-chaining (validate 1..5)\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'Time-CV folds={len(folds)}; validated {mask.sum()}/{n}', flush=True)\n",
        "\n",
        "# LR head (no meta first pass), StandardScaler on embeddings\n",
        "oof = np.zeros(n, dtype=np.float32)\n",
        "te_parts = []\n",
        "t_all = time.time()\n",
        "for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    t0 = time.time()\n",
        "    X_tr = E_tr[tr_idx]; X_va = E_tr[va_idx]; X_te = E_te\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_tr_s = scaler.fit_transform(X_tr).astype(np.float32)\n",
        "    X_va_s = scaler.transform(X_va).astype(np.float32)\n",
        "    X_te_s = scaler.transform(X_te).astype(np.float32)\n",
        "    clf = LogisticRegression(penalty='l2', solver='saga', C=1.0, max_iter=2000, n_jobs=-1, verbose=0)\n",
        "    clf.fit(X_tr_s, y[tr_idx])\n",
        "    va_pred = clf.predict_proba(X_va_s)[:,1].astype(np.float32)\n",
        "    te_pred = clf.predict_proba(X_te_s)[:,1].astype(np.float32)\n",
        "    oof[va_idx] = va_pred; te_parts.append(te_pred)\n",
        "    auc = roc_auc_score(y[va_idx], va_pred)\n",
        "    print(f'[e5 LR] Fold {fi} AUC={auc:.5f} | {time.time()-t0:.1f}s', flush=True)\n",
        "    del X_tr, X_va, X_te, X_tr_s, X_va_s, X_te_s, scaler, clf; gc.collect()\n",
        "\n",
        "auc_mask = roc_auc_score(y[mask], oof[mask])\n",
        "te_mean = np.mean(te_parts, axis=0).astype(np.float32)\n",
        "print(f'[e5 LR] OOF AUC(validated)={auc_mask:.5f} | total {time.time()-t_all:.1f}s', flush=True)\n",
        "np.save('oof_e5_lr_time.npy', oof.astype(np.float32))\n",
        "np.save('test_e5_lr_time.npy', te_mean.astype(np.float32))\n",
        "print('Saved oof_e5_lr_time.npy and test_e5_lr_time.npy', flush=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | GPU: Tesla T4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:   0%|          | 0/23 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:   4%|\u258d         | 1/23 [00:03<01:25,  3.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:   9%|\u258a         | 2/23 [00:06<01:01,  2.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  13%|\u2588\u258e        | 3/23 [00:07<00:47,  2.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  17%|\u2588\u258b        | 4/23 [00:09<00:40,  2.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  22%|\u2588\u2588\u258f       | 5/23 [00:11<00:34,  1.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  26%|\u2588\u2588\u258c       | 6/23 [00:12<00:27,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  30%|\u2588\u2588\u2588       | 7/23 [00:13<00:23,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  35%|\u2588\u2588\u2588\u258d      | 8/23 [00:14<00:19,  1.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  39%|\u2588\u2588\u2588\u2589      | 9/23 [00:15<00:16,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  43%|\u2588\u2588\u2588\u2588\u258e     | 10/23 [00:16<00:14,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  48%|\u2588\u2588\u2588\u2588\u258a     | 11/23 [00:16<00:12,  1.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 12/23 [00:17<00:10,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 13/23 [00:18<00:08,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 14/23 [00:19<00:07,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 15/23 [00:19<00:06,  1.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 16/23 [00:20<00:05,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 17/23 [00:21<00:04,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 18/23 [00:21<00:03,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 19/23 [00:22<00:02,  1.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 20/23 [00:22<00:01,  1.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 21/23 [00:23<00:01,  1.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 22/23 [00:23<00:00,  2.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 23/23 [00:23<00:00,  2.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 23/23 [00:23<00:00,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  10%|\u2588         | 1/10 [00:00<00:03,  3.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  20%|\u2588\u2588        | 2/10 [00:00<00:02,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  30%|\u2588\u2588\u2588       | 3/10 [00:00<00:01,  4.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  40%|\u2588\u2588\u2588\u2588      | 4/10 [00:01<00:01,  4.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:01<00:01,  4.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:01<00:00,  4.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:01<00:00,  5.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:01<00:00,  5.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:01<00:00,  6.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:01<00:00,  5.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded e5: tr (2878, 768) te (1162, 768) | 25.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV folds=5; validated 2398/2878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[e5 LR] Fold 1 AUC=0.60530 | 4.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[e5 LR] Fold 2 AUC=0.53903 | 14.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[e5 LR] Fold 3 AUC=0.53458 | 28.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[e5 LR] Fold 4 AUC=0.57967 | 36.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[e5 LR] Fold 5 AUC=0.54606 | 41.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[e5 LR] OOF AUC(validated)=0.55704 | total 127.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_e5_lr_time.npy and test_e5_lr_time.npy\n"
          ]
        }
      ]
    },
    {
      "id": "e7cdcde9-527a-471b-8efc-7d2acb52d22f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S45: Meta_v2 features (+politeness/hardship/text stats/time) -> XGB (GPU), time-aware CV; cache OOF/test\n",
        "import re, numpy as np, pandas as pd, time, gc, xgboost as xgb\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body(df):\n",
        "    # use request_text only (avoid edit_aware)\n",
        "    return (df['request_text'] if 'request_text' in df.columns else df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "\n",
        "def basic_text_stats(s: pd.Series):\n",
        "    # fast vectorized stats\n",
        "    lens = s.str.len().fillna(0).astype(np.int32).values\n",
        "    n_excl = s.str.count('!').fillna(0).astype(np.int32).values\n",
        "    n_q = s.str.count('\\?').fillna(0).astype(np.int32).values\n",
        "    n_url = s.str.count(r'https?://|www\\.').fillna(0).astype(np.int32).values\n",
        "    imgur_flag = s.str.contains('imgur', case=False, na=False).astype(np.int8).values\n",
        "    n_digit = s.str.count(r'\\d').fillna(0).astype(np.int32).values\n",
        "    # words\n",
        "    words = s.str.split()\n",
        "    n_words = words.apply(lambda x: len(x) if isinstance(x, list) else 0).astype(np.int32).values\n",
        "    # caps ratio\n",
        "    n_caps = s.str.count(r'[A-Z]').fillna(0).astype(np.int32).values\n",
        "    caps_ratio = np.divide(n_caps, np.maximum(lens, 1), dtype=np.float32)\n",
        "    digit_ratio = np.divide(n_digit, np.maximum(lens, 1), dtype=np.float32)\n",
        "    return dict(len=lens.astype(np.float32), n_excl=n_excl.astype(np.float32), n_q=n_q.astype(np.float32),\n",
        "                n_url=n_url.astype(np.float32), imgur=imgur_flag.astype(np.float32), n_words=n_words.astype(np.float32),\n",
        "                caps_ratio=caps_ratio.astype(np.float32), digit_ratio=digit_ratio.astype(np.float32))\n",
        "\n",
        "def keyword_counts(s: pd.Series, patterns):\n",
        "    out = {}\n",
        "    for name, pat in patterns.items():\n",
        "        out[name] = s.str.count(pat).fillna(0).astype(np.float32).values\n",
        "    return out\n",
        "\n",
        "def build_meta_v2(df: pd.DataFrame):\n",
        "    title = get_title(df); body = get_body(df)\n",
        "    text = (title + '\\n' + body)\n",
        "    # stats\n",
        "    ts = basic_text_stats(title); bs = basic_text_stats(body); xs = basic_text_stats(text)\n",
        "    # politeness / social cues / hardship keywords (case-insensitive)\n",
        "    pats = {\n",
        "        'kw_please': r'(?i)\\bplease\\b',\n",
        "        'kw_thanks': r'(?i)\\bthank(s| you)?\\b',\n",
        "        'kw_appreciate': r'(?i)\\bappreciate\\b',\n",
        "        'kw_piz': r'(?i)\\bpizza\\b',\n",
        "        'kw_payit': r'(?i)pay( it)? forward',\n",
        "        'kw_broke': r'(?i)\\bbroke\\b|no money',\n",
        "        'kw_rent': r'(?i)\\brent\\b|\\bbill(s)?\\b|utilities|electric|gas',\n",
        "        'kw_job': r'(?i)\\bjob\\b|\\bunemploy(ed|ment)?\\b|\\bfired\\b',\n",
        "        'kw_student': r'(?i)\\bstudent(s)?\\b|\\bfinal(s)?\\b|\\bexam(s)?\\b|\\bcollege\\b|\\bclass(es)?\\b',\n",
        "        'kw_family': r'(?i)\\bfamily\\b|\\bkid(s)?\\b|\\bchild(ren)?\\b|\\bwife\\b|\\bhusband\\b',\n",
        "        'kw_health': r'(?i)\\bhealth\\b|\\bhospital\\b|\\bsick\\b|\\bill(ness)?\\b',\n",
        "        'kw_money': r'(?i)\\$|dollar(s)?|cash|money'\n",
        "    }\n",
        "    kc = keyword_counts(text, pats)\n",
        "    # time features\n",
        "    ts_unix = df['unix_timestamp_of_request'].astype(np.int64).values\n",
        "    # convert to UTC datetime\n",
        "    dt = pd.to_datetime(ts_unix, unit='s', utc=True)\n",
        "    # dt is a DatetimeIndex; access fields directly\n",
        "    hour = dt.hour.values.astype(np.int16)\n",
        "    weekday = dt.weekday.values.astype(np.int16)\n",
        "    # one-hot hour (0-23) and weekday (0-6) compact\n",
        "    hour_oh = np.eye(24, dtype=np.float32)[hour]\n",
        "    wday_oh = np.eye(7, dtype=np.float32)[weekday]\n",
        "    # assemble feature matrix\n",
        "    cols = []\n",
        "    def stack(d):\n",
        "        for k in sorted(d.keys()):\n",
        "            cols.append(d[k])\n",
        "    stack(ts); stack(bs); stack(xs); stack(kc)\n",
        "    cols.append(hour_oh) ; cols.append(wday_oh)\n",
        "    X = np.column_stack([c if c.ndim==1 else c for c in cols]).astype(np.float32)\n",
        "    return X\n",
        "\n",
        "t0 = time.time()\n",
        "X_tr = build_meta_v2(train); X_te = build_meta_v2(test)\n",
        "print('Meta_v2 shapes:', X_tr.shape, X_te.shape, '| build', f'{time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "# Time-aware 6-block folds (validate 1..5)\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}', flush=True)\n",
        "\n",
        "# XGB params (GPU) same family as earlier\n",
        "params = dict(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='auc',\n",
        "    max_depth=4,\n",
        "    eta=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.6,\n",
        "    min_child_weight=6,\n",
        "    reg_alpha=0.3,\n",
        "    reg_lambda=3.0,\n",
        "    gamma=0.0,\n",
        "    device='cuda',\n",
        "    tree_method='hist'\n",
        ")\n",
        "\n",
        "oof = np.zeros(n, dtype=np.float32)\n",
        "te_parts = []\n",
        "t_all = time.time()\n",
        "for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    t1 = time.time()\n",
        "    Xtr, Xva = X_tr[tr_idx], X_tr[va_idx]\n",
        "    ytr, yva = y[tr_idx], y[va_idx]\n",
        "    dtr = xgb.DMatrix(Xtr, label=ytr); dva = xgb.DMatrix(Xva, label=yva); dte = xgb.DMatrix(X_te)\n",
        "    pos = float((ytr == 1).sum()); neg = float((ytr == 0).sum())\n",
        "    spw = (neg / max(pos, 1.0)) if pos > 0 else 1.0\n",
        "    p = dict(params); p['scale_pos_weight'] = spw; p['seed'] = 42 + fi\n",
        "    booster = xgb.train(p, dtr, num_boost_round=4000, evals=[(dva, 'valid')], early_stopping_rounds=100, verbose_eval=False)\n",
        "    va_pred = booster.predict(dva).astype(np.float32)\n",
        "    te_pred = booster.predict(dte, iteration_range=(0, booster.best_iteration+1 if booster.best_iteration is not None else 0)).astype(np.float32)\n",
        "    oof[va_idx] = va_pred; te_parts.append(te_pred)\n",
        "    auc = roc_auc_score(yva, va_pred)\n",
        "    print(f'[Meta_v2 XGB] Fold {fi} AUC={auc:.5f} | rounds={booster.best_iteration} | {time.time()-t1:.1f}s', flush=True)\n",
        "    del dtr, dva, dte, booster; gc.collect()\n",
        "\n",
        "auc_mask = roc_auc_score(y[mask], oof[mask])\n",
        "te_mean = np.mean(te_parts, axis=0).astype(np.float32)\n",
        "print(f'[Meta_v2 XGB] OOF AUC(validated)={auc_mask:.5f} | total {time.time()-t_all:.1f}s', flush=True)\n",
        "np.save('oof_xgb_meta_time_v2.npy', oof.astype(np.float32))\n",
        "np.save('test_xgb_meta_time_v2.npy', te_mean.astype(np.float32))\n",
        "print('Saved oof_xgb_meta_time_v2.npy and test_xgb_meta_time_v2.npy', flush=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta_v2 shapes: (2878, 67) (1162, 67) | build 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV: 5 folds; validated 2398/2878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta_v2 XGB] Fold 1 AUC=0.58249 | rounds=64 | 0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta_v2 XGB] Fold 2 AUC=0.59500 | rounds=65 | 0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta_v2 XGB] Fold 3 AUC=0.55075 | rounds=3 | 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta_v2 XGB] Fold 4 AUC=0.62124 | rounds=22 | 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta_v2 XGB] Fold 5 AUC=0.58841 | rounds=79 | 0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta_v2 XGB] OOF AUC(validated)=0.58848 | total 2.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_xgb_meta_time_v2.npy and test_xgb_meta_time_v2.npy\n"
          ]
        }
      ]
    },
    {
      "id": "19f3314a-8560-4f78-9902-a3971e016452",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S46: Reblend with meta_v2 option + add LR_main (text-only) + Char LR; gamma-weighted OOF selection; write submission\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "ids = test[id_col].values\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Time blocks and gamma weights (gamma=0.98) over validated rows\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "n = len(train)\n",
        "mask_full = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    mask_full[np.array(blocks[i])] = True\n",
        "gamma = 0.98\n",
        "w_oof = np.zeros(n, dtype=np.float64)\n",
        "for bi in range(1, k):\n",
        "    age = (k - 1) - bi\n",
        "    w_oof[np.array(blocks[bi])] = (gamma ** age)\n",
        "\n",
        "# Load OOF/test for base components\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy')\n",
        "o_lr_ns = np.load('oof_lr_time_nosub_meta.npy')\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_time_v2.npy')\n",
        "o_meta_v1 = np.load('oof_xgb_meta_time.npy')\n",
        "o_meta_v2 = np.load('oof_xgb_meta_time_v2.npy') if Path('oof_xgb_meta_time_v2.npy').exists() else None\n",
        "o_emn = np.load('oof_xgb_emb_meta_time.npy')\n",
        "o_emp = np.load('oof_xgb_emb_mpnet_time.npy')\n",
        "o_svd_dual = np.load('oof_xgb_svd_word192_char128_meta.npy') if Path('oof_xgb_svd_word192_char128_meta.npy').exists() else None\n",
        "o_char = np.load('oof_lr_charwb_time.npy') if Path('oof_lr_charwb_time.npy').exists() else None\n",
        "o_lr_main = np.load('oof_lr_main_time.npy') if Path('oof_lr_main_time.npy').exists() else None\n",
        "\n",
        "# Convert to logits\n",
        "z_lr_w, z_lr_ns = to_logit(o_lr_w), to_logit(o_lr_ns)\n",
        "z_d1, z_d2 = to_logit(o_d1), to_logit(o_d2)\n",
        "z_meta_v1 = to_logit(o_meta_v1)\n",
        "z_meta_v2 = to_logit(o_meta_v2) if o_meta_v2 is not None else None\n",
        "z_emn, z_emp = to_logit(o_emn), to_logit(o_emp)\n",
        "z_svd = to_logit(o_svd_dual) if o_svd_dual is not None else None\n",
        "z_char = to_logit(o_char) if o_char is not None else None\n",
        "z_lrmain = to_logit(o_lr_main) if o_lr_main is not None else None\n",
        "\n",
        "# Test preds\n",
        "t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "t_meta_v1 = np.load('test_xgb_meta_fullbag.npy') if Path('test_xgb_meta_fullbag.npy').exists() else np.load('test_xgb_meta_time.npy')\n",
        "t_meta_v2 = np.load('test_xgb_meta_time_v2.npy') if Path('test_xgb_meta_time_v2.npy').exists() else None\n",
        "t_emn = np.load('test_xgb_emb_minilm_fullbag.npy') if Path('test_xgb_emb_minilm_fullbag.npy').exists() else np.load('test_xgb_emb_meta_time.npy')\n",
        "t_emp = np.load('test_xgb_emb_mpnet_fullbag.npy') if Path('test_xgb_emb_mpnet_fullbag.npy').exists() else np.load('test_xgb_emb_mpnet_time.npy')\n",
        "t_svd = np.load('test_xgb_svd_word192_char128_meta.npy') if Path('test_xgb_svd_word192_char128_meta.npy').exists() else None\n",
        "t_char = np.load('test_lr_charwb_time.npy') if Path('test_lr_charwb_time.npy').exists() else None\n",
        "t_lr_main = np.load('test_lr_main_time.npy') if Path('test_lr_main_time.npy').exists() else None\n",
        "\n",
        "tz = lambda arr: to_logit(arr)\n",
        "\n",
        "# Base gamma-best weights (S37e) as starting point\n",
        "g = 0.97\n",
        "base = dict(w_lr=0.21, w_d1=0.176, w_d2=0.044, w_meta=0.22, w_emn=0.15, w_emp=0.15, w_svd=(0.05 if z_svd is not None else 0.0))\n",
        "\n",
        "def score_cfg(w_char, w_lrmain, use_meta_v2):\n",
        "    extra = (w_char if (z_char is not None) else 0.0) + (w_lrmain if (z_lrmain is not None) else 0.0)\n",
        "    if extra >= 0.20:\n",
        "        return -1.0, None\n",
        "    scale = 1.0 - extra\n",
        "    w_lr = base['w_lr']*scale; w_d1 = base['w_d1']*scale; w_d2 = base['w_d2']*scale\n",
        "    w_meta = base['w_meta']*scale; w_emn = base['w_emn']*scale; w_emp = base['w_emp']*scale; w_svd = base['w_svd']*scale\n",
        "    z_meta_use = (z_meta_v2 if (use_meta_v2 and z_meta_v2 is not None) else z_meta_v1)\n",
        "    z_lr_mix = (1.0 - g)*z_lr_w + g*z_lr_ns\n",
        "    z = (w_lr*z_lr_mix + w_d1*z_d1 + w_d2*z_d2 + w_meta*z_meta_use + w_emn*z_emn + w_emp*z_emp)\n",
        "    if z_svd is not None and w_svd > 0: z += w_svd*z_svd\n",
        "    if (z_char is not None) and (w_char > 0): z += w_char*z_char\n",
        "    if (z_lrmain is not None) and (w_lrmain > 0): z += w_lrmain*z_lrmain\n",
        "    auc = roc_auc_score(y[mask_full], z[mask_full], sample_weight=w_oof[mask_full])\n",
        "    cfg = dict(w_lr=w_lr, w_d1=w_d1, w_d2=w_d2, w_meta=w_meta, w_emn=w_emn, w_emp=w_emp, w_svd=w_svd, w_char=w_char if z_char is not None else 0.0, w_lrmain=w_lrmain if z_lrmain is not None else 0.0, use_meta_v2=bool(use_meta_v2))\n",
        "    return auc, cfg\n",
        "\n",
        "w_char_grid = [0.05, 0.08] if z_char is not None else [0.0]\n",
        "w_lrmain_grid = [0.0, 0.03, 0.05] if z_lrmain is not None else [0.0]\n",
        "use_meta_v2_grid = [False, True] if (z_meta_v2 is not None) else [False]\n",
        "\n",
        "best_auc, best_cfg = -1.0, None\n",
        "for wc in w_char_grid:\n",
        "    for wl in w_lrmain_grid:\n",
        "        for um2 in use_meta_v2_grid:\n",
        "            auc, cfg = score_cfg(wc, wl, um2)\n",
        "            print(f\"[Reblend] w_char={wc:.3f} w_lrmain={wl:.3f} use_meta_v2={um2} | gamma-OOF AUC={auc:.5f}\")\n",
        "            if auc > best_auc:\n",
        "                best_auc, best_cfg = auc, cfg\n",
        "print('[Reblend] Selected cfg:', best_cfg, '| AUC=', f'{best_auc:.5f}')\n",
        "\n",
        "def build_test(cfg):\n",
        "    tz_lr_mix = (1.0 - g)*tz(t_lr_w) + g*tz(t_lr_ns)\n",
        "    t_meta_use = (t_meta_v2 if (cfg['use_meta_v2'] and t_meta_v2 is not None) else t_meta_v1)\n",
        "    parts = [\n",
        "        cfg['w_lr']*tz_lr_mix,\n",
        "        cfg['w_d1']*tz(t_d1),\n",
        "        cfg['w_d2']*tz(t_d2),\n",
        "        cfg['w_meta']*tz(t_meta_use),\n",
        "        cfg['w_emn']*tz(t_emn),\n",
        "        cfg['w_emp']*tz(t_emp)\n",
        "    ]\n",
        "    if (t_svd is not None) and (cfg['w_svd'] > 0): parts.append(cfg['w_svd']*tz(t_svd))\n",
        "    if (t_char is not None) and (cfg['w_char'] > 0): parts.append(cfg['w_char']*tz(t_char))\n",
        "    if (t_lr_main is not None) and (cfg['w_lrmain'] > 0): parts.append(cfg['w_lrmain']*tz(t_lr_main))\n",
        "    zt = np.sum(parts, axis=0)\n",
        "    pt = sigmoid(zt).astype(np.float32)\n",
        "    tag = f\"gamma0p97_meta{'v2' if cfg['use_meta_v2'] else 'v1'}_char{cfg['w_char']:.3f}_lrmain{cfg['w_lrmain']:.3f}\"\n",
        "    out_path = f'submission_{tag}.csv'\n",
        "    pd.DataFrame({id_col: ids, target_col: pt}).to_csv(out_path, index=False)\n",
        "    # 15% shrink-to-equal hedge\n",
        "    comp_logits = [tz_lr_mix, tz(t_d1), tz(t_d2), tz(t_meta_use), tz(t_emn), tz(t_emp)]\n",
        "    w_list = [cfg['w_lr'], cfg['w_d1'], cfg['w_d2'], cfg['w_meta'], cfg['w_emn'], cfg['w_emp']]\n",
        "    if (t_svd is not None) and (cfg['w_svd'] > 0): comp_logits.append(tz(t_svd)); w_list.append(cfg['w_svd'])\n",
        "    if (t_char is not None) and (cfg['w_char'] > 0): comp_logits.append(tz(t_char)); w_list.append(cfg['w_char'])\n",
        "    if (t_lr_main is not None) and (cfg['w_lrmain'] > 0): comp_logits.append(tz(t_lr_main)); w_list.append(cfg['w_lrmain'])\n",
        "    w_vec = np.array(w_list, dtype=np.float64)\n",
        "    w_eq = np.ones_like(w_vec)/len(w_vec)\n",
        "    alpha = 0.15\n",
        "    w_shr = ((1.0 - alpha)*w_vec + alpha*w_eq); w_shr = (w_shr / w_shr.sum()).astype(np.float64)\n",
        "    zt_shr = np.zeros_like(comp_logits[0], dtype=np.float64)\n",
        "    for wi, zi in zip(w_shr, comp_logits):\n",
        "        zt_shr += wi*zi\n",
        "    pt_shr = sigmoid(zt_shr).astype(np.float32)\n",
        "    pd.DataFrame({id_col: ids, target_col: pt_shr}).to_csv(out_path.replace('.csv','_shrunk.csv'), index=False)\n",
        "    # Promote\n",
        "    pd.DataFrame({id_col: ids, target_col: pt}).to_csv('submission.csv', index=False)\n",
        "    print(f\"Wrote {out_path} (+_shrunk) and promoted to submission.csv | mean={pt.mean():.6f}\")\n",
        "\n",
        "build_test(best_cfg)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Reblend] w_char=0.050 w_lrmain=0.000 use_meta_v2=False | gamma-OOF AUC=0.68171\n[Reblend] w_char=0.050 w_lrmain=0.000 use_meta_v2=True | gamma-OOF AUC=0.67177\n[Reblend] w_char=0.050 w_lrmain=0.030 use_meta_v2=False | gamma-OOF AUC=0.68158\n[Reblend] w_char=0.050 w_lrmain=0.030 use_meta_v2=True | gamma-OOF AUC=0.67169\n[Reblend] w_char=0.050 w_lrmain=0.050 use_meta_v2=False | gamma-OOF AUC=0.68144\n[Reblend] w_char=0.050 w_lrmain=0.050 use_meta_v2=True | gamma-OOF AUC=0.67159\n[Reblend] w_char=0.080 w_lrmain=0.000 use_meta_v2=False | gamma-OOF AUC=0.68172\n[Reblend] w_char=0.080 w_lrmain=0.000 use_meta_v2=True | gamma-OOF AUC=0.67179\n[Reblend] w_char=0.080 w_lrmain=0.030 use_meta_v2=False | gamma-OOF AUC=0.68144\n[Reblend] w_char=0.080 w_lrmain=0.030 use_meta_v2=True | gamma-OOF AUC=0.67167\n[Reblend] w_char=0.080 w_lrmain=0.050 use_meta_v2=False | gamma-OOF AUC=0.68126\n[Reblend] w_char=0.080 w_lrmain=0.050 use_meta_v2=True | gamma-OOF AUC=0.67156\n[Reblend] Selected cfg: {'w_lr': 0.1932, 'w_d1': 0.16192, 'w_d2': 0.04048, 'w_meta': 0.2024, 'w_emn': 0.138, 'w_emp': 0.138, 'w_svd': 0.046000000000000006, 'w_char': 0.08, 'w_lrmain': 0.0, 'use_meta_v2': False} | AUC= 0.68172\nWrote submission_gamma0p97_metav1_char0.080_lrmain0.000.csv (+_shrunk) and promoted to submission.csv | mean=0.389134\n"
          ]
        }
      ]
    },
    {
      "id": "cf3cd41c-54a2-403c-9893-9666242d27f9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S47: NB-SVM style word Count(1-2) + LR (time-aware CV); cache OOF/test\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body_no_leak(df):\n",
        "    if 'request_text' in df.columns:\n",
        "        return df['request_text'].fillna('').astype(str)\n",
        "    return df.get('request_text', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def build_text(df):\n",
        "    return (get_title(df) + '\\n' + get_body_no_leak(df)).astype(str)\n",
        "\n",
        "txt_tr = build_text(train)\n",
        "txt_te = build_text(test)\n",
        "\n",
        "# Time-aware 6-block forward-chaining (validate blocks 1..5)\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}', flush=True)\n",
        "\n",
        "# CountVectorizer params (words 1-2); NB-SVM log-count ratios per fold\n",
        "cv_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=2, max_features=200_000)\n",
        "alpha = 1.0  # smoothing\n",
        "C = 1.0\n",
        "\n",
        "oof = np.zeros(n, dtype=np.float32)\n",
        "te_parts = []\n",
        "t_all = time.time()\n",
        "for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    t0 = time.time()\n",
        "    cv = CountVectorizer(**cv_params)\n",
        "    X_tr = cv.fit_transform(txt_tr.iloc[tr_idx])  # csr\n",
        "    X_va = cv.transform(txt_tr.iloc[va_idx])\n",
        "    X_te = cv.transform(txt_te)\n",
        "    ytr = y[tr_idx]\n",
        "    # Log-count ratio r for NB-SVM\n",
        "    pos_mask = (ytr == 1)\n",
        "    neg_mask = ~pos_mask\n",
        "    # Sum counts by class\n",
        "    p = (X_tr[pos_mask].sum(axis=0).A1 + alpha)\n",
        "    q = (X_tr[neg_mask].sum(axis=0).A1 + alpha)\n",
        "    r = np.log(p / q).astype(np.float32)  # shape (n_features,)\n",
        "    # Scale columns by r (use csc for efficient column scaling)\n",
        "    Xtr_scaled = X_tr.tocsc().multiply(r).tocsr()\n",
        "    Xva_scaled = X_va.tocsc().multiply(r).tocsr()\n",
        "    Xte_scaled = X_te.tocsc().multiply(r).tocsr()\n",
        "    # Linear LR on scaled features\n",
        "    clf = LogisticRegression(penalty='l2', solver='saga', C=C, max_iter=2000, n_jobs=-1, verbose=0)\n",
        "    clf.fit(Xtr_scaled, ytr)\n",
        "    va_pred = clf.predict_proba(Xva_scaled)[:,1].astype(np.float32)\n",
        "    te_pred = clf.predict_proba(Xte_scaled)[:,1].astype(np.float32)\n",
        "    oof[va_idx] = va_pred; te_parts.append(te_pred)\n",
        "    auc = roc_auc_score(y[va_idx], va_pred)\n",
        "    print(f'[NB-SVM word(1-2) C={C}] Fold {fi} AUC={auc:.5f} | feats={X_tr.shape[1]} | {time.time()-t0:.1f}s', flush=True)\n",
        "    del cv, X_tr, X_va, X_te, Xtr_scaled, Xva_scaled, Xte_scaled, clf; gc.collect()\n",
        "\n",
        "auc_mask = roc_auc_score(y[mask], oof[mask])\n",
        "te_mean = np.mean(te_parts, axis=0).astype(np.float32)\n",
        "print(f'[NB-SVM word] OOF AUC(validated)={auc_mask:.5f} | total {time.time()-t_all:.1f}s', flush=True)\n",
        "np.save('oof_nbsvm_word_time.npy', oof.astype(np.float32))\n",
        "np.save('test_nbsvm_word_time.npy', te_mean.astype(np.float32))\n",
        "print('Saved oof_nbsvm_word_time.npy and test_nbsvm_word_time.npy', flush=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV: 5 folds; validated 2398/2878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-SVM word(1-2) C=1.0] Fold 1 AUC=0.53857 | feats=7267 | 1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-SVM word(1-2) C=1.0] Fold 2 AUC=0.55304 | feats=13015 | 2.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-SVM word(1-2) C=1.0] Fold 3 AUC=0.49857 | feats=17710 | 4.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-SVM word(1-2) C=1.0] Fold 4 AUC=0.54421 | feats=21695 | 7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-SVM word(1-2) C=1.0] Fold 5 AUC=0.55013 | feats=25193 | 8.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-SVM word] OOF AUC(validated)=0.54113 | total 25.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_nbsvm_word_time.npy and test_nbsvm_word_time.npy\n"
          ]
        }
      ]
    },
    {
      "id": "13f523e8-6349-4ff4-a6e4-36f8d34ac607",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S48: Build rank-avg and logit-avg hedges across top submissions; promote best candidate\n",
        "import numpy as np, time, glob\n",
        "from pathlib import Path\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "\n",
        "def read_sub(path):\n",
        "    ids = []; probs = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        header = f.readline()\n",
        "        for line in f:\n",
        "            rid, p = line.rstrip().split(',', 1)\n",
        "            ids.append(rid);\n",
        "            try: probs.append(float(p))\n",
        "            except: probs.append(0.5)\n",
        "    return ids, np.asarray(probs, dtype=np.float64)\n",
        "\n",
        "def rank01(x):\n",
        "    order = np.argsort(x, kind='mergesort')\n",
        "    ranks = np.empty_like(order, dtype=np.float64)\n",
        "    ranks[order] = np.arange(len(x), dtype=np.float64)\n",
        "    return ranks / max(len(x) - 1, 1)\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p, eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Candidate files (must exist)\n",
        "cands = [\n",
        "  'submission_reblend_svddual_gamma0p98.csv',\n",
        "  'submission_blend_gamma0p98_fullrefits.csv',\n",
        "  'submission_last2blend_last2.csv',\n",
        "  'submission_gamma0p97_svddual_char0.080.csv'\n",
        "]\n",
        "avail = [p for p in cands if Path(p).exists() and Path(p).stat().st_size > 0]\n",
        "assert len(avail) >= 2, f'Not enough submissions available to hedge. Found: {avail}'\n",
        "\n",
        "# Read first to get ids\n",
        "ids0, p0 = read_sub(avail[0])\n",
        "probs = [p0]\n",
        "for p in avail[1:]:\n",
        "    ids_i, pi = read_sub(p)\n",
        "    assert ids_i == ids0, f'ID mismatch between {avail[0]} and {p}'\n",
        "    probs.append(pi)\n",
        "P = np.vstack(probs)  # (m, n_test)\n",
        "\n",
        "# 3-way rank-average (first three if available) and 4-way if all exist\n",
        "def write_sub(path, vals):\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        f.write(f'{id_col},{target_col}\\n')\n",
        "        for rid, v in zip(ids0, vals.astype(np.float32)):\n",
        "            f.write(f'{rid},{v:.8f}\\n')\n",
        "    print(f'Wrote {path} | mean={vals.mean():.6f} | size={Path(path).stat().st_size}', flush=True)\n",
        "\n",
        "m = P.shape[0]\n",
        "ranks = np.vstack([rank01(P[i]) for i in range(m)])  # (m, n)\n",
        "\n",
        "# Build several hedges\n",
        "out_paths = []\n",
        "if m >= 3:\n",
        "    ravg3 = ranks[:3].mean(axis=0)\n",
        "    path3 = 'submission_rankavg_top3.csv'\n",
        "    write_sub(path3, ravg3)\n",
        "    out_paths.append(path3)\n",
        "if m >= 4:\n",
        "    ravg4 = ranks[:4].mean(axis=0)\n",
        "    path4 = 'submission_rankavg_top4.csv'\n",
        "    write_sub(path4, ravg4)\n",
        "    out_paths.append(path4)\n",
        "\n",
        "# Logit-average across available (slightly more aggressive than rank-avg)\n",
        "logits = to_logit(P)\n",
        "z_mean = logits.mean(axis=0)\n",
        "p_mean = sigmoid(z_mean)\n",
        "path_logit = 'submission_logitavg_all.csv'\n",
        "write_sub(path_logit, p_mean)\n",
        "out_paths.append(path_logit)\n",
        "\n",
        "# Promote preferred hedge: prefer rankavg_top4, else rankavg_top3, else logitavg_all\n",
        "prom = next((p for p in ['submission_rankavg_top4.csv','submission_rankavg_top3.csv','submission_logitavg_all.csv'] if Path(p).exists()), None)\n",
        "assert prom is not None, 'No hedge file generated'\n",
        "Path(prom).replace('submission.csv')\n",
        "print(f'Promoted {prom} to submission.csv', flush=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_rankavg_top3.csv | mean=0.500000 | size=23773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_rankavg_top4.csv | mean=0.500000 | size=23773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_logitavg_all.csv | mean=0.394812 | size=23773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promoted submission_rankavg_top4.csv to submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "034f3250-a6bc-4a28-857c-4a729f513170",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S49: CatBoost with native text (title, body) + meta_v1 (GPU), time-aware CV; cache OOF/test\n",
        "import os, sys, time, gc, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure CatBoost installed\n",
        "try:\n",
        "    from catboost import CatBoostClassifier, Pool\n",
        "except Exception:\n",
        "    import subprocess\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'catboost'])\n",
        "    from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body(df):\n",
        "    # Avoid edit_aware; use request_text only\n",
        "    return (df['request_text'] if 'request_text' in df.columns else df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "\n",
        "# Build text columns\n",
        "title_tr = get_title(train); body_tr = get_body(train)\n",
        "title_te = get_title(test);  body_te = get_body(test)\n",
        "\n",
        "# Load meta_v1 features (numeric)\n",
        "Meta_tr = np.load('meta_v1_tr.npy').astype(np.float32)  # shape (n, d)\n",
        "Meta_te = np.load('meta_v1_te.npy').astype(np.float32)\n",
        "n_meta = Meta_tr.shape[1]\n",
        "\n",
        "# Assemble DataFrames with two text cols followed by meta numeric columns\n",
        "meta_cols = [f'm{i}' for i in range(n_meta)]\n",
        "Xtr_df = pd.DataFrame({'title': title_tr, 'body': body_tr})\n",
        "for i, col in enumerate(meta_cols):\n",
        "    Xtr_df[col] = Meta_tr[:, i]\n",
        "Xte_df = pd.DataFrame({'title': title_te, 'body': body_te})\n",
        "for i, col in enumerate(meta_cols):\n",
        "    Xte_df[col] = Meta_te[:, i]\n",
        "\n",
        "# Time-aware 6-block forward-chaining (validate blocks 1..5)\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}', flush=True)\n",
        "\n",
        "# CatBoost params (GPU) per expert guidance\n",
        "base_params = dict(\n",
        "    iterations=3000,\n",
        "    depth=6,\n",
        "    learning_rate=0.05,\n",
        "    l2_leaf_reg=6.0,\n",
        "    bagging_temperature=0.75,\n",
        "    random_strength=1.0,\n",
        "    loss_function='Logloss',\n",
        "    eval_metric='AUC',\n",
        "    task_type='GPU',\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "text_features = [0, 1]  # indices of text columns in Pool\n",
        "\n",
        "oof = np.zeros(n, dtype=np.float32)\n",
        "te_parts = []\n",
        "t_all = time.time()\n",
        "for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    t0 = time.time()\n",
        "    X_tr_fold = Xtr_df.iloc[tr_idx].reset_index(drop=True)\n",
        "    X_va_fold = Xtr_df.iloc[va_idx].reset_index(drop=True)\n",
        "    y_tr = y[tr_idx]; y_va = y[va_idx]\n",
        "    # Class imbalance handling\n",
        "    pos = float((y_tr == 1).sum()); neg = float((y_tr == 0).sum())\n",
        "    spw = (neg / max(pos, 1.0)) if pos > 0 else 1.0\n",
        "    params = dict(base_params); params['scale_pos_weight'] = spw; params['random_seed'] = 42 + fi\n",
        "    # Pools with text feature indices\n",
        "    pool_tr = Pool(X_tr_fold, label=y_tr, text_features=text_features)\n",
        "    pool_va = Pool(X_va_fold, label=y_va, text_features=text_features)\n",
        "    pool_te = Pool(Xte_df, text_features=text_features)\n",
        "    model = CatBoostClassifier(**params)\n",
        "    model.fit(pool_tr, eval_set=pool_va, use_best_model=True, early_stopping_rounds=100)\n",
        "    va_pred = model.predict_proba(pool_va)[:, 1].astype(np.float32)\n",
        "    te_pred = model.predict_proba(pool_te)[:, 1].astype(np.float32)\n",
        "    oof[va_idx] = va_pred\n",
        "    te_parts.append(te_pred)\n",
        "    # On-the-fly AUC (without import to keep this cell self-contained for speed)\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    auc = roc_auc_score(y_va, va_pred) if (y_va.min()!=y_va.max()) else 0.5\n",
        "    best_it = getattr(model, 'best_iteration_', None)\n",
        "    print(f'[CatTextMeta] Fold {fi} AUC={auc:.5f} | spw={spw:.2f} | best_it={best_it} | {time.time()-t0:.1f}s', flush=True)\n",
        "    del pool_tr, pool_va, pool_te, model; gc.collect()\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "auc_oof = roc_auc_score(y[mask], oof[mask])\n",
        "te_mean = np.mean(te_parts, axis=0).astype(np.float32)\n",
        "print(f'[CatTextMeta] DONE | OOF(validated) AUC={auc_oof:.5f} | total {time.time()-t_all:.1f}s', flush=True)\n",
        "np.save('oof_catboost_textmeta.npy', oof.astype(np.float32))\n",
        "np.save('test_catboost_textmeta.npy', te_mean.astype(np.float32))\n",
        "print('Saved oof_catboost_textmeta.npy and test_catboost_textmeta.npy', flush=True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV: 5 folds; validated 2398/2878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8653.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8651.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatTextMeta] Fold 2 AUC=0.66204 | spw=2.33 | best_it=49 | 4.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8651.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatTextMeta] Fold 3 AUC=0.62541 | spw=2.49 | best_it=106 | 6.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8651.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatTextMeta] Fold 4 AUC=0.64291 | spw=2.79 | best_it=32 | 4.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8651.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatTextMeta] Fold 5 AUC=0.62950 | spw=2.83 | best_it=381 | 13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatTextMeta] DONE | OOF(validated) AUC=0.64965 | total 34.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_catboost_textmeta.npy and test_catboost_textmeta.npy\n"
          ]
        }
      ]
    },
    {
      "id": "586df1e3-387b-4a68-8dd2-df832f81009c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S50: Fix e5 (passage: prefix) + XGB heads on e5 and concatenated embeddings (MiniLM||MPNet and e5||MiniLM||MPNet); time-aware CV; cache OOF/test\n",
        "import os, sys, time, gc, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Ensure HF cache local & sentence-transformers available\n",
        "os.environ['HF_HOME'] = os.path.abspath('hf_cache')\n",
        "os.environ['TRANSFORMERS_CACHE'] = os.path.abspath('hf_cache')\n",
        "try:\n",
        "    import torch\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception:\n",
        "    import subprocess\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'sentence-transformers', 'torch'])\n",
        "    import torch\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Device: {device}', flush=True)\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body(df):\n",
        "    return (df['request_text'] if 'request_text' in df.columns else df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "\n",
        "def build_e5_text(df):\n",
        "    # Correct prefix for document/passages per expert advice\n",
        "    return ('passage: ' + (get_title(df) + ' ' + get_body(df))).astype(str).tolist()\n",
        "\n",
        "# Time-aware 6-block forward-chaining (validate blocks 1..5)\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}', flush=True)\n",
        "\n",
        "# 1) Encode e5-base-v2 with correct prefix (normalize=True) and cache\n",
        "emb_e5_tr_path, emb_e5_te_path = 'emb_e5_tr.npy', 'emb_e5_te.npy'\n",
        "reencode_e5 = True  # force re-encode to fix prefix\n",
        "if (not reencode_e5) and Path(emb_e5_tr_path).exists() and Path(emb_e5_te_path).exists():\n",
        "    E5_tr = np.load(emb_e5_tr_path).astype(np.float32)\n",
        "    E5_te = np.load(emb_e5_te_path).astype(np.float32)\n",
        "    print('Loaded cached e5 embeddings:', E5_tr.shape, E5_te.shape, flush=True)\n",
        "else:\n",
        "    model_name = 'intfloat/e5-base-v2'\n",
        "    model = SentenceTransformer(model_name, device=device)\n",
        "    bs = 128\n",
        "    t0 = time.time()\n",
        "    txt_tr = build_e5_text(train); txt_te = build_e5_text(test)\n",
        "    E5_tr = model.encode(txt_tr, batch_size=bs, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True).astype(np.float32)\n",
        "    E5_te = model.encode(txt_te, batch_size=bs, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True).astype(np.float32)\n",
        "    np.save(emb_e5_tr_path, E5_tr); np.save(emb_e5_te_path, E5_te)\n",
        "    print(f'Encoded e5 (passage:): tr {E5_tr.shape} te {E5_te.shape} | {time.time()-t0:.1f}s', flush=True)\n",
        "    del model; torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "# 2) Load other embeddings (MiniLM, MPNet) for concatenation heads\n",
        "Emb_min_tr = np.load('emb_minilm_tr.npy').astype(np.float32)\n",
        "Emb_min_te = np.load('emb_minilm_te.npy').astype(np.float32)\n",
        "Emb_mp_tr = np.load('emb_mpnet_tr.npy').astype(np.float32)\n",
        "Emb_mp_te = np.load('emb_mpnet_te.npy').astype(np.float32)\n",
        "\n",
        "def run_xgb_head(Xtr_raw: np.ndarray, Xte_raw: np.ndarray, tag: str):\n",
        "    # Standardize per fold; XGBoost GPU with per-fold scale_pos_weight and early stopping\n",
        "    oof = np.zeros(n, dtype=np.float32)\n",
        "    te_parts = []\n",
        "    params = dict(\n",
        "        objective='binary:logistic',\n",
        "        eval_metric='auc',\n",
        "        max_depth=4,\n",
        "        eta=0.05,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.6,\n",
        "        min_child_weight=8,\n",
        "        reg_alpha=0.3,\n",
        "        reg_lambda=3.0,\n",
        "        gamma=0.0,\n",
        "        device='cuda',\n",
        "        tree_method='hist'\n",
        "    )\n",
        "    t_all = time.time()\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "        t0 = time.time()\n",
        "        X_tr_f = Xtr_raw[tr_idx]; X_va_f = Xtr_raw[va_idx]\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "        X_tr = scaler.fit_transform(X_tr_f).astype(np.float32)\n",
        "        X_va = scaler.transform(X_va_f).astype(np.float32)\n",
        "        X_te = scaler.transform(Xte_raw).astype(np.float32)\n",
        "        dtr = xgb.DMatrix(X_tr, label=y[tr_idx]); dva = xgb.DMatrix(X_va, label=y[va_idx]); dte = xgb.DMatrix(X_te)\n",
        "        pos = float((y[tr_idx] == 1).sum()); neg = float((y[tr_idx] == 0).sum()); spw = (neg / max(pos, 1.0)) if pos > 0 else 1.0\n",
        "        p = dict(params); p['seed'] = 42 + fi; p['scale_pos_weight'] = spw\n",
        "        booster = xgb.train(p, dtr, num_boost_round=4000, evals=[(dva, 'valid')], early_stopping_rounds=100, verbose_eval=False)\n",
        "        va_pred = booster.predict(dva).astype(np.float32)\n",
        "        te_pred = booster.predict(dte, iteration_range=(0, booster.best_iteration+1 if booster.best_iteration is not None else 0)).astype(np.float32)\n",
        "        oof[va_idx] = va_pred; te_parts.append(te_pred)\n",
        "        auc = roc_auc_score(y[va_idx], va_pred) if (y[va_idx].min()!=y[va_idx].max()) else 0.5\n",
        "        print(f'[{tag}] Fold {fi} AUC={auc:.5f} | rounds={booster.best_iteration} | spw={spw:.2f} | {time.time()-t0:.1f}s', flush=True)\n",
        "        del X_tr_f, X_va_f, X_tr, X_va, X_te, scaler, dtr, dva, dte, booster; gc.collect()\n",
        "    auc_oof = roc_auc_score(y[mask], oof[mask])\n",
        "    te_mean = np.mean(te_parts, axis=0).astype(np.float32)\n",
        "    print(f'[{tag}] DONE | OOF(validated) AUC={auc_oof:.5f} | total {time.time()-t_all:.1f}s', flush=True)\n",
        "    np.save(f'oof_xgb_{tag}.npy', oof.astype(np.float32))\n",
        "    np.save(f'test_xgb_{tag}.npy', te_mean.astype(np.float32))\n",
        "    print(f'Saved oof_xgb_{tag}.npy and test_xgb_{tag}.npy', flush=True)\n",
        "\n",
        "# 3) Train heads:\n",
        "# a) e5-only XGB head\n",
        "run_xgb_head(E5_tr, E5_te, tag='e5_time')\n",
        "\n",
        "# b) MiniLM||MPNet concatenation XGB head\n",
        "Emb_mm_tr = np.hstack([Emb_min_tr, Emb_mp_tr]).astype(np.float32)\n",
        "Emb_mm_te = np.hstack([Emb_min_te, Emb_mp_te]).astype(np.float32)\n",
        "run_xgb_head(Emb_mm_tr, Emb_mm_te, tag='emb_minilm_mpnet_time')\n",
        "\n",
        "# c) e5||MiniLM||MPNet concatenation XGB head\n",
        "Emb_all_tr = np.hstack([E5_tr, Emb_min_tr, Emb_mp_tr]).astype(np.float32)\n",
        "Emb_all_te = np.hstack([E5_te, Emb_min_te, Emb_mp_te]).astype(np.float32)\n",
        "run_xgb_head(Emb_all_tr, Emb_all_te, tag='emb_e5_minilm_mpnet_time')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV: 5 folds; validated 2398/2878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:   0%|          | 0/23 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:   4%|\u258d         | 1/23 [00:03<01:17,  3.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:   9%|\u258a         | 2/23 [00:05<00:58,  2.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  13%|\u2588\u258e        | 3/23 [00:07<00:46,  2.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  17%|\u2588\u258b        | 4/23 [00:09<00:39,  2.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  22%|\u2588\u2588\u258f       | 5/23 [00:10<00:34,  1.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  26%|\u2588\u2588\u258c       | 6/23 [00:11<00:27,  1.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  30%|\u2588\u2588\u2588       | 7/23 [00:13<00:23,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  35%|\u2588\u2588\u2588\u258d      | 8/23 [00:14<00:19,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  39%|\u2588\u2588\u2588\u2589      | 9/23 [00:15<00:17,  1.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  43%|\u2588\u2588\u2588\u2588\u258e     | 10/23 [00:15<00:14,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  48%|\u2588\u2588\u2588\u2588\u258a     | 11/23 [00:16<00:12,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 12/23 [00:17<00:10,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 13/23 [00:18<00:08,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 14/23 [00:19<00:07,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 15/23 [00:19<00:06,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 16/23 [00:20<00:05,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 17/23 [00:21<00:04,  1.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 18/23 [00:21<00:03,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 19/23 [00:22<00:02,  1.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 20/23 [00:22<00:01,  1.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 21/23 [00:23<00:01,  1.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 22/23 [00:23<00:00,  2.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 23/23 [00:23<00:00,  2.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 23/23 [00:23<00:00,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  10%|\u2588         | 1/10 [00:00<00:03,  2.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  20%|\u2588\u2588        | 2/10 [00:00<00:02,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  30%|\u2588\u2588\u2588       | 3/10 [00:00<00:01,  4.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  40%|\u2588\u2588\u2588\u2588      | 4/10 [00:01<00:01,  4.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:01<00:01,  4.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:01<00:00,  4.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:01<00:00,  5.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:01<00:00,  5.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:01<00:00,  6.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:01<00:00,  5.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded e5 (passage:): tr (2878, 768) te (1162, 768) | 25.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[e5_time] Fold 1 AUC=0.54314 | rounds=22 | spw=1.94 | 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[e5_time] Fold 2 AUC=0.61580 | rounds=35 | spw=2.33 | 0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[e5_time] Fold 3 AUC=0.54114 | rounds=23 | spw=2.49 | 0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[e5_time] Fold 4 AUC=0.63140 | rounds=250 | spw=2.79 | 1.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[e5_time] Fold 5 AUC=0.64953 | rounds=70 | spw=2.83 | 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[e5_time] DONE | OOF(validated) AUC=0.58642 | total 5.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_xgb_e5_time.npy and test_xgb_e5_time.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_minilm_mpnet_time] Fold 1 AUC=0.61832 | rounds=48 | spw=1.94 | 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_minilm_mpnet_time] Fold 2 AUC=0.63685 | rounds=2 | spw=2.33 | 0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_minilm_mpnet_time] Fold 3 AUC=0.55047 | rounds=5 | spw=2.49 | 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_minilm_mpnet_time] Fold 4 AUC=0.59546 | rounds=13 | spw=2.79 | 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_minilm_mpnet_time] Fold 5 AUC=0.59977 | rounds=51 | spw=2.83 | 1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_minilm_mpnet_time] DONE | OOF(validated) AUC=0.59924 | total 4.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_xgb_emb_minilm_mpnet_time.npy and test_xgb_emb_minilm_mpnet_time.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_e5_minilm_mpnet_time] Fold 1 AUC=0.62632 | rounds=240 | spw=1.94 | 2.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_e5_minilm_mpnet_time] Fold 2 AUC=0.61172 | rounds=17 | spw=2.33 | 1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_e5_minilm_mpnet_time] Fold 3 AUC=0.52791 | rounds=0 | spw=2.49 | 1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_e5_minilm_mpnet_time] Fold 4 AUC=0.59220 | rounds=81 | spw=2.79 | 1.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_e5_minilm_mpnet_time] Fold 5 AUC=0.61733 | rounds=15 | spw=2.83 | 1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_e5_minilm_mpnet_time] DONE | OOF(validated) AUC=0.58825 | total 8.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_xgb_emb_e5_minilm_mpnet_time.npy and test_xgb_emb_e5_minilm_mpnet_time.npy\n"
          ]
        }
      ]
    },
    {
      "id": "2983c1e4-3f88-4981-a0bd-d8923057c782",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S51: Reblend with CatBoost (text+meta) added; optimize gamma and block-weighted objectives; promote best\n",
        "import numpy as np, pandas as pd, time\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Time blocks and masks\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "n = len(train)\n",
        "mask_full = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    mask_full[np.array(blocks[i])] = True\n",
        "mask_last2 = np.zeros(n, dtype=bool)\n",
        "for i in [4,5]:\n",
        "    mask_last2[np.array(blocks[i])] = True\n",
        "print(f'Time-CV validated full: {mask_full.sum()}/{n} | last2: {mask_last2.sum()}', flush=True)\n",
        "\n",
        "# Load OOF/test preds for core bases\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy');    t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "o_lr_ns = np.load('oof_lr_time_nosub_meta.npy');     t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy');            t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_time_v2.npy');         t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta_time.npy');           t_meta = np.load('test_xgb_meta_time.npy') if not Path('test_xgb_meta_fullbag.npy').exists() else np.load('test_xgb_meta_fullbag.npy')\n",
        "o_emn = np.load('oof_xgb_emb_meta_time.npy');        t_emn = np.load('test_xgb_emb_meta_time.npy') if not Path('test_xgb_emb_minilm_fullbag.npy').exists() else np.load('test_xgb_emb_minilm_fullbag.npy')\n",
        "o_emp = np.load('oof_xgb_emb_mpnet_time.npy');       t_emp = np.load('test_xgb_emb_mpnet_time.npy') if not Path('test_xgb_emb_mpnet_fullbag.npy').exists() else np.load('test_xgb_emb_mpnet_fullbag.npy')\n",
        "# Dual-view SVD XGB base (optional)\n",
        "has_svd_dual = Path('oof_xgb_svd_word192_char128_meta.npy').exists() and Path('test_xgb_svd_word192_char128_meta.npy').exists()\n",
        "if has_svd_dual:\n",
        "    o_svd_dual = np.load('oof_xgb_svd_word192_char128_meta.npy'); t_svd_dual = np.load('test_xgb_svd_word192_char128_meta.npy')\n",
        "# CatBoost text+meta\n",
        "has_cat = Path('oof_catboost_textmeta.npy').exists() and Path('test_catboost_textmeta.npy').exists()\n",
        "if has_cat:\n",
        "    o_cat = np.load('oof_catboost_textmeta.npy'); t_cat = np.load('test_catboost_textmeta.npy')\n",
        "else:\n",
        "    raise FileNotFoundError('CatBoost OOF/test not found; run S49 first.')\n",
        "\n",
        "# Convert OOF to logits\n",
        "z_lr_w, z_lr_ns = to_logit(o_lr_w), to_logit(o_lr_ns)\n",
        "z_d1, z_d2, z_meta = to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)\n",
        "z_emn, z_emp = to_logit(o_emn), to_logit(o_emp)\n",
        "z_svd_dual = to_logit(o_svd_dual) if has_svd_dual else None\n",
        "z_cat = to_logit(o_cat)\n",
        "\n",
        "# Convert test to logits\n",
        "tz_lr_w, tz_lr_ns = to_logit(t_lr_w), to_logit(t_lr_ns)\n",
        "tz_d1, tz_d2, tz_meta = to_logit(t_meta), to_logit(t_d2), to_logit(t_meta)\n",
        "tz_emn, tz_emp = to_logit(t_emn), to_logit(t_emp)\n",
        "tz_svd_dual = to_logit(t_svd_dual) if has_svd_dual else None\n",
        "tz_cat = to_logit(t_cat)\n",
        "\n",
        "# Grids (per expert guidance):\n",
        "g_grid = [0.975, 0.98, 0.99]\n",
        "meta_grid = [0.18, 0.20, 0.22]\n",
        "dense_tot_grid = [0.0, 0.06, 0.12, 0.18]  # allow Dense to drop to 0\n",
        "dense_split = [(0.6, 0.4), (0.7, 0.3)]\n",
        "emb_tot_grid = [0.24, 0.30, 0.34]         # raise embedding cap\n",
        "emb_split = [(0.6, 0.4), (0.5, 0.5)]\n",
        "svd_dual_grid = [0.0, 0.05, 0.08, 0.10] if has_svd_dual else [0.0]\n",
        "cat_grid = [0.06, 0.10, 0.14, 0.18, 0.20]\n",
        "w_lr_min_grid = [0.22, 0.25]\n",
        "\n",
        "def search(mask, sample_weight=None):\n",
        "    best_auc, best_cfg, tried = -1.0, None, 0\n",
        "    t0 = time.time()\n",
        "    for g in g_grid:\n",
        "        z_lr_mix = (1.0 - g)*z_lr_w + g*z_lr_ns\n",
        "        for w_lr_min in w_lr_min_grid:\n",
        "            for w_meta in meta_grid:\n",
        "                for d_tot in dense_tot_grid:\n",
        "                    for dv1, dv2 in dense_split:\n",
        "                        w_d1 = d_tot * dv1; w_d2 = d_tot * dv2\n",
        "                        for e_tot in emb_tot_grid:\n",
        "                            for emn_fr, emp_fr in emb_split:\n",
        "                                w_emn = e_tot * emn_fr; w_emp = e_tot * emp_fr\n",
        "                                for w_svd in svd_dual_grid:\n",
        "                                    for w_cat in cat_grid:\n",
        "                                        rem = 1.0 - (w_meta + w_d1 + w_d2 + w_emn + w_emp + w_svd + w_cat)\n",
        "                                        if rem <= 0:\n",
        "                                            continue\n",
        "                                        w_lr = rem\n",
        "                                        if w_lr < w_lr_min:\n",
        "                                            continue\n",
        "                                        z_oof = (w_lr*z_lr_mix +\n",
        "                                                 w_d1*z_d1 + w_d2*z_d2 +\n",
        "                                                 w_meta*z_meta +\n",
        "                                                 w_emn*z_emn + w_emp*z_emp +\n",
        "                                                 (w_svd*z_svd_dual if (has_svd_dual and w_svd>0) else 0) +\n",
        "                                                 w_cat*z_cat)\n",
        "                                        auc = roc_auc_score(y[mask], z_oof[mask], sample_weight=(sample_weight[mask] if sample_weight is not None else None))\n",
        "                                        tried += 1\n",
        "                                        if tried % 2000 == 0:\n",
        "                                            print(f'  tried={tried} | best={best_auc:.5f} | elapsed={time.time()-t0:.1f}s', flush=True)\n",
        "                                        if auc > best_auc:\n",
        "                                            best_auc = auc\n",
        "                                            best_cfg = dict(g=float(g), w_lr=float(w_lr), w_d1=float(w_d1), w_d2=float(w_d2),\n",
        "                                                            w_meta=float(w_meta), w_emn=float(w_emn), w_emp=float(w_emp),\n",
        "                                                            w_svd=float(w_svd), w_cat=float(w_cat))\n",
        "    print(f'  search done | tried={tried} | best={best_auc:.5f} | {time.time()-t0:.1f}s', flush=True)\n",
        "    return best_auc, best_cfg, tried\n",
        "\n",
        "# 1) Full-mask objective\n",
        "auc_full, cfg_full, tried_full = search(mask_full)\n",
        "print(f'[Full] tried={tried_full} | best OOF(z) AUC={auc_full:.5f} | cfg={cfg_full}', flush=True)\n",
        "\n",
        "# 2) Last-2 objective\n",
        "auc_last2, cfg_last2, tried_last2 = search(mask_last2)\n",
        "print(f'[Last2] tried={tried_last2} | best OOF(z,last2) AUC={auc_last2:.5f} | cfg={cfg_last2}', flush=True)\n",
        "\n",
        "# 3) Gamma-decayed block weights (optimize on validated blocks with per-block gamma)\n",
        "best_gamma, best_auc_g, best_cfg_g = None, -1.0, None\n",
        "for gamma in [0.975, 0.98, 0.99]:\n",
        "    w = np.zeros(n, dtype=np.float64)\n",
        "    for bi in range(1, k):\n",
        "        age = (k - 1) - bi\n",
        "        w[np.array(blocks[bi])] = (gamma ** age)\n",
        "    auc_g, cfg_g, _ = search(mask_full, sample_weight=w)\n",
        "    print(f'[Gamma {gamma}] best OOF(z,weighted) AUC={auc_g:.5f}', flush=True)\n",
        "    if auc_g > best_auc_g:\n",
        "        best_auc_g, best_cfg_g, best_gamma = auc_g, cfg_g, gamma\n",
        "print(f'[Gamma-best] gamma={best_gamma} | AUC={best_auc_g:.5f} | cfg={best_cfg_g}', flush=True)\n",
        "\n",
        "# 4) Block-weighted objective (explicit per-block weights [0.1,0.2,0.3,0.4,0.5])\n",
        "bw = np.zeros(n, dtype=np.float64)\n",
        "weights = [0.1, 0.2, 0.3, 0.4, 0.5]  # for blocks 1..5\n",
        "for bi in range(1, k):\n",
        "    bw[np.array(blocks[bi])] = weights[bi-1]\n",
        "auc_bw, cfg_bw, _ = search(mask_full, sample_weight=bw)\n",
        "print(f'[Block-weighted] best OOF(z,weighted) AUC={auc_bw:.5f} | cfg={cfg_bw}', flush=True)\n",
        "\n",
        "def build_and_save(tag, cfg):\n",
        "    g = cfg['g']\n",
        "    tz_lr_mix = (1.0 - g)*tz_lr_w + g*tz_lr_ns\n",
        "    parts = [\n",
        "        cfg['w_lr']*tz_lr_mix,\n",
        "        cfg['w_d1']*to_logit(t_d1),\n",
        "        cfg['w_d2']*to_logit(t_d2),\n",
        "        cfg['w_meta']*to_logit(t_meta),\n",
        "        cfg['w_emn']*to_logit(t_emn),\n",
        "        cfg['w_emp']*to_logit(t_emp),\n",
        "        cfg['w_cat']*tz_cat\n",
        "    ]\n",
        "    if has_svd_dual and cfg['w_svd'] > 0:\n",
        "        parts.append(cfg['w_svd']*to_logit(t_svd_dual))\n",
        "    zt = np.sum(parts, axis=0)\n",
        "    pt = sigmoid(zt).astype(np.float32)\n",
        "    out_path = f'submission_reblend_cat_{tag}.csv'\n",
        "    pd.DataFrame({id_col: ids, target_col: pt}).to_csv(out_path, index=False)\n",
        "    # 15% shrink-to-equal hedge\n",
        "    w_list = [cfg['w_lr'], cfg['w_d1'], cfg['w_d2'], cfg['w_meta'], cfg['w_emn'], cfg['w_emp'], cfg['w_cat']] + ([cfg['w_svd']] if (has_svd_dual and cfg['w_svd']>0) else [])\n",
        "    comp_logits = [tz_lr_mix, to_logit(t_d1), to_logit(t_d2), to_logit(t_meta), to_logit(t_emn), to_logit(t_emp), tz_cat] + ([to_logit(t_svd_dual)] if (has_svd_dual and cfg['w_svd']>0) else [])\n",
        "    w_vec = np.array(w_list, dtype=np.float64)\n",
        "    w_eq = np.ones_like(w_vec)/len(w_vec)\n",
        "    alpha = 0.15\n",
        "    w_shr = ((1.0 - alpha)*w_vec + alpha*w_eq); w_shr = (w_shr / w_shr.sum()).astype(np.float64)\n",
        "    zt_shr = 0.0\n",
        "    for wi, zi in zip(w_shr, comp_logits):\n",
        "        zt_shr += wi*zi\n",
        "    pt_shr = sigmoid(zt_shr).astype(np.float32)\n",
        "    pd.DataFrame({id_col: ids, target_col: pt_shr}).to_csv(out_path.replace('.csv','_shrunk.csv'), index=False)\n",
        "    return out_path\n",
        "\n",
        "p_full = build_and_save('full', cfg_full)\n",
        "p_last2 = build_and_save('last2', cfg_last2)\n",
        "p_gam = build_and_save(f'gamma{best_gamma:.3f}'.replace('.', 'p'), best_cfg_g)\n",
        "p_bw = build_and_save('blockw', cfg_bw)\n",
        "\n",
        "# Promote best among gamma-best and block-weighted; prefer block-weighted if AUC higher, else gamma-best\n",
        "primary = p_bw if (auc_bw >= best_auc_g) else p_gam\n",
        "pd.read_csv(primary).to_csv('submission.csv', index=False)\n",
        "print(f'Promoted {Path(primary).name} to submission.csv', flush=True)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV validated full: 2398/2878 | last2: 958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=2000 | best=0.68230 | elapsed=4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=4000 | best=0.68233 | elapsed=8.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=6000 | best=0.68233 | elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  search done | tried=7704 | best=0.68236 | 16.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Full] tried=7704 | best OOF(z) AUC=0.68236 | cfg={'g': 0.99, 'w_lr': 0.25, 'w_d1': 0.08399999999999999, 'w_d2': 0.036, 'w_meta': 0.22, 'w_emn': 0.15, 'w_emp': 0.15, 'w_svd': 0.05, 'w_cat': 0.06}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=2000 | best=0.64807 | elapsed=3.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=4000 | best=0.64807 | elapsed=7.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=6000 | best=0.64808 | elapsed=10.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  search done | tried=7704 | best=0.64808 | 13.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Last2] tried=7704 | best OOF(z,last2) AUC=0.64808 | cfg={'g': 0.99, 'w_lr': 0.24, 'w_d1': 0.126, 'w_d2': 0.054, 'w_meta': 0.18, 'w_emn': 0.20400000000000001, 'w_emp': 0.136, 'w_svd': 0.0, 'w_cat': 0.06}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=2000 | best=0.68079 | elapsed=4.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=4000 | best=0.68082 | elapsed=9.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=6000 | best=0.68082 | elapsed=14.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  search done | tried=7704 | best=0.68085 | 18.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gamma 0.975] best OOF(z,weighted) AUC=0.68085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=2000 | best=0.68110 | elapsed=4.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=4000 | best=0.68113 | elapsed=9.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=6000 | best=0.68113 | elapsed=14.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  search done | tried=7704 | best=0.68115 | 18.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gamma 0.98] best OOF(z,weighted) AUC=0.68115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=2000 | best=0.68170 | elapsed=4.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=4000 | best=0.68173 | elapsed=9.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=6000 | best=0.68173 | elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  search done | tried=7704 | best=0.68176 | 17.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gamma 0.99] best OOF(z,weighted) AUC=0.68176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gamma-best] gamma=0.99 | AUC=0.68176 | cfg={'g': 0.99, 'w_lr': 0.25, 'w_d1': 0.08399999999999999, 'w_d2': 0.036, 'w_meta': 0.22, 'w_emn': 0.15, 'w_emp': 0.15, 'w_svd': 0.05, 'w_cat': 0.06}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=2000 | best=0.66215 | elapsed=4.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=4000 | best=0.66218 | elapsed=9.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=6000 | best=0.66220 | elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  search done | tried=7704 | best=0.66222 | 17.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Block-weighted] best OOF(z,weighted) AUC=0.66222 | cfg={'g': 0.99, 'w_lr': 0.22999999999999998, 'w_d1': 0.08399999999999999, 'w_d2': 0.036, 'w_meta': 0.2, 'w_emn': 0.17, 'w_emp': 0.17, 'w_svd': 0.05, 'w_cat': 0.06}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promoted submission_reblend_cat_gamma0p990.csv to submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "81f4682d-beb7-4c19-9746-e49cbb5874af",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S52: LinearSVC (word 1-2 + char_wb 3-6 TF-IDF) with isotonic calibration; time-aware CV; cache OOF/test\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body(df):\n",
        "    # Avoid edit_aware; use request_text only\n",
        "    return (df['request_text'] if 'request_text' in df.columns else df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "def build_text(df):\n",
        "    return (get_title(df) + '\\n' + get_body(df)).astype(str)\n",
        "\n",
        "txt_tr = build_text(train)\n",
        "txt_te = build_text(test)\n",
        "\n",
        "# Time-aware 6-block forward-chaining (validate blocks 1..5)\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}', flush=True)\n",
        "\n",
        "# TF-IDF views\n",
        "word_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=2, max_features=300_000, sublinear_tf=True, smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "char_params = dict(analyzer='char_wb', ngram_range=(3,6), lowercase=True, min_df=2, max_features=250_000, sublinear_tf=True, smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "\n",
        "C_grid = [0.5, 1.0, 2.0]\n",
        "best = dict(auc=-1.0, C=None, oof=None, te=None)\n",
        "results = []\n",
        "\n",
        "for C in C_grid:\n",
        "    oof = np.zeros(n, dtype=np.float32)\n",
        "    te_accum = []\n",
        "    tC = time.time()\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "        t0 = time.time()\n",
        "        tr_text = txt_tr.iloc[tr_idx]; va_text = txt_tr.iloc[va_idx]\n",
        "        # Fit vectorizers on train fold only\n",
        "        tf_w = TfidfVectorizer(**word_params)\n",
        "        Xw_tr = tf_w.fit_transform(tr_text); Xw_va = tf_w.transform(va_text); Xw_te = tf_w.transform(txt_te)\n",
        "        tf_c = TfidfVectorizer(**char_params)\n",
        "        Xc_tr = tf_c.fit_transform(tr_text); Xc_va = tf_c.transform(va_text); Xc_te = tf_c.transform(txt_te)\n",
        "        X_tr = hstack([Xw_tr, Xc_tr], format='csr')\n",
        "        X_va = hstack([Xw_va, Xc_va], format='csr')\n",
        "        X_te = hstack([Xw_te, Xc_te], format='csr')\n",
        "        # Base SVM\n",
        "        base = LinearSVC(C=C, dual=False, max_iter=5000)\n",
        "        # Calibrate on train fold via 3-fold CV (inside tr_idx), isotonic\n",
        "        clf = CalibratedClassifierCV(estimator=base, method='isotonic', cv=3)\n",
        "        clf.fit(X_tr, y[tr_idx])\n",
        "        va_pred = clf.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "        te_pred = clf.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "        oof[va_idx] = va_pred; te_accum.append(te_pred)\n",
        "        auc = roc_auc_score(y[va_idx], va_pred) if (y[va_idx].min()!=y[va_idx].max()) else 0.5\n",
        "        print(f'[LinSVM+Cali C={C}] Fold {fi} AUC={auc:.5f} | feats={X_tr.shape[1]} | {time.time()-t0:.1f}s', flush=True)\n",
        "        del tf_w, tf_c, Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, X_tr, X_va, X_te, base, clf; gc.collect()\n",
        "    auc_oof = roc_auc_score(y[mask], oof[mask])\n",
        "    te_mean = np.mean(te_accum, axis=0).astype(np.float32)\n",
        "    results.append((C, auc_oof))\n",
        "    print(f'[LinSVM+Cali C={C}] OOF(validated) AUC={auc_oof:.5f} | total {time.time()-tC:.1f}s', flush=True)\n",
        "    if auc_oof > best['auc']:\n",
        "        best.update(dict(auc=auc_oof, C=C, oof=oof.copy(), te=te_mean.copy()))\n",
        "    del oof, te_accum; gc.collect()\n",
        "\n",
        "print('C grid results:', results)\n",
        "print(f'Best C={best[\"C\"]} | OOF(validated) AUC={best[\"auc\"]:.5f}')\n",
        "np.save('oof_svm_wordchar_time.npy', best['oof'].astype(np.float32))\n",
        "np.save('test_svm_wordchar_time.npy', best['te'].astype(np.float32))\n",
        "print('Saved oof_svm_wordchar_time.npy and test_svm_wordchar_time.npy', flush=True)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV: 5 folds; validated 2398/2878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LinSVM+Cali C=0.5] Fold 1 AUC=0.66099 | feats=32995 | 1.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LinSVM+Cali C=0.5] Fold 2 AUC=0.59132 | feats=52069 | 2.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LinSVM+Cali C=0.5] Fold 3 AUC=0.56935 | feats=66699 | 2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LinSVM+Cali C=0.5] Fold 4 AUC=0.62774 | feats=77440 | 3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LinSVM+Cali C=0.5] Fold 5 AUC=0.62111 | feats=86989 | 3.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LinSVM+Cali C=0.5] OOF(validated) AUC=0.60804 | total 14.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LinSVM+Cali C=1.0] Fold 1 AUC=0.65118 | feats=32995 | 1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LinSVM+Cali C=1.0] Fold 2 AUC=0.57915 | feats=52069 | 1.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LinSVM+Cali C=1.0] Fold 3 AUC=0.56104 | feats=66699 | 2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LinSVM+Cali C=1.0] Fold 4 AUC=0.61897 | feats=77440 | 3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LinSVM+Cali C=1.0] Fold 5 AUC=0.62645 | feats=86989 | 4.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LinSVM+Cali C=1.0] OOF(validated) AUC=0.60155 | total 14.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LinSVM+Cali C=2.0] Fold 1 AUC=0.64495 | feats=32995 | 1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LinSVM+Cali C=2.0] Fold 2 AUC=0.57536 | feats=52069 | 2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LinSVM+Cali C=2.0] Fold 3 AUC=0.56365 | feats=66699 | 2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LinSVM+Cali C=2.0] Fold 4 AUC=0.61264 | feats=77440 | 3.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LinSVM+Cali C=2.0] Fold 5 AUC=0.62111 | feats=86989 | 4.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LinSVM+Cali C=2.0] OOF(validated) AUC=0.59804 | total 15.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C grid results: [(0.5, 0.6080356862468008), (1.0, 0.6015518276885392), (2.0, 0.5980380611427765)]\nBest C=0.5 | OOF(validated) AUC=0.60804\nSaved oof_svm_wordchar_time.npy and test_svm_wordchar_time.npy\n"
          ]
        }
      ]
    },
    {
      "id": "4ba7c173-3b43-4db2-8cc9-b4bfb470af0f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S53: CatBoost text+meta (GPU) small hyperparam sweep; pick best OOF and overwrite oof_catboost_textmeta.npy/test_*.npy\n",
        "import os, sys, time, gc, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "try:\n",
        "    from catboost import CatBoostClassifier, Pool\n",
        "except Exception:\n",
        "    import subprocess\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'catboost'])\n",
        "    from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body(df):\n",
        "    return (df['request_text'] if 'request_text' in df.columns else df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "\n",
        "# Build text columns\n",
        "title_tr = get_title(train); body_tr = get_body(train)\n",
        "title_te = get_title(test);  body_te = get_body(test)\n",
        "\n",
        "# Load meta_v1 features (numeric)\n",
        "Meta_tr = np.load('meta_v1_tr.npy').astype(np.float32)\n",
        "Meta_te = np.load('meta_v1_te.npy').astype(np.float32)\n",
        "n_meta = Meta_tr.shape[1]\n",
        "meta_cols = [f'm{i}' for i in range(n_meta)]\n",
        "Xtr_df = pd.DataFrame({'title': title_tr, 'body': body_tr})\n",
        "for i, col in enumerate(meta_cols):\n",
        "    Xtr_df[col] = Meta_tr[:, i]\n",
        "Xte_df = pd.DataFrame({'title': title_te, 'body': body_te})\n",
        "for i, col in enumerate(meta_cols):\n",
        "    Xte_df[col] = Meta_te[:, i]\n",
        "\n",
        "# Time-aware 6-block forward-chaining (validate 1..5)\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}', flush=True)\n",
        "\n",
        "text_features = [0, 1]\n",
        "\n",
        "grid = []\n",
        "for depth in [6, 8]:\n",
        "    for lr in [0.03, 0.05]:\n",
        "        for l2 in [3.0, 6.0, 9.0]:\n",
        "            for bt in [0.5, 1.0]:\n",
        "                grid.append(dict(depth=depth, learning_rate=lr, l2_leaf_reg=l2, bagging_temperature=bt))\n",
        "print(f'Grid size: {len(grid)}', flush=True)\n",
        "\n",
        "best_auc, best_cfg, best_oof, best_te = -1.0, None, None, None\n",
        "t_all = time.time()\n",
        "for gi, cfg in enumerate(grid, 1):\n",
        "    params = dict(\n",
        "        iterations=4000,\n",
        "        depth=cfg['depth'],\n",
        "        learning_rate=cfg['learning_rate'],\n",
        "        l2_leaf_reg=cfg['l2_leaf_reg'],\n",
        "        bagging_temperature=cfg['bagging_temperature'],\n",
        "        random_strength=1.0,\n",
        "        loss_function='Logloss',\n",
        "        eval_metric='AUC',\n",
        "        task_type='GPU',\n",
        "        verbose=False\n",
        "    )\n",
        "    oof = np.zeros(n, dtype=np.float32); te_parts = []\n",
        "    t0 = time.time()\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "        X_tr_fold = Xtr_df.iloc[tr_idx].reset_index(drop=True)\n",
        "        X_va_fold = Xtr_df.iloc[va_idx].reset_index(drop=True)\n",
        "        y_tr = y[tr_idx]; y_va = y[va_idx]\n",
        "        pos = float((y_tr == 1).sum()); neg = float((y_tr == 0).sum())\n",
        "        spw = (neg / max(pos, 1.0)) if pos > 0 else 1.0\n",
        "        p = dict(params); p['scale_pos_weight'] = spw; p['random_seed'] = 4242 + gi*10 + fi\n",
        "        pool_tr = Pool(X_tr_fold, label=y_tr, text_features=text_features)\n",
        "        pool_va = Pool(X_va_fold, label=y_va, text_features=text_features)\n",
        "        pool_te = Pool(Xte_df, text_features=text_features)\n",
        "        model = CatBoostClassifier(**p)\n",
        "        model.fit(pool_tr, eval_set=pool_va, use_best_model=True, early_stopping_rounds=100)\n",
        "        va_pred = model.predict_proba(pool_va)[:,1].astype(np.float32)\n",
        "        te_pred = model.predict_proba(pool_te)[:,1].astype(np.float32)\n",
        "        oof[va_idx] = va_pred; te_parts.append(te_pred)\n",
        "        del pool_tr, pool_va, pool_te, model; gc.collect()\n",
        "    auc_oof = roc_auc_score(y[mask], oof[mask])\n",
        "    te_mean = np.mean(te_parts, axis=0).astype(np.float32)\n",
        "    print(f'[CatSweep {gi}/{len(grid)}] cfg={cfg} | OOF AUC={auc_oof:.5f} | {time.time()-t0:.1f}s', flush=True)\n",
        "    if auc_oof > best_auc:\n",
        "        best_auc, best_cfg, best_oof, best_te = auc_oof, cfg, oof.copy(), te_mean.copy()\n",
        "    del oof, te_parts; gc.collect()\n",
        "\n",
        "print(f'[CatSweep] BEST OOF={best_auc:.5f} | cfg={best_cfg} | total {time.time()-t_all:.1f}s', flush=True)\n",
        "np.save('oof_catboost_textmeta.npy', best_oof.astype(np.float32))\n",
        "np.save('test_catboost_textmeta.npy', best_te.astype(np.float32))\n",
        "print('Overwrote oof_catboost_textmeta.npy and test_catboost_textmeta.npy with best sweep results', flush=True)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV: 5 folds; validated 2398/2878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid size: 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 1/24] cfg={'depth': 6, 'learning_rate': 0.03, 'l2_leaf_reg': 3.0, 'bagging_temperature': 0.5} | OOF AUC=0.63802 | 23.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 2/24] cfg={'depth': 6, 'learning_rate': 0.03, 'l2_leaf_reg': 3.0, 'bagging_temperature': 1.0} | OOF AUC=0.63555 | 21.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 3/24] cfg={'depth': 6, 'learning_rate': 0.03, 'l2_leaf_reg': 6.0, 'bagging_temperature': 0.5} | OOF AUC=0.63921 | 27.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 4/24] cfg={'depth': 6, 'learning_rate': 0.03, 'l2_leaf_reg': 6.0, 'bagging_temperature': 1.0} | OOF AUC=0.64568 | 26.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 5/24] cfg={'depth': 6, 'learning_rate': 0.03, 'l2_leaf_reg': 9.0, 'bagging_temperature': 0.5} | OOF AUC=0.64461 | 27.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 6/24] cfg={'depth': 6, 'learning_rate': 0.03, 'l2_leaf_reg': 9.0, 'bagging_temperature': 1.0} | OOF AUC=0.65356 | 36.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 7/24] cfg={'depth': 6, 'learning_rate': 0.05, 'l2_leaf_reg': 3.0, 'bagging_temperature': 0.5} | OOF AUC=0.63770 | 22.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 8/24] cfg={'depth': 6, 'learning_rate': 0.05, 'l2_leaf_reg': 3.0, 'bagging_temperature': 1.0} | OOF AUC=0.64195 | 31.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 9/24] cfg={'depth': 6, 'learning_rate': 0.05, 'l2_leaf_reg': 6.0, 'bagging_temperature': 0.5} | OOF AUC=0.64191 | 24.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 10/24] cfg={'depth': 6, 'learning_rate': 0.05, 'l2_leaf_reg': 6.0, 'bagging_temperature': 1.0} | OOF AUC=0.63665 | 22.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 11/24] cfg={'depth': 6, 'learning_rate': 0.05, 'l2_leaf_reg': 9.0, 'bagging_temperature': 0.5} | OOF AUC=0.62863 | 26.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 12/24] cfg={'depth': 6, 'learning_rate': 0.05, 'l2_leaf_reg': 9.0, 'bagging_temperature': 1.0} | OOF AUC=0.63756 | 35.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 13/24] cfg={'depth': 8, 'learning_rate': 0.03, 'l2_leaf_reg': 3.0, 'bagging_temperature': 0.5} | OOF AUC=0.64530 | 76.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 14/24] cfg={'depth': 8, 'learning_rate': 0.03, 'l2_leaf_reg': 3.0, 'bagging_temperature': 1.0} | OOF AUC=0.63980 | 115.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 15/24] cfg={'depth': 8, 'learning_rate': 0.03, 'l2_leaf_reg': 6.0, 'bagging_temperature': 0.5} | OOF AUC=0.63825 | 75.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 16/24] cfg={'depth': 8, 'learning_rate': 0.03, 'l2_leaf_reg': 6.0, 'bagging_temperature': 1.0} | OOF AUC=0.63747 | 98.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 17/24] cfg={'depth': 8, 'learning_rate': 0.03, 'l2_leaf_reg': 9.0, 'bagging_temperature': 0.5} | OOF AUC=0.61591 | 53.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 18/24] cfg={'depth': 8, 'learning_rate': 0.03, 'l2_leaf_reg': 9.0, 'bagging_temperature': 1.0} | OOF AUC=0.62428 | 65.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 19/24] cfg={'depth': 8, 'learning_rate': 0.05, 'l2_leaf_reg': 3.0, 'bagging_temperature': 0.5} | OOF AUC=0.62876 | 53.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 20/24] cfg={'depth': 8, 'learning_rate': 0.05, 'l2_leaf_reg': 3.0, 'bagging_temperature': 1.0} | OOF AUC=0.61497 | 71.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 21/24] cfg={'depth': 8, 'learning_rate': 0.05, 'l2_leaf_reg': 6.0, 'bagging_temperature': 0.5} | OOF AUC=0.63728 | 61.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 22/24] cfg={'depth': 8, 'learning_rate': 0.05, 'l2_leaf_reg': 6.0, 'bagging_temperature': 1.0} | OOF AUC=0.64554 | 56.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 23/24] cfg={'depth': 8, 'learning_rate': 0.05, 'l2_leaf_reg': 9.0, 'bagging_temperature': 0.5} | OOF AUC=0.64561 | 76.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep 24/24] cfg={'depth': 8, 'learning_rate': 0.05, 'l2_leaf_reg': 9.0, 'bagging_temperature': 1.0} | OOF AUC=0.63103 | 61.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatSweep] BEST OOF=0.65356 | cfg={'depth': 6, 'learning_rate': 0.03, 'l2_leaf_reg': 9.0, 'bagging_temperature': 1.0} | total 1195.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwrote oof_catboost_textmeta.npy and test_catboost_textmeta.npy with best sweep results\n"
          ]
        }
      ]
    },
    {
      "id": "47a4f26b-16e7-41c6-bd62-79a66cb301e5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S54: Embedding heads + meta_v1 (per-fold StandardScaler) with XGB (GPU); cache OOF/test\n",
        "import os, time, gc, numpy as np, pandas as pd, xgboost as xgb\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "# Time-aware 6-block forward-chaining (validate blocks 1..5)\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}', flush=True)\n",
        "\n",
        "# Load embeddings and meta_v1\n",
        "E5_tr = np.load('emb_e5_tr.npy').astype(np.float32)\n",
        "E5_te = np.load('emb_e5_te.npy').astype(np.float32)\n",
        "Emb_min_tr = np.load('emb_minilm_tr.npy').astype(np.float32)\n",
        "Emb_min_te = np.load('emb_minilm_te.npy').astype(np.float32)\n",
        "Emb_mp_tr = np.load('emb_mpnet_tr.npy').astype(np.float32)\n",
        "Emb_mp_te = np.load('emb_mpnet_te.npy').astype(np.float32)\n",
        "Meta_tr = np.load('meta_v1_tr.npy').astype(np.float32)\n",
        "Meta_te = np.load('meta_v1_te.npy').astype(np.float32)\n",
        "\n",
        "def run_xgb_head_with_meta(Xtr_emb: np.ndarray, Xte_emb: np.ndarray, tag: str):\n",
        "    oof = np.zeros(n, dtype=np.float32)\n",
        "    te_parts = []\n",
        "    params = dict(\n",
        "        objective='binary:logistic',\n",
        "        eval_metric='auc',\n",
        "        max_depth=4,\n",
        "        eta=0.05,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.6,\n",
        "        min_child_weight=8,\n",
        "        reg_alpha=0.3,\n",
        "        reg_lambda=3.0,\n",
        "        gamma=0.0,\n",
        "        device='cuda',\n",
        "        tree_method='hist'\n",
        "    )\n",
        "    t_all = time.time()\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "        t0 = time.time()\n",
        "        X_tr_f = np.hstack([Xtr_emb[tr_idx], Meta_tr[tr_idx]]).astype(np.float32)\n",
        "        X_va_f = np.hstack([Xtr_emb[va_idx], Meta_tr[va_idx]]).astype(np.float32)\n",
        "        X_te_f = np.hstack([Xte_emb, Meta_te]).astype(np.float32)\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "        X_tr = scaler.fit_transform(X_tr_f).astype(np.float32)\n",
        "        X_va = scaler.transform(X_va_f).astype(np.float32)\n",
        "        X_te = scaler.transform(X_te_f).astype(np.float32)\n",
        "        dtr = xgb.DMatrix(X_tr, label=y[tr_idx]); dva = xgb.DMatrix(X_va, label=y[va_idx]); dte = xgb.DMatrix(X_te)\n",
        "        pos = float((y[tr_idx] == 1).sum()); neg = float((y[tr_idx] == 0).sum()); spw = (neg / max(pos, 1.0)) if pos > 0 else 1.0\n",
        "        p = dict(params); p['seed'] = 4242 + fi; p['scale_pos_weight'] = spw\n",
        "        booster = xgb.train(p, dtr, num_boost_round=4000, evals=[(dva, 'valid')], early_stopping_rounds=100, verbose_eval=False)\n",
        "        va_pred = booster.predict(dva).astype(np.float32)\n",
        "        te_pred = booster.predict(dte, iteration_range=(0, booster.best_iteration+1 if booster.best_iteration is not None else 0)).astype(np.float32)\n",
        "        oof[va_idx] = va_pred; te_parts.append(te_pred)\n",
        "        auc = roc_auc_score(y[va_idx], va_pred) if (y[va_idx].min()!=y[va_idx].max()) else 0.5\n",
        "        print(f'[{tag}] Fold {fi} AUC={auc:.5f} | rounds={booster.best_iteration} | spw={spw:.2f} | {time.time()-t0:.1f}s', flush=True)\n",
        "        del X_tr_f, X_va_f, X_te_f, X_tr, X_va, X_te, scaler, dtr, dva, dte, booster; gc.collect()\n",
        "    auc_oof = roc_auc_score(y[mask], oof[mask])\n",
        "    te_mean = np.mean(te_parts, axis=0).astype(np.float32)\n",
        "    print(f'[{tag}] DONE | OOF(validated) AUC={auc_oof:.5f} | total {time.time()-t_all:.1f}s', flush=True)\n",
        "    np.save(f'oof_xgb_{tag}.npy', oof.astype(np.float32))\n",
        "    np.save(f'test_xgb_{tag}.npy', te_mean.astype(np.float32))\n",
        "    print(f'Saved oof_xgb_{tag}.npy and test_xgb_{tag}.npy', flush=True)\n",
        "\n",
        "# a) e5 + meta\n",
        "run_xgb_head_with_meta(E5_tr, E5_te, tag='e5_meta_time')\n",
        "\n",
        "# b) MiniLM||MPNet + meta\n",
        "Emb_mm_tr = np.hstack([Emb_min_tr, Emb_mp_tr]).astype(np.float32)\n",
        "Emb_mm_te = np.hstack([Emb_min_te, Emb_mp_te]).astype(np.float32)\n",
        "run_xgb_head_with_meta(Emb_mm_tr, Emb_mm_te, tag='emb_minilm_mpnet_meta_time')\n",
        "\n",
        "# c) e5||MiniLM||MPNet + meta\n",
        "Emb_all_tr = np.hstack([E5_tr, Emb_min_tr, Emb_mp_tr]).astype(np.float32)\n",
        "Emb_all_te = np.hstack([E5_te, Emb_min_te, Emb_mp_te]).astype(np.float32)\n",
        "run_xgb_head_with_meta(Emb_all_tr, Emb_all_te, tag='emb_e5_minilm_mpnet_meta_time')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV: 5 folds; validated 2398/2878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[e5_meta_time] Fold 1 AUC=0.59570 | rounds=210 | spw=1.94 | 1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[e5_meta_time] Fold 2 AUC=0.66896 | rounds=10 | spw=2.33 | 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[e5_meta_time] Fold 3 AUC=0.61099 | rounds=170 | spw=2.49 | 1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[e5_meta_time] Fold 4 AUC=0.61646 | rounds=271 | spw=2.79 | 1.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[e5_meta_time] Fold 5 AUC=0.62050 | rounds=159 | spw=2.83 | 1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[e5_meta_time] DONE | OOF(validated) AUC=0.61579 | total 7.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_xgb_e5_meta_time.npy and test_xgb_e5_meta_time.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_minilm_mpnet_meta_time] Fold 1 AUC=0.65535 | rounds=84 | spw=1.94 | 1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_minilm_mpnet_meta_time] Fold 2 AUC=0.68180 | rounds=12 | spw=2.33 | 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_minilm_mpnet_meta_time] Fold 3 AUC=0.56828 | rounds=59 | spw=2.49 | 1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_minilm_mpnet_meta_time] Fold 4 AUC=0.61658 | rounds=81 | spw=2.79 | 1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_minilm_mpnet_meta_time] Fold 5 AUC=0.60448 | rounds=14 | spw=2.83 | 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_minilm_mpnet_meta_time] DONE | OOF(validated) AUC=0.62118 | total 5.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_xgb_emb_minilm_mpnet_meta_time.npy and test_xgb_emb_minilm_mpnet_meta_time.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_e5_minilm_mpnet_meta_time] Fold 1 AUC=0.65285 | rounds=520 | spw=1.94 | 3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_e5_minilm_mpnet_meta_time] Fold 2 AUC=0.68805 | rounds=58 | spw=2.33 | 1.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_e5_minilm_mpnet_meta_time] Fold 3 AUC=0.54808 | rounds=4 | spw=2.49 | 1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_e5_minilm_mpnet_meta_time] Fold 4 AUC=0.59463 | rounds=27 | spw=2.79 | 1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_e5_minilm_mpnet_meta_time] Fold 5 AUC=0.61489 | rounds=144 | spw=2.83 | 2.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[emb_e5_minilm_mpnet_meta_time] DONE | OOF(validated) AUC=0.61816 | total 10.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_xgb_emb_e5_minilm_mpnet_meta_time.npy and test_xgb_emb_e5_minilm_mpnet_meta_time.npy\n"
          ]
        }
      ]
    },
    {
      "id": "47a573c1-01ec-437b-9f9f-d14354a5d389",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S55: CatBoost v2 - single text + hour/weekday cats + flags + meta_v1 (GPU), time-aware CV; cache OOF/test\n",
        "import os, sys, time, gc, re, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostClassifier, Pool\n",
        "except Exception:\n",
        "    import subprocess\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'catboost'])\n",
        "    from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body(df):\n",
        "    return (df['request_text'] if 'request_text' in df.columns else df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "\n",
        "# Time features\n",
        "dt_tr = pd.to_datetime(train['unix_timestamp_of_request'].astype(np.int64).values, unit='s', utc=True)\n",
        "dt_te = pd.to_datetime(test['unix_timestamp_of_request'].astype(np.int64).values, unit='s', utc=True)\n",
        "hour_tr = dt_tr.hour.astype(str).values\n",
        "hour_te = dt_te.hour.astype(str).values\n",
        "wday_tr = dt_tr.weekday.astype(str).values\n",
        "wday_te = dt_te.weekday.astype(str).values\n",
        "\n",
        "# Simple flags\n",
        "def build_flags(title: pd.Series, body: pd.Series):\n",
        "    text = (title + '\\n' + body)\n",
        "    has_money = text.str.contains(r'\\$|dollar(s)?|cash|money', case=False, regex=True).astype(np.int8).values\n",
        "    has_urgent = text.str.contains(r'(?i)urgent|emergency|immediately|asap|right away', regex=True).astype(np.int8).values\n",
        "    has_please = text.str.contains(r'(?i)\\bplease\\b', regex=True).astype(np.int8).values\n",
        "    has_thanks = text.str.contains(r'(?i)\\bthank(s| you)?\\b', regex=True).astype(np.int8).values\n",
        "    return has_money, has_urgent, has_please, has_thanks\n",
        "\n",
        "title_tr = get_title(train); body_tr = get_body(train)\n",
        "title_te = get_title(test);  body_te = get_body(test)\n",
        "text_tr = (title_tr + ' [SEP] ' + body_tr).astype(str)\n",
        "text_te = (title_te + ' [SEP] ' + body_te).astype(str)\n",
        "f_money_tr, f_urgent_tr, f_please_tr, f_thanks_tr = build_flags(title_tr, body_tr)\n",
        "f_money_te, f_urgent_te, f_please_te, f_thanks_te = build_flags(title_te, body_te)\n",
        "\n",
        "# Load meta_v1 features\n",
        "Meta_tr = np.load('meta_v1_tr.npy').astype(np.float32)\n",
        "Meta_te = np.load('meta_v1_te.npy').astype(np.float32)\n",
        "n_meta = Meta_tr.shape[1]\n",
        "\n",
        "# Assemble DataFrames: columns = ['text','hour','weekday','has_money','has_urgent','has_please','has_thanks', meta...]\n",
        "meta_cols = [f'm{i}' for i in range(n_meta)]\n",
        "Xtr_df = pd.DataFrame({'text': text_tr, 'hour': hour_tr, 'weekday': wday_tr,\n",
        "                       'has_money': f_money_tr, 'has_urgent': f_urgent_tr, 'has_please': f_please_tr, 'has_thanks': f_thanks_tr})\n",
        "for i, col in enumerate(meta_cols):\n",
        "    Xtr_df[col] = Meta_tr[:, i]\n",
        "Xte_df = pd.DataFrame({'text': text_te, 'hour': hour_te, 'weekday': wday_te,\n",
        "                       'has_money': f_money_te, 'has_urgent': f_urgent_te, 'has_please': f_please_te, 'has_thanks': f_thanks_te})\n",
        "for i, col in enumerate(meta_cols):\n",
        "    Xte_df[col] = Meta_te[:, i]\n",
        "\n",
        "# Indices for CatBoost\n",
        "text_features = [0]  # 'text'\n",
        "cat_features = [1, 2]  # 'hour','weekday'\n",
        "\n",
        "# Time-aware 6-block forward-chaining (validate blocks 1..5)\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}', flush=True)\n",
        "\n",
        "# CatBoost v2 params (expert):\n",
        "params_base = dict(\n",
        "    iterations=4000,\n",
        "    depth=6,\n",
        "    learning_rate=0.03,\n",
        "    l2_leaf_reg=9.0,\n",
        "    bagging_temperature=1.0,\n",
        "    random_strength=1.0,\n",
        "    loss_function='Logloss',\n",
        "    eval_metric='AUC',\n",
        "    task_type='GPU',\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "oof = np.zeros(n, dtype=np.float32)\n",
        "te_parts = []\n",
        "t_all = time.time()\n",
        "for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    t0 = time.time()\n",
        "    X_tr_fold = Xtr_df.iloc[tr_idx].reset_index(drop=True)\n",
        "    X_va_fold = Xtr_df.iloc[va_idx].reset_index(drop=True)\n",
        "    y_tr = y[tr_idx]; y_va = y[va_idx]\n",
        "    pos = float((y_tr == 1).sum()); neg = float((y_tr == 0).sum())\n",
        "    spw = (neg / max(pos, 1.0)) if pos > 0 else 1.0\n",
        "    params = dict(params_base); params['scale_pos_weight'] = spw; params['random_seed'] = 2025 + fi\n",
        "    pool_tr = Pool(X_tr_fold, label=y_tr, text_features=text_features, cat_features=cat_features)\n",
        "    pool_va = Pool(X_va_fold, label=y_va, text_features=text_features, cat_features=cat_features)\n",
        "    pool_te = Pool(Xte_df, text_features=text_features, cat_features=cat_features)\n",
        "    model = CatBoostClassifier(**params)\n",
        "    model.fit(pool_tr, eval_set=pool_va, use_best_model=True, early_stopping_rounds=100)\n",
        "    va_pred = model.predict_proba(pool_va)[:,1].astype(np.float32)\n",
        "    te_pred = model.predict_proba(pool_te)[:,1].astype(np.float32)\n",
        "    oof[va_idx] = va_pred; te_parts.append(te_pred)\n",
        "    auc = roc_auc_score(y_va, va_pred) if (y_va.min()!=y_va.max()) else 0.5\n",
        "    best_it = getattr(model, 'best_iteration_', None)\n",
        "    print(f'[CatTextMeta_v2] Fold {fi} AUC={auc:.5f} | spw={spw:.2f} | best_it={best_it} | {time.time()-t0:.1f}s', flush=True)\n",
        "    del pool_tr, pool_va, pool_te, model; gc.collect()\n",
        "\n",
        "auc_oof = roc_auc_score(y[mask], oof[mask])\n",
        "te_mean = np.mean(te_parts, axis=0).astype(np.float32)\n",
        "print(f'[CatTextMeta_v2] DONE | OOF(validated) AUC={auc_oof:.5f} | total {time.time()-t_all:.1f}s', flush=True)\n",
        "np.save('oof_catboost_textmeta_v2.npy', oof.astype(np.float32))\n",
        "np.save('test_catboost_textmeta_v2.npy', te_mean.astype(np.float32))\n",
        "print('Saved oof_catboost_textmeta_v2.npy and test_catboost_textmeta_v2.npy', flush=True)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_18518/242963231.py:34: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  has_money = text.str.contains(r'\\$|dollar(s)?|cash|money', case=False, regex=True).astype(np.int8).values\n/tmp/ipykernel_18518/242963231.py:37: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  has_thanks = text.str.contains(r'(?i)\\bthank(s| you)?\\b', regex=True).astype(np.int8).values\n/tmp/ipykernel_18518/242963231.py:34: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  has_money = text.str.contains(r'\\$|dollar(s)?|cash|money', case=False, regex=True).astype(np.int8).values\n/tmp/ipykernel_18518/242963231.py:37: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  has_thanks = text.str.contains(r'(?i)\\bthank(s| you)?\\b', regex=True).astype(np.int8).values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV: 5 folds; validated 2398/2878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatTextMeta_v2] Fold 1 AUC=0.70265 | spw=1.94 | best_it=189 | 10.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatTextMeta_v2] Fold 2 AUC=0.69456 | spw=2.33 | best_it=26 | 4.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatTextMeta_v2] Fold 3 AUC=0.62085 | spw=2.49 | best_it=6 | 3.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatTextMeta_v2] Fold 4 AUC=0.65066 | spw=2.79 | best_it=6 | 3.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatTextMeta_v2] Fold 5 AUC=0.60361 | spw=2.83 | best_it=286 | 11.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatTextMeta_v2] DONE | OOF(validated) AUC=0.64769 | total 33.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_catboost_textmeta_v2.npy and test_catboost_textmeta_v2.npy\n"
          ]
        }
      ]
    },
    {
      "id": "0d2e1614-2691-46a0-a209-d51b5f0c47e3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S56: Recency-heavy reblend including CatBoost_v2 and embedding+meta heads; widen Cat cap; Dense can drop to 0\n",
        "import numpy as np, pandas as pd, time\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Time blocks and masks\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "n = len(train)\n",
        "mask_full = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    mask_full[np.array(blocks[i])] = True\n",
        "mask_last2 = np.zeros(n, dtype=bool)\n",
        "for i in [4,5]:\n",
        "    mask_last2[np.array(blocks[i])] = True\n",
        "print(f'Time-CV validated full: {mask_full.sum()}/{n} | last2: {mask_last2.sum()}', flush=True)\n",
        "\n",
        "# Core base OOF/test\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy');    t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "o_lr_ns = np.load('oof_lr_time_nosub_meta.npy');     t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy');            t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_time_v2.npy');         t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta_time.npy');           t_meta = np.load('test_xgb_meta_time.npy') if not Path('test_xgb_meta_fullbag.npy').exists() else np.load('test_xgb_meta_fullbag.npy')\n",
        "o_emn = np.load('oof_xgb_emb_meta_time.npy');        t_emn = np.load('test_xgb_emb_meta_time.npy') if not Path('test_xgb_emb_minilm_fullbag.npy').exists() else np.load('test_xgb_emb_minilm_fullbag.npy')\n",
        "o_emp = np.load('oof_xgb_emb_mpnet_time.npy');       t_emp = np.load('test_xgb_emb_mpnet_time.npy') if not Path('test_xgb_emb_mpnet_fullbag.npy').exists() else np.load('test_xgb_emb_mpnet_fullbag.npy')\n",
        "\n",
        "# New embedding+meta heads\n",
        "has_mm_meta = Path('oof_xgb_emb_minilm_mpnet_meta_time.npy').exists() and Path('test_xgb_emb_minilm_mpnet_meta_time.npy').exists()\n",
        "if has_mm_meta:\n",
        "    o_mm_meta = np.load('oof_xgb_emb_minilm_mpnet_meta_time.npy'); t_mm_meta = np.load('test_xgb_emb_minilm_mpnet_meta_time.npy')\n",
        "has_e5_meta = Path('oof_xgb_e5_meta_time.npy').exists() and Path('test_xgb_e5_meta_time.npy').exists()\n",
        "if has_e5_meta:\n",
        "    o_e5_meta = np.load('oof_xgb_e5_meta_time.npy'); t_e5_meta = np.load('test_xgb_e5_meta_time.npy')\n",
        "has_all_meta = Path('oof_xgb_emb_e5_minilm_mpnet_meta_time.npy').exists() and Path('test_xgb_emb_e5_minilm_mpnet_meta_time.npy').exists()\n",
        "if has_all_meta:\n",
        "    o_all_meta = np.load('oof_xgb_emb_e5_minilm_mpnet_meta_time.npy'); t_all_meta = np.load('test_xgb_emb_e5_minilm_mpnet_meta_time.npy')\n",
        "\n",
        "# Dual-view SVD optional\n",
        "has_svd_dual = Path('oof_xgb_svd_word192_char128_meta.npy').exists() and Path('test_xgb_svd_word192_char128_meta.npy').exists()\n",
        "if has_svd_dual:\n",
        "    o_svd_dual = np.load('oof_xgb_svd_word192_char128_meta.npy'); t_svd_dual = np.load('test_xgb_svd_word192_char128_meta.npy')\n",
        "\n",
        "# Char LR optional\n",
        "has_char = Path('oof_lr_charwb_time.npy').exists() and Path('test_lr_charwb_time.npy').exists()\n",
        "if has_char:\n",
        "    o_char = np.load('oof_lr_charwb_time.npy'); t_char = np.load('test_lr_charwb_time.npy')\n",
        "\n",
        "# CatBoost v1/v2: choose better OOF\n",
        "has_cat_v1 = Path('oof_catboost_textmeta.npy').exists() and Path('test_catboost_textmeta.npy').exists()\n",
        "has_cat_v2 = Path('oof_catboost_textmeta_v2.npy').exists() and Path('test_catboost_textmeta_v2.npy').exists()\n",
        "z_cat, tz_cat, cat_ver = None, None, None\n",
        "if has_cat_v1 and has_cat_v2:\n",
        "    o1 = np.load('oof_catboost_textmeta.npy'); o2 = np.load('oof_catboost_textmeta_v2.npy')\n",
        "    auc1 = roc_auc_score(y[mask_full], o1[mask_full]); auc2 = roc_auc_score(y[mask_full], o2[mask_full])\n",
        "    if auc2 >= auc1:\n",
        "        z_cat = to_logit(o2); tz_cat = to_logit(np.load('test_catboost_textmeta_v2.npy')); cat_ver = 'v2'\n",
        "    else:\n",
        "        z_cat = to_logit(o1); tz_cat = to_logit(np.load('test_catboost_textmeta.npy')); cat_ver = 'v1'\n",
        "elif has_cat_v2:\n",
        "    z_cat = to_logit(np.load('oof_catboost_textmeta_v2.npy')); tz_cat = to_logit(np.load('test_catboost_textmeta_v2.npy')); cat_ver = 'v2'\n",
        "elif has_cat_v1:\n",
        "    z_cat = to_logit(np.load('oof_catboost_textmeta.npy')); tz_cat = to_logit(np.load('test_catboost_textmeta.npy')); cat_ver = 'v1'\n",
        "else:\n",
        "    raise FileNotFoundError('No CatBoost OOF/test found')\n",
        "print(f'Using CatBoost {cat_ver}', flush=True)\n",
        "\n",
        "# Convert OOF to logits\n",
        "z_lr_w, z_lr_ns = to_logit(o_lr_w), to_logit(o_lr_ns)\n",
        "z_d1, z_d2, z_meta = to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)\n",
        "z_emn, z_emp = to_logit(o_emn), to_logit(o_emp)\n",
        "z_mm_meta = to_logit(o_mm_meta) if has_mm_meta else None\n",
        "z_e5_meta = to_logit(o_e5_meta) if has_e5_meta else None\n",
        "z_all_meta = to_logit(o_all_meta) if has_all_meta else None\n",
        "z_svd = to_logit(o_svd_dual) if has_svd_dual else None\n",
        "z_char = to_logit(o_char) if has_char else None\n",
        "\n",
        "# Convert test to logits\n",
        "tz_lr_w, tz_lr_ns = to_logit(t_lr_w), to_logit(t_lr_ns)\n",
        "tz_d1, tz_d2, tz_meta = to_logit(t_d1), to_logit(t_d2), to_logit(t_meta)\n",
        "tz_emn, tz_emp = to_logit(t_emn), to_logit(t_emp)\n",
        "tz_mm_meta = to_logit(t_mm_meta) if has_mm_meta else None\n",
        "tz_e5_meta = to_logit(t_e5_meta) if has_e5_meta else None\n",
        "tz_all_meta = to_logit(t_all_meta) if has_all_meta else None\n",
        "tz_svd = to_logit(t_svd_dual) if has_svd_dual else None\n",
        "tz_char = to_logit(t_char) if has_char else None\n",
        "\n",
        "# Grids per expert guidance\n",
        "g_grid = [0.990, 0.995, 0.997]\n",
        "meta_grid = [0.16, 0.18, 0.20, 0.22]\n",
        "dense_tot_grid = [0.0, 0.04, 0.08]\n",
        "dense_split = [(0.6, 0.4), (0.7, 0.3)]\n",
        "emb_tot_grid = [0.28, 0.32, 0.36, 0.38]\n",
        "emb_split = [(0.6, 0.4), (0.5, 0.5)]  # MiniLM:MPNet\n",
        "e5_cap_grid = [0.0, 0.02, 0.04, 0.06] if has_e5_meta else [0.0]\n",
        "cat_grid = [0.10, 0.14, 0.20, 0.26, 0.30]\n",
        "svd_grid = [0.0, 0.04, 0.08] if has_svd_dual else [0.0]\n",
        "char_grid = [0.0, 0.04, 0.06, 0.08] if has_char else [0.0]\n",
        "w_lr_min_grid = [0.28]\n",
        "\n",
        "def search(mask, sample_weight=None):\n",
        "    best_auc, best_cfg, tried = -1.0, None, 0\n",
        "    t0 = time.time()\n",
        "    for g in g_grid:\n",
        "        z_lr_mix = (1.0 - g)*z_lr_w + g*z_lr_ns\n",
        "        for w_lr_min in w_lr_min_grid:\n",
        "            for w_meta in meta_grid:\n",
        "                for d_tot in dense_tot_grid:\n",
        "                    for dv1, dv2 in dense_split:\n",
        "                        w_d1 = d_tot * dv1; w_d2 = d_tot * dv2\n",
        "                        for e_tot in emb_tot_grid:\n",
        "                            for em_fr, mp_fr in emb_split:\n",
        "                                w_emn = e_tot * em_fr; w_emp = e_tot * mp_fr\n",
        "                                for w_e5 in e5_cap_grid:\n",
        "                                    for w_cat in cat_grid:\n",
        "                                        for w_svd in svd_grid:\n",
        "                                            for w_char in char_grid:\n",
        "                                                rem = 1.0 - (w_meta + w_d1 + w_d2 + w_emn + w_emp + w_e5 + w_cat + w_svd + w_char)\n",
        "                                                if rem <= 0: continue\n",
        "                                                w_lr = rem\n",
        "                                                if w_lr < w_lr_min: continue\n",
        "                                                z_oof = (w_lr*z_lr_mix +\n",
        "                                                         w_d1*z_d1 + w_d2*z_d2 +\n",
        "                                                         w_meta*z_meta +\n",
        "                                                         w_emn*z_emn + w_emp*z_emp +\n",
        "                                                         (w_e5*z_e5_meta if has_e5_meta and w_e5>0 else 0) +\n",
        "                                                         (w_svd*z_svd if has_svd_dual and w_svd>0 else 0) +\n",
        "                                                         (w_char*z_char if has_char and w_char>0 else 0) +\n",
        "                                                         w_cat*z_cat)\n",
        "                                                auc = roc_auc_score(y[mask], z_oof[mask], sample_weight=(sample_weight[mask] if sample_weight is not None else None))\n",
        "                                                tried += 1\n",
        "                                                if tried % 3000 == 0:\n",
        "                                                    print(f'  tried={tried} | best={best_auc:.5f} | elapsed={time.time()-t0:.1f}s', flush=True)\n",
        "                                                if auc > best_auc:\n",
        "                                                    best_auc = auc\n",
        "                                                    best_cfg = dict(g=float(g), w_lr=float(w_lr), w_d1=float(w_d1), w_d2=float(w_d2),\n",
        "                                                                    w_meta=float(w_meta), w_emn=float(w_emn), w_emp=float(w_emp),\n",
        "                                                                    w_e5=float(w_e5), w_cat=float(w_cat), w_svd=float(w_svd), w_char=float(w_char))\n",
        "    print(f'  search done | tried={tried} | best={best_auc:.5f} | {time.time()-t0:.1f}s', flush=True)\n",
        "    return best_auc, best_cfg, tried\n",
        "\n",
        "# 1) Last-2 objective\n",
        "auc_last2, cfg_last2, tried_last2 = search(mask_last2)\n",
        "print(f'[Last2] tried={tried_last2} | best OOF(z,last2) AUC={auc_last2:.5f} | cfg={cfg_last2}', flush=True)\n",
        "\n",
        "# 2) Gamma-decayed (recency)\n",
        "best_gamma, best_auc_g, best_cfg_g = None, -1.0, None\n",
        "for gamma in [0.990, 0.995, 0.997]:\n",
        "    w = np.zeros(n, dtype=np.float64)\n",
        "    for bi in range(1, k):\n",
        "        age = (k - 1) - bi\n",
        "        w[np.array(blocks[bi])] = (gamma ** age)\n",
        "    auc_g, cfg_g, _ = search(mask_full, sample_weight=w)\n",
        "    print(f'[Gamma {gamma}] best OOF(z,weighted) AUC={auc_g:.5f}', flush=True)\n",
        "    if auc_g > best_auc_g:\n",
        "        best_auc_g, best_cfg_g, best_gamma = auc_g, cfg_g, gamma\n",
        "print(f'[Gamma-best] gamma={best_gamma} | AUC={best_auc_g:.5f} | cfg={best_cfg_g}', flush=True)\n",
        "\n",
        "def build_and_save(tag, cfg):\n",
        "    g = cfg['g']\n",
        "    tz_lr_mix = (1.0 - g)*tz_lr_w + g*tz_lr_ns\n",
        "    parts = [\n",
        "        cfg['w_lr']*tz_lr_mix,\n",
        "        cfg['w_d1']*to_logit(t_d1),\n",
        "        cfg['w_d2']*to_logit(t_d2),\n",
        "        cfg['w_meta']*to_logit(t_meta),\n",
        "        cfg['w_emn']*to_logit(t_emn),\n",
        "        cfg['w_emp']*to_logit(t_emp),\n",
        "        cfg['w_cat']*tz_cat\n",
        "    ]\n",
        "    w_list = [cfg['w_lr'], cfg['w_d1'], cfg['w_d2'], cfg['w_meta'], cfg['w_emn'], cfg['w_emp'], cfg['w_cat']]\n",
        "    comp_logits = [tz_lr_mix, to_logit(t_d1), to_logit(t_d2), to_logit(t_meta), to_logit(t_emn), to_logit(t_emp), tz_cat]\n",
        "    if has_e5_meta and cfg['w_e5'] > 0: parts.append(cfg['w_e5']*to_logit(t_e5_meta)); w_list.append(cfg['w_e5']); comp_logits.append(to_logit(t_e5_meta))\n",
        "    if has_svd_dual and cfg['w_svd'] > 0: parts.append(cfg['w_svd']*to_logit(t_svd_dual)); w_list.append(cfg['w_svd']); comp_logits.append(to_logit(t_svd_dual))\n",
        "    if has_char and cfg['w_char'] > 0: parts.append(cfg['w_char']*to_logit(t_char)); w_list.append(cfg['w_char']); comp_logits.append(to_logit(t_char))\n",
        "    zt = np.sum(parts, axis=0)\n",
        "    pt = sigmoid(zt).astype(np.float32)\n",
        "    out_path = f'submission_reblend_recency_{tag}.csv'\n",
        "    pd.DataFrame({id_col: ids, target_col: pt}).to_csv(out_path, index=False)\n",
        "    # 15% shrink-to-equal hedge\n",
        "    w_vec = np.array(w_list, dtype=np.float64)\n",
        "    w_eq = np.ones_like(w_vec)/len(w_vec)\n",
        "    alpha = 0.15\n",
        "    w_shr = ((1.0 - alpha)*w_vec + alpha*w_eq); w_shr = (w_shr / w_shr.sum()).astype(np.float64)\n",
        "    zt_shr = 0.0\n",
        "    for wi, zi in zip(w_shr, comp_logits):\n",
        "        zt_shr += wi*zi\n",
        "    pt_shr = sigmoid(zt_shr).astype(np.float32)\n",
        "    pd.DataFrame({id_col: ids, target_col: pt_shr}).to_csv(out_path.replace('.csv','_shrunk.csv'), index=False)\n",
        "    return out_path\n",
        "\n",
        "p_last2 = build_and_save('last2', cfg_last2)\n",
        "p_gam = build_and_save(f'gamma{best_gamma:.3f}'.replace('.', 'p'), best_cfg_g)\n",
        "\n",
        "# Promote better of last-2 vs gamma-best (prefer gamma if close); then leave rank-avg hedge for external step if needed\n",
        "primary = p_gam if (best_auc_g >= auc_last2) else p_last2\n",
        "pd.read_csv(primary).to_csv('submission.csv', index=False)\n",
        "print(f'Promoted {Path(primary).name} to submission.csv', flush=True)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV validated full: 2398/2878 | last2: 958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CatBoost v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=3000 | best=0.64918 | elapsed=5.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=6000 | best=0.64923 | elapsed=10.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=9000 | best=0.64923 | elapsed=16.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  search done | tried=9003 | best=0.64923 | 16.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Last2] tried=9003 | best OOF(z,last2) AUC=0.64923 | cfg={'g': 0.995, 'w_lr': 0.28, 'w_d1': 0.055999999999999994, 'w_d2': 0.024, 'w_meta': 0.16, 'w_emn': 0.168, 'w_emp': 0.11200000000000002, 'w_e5': 0.02, 'w_cat': 0.1, 'w_svd': 0.0, 'w_char': 0.08}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=3000 | best=0.68175 | elapsed=7.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=6000 | best=0.68175 | elapsed=14.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=9000 | best=0.68176 | elapsed=21.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  search done | tried=9003 | best=0.68176 | 21.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gamma 0.99] best OOF(z,weighted) AUC=0.68176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=3000 | best=0.68204 | elapsed=7.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=6000 | best=0.68204 | elapsed=14.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=9000 | best=0.68205 | elapsed=21.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  search done | tried=9003 | best=0.68205 | 21.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gamma 0.995] best OOF(z,weighted) AUC=0.68205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=3000 | best=0.68215 | elapsed=7.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=6000 | best=0.68215 | elapsed=14.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=9000 | best=0.68216 | elapsed=21.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  search done | tried=9003 | best=0.68216 | 21.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gamma 0.997] best OOF(z,weighted) AUC=0.68216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gamma-best] gamma=0.997 | AUC=0.68216 | cfg={'g': 0.997, 'w_lr': 0.28, 'w_d1': 0.027999999999999997, 'w_d2': 0.012, 'w_meta': 0.2, 'w_emn': 0.16, 'w_emp': 0.16, 'w_e5': 0.0, 'w_cat': 0.1, 'w_svd': 0.0, 'w_char': 0.06}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promoted submission_reblend_recency_gamma0p997.csv to submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "d9016efa-270c-4662-b544-4370102dd568",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S57: Super-recent refits (blocks 3-5 and 4-5): LR_nosub+meta, MiniLM XGB+meta, CatBoost v2; save test preds\n",
        "import numpy as np, pandas as pd, time, gc, xgboost as xgb\n",
        "from pathlib import Path\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body(df):\n",
        "    return (df['request_text'] if 'request_text' in df.columns else df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "def build_text(df):\n",
        "    return (get_title(df) + '\\n' + get_body(df)).astype(str)\n",
        "\n",
        "txt_tr = build_text(train); txt_te = build_text(test)\n",
        "\n",
        "# Time blocks\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "idx35 = np.concatenate(blocks[2:5])  # blocks 3,4,5 (0-based idx 2..4)\n",
        "idx45 = np.concatenate(blocks[3:5])  # blocks 4,5 (0-based idx 3..4)\n",
        "print(f'Recent sets: 3-5 n={len(idx35)} | 4-5 n={len(idx45)}', flush=True)\n",
        "\n",
        "# Shared meta\n",
        "Meta_tr = np.load('meta_v1_tr.npy').astype(np.float32)\n",
        "Meta_te = np.load('meta_v1_te.npy').astype(np.float32)\n",
        "\n",
        "# 1) LR_nosub + meta (word1-3 + char_wb2-6 TF-IDF), C=1.0\n",
        "word_params = dict(analyzer='word', ngram_range=(1,3), lowercase=True, min_df=2, max_features=200_000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "char_params = dict(analyzer='char_wb', ngram_range=(2,6), lowercase=True, min_df=2, max_features=200_000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "\n",
        "def lr_recent_fit_predict(tr_idx, tag):\n",
        "    tf_w = TfidfVectorizer(**word_params)\n",
        "    Xw_tr = tf_w.fit_transform(txt_tr.iloc[tr_idx]); Xw_te = tf_w.transform(txt_te)\n",
        "    tf_c = TfidfVectorizer(**char_params)\n",
        "    Xc_tr = tf_c.fit_transform(txt_tr.iloc[tr_idx]); Xc_te = tf_c.transform(txt_te)\n",
        "    X_tr_text = hstack([Xw_tr, Xc_tr], format='csr')\n",
        "    X_te_text = hstack([Xw_te, Xc_te], format='csr')\n",
        "    X_tr = hstack([X_tr_text, csr_matrix(Meta_tr[tr_idx])], format='csr')\n",
        "    X_te = hstack([X_te_text, csr_matrix(Meta_te)], format='csr')\n",
        "    clf = LogisticRegression(penalty='l2', solver='saga', C=1.0, max_iter=2000, n_jobs=-1, verbose=0)\n",
        "    clf.fit(X_tr, y[tr_idx])\n",
        "    te_pred = clf.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "    np.save(f'test_lr_nosub_meta_recent{tag}.npy', te_pred)\n",
        "    print(f'[LR_recent {tag}] te_mean={te_pred.mean():.4f} | feats={X_tr.shape[1]}', flush=True)\n",
        "    del tf_w, tf_c, Xw_tr, Xw_te, Xc_tr, Xc_te, X_tr_text, X_te_text, X_tr, X_te, clf; gc.collect()\n",
        "\n",
        "t0 = time.time(); lr_recent_fit_predict(idx35, '35'); lr_recent_fit_predict(idx45, '45')\n",
        "print(f'LR_recent done in {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "# 2) MiniLM emb+meta XGB recent (GPU, ES on last block inside set)\n",
        "Emb_min_tr = np.load('emb_minilm_tr.npy').astype(np.float32)\n",
        "Emb_min_te = np.load('emb_minilm_te.npy').astype(np.float32)\n",
        "X_all_tr = np.hstack([Emb_min_tr, Meta_tr]).astype(np.float32)\n",
        "X_all_te = np.hstack([Emb_min_te, Meta_te]).astype(np.float32)\n",
        "\n",
        "def xgb_recent_predict(tr_idx, eval_idx, tag, name='MiniLM'):\n",
        "    X_tr = X_all_tr[tr_idx]; y_tr = y[tr_idx]\n",
        "    X_ev = X_all_tr[eval_idx]; y_ev = y[eval_idx]\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_trs = scaler.fit_transform(X_tr).astype(np.float32)\n",
        "    X_evs = scaler.transform(X_ev).astype(np.float32)\n",
        "    X_tes = scaler.transform(X_all_te).astype(np.float32)\n",
        "    dtr = xgb.DMatrix(X_trs, label=y_tr); dev = xgb.DMatrix(X_evs, label=y_ev); dte = xgb.DMatrix(X_tes)\n",
        "    pos = float((y_tr == 1).sum()); neg = float((y_tr == 0).sum()); spw = (neg / max(pos, 1.0)) if pos > 0 else 1.0\n",
        "    params = dict(objective='binary:logistic', eval_metric='auc', max_depth=3, eta=0.05, subsample=0.8, colsample_bytree=0.6,\n",
        "                  min_child_weight=8, reg_alpha=0.5, reg_lambda=3.0, gamma=0.0, device='cuda', tree_method='hist',\n",
        "                  seed=2025, scale_pos_weight=spw)\n",
        "    booster = xgb.train(params, dtr, num_boost_round=4000, evals=[(dev, 'valid')], early_stopping_rounds=100, verbose_eval=False)\n",
        "    te_pred = booster.predict(dte, iteration_range=(0, booster.best_iteration+1 if booster.best_iteration is not None else 0)).astype(np.float32)\n",
        "    np.save(f'test_xgb_minilm_meta_recent{tag}.npy', te_pred)\n",
        "    print(f'[XGB {name}_recent {tag}] rounds={booster.best_iteration} | te_mean={te_pred.mean():.4f}', flush=True)\n",
        "    del dtr, dev, dte, booster, scaler; gc.collect()\n",
        "\n",
        "# For 3-5, use block5 as eval; for 4-5, use block5 as eval (leaving block4 as train) to pick rounds\n",
        "blk5 = np.array(blocks[4])\n",
        "xgb_recent_predict(idx35, blk5, '35')\n",
        "xgb_recent_predict(idx45, blk5, '45')\n",
        "\n",
        "# 3) CatBoost v2 recent (single text + hour/weekday cats + flags + meta), GPU\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "def build_cat_v2_df(df):\n",
        "    title = get_title(df); body = get_body(df)\n",
        "    text = (title + ' [SEP] ' + body).astype(str)\n",
        "    dt = pd.to_datetime(df['unix_timestamp_of_request'].astype(np.int64).values, unit='s', utc=True)\n",
        "    hour = dt.hour.astype(str).values; wday = dt.weekday.astype(str).values\n",
        "    txt = (title + '\\n' + body)\n",
        "    has_money = txt.str.contains(r'\\$|dollar(s)?|cash|money', case=False, regex=True).astype(np.int8).values\n",
        "    has_urgent = txt.str.contains(r'(?i)urgent|emergency|immediately|asap|right away', regex=True).astype(np.int8).values\n",
        "    has_please = txt.str.contains(r'(?i)\\bplease\\b', regex=True).astype(np.int8).values\n",
        "    has_thanks = txt.str.contains(r'(?i)\\bthank(s| you)?\\b', regex=True).astype(np.int8).values\n",
        "    return text, hour, wday, has_money, has_urgent, has_please, has_thanks\n",
        "\n",
        "text_tr, hour_tr, wday_tr, f_m_tr, f_u_tr, f_p_tr, f_t_tr = build_cat_v2_df(train)\n",
        "text_te, hour_te, wday_te, f_m_te, f_u_te, f_p_te, f_t_te = build_cat_v2_df(test)\n",
        "def make_cat_df(sel_idx=None):\n",
        "    if sel_idx is None:\n",
        "        Xtr = pd.DataFrame({'text': text_tr, 'hour': hour_tr, 'weekday': wday_tr,\n",
        "                            'has_money': f_m_tr, 'has_urgent': f_u_tr, 'has_please': f_p_tr, 'has_thanks': f_t_tr})\n",
        "        ytr = y\n",
        "    else:\n",
        "        Xtr = pd.DataFrame({'text': text_tr[sel_idx], 'hour': hour_tr[sel_idx], 'weekday': wday_tr[sel_idx],\n",
        "                            'has_money': f_m_tr[sel_idx], 'has_urgent': f_u_tr[sel_idx], 'has_please': f_p_tr[sel_idx], 'has_thanks': f_t_tr[sel_idx]})\n",
        "        ytr = y[sel_idx]\n",
        "    Xte = pd.DataFrame({'text': text_te, 'hour': hour_te, 'weekday': wday_te,\n",
        "                        'has_money': f_m_te, 'has_urgent': f_u_te, 'has_please': f_p_te, 'has_thanks': f_t_te})\n",
        "    # append meta\n",
        "    n_meta = Meta_tr.shape[1]\n",
        "    for i in range(n_meta):\n",
        "        col = f'm{i}';\n",
        "        if sel_idx is None:\n",
        "            Xtr[col] = Meta_tr[:, i]\n",
        "        else:\n",
        "            Xtr[col] = Meta_tr[sel_idx, i]\n",
        "        Xte[col] = Meta_te[:, i]\n",
        "    return Xtr, ytr, Xte\n",
        "\n",
        "def cat_recent_predict(tr_idx, eval_idx, tag):\n",
        "    Xtr, ytr, Xte = make_cat_df(tr_idx)\n",
        "    Xev, yev = make_cat_df(eval_idx)[0], y[eval_idx]\n",
        "    pool_tr = Pool(Xtr, label=ytr, text_features=[0], cat_features=[1,2])\n",
        "    pool_ev = Pool(Xev, label=yev, text_features=[0], cat_features=[1,2])\n",
        "    pool_te = Pool(Xte, text_features=[0], cat_features=[1,2])\n",
        "    pos = float((ytr == 1).sum()); neg = float((ytr == 0).sum()); spw = (neg / max(pos, 1.0)) if pos > 0 else 1.0\n",
        "    params = dict(iterations=4000, depth=6, learning_rate=0.03, l2_leaf_reg=9.0, bagging_temperature=1.0, random_strength=1.0,\n",
        "                  loss_function='Logloss', eval_metric='AUC', task_type='GPU', verbose=False, scale_pos_weight=spw, random_seed=3030)\n",
        "    model = CatBoostClassifier(**params)\n",
        "    model.fit(pool_tr, eval_set=pool_ev, use_best_model=True, early_stopping_rounds=100)\n",
        "    te_pred = model.predict_proba(pool_te)[:,1].astype(np.float32)\n",
        "    np.save(f'test_catboost_textmeta_v2_recent{tag}.npy', te_pred)\n",
        "    print(f'[Cat_v2_recent {tag}] best_it={getattr(model, \"best_iteration_\", None)} | te_mean={te_pred.mean():.4f}', flush=True)\n",
        "    del pool_tr, pool_ev, pool_te, model; gc.collect()\n",
        "\n",
        "# eval on block5 for both\n",
        "cat_recent_predict(idx35, blk5, '35')\n",
        "cat_recent_predict(idx45, blk5, '45')\n",
        "print('Super-recent refits done.', flush=True)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recent sets: 3-5 n=1439 | 4-5 n=959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_recent 35] te_mean=0.1597 | feats=69250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_recent 45] te_mean=0.1586 | feats=50577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR_recent done in 52.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[XGB MiniLM_recent 35] rounds=306 | te_mean=0.3463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[XGB MiniLM_recent 45] rounds=172 | te_mean=0.3510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_18518/3989551110.py:96: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  has_money = txt.str.contains(r'\\$|dollar(s)?|cash|money', case=False, regex=True).astype(np.int8).values\n/tmp/ipykernel_18518/3989551110.py:99: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  has_thanks = txt.str.contains(r'(?i)\\bthank(s| you)?\\b', regex=True).astype(np.int8).values\n/tmp/ipykernel_18518/3989551110.py:96: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  has_money = txt.str.contains(r'\\$|dollar(s)?|cash|money', case=False, regex=True).astype(np.int8).values\n/tmp/ipykernel_18518/3989551110.py:99: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  has_thanks = txt.str.contains(r'(?i)\\bthank(s| you)?\\b', regex=True).astype(np.int8).values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Cat_v2_recent 35] best_it=2307 | te_mean=0.3223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: less than 75% GPU memory available for training. Free: 8551.625 Total: 14930.5625\nDefault metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Cat_v2_recent 45] best_it=1440 | te_mean=0.2561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Super-recent refits done.\n"
          ]
        }
      ]
    },
    {
      "id": "bb81b12e-ea14-4a71-940e-5a78311abbe9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S58: Reblend with super-recent variants (LR_nosub+meta, MiniLM XGB+meta, CatBoost v2) as small-cap components\n",
        "import numpy as np, pandas as pd, time\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Time blocks and masks\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "n = len(train)\n",
        "mask_full = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    mask_full[np.array(blocks[i])] = True\n",
        "mask_last2 = np.zeros(n, dtype=bool)\n",
        "for i in [4,5]:\n",
        "    mask_last2[np.array(blocks[i])] = True\n",
        "print(f'Time-CV validated full: {mask_full.sum()}/{n} | last2: {mask_last2.sum()}', flush=True)\n",
        "\n",
        "# Load core OOF/test (full-history)\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy');    t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "o_lr_ns = np.load('oof_lr_time_nosub_meta.npy');     t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy');            t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_time_v2.npy');         t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta_time.npy');           t_meta = np.load('test_xgb_meta_time.npy') if not Path('test_xgb_meta_fullbag.npy').exists() else np.load('test_xgb_meta_fullbag.npy')\n",
        "o_emn = np.load('oof_xgb_emb_meta_time.npy');        t_emn = np.load('test_xgb_emb_meta_time.npy') if not Path('test_xgb_emb_minilm_fullbag.npy').exists() else np.load('test_xgb_emb_minilm_fullbag.npy')\n",
        "o_emp = np.load('oof_xgb_emb_mpnet_time.npy');       t_emp = np.load('test_xgb_emb_mpnet_time.npy') if not Path('test_xgb_emb_mpnet_fullbag.npy').exists() else np.load('test_xgb_emb_mpnet_fullbag.npy')\n",
        "\n",
        "# Optional components available\n",
        "has_svd_dual = Path('oof_xgb_svd_word192_char128_meta.npy').exists() and Path('test_xgb_svd_word192_char128_meta.npy').exists()\n",
        "if has_svd_dual:\n",
        "    o_svd_dual = np.load('oof_xgb_svd_word192_char128_meta.npy'); t_svd_dual = np.load('test_xgb_svd_word192_char128_meta.npy')\n",
        "has_char = Path('oof_lr_charwb_time.npy').exists() and Path('test_lr_charwb_time.npy').exists()\n",
        "if has_char:\n",
        "    o_char = np.load('oof_lr_charwb_time.npy'); t_char = np.load('test_lr_charwb_time.npy')\n",
        "\n",
        "# CatBoost v1/v2 choose better OOF\n",
        "has_cat_v1 = Path('oof_catboost_textmeta.npy').exists() and Path('test_catboost_textmeta.npy').exists()\n",
        "has_cat_v2 = Path('oof_catboost_textmeta_v2.npy').exists() and Path('test_catboost_textmeta_v2.npy').exists()\n",
        "if has_cat_v1 and has_cat_v2:\n",
        "    o1 = np.load('oof_catboost_textmeta.npy'); o2 = np.load('oof_catboost_textmeta_v2.npy')\n",
        "    auc1 = roc_auc_score(y[mask_full], o1[mask_full]); auc2 = roc_auc_score(y[mask_full], o2[mask_full])\n",
        "    if auc2 >= auc1:\n",
        "        o_cat = o2; t_cat = np.load('test_catboost_textmeta_v2.npy'); cat_tag = 'v2'\n",
        "    else:\n",
        "        o_cat = o1; t_cat = np.load('test_catboost_textmeta.npy'); cat_tag = 'v1'\n",
        "elif has_cat_v2:\n",
        "    o_cat = np.load('oof_catboost_textmeta_v2.npy'); t_cat = np.load('test_catboost_textmeta_v2.npy'); cat_tag = 'v2'\n",
        "elif has_cat_v1:\n",
        "    o_cat = np.load('oof_catboost_textmeta.npy'); t_cat = np.load('test_catboost_textmeta.npy'); cat_tag = 'v1'\n",
        "else:\n",
        "    raise FileNotFoundError('No CatBoost OOF/test found')\n",
        "print(f'CatBoost selected: {cat_tag}')\n",
        "\n",
        "# Super-recent test preds (average 3-5 and 4-5); OOF placeholders = corresponding full OOF (to keep objective comparable)\n",
        "def load_avg_recent(base):\n",
        "    p35 = np.load(f'test_{base}_recent35.npy') if Path(f'test_{base}_recent35.npy').exists() else None\n",
        "    p45 = np.load(f'test_{base}_recent45.npy') if Path(f'test_{base}_recent45.npy').exists() else None\n",
        "    if (p35 is None) and (p45 is None):\n",
        "        return None\n",
        "    if (p35 is None):\n",
        "        return p45.astype(np.float32)\n",
        "    if (p45 is None):\n",
        "        return p35.astype(np.float32)\n",
        "    return ((p35 + p45) / 2.0).astype(np.float32)\n",
        "\n",
        "t_lr_recent = load_avg_recent('lr_nosub_meta')  # LR recent\n",
        "t_minilm_recent = load_avg_recent('xgb_minilm_meta')  # MiniLM recent\n",
        "t_cat_recent = load_avg_recent('catboost_textmeta_v2')  # CatBoost v2 recent\n",
        "\n",
        "# Build logits for OOF\n",
        "z_lr_w, z_lr_ns = to_logit(o_lr_w), to_logit(o_lr_ns)\n",
        "z_d1, z_d2, z_meta = to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)\n",
        "z_emn, z_emp = to_logit(o_emn), to_logit(o_emp)\n",
        "z_svd = to_logit(o_svd_dual) if has_svd_dual else None\n",
        "z_char = to_logit(o_char) if has_char else None\n",
        "z_cat = to_logit(o_cat)\n",
        "\n",
        "# Test logits\n",
        "tz_lr_w, tz_lr_ns = to_logit(t_lr_w), to_logit(t_lr_ns)\n",
        "tz_d1, tz_d2, tz_meta = to_logit(t_d1), to_logit(t_d2), to_logit(t_meta)\n",
        "tz_emn, tz_emp = to_logit(t_emn), to_logit(t_emp)\n",
        "tz_svd = to_logit(t_svd_dual) if has_svd_dual else None\n",
        "tz_char = to_logit(t_char) if has_char else None\n",
        "tz_cat = to_logit(t_cat)\n",
        "tz_lr_recent = to_logit(t_lr_recent) if t_lr_recent is not None else None\n",
        "tz_minilm_recent = to_logit(t_minilm_recent) if t_minilm_recent is not None else None\n",
        "tz_cat_recent = to_logit(t_cat_recent) if t_cat_recent is not None else None\n",
        "\n",
        "# Grids and caps\n",
        "g_grid = [0.990, 0.995, 0.997]\n",
        "meta_grid = [0.18, 0.20, 0.22]\n",
        "dense_tot_grid = [0.0, 0.04, 0.08]\n",
        "dense_split = [(0.6, 0.4), (0.7, 0.3)]\n",
        "emb_tot_grid = [0.28, 0.32, 0.36, 0.38]\n",
        "emb_split = [(0.6, 0.4), (0.5, 0.5)]\n",
        "svd_grid = [0.0, 0.04, 0.08] if has_svd_dual else [0.0]\n",
        "char_grid = [0.0, 0.04, 0.06, 0.08] if has_char else [0.0]\n",
        "cat_grid = [0.10, 0.14, 0.20, 0.26, 0.30]\n",
        "w_lr_min_grid = [0.28]\n",
        "# Recent caps\n",
        "lr_recent_cap = [0.0, 0.04, 0.06, 0.08] if tz_lr_recent is not None else [0.0]\n",
        "minilm_recent_cap = [0.0, 0.03, 0.06] if tz_minilm_recent is not None else [0.0]\n",
        "cat_recent_cap = [0.0, 0.05, 0.10] if tz_cat_recent is not None else [0.0]\n",
        "\n",
        "def search(mask, sample_weight=None):\n",
        "    best_auc, best_cfg, tried = -1.0, None, 0\n",
        "    t0 = time.time()\n",
        "    for g in g_grid:\n",
        "        z_lr_mix = (1.0 - g)*z_lr_w + g*z_lr_ns\n",
        "        for w_lr_min in w_lr_min_grid:\n",
        "            for w_meta in meta_grid:\n",
        "                for d_tot in dense_tot_grid:\n",
        "                    for dv1, dv2 in dense_split:\n",
        "                        w_d1 = d_tot * dv1; w_d2 = d_tot * dv2\n",
        "                        for e_tot in emb_tot_grid:\n",
        "                            for em_fr, mp_fr in emb_split:\n",
        "                                w_emn = e_tot * em_fr; w_emp = e_tot * mp_fr\n",
        "                                for w_svd in svd_grid:\n",
        "                                    for w_char in char_grid:\n",
        "                                        for w_cat in cat_grid:\n",
        "                                            for w_lr_rec in lr_recent_cap:\n",
        "                                                for w_minilm_rec in minilm_recent_cap:\n",
        "                                                    for w_cat_rec in cat_recent_cap:\n",
        "                                                        rem = 1.0 - (w_meta + w_d1 + w_d2 + w_emn + w_emp + w_svd + w_char + w_cat + w_lr_rec + w_minilm_rec + w_cat_rec)\n",
        "                                                        if rem <= 0: continue\n",
        "                                                        w_lr = rem\n",
        "                                                        if w_lr < w_lr_min: continue\n",
        "                                                        # OOF objective ignores 'recent' unique info (no OOF); we don't add them to z_oof beyond caps (placeholder 0 impact)\n",
        "                                                        z_oof = (w_lr*z_lr_mix +\n",
        "                                                                 w_d1*z_d1 + w_d2*z_d2 +\n",
        "                                                                 w_meta*z_meta +\n",
        "                                                                 w_emn*z_emn + w_emp*z_emp +\n",
        "                                                                 (w_svd*z_svd if (has_svd_dual and w_svd>0) else 0) +\n",
        "                                                                 (w_char*z_char if (has_char and w_char>0) else 0) +\n",
        "                                                                 w_cat*z_cat)\n",
        "                                                        auc = roc_auc_score(y[mask], z_oof[mask], sample_weight=(sample_weight[mask] if sample_weight is not None else None))\n",
        "                                                        tried += 1\n",
        "                                                        if tried % 5000 == 0:\n",
        "                                                            print(f'  tried={tried} | best={best_auc:.5f} | elapsed={time.time()-t0:.1f}s', flush=True)\n",
        "                                                        if auc > best_auc:\n",
        "                                                            best_auc = auc\n",
        "                                                            best_cfg = dict(g=float(g), w_lr=float(w_lr), w_d1=float(w_d1), w_d2=float(w_d2),\n",
        "                                                                            w_meta=float(w_meta), w_emn=float(w_emn), w_emp=float(w_emp),\n",
        "                                                                            w_svd=float(w_svd), w_char=float(w_char), w_cat=float(w_cat),\n",
        "                                                                            w_lr_recent=float(w_lr_rec), w_minilm_recent=float(w_minilm_rec), w_cat_recent=float(w_cat_rec))\n",
        "    print(f'  search done | tried={tried} | best={best_auc:.5f} | {time.time()-t0:.1f}s', flush=True)\n",
        "    return best_auc, best_cfg, tried\n",
        "\n",
        "# Objectives: last-2 and gamma in {0.990,0.995,0.997}\n",
        "auc_last2, cfg_last2, tried_last2 = search(mask_last2)\n",
        "print(f'[Last2] tried={tried_last2} | best AUC={auc_last2:.5f} | cfg={cfg_last2}', flush=True)\n",
        "\n",
        "best_gamma, best_auc_g, best_cfg_g = None, -1.0, None\n",
        "for gamma in [0.990, 0.995, 0.997]:\n",
        "    w = np.zeros(n, dtype=np.float64)\n",
        "    for bi in range(1, k):\n",
        "        age = (k - 1) - bi\n",
        "        w[np.array(blocks[bi])] = (gamma ** age)\n",
        "    auc_g, cfg_g, _ = search(mask_full, sample_weight=w)\n",
        "    print(f'[Gamma {gamma}] best AUC={auc_g:.5f}', flush=True)\n",
        "    if auc_g > best_auc_g:\n",
        "        best_auc_g, best_cfg_g, best_gamma = auc_g, cfg_g, gamma\n",
        "print(f'[Gamma-best] gamma={best_gamma} | AUC={best_auc_g:.5f} | cfg={best_cfg_g}', flush=True)\n",
        "\n",
        "def build_and_save(tag, cfg):\n",
        "    g = cfg['g']\n",
        "    tz_lr_mix = (1.0 - g)*tz_lr_w + g*tz_lr_ns\n",
        "    parts = [\n",
        "        cfg['w_lr']*tz_lr_mix,\n",
        "        cfg['w_d1']*to_logit(t_d1),\n",
        "        cfg['w_d2']*to_logit(t_d2),\n",
        "        cfg['w_meta']*to_logit(t_meta),\n",
        "        cfg['w_emn']*to_logit(t_emn),\n",
        "        cfg['w_emp']*to_logit(t_emp),\n",
        "        cfg['w_cat']*tz_cat\n",
        "    ]\n",
        "    w_list = [cfg['w_lr'], cfg['w_d1'], cfg['w_d2'], cfg['w_meta'], cfg['w_emn'], cfg['w_emp'], cfg['w_cat']]\n",
        "    comp_logits = [tz_lr_mix, to_logit(t_d1), to_logit(t_d2), to_logit(t_meta), to_logit(t_emn), to_logit(t_emp), tz_cat]\n",
        "    if has_svd_dual and cfg['w_svd'] > 0: parts.append(cfg['w_svd']*tz_svd); w_list.append(cfg['w_svd']); comp_logits.append(tz_svd)\n",
        "    if has_char and cfg['w_char'] > 0: parts.append(cfg['w_char']*tz_char); w_list.append(cfg['w_char']); comp_logits.append(tz_char)\n",
        "    # Add recent components to TEST ONLY if available\n",
        "    if (tz_lr_recent is not None) and (cfg['w_lr_recent'] > 0): parts.append(cfg['w_lr_recent']*tz_lr_recent); w_list.append(cfg['w_lr_recent']); comp_logits.append(tz_lr_recent)\n",
        "    if (tz_minilm_recent is not None) and (cfg['w_minilm_recent'] > 0): parts.append(cfg['w_minilm_recent']*tz_minilm_recent); w_list.append(cfg['w_minilm_recent']); comp_logits.append(tz_minilm_recent)\n",
        "    if (tz_cat_recent is not None) and (cfg['w_cat_recent'] > 0): parts.append(cfg['w_cat_recent']*tz_cat_recent); w_list.append(cfg['w_cat_recent']); comp_logits.append(tz_cat_recent)\n",
        "    zt = np.sum(parts, axis=0)\n",
        "    pt = sigmoid(zt).astype(np.float32)\n",
        "    out_path = f'submission_reblend_with_recent_{tag}.csv'\n",
        "    pd.DataFrame({id_col: ids, target_col: pt}).to_csv(out_path, index=False)\n",
        "    # 15% shrink hedge\n",
        "    w_vec = np.array(w_list, dtype=np.float64)\n",
        "    w_eq = np.ones_like(w_vec)/len(w_vec)\n",
        "    alpha = 0.15\n",
        "    w_shr = ((1.0 - alpha)*w_vec + alpha*w_eq); w_shr = (w_shr / w_shr.sum()).astype(np.float64)\n",
        "    zt_shr = 0.0\n",
        "    for wi, zi in zip(w_shr, comp_logits):\n",
        "        zt_shr += wi*zi\n",
        "    pt_shr = sigmoid(zt_shr).astype(np.float32)\n",
        "    pd.DataFrame({id_col: ids, target_col: pt_shr}).to_csv(out_path.replace('.csv','_shrunk.csv'), index=False)\n",
        "    return out_path\n",
        "\n",
        "p_last2 = build_and_save('last2', cfg_last2)\n",
        "p_gam = build_and_save(f'gamma{best_gamma:.3f}'.replace('.', 'p'), best_cfg_g)\n",
        "\n",
        "primary = p_gam if (best_auc_g >= auc_last2) else p_last2\n",
        "pd.read_csv(primary).to_csv('submission.csv', index=False)\n",
        "print(f'Promoted {Path(primary).name} to submission.csv', flush=True)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV validated full: 2398/2878 | last2: 958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost selected: v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=5000 | best=0.64911 | elapsed=8.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  search done | tried=8445 | best=0.64911 | 15.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Last2] tried=8445 | best AUC=0.64911 | cfg={'g': 0.995, 'w_lr': 0.28, 'w_d1': 0.055999999999999994, 'w_d2': 0.024, 'w_meta': 0.18, 'w_emn': 0.168, 'w_emp': 0.11200000000000002, 'w_svd': 0.0, 'w_char': 0.08, 'w_cat': 0.1, 'w_lr_recent': 0.0, 'w_minilm_recent': 0.0, 'w_cat_recent': 0.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=5000 | best=0.68174 | elapsed=11.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  search done | tried=8445 | best=0.68174 | 19.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gamma 0.99] best AUC=0.68174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=5000 | best=0.68202 | elapsed=11.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  search done | tried=8445 | best=0.68202 | 19.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gamma 0.995] best AUC=0.68202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  tried=5000 | best=0.68213 | elapsed=11.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  search done | tried=8445 | best=0.68213 | 19.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gamma 0.997] best AUC=0.68213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gamma-best] gamma=0.997 | AUC=0.68213 | cfg={'g': 0.997, 'w_lr': 0.28, 'w_d1': 0.0, 'w_d2': 0.0, 'w_meta': 0.18, 'w_emn': 0.18, 'w_emp': 0.18, 'w_svd': 0.0, 'w_char': 0.08, 'w_cat': 0.1, 'w_lr_recent': 0.0, 'w_minilm_recent': 0.0, 'w_cat_recent': 0.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promoted submission_reblend_with_recent_gamma0p997.csv to submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "2d80237d-23a0-4bf2-96d2-2bfefbb604e1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S59: Last-block tuned nonnegative blend incl. super-recent components + isotonic calibration\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Time blocks and last-block indices\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "n = len(train)\n",
        "b5_idx = np.array(blocks[5])\n",
        "\n",
        "# Load full-history base OOF/test\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy');    t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "o_lr_ns = np.load('oof_lr_time_nosub_meta.npy');     t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy');            t_d1 = np.load('test_xgb_dense_time.npy')  # Dense v1\n",
        "o_meta = np.load('oof_xgb_meta_time.npy');           t_meta = np.load('test_xgb_meta_fullbag.npy') if Path('test_xgb_meta_fullbag.npy').exists() else np.load('test_xgb_meta_time.npy')\n",
        "o_emn = np.load('oof_xgb_emb_meta_time.npy');        t_emn = np.load('test_xgb_emb_minilm_fullbag.npy') if Path('test_xgb_emb_minilm_fullbag.npy').exists() else np.load('test_xgb_emb_meta_time.npy')\n",
        "o_emp = np.load('oof_xgb_emb_mpnet_time.npy');       t_emp = np.load('test_xgb_emb_mpnet_fullbag.npy') if Path('test_xgb_emb_mpnet_fullbag.npy').exists() else np.load('test_xgb_emb_mpnet_time.npy')\n",
        "has_char = Path('oof_lr_charwb_time.npy').exists() and Path('test_lr_charwb_time.npy').exists()\n",
        "if has_char:\n",
        "    o_char = np.load('oof_lr_charwb_time.npy'); t_char = np.load('test_lr_charwb_time.npy')\n",
        "else:\n",
        "    o_char = None; t_char = None\n",
        "has_svd = Path('oof_xgb_svd_word192_char128_meta.npy').exists() and Path('test_xgb_svd_word192_char128_meta.npy').exists()\n",
        "if has_svd:\n",
        "    o_svd = np.load('oof_xgb_svd_word192_char128_meta.npy'); t_svd = np.load('test_xgb_svd_word192_char128_meta.npy')\n",
        "else:\n",
        "    o_svd = None; t_svd = None\n",
        "\n",
        "# CatBoost: prefer v1 (historically better blend) but choose best available\n",
        "has_cat_v1 = Path('oof_catboost_textmeta.npy').exists() and Path('test_catboost_textmeta.npy').exists()\n",
        "has_cat_v2 = Path('oof_catboost_textmeta_v2.npy').exists() and Path('test_catboost_textmeta_v2.npy').exists()\n",
        "if has_cat_v1 and has_cat_v2:\n",
        "    o1 = np.load('oof_catboost_textmeta.npy'); o2 = np.load('oof_catboost_textmeta_v2.npy')\n",
        "    auc1 = roc_auc_score(y[b5_idx], o1[b5_idx]); auc2 = roc_auc_score(y[b5_idx], o2[b5_idx])\n",
        "    if auc1 >= auc2:\n",
        "        o_cat = o1; t_cat = np.load('test_catboost_textmeta.npy')\n",
        "    else:\n",
        "        o_cat = o2; t_cat = np.load('test_catboost_textmeta_v2.npy')\n",
        "elif has_cat_v1:\n",
        "    o_cat = np.load('oof_catboost_textmeta.npy'); t_cat = np.load('test_catboost_textmeta.npy')\n",
        "elif has_cat_v2:\n",
        "    o_cat = np.load('oof_catboost_textmeta_v2.npy'); t_cat = np.load('test_catboost_textmeta_v2.npy')\n",
        "else:\n",
        "    raise FileNotFoundError('No CatBoost OOF/test found')\n",
        "\n",
        "# Super-recent TEST preds (from S57). For last-block tuning, proxy their block-5 logits with corresponding full-history base logits.\n",
        "def load_avg_recent(base):\n",
        "    p35 = np.load(f'test_{base}_recent35.npy') if Path(f'test_{base}_recent35.npy').exists() else None\n",
        "    p45 = np.load(f'test_{base}_recent45.npy') if Path(f'test_{base}_recent45.npy').exists() else None\n",
        "    if (p35 is None) and (p45 is None):\n",
        "        return None\n",
        "    if p35 is None: return p45.astype(np.float32)\n",
        "    if p45 is None: return p35.astype(np.float32)\n",
        "    return ((p35 + p45) / 2.0).astype(np.float32)\n",
        "t_lr_recent = load_avg_recent('lr_nosub_meta')\n",
        "t_minilm_recent = load_avg_recent('xgb_minilm_meta')\n",
        "t_cat_recent = load_avg_recent('catboost_textmeta_v2')\n",
        "\n",
        "# Build logits\n",
        "z_lr_w, z_lr_ns = to_logit(o_lr_w), to_logit(o_lr_ns)\n",
        "z_d1, z_meta = to_logit(o_d1), to_logit(o_meta)\n",
        "z_emn, z_emp = to_logit(o_emn), to_logit(o_emp)\n",
        "z_char = to_logit(o_char) if o_char is not None else None\n",
        "z_svd = to_logit(o_svd) if o_svd is not None else None\n",
        "z_cat = to_logit(o_cat)\n",
        "tz_lr_w, tz_lr_ns = to_logit(t_lr_w), to_logit(t_lr_ns)\n",
        "tz_d1, tz_meta = to_logit(t_d1), to_logit(t_meta)\n",
        "tz_emn, tz_emp = to_logit(t_emn), to_logit(t_emp)\n",
        "tz_char = to_logit(t_char) if t_char is not None else None\n",
        "tz_svd = to_logit(t_svd) if t_svd is not None else None\n",
        "tz_cat = to_logit(t_cat)\n",
        "tz_lr_recent = to_logit(t_lr_recent) if t_lr_recent is not None else None\n",
        "tz_minilm_recent = to_logit(t_minilm_recent) if t_minilm_recent is not None else None\n",
        "tz_cat_recent = to_logit(t_cat_recent) if t_cat_recent is not None else None\n",
        "\n",
        "# Fix gamma for LR_mix per expert: 0.999\n",
        "g = 0.999\n",
        "z_lr_mix = (1.0 - g)*z_lr_w + g*z_lr_ns\n",
        "tz_lr_mix = (1.0 - g)*tz_lr_w + g*tz_lr_ns\n",
        "\n",
        "# Component dicts for convenience\n",
        "oof_cols = {'lr': z_lr_mix, 'dense': z_d1, 'meta': z_meta, 'emn': z_emn, 'emp': z_emp, 'cat': z_cat}\n",
        "test_cols = {'lr': tz_lr_mix, 'dense': tz_d1, 'meta': tz_meta, 'emn': tz_emn, 'emp': tz_emp, 'cat': tz_cat}\n",
        "if z_char is not None:\n",
        "    oof_cols['char'] = z_char; test_cols['char'] = tz_char\n",
        "if z_svd is not None:\n",
        "    oof_cols['svd'] = z_svd; test_cols['svd'] = tz_svd\n",
        "# Recent-only test columns\n",
        "if tz_lr_recent is not None: test_cols['lr_recent'] = tz_lr_recent\n",
        "if tz_minilm_recent is not None: test_cols['minilm_recent'] = tz_minilm_recent\n",
        "if tz_cat_recent is not None: test_cols['cat_recent'] = tz_cat_recent\n",
        "\n",
        "# Bounds and constraints (nonnegative, sum=1)\n",
        "bounds = {\n",
        "  'lr': (0.25, 0.35),\n",
        "  'cat': (0.15, 0.25),\n",
        "  'emn': (0.10, 0.30),\n",
        "  'emp': (0.10, 0.30),\n",
        "  'char': (0.04, 0.08) if 'char' in oof_cols else (0.0, 0.0),\n",
        "  'dense': (0.0, 0.10),\n",
        "  'meta': (0.16, 0.22),\n",
        "  'svd': (0.0, 0.08) if 'svd' in oof_cols else (0.0, 0.0),\n",
        "  'lr_recent': (0.06, 0.12) if 'lr_recent' in test_cols else (0.0, 0.0),\n",
        "  'minilm_recent': (0.06, 0.12) if 'minilm_recent' in test_cols else (0.0, 0.0),\n",
        "  'cat_recent': (0.06, 0.12) if 'cat_recent' in test_cols else (0.0, 0.0)\n",
        "}\n",
        "\n",
        "keys = [k for k in ['lr','cat','emn','emp','char','dense','meta','svd','lr_recent','minilm_recent','cat_recent'] if bounds[k][1] > 0 or k in ['lr','cat','emn','emp','dense','meta']]\n",
        "\n",
        "def sample_weights(rng: np.random.Generator):\n",
        "    w = {}\n",
        "    # sample core per bounds\n",
        "    for k in keys:\n",
        "        low, high = bounds[k]\n",
        "        val = rng.uniform(low, high) if high > low else low\n",
        "        w[k] = float(val)\n",
        "    # enforce embedding total within [0.30, 0.36]\n",
        "    emb_tot = w.get('emn',0.0) + w.get('emp',0.0)\n",
        "    if not (0.30 <= emb_tot <= 0.36):\n",
        "        scale = rng.uniform(0.30, 0.36) / max(emb_tot, 1e-6) if emb_tot > 0 else 0.33\n",
        "        w['emn'] *= scale; w['emp'] *= scale\n",
        "    # recent total >= 0.15 if any present\n",
        "    r_keys = [k for k in ['lr_recent','minilm_recent','cat_recent'] if bounds[k][1] > 0]\n",
        "    if r_keys:\n",
        "        r_tot = sum(w[k] for k in r_keys)\n",
        "        if r_tot < 0.15:\n",
        "            # bump proportionally up to 0.15\n",
        "            if r_tot > 0:\n",
        "                mul = 0.15 / r_tot\n",
        "                for k in r_keys: w[k] *= mul\n",
        "            else:\n",
        "                # distribute evenly\n",
        "                for k in r_keys: w[k] = 0.15 / len(r_keys)\n",
        "        # cap each at <= 0.15\n",
        "        for k in r_keys: w[k] = min(w[k], 0.15)\n",
        "    # normalize sum to 1 while maintaining nonnegativity\n",
        "    s = sum(w.values())\n",
        "    if s <= 0:\n",
        "        for k in keys: w[k] = 0.0\n",
        "        w['lr'] = 1.0\n",
        "        return w\n",
        "    for k in keys: w[k] /= s\n",
        "    # re-check lr and cat floors; if violated due to normalization, rescale minimally\n",
        "    def enforce_floor(name, floor):\n",
        "        if w.get(name,0.0) < floor:\n",
        "            deficit = floor - w.get(name,0.0)\n",
        "            # take from the largest buckets excluding this key\n",
        "            donors = sorted([(kk,vv) for kk,vv in w.items() if kk!=name and vv>0], key=lambda x: -x[1])\n",
        "            for kk,vv in donors:\n",
        "                take = min(deficit, max(0.0, vv - bounds[kk][0]))\n",
        "                if take>0:\n",
        "                    w[kk] -= take; w[name] += take; deficit -= take\n",
        "                if deficit <= 1e-9: break\n",
        "    enforce_floor('lr', bounds['lr'][0])\n",
        "    enforce_floor('cat', bounds['cat'][0])\n",
        "    return w\n",
        "\n",
        "def score_on_block5(w):\n",
        "    # Build blended logits on block 5 using OOF for full-history components; recent columns proxy with corresponding bases on block 5 (already included via full components), so ignore in objective.\n",
        "    z = (w.get('lr',0)*oof_cols['lr'] +\n",
        "         w.get('dense',0)*oof_cols['dense'] +\n",
        "         w.get('meta',0)*oof_cols['meta'] +\n",
        "         w.get('emn',0)*oof_cols['emn'] +\n",
        "         w.get('emp',0)*oof_cols['emp'] +\n",
        "         w.get('cat',0)*oof_cols['cat'])\n",
        "    if 'char' in oof_cols: z = z + w.get('char',0)*oof_cols['char']\n",
        "    if 'svd' in oof_cols: z = z + w.get('svd',0)*oof_cols['svd']\n",
        "    return roc_auc_score(y[b5_idx], z[b5_idx])\n",
        "\n",
        "rng = np.random.default_rng(1337)\n",
        "best_auc, best_w = -1.0, None\n",
        "n_iter = 12000\n",
        "t0 = time.time()\n",
        "for it in range(1, n_iter+1):\n",
        "    w = sample_weights(rng)\n",
        "    auc = score_on_block5(w)\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_w = auc, w.copy()\n",
        "    if it % 1000 == 0:\n",
        "        print(f'  iter={it} | best_auc_b5={best_auc:.5f} | elapsed={time.time()-t0:.1f}s', flush=True)\n",
        "print('Best block-5 AUC:', f'{best_auc:.5f}', '| weights:', best_w)\n",
        "\n",
        "# Build test blend (include recent-only TEST logits with tuned weights) and last-block blend probs for calibration\n",
        "def build_probs(w):\n",
        "    zt = (w.get('lr',0)*test_cols['lr'] +\n",
        "          w.get('dense',0)*test_cols['dense'] +\n",
        "          w.get('meta',0)*test_cols['meta'] +\n",
        "          w.get('emn',0)*test_cols['emn'] +\n",
        "          w.get('emp',0)*test_cols['emp'] +\n",
        "          w.get('cat',0)*test_cols['cat'])\n",
        "    if 'char' in test_cols: zt = zt + w.get('char',0)*test_cols['char']\n",
        "    if 'svd' in test_cols: zt = zt + w.get('svd',0)*test_cols['svd']\n",
        "    # add recent-only components on TEST if available\n",
        "    if 'lr_recent' in test_cols: zt = zt + w.get('lr_recent',0)*test_cols['lr_recent']\n",
        "    if 'minilm_recent' in test_cols: zt = zt + w.get('minilm_recent',0)*test_cols['minilm_recent']\n",
        "    if 'cat_recent' in test_cols: zt = zt + w.get('cat_recent',0)*test_cols['cat_recent']\n",
        "    pt = sigmoid(zt).astype(np.float32)\n",
        "    # also compute last-block probs for calibration (using OOF columns only)\n",
        "    zb5 = (w.get('lr',0)*oof_cols['lr'] +\n",
        "           w.get('dense',0)*oof_cols['dense'] +\n",
        "           w.get('meta',0)*oof_cols['meta'] +\n",
        "           w.get('emn',0)*oof_cols['emn'] +\n",
        "           w.get('emp',0)*oof_cols['emp'] +\n",
        "           w.get('cat',0)*oof_cols['cat'])\n",
        "    if 'char' in oof_cols: zb5 = zb5 + w.get('char',0)*oof_cols['char']\n",
        "    if 'svd' in oof_cols: zb5 = zb5 + w.get('svd',0)*oof_cols['svd']\n",
        "    pb5 = sigmoid(zb5[b5_idx]).astype(np.float32)\n",
        "    yb5 = y[b5_idx]\n",
        "    return pt, pb5, yb5\n",
        "\n",
        "pt_uncal, pb5, yb5 = build_probs(best_w)\n",
        "sub_uncal = pd.DataFrame({id_col: ids, target_col: pt_uncal})\n",
        "path_uncal = 'submission_lastblock_opt_uncalibrated.csv'\n",
        "sub_uncal.to_csv(path_uncal, index=False)\n",
        "print(f'Wrote {path_uncal} | mean={pt_uncal.mean():.6f}')\n",
        "\n",
        "# Isotonic calibration on block 5\n",
        "iso = IsotonicRegression(out_of_bounds='clip')\n",
        "iso.fit(pb5, yb5)\n",
        "pt_cal = iso.transform(pt_uncal).astype(np.float32)\n",
        "sub_cal = pd.DataFrame({id_col: ids, target_col: pt_cal})\n",
        "path_cal = 'submission_lastblock_opt_calibrated.csv'\n",
        "sub_cal.to_csv(path_cal, index=False)\n",
        "print(f'Wrote {path_cal} | mean={pt_cal.mean():.6f}')\n",
        "\n",
        "# Promote calibrated primary\n",
        "sub_cal.to_csv('submission.csv', index=False)\n",
        "print('Promoted submission_lastblock_opt_calibrated.csv to submission.csv')\n",
        "gc.collect();"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  iter=1000 | best_auc_b5=0.65075 | elapsed=1.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  iter=2000 | best_auc_b5=0.65075 | elapsed=3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  iter=3000 | best_auc_b5=0.65075 | elapsed=4.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  iter=4000 | best_auc_b5=0.65075 | elapsed=6.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  iter=5000 | best_auc_b5=0.65075 | elapsed=8.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  iter=6000 | best_auc_b5=0.65075 | elapsed=10.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  iter=7000 | best_auc_b5=0.65075 | elapsed=11.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  iter=8000 | best_auc_b5=0.65075 | elapsed=13.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  iter=9000 | best_auc_b5=0.65075 | elapsed=15.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  iter=10000 | best_auc_b5=0.65075 | elapsed=16.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  iter=11000 | best_auc_b5=0.65075 | elapsed=18.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  iter=12000 | best_auc_b5=0.65075 | elapsed=20.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best block-5 AUC: 0.65075 | weights: {'lr': 0.25, 'cat': 0.15770904415767784, 'emn': 0.16250498376207562, 'emp': 0.08801630044376484, 'char': 0.05024025578059258, 'dense': 0.003904651381461899, 'meta': 0.1152388717235414, 'svd': 0.00651242641181899, 'lr_recent': 0.05349244679248755, 'minilm_recent': 0.06360229005438262, 'cat_recent': 0.048778729492196686}\nWrote submission_lastblock_opt_uncalibrated.csv | mean=0.353231\nWrote submission_lastblock_opt_calibrated.csv | mean=0.129195\nPromoted submission_lastblock_opt_calibrated.csv to submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "a64927e4-acf8-42d5-aab2-4834161efe84",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S60: Per-model recent interpolation (alphas) with gamma=0.999; build A/B variants + shrink hedges + rank-avg\n",
        "import numpy as np, pandas as pd, time\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Time masks for OOF reporting\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "n = len(train)\n",
        "mask_valid = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    mask_valid[np.array(blocks[i])] = True\n",
        "# gamma weights for reporting (0.999 per expert)\n",
        "gamma = 0.999\n",
        "w_oof = np.zeros(n, dtype=np.float64)\n",
        "for bi in range(1, k):\n",
        "    age = (k - 1) - bi\n",
        "    w_oof[np.array(blocks[bi])] = (gamma ** age)\n",
        "\n",
        "# Load full-history OOF/test\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy');    t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "o_lr_ns = np.load('oof_lr_time_nosub_meta.npy');     t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy');            t_d1 = np.load('test_xgb_dense_time.npy')  # Dense v1 only\n",
        "o_meta = np.load('oof_xgb_meta_time.npy');           t_meta = np.load('test_xgb_meta_fullbag.npy') if Path('test_xgb_meta_fullbag.npy').exists() else np.load('test_xgb_meta_time.npy')\n",
        "o_emn = np.load('oof_xgb_emb_meta_time.npy');        t_emn = np.load('test_xgb_emb_minilm_fullbag.npy') if Path('test_xgb_emb_minilm_fullbag.npy').exists() else np.load('test_xgb_emb_meta_time.npy')\n",
        "o_emp = np.load('oof_xgb_emb_mpnet_time.npy');       t_emp = np.load('test_xgb_emb_mpnet_fullbag.npy') if Path('test_xgb_emb_mpnet_fullbag.npy').exists() else np.load('test_xgb_emb_mpnet_time.npy')\n",
        "has_char = Path('oof_lr_charwb_time.npy').exists() and Path('test_lr_charwb_time.npy').exists()\n",
        "if has_char:\n",
        "    o_char = np.load('oof_lr_charwb_time.npy'); t_char = np.load('test_lr_charwb_time.npy')\n",
        "else:\n",
        "    o_char = None; t_char = None\n",
        "\n",
        "# CatBoost: prefer v1 per blend performance\n",
        "has_cat_v1 = Path('oof_catboost_textmeta.npy').exists() and Path('test_catboost_textmeta.npy').exists()\n",
        "has_cat_v2 = Path('oof_catboost_textmeta_v2.npy').exists() and Path('test_catboost_textmeta_v2.npy').exists()\n",
        "if has_cat_v1 and has_cat_v2:\n",
        "    o1 = np.load('oof_catboost_textmeta.npy'); o2 = np.load('oof_catboost_textmeta_v2.npy')\n",
        "    auc1 = roc_auc_score(y[mask_valid], o1[mask_valid]); auc2 = roc_auc_score(y[mask_valid], o2[mask_valid])\n",
        "    if auc1 >= auc2:\n",
        "        o_cat = o1; t_cat = np.load('test_catboost_textmeta.npy'); cat_ver = 'v1'\n",
        "    else:\n",
        "        o_cat = o2; t_cat = np.load('test_catboost_textmeta_v2.npy'); cat_ver = 'v2'\n",
        "elif has_cat_v1:\n",
        "    o_cat = np.load('oof_catboost_textmeta.npy'); t_cat = np.load('test_catboost_textmeta.npy'); cat_ver = 'v1'\n",
        "elif has_cat_v2:\n",
        "    o_cat = np.load('oof_catboost_textmeta_v2.npy'); t_cat = np.load('test_catboost_textmeta_v2.npy'); cat_ver = 'v2'\n",
        "else:\n",
        "    raise FileNotFoundError('No CatBoost OOF/test found')\n",
        "print('CatBoost base:', cat_ver)\n",
        "\n",
        "# Recent TEST-only preds\n",
        "def load_avg_recent(base):\n",
        "    p35 = np.load(f'test_{base}_recent35.npy') if Path(f'test_{base}_recent35.npy').exists() else None\n",
        "    p45 = np.load(f'test_{base}_recent45.npy') if Path(f'test_{base}_recent45.npy').exists() else None\n",
        "    if (p35 is None) and (p45 is None):\n",
        "        return None\n",
        "    if p35 is None: return p45.astype(np.float32)\n",
        "    if p45 is None: return p35.astype(np.float32)\n",
        "    return ((p35 + p45) / 2.0).astype(np.float32)\n",
        "t_lr_recent = load_avg_recent('lr_nosub_meta')\n",
        "t_minilm_recent = load_avg_recent('xgb_minilm_meta')\n",
        "t_cat_recent = load_avg_recent('catboost_textmeta_v2')  # recent available for v2\n",
        "\n",
        "# Convert to logits\n",
        "z_lr_w, z_lr_ns = to_logit(o_lr_w), to_logit(o_lr_ns)\n",
        "z_d1, z_meta = to_logit(o_d1), to_logit(o_meta)\n",
        "z_emn, z_emp = to_logit(o_emn), to_logit(o_emp)\n",
        "z_char = to_logit(o_char) if o_char is not None else None\n",
        "z_cat = to_logit(o_cat)\n",
        "tz_lr_w, tz_lr_ns = to_logit(t_lr_w), to_logit(t_lr_ns)\n",
        "tz_d1, tz_meta = to_logit(t_d1), to_logit(t_meta)\n",
        "tz_emn, tz_emp = to_logit(t_emn), to_logit(t_emp)\n",
        "tz_char = to_logit(t_char) if t_char is not None else None\n",
        "tz_cat = to_logit(t_cat)\n",
        "tz_lr_recent = to_logit(t_lr_recent) if t_lr_recent is not None else None\n",
        "tz_minilm_recent = to_logit(t_minilm_recent) if t_minilm_recent is not None else None\n",
        "tz_cat_recent = to_logit(t_cat_recent) if t_cat_recent is not None else None\n",
        "\n",
        "# LR mix with gamma=0.999\n",
        "g_lr = 0.999\n",
        "z_lr_mix = (1.0 - g_lr)*z_lr_w + g_lr*z_lr_ns\n",
        "tz_lr_mix = (1.0 - g_lr)*tz_lr_w + g_lr*tz_lr_ns\n",
        "\n",
        "def build_submission(tag, weights, alphas):\n",
        "    # weights dict keys: lr, cat, emn, emp, char, dense, meta\n",
        "    # alphas dict keys: lr, minilm, cat (interpolation factors applied on TEST only)\n",
        "    # OOF blend (full-history only; for reporting)\n",
        "    z_oof = (weights['lr']*z_lr_mix +\n",
        "             weights['dense']*z_d1 +\n",
        "             weights['meta']*z_meta +\n",
        "             weights['emn']*z_emn +\n",
        "             weights['emp']*z_emp +\n",
        "             weights['cat']*z_cat)\n",
        "    if (z_char is not None) and (weights.get('char',0) > 0):\n",
        "        z_oof = z_oof + weights['char']*z_char\n",
        "    auc_g = roc_auc_score(y[mask_valid], z_oof[mask_valid], sample_weight=w_oof[mask_valid])\n",
        "    print(f'[{tag}] gamma-weighted OOF(z) AUC={auc_g:.5f}')\n",
        "    # TEST blend with per-model interpolation to recent\n",
        "    tz_lr_interp = tz_lr_mix if tz_lr_recent is None else ((1.0 - alphas.get('lr',0.0))*tz_lr_mix + alphas.get('lr',0.0)*tz_lr_recent)\n",
        "    tz_minilm_interp = tz_emn if tz_minilm_recent is None else ((1.0 - alphas.get('minilm',0.0))*tz_emn + alphas.get('minilm',0.0)*tz_minilm_recent)\n",
        "    tz_cat_interp = tz_cat if tz_cat_recent is None else ((1.0 - alphas.get('cat',0.0))*tz_cat + alphas.get('cat',0.0)*tz_cat_recent)\n",
        "    parts = [\n",
        "        weights['lr']*tz_lr_interp,\n",
        "        weights['dense']*tz_d1,\n",
        "        weights['meta']*tz_meta,\n",
        "        weights['emn']*tz_minilm_interp,\n",
        "        weights['emp']*tz_emp,\n",
        "        weights['cat']*tz_cat_interp\n",
        "    ]\n",
        "    if (tz_char is not None) and (weights.get('char',0) > 0):\n",
        "        parts.append(weights['char']*tz_char)\n",
        "    zt = np.sum(parts, axis=0)\n",
        "    pt = sigmoid(zt).astype(np.float32)\n",
        "    out_path = f'submission_interp_{tag}.csv'\n",
        "    pd.DataFrame({id_col: ids, target_col: pt}).to_csv(out_path, index=False)\n",
        "    # 15% shrink-to-equal hedge\n",
        "    comp_logits = [tz_lr_interp, tz_d1, tz_meta, tz_minilm_interp, tz_emp, tz_cat_interp] + ([tz_char] if (tz_char is not None and weights.get('char',0)>0) else [])\n",
        "    w_list = [weights['lr'], weights['dense'], weights['meta'], weights['emn'], weights['emp'], weights['cat']] + ([weights['char']] if (tz_char is not None and weights.get('char',0)>0) else [])\n",
        "    w_vec = np.asarray(w_list, dtype=np.float64)\n",
        "    w_eq = np.ones_like(w_vec)/len(w_vec)\n",
        "    alpha = 0.15\n",
        "    w_shr = ((1.0 - alpha)*w_vec + alpha*w_eq); w_shr = (w_shr / w_shr.sum()).astype(np.float64)\n",
        "    zt_shr = np.zeros_like(comp_logits[0], dtype=np.float64)\n",
        "    for wi, zi in zip(w_shr, comp_logits):\n",
        "        zt_shr += wi*zi\n",
        "    pt_shr = sigmoid(zt_shr).astype(np.float32)\n",
        "    out_shr = out_path.replace('.csv','_shrunk.csv')\n",
        "    pd.DataFrame({id_col: ids, target_col: pt_shr}).to_csv(out_shr, index=False)\n",
        "    print(f'Wrote {out_path} (+_shrunk) | mean={pt.mean():.6f}')\n",
        "    return out_path, out_shr, auc_g\n",
        "\n",
        "# Variant A (expert guidance)\n",
        "weights_A = dict(lr=0.30, cat=0.20, emn=0.17, emp=0.17, char=0.06 if z_char is not None else 0.0, dense=0.10, meta=0.20)\n",
        "alphas_A = dict(lr=0.20, minilm=0.20, cat=0.20)\n",
        "pA, pA_shr, aucA = build_submission('gamma999_interp_A', weights_A, alphas_A)\n",
        "\n",
        "# Variant B (slightly higher Cat/emb, lower dense)\n",
        "weights_B = dict(lr=0.28, cat=0.22, emn=0.20, emp=0.16, char=0.06 if z_char is not None else 0.0, dense=0.05, meta=0.18)\n",
        "alphas_B = dict(lr=0.30, minilm=0.20, cat=0.30)\n",
        "pB, pB_shr, aucB = build_submission('gamma999_interp_B', weights_B, alphas_B)\n",
        "\n",
        "# Rank-average A and B primary as hedge\n",
        "def read_probs(path):\n",
        "    return pd.read_csv(path)[target_col].values.astype(np.float64)\n",
        "def rank01(x):\n",
        "    order = np.argsort(x, kind='mergesort')\n",
        "    ranks = np.empty_like(order, dtype=np.float64)\n",
        "    ranks[order] = np.arange(len(x), dtype=np.float64)\n",
        "    return ranks / max(len(x) - 1, 1)\n",
        "pa = read_probs(pA); pb = read_probs(pB)\n",
        "ra = rank01(pa); rb = rank01(pb)\n",
        "ravg = (ra + rb) / 2.0\n",
        "sub_rank = 'submission_interp_rankavg_AB.csv'\n",
        "pd.DataFrame({id_col: ids, target_col: ravg.astype(np.float32)}).to_csv(sub_rank, index=False)\n",
        "print('Wrote', sub_rank, '| mean=', f'{ravg.mean():.6f}')\n",
        "\n",
        "# Promote rank-avg hedge\n",
        "pd.read_csv(sub_rank).to_csv('submission.csv', index=False)\n",
        "print('Promoted', sub_rank, 'to submission.csv')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost base: v1\n[gamma999_interp_A] gamma-weighted OOF(z) AUC=0.68226\nWrote submission_interp_gamma999_interp_A.csv (+_shrunk) | mean=0.349913\n[gamma999_interp_B] gamma-weighted OOF(z) AUC=0.68176\nWrote submission_interp_gamma999_interp_B.csv (+_shrunk) | mean=0.344665\nWrote submission_interp_rankavg_AB.csv | mean= 0.500000\nPromoted submission_interp_rankavg_AB.csv to submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "3f8ef364-bb95-4d05-bdaf-79a4049a499d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S61: Expanded recent-interpolation variants (higher recency + Cat weight), select top-3 by gamma-OOF and rank-average\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Time masks and gamma weights (0.999)\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "n = len(train)\n",
        "mask_valid = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    mask_valid[np.array(blocks[i])] = True\n",
        "gamma = 0.999\n",
        "w_oof = np.zeros(n, dtype=np.float64)\n",
        "for bi in range(1, k):\n",
        "    age = (k - 1) - bi\n",
        "    w_oof[np.array(blocks[bi])] = (gamma ** age)\n",
        "\n",
        "# Load OOF/test bases\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy');    t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "o_lr_ns = np.load('oof_lr_time_nosub_meta.npy');     t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy');            t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "o_meta = np.load('oof_xgb_meta_time.npy');           t_meta = np.load('test_xgb_meta_fullbag.npy') if Path('test_xgb_meta_fullbag.npy').exists() else np.load('test_xgb_meta_time.npy')\n",
        "o_emn = np.load('oof_xgb_emb_meta_time.npy');        t_emn = np.load('test_xgb_emb_minilm_fullbag.npy') if Path('test_xgb_emb_minilm_fullbag.npy').exists() else np.load('test_xgb_emb_meta_time.npy')\n",
        "o_emp = np.load('oof_xgb_emb_mpnet_time.npy');       t_emp = np.load('test_xgb_emb_mpnet_fullbag.npy') if Path('test_xgb_emb_mpnet_fullbag.npy').exists() else np.load('test_xgb_emb_mpnet_time.npy')\n",
        "has_char = Path('oof_lr_charwb_time.npy').exists() and Path('test_lr_charwb_time.npy').exists()\n",
        "if has_char:\n",
        "    o_char = np.load('oof_lr_charwb_time.npy'); t_char = np.load('test_lr_charwb_time.npy')\n",
        "else:\n",
        "    o_char = None; t_char = None\n",
        "\n",
        "# CatBoost: prefer v1\n",
        "has_cat_v1 = Path('oof_catboost_textmeta.npy').exists() and Path('test_catboost_textmeta.npy').exists()\n",
        "has_cat_v2 = Path('oof_catboost_textmeta_v2.npy').exists() and Path('test_catboost_textmeta_v2.npy').exists()\n",
        "if has_cat_v1 and has_cat_v2:\n",
        "    o1 = np.load('oof_catboost_textmeta.npy'); o2 = np.load('oof_catboost_textmeta_v2.npy')\n",
        "    auc1 = roc_auc_score(y[mask_valid], o1[mask_valid]); auc2 = roc_auc_score(y[mask_valid], o2[mask_valid])\n",
        "    if auc1 >= auc2:\n",
        "        o_cat = o1; t_cat = np.load('test_catboost_textmeta.npy')\n",
        "    else:\n",
        "        o_cat = o2; t_cat = np.load('test_catboost_textmeta_v2.npy')\n",
        "elif has_cat_v1:\n",
        "    o_cat = np.load('oof_catboost_textmeta.npy'); t_cat = np.load('test_catboost_textmeta.npy')\n",
        "elif has_cat_v2:\n",
        "    o_cat = np.load('oof_catboost_textmeta_v2.npy'); t_cat = np.load('test_catboost_textmeta_v2.npy')\n",
        "else:\n",
        "    raise FileNotFoundError('No CatBoost OOF/test found')\n",
        "\n",
        "# Recent TEST-only preds\n",
        "def load_avg_recent(base):\n",
        "    p35 = np.load(f'test_{base}_recent35.npy') if Path(f'test_{base}_recent35.npy').exists() else None\n",
        "    p45 = np.load(f'test_{base}_recent45.npy') if Path(f'test_{base}_recent45.npy').exists() else None\n",
        "    if (p35 is None) and (p45 is None): return None\n",
        "    if p35 is None: return p45.astype(np.float32)\n",
        "    if p45 is None: return p35.astype(np.float32)\n",
        "    return ((p35 + p45) / 2.0).astype(np.float32)\n",
        "t_lr_recent = load_avg_recent('lr_nosub_meta')\n",
        "t_minilm_recent = load_avg_recent('xgb_minilm_meta')\n",
        "t_cat_recent = load_avg_recent('catboost_textmeta_v2')\n",
        "\n",
        "# Convert to logits\n",
        "z_lr_w, z_lr_ns = to_logit(o_lr_w), to_logit(o_lr_ns)\n",
        "z_d1, z_meta = to_logit(o_d1), to_logit(o_meta)\n",
        "z_emn, z_emp = to_logit(o_emn), to_logit(o_emp)\n",
        "z_char = to_logit(o_char) if o_char is not None else None\n",
        "z_cat = to_logit(o_cat)\n",
        "tz_lr_w, tz_lr_ns = to_logit(t_lr_w), to_logit(t_lr_ns)\n",
        "tz_d1, tz_meta = to_logit(t_d1), to_logit(t_meta)\n",
        "tz_emn, tz_emp = to_logit(t_emn), to_logit(t_emp)\n",
        "tz_char = to_logit(t_char) if t_char is not None else None\n",
        "tz_cat = to_logit(t_cat)\n",
        "tz_lr_recent = to_logit(t_lr_recent) if t_lr_recent is not None else None\n",
        "tz_minilm_recent = to_logit(t_minilm_recent) if t_minilm_recent is not None else None\n",
        "tz_cat_recent = to_logit(t_cat_recent) if t_cat_recent is not None else None\n",
        "\n",
        "# LR mix with gamma=0.999\n",
        "g_lr = 0.999\n",
        "z_lr_mix = (1.0 - g_lr)*z_lr_w + g_lr*z_lr_ns\n",
        "tz_lr_mix = (1.0 - g_lr)*tz_lr_w + g_lr*tz_lr_ns\n",
        "\n",
        "def build_variant(tag, weights, alphas):\n",
        "    z_oof = (weights['lr']*z_lr_mix +\n",
        "             weights['dense']*z_d1 +\n",
        "             weights['meta']*z_meta +\n",
        "             weights['emn']*z_emn +\n",
        "             weights['emp']*z_emp +\n",
        "             weights['cat']*z_cat)\n",
        "    if (z_char is not None) and (weights.get('char',0) > 0):\n",
        "        z_oof = z_oof + weights['char']*z_char\n",
        "    auc_g = roc_auc_score(y[mask_valid], z_oof[mask_valid], sample_weight=w_oof[mask_valid])\n",
        "    tz_lr_interp = tz_lr_mix if tz_lr_recent is None else ((1.0 - alphas.get('lr',0.0))*tz_lr_mix + alphas.get('lr',0.0)*tz_lr_recent)\n",
        "    tz_minilm_interp = tz_emn if tz_minilm_recent is None else ((1.0 - alphas.get('minilm',0.0))*tz_emn + alphas.get('minilm',0.0)*tz_minilm_recent)\n",
        "    tz_cat_interp = tz_cat if tz_cat_recent is None else ((1.0 - alphas.get('cat',0.0))*tz_cat + alphas.get('cat',0.0)*tz_cat_recent)\n",
        "    parts = [weights['lr']*tz_lr_interp, weights['dense']*tz_d1, weights['meta']*tz_meta, weights['emn']*tz_minilm_interp, weights['emp']*tz_emp, weights['cat']*tz_cat_interp]\n",
        "    if (tz_char is not None) and (weights.get('char',0) > 0): parts.append(weights['char']*tz_char)\n",
        "    zt = np.sum(parts, axis=0)\n",
        "    pt = sigmoid(zt).astype(np.float32)\n",
        "    out_path = f'submission_interp_{tag}.csv'\n",
        "    pd.DataFrame({id_col: ids, target_col: pt}).to_csv(out_path, index=False)\n",
        "    return auc_g, out_path\n",
        "\n",
        "def renorm(weights):\n",
        "    s = sum(weights.values())\n",
        "    return {k: (v/s) for k,v in weights.items()} if s>0 else weights\n",
        "\n",
        "# Define stronger-recency variants (C/D/E) within expert ranges, then renormalize\n",
        "# C: higher LR, Cat, moderate emb, small dense, keep meta\n",
        "wC = renorm(dict(lr=0.32, cat=0.24, emn=0.18, emp=0.16, char=(0.06 if has_char else 0.0), dense=0.04, meta=0.20))\n",
        "aC = dict(lr=0.30, minilm=0.30, cat=0.30)\n",
        "# D: drop dense, push Cat and embeddings up\n",
        "wD = renorm(dict(lr=0.30, cat=0.26, emn=0.20, emp=0.18, char=(0.06 if has_char else 0.0), dense=0.00, meta=0.10))\n",
        "aD = dict(lr=0.30, minilm=0.30, cat=0.35)\n",
        "# E: max embeddings total ~0.36, cat 0.22, lr 0.30, meta 0.12, small dense\n",
        "wE = renorm(dict(lr=0.30, cat=0.22, emn=0.20, emp=0.16, char=(0.08 if has_char else 0.0), dense=0.02, meta=0.12))\n",
        "aE = dict(lr=0.40, minilm=0.30, cat=0.30)\n",
        "\n",
        "cands = []\n",
        "for tag, w, a in [\n",
        "    ('gamma999_interp_C', wC, aC),\n",
        "    ('gamma999_interp_D', wD, aD),\n",
        "    ('gamma999_interp_E', wE, aE),\n",
        "]:\n",
        "    auc, path = build_variant(tag, w, a)\n",
        "    print(f'[{tag}] gamma-weighted OOF(z) AUC={auc:.5f}')\n",
        "    cands.append((auc, path))\n",
        "\n",
        "# Add previous A/B for consideration if exist\n",
        "for tag in ['gamma999_interp_A','gamma999_interp_B']:\n",
        "    p = f'submission_interp_{tag}.csv'\n",
        "    if Path(p).exists():\n",
        "        # Recompute AUC using the same weights (approx using stored OOF from earlier run isn't trivial); just include as hedge without AUC sort benefit\n",
        "        cands.append((-1.0, p))\n",
        "\n",
        "# Select top-3 by AUC (valid ones), fill with others if needed\n",
        "valid = sorted([x for x in cands if x[0] >= 0], key=lambda x: -x[0])\n",
        "paths = [p for _, p in valid[:3]]\n",
        "if len(paths) < 3:\n",
        "    extra = [p for _, p in cands if p not in paths]\n",
        "    for p in extra:\n",
        "        if p not in paths:\n",
        "            paths.append(p)\n",
        "        if len(paths) >= 3: break\n",
        "print('Chosen for rank-avg:', paths)\n",
        "\n",
        "def read_probs(path):\n",
        "    return pd.read_csv(path)[target_col].values.astype(np.float64)\n",
        "def rank01(x):\n",
        "    order = np.argsort(x, kind='mergesort')\n",
        "    ranks = np.empty_like(order, dtype=np.float64)\n",
        "    ranks[order] = np.arange(len(x), dtype=np.float64)\n",
        "    return ranks / max(len(x) - 1, 1)\n",
        "\n",
        "R = []\n",
        "for p in paths:\n",
        "    R.append(rank01(read_probs(p)))\n",
        "ravg = np.mean(np.vstack(R), axis=0)\n",
        "sub_rank = 'submission_interp_rankavg_top3_expanded.csv'\n",
        "pd.DataFrame({id_col: ids, target_col: ravg.astype(np.float32)}).to_csv(sub_rank, index=False)\n",
        "print('Wrote', sub_rank, '| mean=', f'{ravg.mean():.6f}')\n",
        "\n",
        "# Promote rank-avg hedge\n",
        "pd.read_csv(sub_rank).to_csv('submission.csv', index=False)\n",
        "print('Promoted', sub_rank, 'to submission.csv')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[gamma999_interp_C] gamma-weighted OOF(z) AUC=0.68157\n[gamma999_interp_D] gamma-weighted OOF(z) AUC=0.68067\n[gamma999_interp_E] gamma-weighted OOF(z) AUC=0.68109\nChosen for rank-avg: ['submission_interp_gamma999_interp_C.csv', 'submission_interp_gamma999_interp_E.csv', 'submission_interp_gamma999_interp_D.csv']\nWrote submission_interp_rankavg_top3_expanded.csv | mean= 0.500000\nPromoted submission_interp_rankavg_top3_expanded.csv to submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "0caa93bb-f249-41d1-9f93-8de787df9c39",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S62: Block-5 optimizer with recent-only components (r24, r30, gamma9995+blk5x2) + logit-average hedge (no calibration)\n",
        "import numpy as np, pandas as pd, time\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Time blocks and masks\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "n = len(train)\n",
        "mask_valid = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    mask_valid[np.array(blocks[i])] = True\n",
        "b5_idx = np.array(blocks[5])\n",
        "\n",
        "# Load full-history OOF/test\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy');    t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "o_lr_ns = np.load('oof_lr_time_nosub_meta.npy');     t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy');            t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "o_meta = np.load('oof_xgb_meta_time.npy');           t_meta = np.load('test_xgb_meta_fullbag.npy') if Path('test_xgb_meta_fullbag.npy').exists() else np.load('test_xgb_meta_time.npy')\n",
        "o_emn = np.load('oof_xgb_emb_meta_time.npy');        t_emn = np.load('test_xgb_emb_minilm_fullbag.npy') if Path('test_xgb_emb_minilm_fullbag.npy').exists() else np.load('test_xgb_emb_meta_time.npy')\n",
        "o_emp = np.load('oof_xgb_emb_mpnet_time.npy');       t_emp = np.load('test_xgb_emb_mpnet_fullbag.npy') if Path('test_xgb_emb_mpnet_fullbag.npy').exists() else np.load('test_xgb_emb_mpnet_time.npy')\n",
        "has_char = Path('oof_lr_charwb_time.npy').exists() and Path('test_lr_charwb_time.npy').exists()\n",
        "if has_char:\n",
        "    o_char = np.load('oof_lr_charwb_time.npy'); t_char = np.load('test_lr_charwb_time.npy')\n",
        "else:\n",
        "    o_char = None; t_char = None\n",
        "# SVD dual disabled per bounds\n",
        "\n",
        "# CatBoost: prefer v1\n",
        "has_cat_v1 = Path('oof_catboost_textmeta.npy').exists() and Path('test_catboost_textmeta.npy').exists()\n",
        "has_cat_v2 = Path('oof_catboost_textmeta_v2.npy').exists() and Path('test_catboost_textmeta_v2.npy').exists()\n",
        "if has_cat_v1 and has_cat_v2:\n",
        "    o1 = np.load('oof_catboost_textmeta.npy'); o2 = np.load('oof_catboost_textmeta_v2.npy')\n",
        "    auc1 = roc_auc_score(y[b5_idx], o1[b5_idx]); auc2 = roc_auc_score(y[b5_idx], o2[b5_idx])\n",
        "    if auc1 >= auc2:\n",
        "        o_cat = o1; t_cat = np.load('test_catboost_textmeta.npy')\n",
        "    else:\n",
        "        o_cat = o2; t_cat = np.load('test_catboost_textmeta_v2.npy')\n",
        "elif has_cat_v1:\n",
        "    o_cat = np.load('oof_catboost_textmeta.npy'); t_cat = np.load('test_catboost_textmeta.npy')\n",
        "elif has_cat_v2:\n",
        "    o_cat = np.load('oof_catboost_textmeta_v2.npy'); t_cat = np.load('test_catboost_textmeta_v2.npy')\n",
        "else:\n",
        "    raise FileNotFoundError('No CatBoost OOF/test found')\n",
        "\n",
        "# Recent TEST-only preds (from S57)\n",
        "def load_avg_recent(base):\n",
        "    p35 = np.load(f'test_{base}_recent35.npy') if Path(f'test_{base}_recent35.npy').exists() else None\n",
        "    p45 = np.load(f'test_{base}_recent45.npy') if Path(f'test_{base}_recent45.npy').exists() else None\n",
        "    if (p35 is None) and (p45 is None): return None\n",
        "    if p35 is None: return p45.astype(np.float32)\n",
        "    if p45 is None: return p35.astype(np.float32)\n",
        "    return ((p35 + p45) / 2.0).astype(np.float32)\n",
        "t_lr_recent = load_avg_recent('lr_nosub_meta')\n",
        "t_minilm_recent = load_avg_recent('xgb_minilm_meta')\n",
        "t_cat_recent = load_avg_recent('catboost_textmeta_v2')\n",
        "\n",
        "# Convert to logits\n",
        "z_lr_w, z_lr_ns = to_logit(o_lr_w), to_logit(o_lr_ns)\n",
        "z_d1, z_meta = to_logit(o_d1), to_logit(o_meta)\n",
        "z_emn, z_emp = to_logit(o_emn), to_logit(o_emp)\n",
        "z_char = to_logit(o_char) if o_char is not None else None\n",
        "z_cat = to_logit(o_cat)\n",
        "tz_lr_w, tz_lr_ns = to_logit(t_lr_w), to_logit(t_lr_ns)\n",
        "tz_d1, tz_meta = to_logit(t_d1), to_logit(t_meta)\n",
        "tz_emn, tz_emp = to_logit(t_emn), to_logit(t_emp)\n",
        "tz_char = to_logit(t_char) if t_char is not None else None\n",
        "tz_cat = to_logit(t_cat)\n",
        "tz_lr_recent = to_logit(t_lr_recent) if t_lr_recent is not None else None\n",
        "tz_minilm_recent = to_logit(t_minilm_recent) if t_minilm_recent is not None else None\n",
        "tz_cat_recent = to_logit(t_cat_recent) if t_cat_recent is not None else None\n",
        "\n",
        "# Define LR mix with g_lr (use 0.9995 as expert recency suggestion)\n",
        "g_lr = 0.9995\n",
        "z_lr_mix = (1.0 - g_lr)*z_lr_w + g_lr*z_lr_ns\n",
        "tz_lr_mix = (1.0 - g_lr)*tz_lr_w + g_lr*tz_lr_ns\n",
        "\n",
        "# Core components (OOF/test logits) - no SVD\n",
        "oof_cols = {'lr': z_lr_mix, 'dense': z_d1, 'meta': z_meta, 'emn': z_emn, 'emp': z_emp, 'cat': z_cat}\n",
        "test_cols = {'lr': tz_lr_mix, 'dense': tz_d1, 'meta': tz_meta, 'emn': tz_emn, 'emp': tz_emp, 'cat': tz_cat}\n",
        "if z_char is not None:\n",
        "    oof_cols['char'] = z_char; test_cols['char'] = tz_char\n",
        "\n",
        "# Bounds\n",
        "core_bounds = {\n",
        "  'lr': (0.30, 0.36),\n",
        "  'cat': (0.18, 0.26),\n",
        "  'meta': (0.16, 0.22),\n",
        "  'dense': (0.0, 0.06),\n",
        "  'char': (0.04, 0.08) if 'char' in oof_cols else (0.0, 0.0),\n",
        "  # emn+emp total in [0.30,0.36], split in {(0.6,0.4),(0.5,0.5)}\n",
        "}\n",
        "emb_splits = [(0.6,0.4), (0.5,0.5)]\n",
        "\n",
        "recent_bounds = {\n",
        "  'lr_recent': (0.08, 0.15) if tz_lr_recent is not None else (0.0, 0.0),\n",
        "  'minilm_recent': (0.08, 0.15) if tz_minilm_recent is not None else (0.0, 0.0),\n",
        "  'cat_recent': (0.08, 0.15) if tz_cat_recent is not None else (0.0, 0.0),\n",
        "}\n",
        "\n",
        "def sample_core_weights(rng: np.random.Generator, emb_total_low=0.30, emb_total_high=0.36):\n",
        "    # sample core per bounds\n",
        "    w = {}\n",
        "    for k,(lo,hi) in core_bounds.items():\n",
        "        val = rng.uniform(lo, hi) if hi > lo else lo\n",
        "        w[k] = float(val)\n",
        "    # sample embedding total and split\n",
        "    emb_tot = rng.uniform(emb_total_low, emb_total_high)\n",
        "    split = emb_splits[rng.integers(0, len(emb_splits))]\n",
        "    w['emn'] = emb_tot * split[0]\n",
        "    w['emp'] = emb_tot * split[1]\n",
        "    return w\n",
        "\n",
        "def renorm_core(w_core: dict, core_sum_target: float):\n",
        "    keys = ['lr','cat','meta','dense','char'] + ['emn','emp']\n",
        "    s = sum(w_core.get(k,0.0) for k in keys)\n",
        "    if s <= 0:\n",
        "        return {k:(0.0) for k in keys}\n",
        "    scale = core_sum_target / s\n",
        "    for k in keys:\n",
        "        w_core[k] = w_core.get(k,0.0) * scale\n",
        "    # floors on lr/cat already handled by initial sampling and scale preserves ratios\n",
        "    return w_core\n",
        "\n",
        "def score_block5(w_core: dict):\n",
        "    z = (w_core.get('lr',0)*oof_cols['lr'] +\n",
        "         w_core.get('dense',0)*oof_cols['dense'] +\n",
        "         w_core.get('meta',0)*oof_cols['meta'] +\n",
        "         w_core.get('emn',0)*oof_cols['emn'] +\n",
        "         w_core.get('emp',0)*oof_cols['emp'] +\n",
        "         w_core.get('cat',0)*oof_cols['cat'])\n",
        "    if 'char' in oof_cols: z = z + w_core.get('char',0)*oof_cols['char']\n",
        "    return roc_auc_score(y[b5_idx], z[b5_idx])\n",
        "\n",
        "def score_gamma9995_blk5x2(w_core: dict):\n",
        "    # gamma=0.9995 over blocks 1..5, with 2x weight for block 5\n",
        "    weights = np.zeros(n, dtype=np.float64)\n",
        "    gamma = 0.9995\n",
        "    for bi in range(1, k):\n",
        "        age = (k - 1) - bi\n",
        "        weights[np.array(blocks[bi])] = (gamma ** age)\n",
        "    weights[b5_idx] *= 2.0\n",
        "    z = (w_core.get('lr',0)*oof_cols['lr'] +\n",
        "         w_core.get('dense',0)*oof_cols['dense'] +\n",
        "         w_core.get('meta',0)*oof_cols['meta'] +\n",
        "         w_core.get('emn',0)*oof_cols['emn'] +\n",
        "         w_core.get('emp',0)*oof_cols['emp'] +\n",
        "         w_core.get('cat',0)*oof_cols['cat'])\n",
        "    if 'char' in oof_cols: z = z + w_core.get('char',0)*oof_cols['char']\n",
        "    return roc_auc_score(y[mask_valid], z[mask_valid], sample_weight=weights[mask_valid])\n",
        "\n",
        "def build_test_probs(w_core: dict, recent: dict):\n",
        "    zt = (w_core.get('lr',0)*test_cols['lr'] +\n",
        "          w_core.get('dense',0)*test_cols['dense'] +\n",
        "          w_core.get('meta',0)*test_cols['meta'] +\n",
        "          w_core.get('emn',0)*test_cols['emn'] +\n",
        "          w_core.get('emp',0)*test_cols['emp'] +\n",
        "          w_core.get('cat',0)*test_cols['cat'])\n",
        "    if 'char' in test_cols: zt = zt + w_core.get('char',0)*test_cols['char']\n",
        "    # add recent-only components on TEST\n",
        "    if (tz_lr_recent is not None) and (recent.get('lr_recent',0)>0): zt += recent['lr_recent']*tz_lr_recent\n",
        "    if (tz_minilm_recent is not None) and (recent.get('minilm_recent',0)>0): zt += recent['minilm_recent']*tz_minilm_recent\n",
        "    if (tz_cat_recent is not None) and (recent.get('cat_recent',0)>0): zt += recent['cat_recent']*tz_cat_recent\n",
        "    return sigmoid(zt).astype(np.float32)\n",
        "\n",
        "def optimize_variant(tag: str, recent_total_target: float|None, n_iter: int, objective: str):\n",
        "    rng = np.random.default_rng(20250912 if tag=='r24' else (20250913 if tag=='r30' else 20250914))\n",
        "    best_auc, best_core, tried = -1.0, None, 0\n",
        "    t0 = time.time()\n",
        "    for it in range(1, n_iter+1):\n",
        "        core = sample_core_weights(rng)\n",
        "        # renorm core to 1 - recent_total\n",
        "        if recent_total_target is None:\n",
        "            recent_total = rng.uniform(0.24, 0.30)\n",
        "        else:\n",
        "            recent_total = recent_total_target\n",
        "        core = renorm_core(core, core_sum_target=(1.0 - recent_total))\n",
        "        # score objective (recent components ignored)\n",
        "        if objective == 'b5':\n",
        "            auc = score_block5(core)\n",
        "        elif objective == 'gam9995_blk5x2':\n",
        "            auc = score_gamma9995_blk5x2(core)\n",
        "        else:\n",
        "            raise ValueError('unknown objective')\n",
        "        tried += 1\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_core = auc, core.copy()\n",
        "        if it % 1000 == 0:\n",
        "            print(f'  [{tag}] iter={it} | best_auc={best_auc:.5f} | elapsed={time.time()-t0:.1f}s', flush=True)\n",
        "    print(f'[{tag}] search done | tried={tried} | best_auc={best_auc:.5f} | {time.time()-t0:.1f}s | core={best_core}', flush=True)\n",
        "    # Assign recent weights within bounds and to recent_total\n",
        "    recent = {}\n",
        "    r_keys = [k for k,(lo,hi) in recent_bounds.items() if hi > lo]\n",
        "    if len(r_keys) > 0:\n",
        "        # Start from random within bounds\n",
        "        raw = np.array([rng.uniform(recent_bounds[k][0], recent_bounds[k][1]) for k in r_keys], dtype=np.float64)\n",
        "        raw_sum = raw.sum() if raw.sum() > 0 else 1.0\n",
        "        if recent_total_target is None:\n",
        "            recent_total = rng.uniform(0.24, 0.30)\n",
        "        else:\n",
        "            recent_total = recent_total_target\n",
        "        scaled = raw / raw_sum * recent_total\n",
        "        for k,val in zip(r_keys, scaled):\n",
        "            recent[k] = float(val)\n",
        "    return best_core, recent\n",
        "\n",
        "# Run three variants per expert\n",
        "core_r24, recent_r24 = optimize_variant('r24', recent_total_target=0.24, n_iter=8000, objective='b5')\n",
        "core_r30, recent_r30 = optimize_variant('r30', recent_total_target=0.30, n_iter=8000, objective='b5')\n",
        "core_gx, recent_gx = optimize_variant('gamma9995_blk5x2', recent_total_target=None, n_iter=8000, objective='gam9995_blk5x2')\n",
        "\n",
        "def write_sub(path, probs):\n",
        "    pd.DataFrame({id_col: ids, target_col: probs}).to_csv(path, index=False)\n",
        "    print(f'Wrote {path} | mean={probs.mean():.6f}', flush=True)\n",
        "\n",
        "p_r24 = build_test_probs(core_r24, recent_r24)\n",
        "p_r30 = build_test_probs(core_r30, recent_r30)\n",
        "p_gx  = build_test_probs(core_gx, recent_gx)\n",
        "path_r24 = 'submission_block5opt_r24.csv'; write_sub(path_r24, p_r24)\n",
        "path_r30 = 'submission_block5opt_r30.csv'; write_sub(path_r30, p_r30)\n",
        "path_gx  = 'submission_block5opt_gamma9995_blk5x2.csv'; write_sub(path_gx, p_gx)\n",
        "\n",
        "# Logit-average hedges (preferred over rank-avg):\n",
        "def p_to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def logit_avg(paths, out_path):\n",
        "    arrs = [pd.read_csv(p)[target_col].values.astype(np.float64) for p in paths]\n",
        "    Z = np.vstack([p_to_logit(a) for a in arrs])\n",
        "    p_mean = sigmoid(Z.mean(axis=0)).astype(np.float32)\n",
        "    write_sub(out_path, p_mean)\n",
        "    return out_path\n",
        "\n",
        "# Typically average r24 + gamma9995; also option to include r30\n",
        "prom_pair = logit_avg([path_r24, path_gx], 'submission_logitavg_r24_gamma9995.csv')\n",
        "logit_avg([path_r24, path_r30, path_gx], 'submission_logitavg_r24_r30_gamma9995.csv')\n",
        "\n",
        "# Promote pair logit-average as primary\n",
        "pd.read_csv(prom_pair).to_csv('submission.csv', index=False)\n",
        "print('Promoted', prom_pair, 'to submission.csv', flush=True)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [r24] iter=1000 | best_auc=0.65148 | elapsed=1.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [r24] iter=2000 | best_auc=0.65168 | elapsed=3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [r24] iter=3000 | best_auc=0.65185 | elapsed=5.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [r24] iter=4000 | best_auc=0.65185 | elapsed=6.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [r24] iter=5000 | best_auc=0.65185 | elapsed=8.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [r24] iter=6000 | best_auc=0.65185 | elapsed=10.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [r24] iter=7000 | best_auc=0.65185 | elapsed=11.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [r24] iter=8000 | best_auc=0.65185 | elapsed=13.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[r24] search done | tried=8000 | best_auc=0.65185 | 13.5s | core={'lr': 0.2091373762438391, 'cat': 0.1267646003622472, 'meta': 0.11194764105173324, 'dense': 0.027994307728506775, 'char': 0.054949179783834616, 'emn': 0.13752413689790344, 'emp': 0.09168275793193562}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [r30] iter=1000 | best_auc=0.65148 | elapsed=1.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [r30] iter=2000 | best_auc=0.65148 | elapsed=3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [r30] iter=3000 | best_auc=0.65148 | elapsed=5.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [r30] iter=4000 | best_auc=0.65168 | elapsed=6.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [r30] iter=5000 | best_auc=0.65168 | elapsed=8.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [r30] iter=6000 | best_auc=0.65191 | elapsed=10.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [r30] iter=7000 | best_auc=0.65191 | elapsed=11.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [r30] iter=8000 | best_auc=0.65191 | elapsed=13.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[r30] search done | tried=8000 | best_auc=0.65191 | 13.4s | core={'lr': 0.19470726325081586, 'cat': 0.11318440767022928, 'meta': 0.10008503666306624, 'dense': 0.031738063457150384, 'char': 0.04866208087453025, 'emn': 0.12697388885052477, 'emp': 0.0846492592336832}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [gamma9995_blk5x2] iter=1000 | best_auc=0.67701 | elapsed=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [gamma9995_blk5x2] iter=2000 | best_auc=0.67701 | elapsed=5.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [gamma9995_blk5x2] iter=3000 | best_auc=0.67701 | elapsed=7.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [gamma9995_blk5x2] iter=4000 | best_auc=0.67701 | elapsed=9.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [gamma9995_blk5x2] iter=5000 | best_auc=0.67701 | elapsed=12.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [gamma9995_blk5x2] iter=6000 | best_auc=0.67701 | elapsed=14.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [gamma9995_blk5x2] iter=7000 | best_auc=0.67701 | elapsed=17.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [gamma9995_blk5x2] iter=8000 | best_auc=0.67711 | elapsed=19.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[gamma9995_blk5x2] search done | tried=8000 | best_auc=0.67711 | 19.7s | core={'lr': 0.1883169615457813, 'cat': 0.11270430827145297, 'meta': 0.12488238635974469, 'dense': 0.035741154404510686, 'char': 0.049384449937499506, 'emn': 0.10922857731367337, 'emp': 0.10922857731367337}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_block5opt_r24.csv | mean=0.342686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_block5opt_r30.csv | mean=0.331348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_block5opt_gamma9995_blk5x2.csv | mean=0.343543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_logitavg_r24_gamma9995.csv | mean=0.343102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_logitavg_r24_r30_gamma9995.csv | mean=0.339154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promoted submission_logitavg_r24_gamma9995.csv to submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "5f232143-b73b-4ba3-aaf2-1f5ec99a7104",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S63: Promote 3-way logit-average hedge (r24 + r30 + gamma9995_blk5x2) if available\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "path = 'submission_logitavg_r24_r30_gamma9995.csv'\n",
        "if Path(path).exists() and Path(path).stat().st_size > 0:\n",
        "    sub = pd.read_csv(path)\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print(f'Promoted {path} to submission.csv | mean={sub.iloc[:,1].mean():.6f}')\n",
        "else:\n",
        "    print('3-way logit-average file not found; leaving current submission.csv as-is')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promoted submission_logitavg_r24_r30_gamma9995.csv to submission.csv | mean=0.339154\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}