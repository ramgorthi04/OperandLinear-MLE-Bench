{
  "cells": [
    {
      "id": "fa98c399-fc1a-42a2-b8a1-4d07d81ad99d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Lean production: build robust submissions S1, S2, S3 from cached OOF/test .npy\n",
        "import numpy as np, pandas as pd, json, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "print('Loading train for y and test ids...')\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "test_ids = test[id_col].values\n",
        "\n",
        "print('Loading cached OOF/test predictions...')\n",
        "o_lr = np.load('oof_lr_pivot.npy')\n",
        "o_d1 = np.load('oof_xgb_dense.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta.npy')\n",
        "t_lr = np.load('test_lr_pivot.npy')\n",
        "t_d1 = np.load('test_xgb_dense.npy')\n",
        "t_d2 = np.load('test_xgb_dense_v2.npy')\n",
        "t_meta = np.load('test_xgb_meta.npy')\n",
        "\n",
        "# Optional 5th model: alternate sparse LR view\n",
        "try:\n",
        "    o_lr_alt = np.load('oof_lr_alt.npy')\n",
        "    t_lr_alt = np.load('test_lr_alt.npy')\n",
        "except Exception:\n",
        "    o_lr_alt = None; t_lr_alt = None\n",
        "\n",
        "# Quick OOF diagnostics\n",
        "def auc_prob(arr):\n",
        "    return roc_auc_score(y, arr)\n",
        "def auc_logit_from_probs(*probs, weights=None):\n",
        "    zs = [to_logit(p) for p in probs]\n",
        "    if weights is None:\n",
        "        w = np.ones(len(zs), dtype=np.float64) / len(zs)\n",
        "    else:\n",
        "        w = np.array(weights, dtype=np.float64)\n",
        "    z = np.zeros_like(zs[0], dtype=np.float64)\n",
        "    for wi, zi in zip(w, zs):\n",
        "        z += wi * zi\n",
        "    return roc_auc_score(y, z)\n",
        "\n",
        "print('Single-model OOF AUCs:')\n",
        "print({'LR': auc_prob(o_lr), 'Dense1': auc_prob(o_d1), 'Dense2': auc_prob(o_d2), 'Meta': auc_prob(o_meta), 'LR_alt': (auc_prob(o_lr_alt) if o_lr_alt is not None else None)})\n",
        "\n",
        "# S1: Global 4-way logit reference blend (fixed best weights from main notebook refine)\n",
        "w_ref = (0.3381, 0.266786, 0.163514, 0.2316)  # (LR, Dense1, Dense2, Meta)\n",
        "z_ref_oof = w_ref[0]*to_logit(o_lr) + w_ref[1]*to_logit(o_d1) + w_ref[2]*to_logit(o_d2) + w_ref[3]*to_logit(o_meta)\n",
        "auc_s1 = roc_auc_score(y, z_ref_oof)\n",
        "print(f'S1 OOF AUC(z): {auc_s1:.5f}')\n",
        "z_ref_te = w_ref[0]*to_logit(t_lr) + w_ref[1]*to_logit(t_d1) + w_ref[2]*to_logit(t_d2) + w_ref[3]*to_logit(t_meta)\n",
        "p_s1 = sigmoid(z_ref_te).astype(np.float32)\n",
        "pd.DataFrame({id_col: test_ids, target_col: p_s1}).to_csv('submission_s1_ref4_logit.csv', index=False)\n",
        "\n",
        "# S2: Equal-weight probability average over 4 models + shrinkage variant toward mean\n",
        "p_eq = (t_lr + t_d1 + t_d2 + t_meta) / 4.0\n",
        "pd.DataFrame({id_col: test_ids, target_col: p_eq.astype(np.float32)}).to_csv('submission_s2_equal_prob.csv', index=False)\n",
        "# Shrinkage: 0.7*S1_probs + 0.3*mean(models)\n",
        "p_eq_shrink = (0.7*p_s1 + 0.3*p_eq).astype(np.float32)\n",
        "pd.DataFrame({id_col: test_ids, target_col: p_eq_shrink}).to_csv('submission_s2_shrink_prob.csv', index=False)\n",
        "\n",
        "# S3: 5-way logit blend with tiny LR_alt weight (3-6%), others scaled from S1\n",
        "best_auc_s3 = -1.0; best_cfg_s3 = None; p_s3_best = None\n",
        "if (o_lr_alt is not None) and (t_lr_alt is not None):\n",
        "    z_lr, z_d1, z_d2, z_m = to_logit(o_lr), to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)\n",
        "    tz_lr, tz_d1, tz_d2, tz_m = to_logit(t_lr), to_logit(t_d1), to_logit(t_d2), to_logit(t_meta)\n",
        "    z_alt, tz_alt = to_logit(o_lr_alt), to_logit(t_lr_alt)\n",
        "    for w_alt in [0.03, 0.04, 0.05, 0.06]:\n",
        "        rem = 1.0 - w_alt\n",
        "        w1 = w_ref[0] * rem\n",
        "        w2 = w_ref[1] * rem\n",
        "        w3 = w_ref[2] * rem\n",
        "        w4 = w_ref[3] * rem\n",
        "        z_oof = w1*z_lr + w2*z_d1 + w3*z_d2 + w4*z_m + w_alt*z_alt\n",
        "        auc_here = roc_auc_score(y, z_oof)\n",
        "        if auc_here > best_auc_s3:\n",
        "            best_auc_s3 = auc_here\n",
        "            best_cfg_s3 = (w1, w2, w3, w4, w_alt)\n",
        "            z_te = w1*tz_lr + w2*tz_d1 + w3*tz_d2 + w4*tz_m + w_alt*tz_alt\n",
        "            p_s3_best = sigmoid(z_te).astype(np.float32)\n",
        "    if best_cfg_s3 is not None:\n",
        "        print(f'S3 5-way best (logit) OOF AUC(z): {best_auc_s3:.5f} | weights(LR,D1,D2,Meta,LR_alt)={best_cfg_s3}')\n",
        "        pd.DataFrame({id_col: test_ids, target_col: p_s3_best}).to_csv('submission_s3_5way_logit.csv', index=False)\n",
        "    else:\n",
        "        print('S3 skipped: LR_alt arrays not found or no config evaluated.')\n",
        "else:\n",
        "    print('S3 skipped: Missing LR_alt cached predictions.')\n",
        "\n",
        "# Default submission.csv: use S1 (ref 4-way logit) as primary per expert advice\n",
        "pd.DataFrame({id_col: test_ids, target_col: p_s1}).to_csv('submission.csv', index=False)\n",
        "print('Wrote default submission.csv as S1 (ref 4-way logit). Also wrote S2 equal/shrink and S3 5-way (if available).')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading train for y and test ids...\nLoading cached OOF/test predictions...\nSingle-model OOF AUCs:\n{'LR': 0.6786023038450223, 'Dense1': 0.6756124134764913, 'Dense2': 0.6749017972318943, 'Meta': 0.666192706969406, 'LR_alt': 0.674147858613878}\nS1 OOF AUC(z): 0.69242\nS3 5-way best (logit) OOF AUC(z): 0.69237 | weights(LR,D1,D2,Meta,LR_alt)=(0.327957, 0.25878242, 0.15860858, 0.224652, 0.03)\nWrote default submission.csv as S1 (ref 4-way logit). Also wrote S2 equal/shrink and S3 5-way (if available).\n"
          ]
        }
      ]
    },
    {
      "id": "b9318569-73d3-42c4-ad4b-7baf27cef03c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Overwrite submission.csv with S2 equal-weight probability hedge\n",
        "import pandas as pd\n",
        "s2 = pd.read_csv('submission_s2_equal_prob.csv')\n",
        "s2.to_csv('submission.csv', index=False)\n",
        "print('submission.csv overwritten with S2 equal-weight probability blend. Head:')\n",
        "print(s2.head())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv overwritten with S2 equal-weight probability blend. Head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.340917\n1   t3_roiuw                  0.225581\n2   t3_mjnbq                  0.215592\n3   t3_t8wd1                  0.208369\n4  t3_1m4zxu                  0.214607\n"
          ]
        }
      ]
    },
    {
      "id": "386bd2f0-1bd1-4fb5-ae3d-1e2a307c65df",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Overwrite submission.csv with S3 5-way logit (tiny LR_alt weight) hedge\n",
        "import pandas as pd\n",
        "s3 = pd.read_csv('submission_s3_5way_logit.csv')\n",
        "s3.to_csv('submission.csv', index=False)\n",
        "print('submission.csv overwritten with S3 5-way logit (tiny LR_alt) blend. Head:')\n",
        "print(s3.head())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv overwritten with S3 5-way logit (tiny LR_alt) blend. Head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.330550\n1   t3_roiuw                  0.215626\n2   t3_mjnbq                  0.211781\n3   t3_t8wd1                  0.210184\n4  t3_1m4zxu                  0.217150\n"
          ]
        }
      ]
    },
    {
      "id": "ab843cf4-6671-4498-af68-b622e767d3b7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Overwrite submission.csv with S2 shrinkage probability hedge\n",
        "import pandas as pd\n",
        "s2s = pd.read_csv('submission_s2_shrink_prob.csv')\n",
        "s2s.to_csv('submission.csv', index=False)\n",
        "print('submission.csv overwritten with S2 shrinkage probability blend. Head:')\n",
        "print(s2s.head())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv overwritten with S2 shrinkage probability blend. Head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.334539\n1   t3_roiuw                  0.219630\n2   t3_mjnbq                  0.214051\n3   t3_t8wd1                  0.208835\n4  t3_1m4zxu                  0.214938\n"
          ]
        }
      ]
    },
    {
      "id": "70a1b831-da02-49b9-aaf7-1d91b92dddf0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build and write blend-of-blends (logit): mix Ref4, PerFold4(no-zstd), Ref3\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Load base OOF/test probs\n",
        "o_lr = np.load('oof_lr_pivot.npy'); t_lr = np.load('test_lr_pivot.npy')\n",
        "o_d1 = np.load('oof_xgb_dense.npy'); t_d1 = np.load('test_xgb_dense.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_v2.npy'); t_d2 = np.load('test_xgb_dense_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta.npy'); t_meta = np.load('test_xgb_meta.npy')\n",
        "\n",
        "# Convert to logits\n",
        "z1, z2, z3, z4 = to_logit(o_lr), to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)\n",
        "tz1, tz2, tz3, tz4 = to_logit(t_lr), to_logit(t_d1), to_logit(t_d2), to_logit(t_meta)\n",
        "\n",
        "# Ref4: fixed weights\n",
        "w_ref4 = (0.3381, 0.266786, 0.163514, 0.2316)  # (LR, D1, D2, Meta)\n",
        "z_ref4 = w_ref4[0]*z1 + w_ref4[1]*z2 + w_ref4[2]*z3 + w_ref4[3]*z4\n",
        "tz_ref4 = w_ref4[0]*tz1 + w_ref4[1]*tz2 + w_ref4[2]*tz3 + w_ref4[3]*tz4\n",
        "print(f'Ref4 OOF AUC(z): {roc_auc_score(y, z_ref4):.5f}')\n",
        "\n",
        "# Ref3: best 3-way logit (LR, D1, Meta) via coarse grid\n",
        "best_auc3, best_w3 = -1.0, None\n",
        "grid = np.arange(0.20, 0.50+1e-12, 0.01)  # search reasonable simplex\n",
        "t0 = time.time(); tried = 0\n",
        "for w_lr in grid:\n",
        "    for w_d1 in grid:\n",
        "        w_meta = 1.0 - w_lr - w_d1\n",
        "        if w_meta < 0 or w_meta > 1: continue\n",
        "        z = w_lr*z1 + w_d1*z2 + w_meta*z4\n",
        "        auc = roc_auc_score(y, z); tried += 1\n",
        "        if auc > best_auc3:\n",
        "            best_auc3, best_w3 = auc, (float(w_lr), float(w_d1), float(w_meta))\n",
        "print(f'Ref3 OOF AUC(z): {best_auc3:.5f} | best_w3={best_w3} | tried={tried}')\n",
        "tz_ref3 = best_w3[0]*tz1 + best_w3[1]*tz2 + best_w3[2]*tz4\n",
        "z_ref3 = best_w3[0]*z1 + best_w3[1]*z2 + best_w3[2]*z4\n",
        "\n",
        "# PerFold4 (no z-std): for each CV fold, pick best weights on train_idx (coarse grid), apply to val_idx; test logits averaged over fold weights\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "z_pf4 = np.zeros_like(y, dtype=np.float64)\n",
        "tz_pf4_parts = []\n",
        "grid_w = np.arange(0.28, 0.40+1e-12, 0.004)  # narrow window around ref4\n",
        "grid_wd = np.arange(0.38, 0.48+1e-12, 0.004) # total dense weight\n",
        "alpha_grid = np.arange(0.20, 0.50+1e-12, 0.05) # split D1/D2\n",
        "for fold, (tr_idx, va_idx) in enumerate(skf.split(train, y), 1):\n",
        "    best_auc_f, best_w_f = -1.0, None\n",
        "    y_tr = y[tr_idx]\n",
        "    for w_lr in grid_w:\n",
        "        for wd in grid_wd:\n",
        "            w_meta = 1.0 - w_lr - wd\n",
        "            if w_meta < 0 or w_meta > 1: continue\n",
        "            for a in alpha_grid:\n",
        "                w_d2 = wd * a; w_d1 = wd - w_d2\n",
        "                z_tr = w_lr*z1[tr_idx] + w_d1*z2[tr_idx] + w_d2*z3[tr_idx] + w_meta*z4[tr_idx]\n",
        "                auc = roc_auc_score(y_tr, z_tr)\n",
        "                if auc > best_auc_f:\n",
        "                    best_auc_f, best_w_f = auc, (float(w_lr), float(w_d1), float(w_d2), float(w_meta))\n",
        "    w_lr, w_d1, w_d2, w_meta = best_w_f\n",
        "    z_pf4[va_idx] = w_lr*z1[va_idx] + w_d1*z2[va_idx] + w_d2*z3[va_idx] + w_meta*z4[va_idx]\n",
        "    tz_pf4_parts.append(w_lr*tz1 + w_d1*tz2 + w_d2*tz3 + w_meta*tz4)\n",
        "    print(f'PerFold4 Fold {fold} best_w={best_w_f}')\n",
        "auc_pf4 = roc_auc_score(y, z_pf4)\n",
        "tz_pf4 = np.mean(tz_pf4_parts, axis=0)\n",
        "print(f'PerFold4 OOF AUC(z): {auc_pf4:.5f}')\n",
        "\n",
        "# Blend-of-blends: mix z_ref4, z_pf4, z_ref3 with a narrow grid around (~0.366, 0.432, 0.202)\n",
        "wr_c, wp_c, w3_c = 0.366, 0.432, 0.202\n",
        "step = 0.01\n",
        "wr_grid = np.arange(max(0.2, wr_c-0.06), min(0.6, wr_c+0.06)+1e-12, step)\n",
        "wp_grid = np.arange(max(0.2, wp_c-0.06), min(0.6, wp_c+0.06)+1e-12, step)\n",
        "best_auc_mix, best_w_mix = -1.0, None\n",
        "t1 = time.time(); tried = 0\n",
        "for wr in wr_grid:\n",
        "    for wp in wp_grid:\n",
        "        w3 = 1.0 - wr - wp\n",
        "        if w3 < 0 or w3 > 1: continue\n",
        "        z_mix = wr*z_ref4 + wp*z_pf4 + w3*z_ref3\n",
        "        auc = roc_auc_score(y, z_mix); tried += 1\n",
        "        if auc > best_auc_mix:\n",
        "            best_auc_mix, best_w_mix = auc, (float(wr), float(wp), float(w3))\n",
        "print(f'Blend-of-blends tried {tried} combos | best_w={best_w_mix} OOF AUC(z): {best_auc_mix:.5f} | {time.time()-t1:.1f}s')\n",
        "\n",
        "# Build test predictions for best weights\n",
        "wr, wp, w3 = best_w_mix\n",
        "tz_mix = wr*tz_ref4 + wp*tz_pf4 + w3*tz_ref3\n",
        "pt = sigmoid(tz_mix).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: ids, target_col: pt})\n",
        "sub.to_csv('submission_blend_of_blends_logit.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (blend-of-blends logit). Head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ref4 OOF AUC(z): 0.69242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ref3 OOF AUC(z): 0.69201 | best_w3=(0.35000000000000014, 0.4200000000000002, 0.2299999999999997) | tried=960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PerFold4 Fold 1 best_w=(0.3440000000000001, 0.28600000000000003, 0.154, 0.21599999999999986)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PerFold4 Fold 2 best_w=(0.3400000000000001, 0.31920000000000004, 0.1368, 0.20399999999999985)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PerFold4 Fold 3 best_w=(0.3400000000000001, 0.23800000000000007, 0.23800000000000002, 0.18399999999999983)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PerFold4 Fold 4 best_w=(0.30000000000000004, 0.23000000000000007, 0.23, 0.23999999999999988)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PerFold4 Fold 5 best_w=(0.32400000000000007, 0.19000000000000003, 0.18999999999999997, 0.29599999999999993)\nPerFold4 OOF AUC(z): 0.69131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blend-of-blends tried 169 combos | best_w=(0.35600000000000004, 0.372, 0.2719999999999999) OOF AUC(z): 0.69198 | 0.3s\nSaved submission.csv (blend-of-blends logit). Head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.331289\n1   t3_roiuw                  0.216208\n2   t3_mjnbq                  0.213753\n3   t3_t8wd1                  0.208131\n4  t3_1m4zxu                  0.214751\n"
          ]
        }
      ]
    },
    {
      "id": "b753454d-80ab-41b7-8f6c-b5ea8801e5a2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S4: Hedge variant - mix LR and LR_noSub in logit space for LR component, keep ref weights for others\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "# Load base preds\n",
        "o_lr = np.load('oof_lr_pivot.npy'); t_lr = np.load('test_lr_pivot.npy')\n",
        "o_lr_ns = np.load('oof_lr_nosub.npy'); t_lr_ns = np.load('test_lr_nosub.npy')\n",
        "o_d1 = np.load('oof_xgb_dense.npy'); t_d1 = np.load('test_xgb_dense.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_v2.npy'); t_d2 = np.load('test_xgb_dense_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta.npy'); t_meta = np.load('test_xgb_meta.npy')\n",
        "\n",
        "# Convert to logits\n",
        "z_lr, z_lr_ns = to_logit(o_lr), to_logit(o_lr_ns)\n",
        "tz_lr, tz_lr_ns = to_logit(t_lr), to_logit(t_lr_ns)\n",
        "z_d1, z_d2, z_meta = to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)\n",
        "tz_d1, tz_d2, tz_meta = to_logit(t_d1), to_logit(t_d2), to_logit(t_meta)\n",
        "\n",
        "# Reference weights for 4-way\n",
        "w_ref = (0.3381, 0.266786, 0.163514, 0.2316)  # (LR, D1, D2, Meta)\n",
        "\n",
        "# Small grid for mixing LR with LR_noSub to hedge subreddit drift\n",
        "best_auc, best_g = -1.0, None\n",
        "for g in [0.2, 0.3, 0.4]:\n",
        "    z_lr_mix = (1.0 - g)*z_lr + g*z_lr_ns\n",
        "    z_oof = w_ref[0]*z_lr_mix + w_ref[1]*z_d1 + w_ref[2]*z_d2 + w_ref[3]*z_meta\n",
        "    auc = roc_auc_score(y, z_oof)\n",
        "    print(f'g={g:.2f} | OOF AUC(z): {auc:.5f}')\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_g = auc, g\n",
        "print(f'Chosen g={best_g:.2f} with OOF AUC(z)={best_auc:.5f}')\n",
        "\n",
        "# Build test with chosen g\n",
        "tz_lr_mix = (1.0 - best_g)*tz_lr + best_g*tz_lr_ns\n",
        "zt = w_ref[0]*tz_lr_mix + w_ref[1]*tz_d1 + w_ref[2]*tz_d2 + w_ref[3]*tz_meta\n",
        "pt = sigmoid(zt).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "sub.to_csv('submission_s4_lr_mix_logit.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('submission.csv overwritten with S4 LR/LR_noSub mix hedge. Head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "g=0.20 | OOF AUC(z): 0.69246\ng=0.30 | OOF AUC(z): 0.69246\ng=0.40 | OOF AUC(z): 0.69252\nChosen g=0.40 with OOF AUC(z)=0.69252\nsubmission.csv overwritten with S4 LR/LR_noSub mix hedge. Head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.333798\n1   t3_roiuw                  0.216595\n2   t3_mjnbq                  0.208447\n3   t3_t8wd1                  0.211159\n4  t3_1m4zxu                  0.216228\n"
          ]
        }
      ]
    },
    {
      "id": "106502a6-875a-4bdc-b82e-932506b8ad59",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S5: 2-way global logit blend: LR + Dense v1, sweep LR weight in {0.55, 0.60, 0.65, 0.70}\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "o_lr = np.load('oof_lr_pivot.npy'); t_lr = np.load('test_lr_pivot.npy')\n",
        "o_d1 = np.load('oof_xgb_dense.npy'); t_d1 = np.load('test_xgb_dense.npy')\n",
        "\n",
        "z_lr, z_d1 = to_logit(o_lr), to_logit(o_d1)\n",
        "tz_lr, tz_d1 = to_logit(t_lr), to_logit(t_d1)\n",
        "\n",
        "best_auc, best_w = -1.0, None\n",
        "for w_lr in [0.55, 0.60, 0.65, 0.70]:\n",
        "    w_d1 = 1.0 - w_lr\n",
        "    z = w_lr*z_lr + w_d1*z_d1\n",
        "    auc = roc_auc_score(y, z)\n",
        "    print(f'w_lr={w_lr:.2f}, w_d1={w_d1:.2f} | OOF AUC(z): {auc:.5f}')\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_w = auc, (w_lr, w_d1)\n",
        "print(f'Chosen weights (LR, Dense1)={best_w} with OOF AUC(z)={best_auc:.5f}')\n",
        "\n",
        "zt = best_w[0]*tz_lr + best_w[1]*tz_d1\n",
        "pt = sigmoid(zt).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "sub.to_csv('submission_s5_2way_logit.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('submission.csv overwritten with S5 2-way global logit (LR + Dense1). Head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w_lr=0.55, w_d1=0.45 | OOF AUC(z): 0.68983\nw_lr=0.60, w_d1=0.40 | OOF AUC(z): 0.68932\nw_lr=0.65, w_d1=0.35 | OOF AUC(z): 0.68852\nw_lr=0.70, w_d1=0.30 | OOF AUC(z): 0.68747\nChosen weights (LR, Dense1)=(0.55, 0.44999999999999996) with OOF AUC(z)=0.68983\nsubmission.csv overwritten with S5 2-way global logit (LR + Dense1). Head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.313329\n1   t3_roiuw                  0.197200\n2   t3_mjnbq                  0.208460\n3   t3_t8wd1                  0.204065\n4  t3_1m4zxu                  0.209776\n"
          ]
        }
      ]
    },
    {
      "id": "9c518b76-d50a-4a17-a00b-e67c27e0c38e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S4-highg and S6: cached-only hedges\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Load base preds\n",
        "o_lr = np.load('oof_lr_pivot.npy'); t_lr = np.load('test_lr_pivot.npy')\n",
        "o_lr_ns = np.load('oof_lr_nosub.npy'); t_lr_ns = np.load('test_lr_nosub.npy')\n",
        "o_d1 = np.load('oof_xgb_dense.npy'); t_d1 = np.load('test_xgb_dense.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_v2.npy'); t_d2 = np.load('test_xgb_dense_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta.npy'); t_meta = np.load('test_xgb_meta.npy')\n",
        "\n",
        "z_lr, z_lr_ns = to_logit(o_lr), to_logit(o_lr_ns)\n",
        "tz_lr, tz_lr_ns = to_logit(t_lr), to_logit(t_lr_ns)\n",
        "z_d1, z_d2, z_meta = to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)\n",
        "tz_d1, tz_d2, tz_meta = to_logit(t_d1), to_logit(t_d2), to_logit(t_meta)\n",
        "\n",
        "# Reference 4-way weights\n",
        "w_ref = (0.3381, 0.266786, 0.163514, 0.2316)  # (LR, D1, D2, Meta)\n",
        "\n",
        "# S4-highg: 4-way global logit with LR_mix using high g values (0.60, 0.70)\n",
        "for g in [0.60, 0.70]:\n",
        "    z_lr_mix = (1.0 - g)*z_lr + g*z_lr_ns\n",
        "    z_oof = w_ref[0]*z_lr_mix + w_ref[1]*z_d1 + w_ref[2]*z_d2 + w_ref[3]*z_meta\n",
        "    auc = roc_auc_score(y, z_oof)\n",
        "    print(f'S4 g={g:.2f} | OOF AUC(z): {auc:.5f}')\n",
        "    tz_lr_mix = (1.0 - g)*tz_lr + g*tz_lr_ns\n",
        "    zt = w_ref[0]*tz_lr_mix + w_ref[1]*tz_d1 + w_ref[2]*tz_d2 + w_ref[3]*tz_meta\n",
        "    pt = sigmoid(zt).astype(np.float32)\n",
        "    sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "    sub.to_csv(f'submission_s4_lr_mix_g{int(g*100)}.csv', index=False)\n",
        "\n",
        "# S6: Equal-weight probability average across 5 models (add LR_alt), with mild clip [0.02, 0.98]\n",
        "try:\n",
        "    t_lr_alt = np.load('test_lr_alt.npy')\n",
        "    o_lr_alt = np.load('oof_lr_alt.npy')\n",
        "    # Log simple OOF for reference (prob space AUC)\n",
        "    try:\n",
        "        auc_alt = roc_auc_score(y, o_lr_alt)\n",
        "        print(f'LR_alt OOF(prob) AUC: {auc_alt:.5f}')\n",
        "    except Exception:\n",
        "        pass\n",
        "    p5_te = (t_lr + t_d1 + t_d2 + t_meta + t_lr_alt) / 5.0\n",
        "    p5_te = np.clip(p5_te, 0.02, 0.98).astype(np.float32)\n",
        "    pd.DataFrame({id_col: test[id_col].values, target_col: p5_te}).to_csv('submission_s6_equal5_clip.csv', index=False)\n",
        "    print('Wrote S6: submission_s6_equal5_clip.csv')\n",
        "except Exception as e:\n",
        "    print('S6 skipped (missing LR_alt cache):', e)\n",
        "\n",
        "# Default for this cell: set submission.csv to S4 g=0.70 per expert hedge\n",
        "s4g70 = pd.read_csv('submission_s4_lr_mix_g70.csv')\n",
        "s4g70.to_csv('submission.csv', index=False)\n",
        "print('submission.csv overwritten with S4 g=0.70 (LR_mix high-g) variant. Head:')\n",
        "print(s4g70.head())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S4 g=0.60 | OOF AUC(z): 0.69233\nS4 g=0.70 | OOF AUC(z): 0.69217\nLR_alt OOF(prob) AUC: 0.67415\nWrote S6: submission_s6_equal5_clip.csv\nsubmission.csv overwritten with S4 g=0.70 (LR_mix high-g) variant. Head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.335296\n1   t3_roiuw                  0.216232\n2   t3_mjnbq                  0.204796\n3   t3_t8wd1                  0.212763\n4  t3_1m4zxu                  0.217092\n"
          ]
        }
      ]
    },
    {
      "id": "316c7f3b-2568-4e15-8821-e2e92c50c916",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Overwrite submission.csv with S6: equal-weight 5-model probability blend with clipping\n",
        "import pandas as pd\n",
        "s6 = pd.read_csv('submission_s6_equal5_clip.csv')\n",
        "s6.to_csv('submission.csv', index=False)\n",
        "print('submission.csv overwritten with S6 equal5 clipped probability blend. Head:')\n",
        "print(s6.head())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv overwritten with S6 equal5 clipped probability blend. Head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.330999\n1   t3_roiuw                  0.214951\n2   t3_mjnbq                  0.205373\n3   t3_t8wd1                  0.216660\n4  t3_1m4zxu                  0.230018\n"
          ]
        }
      ]
    },
    {
      "id": "a94f7e5e-d854-4e8d-9928-2b69c5ccb2be",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Time-aware CV retrain for LR (with and without subreddit TF-IDF); cache OOF/test for robust blending\n",
        "import numpy as np, pandas as pd, time, sys, gc\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def build_combined_text(df: pd.DataFrame) -> pd.Series:\n",
        "    title = df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "    body = df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "    return (title + ' \\n ' + body).astype(str)\n",
        "\n",
        "def build_subreddit_text(df: pd.DataFrame) -> pd.Series:\n",
        "    if 'requester_subreddits_at_request' not in df.columns:\n",
        "        return pd.Series(['']*len(df))\n",
        "    sr = df['requester_subreddits_at_request']\n",
        "    return sr.apply(lambda x: ' '.join([str(s).lower() for s in x]) if isinstance(x, (list, tuple)) else '')\n",
        "\n",
        "txt_tr = build_combined_text(train); txt_te = build_combined_text(test)\n",
        "subs_tr = build_subreddit_text(train); subs_te = build_subreddit_text(test)\n",
        "\n",
        "# Build time-ordered forward-chaining folds (5 blocks)\n",
        "assert 'unix_timestamp_of_request' in train.columns, 'Missing timestamp for time-aware CV'\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 5\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i])\n",
        "    tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx))\n",
        "print(f'Time-CV: {len(folds)} folds (forward chaining) built.')\n",
        "\n",
        "word_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=50000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "char_params = dict(analyzer='char_wb', ngram_range=(3,5), lowercase=True, min_df=3, max_features=50000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "subs_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=30000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "\n",
        "def run_lr_time(with_subs: bool, tag: str):\n",
        "    t0 = time.time()\n",
        "    oof = np.zeros(n, dtype=np.float32)\n",
        "    mask = np.zeros(n, dtype=bool)\n",
        "    test_fold_preds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "        fstart = time.time()\n",
        "        tr_text = txt_tr.iloc[tr_idx]; va_text = txt_tr.iloc[va_idx]\n",
        "        tfidf_w = TfidfVectorizer(**word_params)\n",
        "        Xw_tr = tfidf_w.fit_transform(tr_text); Xw_va = tfidf_w.transform(va_text); Xw_te = tfidf_w.transform(txt_te)\n",
        "        tfidf_c = TfidfVectorizer(**char_params)\n",
        "        Xc_tr = tfidf_c.fit_transform(tr_text); Xc_va = tfidf_c.transform(va_text); Xc_te = tfidf_c.transform(txt_te)\n",
        "        if with_subs:\n",
        "            tfidf_s = TfidfVectorizer(**subs_params)\n",
        "            Xs_tr = tfidf_s.fit_transform(subs_tr.iloc[tr_idx]); Xs_va = tfidf_s.transform(subs_tr.iloc[va_idx]); Xs_te = tfidf_s.transform(subs_te)\n",
        "            X_tr = hstack([Xw_tr, Xc_tr, Xs_tr], format='csr')\n",
        "            X_va = hstack([Xw_va, Xc_va, Xs_va], format='csr')\n",
        "            X_te = hstack([Xw_te, Xc_te, Xs_te], format='csr')\n",
        "        else:\n",
        "            X_tr = hstack([Xw_tr, Xc_tr], format='csr')\n",
        "            X_va = hstack([Xw_va, Xc_va], format='csr')\n",
        "            X_te = hstack([Xw_te, Xc_te], format='csr')\n",
        "        clf = LogisticRegression(solver='saga', penalty='l2', C=0.5, max_iter=4000, n_jobs=-1, class_weight=None, random_state=42, verbose=0)\n",
        "        clf.fit(X_tr, y[tr_idx])\n",
        "        va_pred = clf.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "        te_pred = clf.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "        oof[va_idx] = va_pred\n",
        "        mask[va_idx] = True\n",
        "        test_fold_preds.append(te_pred)\n",
        "        auc = roc_auc_score(y[va_idx], va_pred)\n",
        "        print(f'[{tag}] Fold {fi}/{len(folds)} AUC: {auc:.5f} | elapsed {time.time()-fstart:.1f}s')\n",
        "        del tfidf_w, tfidf_c, Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, X_tr, X_va, X_te\n",
        "        if with_subs:\n",
        "            del tfidf_s, Xs_tr, Xs_va, Xs_te\n",
        "        gc.collect()\n",
        "    validated = int(mask.sum())\n",
        "    auc_oof = roc_auc_score(y[mask], oof[mask]); print(f'[{tag}] OOF AUC (on validated {validated}/{n}): {auc_oof:.5f} | total {time.time()-t0:.1f}s')\n",
        "    test_pred = np.mean(test_fold_preds, axis=0).astype(np.float32)\n",
        "    np.save(f'oof_lr_time_{tag}.npy', oof.astype(np.float32))\n",
        "    np.save(f'test_lr_time_{tag}.npy', test_pred)\n",
        "    return auc_oof\n",
        "\n",
        "auc_with = run_lr_time(True, 'withsub')\n",
        "auc_nosub = run_lr_time(False, 'nosub')\n",
        "print({'time_lr_withsub': auc_with, 'time_lr_nosub': auc_nosub})"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV: 4 folds (forward chaining) built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[withsub] Fold 1/4 AUC: 0.67061 | elapsed 3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[withsub] Fold 2/4 AUC: 0.59622 | elapsed 5.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[withsub] Fold 3/4 AUC: 0.60989 | elapsed 8.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[withsub] Fold 4/4 AUC: 0.58196 | elapsed 10.7s\n[withsub] OOF AUC (on validated 2302/2878): 0.61641 | total 27.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nosub] Fold 1/4 AUC: 0.67949 | elapsed 3.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nosub] Fold 2/4 AUC: 0.56102 | elapsed 6.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nosub] Fold 3/4 AUC: 0.58402 | elapsed 8.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nosub] Fold 4/4 AUC: 0.61858 | elapsed 10.8s\n[nosub] OOF AUC (on validated 2302/2878): 0.60963 | total 30.1s\n{'time_lr_withsub': 0.6164125729343122, 'time_lr_nosub': 0.6096292964384343}\n"
          ]
        }
      ]
    },
    {
      "id": "b8945111-53b6-4b81-b1c2-aad2799326d2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Overwrite submission.csv with S4 g=0.60 (LR_mix) per expert hedge\n",
        "import pandas as pd\n",
        "s4g60 = pd.read_csv('submission_s4_lr_mix_g60.csv')\n",
        "s4g60.to_csv('submission.csv', index=False)\n",
        "print('submission.csv overwritten with S4 g=0.60 (LR_mix) variant. Head:')\n",
        "print(s4g60.head())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv overwritten with S4 g=0.60 (LR_mix) variant. Head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.334796\n1   t3_roiuw                  0.216353\n2   t3_mjnbq                  0.206008\n3   t3_t8wd1                  0.212227\n4  t3_1m4zxu                  0.216804\n"
          ]
        }
      ]
    },
    {
      "id": "cfdefd9e-9d4c-4d6a-9dd0-d7e7c76a2e70",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S7: 4-way global logit with weights shrunk toward equal (15-20%)\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "# Load base preds\n",
        "o_lr = np.load('oof_lr_pivot.npy'); t_lr = np.load('test_lr_pivot.npy')\n",
        "o_d1 = np.load('oof_xgb_dense.npy'); t_d1 = np.load('test_xgb_dense.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_v2.npy'); t_d2 = np.load('test_xgb_dense_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta.npy'); t_meta = np.load('test_xgb_meta.npy')\n",
        "\n",
        "z1, z2, z3, z4 = to_logit(o_lr), to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)\n",
        "tz1, tz2, tz3, tz4 = to_logit(t_lr), to_logit(t_d1), to_logit(t_d2), to_logit(t_meta)\n",
        "\n",
        "# Reference weights and equal weights\n",
        "w_ref = np.array([0.3381, 0.266786, 0.163514, 0.2316], dtype=np.float64)\n",
        "w_eq = np.array([0.25, 0.25, 0.25, 0.25], dtype=np.float64)\n",
        "\n",
        "best_auc, best_alpha, best_w = -1.0, None, None\n",
        "for alpha in [0.15, 0.20]:\n",
        "    w = (1.0 - alpha) * w_ref + alpha * w_eq\n",
        "    w = w / w.sum()\n",
        "    z_oof = w[0]*z1 + w[1]*z2 + w[2]*z3 + w[3]*z4\n",
        "    auc = roc_auc_score(y, z_oof)\n",
        "    print(f'alpha={alpha:.2f} | shrunk weights={tuple(w)} | OOF AUC(z): {auc:.5f}')\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_alpha, best_w = auc, alpha, w\n",
        "print(f'Chosen alpha={best_alpha:.2f} | weights={tuple(best_w)} | OOF AUC(z): {best_auc:.5f}')\n",
        "\n",
        "zt = best_w[0]*tz1 + best_w[1]*tz2 + best_w[2]*tz3 + best_w[3]*tz4\n",
        "pt = sigmoid(zt).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "sub.to_csv('submission_s7_shrunk_logit.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('submission.csv overwritten with S7 shrunk-weight 4-way logit. Head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alpha=0.15 | shrunk weights=(0.324885, 0.2642681, 0.1764869, 0.23436) | OOF AUC(z): 0.69237\nalpha=0.20 | shrunk weights=(0.32048, 0.2634288, 0.1808112, 0.23528) | OOF AUC(z): 0.69237\nChosen alpha=0.15 | weights=(0.324885, 0.2642681, 0.1764869, 0.23436) | OOF AUC(z): 0.69237\nsubmission.csv overwritten with S7 shrunk-weight 4-way logit. Head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.333016\n1   t3_roiuw                  0.218114\n2   t3_mjnbq                  0.213677\n3   t3_t8wd1                  0.208816\n4  t3_1m4zxu                  0.213635\n"
          ]
        }
      ]
    },
    {
      "id": "3d73d2ca-77f0-4251-bb91-23d15151cb07",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S8: Time-CV tuned 4-way global logit with LR_mix (use LR time-CV OOF/test; others from cache); coarse shrink toward equal\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Build time-CV mask (same as Cell 9 forward-chaining on unix_timestamp)\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 5\n",
        "blocks = np.array_split(order, k)\n",
        "mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); mask[va_idx] = True\n",
        "print(f'Time-CV validated count: {mask.sum()}/{n}')\n",
        "\n",
        "# Load OOF/test predictions\n",
        "# LR (time-CV) with/without subreddit\n",
        "o_lr_time_w = np.load('oof_lr_time_withsub.npy')\n",
        "o_lr_time_ns = np.load('oof_lr_time_nosub.npy')\n",
        "t_lr_time_w = np.load('test_lr_time_withsub.npy')\n",
        "t_lr_time_ns = np.load('test_lr_time_nosub.npy')\n",
        "# Dense/meta from cached (stratified OOF), use as-is for weight tuning on time mask\n",
        "o_d1 = np.load('oof_xgb_dense.npy'); t_d1 = np.load('test_xgb_dense.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_v2.npy'); t_d2 = np.load('test_xgb_dense_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta.npy'); t_meta = np.load('test_xgb_meta.npy')\n",
        "\n",
        "# Convert to logits\n",
        "z_lr_w, z_lr_ns = to_logit(o_lr_time_w), to_logit(o_lr_time_ns)\n",
        "tz_lr_w, tz_lr_ns = to_logit(t_lr_time_w), to_logit(t_lr_time_ns)\n",
        "z_d1, z_d2, z_meta = to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)\n",
        "tz_d1, tz_d2, tz_meta = to_logit(t_d1), to_logit(t_d2), to_logit(t_meta)\n",
        "\n",
        "# Reference 4-way weights\n",
        "w_ref = np.array([0.3381, 0.266786, 0.163514, 0.2316], dtype=np.float64)  # (LR, D1, D2, Meta)\n",
        "w_eq = np.array([0.25, 0.25, 0.25, 0.25], dtype=np.float64)\n",
        "\n",
        "best_auc, best_cfg = -1.0, None\n",
        "for g in [0.50, 0.60, 0.65, 0.70]:\n",
        "    z_lr_mix = (1.0 - g)*z_lr_w + g*z_lr_ns\n",
        "    tz_lr_mix = (1.0 - g)*tz_lr_w + g*tz_lr_ns\n",
        "    for alpha in [0.0, 0.15]:\n",
        "        w = (1.0 - alpha)*w_ref + alpha*w_eq\n",
        "        w = w / w.sum()\n",
        "        # OOF blend (score only on time mask)\n",
        "        z_oof = w[0]*z_lr_mix + w[1]*z_d1 + w[2]*z_d2 + w[3]*z_meta\n",
        "        auc = roc_auc_score(y[mask], z_oof[mask])\n",
        "        print(f'g={g:.2f}, alpha={alpha:.2f} | OOF(z,time-mask) AUC: {auc:.5f} | weights={tuple(w)}')\n",
        "        if auc > best_auc:\n",
        "            best_auc = auc; best_cfg = (g, alpha, w, tz_lr_mix)\n",
        "\n",
        "g_best, alpha_best, w_best, tz_lr_mix_best = best_cfg\n",
        "print(f'Chosen config: g={g_best:.2f}, alpha={alpha_best:.2f}, weights={tuple(w_best)} | time-mask OOF AUC(z)={best_auc:.5f}')\n",
        "\n",
        "# Build test prediction with chosen config\n",
        "zt = w_best[0]*tz_lr_mix_best + w_best[1]*tz_d1 + w_best[2]*tz_d2 + w_best[3]*tz_meta\n",
        "pt = sigmoid(zt).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "sub.to_csv('submission_s8_timecv_lr_mix.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('submission.csv overwritten with S8 time-CV tuned 4-way (LR_mix) logit. Head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV validated count: 2302/2878\ng=0.50, alpha=0.00 | OOF(z,time-mask) AUC: 0.68662 | weights=(0.3381, 0.266786, 0.163514, 0.2316)\ng=0.50, alpha=0.15 | OOF(z,time-mask) AUC: 0.68700 | weights=(0.324885, 0.2642681, 0.1764869, 0.23436)\ng=0.60, alpha=0.00 | OOF(z,time-mask) AUC: 0.68656 | weights=(0.3381, 0.266786, 0.163514, 0.2316)\ng=0.60, alpha=0.15 | OOF(z,time-mask) AUC: 0.68698 | weights=(0.324885, 0.2642681, 0.1764869, 0.23436)\ng=0.65, alpha=0.00 | OOF(z,time-mask) AUC: 0.68654 | weights=(0.3381, 0.266786, 0.163514, 0.2316)\ng=0.65, alpha=0.15 | OOF(z,time-mask) AUC: 0.68693 | weights=(0.324885, 0.2642681, 0.1764869, 0.23436)\ng=0.70, alpha=0.00 | OOF(z,time-mask) AUC: 0.68639 | weights=(0.3381, 0.266786, 0.163514, 0.2316)\ng=0.70, alpha=0.15 | OOF(z,time-mask) AUC: 0.68683 | weights=(0.324885, 0.2642681, 0.1764869, 0.23436)\nChosen config: g=0.50, alpha=0.15, weights=(0.324885, 0.2642681, 0.1764869, 0.23436) | time-mask OOF AUC(z)=0.68700\nsubmission.csv overwritten with S8 time-CV tuned 4-way (LR_mix) logit. Head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.350145\n1   t3_roiuw                  0.261291\n2   t3_mjnbq                  0.236648\n3   t3_t8wd1                  0.235512\n4  t3_1m4zxu                  0.212814\n"
          ]
        }
      ]
    },
    {
      "id": "e5efd788-306c-425c-9708-6723e6759cab",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S9: Time-aware CV Dense XGB (SVD on TF-IDF word/char/subs + enriched meta incl. hour_sin/cos), 3-seed bag; 6-block CV; per-fold scale_pos_weight\n",
        "import numpy as np, pandas as pd, time, re, gc, sys\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import xgboost as xgb\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df: pd.DataFrame) -> pd.Series:\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "\n",
        "def get_body(df: pd.DataFrame) -> pd.Series:\n",
        "    return df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "\n",
        "def combine_text(df: pd.DataFrame) -> pd.Series:\n",
        "    return (get_title(df) + ' \\n ' + get_body(df)).astype(str)\n",
        "\n",
        "def clean_text_series(s: pd.Series) -> pd.Series:\n",
        "    s = s.str.lower()\n",
        "    s = s.str.replace(r'https?://\\S+', ' url ', regex=True)\n",
        "    s = s.str.replace(r'\\d+', ' number ', regex=True)\n",
        "    s = s.str.replace(r'\\s+', ' ', regex=True)\n",
        "    return s\n",
        "\n",
        "def build_subreddit_text(df: pd.DataFrame) -> pd.Series:\n",
        "    if 'requester_subreddits_at_request' not in df.columns:\n",
        "        return pd.Series(['']*len(df))\n",
        "    sr = df['requester_subreddits_at_request']\n",
        "    return sr.apply(lambda x: ' '.join([str(s).lower() for s in x]) if isinstance(x, (list, tuple)) else '')\n",
        "\n",
        "def build_meta(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    title = get_title(df)\n",
        "    body = get_body(df)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    # Text meta\n",
        "    out['title_len'] = title.str.len().astype(np.float32)\n",
        "    out['body_len'] = body.str.len().astype(np.float32)\n",
        "    out['title_body_ratio'] = (out['title_len'] / (1.0 + out['body_len'])).astype(np.float32)\n",
        "    out['has_url'] = body.str.contains(r'https?://', regex=True).astype(np.float32)\n",
        "    out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n",
        "    # Time meta (from unix timestamp if available) + hour sin/cos\n",
        "    if 'unix_timestamp_of_request' in df.columns:\n",
        "        dt = pd.to_datetime(df['unix_timestamp_of_request'], unit='s', utc=True, errors='coerce')\n",
        "    else:\n",
        "        dt = pd.to_datetime(0, unit='s', utc=True) + pd.to_timedelta(np.zeros(len(df)), unit='s')\n",
        "    hour = dt.dt.hour.fillna(0).astype(np.float32)\n",
        "    out['hour'] = hour\n",
        "    out['dayofweek'] = dt.dt.dayofweek.fillna(0).astype(np.float32)\n",
        "    out['is_weekend'] = out['dayofweek'].isin([5,6]).astype(np.float32)\n",
        "    out['hour_sin'] = np.sin(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    out['hour_cos'] = np.cos(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    # Account/karma proxies (if present)\n",
        "    for c in [\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_posts_at_request'\n",
        "    ]:\n",
        "        if c in df.columns:\n",
        "            out[c] = pd.to_numeric(df[c], errors='coerce').astype(np.float32)\n",
        "        else:\n",
        "            out[c] = 0.0\n",
        "    # log1p heavy-tailed fields\n",
        "    for c in ['title_len','body_len','title_body_ratio',\n",
        "              'requester_upvotes_minus_downvotes_at_request',\n",
        "              'requester_upvotes_plus_downvotes_at_request',\n",
        "              'requester_number_of_comments_at_request',\n",
        "              'requester_number_of_posts_at_request']:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(out[c].clip(lower=0)).astype(np.float32)\n",
        "    out = out.replace([np.inf,-np.inf], 0).fillna(0).astype(np.float32)\n",
        "    return out\n",
        "\n",
        "# Build time-ordered forward-chaining folds (6 blocks -> 5 folds) and validated mask\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []\n",
        "mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'Time-CV Dense: {len(folds)} folds; validated {mask.sum()}/{n}')\n",
        "\n",
        "# Precompute cleaned text and subs for train/test\n",
        "raw_tr_text = combine_text(train); raw_te_text = combine_text(test)\n",
        "clean_tr_text = clean_text_series(raw_tr_text); clean_te_text = clean_text_series(raw_te_text)\n",
        "subs_tr = build_subreddit_text(train); subs_te = build_subreddit_text(test)\n",
        "meta_te_base = build_meta(test).astype(np.float32).values\n",
        "\n",
        "# Vectorizer params\n",
        "word_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=60000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "char_params = dict(analyzer='char_wb', ngram_range=(3,5), lowercase=True, min_df=3, max_features=60000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "subs_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=20000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "\n",
        "# SVD dims: word=150, char=150, subs=50\n",
        "svd_w_n, svd_c_n, svd_s_n = 150, 150, 50\n",
        "\n",
        "# XGB params for drift robustness (eta tweaked); scale_pos_weight set per fold\n",
        "base_params = dict(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='auc',\n",
        "    max_depth=3,\n",
        "    eta=0.035,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.6,\n",
        "    min_child_weight=8,\n",
        "    reg_alpha=1.0,\n",
        "    reg_lambda=4.0,\n",
        "    gamma=0.1,\n",
        "    device='cuda',\n",
        "    tree_method='hist'\n",
        ")\n",
        "num_boost_round = 6000\n",
        "early_stopping_rounds = 300\n",
        "seeds = [42, 1337, 2025]\n",
        "\n",
        "oof_sum = np.zeros(n, dtype=np.float64)\n",
        "oof_cnt = np.zeros(n, dtype=np.float64)\n",
        "test_seed_preds = []\n",
        "\n",
        "for si, seed in enumerate(seeds, 1):\n",
        "    print(f'=== Seed {seed} ({si}/{len(seeds)}) ===')\n",
        "    params = dict(base_params); params['seed'] = seed\n",
        "    oof_seed = np.zeros(n, dtype=np.float32)\n",
        "    test_folds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "        fold_t0 = time.time()\n",
        "        # Build per-fold meta\n",
        "        meta_tr = build_meta(train.iloc[tr_idx]).astype(np.float32).values\n",
        "        meta_va = build_meta(train.iloc[va_idx]).astype(np.float32).values\n",
        "\n",
        "        # TF-IDF fit on train, transform val/test\n",
        "        t_tfidf = time.time()\n",
        "        tfidf_w = TfidfVectorizer(**word_params)\n",
        "        Xw_tr = tfidf_w.fit_transform(clean_tr_text.iloc[tr_idx]); Xw_va = tfidf_w.transform(clean_tr_text.iloc[va_idx]); Xw_te = tfidf_w.transform(clean_te_text)\n",
        "        tfidf_c = TfidfVectorizer(**char_params)\n",
        "        Xc_tr = tfidf_c.fit_transform(clean_tr_text.iloc[tr_idx]); Xc_va = tfidf_c.transform(clean_tr_text.iloc[va_idx]); Xc_te = tfidf_c.transform(clean_te_text)\n",
        "        tfidf_s = TfidfVectorizer(**subs_params)\n",
        "        Xs_tr = tfidf_s.fit_transform(subs_tr.iloc[tr_idx]); Xs_va = tfidf_s.transform(subs_tr.iloc[va_idx]); Xs_te = tfidf_s.transform(subs_te)\n",
        "        print(f'[Seed {seed} Fold {fi}] TF-IDF done in {time.time()-t_tfidf:.1f}s | shapes W:{Xw_tr.shape} C:{Xc_tr.shape} S:{Xs_tr.shape}')\n",
        "\n",
        "        # SVD per view with seed-aligned random_state\n",
        "        t_svd = time.time()\n",
        "        svd_w = TruncatedSVD(n_components=svd_w_n, random_state=seed)\n",
        "        Zw_tr = svd_w.fit_transform(Xw_tr); Zw_va = svd_w.transform(Xw_va); Zw_te = svd_w.transform(Xw_te)\n",
        "        svd_c = TruncatedSVD(n_components=svd_c_n, random_state=seed)\n",
        "        Zc_tr = svd_c.fit_transform(Xc_tr); Zc_va = svd_c.transform(Xc_va); Zc_te = svd_c.transform(Xc_te)\n",
        "        svd_s = TruncatedSVD(n_components=svd_s_n, random_state=seed)\n",
        "        Zs_tr = svd_s.fit_transform(Xs_tr); Zs_va = svd_s.transform(Xs_va); Zs_te = svd_s.transform(Xs_te)\n",
        "        ev_w = float(getattr(svd_w, 'explained_variance_ratio_', np.array([])).sum()) if hasattr(svd_w, 'explained_variance_ratio_') else np.nan\n",
        "        ev_c = float(getattr(svd_c, 'explained_variance_ratio_', np.array([])).sum()) if hasattr(svd_c, 'explained_variance_ratio_') else np.nan\n",
        "        ev_s = float(getattr(svd_s, 'explained_variance_ratio_', np.array([])).sum()) if hasattr(svd_s, 'explained_variance_ratio_') else np.nan\n",
        "        print(f'[Seed {seed} Fold {fi}] SVD done in {time.time()-t_svd:.1f}s | EV sums W:{ev_w:.3f} C:{ev_c:.3f} S:{ev_s:.3f}')\n",
        "\n",
        "        # Stack and scale\n",
        "        t_stack = time.time()\n",
        "        Xtr_dense = np.hstack([Zw_tr, Zc_tr, Zs_tr, meta_tr]).astype(np.float32)\n",
        "        Xva_dense = np.hstack([Zw_va, Zc_va, Zs_va, meta_va]).astype(np.float32)\n",
        "        Xte_dense = np.hstack([Zw_te, Zc_te, Zs_te, meta_te_base]).astype(np.float32)\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "        Xtr = scaler.fit_transform(Xtr_dense); Xva = scaler.transform(Xva_dense); Xte = scaler.transform(Xte_dense)\n",
        "        print(f'[Seed {seed} Fold {fi}] Stack/scale done in {time.time()-t_stack:.1f}s | shapes tr:{Xtr.shape} va:{Xva.shape} te:{Xte.shape}')\n",
        "\n",
        "        # Train XGBoost with per-fold class balance\n",
        "        pos = float((y[tr_idx] == 1).sum()); neg = float((y[tr_idx] == 0).sum())\n",
        "        spw = (neg / max(pos, 1.0)) if pos > 0 else 1.0\n",
        "        params['scale_pos_weight'] = spw\n",
        "        t_train = time.time()\n",
        "        dtrain = xgb.DMatrix(Xtr, label=y[tr_idx])\n",
        "        dvalid = xgb.DMatrix(Xva, label=y[va_idx])\n",
        "        dtest  = xgb.DMatrix(Xte)\n",
        "        booster = xgb.train(params, dtrain, num_boost_round=num_boost_round, evals=[(dvalid, 'valid')], early_stopping_rounds=early_stopping_rounds, verbose_eval=False)\n",
        "        va_pred = booster.predict(dvalid, iteration_range=(0, booster.best_iteration+1)).astype(np.float32)\n",
        "        te_pred = booster.predict(dtest, iteration_range=(0, booster.best_iteration+1)).astype(np.float32)\n",
        "        oof_seed[va_idx] = va_pred\n",
        "        test_folds.append(te_pred)\n",
        "        auc = roc_auc_score(y[va_idx], va_pred)\n",
        "        print(f'[Seed {seed} Fold {fi}] Train {time.time()-t_train:.1f}s | best_iter={booster.best_iteration} | spw={spw:.2f} | AUC: {auc:.5f} | total {time.time()-fold_t0:.1f}s')\n",
        "\n",
        "        # Cleanup\n",
        "        del (tfidf_w, tfidf_c, tfidf_s, Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, Xs_tr, Xs_va, Xs_te,\n",
        "             svd_w, svd_c, svd_s, Zw_tr, Zw_va, Zw_te, Zc_tr, Zc_va, Zc_te, Zs_tr, Zs_va, Zs_te,\n",
        "             Xtr_dense, Xva_dense, Xte_dense, Xtr, Xva, Xte, dtrain, dvalid, dtest, booster)\n",
        "        gc.collect()\n",
        "\n",
        "    # Aggregate per-seed\n",
        "    seed_auc = roc_auc_score(y[mask], oof_seed[mask])\n",
        "    print(f'[Seed {seed}] OOF AUC (validated only): {seed_auc:.5f}')\n",
        "    oof_sum[mask] += oof_seed[mask]\n",
        "    oof_cnt[mask] += 1.0\n",
        "    test_seed_preds.append(np.mean(test_folds, axis=0).astype(np.float64))\n",
        "    del oof_seed, test_folds\n",
        "    gc.collect()\n",
        "\n",
        "# Final averaged OOF/test\n",
        "oof_avg = np.zeros(n, dtype=np.float32)\n",
        "oof_avg[mask] = (oof_sum[mask] / np.maximum(oof_cnt[mask], 1.0)).astype(np.float32)\n",
        "test_avg = np.mean(test_seed_preds, axis=0).astype(np.float32)\n",
        "\n",
        "auc_oof = roc_auc_score(y[mask], oof_avg[mask])\n",
        "print(f'Dense Time-CV OOF AUC (validated only, 3-seed avg): {auc_oof:.5f}')\n",
        "np.save('oof_xgb_dense_time.npy', oof_avg.astype(np.float32))\n",
        "np.save('test_xgb_dense_time.npy', test_avg)\n",
        "print('Saved oof_xgb_dense_time.npy and test_xgb_dense_time.npy')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV Dense: 5 folds; validated 2398/2878\n=== Seed 42 (1/3) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42 Fold 1] TF-IDF done in 1.7s | shapes W:(480, 3990) C:(480, 13463) S:(480, 511)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42 Fold 1] SVD done in 1.7s | EV sums W:0.520 C:0.569 S:0.604\n[Seed 42 Fold 1] Stack/scale done in 0.0s | shapes tr:(480, 364) va:(480, 364) te:(1162, 364)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42 Fold 1] Train 0.9s | best_iter=41 | spw=1.94 | AUC: 0.69377 | total 4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42 Fold 2] TF-IDF done in 1.9s | shapes W:(960, 7415) C:(960, 19786) S:(960, 1035)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42 Fold 2] SVD done in 3.5s | EV sums W:0.338 C:0.415 S:0.484\n[Seed 42 Fold 2] Stack/scale done in 0.0s | shapes tr:(960, 364) va:(480, 364) te:(1162, 364)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42 Fold 2] Train 0.8s | best_iter=4 | spw=2.33 | AUC: 0.68735 | total 6.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42 Fold 3] TF-IDF done in 2.2s | shapes W:(1440, 10152) C:(1440, 24292) S:(1440, 1621)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42 Fold 3] SVD done in 4.3s | EV sums W:0.273 C:0.357 S:0.412\n[Seed 42 Fold 3] Stack/scale done in 0.0s | shapes tr:(1440, 364) va:(480, 364) te:(1162, 364)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42 Fold 3] Train 0.9s | best_iter=23 | spw=2.49 | AUC: 0.63294 | total 7.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42 Fold 4] TF-IDF done in 2.5s | shapes W:(1920, 12497) C:(1920, 27408) S:(1920, 2246)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42 Fold 4] SVD done in 5.2s | EV sums W:0.238 C:0.329 S:0.357\n[Seed 42 Fold 4] Stack/scale done in 0.0s | shapes tr:(1920, 364) va:(479, 364) te:(1162, 364)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42 Fold 4] Train 1.3s | best_iter=203 | spw=2.79 | AUC: 0.64135 | total 9.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42 Fold 5] TF-IDF done in 2.9s | shapes W:(2399, 14639) C:(2399, 30209) S:(2399, 3258)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42 Fold 5] SVD done in 5.9s | EV sums W:0.216 C:0.311 S:0.302\n[Seed 42 Fold 5] Stack/scale done in 0.0s | shapes tr:(2399, 364) va:(479, 364) te:(1162, 364)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42 Fold 5] Train 1.6s | best_iter=354 | spw=2.83 | AUC: 0.61672 | total 10.5s\n[Seed 42] OOF AUC (validated only): 0.63948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Seed 1337 (2/3) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337 Fold 1] TF-IDF done in 1.5s | shapes W:(480, 3990) C:(480, 13463) S:(480, 511)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337 Fold 1] SVD done in 1.8s | EV sums W:0.520 C:0.569 S:0.604\n[Seed 1337 Fold 1] Stack/scale done in 0.0s | shapes tr:(480, 364) va:(480, 364) te:(1162, 364)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337 Fold 1] Train 0.9s | best_iter=64 | spw=1.94 | AUC: 0.66188 | total 4.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337 Fold 2] TF-IDF done in 2.0s | shapes W:(960, 7415) C:(960, 19786) S:(960, 1035)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337 Fold 2] SVD done in 3.5s | EV sums W:0.338 C:0.415 S:0.485\n[Seed 1337 Fold 2] Stack/scale done in 0.0s | shapes tr:(960, 364) va:(480, 364) te:(1162, 364)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337 Fold 2] Train 0.8s | best_iter=2 | spw=2.33 | AUC: 0.67431 | total 6.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337 Fold 3] TF-IDF done in 2.3s | shapes W:(1440, 10152) C:(1440, 24292) S:(1440, 1621)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337 Fold 3] SVD done in 4.2s | EV sums W:0.273 C:0.357 S:0.412\n[Seed 1337 Fold 3] Stack/scale done in 0.1s | shapes tr:(1440, 364) va:(480, 364) te:(1162, 364)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337 Fold 3] Train 0.8s | best_iter=1 | spw=2.49 | AUC: 0.65287 | total 7.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337 Fold 4] TF-IDF done in 2.6s | shapes W:(1920, 12497) C:(1920, 27408) S:(1920, 2246)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337 Fold 4] SVD done in 5.1s | EV sums W:0.238 C:0.329 S:0.357\n[Seed 1337 Fold 4] Stack/scale done in 0.0s | shapes tr:(1920, 364) va:(479, 364) te:(1162, 364)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337 Fold 4] Train 1.0s | best_iter=85 | spw=2.79 | AUC: 0.61859 | total 8.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337 Fold 5] TF-IDF done in 2.9s | shapes W:(2399, 14639) C:(2399, 30209) S:(2399, 3258)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337 Fold 5] SVD done in 5.8s | EV sums W:0.216 C:0.311 S:0.302\n[Seed 1337 Fold 5] Stack/scale done in 0.0s | shapes tr:(2399, 364) va:(479, 364) te:(1162, 364)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337 Fold 5] Train 2.3s | best_iter=669 | spw=2.83 | AUC: 0.62785 | total 11.1s\n[Seed 1337] OOF AUC (validated only): 0.62327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Seed 2025 (3/3) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 2025 Fold 1] TF-IDF done in 1.6s | shapes W:(480, 3990) C:(480, 13463) S:(480, 511)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 2025 Fold 1] SVD done in 1.8s | EV sums W:0.520 C:0.569 S:0.603\n[Seed 2025 Fold 1] Stack/scale done in 0.0s | shapes tr:(480, 364) va:(480, 364) te:(1162, 364)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 2025 Fold 1] Train 0.9s | best_iter=29 | spw=1.94 | AUC: 0.69895 | total 4.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 2025 Fold 2] TF-IDF done in 1.9s | shapes W:(960, 7415) C:(960, 19786) S:(960, 1035)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 2025 Fold 2] SVD done in 3.4s | EV sums W:0.338 C:0.415 S:0.484\n[Seed 2025 Fold 2] Stack/scale done in 0.0s | shapes tr:(960, 364) va:(480, 364) te:(1162, 364)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 2025 Fold 2] Train 0.8s | best_iter=7 | spw=2.33 | AUC: 0.66537 | total 6.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 2025 Fold 3] TF-IDF done in 2.2s | shapes W:(1440, 10152) C:(1440, 24292) S:(1440, 1621)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 2025 Fold 3] SVD done in 4.4s | EV sums W:0.273 C:0.357 S:0.412\n[Seed 2025 Fold 3] Stack/scale done in 0.0s | shapes tr:(1440, 364) va:(480, 364) te:(1162, 364)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 2025 Fold 3] Train 0.9s | best_iter=4 | spw=2.49 | AUC: 0.62353 | total 7.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 2025 Fold 4] TF-IDF done in 2.5s | shapes W:(1920, 12497) C:(1920, 27408) S:(1920, 2246)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 2025 Fold 4] SVD done in 5.1s | EV sums W:0.238 C:0.329 S:0.357\n[Seed 2025 Fold 4] Stack/scale done in 0.0s | shapes tr:(1920, 364) va:(479, 364) te:(1162, 364)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 2025 Fold 4] Train 1.0s | best_iter=70 | spw=2.79 | AUC: 0.63544 | total 8.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/501625183.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 2025 Fold 5] TF-IDF done in 2.9s | shapes W:(2399, 14639) C:(2399, 30209) S:(2399, 3258)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 2025 Fold 5] SVD done in 6.0s | EV sums W:0.216 C:0.311 S:0.302\n[Seed 2025 Fold 5] Stack/scale done in 0.0s | shapes tr:(2399, 364) va:(479, 364) te:(1162, 364)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 2025 Fold 5] Train 1.3s | best_iter=210 | spw=2.83 | AUC: 0.62317 | total 10.3s\n[Seed 2025] OOF AUC (validated only): 0.63970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense Time-CV OOF AUC (validated only, 3-seed avg): 0.64533\nSaved oof_xgb_dense_time.npy and test_xgb_dense_time.npy\n"
          ]
        }
      ]
    },
    {
      "id": "a2af0a94-40d1-48a4-ab23-02e11bfac104",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S10: Pure time-CV logit blends: LR_time(with/no sub) + Dense_time; tune on time-mask OOF; write submissions\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Time-mask (same 5 blocks -> 4 folds forward chaining)\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 5\n",
        "blocks = np.array_split(order, k)\n",
        "mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    mask[np.array(blocks[i])] = True\n",
        "print(f'Time-CV validated count: {mask.sum()}/{n}')\n",
        "\n",
        "# Load time-CV base predictions\n",
        "o_lr_w = np.load('oof_lr_time_withsub.npy'); t_lr_w = np.load('test_lr_time_withsub.npy')\n",
        "o_lr_ns = np.load('oof_lr_time_nosub.npy'); t_lr_ns = np.load('test_lr_time_nosub.npy')\n",
        "o_den = np.load('oof_xgb_dense_time.npy'); t_den = np.load('test_xgb_dense_time.npy')\n",
        "\n",
        "# Convert to logits\n",
        "z_lr_w, z_lr_ns, z_den = to_logit(o_lr_w), to_logit(o_lr_ns), to_logit(o_den)\n",
        "tz_lr_w, tz_lr_ns, tz_den = to_logit(t_lr_w), to_logit(t_lr_ns), to_logit(t_den)\n",
        "\n",
        "# Grid: mix LR_withsub and LR_nosub in logit space with g; then 2-way mix LR_mix and Dense_time with weight w_lr\n",
        "g_grid = [0.50, 0.60, 0.65, 0.70]\n",
        "wlr_grid = np.arange(0.55, 0.71, 0.03)  # LR weight in final 2-way logit blend\n",
        "best_auc, best_cfg = -1.0, None\n",
        "for g in g_grid:\n",
        "    z_lr_mix = (1.0 - g)*z_lr_w + g*z_lr_ns\n",
        "    for w_lr in wlr_grid:\n",
        "        z_oof = w_lr*z_lr_mix + (1.0 - w_lr)*z_den\n",
        "        auc = roc_auc_score(y[mask], z_oof[mask])\n",
        "        if auc > best_auc:\n",
        "            best_auc = auc; best_cfg = (g, w_lr)\n",
        "print(f'Best time-CV logit blend: g={best_cfg[0]:.2f}, w_lr={best_cfg[1]:.2f} | OOF(z,time-mask) AUC={best_auc:.5f}')\n",
        "\n",
        "# Build primary test submission with best cfg\n",
        "g_best, wlr_best = best_cfg\n",
        "tz_lr_mix_best = (1.0 - g_best)*tz_lr_w + g_best*tz_lr_ns\n",
        "zt_best = wlr_best*tz_lr_mix_best + (1.0 - wlr_best)*tz_den\n",
        "pt_best = sigmoid(zt_best).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: pt_best}).to_csv('submission_time_blend_best.csv', index=False)\n",
        "\n",
        "# Shrunk-weight variant: shrink 15% toward equal two-way weights (0.5/0.5) to hedge\n",
        "alpha = 0.15\n",
        "wlr_shrunk = (1.0 - alpha)*wlr_best + alpha*0.5\n",
        "zt_shrunk = wlr_shrunk*tz_lr_mix_best + (1.0 - wlr_shrunk)*tz_den\n",
        "pt_shrunk = sigmoid(zt_shrunk).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: pt_shrunk}).to_csv('submission_time_blend_shrunk.csv', index=False)\n",
        "\n",
        "# Equal-prob hedge among the three time-CV bases (prob space) + mild clipping\n",
        "p_eq = np.clip((t_lr_w + t_lr_ns + t_den) / 3.0, 0.01, 0.99).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: p_eq}).to_csv('submission_time_equal_prob.csv', index=False)\n",
        "\n",
        "# Set default submission.csv to primary time-CV blend\n",
        "pd.DataFrame({id_col: ids, target_col: pt_best}).to_csv('submission.csv', index=False)\n",
        "print('Wrote submission_time_blend_best.csv (primary), shrunk and equal-prob hedges; submission.csv set to primary.')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV validated count: 2302/2878\nBest time-CV logit blend: g=0.50, w_lr=0.55 | OOF(z,time-mask) AUC=0.63329\nWrote submission_time_blend_best.csv (primary), shrunk and equal-prob hedges; submission.csv set to primary.\n"
          ]
        }
      ]
    },
    {
      "id": "eab5ea7b-dac8-4c28-be67-0f7dca0dba12",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S11: 6-block forward-chaining LR_time with enriched meta (incl. hour_sin/cos); withsub and nosub; cache OOF/test\n",
        "import numpy as np, pandas as pd, gc, time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "\n",
        "def get_body(df):\n",
        "    return df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "\n",
        "def build_text(df):\n",
        "    return (get_title(df) + ' \\n ' + get_body(df)).astype(str)\n",
        "\n",
        "def build_subs(df):\n",
        "    if 'requester_subreddits_at_request' not in df.columns:\n",
        "        return pd.Series(['']*len(df))\n",
        "    sr = df['requester_subreddits_at_request']\n",
        "    return sr.apply(lambda x: ' '.join([str(s).lower() for s in x]) if isinstance(x, (list, tuple)) else '')\n",
        "\n",
        "def build_meta(df):\n",
        "    title = get_title(df); body = get_body(df)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    out['title_len'] = title.str.len().astype(np.float32)\n",
        "    out['body_len'] = body.str.len().astype(np.float32)\n",
        "    out['title_body_ratio'] = (out['title_len'] / (1.0 + out['body_len'])).astype(np.float32)\n",
        "    out['has_url'] = body.str.contains(r'https?://', regex=True).astype(np.float32)\n",
        "    out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n",
        "    if 'unix_timestamp_of_request' in df.columns:\n",
        "        dt = pd.to_datetime(df['unix_timestamp_of_request'], unit='s', utc=True, errors='coerce')\n",
        "    else:\n",
        "        dt = pd.to_datetime(0, unit='s', utc=True) + pd.to_timedelta(np.zeros(len(df)), unit='s')\n",
        "    hour = dt.dt.hour.fillna(0).astype(np.float32)\n",
        "    out['hour'] = hour\n",
        "    out['dayofweek'] = dt.dt.dayofweek.fillna(0).astype(np.float32)\n",
        "    out['is_weekend'] = out['dayofweek'].isin([5,6]).astype(np.float32)\n",
        "    # hour sin/cos\n",
        "    out['hour_sin'] = np.sin(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    out['hour_cos'] = np.cos(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    for c in [\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_posts_at_request'\n",
        "    ]:\n",
        "        if c in df.columns:\n",
        "            out[c] = pd.to_numeric(df[c], errors='coerce').astype(np.float32)\n",
        "        else:\n",
        "            out[c] = 0.0\n",
        "    for c in ['title_len','body_len','title_body_ratio',\n",
        "              'requester_upvotes_minus_downvotes_at_request',\n",
        "              'requester_upvotes_plus_downvotes_at_request',\n",
        "              'requester_number_of_comments_at_request',\n",
        "              'requester_number_of_posts_at_request']:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(out[c].clip(lower=0)).astype(np.float32)\n",
        "    out = out.replace([np.inf, -np.inf], 0).fillna(0).astype(np.float32)\n",
        "    return out\n",
        "\n",
        "# 6-block forward-chaining folds (\u2192 5 folds); score only validated indices\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'Time-CV LR (with meta): {len(folds)} folds; validated {mask.sum()}/{n}')\n",
        "\n",
        "txt_tr = build_text(train); txt_te = build_text(test)\n",
        "subs_tr = build_subs(train); subs_te = build_subs(test)\n",
        "meta_te = build_meta(test).astype(np.float32).values\n",
        "\n",
        "word_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=60000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "char_params = dict(analyzer='char_wb', ngram_range=(3,5), lowercase=True, min_df=3, max_features=60000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "subs_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=20000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "\n",
        "def run_lr_time_meta(with_subs: bool, tag: str):\n",
        "    t0 = time.time()\n",
        "    oof = np.zeros(n, dtype=np.float32)\n",
        "    test_fold_preds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "        f0 = time.time()\n",
        "        # Text TF-IDF per fold\n",
        "        tfidf_w = TfidfVectorizer(**word_params)\n",
        "        Xw_tr = tfidf_w.fit_transform(txt_tr.iloc[tr_idx]); Xw_va = tfidf_w.transform(txt_tr.iloc[va_idx]); Xw_te = tfidf_w.transform(txt_te)\n",
        "        tfidf_c = TfidfVectorizer(**char_params)\n",
        "        Xc_tr = tfidf_c.fit_transform(txt_tr.iloc[tr_idx]); Xc_va = tfidf_c.transform(txt_tr.iloc[va_idx]); Xc_te = tfidf_c.transform(txt_te)\n",
        "        if with_subs:\n",
        "            tfidf_s = TfidfVectorizer(**subs_params)\n",
        "            Xs_tr = tfidf_s.fit_transform(subs_tr.iloc[tr_idx]); Xs_va = tfidf_s.transform(subs_tr.iloc[va_idx]); Xs_te = tfidf_s.transform(subs_te)\n",
        "        # Meta per fold + scale\n",
        "        meta_tr = build_meta(train.iloc[tr_idx]).astype(np.float32).values\n",
        "        meta_va = build_meta(train.iloc[va_idx]).astype(np.float32).values\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "        meta_tr_s = scaler.fit_transform(meta_tr).astype(np.float32)\n",
        "        meta_va_s = scaler.transform(meta_va).astype(np.float32)\n",
        "        meta_te_s = scaler.transform(meta_te).astype(np.float32)\n",
        "        # Stack text + meta\n",
        "        if with_subs:\n",
        "            X_tr = hstack([Xw_tr, Xc_tr, Xs_tr, csr_matrix(meta_tr_s)], format='csr')\n",
        "            X_va = hstack([Xw_va, Xc_va, Xs_va, csr_matrix(meta_va_s)], format='csr')\n",
        "            X_te = hstack([Xw_te, Xc_te, Xs_te, csr_matrix(meta_te_s)], format='csr')\n",
        "        else:\n",
        "            X_tr = hstack([Xw_tr, Xc_tr, csr_matrix(meta_tr_s)], format='csr')\n",
        "            X_va = hstack([Xw_va, Xc_va, csr_matrix(meta_va_s)], format='csr')\n",
        "            X_te = hstack([Xw_te, Xc_te, csr_matrix(meta_te_s)], format='csr')\n",
        "        # Train LR\n",
        "        clf = LogisticRegression(solver='saga', penalty='l2', C=0.8, max_iter=4000, n_jobs=-1, random_state=42)\n",
        "        clf.fit(X_tr, y[tr_idx])\n",
        "        va_pred = clf.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "        te_pred = clf.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "        oof[va_idx] = va_pred\n",
        "        test_fold_preds.append(te_pred)\n",
        "        auc = roc_auc_score(y[va_idx], va_pred)\n",
        "        print(f'[{tag}] Fold {fi}/{len(folds)} AUC: {auc:.5f} | elapsed {time.time()-f0:.1f}s')\n",
        "        # cleanup\n",
        "        del tfidf_w, tfidf_c, Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, scaler, meta_tr, meta_va, meta_tr_s, meta_va_s, meta_te_s, X_tr, X_va, X_te, clf\n",
        "        if with_subs:\n",
        "            del tfidf_s, Xs_tr, Xs_va, Xs_te\n",
        "        gc.collect()\n",
        "    auc_oof = roc_auc_score(y[mask], oof[mask])\n",
        "    print(f'[{tag}] OOF AUC (validated only): {auc_oof:.5f} | total {time.time()-t0:.1f}s')\n",
        "    test_pred = np.mean(test_fold_preds, axis=0).astype(np.float32)\n",
        "    np.save(f'oof_lr_time_{tag}_meta.npy', oof.astype(np.float32))\n",
        "    np.save(f'test_lr_time_{tag}_meta.npy', test_pred)\n",
        "    return auc_oof\n",
        "\n",
        "auc_with_meta = run_lr_time_meta(True, 'withsub')\n",
        "auc_nosub_meta = run_lr_time_meta(False, 'nosub')\n",
        "print({'time_lr_withsub_meta': auc_with_meta, 'time_lr_nosub_meta': auc_nosub_meta})"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV LR (with meta): 5 folds; validated 2398/2878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1816970215.py:36: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1816970215.py:36: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/1816970215.py:36: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[withsub] Fold 1/5 AUC: 0.72358 | elapsed 4.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1816970215.py:36: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/1816970215.py:36: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[withsub] Fold 2/5 AUC: 0.65111 | elapsed 12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1816970215.py:36: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/1816970215.py:36: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[withsub] Fold 3/5 AUC: 0.63332 | elapsed 25.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1816970215.py:36: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/1816970215.py:36: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[withsub] Fold 4/5 AUC: 0.60852 | elapsed 31.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1816970215.py:36: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/1816970215.py:36: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[withsub] Fold 5/5 AUC: 0.61538 | elapsed 42.1s\n[withsub] OOF AUC (validated only): 0.64640 | total 117.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1816970215.py:36: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/1816970215.py:36: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nosub] Fold 1/5 AUC: 0.73879 | elapsed 6.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1816970215.py:36: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/1816970215.py:36: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nosub] Fold 2/5 AUC: 0.66851 | elapsed 18.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1816970215.py:36: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/1816970215.py:36: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nosub] Fold 3/5 AUC: 0.63060 | elapsed 25.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1816970215.py:36: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/1816970215.py:36: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nosub] Fold 4/5 AUC: 0.62357 | elapsed 37.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1816970215.py:36: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/1816970215.py:36: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nosub] Fold 5/5 AUC: 0.64427 | elapsed 44.9s\n[nosub] OOF AUC (validated only): 0.66058 | total 133.7s\n{'time_lr_withsub_meta': 0.6464044860018527, 'time_lr_nosub_meta': 0.660576334259739}\n"
          ]
        }
      ]
    },
    {
      "id": "9b1d73a7-4a6f-4cd7-88e7-56aebd912576",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S12: Dense XGB v2 (no-subreddit view) under 6-block time-CV with enriched meta (incl. hour_sin/cos), per-fold scale_pos_weight; 3-seed bag\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import xgboost as xgb\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body(df):\n",
        "    return df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "def combine_text(df):\n",
        "    return (get_title(df) + ' \\n ' + get_body(df)).astype(str)\n",
        "def clean_text_series(s):\n",
        "    s = s.str.lower()\n",
        "    s = s.str.replace(r'https?://\\S+', ' url ', regex=True)\n",
        "    s = s.str.replace(r'\\d+', ' number ', regex=True)\n",
        "    s = s.str.replace(r'\\s+', ' ', regex=True)\n",
        "    return s\n",
        "def build_meta(df):\n",
        "    title = get_title(df); body = get_body(df)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    out['title_len'] = title.str.len().astype(np.float32)\n",
        "    out['body_len'] = body.str.len().astype(np.float32)\n",
        "    out['title_body_ratio'] = (out['title_len'] / (1.0 + out['body_len'])).astype(np.float32)\n",
        "    out['has_url'] = body.str.contains(r'https?://', regex=True).astype(np.float32)\n",
        "    out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n",
        "    if 'unix_timestamp_of_request' in df.columns:\n",
        "        dt = pd.to_datetime(df['unix_timestamp_of_request'], unit='s', utc=True, errors='coerce')\n",
        "    else:\n",
        "        dt = pd.to_datetime(0, unit='s', utc=True) + pd.to_timedelta(np.zeros(len(df)), unit='s')\n",
        "    hour = dt.dt.hour.fillna(0).astype(np.float32)\n",
        "    out['hour'] = hour\n",
        "    out['dayofweek'] = dt.dt.dayofweek.fillna(0).astype(np.float32)\n",
        "    out['is_weekend'] = out['dayofweek'].isin([5,6]).astype(np.float32)\n",
        "    out['hour_sin'] = np.sin(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    out['hour_cos'] = np.cos(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    for c in [\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_posts_at_request'\n",
        "    ]:\n",
        "        if c in df.columns:\n",
        "            out[c] = pd.to_numeric(df[c], errors='coerce').astype(np.float32)\n",
        "        else:\n",
        "            out[c] = 0.0\n",
        "    for c in ['title_len','body_len','title_body_ratio',\n",
        "              'requester_upvotes_minus_downvotes_at_request',\n",
        "              'requester_upvotes_plus_downvotes_at_request',\n",
        "              'requester_number_of_comments_at_request',\n",
        "              'requester_number_of_posts_at_request']:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(out[c].clip(lower=0)).astype(np.float32)\n",
        "    out = out.replace([np.inf,-np.inf], 0).fillna(0).astype(np.float32)\n",
        "    return out\n",
        "\n",
        "# 6-block forward chaining (5 folds) and mask\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'Dense v2 Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}')\n",
        "\n",
        "# Precompute text/meta\n",
        "raw_tr_text = combine_text(train); raw_te_text = combine_text(test)\n",
        "clean_tr_text = clean_text_series(raw_tr_text); clean_te_text = clean_text_series(raw_te_text)\n",
        "meta_te_base = build_meta(test).astype(np.float32).values\n",
        "\n",
        "# Vectorizers (no subreddit view here)\n",
        "word_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=80000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "char_params = dict(analyzer='char_wb', ngram_range=(3,5), lowercase=True, min_df=3, max_features=60000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "\n",
        "# SVD dims for v2: word=250, char=120\n",
        "svd_w_n, svd_c_n = 250, 120\n",
        "\n",
        "# XGB params for v2 (stronger reg, no subs view); per-fold scale_pos_weight\n",
        "base_params = dict(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='auc',\n",
        "    max_depth=3,\n",
        "    eta=0.03,\n",
        "    min_child_weight=10,\n",
        "    subsample=0.75,\n",
        "    colsample_bytree=0.7,\n",
        "    reg_alpha=1.5,\n",
        "    reg_lambda=5.0,\n",
        "    gamma=0.2,\n",
        "    device='cuda',\n",
        "    tree_method='hist'\n",
        ")\n",
        "num_boost_round = 6000\n",
        "early_stopping_rounds = 300\n",
        "seeds = [42, 1337, 2025]\n",
        "\n",
        "oof_sum = np.zeros(n, dtype=np.float64)\n",
        "oof_cnt = np.zeros(n, dtype=np.float64)\n",
        "test_seed_preds = []\n",
        "\n",
        "for si, seed in enumerate(seeds, 1):\n",
        "    print(f'=== Dense v2 Seed {seed} ({si}/{len(seeds)}) ===')\n",
        "    params = dict(base_params); params['seed'] = seed\n",
        "    oof_seed = np.zeros(n, dtype=np.float32)\n",
        "    test_folds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "        t0 = time.time()\n",
        "        meta_tr = build_meta(train.iloc[tr_idx]).astype(np.float32).values\n",
        "        meta_va = build_meta(train.iloc[va_idx]).astype(np.float32).values\n",
        "\n",
        "        # TF-IDF\n",
        "        t_tfidf = time.time()\n",
        "        tfidf_w = TfidfVectorizer(**word_params)\n",
        "        Xw_tr = tfidf_w.fit_transform(clean_tr_text.iloc[tr_idx]); Xw_va = tfidf_w.transform(clean_tr_text.iloc[va_idx]); Xw_te = tfidf_w.transform(clean_te_text)\n",
        "        tfidf_c = TfidfVectorizer(**char_params)\n",
        "        Xc_tr = tfidf_c.fit_transform(clean_tr_text.iloc[tr_idx]); Xc_va = tfidf_c.transform(clean_tr_text.iloc[va_idx]); Xc_te = tfidf_c.transform(clean_te_text)\n",
        "        print(f'[v2 Seed {seed} Fold {fi}] TF-IDF {time.time()-t_tfidf:.1f}s | W:{Xw_tr.shape} C:{Xc_tr.shape}')\n",
        "\n",
        "        # SVD\n",
        "        t_svd = time.time()\n",
        "        svd_w = TruncatedSVD(n_components=svd_w_n, random_state=seed)\n",
        "        Zw_tr = svd_w.fit_transform(Xw_tr); Zw_va = svd_w.transform(Xw_va); Zw_te = svd_w.transform(Xw_te)\n",
        "        svd_c = TruncatedSVD(n_components=svd_c_n, random_state=seed)\n",
        "        Zc_tr = svd_c.fit_transform(Xc_tr); Zc_va = svd_c.transform(Xc_va); Zc_te = svd_c.transform(Xc_te)\n",
        "        ev_w = float(getattr(svd_w, 'explained_variance_ratio_', np.array([])).sum()) if hasattr(svd_w, 'explained_variance_ratio_') else np.nan\n",
        "        ev_c = float(getattr(svd_c, 'explained_variance_ratio_', np.array([])).sum()) if hasattr(svd_c, 'explained_variance_ratio_') else np.nan\n",
        "        print(f'[v2 Seed {seed} Fold {fi}] SVD {time.time()-t_svd:.1f}s | EV W:{ev_w:.3f} C:{ev_c:.3f}')\n",
        "\n",
        "        # Stack + scale\n",
        "        t_stack = time.time()\n",
        "        Xtr_dense = np.hstack([Zw_tr, Zc_tr, meta_tr]).astype(np.float32)\n",
        "        Xva_dense = np.hstack([Zw_va, Zc_va, meta_va]).astype(np.float32)\n",
        "        Xte_dense = np.hstack([Zw_te, Zc_te, meta_te_base]).astype(np.float32)\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "        Xtr = scaler.fit_transform(Xtr_dense); Xva = scaler.transform(Xva_dense); Xte = scaler.transform(Xte_dense)\n",
        "        print(f'[v2 Seed {seed} Fold {fi}] Stack/scale {time.time()-t_stack:.1f}s | tr:{Xtr.shape} va:{Xva.shape} te:{Xte.shape}')\n",
        "\n",
        "        # Train XGB with per-fold class balance\n",
        "        pos = float((y[tr_idx] == 1).sum()); neg = float((y[tr_idx] == 0).sum())\n",
        "        params['scale_pos_weight'] = (neg / max(pos, 1.0)) if pos > 0 else 1.0\n",
        "        t_train = time.time()\n",
        "        dtrain = xgb.DMatrix(Xtr, label=y[tr_idx]); dvalid = xgb.DMatrix(Xva, label=y[va_idx]); dtest = xgb.DMatrix(Xte)\n",
        "        booster = xgb.train(params, dtrain, num_boost_round=num_boost_round, evals=[(dvalid, 'valid')], early_stopping_rounds=early_stopping_rounds, verbose_eval=False)\n",
        "        va_pred = booster.predict(dvalid, iteration_range=(0, booster.best_iteration+1)).astype(np.float32)\n",
        "        te_pred = booster.predict(dtest, iteration_range=(0, booster.best_iteration+1)).astype(np.float32)\n",
        "        oof_seed[va_idx] = va_pred; test_folds.append(te_pred)\n",
        "        auc = roc_auc_score(y[va_idx], va_pred)\n",
        "        print(f'[v2 Seed {seed} Fold {fi}] Train {time.time()-t_train:.1f}s | best_iter={booster.best_iteration} | AUC: {auc:.5f} | total {time.time()-t0:.1f}s')\n",
        "\n",
        "        del (tfidf_w, tfidf_c, Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, svd_w, svd_c, Zw_tr, Zw_va, Zw_te, Zc_tr, Zc_va, Zc_te,\n",
        "             Xtr_dense, Xva_dense, Xte_dense, Xtr, Xva, Xte, dtrain, dvalid, dtest, booster)\n",
        "        gc.collect()\n",
        "\n",
        "    seed_auc = roc_auc_score(y[mask], oof_seed[mask])\n",
        "    print(f'[Dense v2 Seed {seed}] OOF AUC (validated only): {seed_auc:.5f}')\n",
        "    oof_sum[mask] += oof_seed[mask]; oof_cnt[mask] += 1.0\n",
        "    test_seed_preds.append(np.mean(test_folds, axis=0).astype(np.float64))\n",
        "    del oof_seed, test_folds; gc.collect()\n",
        "\n",
        "oof_avg = np.zeros(n, dtype=np.float32)\n",
        "oof_avg[mask] = (oof_sum[mask] / np.maximum(oof_cnt[mask], 1.0)).astype(np.float32)\n",
        "test_avg = np.mean(test_seed_preds, axis=0).astype(np.float32)\n",
        "auc_oof = roc_auc_score(y[mask], oof_avg[mask])\n",
        "print(f'Dense v2 Time-CV OOF AUC (validated only, 3-seed avg): {auc_oof:.5f}')\n",
        "np.save('oof_xgb_dense_time_v2.npy', oof_avg.astype(np.float32))\n",
        "np.save('test_xgb_dense_time_v2.npy', test_avg)\n",
        "print('Saved oof_xgb_dense_time_v2.npy and test_xgb_dense_time_v2.npy')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense v2 Time-CV: 5 folds; validated 2398/2878\n=== Dense v2 Seed 42 (1/3) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 42 Fold 1] TF-IDF 1.5s | W:(480, 3990) C:(480, 13463)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 42 Fold 1] SVD 1.6s | EV W:0.723 C:0.493\n[v2 Seed 42 Fold 1] Stack/scale 0.0s | tr:(480, 384) va:(480, 384) te:(1162, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 42 Fold 1] Train 0.8s | best_iter=4 | AUC: 0.69459 | total 3.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 42 Fold 2] TF-IDF 1.8s | W:(960, 7415) C:(960, 19786)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 42 Fold 2] SVD 2.7s | EV W:0.482 C:0.359\n[v2 Seed 42 Fold 2] Stack/scale 0.0s | tr:(960, 384) va:(480, 384) te:(1162, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 42 Fold 2] Train 0.8s | best_iter=4 | AUC: 0.67715 | total 5.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 42 Fold 3] TF-IDF 2.1s | W:(1440, 10152) C:(1440, 24292)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 42 Fold 3] SVD 4.0s | EV W:0.388 C:0.309\n[v2 Seed 42 Fold 3] Stack/scale 0.0s | tr:(1440, 384) va:(480, 384) te:(1162, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 42 Fold 3] Train 0.8s | best_iter=10 | AUC: 0.61439 | total 7.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 42 Fold 4] TF-IDF 2.3s | W:(1920, 12497) C:(1920, 27408)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 42 Fold 4] SVD 5.3s | EV W:0.337 C:0.285\n[v2 Seed 42 Fold 4] Stack/scale 0.0s | tr:(1920, 384) va:(479, 384) te:(1162, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 42 Fold 4] Train 0.9s | best_iter=43 | AUC: 0.61612 | total 8.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 42 Fold 5] TF-IDF 2.6s | W:(2399, 14639) C:(2399, 30209)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 42 Fold 5] SVD 6.1s | EV W:0.305 C:0.270\n[v2 Seed 42 Fold 5] Stack/scale 0.0s | tr:(2399, 384) va:(479, 384) te:(1162, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 42 Fold 5] Train 2.7s | best_iter=816 | AUC: 0.60797 | total 11.5s\n[Dense v2 Seed 42] OOF AUC (validated only): 0.62501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Dense v2 Seed 1337 (2/3) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 1337 Fold 1] TF-IDF 1.5s | W:(480, 3990) C:(480, 13463)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 1337 Fold 1] SVD 1.5s | EV W:0.723 C:0.493\n[v2 Seed 1337 Fold 1] Stack/scale 0.0s | tr:(480, 384) va:(480, 384) te:(1162, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 1337 Fold 1] Train 1.0s | best_iter=89 | AUC: 0.67355 | total 4.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 1337 Fold 2] TF-IDF 1.8s | W:(960, 7415) C:(960, 19786)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 1337 Fold 2] SVD 2.7s | EV W:0.482 C:0.359\n[v2 Seed 1337 Fold 2] Stack/scale 0.0s | tr:(960, 384) va:(480, 384) te:(1162, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 1337 Fold 2] Train 0.9s | best_iter=43 | AUC: 0.67412 | total 5.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 1337 Fold 3] TF-IDF 2.2s | W:(1440, 10152) C:(1440, 24292)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 1337 Fold 3] SVD 4.0s | EV W:0.388 C:0.309\n[v2 Seed 1337 Fold 3] Stack/scale 0.0s | tr:(1440, 384) va:(480, 384) te:(1162, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 1337 Fold 3] Train 0.8s | best_iter=17 | AUC: 0.61999 | total 7.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 1337 Fold 4] TF-IDF 2.4s | W:(1920, 12497) C:(1920, 27408)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 1337 Fold 4] SVD 5.4s | EV W:0.337 C:0.285\n[v2 Seed 1337 Fold 4] Stack/scale 0.0s | tr:(1920, 384) va:(479, 384) te:(1162, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 1337 Fold 4] Train 1.6s | best_iter=360 | AUC: 0.62720 | total 9.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 1337 Fold 5] TF-IDF 2.7s | W:(2399, 14639) C:(2399, 30209)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 1337 Fold 5] SVD 6.2s | EV W:0.305 C:0.270\n[v2 Seed 1337 Fold 5] Stack/scale 0.0s | tr:(2399, 384) va:(479, 384) te:(1162, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 1337 Fold 5] Train 0.9s | best_iter=5 | AUC: 0.61138 | total 9.9s\n[Dense v2 Seed 1337] OOF AUC (validated only): 0.62268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Dense v2 Seed 2025 (3/3) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 2025 Fold 1] TF-IDF 1.5s | W:(480, 3990) C:(480, 13463)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 2025 Fold 1] SVD 1.6s | EV W:0.723 C:0.493\n[v2 Seed 2025 Fold 1] Stack/scale 0.0s | tr:(480, 384) va:(480, 384) te:(1162, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 2025 Fold 1] Train 1.0s | best_iter=96 | AUC: 0.67896 | total 4.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 2025 Fold 2] TF-IDF 1.8s | W:(960, 7415) C:(960, 19786)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 2025 Fold 2] SVD 2.9s | EV W:0.482 C:0.359\n[v2 Seed 2025 Fold 2] Stack/scale 0.0s | tr:(960, 384) va:(480, 384) te:(1162, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 2025 Fold 2] Train 0.8s | best_iter=16 | AUC: 0.67580 | total 5.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 2025 Fold 3] TF-IDF 2.3s | W:(1440, 10152) C:(1440, 24292)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 2025 Fold 3] SVD 4.0s | EV W:0.388 C:0.309\n[v2 Seed 2025 Fold 3] Stack/scale 0.0s | tr:(1440, 384) va:(480, 384) te:(1162, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 2025 Fold 3] Train 0.8s | best_iter=10 | AUC: 0.61992 | total 7.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 2025 Fold 4] TF-IDF 2.5s | W:(1920, 12497) C:(1920, 27408)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 2025 Fold 4] SVD 5.5s | EV W:0.337 C:0.285\n[v2 Seed 2025 Fold 4] Stack/scale 0.0s | tr:(1920, 384) va:(479, 384) te:(1162, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 2025 Fold 4] Train 0.9s | best_iter=24 | AUC: 0.62906 | total 8.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2992723547.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 2025 Fold 5] TF-IDF 2.7s | W:(2399, 14639) C:(2399, 30209)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 2025 Fold 5] SVD 6.2s | EV W:0.305 C:0.270\n[v2 Seed 2025 Fold 5] Stack/scale 0.0s | tr:(2399, 384) va:(479, 384) te:(1162, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v2 Seed 2025 Fold 5] Train 1.7s | best_iter=406 | AUC: 0.61535 | total 10.7s\n[Dense v2 Seed 2025] OOF AUC (validated only): 0.63284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense v2 Time-CV OOF AUC (validated only, 3-seed avg): 0.64086\nSaved oof_xgb_dense_time_v2.npy and test_xgb_dense_time_v2.npy\n"
          ]
        }
      ]
    },
    {
      "id": "405c8d4e-b632-42fd-909e-b2d88ab30972",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S13: Meta-only XGB (time-aware, 6-block CV, per-fold scale_pos_weight), 3-seed bag; cache OOF/test\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import xgboost as xgb\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body(df):\n",
        "    return df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "\n",
        "def build_meta(df):\n",
        "    title = get_title(df); body = get_body(df)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    out['title_len'] = title.str.len().astype(np.float32)\n",
        "    out['body_len'] = body.str.len().astype(np.float32)\n",
        "    out['title_body_ratio'] = (out['title_len'] / (1.0 + out['body_len'])).astype(np.float32)\n",
        "    out['has_url'] = body.str.contains(r'https?://', regex=True).astype(np.float32)\n",
        "    out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n",
        "    if 'unix_timestamp_of_request' in df.columns:\n",
        "        dt = pd.to_datetime(df['unix_timestamp_of_request'], unit='s', utc=True, errors='coerce')\n",
        "    else:\n",
        "        dt = pd.to_datetime(0, unit='s', utc=True) + pd.to_timedelta(np.zeros(len(df)), unit='s')\n",
        "    hour = dt.dt.hour.fillna(0).astype(np.float32)\n",
        "    out['hour'] = hour\n",
        "    out['dayofweek'] = dt.dt.dayofweek.fillna(0).astype(np.float32)\n",
        "    out['is_weekend'] = out['dayofweek'].isin([5,6]).astype(np.float32)\n",
        "    out['hour_sin'] = np.sin(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    out['hour_cos'] = np.cos(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    for c in [\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_posts_at_request'\n",
        "    ]:\n",
        "        if c in df.columns:\n",
        "            out[c] = pd.to_numeric(df[c], errors='coerce').astype(np.float32)\n",
        "        else:\n",
        "            out[c] = 0.0\n",
        "    for c in ['title_len','body_len','title_body_ratio',\n",
        "              'requester_upvotes_minus_downvotes_at_request',\n",
        "              'requester_upvotes_plus_downvotes_at_request',\n",
        "              'requester_number_of_comments_at_request',\n",
        "              'requester_number_of_posts_at_request']:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(out[c].clip(lower=0)).astype(np.float32)\n",
        "    out = out.replace([np.inf,-np.inf], 0).fillna(0).astype(np.float32)\n",
        "    return out\n",
        "\n",
        "# 6-block forward-chaining folds and mask\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'Meta-only Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}')\n",
        "\n",
        "# Precompute meta for test\n",
        "meta_te_full = build_meta(test).astype(np.float32).values\n",
        "\n",
        "# XGB params for meta-only model\n",
        "base_params = dict(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='auc',\n",
        "    max_depth=3,\n",
        "    eta=0.05,\n",
        "    min_child_weight=8,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.5,\n",
        "    reg_lambda=3.0,\n",
        "    gamma=0.0,\n",
        "    device='cuda',\n",
        "    tree_method='hist'\n",
        ")\n",
        "num_boost_round = 4000\n",
        "early_stopping_rounds = 200\n",
        "seeds = [42, 1337, 2025]\n",
        "\n",
        "oof_sum = np.zeros(n, dtype=np.float64)\n",
        "oof_cnt = np.zeros(n, dtype=np.float64)\n",
        "test_seed_preds = []\n",
        "\n",
        "for si, seed in enumerate(seeds, 1):\n",
        "    print(f'=== Meta Seed {seed} ({si}/{len(seeds)}) ===')\n",
        "    params = dict(base_params); params['seed'] = seed\n",
        "    oof_seed = np.zeros(n, dtype=np.float32)\n",
        "    test_folds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "        t0 = time.time()\n",
        "        M_tr = build_meta(train.iloc[tr_idx]).astype(np.float32).values\n",
        "        M_va = build_meta(train.iloc[va_idx]).astype(np.float32).values\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "        Xtr = scaler.fit_transform(M_tr).astype(np.float32)\n",
        "        Xva = scaler.transform(M_va).astype(np.float32)\n",
        "        Xte = scaler.transform(meta_te_full).astype(np.float32)\n",
        "        pos = float((y[tr_idx] == 1).sum()); neg = float((y[tr_idx] == 0).sum())\n",
        "        params['scale_pos_weight'] = (neg / max(pos, 1.0)) if pos > 0 else 1.0\n",
        "        dtrain = xgb.DMatrix(Xtr, label=y[tr_idx])\n",
        "        dvalid = xgb.DMatrix(Xva, label=y[va_idx])\n",
        "        dtest  = xgb.DMatrix(Xte)\n",
        "        booster = xgb.train(params, dtrain, num_boost_round=num_boost_round, evals=[(dvalid, 'valid')], early_stopping_rounds=early_stopping_rounds, verbose_eval=False)\n",
        "        va_pred = booster.predict(dvalid, iteration_range=(0, booster.best_iteration+1)).astype(np.float32)\n",
        "        te_pred = booster.predict(dtest, iteration_range=(0, booster.best_iteration+1)).astype(np.float32)\n",
        "        oof_seed[va_idx] = va_pred; test_folds.append(te_pred)\n",
        "        auc = roc_auc_score(y[va_idx], va_pred)\n",
        "        print(f'[Meta Seed {seed} Fold {fi}] best_iter={booster.best_iteration} | spw={params[\"scale_pos_weight\"]:.2f} | AUC: {auc:.5f} | {time.time()-t0:.1f}s')\n",
        "        del M_tr, M_va, scaler, Xtr, Xva, Xte, dtrain, dvalid, dtest, booster\n",
        "        gc.collect()\n",
        "    seed_auc = roc_auc_score(y[mask], oof_seed[mask])\n",
        "    print(f'[Meta Seed {seed}] OOF AUC (validated only): {seed_auc:.5f}')\n",
        "    oof_sum[mask] += oof_seed[mask]; oof_cnt[mask] += 1.0\n",
        "    test_seed_preds.append(np.mean(test_folds, axis=0).astype(np.float64))\n",
        "    del oof_seed, test_folds; gc.collect()\n",
        "\n",
        "oof_avg = np.zeros(n, dtype=np.float32)\n",
        "oof_avg[mask] = (oof_sum[mask] / np.maximum(oof_cnt[mask], 1.0)).astype(np.float32)\n",
        "test_avg = np.mean(test_seed_preds, axis=0).astype(np.float32)\n",
        "auc_oof = roc_auc_score(y[mask], oof_avg[mask])\n",
        "print(f'Meta-only Time-CV OOF AUC (validated only, 3-seed avg): {auc_oof:.5f}')\n",
        "np.save('oof_xgb_meta_time.npy', oof_avg.astype(np.float32))\n",
        "np.save('test_xgb_meta_time.npy', test_avg)\n",
        "print('Saved oof_xgb_meta_time.npy and test_xgb_meta_time.npy')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-only Time-CV: 5 folds; validated 2398/2878\n=== Meta Seed 42 (1/3) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta Seed 42 Fold 1] best_iter=42 | spw=1.94 | AUC: 0.73802 | 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta Seed 42 Fold 2] best_iter=21 | spw=2.33 | AUC: 0.67327 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta Seed 42 Fold 3] best_iter=67 | spw=2.49 | AUC: 0.63566 | 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta Seed 42 Fold 4] best_iter=6 | spw=2.79 | AUC: 0.63838 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta Seed 42 Fold 5] best_iter=4 | spw=2.83 | AUC: 0.58887 | 0.4s\n[Meta Seed 42] OOF AUC (validated only): 0.65498\n=== Meta Seed 1337 (2/3) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta Seed 1337 Fold 1] best_iter=66 | spw=1.94 | AUC: 0.73519 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta Seed 1337 Fold 2] best_iter=65 | spw=2.33 | AUC: 0.66432 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta Seed 1337 Fold 3] best_iter=1 | spw=2.49 | AUC: 0.63257 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta Seed 1337 Fold 4] best_iter=69 | spw=2.79 | AUC: 0.61290 | 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta Seed 1337 Fold 5] best_iter=487 | spw=2.83 | AUC: 0.59533 | 1.1s\n[Meta Seed 1337] OOF AUC (validated only): 0.64127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Meta Seed 2025 (3/3) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta Seed 2025 Fold 1] best_iter=38 | spw=1.94 | AUC: 0.73981 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta Seed 2025 Fold 2] best_iter=58 | spw=2.33 | AUC: 0.67307 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta Seed 2025 Fold 3] best_iter=25 | spw=2.49 | AUC: 0.63164 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta Seed 2025 Fold 4] best_iter=24 | spw=2.79 | AUC: 0.62542 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/2413334163.py:24: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta Seed 2025 Fold 5] best_iter=207 | spw=2.83 | AUC: 0.59727 | 0.7s\n[Meta Seed 2025] OOF AUC (validated only): 0.64888\nMeta-only Time-CV OOF AUC (validated only, 3-seed avg): 0.65507\nSaved oof_xgb_meta_time.npy and test_xgb_meta_time.npy\n"
          ]
        }
      ]
    },
    {
      "id": "14b54f97-00b7-482c-ac21-16be0403921d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S14: Constrained time-consistent logit blend over 6-block bases (LR_time_meta mix + Dense v1/v2 + Meta_time)\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# 6-block forward-chaining mask (validated indices only)\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    mask[np.array(blocks[i])] = True\n",
        "print(f'Time-CV (6 blocks) validated count: {mask.sum()}/{n}')\n",
        "\n",
        "# Load time-CV base predictions (all 6-block forward-chaining)\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy')\n",
        "o_lr_ns = np.load('oof_lr_time_nosub_meta.npy')\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_time_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta_time.npy')\n",
        "t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "t_meta = np.load('test_xgb_meta_time.npy')\n",
        "\n",
        "# Convert to logits\n",
        "z_lr_w, z_lr_ns = to_logit(o_lr_w), to_logit(o_lr_ns)\n",
        "z_d1, z_d2, z_meta = to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)\n",
        "tz_lr_w, tz_lr_ns = to_logit(t_lr_w), to_logit(t_lr_ns)\n",
        "tz_d1, tz_d2, tz_meta = to_logit(t_d1), to_logit(t_d2), to_logit(t_meta)\n",
        "\n",
        "# Grid per expert constraints (LR-heavy, modest meta, capped dense):\n",
        "# - g in np.arange(0.50, 0.90 + 1e-12, 0.025)  # extended to 0.90 since best sat at 0.85\n",
        "# - meta_w in [0.08, 0.10, 0.12, 0.15, 0.18, 0.20]  # allow a bit more meta\n",
        "# - dense_total in np.arange(0.15, 0.35 + 1e-12, 0.05)  # keep tight\n",
        "# - alpha in {0.2, 0.35, 0.5, 0.65, 0.8}  # split Dense v1/v2\n",
        "# - w_lr = 1 - meta_w - dense_total\n",
        "g_grid = np.arange(0.50, 0.90 + 1e-12, 0.025)\n",
        "meta_grid = [0.08, 0.10, 0.12, 0.15, 0.18, 0.20]\n",
        "dense_tot_grid = np.arange(0.15, 0.35 + 1e-12, 0.05)\n",
        "alpha_grid = [0.2, 0.35, 0.5, 0.65, 0.8]\n",
        "\n",
        "best_auc, best_cfg = -1.0, None\n",
        "t0 = time.time(); tried = 0\n",
        "for g in g_grid:\n",
        "    z_lr_mix = (1.0 - g)*z_lr_w + g*z_lr_ns\n",
        "    tz_lr_mix = (1.0 - g)*tz_lr_w + g*tz_lr_ns\n",
        "    for meta_w in meta_grid:\n",
        "        for d_tot in dense_tot_grid:\n",
        "            w_lr = 1.0 - meta_w - d_tot\n",
        "            if w_lr <= 0 or w_lr >= 1:\n",
        "                continue\n",
        "            for a in alpha_grid:\n",
        "                w_d2 = d_tot * a\n",
        "                w_d1 = d_tot - w_d2\n",
        "                if w_d1 < 0 or w_d2 < 0:\n",
        "                    continue\n",
        "                z_oof = w_lr*z_lr_mix + w_d1*z_d1 + w_d2*z_d2 + meta_w*z_meta\n",
        "                auc = roc_auc_score(y[mask], z_oof[mask])\n",
        "                tried += 1\n",
        "                if auc > best_auc:\n",
        "                    best_auc = auc\n",
        "                    best_cfg = dict(g=float(g), w_lr=float(w_lr), w_d1=float(w_d1), w_d2=float(w_d2), w_meta=float(meta_w), tz_lr_mix=tz_lr_mix)\n",
        "cfg_print = {k: v for k, v in best_cfg.items() if k != 'tz_lr_mix'} if best_cfg is not None else {}\n",
        "print(f'Constrained blend grid tried {tried} configs | Best OOF(z,time-mask) AUC: {best_auc:.5f} | cfg={cfg_print}')\n",
        "\n",
        "# Build primary test submission using best weights\n",
        "g = best_cfg['g']; w_lr = best_cfg['w_lr']; w_d1 = best_cfg['w_d1']; w_d2 = best_cfg['w_d2']; w_meta = best_cfg['w_meta']\n",
        "tz_lr_mix = best_cfg['tz_lr_mix']\n",
        "zt_best = w_lr*tz_lr_mix + w_d1*tz_d1 + w_d2*tz_d2 + w_meta*tz_meta\n",
        "pt_best = sigmoid(zt_best).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: pt_best}).to_csv('submission_time_blend_constrained_best.csv', index=False)\n",
        "\n",
        "# Shrunk variant: 15% toward equal weights across used models (LRmix, D1, D2, Meta)\n",
        "alpha_shrink = 0.15\n",
        "w_vec = np.array([w_lr, w_d1, w_d2, w_meta], dtype=np.float64)\n",
        "w_eq = np.ones_like(w_vec) / 4.0\n",
        "w_shrunk = (1.0 - alpha_shrink)*w_vec + alpha_shrink*w_eq\n",
        "w_shrunk = (w_shrunk / w_shrunk.sum()).astype(np.float64)\n",
        "zt_shrunk = w_shrunk[0]*tz_lr_mix + w_shrunk[1]*tz_d1 + w_shrunk[2]*tz_d2 + w_shrunk[3]*tz_meta\n",
        "pt_shrunk = sigmoid(zt_shrunk).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: pt_shrunk}).to_csv('submission_time_blend_constrained_shrunk.csv', index=False)\n",
        "\n",
        "# Equal-prob hedge across all five bases (prob space) with mild clipping\n",
        "p_eq5 = np.clip((np.clip(sigmoid(tz_lr_w), 1e-6, 1-1e-6) + np.clip(sigmoid(tz_lr_ns), 1e-6, 1-1e-6) + np.clip(t_d1, 1e-6, 1-1e-6) + np.clip(t_d2, 1e-6, 1-1e-6) + np.clip(t_meta, 1e-6, 1-1e-6)) / 5.0, 0.01, 0.99).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: p_eq5}).to_csv('submission_time_equal5_prob.csv', index=False)\n",
        "\n",
        "# Set primary submission\n",
        "pd.DataFrame({id_col: ids, target_col: pt_best}).to_csv('submission.csv', index=False)\n",
        "print('Wrote constrained time-CV blends (best, shrunk) and equal5 hedge; submission.csv set to primary.')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV (6 blocks) validated count: 2398/2878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constrained blend grid tried 2550 configs | Best OOF(z,time-mask) AUC: 0.67198 | cfg={'g': 0.9000000000000004, 'w_lr': 0.44999999999999996, 'w_d1': 0.2800000000000001, 'w_d2': 0.07000000000000002, 'w_meta': 0.2}\nWrote constrained time-CV blends (best, shrunk) and equal5 hedge; submission.csv set to primary.\n"
          ]
        }
      ]
    },
    {
      "id": "4b8c6fb5-4d03-4d6a-9597-16a21dd3dbe3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S15: Time-aware ridge stacker on logits of 5 bases (LR_withsub_meta, LR_nosub_meta, Dense_v1, Dense_v2, Meta_time)\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "# 6-block forward-chaining folds and validated mask\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'Stacker Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}')\n",
        "\n",
        "# Load base predictions (time-consistent 6-block CV) and convert to logits\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy');   t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "o_lr_ns = np.load('oof_lr_time_nosub_meta.npy');    t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy');           t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_time_v2.npy');        t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta_time.npy');          t_meta = np.load('test_xgb_meta_time.npy')\n",
        "\n",
        "Z_oof = np.vstack([to_logit(o_lr_w), to_logit(o_lr_ns), to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)]).T.astype(np.float64)\n",
        "Z_test = np.vstack([to_logit(t_lr_w), to_logit(t_lr_ns), to_logit(t_d1), to_logit(t_d2), to_logit(t_meta)]).T.astype(np.float64)\n",
        "print('Stacker feature shapes:', Z_oof.shape, Z_test.shape)\n",
        "\n",
        "# Alpha grid\n",
        "alpha_grid = [0.1, 0.3, 1.0, 3.0, 10.0, 30.0]\n",
        "best_auc, best_alpha = -1.0, None\n",
        "best_oof_preds = None; best_test_preds = None\n",
        "\n",
        "for ai, alpha in enumerate(alpha_grid, 1):\n",
        "    t0 = time.time()\n",
        "    oof_preds = np.zeros(n, dtype=np.float64)\n",
        "    test_fold_preds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "        Xtr, ytr = Z_oof[tr_idx], y[tr_idx]\n",
        "        Xva = Z_oof[va_idx]\n",
        "        model = Ridge(alpha=alpha, fit_intercept=True, random_state=42)\n",
        "        model.fit(Xtr, ytr)\n",
        "        oof_preds[va_idx] = model.predict(Xva)\n",
        "        test_fold_preds.append(model.predict(Z_test))\n",
        "        if fi % 1 == 0:\n",
        "            print(f'[alpha={alpha}] Fold {fi}/{len(folds)} done')\n",
        "        del Xtr, ytr, Xva, model; gc.collect()\n",
        "    auc = roc_auc_score(y[mask], oof_preds[mask])\n",
        "    test_avg = np.mean(test_fold_preds, axis=0).astype(np.float64)\n",
        "    print(f'Alpha {alpha} | OOF(z,time-mask) AUC: {auc:.5f} | time {time.time()-t0:.1f}s')\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_alpha = auc, alpha\n",
        "        best_oof_preds = oof_preds.copy()\n",
        "        best_test_preds = test_avg.copy()\n",
        "\n",
        "print(f'Best ridge alpha={best_alpha} | OOF(z,time-mask) AUC: {best_auc:.5f}')\n",
        "\n",
        "# Compare to S14 constrained AUC to decide if we promote to primary\n",
        "s14_oof_auc = 0.67173  # from Cell 18 log\n",
        "improvement = best_auc - s14_oof_auc\n",
        "print(f'Improvement over S14: {improvement:.5f}')\n",
        "\n",
        "# Save stacker submission; promote to primary only if >= +0.002 OOF gain\n",
        "pt_stack = 1.0 / (1.0 + np.exp(-best_test_preds))\n",
        "pd.DataFrame({id_col: ids, target_col: pt_stack.astype(np.float32)}).to_csv('submission_time_stacker_ridge.csv', index=False)\n",
        "if improvement >= 0.002:\n",
        "    pd.DataFrame({id_col: ids, target_col: pt_stack.astype(np.float32)}).to_csv('submission.csv', index=False)\n",
        "    print('Promoted stacker to primary submission.csv')\n",
        "else:\n",
        "    print('Kept S14 primary; stacker saved as submission_time_stacker_ridge.csv')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacker Time-CV: 5 folds; validated 2398/2878\nStacker feature shapes: (2878, 5) (1162, 5)\n[alpha=0.1] Fold 1/5 done\n[alpha=0.1] Fold 2/5 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[alpha=0.1] Fold 3/5 done\n[alpha=0.1] Fold 4/5 done\n[alpha=0.1] Fold 5/5 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha 0.1 | OOF(z,time-mask) AUC: 0.56571 | time 0.7s\n[alpha=0.3] Fold 1/5 done\n[alpha=0.3] Fold 2/5 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[alpha=0.3] Fold 3/5 done\n[alpha=0.3] Fold 4/5 done\n[alpha=0.3] Fold 5/5 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha 0.3 | OOF(z,time-mask) AUC: 0.56599 | time 0.7s\n[alpha=1.0] Fold 1/5 done\n[alpha=1.0] Fold 2/5 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[alpha=1.0] Fold 3/5 done\n[alpha=1.0] Fold 4/5 done\n[alpha=1.0] Fold 5/5 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha 1.0 | OOF(z,time-mask) AUC: 0.56658 | time 0.7s\n[alpha=3.0] Fold 1/5 done\n[alpha=3.0] Fold 2/5 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[alpha=3.0] Fold 3/5 done\n[alpha=3.0] Fold 4/5 done\n[alpha=3.0] Fold 5/5 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha 3.0 | OOF(z,time-mask) AUC: 0.56830 | time 0.7s\n[alpha=10.0] Fold 1/5 done\n[alpha=10.0] Fold 2/5 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[alpha=10.0] Fold 3/5 done\n[alpha=10.0] Fold 4/5 done\n[alpha=10.0] Fold 5/5 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha 10.0 | OOF(z,time-mask) AUC: 0.57016 | time 0.7s\n[alpha=30.0] Fold 1/5 done\n[alpha=30.0] Fold 2/5 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[alpha=30.0] Fold 3/5 done\n[alpha=30.0] Fold 4/5 done\n[alpha=30.0] Fold 5/5 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha 30.0 | OOF(z,time-mask) AUC: 0.57190 | time 0.7s\nBest ridge alpha=30.0 | OOF(z,time-mask) AUC: 0.57190\nImprovement over S14: -0.09983\nKept S14 primary; stacker saved as submission_time_stacker_ridge.csv\n"
          ]
        }
      ]
    },
    {
      "id": "5de70a94-7f22-4655-9934-4430986086ec",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S16: NB-SVM (log-count ratio) under 6-block forward-chaining CV with word+char TF-IDF; cache OOF/test\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body(df):\n",
        "    return df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "def combine_text(df):\n",
        "    return (get_title(df) + ' \\n ' + get_body(df)).astype(str)\n",
        "def clean_text_series(s):\n",
        "    s = s.str.lower()\n",
        "    s = s.str.replace(r'https?://\\S+', ' url ', regex=True)\n",
        "    s = s.str.replace(r'\\d+', ' number ', regex=True)\n",
        "    s = s.str.replace(r'\\s+', ' ', regex=True)\n",
        "    return s\n",
        "\n",
        "txt_tr_raw = combine_text(train); txt_te_raw = combine_text(test)\n",
        "txt_tr = clean_text_series(txt_tr_raw); txt_te = clean_text_series(txt_te_raw)\n",
        "\n",
        "# 6-block forward-chaining folds\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'NB-SVM Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}')\n",
        "\n",
        "# Vectorizers\n",
        "tfidf_word_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=80000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "tfidf_char_params = dict(analyzer='char_wb', ngram_range=(3,5), lowercase=True, min_df=3, max_features=60000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "count_word_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=80000, binary=False)\n",
        "count_char_params = dict(analyzer='char_wb', ngram_range=(3,5), lowercase=True, min_df=3, max_features=60000, binary=False)\n",
        "\n",
        "def log_count_ratio(X_counts, y_bin, alpha=1.0):\n",
        "    # X_counts: csr (n_samples x n_features)\n",
        "    y_bin = y_bin.astype(bool)\n",
        "    pos_sum = (X_counts[y_bin].sum(axis=0) + alpha).A1\n",
        "    neg_sum = (X_counts[~y_bin].sum(axis=0) + alpha).A1\n",
        "    r = np.log(pos_sum / neg_sum)\n",
        "    return r.astype(np.float32)\n",
        "\n",
        "oof = np.zeros(n, dtype=np.float32)\n",
        "test_fold_preds = []\n",
        "\n",
        "for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    t0 = time.time()\n",
        "    y_tr = y[tr_idx]\n",
        "    # Fit TF-IDF on train fold\n",
        "    tfidf_w = TfidfVectorizer(**tfidf_word_params)\n",
        "    Xw_tr_tfidf = tfidf_w.fit_transform(txt_tr.iloc[tr_idx]); Xw_va_tfidf = tfidf_w.transform(txt_tr.iloc[va_idx]); Xw_te_tfidf = tfidf_w.transform(txt_te)\n",
        "    tfidf_c = TfidfVectorizer(**tfidf_char_params)\n",
        "    Xc_tr_tfidf = tfidf_c.fit_transform(txt_tr.iloc[tr_idx]); Xc_va_tfidf = tfidf_c.transform(txt_tr.iloc[va_idx]); Xc_te_tfidf = tfidf_c.transform(txt_te)\n",
        "    # Fit CountVectorizer with the same vocabulary as TF-IDF for r\n",
        "    cnt_w = CountVectorizer(**count_word_params, vocabulary=tfidf_w.vocabulary_)\n",
        "    Xw_tr_cnt = cnt_w.fit_transform(txt_tr.iloc[tr_idx])  # fit with fixed vocab for term indexing\n",
        "    cnt_c = CountVectorizer(**count_char_params, vocabulary=tfidf_c.vocabulary_)\n",
        "    Xc_tr_cnt = cnt_c.fit_transform(txt_tr.iloc[tr_idx])\n",
        "    # Compute log-count ratios per view\n",
        "    r_w = log_count_ratio(Xw_tr_cnt, y_tr, alpha=1.0)\n",
        "    r_c = log_count_ratio(Xc_tr_cnt, y_tr, alpha=1.0)\n",
        "    # Apply NB transform: multiply TF-IDF columns by r\n",
        "    Xtr_nb = hstack([Xw_tr_tfidf.multiply(r_w), Xc_tr_tfidf.multiply(r_c)], format='csr')\n",
        "    Xva_nb = hstack([Xw_va_tfidf.multiply(r_w), Xc_va_tfidf.multiply(r_c)], format='csr')\n",
        "    Xte_nb = hstack([Xw_te_tfidf.multiply(r_w), Xc_te_tfidf.multiply(r_c)], format='csr')\n",
        "    # Train logistic regression on NB features\n",
        "    clf = LogisticRegression(solver='saga', penalty='l2', C=2.0, max_iter=4000, n_jobs=-1, random_state=42)\n",
        "    clf.fit(Xtr_nb, y_tr)\n",
        "    va_pred = clf.predict_proba(Xva_nb)[:,1].astype(np.float32)\n",
        "    te_pred = clf.predict_proba(Xte_nb)[:,1].astype(np.float32)\n",
        "    oof[va_idx] = va_pred\n",
        "    test_fold_preds.append(te_pred)\n",
        "    auc = roc_auc_score(y[va_idx], va_pred)\n",
        "    print(f'[NB-SVM] Fold {fi}/{len(folds)} AUC: {auc:.5f} | elapsed {time.time()-t0:.1f}s | shapes tr:{Xtr_nb.shape} va:{Xva_nb.shape}')\n",
        "    del (tfidf_w, tfidf_c, cnt_w, cnt_c, Xw_tr_tfidf, Xw_va_tfidf, Xw_te_tfidf, Xc_tr_tfidf, Xc_va_tfidf, Xc_te_tfidf,\n",
        "         Xw_tr_cnt, Xc_tr_cnt, r_w, r_c, Xtr_nb, Xva_nb, Xte_nb, clf)\n",
        "    gc.collect()\n",
        "\n",
        "auc_oof = roc_auc_score(y[mask], oof[mask])\n",
        "print(f'NB-SVM Time-CV OOF AUC (validated only): {auc_oof:.5f}')\n",
        "test_avg = np.mean(test_fold_preds, axis=0).astype(np.float32)\n",
        "np.save('oof_nbsvm_time.npy', oof.astype(np.float32))\n",
        "np.save('test_nbsvm_time.npy', test_avg)\n",
        "print('Saved oof_nbsvm_time.npy and test_nbsvm_time.npy')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM Time-CV: 5 folds; validated 2398/2878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-SVM] Fold 1/5 AUC: 0.51799 | elapsed 2.0s | shapes tr:(480, 17453) va:(480, 17453)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-SVM] Fold 2/5 AUC: 0.53152 | elapsed 4.5s | shapes tr:(960, 27201) va:(480, 27201)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-SVM] Fold 3/5 AUC: 0.53900 | elapsed 8.2s | shapes tr:(1440, 34444) va:(480, 34444)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-SVM] Fold 4/5 AUC: 0.57269 | elapsed 12.9s | shapes tr:(1920, 39905) va:(479, 39905)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-SVM] Fold 5/5 AUC: 0.59149 | elapsed 15.4s | shapes tr:(2399, 44848) va:(479, 44848)\nNB-SVM Time-CV OOF AUC (validated only): 0.55642\nSaved oof_nbsvm_time.npy and test_nbsvm_time.npy\n"
          ]
        }
      ]
    },
    {
      "id": "3240c248-de54-4ddc-9c07-fb33f5dd0453",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S17: Time-safe target encoding (TE) for requester_subreddits_at_request with m-estimate; cache OOF/train and test TE features\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def as_sub_list(x):\n",
        "    if isinstance(x, (list, tuple)):\n",
        "        return [str(s).lower() for s in x]\n",
        "    return []\n",
        "\n",
        "subs_tr_lists = train.get('requester_subreddits_at_request', pd.Series([[]]*len(train))).apply(as_sub_list)\n",
        "subs_te_lists = test.get('requester_subreddits_at_request', pd.Series([[]]*len(test))).apply(as_sub_list)\n",
        "\n",
        "def logit(p):\n",
        "    p = float(np.clip(p, 0.01, 0.99))\n",
        "    return np.log(p/(1.0-p))\n",
        "\n",
        "# Build 6-block forward-chaining folds and validated mask\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'TE folds: {len(folds)}; validated {mask.sum()}/{n}')\n",
        "\n",
        "m_list = [50, 200]\n",
        "# Features per row: for each m -> mean_logodds, max_logodds, sum_logodds (3*len(m_list));\n",
        "# plus coverage, subs_count_log1p, and logcnt aggregates: mean_logcnt, max_logcnt, sum_logcnt (3)\n",
        "F = 3*len(m_list) + 2 + 3\n",
        "oof_te = np.zeros((n, F), dtype=np.float32)\n",
        "\n",
        "def build_counts(indices):\n",
        "    cnt = Counter(); pos = Counter()\n",
        "    for idx in indices:\n",
        "        labs = int(y[idx])\n",
        "        subs = set(subs_tr_lists.iloc[idx])\n",
        "        if not subs: continue\n",
        "        for s in subs:\n",
        "            cnt[s] += 1\n",
        "            if labs == 1:\n",
        "                pos[s] += 1\n",
        "    return cnt, pos\n",
        "\n",
        "def row_agg(subs, cnt, pos, p_global, m_list):\n",
        "    total = len(subs)\n",
        "    seen = 0\n",
        "    logcnt_vals = []\n",
        "    # precompute smoothed log-odds per m for row\n",
        "    lodds_by_m = [[] for _ in m_list]\n",
        "    for s in subs:\n",
        "        c = cnt.get(s, 0)\n",
        "        if c > 0: seen += 1\n",
        "        pc = pos.get(s, 0)\n",
        "        lc = np.log1p(c)\n",
        "        logcnt_vals.append(lc)\n",
        "        for mi, m in enumerate(m_list):\n",
        "            phat = (pc + m*p_global) / (c + m) if (c + m) > 0 else p_global\n",
        "            lodds_by_m[mi].append(logit(phat))\n",
        "    feats = []\n",
        "    for lodds in lodds_by_m:\n",
        "        if len(lodds) == 0:\n",
        "            feats.extend([logit(p_global), logit(p_global), logit(p_global)])\n",
        "        else:\n",
        "            arr = np.array(lodds, dtype=np.float32)\n",
        "            feats.extend([float(arr.mean()), float(arr.max()), float(arr.sum())])\n",
        "    coverage = (seen / total) if total > 0 else 0.0\n",
        "    subs_count = np.log1p(total)\n",
        "    if len(logcnt_vals) == 0:\n",
        "        feats.extend([0.0, 0.0, 0.0])\n",
        "    else:\n",
        "        arrc = np.array(logcnt_vals, dtype=np.float32)\n",
        "        feats.extend([float(arrc.mean()), float(arrc.max()), float(arrc.sum())])\n",
        "    feats.extend([float(coverage), float(subs_count)])  # add at end to keep order stable with comment above\n",
        "    # Reorder to match declared order: for clarity we defined coverage and subs_count at end; reorder now:\n",
        "    # Declared order: [for m: mean,max,sum]... , coverage, subs_count_log1p, mean_logcnt, max_logcnt, sum_logcnt\n",
        "    # Current feats: [for m: mean,max,sum]*, mean_logcnt, max_logcnt, sum_logcnt, coverage, subs_count\n",
        "    # Fix:\n",
        "    m_feats = feats[:3*len(m_list)]\n",
        "    mean_logcnt, max_logcnt, sum_logcnt, coverage, subs_count = feats[3*len(m_list):]\n",
        "    return m_feats + [coverage, subs_count, mean_logcnt, max_logcnt, sum_logcnt]\n",
        "\n",
        "t0 = time.time()\n",
        "for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    f0 = time.time()\n",
        "    cnt, pos = build_counts(tr_idx)\n",
        "    p_global = float((y[tr_idx] == 1).mean()) if len(tr_idx) > 0 else float((y == 1).mean())\n",
        "    # Fill OOF TE features for this val block\n",
        "    for idx in va_idx:\n",
        "        subs = subs_tr_lists.iloc[idx]\n",
        "        feats = row_agg(subs, cnt, pos, p_global, m_list)\n",
        "        oof_te[idx, :] = np.array(feats, dtype=np.float32)\n",
        "    print(f'[TE] Fold {fi}/{len(folds)} done | train_cnt={len(cnt)} | p_global={p_global:.4f} | elapsed {time.time()-f0:.1f}s')\n",
        "print(f'TE OOF features built in {time.time()-t0:.1f}s')\n",
        "\n",
        "# Build test TE features using full train stats (refit on full train)\n",
        "cnt_full, pos_full = build_counts(np.arange(n))\n",
        "p_global_full = float((y == 1).mean())\n",
        "te_test = np.zeros((len(test), F), dtype=np.float32)\n",
        "for i in range(len(test)):\n",
        "    subs = subs_te_lists.iloc[i]\n",
        "    feats = row_agg(subs, cnt_full, pos_full, p_global_full, m_list)\n",
        "    te_test[i, :] = np.array(feats, dtype=np.float32)\n",
        "print('Test TE features built.')\n",
        "\n",
        "np.save('te_subs_oof.npy', oof_te.astype(np.float32))\n",
        "np.save('te_subs_test.npy', te_test.astype(np.float32))\n",
        "print('Saved te_subs_oof.npy and te_subs_test.npy with shape', oof_te.shape, te_test.shape)\n",
        "\n",
        "# Quick diagnostic: correlate single strongest base logit with a TE signal if available (optional)\n",
        "try:\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    z_lr_ns = np.log(np.clip(np.load('oof_lr_time_nosub_meta.npy'), 1e-6, 1-1e-6) / (1-np.clip(np.load('oof_lr_time_nosub_meta.npy'), 1e-6, 1-1e-6)))\n",
        "    scaler = StandardScaler()\n",
        "    te_std = scaler.fit_transform(oof_te[mask])\n",
        "    corr = np.corrcoef(z_lr_ns[mask], te_std[:,0])[0,1]\n",
        "    print(f'Debug corr(logit LR_nosub_meta, TE_feat0 on validated): {corr:.4f}')\n",
        "except Exception as e:\n",
        "    print('TE debug skipped:', e)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TE folds: 5; validated 2398/2878\n[TE] Fold 1/5 done | train_cnt=968 | p_global=0.3396 | elapsed 0.1s\n[TE] Fold 2/5 done | train_cnt=1730 | p_global=0.3000 | elapsed 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TE] Fold 3/5 done | train_cnt=2569 | p_global=0.2868 | elapsed 0.1s\n[TE] Fold 4/5 done | train_cnt=3571 | p_global=0.2635 | elapsed 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TE] Fold 5/5 done | train_cnt=4880 | p_global=0.2614 | elapsed 0.2s\nTE OOF features built in 0.8s\nTest TE features built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved te_subs_oof.npy and te_subs_test.npy with shape (2878, 11) (1162, 11)\nDebug corr(logit LR_nosub_meta, TE_feat0 on validated): 0.2433\n"
          ]
        }
      ]
    },
    {
      "id": "666b93cc-32d1-4e54-9872-438e5a2eccca",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S18: LR_time with enriched meta + subreddit TE (m=50,200 aggregates), 6-block forward-chaining; cache OOF/test\n",
        "import numpy as np, pandas as pd, gc, time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from collections import Counter\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body(df):\n",
        "    return df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "def build_text(df):\n",
        "    return (get_title(df) + ' \\n ' + get_body(df)).astype(str)\n",
        "def build_subs_str(df):\n",
        "    if 'requester_subreddits_at_request' not in df.columns:\n",
        "        return pd.Series(['']*len(df))\n",
        "    sr = df['requester_subreddits_at_request']\n",
        "    return sr.apply(lambda x: ' '.join([str(s).lower() for s in x]) if isinstance(x, (list, tuple)) else '')\n",
        "def as_sub_list(x):\n",
        "    if isinstance(x, (list, tuple)):\n",
        "        return [str(s).lower() for s in x]\n",
        "    return []\n",
        "subs_tr_lists = train.get('requester_subreddits_at_request', pd.Series([[]]*len(train))).apply(as_sub_list)\n",
        "subs_te_lists = test.get('requester_subreddits_at_request', pd.Series([[]]*len(test))).apply(as_sub_list)\n",
        "\n",
        "def build_meta(df):\n",
        "    title = get_title(df); body = get_body(df)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    out['title_len'] = title.str.len().astype(np.float32)\n",
        "    out['body_len'] = body.str.len().astype(np.float32)\n",
        "    out['title_body_ratio'] = (out['title_len'] / (1.0 + out['body_len'])).astype(np.float32)\n",
        "    out['has_url'] = body.str.contains(r'https?://', regex=True).astype(np.float32)\n",
        "    out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n",
        "    if 'unix_timestamp_of_request' in df.columns:\n",
        "        dt = pd.to_datetime(df['unix_timestamp_of_request'], unit='s', utc=True, errors='coerce')\n",
        "    else:\n",
        "        dt = pd.to_datetime(0, unit='s', utc=True) + pd.to_timedelta(np.zeros(len(df)), unit='s')\n",
        "    hour = dt.dt.hour.fillna(0).astype(np.float32)\n",
        "    out['hour'] = hour\n",
        "    out['dayofweek'] = dt.dt.dayofweek.fillna(0).astype(np.float32)\n",
        "    out['is_weekend'] = out['dayofweek'].isin([5,6]).astype(np.float32)\n",
        "    out['hour_sin'] = np.sin(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    out['hour_cos'] = np.cos(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    for c in [\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_posts_at_request'\n",
        "    ]:\n",
        "        if c in df.columns:\n",
        "            out[c] = pd.to_numeric(df[c], errors='coerce').astype(np.float32)\n",
        "        else:\n",
        "            out[c] = 0.0\n",
        "    for c in ['title_len','body_len','title_body_ratio',\n",
        "              'requester_upvotes_minus_downvotes_at_request',\n",
        "              'requester_upvotes_plus_downvotes_at_request',\n",
        "              'requester_number_of_comments_at_request',\n",
        "              'requester_number_of_posts_at_request']:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(out[c].clip(lower=0)).astype(np.float32)\n",
        "    out = out.replace([np.inf, -np.inf], 0).fillna(0).astype(np.float32)\n",
        "    return out\n",
        "\n",
        "def logit_clip(p):\n",
        "    p = float(np.clip(p, 0.01, 0.99))\n",
        "    return np.log(p/(1.0-p))\n",
        "\n",
        "def build_counts(indices):\n",
        "    cnt = Counter(); pos = Counter()\n",
        "    for idx in indices:\n",
        "        labs = int(y[idx])\n",
        "        subs = set(subs_tr_lists.iloc[idx])\n",
        "        if not subs: continue\n",
        "        for s in subs:\n",
        "            cnt[s] += 1\n",
        "            if labs == 1:\n",
        "                pos[s] += 1\n",
        "    return cnt, pos\n",
        "\n",
        "def row_te_feats(subs, cnt, pos, p_global, m_list=(50,200)):\n",
        "    total = len(subs); seen = 0\n",
        "    logcnt_vals = []\n",
        "    lodds_by_m = [[] for _ in m_list]\n",
        "    for s in subs:\n",
        "        c = cnt.get(s, 0); pc = pos.get(s, 0)\n",
        "        if c > 0: seen += 1\n",
        "        logcnt_vals.append(np.log1p(c))\n",
        "        for mi, m in enumerate(m_list):\n",
        "            ph = (pc + m*p_global) / (c + m) if (c + m) > 0 else p_global\n",
        "            lodds_by_m[mi].append(logit_clip(ph))\n",
        "    feats = []\n",
        "    for lodds in lodds_by_m:\n",
        "        if len(lodds) == 0:\n",
        "            feats.extend([logit_clip(p_global), logit_clip(p_global), logit_clip(p_global)])\n",
        "        else:\n",
        "            arr = np.array(lodds, dtype=np.float32)\n",
        "            feats.extend([float(arr.mean()), float(arr.max()), float(arr.sum())])\n",
        "    coverage = (seen/total) if total > 0 else 0.0\n",
        "    subs_count = np.log1p(total)\n",
        "    if len(logcnt_vals) == 0:\n",
        "        mean_lc = max_lc = sum_lc = 0.0\n",
        "    else:\n",
        "        arrc = np.array(logcnt_vals, dtype=np.float32)\n",
        "        mean_lc, max_lc, sum_lc = float(arrc.mean()), float(arrc.max()), float(arrc.sum())\n",
        "    # Order: for m: mean,max,sum,... then coverage, subs_count, mean_logcnt, max_logcnt, sum_logcnt\n",
        "    feats.extend([coverage, subs_count, mean_lc, max_lc, sum_lc])\n",
        "    return np.array(feats, dtype=np.float32)\n",
        "\n",
        "# 6-block forward-chaining folds (\u2192 5 folds) and mask\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'LR+TE Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}')\n",
        "\n",
        "txt_tr = build_text(train); txt_te = build_text(test)\n",
        "subs_tr_str = build_subs_str(train); subs_te_str = build_subs_str(test)\n",
        "meta_te_base = build_meta(test).astype(np.float32).values\n",
        "\n",
        "word_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=60000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "char_params = dict(analyzer='char_wb', ngram_range=(3,5), lowercase=True, min_df=3, max_features=60000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "subs_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=20000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "\n",
        "def run_lr_time_meta_te(with_subs: bool, tag: str):\n",
        "    t0 = time.time()\n",
        "    oof = np.zeros(n, dtype=np.float32)\n",
        "    test_fold_preds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "        f0 = time.time()\n",
        "        # Text TF-IDF per fold\n",
        "        tfidf_w = TfidfVectorizer(**word_params)\n",
        "        Xw_tr = tfidf_w.fit_transform(txt_tr.iloc[tr_idx]); Xw_va = tfidf_w.transform(txt_tr.iloc[va_idx]); Xw_te = tfidf_w.transform(txt_te)\n",
        "        tfidf_c = TfidfVectorizer(**char_params)\n",
        "        Xc_tr = tfidf_c.fit_transform(txt_tr.iloc[tr_idx]); Xc_va = tfidf_c.transform(txt_tr.iloc[va_idx]); Xc_te = tfidf_c.transform(txt_te)\n",
        "        if with_subs:\n",
        "            tfidf_s = TfidfVectorizer(**subs_params)\n",
        "            Xs_tr = tfidf_s.fit_transform(subs_tr_str.iloc[tr_idx]); Xs_va = tfidf_s.transform(subs_tr_str.iloc[va_idx]); Xs_te = tfidf_s.transform(subs_te_str)\n",
        "        # Meta per fold\n",
        "        meta_tr = build_meta(train.iloc[tr_idx]).astype(np.float32).values\n",
        "        meta_va = build_meta(train.iloc[va_idx]).astype(np.float32).values\n",
        "        # TE per fold: fit counts on train_idx only\n",
        "        cnt, pos = build_counts(tr_idx)\n",
        "        p_global = float((y[tr_idx] == 1).mean()) if len(tr_idx) > 0 else float((y == 1).mean())\n",
        "        F = 3*2 + 2 + 3  # m=50,200 -> 6 + coverage/subs_count + 3 logcnt = 11\n",
        "        te_tr = np.zeros((len(tr_idx), F), dtype=np.float32)\n",
        "        te_va = np.zeros((len(va_idx), F), dtype=np.float32)\n",
        "        for j, idx in enumerate(tr_idx):\n",
        "            te_tr[j, :] = row_te_feats(subs_tr_lists.iloc[idx], cnt, pos, p_global)\n",
        "        for j, idx in enumerate(va_idx):\n",
        "            te_va[j, :] = row_te_feats(subs_tr_lists.iloc[idx], cnt, pos, p_global)\n",
        "        # Test TE using full-train stats each fold (consistent with protocol refit on full train for final test);\n",
        "        # here we approximate by using full-train stats once outside loops is costly to rebuild; compute here quickly.\n",
        "        cnt_full, pos_full = build_counts(np.arange(n))\n",
        "        p_global_full = float((y == 1).mean())\n",
        "        te_te = np.zeros((len(test), F), dtype=np.float32)\n",
        "        for j in range(len(test)):\n",
        "            te_te[j, :] = row_te_feats(subs_te_lists.iloc[j], cnt_full, pos_full, p_global_full)\n",
        "        # Scale meta+TE jointly per fold\n",
        "        meta_tr_te = np.hstack([meta_tr, te_tr]).astype(np.float32)\n",
        "        meta_va_te = np.hstack([meta_va, te_va]).astype(np.float32)\n",
        "        meta_te_all = np.hstack([meta_te_base, te_te]).astype(np.float32)\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "        meta_tr_s = scaler.fit_transform(meta_tr_te).astype(np.float32)\n",
        "        meta_va_s = scaler.transform(meta_va_te).astype(np.float32)\n",
        "        meta_te_s = scaler.transform(meta_te_all).astype(np.float32)\n",
        "        # Stack text + scaled meta+TE\n",
        "        if with_subs:\n",
        "            X_tr = hstack([Xw_tr, Xc_tr, Xs_tr, csr_matrix(meta_tr_s)], format='csr')\n",
        "            X_va = hstack([Xw_va, Xc_va, Xs_va, csr_matrix(meta_va_s)], format='csr')\n",
        "            X_te = hstack([Xw_te, Xc_te, Xs_te, csr_matrix(meta_te_s)], format='csr')\n",
        "        else:\n",
        "            X_tr = hstack([Xw_tr, Xc_tr, csr_matrix(meta_tr_s)], format='csr')\n",
        "            X_va = hstack([Xw_va, Xc_va, csr_matrix(meta_va_s)], format='csr')\n",
        "            X_te = hstack([Xw_te, Xc_te, csr_matrix(meta_te_s)], format='csr')\n",
        "        # Train LR\n",
        "        clf = LogisticRegression(solver='saga', penalty='l2', C=0.8, max_iter=4000, n_jobs=-1, random_state=42)\n",
        "        clf.fit(X_tr, y[tr_idx])\n",
        "        va_pred = clf.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "        te_pred = clf.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "        oof[va_idx] = va_pred; test_fold_preds.append(te_pred)\n",
        "        auc = roc_auc_score(y[va_idx], va_pred)\n",
        "        print(f'[LR+TE {tag}] Fold {fi}/{len(folds)} AUC: {auc:.5f} | {time.time()-f0:.1f}s | shapes tr:{X_tr.shape} va:{X_va.shape}')\n",
        "        # cleanup\n",
        "        del (tfidf_w, tfidf_c, Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, scaler, meta_tr, meta_va, meta_tr_te, meta_va_te, meta_te_all, meta_tr_s, meta_va_s, meta_te_s, X_tr, X_va, X_te, clf, te_tr, te_va, te_te, cnt, pos, cnt_full, pos_full)\n",
        "        if with_subs:\n",
        "            del tfidf_s, Xs_tr, Xs_va, Xs_te\n",
        "        gc.collect()\n",
        "    auc_oof = roc_auc_score(y[mask], oof[mask])\n",
        "    print(f'[LR+TE {tag}] OOF AUC (validated only): {auc_oof:.5f} | total {time.time()-t0:.1f}s')\n",
        "    test_pred = np.mean(test_fold_preds, axis=0).astype(np.float32)\n",
        "    np.save(f'oof_lr_time_{tag}_meta_te.npy', oof.astype(np.float32))\n",
        "    np.save(f'test_lr_time_{tag}_meta_te.npy', test_pred)\n",
        "    return auc_oof\n",
        "\n",
        "auc_with_te = run_lr_time_meta_te(True, 'withsub')\n",
        "auc_nosub_te = run_lr_time_meta_te(False, 'nosub')\n",
        "print({'time_lr_withsub_meta_te': auc_with_te, 'time_lr_nosub_meta_te': auc_nosub_te})"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR+TE Time-CV: 5 folds; validated 2398/2878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3672965514.py:40: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3672965514.py:40: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3672965514.py:40: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR+TE withsub] Fold 1/5 AUC: 0.66812 | 3.7s | shapes tr:(480, 18274) va:(480, 18274)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3672965514.py:40: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3672965514.py:40: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR+TE withsub] Fold 2/5 AUC: 0.62384 | 8.4s | shapes tr:(960, 28965) va:(480, 28965)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3672965514.py:40: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3672965514.py:40: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR+TE withsub] Fold 3/5 AUC: 0.60918 | 14.6s | shapes tr:(1440, 37079) va:(480, 37079)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3672965514.py:40: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3672965514.py:40: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR+TE withsub] Fold 4/5 AUC: 0.58777 | 17.5s | shapes tr:(1920, 43385) va:(479, 43385)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3672965514.py:40: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3672965514.py:40: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR+TE withsub] Fold 5/5 AUC: 0.57336 | 18.4s | shapes tr:(2399, 49550) va:(479, 49550)\n[LR+TE withsub] OOF AUC (validated only): 0.61204 | total 63.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3672965514.py:40: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3672965514.py:40: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR+TE nosub] Fold 1/5 AUC: 0.67468 | 3.5s | shapes tr:(480, 17763) va:(480, 17763)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3672965514.py:40: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3672965514.py:40: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR+TE nosub] Fold 2/5 AUC: 0.63617 | 5.5s | shapes tr:(960, 27930) va:(480, 27930)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3672965514.py:40: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3672965514.py:40: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR+TE nosub] Fold 3/5 AUC: 0.61293 | 8.6s | shapes tr:(1440, 35458) va:(480, 35458)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3672965514.py:40: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3672965514.py:40: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR+TE nosub] Fold 4/5 AUC: 0.59936 | 16.3s | shapes tr:(1920, 41139) va:(479, 41139)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3672965514.py:40: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3672965514.py:40: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR+TE nosub] Fold 5/5 AUC: 0.57731 | 21.4s | shapes tr:(2399, 46292) va:(479, 46292)\n[LR+TE nosub] OOF AUC (validated only): 0.61915 | total 55.9s\n{'time_lr_withsub_meta_te': 0.6120376803743307, 'time_lr_nosub_meta_te': 0.6191540267244493}\n"
          ]
        }
      ]
    },
    {
      "id": "034b721e-2e37-4cf7-b66b-183d3ca0350f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S19: Time-aware LogisticRegression stacker on base logits (5 bases), with per-fold scaling; promote if >= +0.002 over S14\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "# 6-block forward-chaining folds and validated mask\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'Stacker(LR) Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}')\n",
        "\n",
        "# Load base predictions (time-consistent 6-block CV) and convert to logits\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy');   t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "o_lr_ns = np.load('oof_lr_time_nosub_meta.npy');    t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy');           t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_time_v2.npy');        t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta_time.npy');          t_meta = np.load('test_xgb_meta_time.npy')\n",
        "\n",
        "Z_oof_raw = np.vstack([to_logit(o_lr_w), to_logit(o_lr_ns), to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)]).T.astype(np.float64)\n",
        "Z_test_raw = np.vstack([to_logit(t_lr_w), to_logit(t_lr_ns), to_logit(t_d1), to_logit(t_d2), to_logit(t_meta)]).T.astype(np.float64)\n",
        "print('Stacker raw feature shapes:', Z_oof_raw.shape, Z_test_raw.shape)\n",
        "\n",
        "# C grid for LogisticRegression\n",
        "C_grid = [0.1, 0.3, 1.0, 3.0, 10.0]\n",
        "best_auc, best_C = -1.0, None\n",
        "best_oof_scores = None; best_test_scores = None\n",
        "\n",
        "for Ci, C in enumerate(C_grid, 1):\n",
        "    t0 = time.time()\n",
        "    oof_scores = np.zeros(n, dtype=np.float64)\n",
        "    test_fold_scores = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "        Xtr_raw, Xva_raw = Z_oof_raw[tr_idx], Z_oof_raw[va_idx]\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "        Xtr = scaler.fit_transform(Xtr_raw); Xva = scaler.transform(Xva_raw); Xte = scaler.transform(Z_test_raw)\n",
        "        clf = LogisticRegression(penalty='l2', solver='lbfgs', C=C, max_iter=2000, random_state=42)\n",
        "        clf.fit(Xtr, y[tr_idx])\n",
        "        oof_scores[va_idx] = clf.predict_proba(Xva)[:,1]\n",
        "        test_fold_scores.append(clf.predict_proba(Xte)[:,1])\n",
        "        print(f'[C={C}] Fold {fi}/{len(folds)} done')\n",
        "        del Xtr_raw, Xva_raw, Xtr, Xva, Xte, scaler, clf; gc.collect()\n",
        "    auc = roc_auc_score(y[mask], oof_scores[mask])\n",
        "    test_avg = np.mean(test_fold_scores, axis=0).astype(np.float64)\n",
        "    print(f'C={C} | OOF(prob,time-mask) AUC: {auc:.5f} | time {time.time()-t0:.1f}s')\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_C = auc, C\n",
        "        best_oof_scores = oof_scores.copy()\n",
        "        best_test_scores = test_avg.copy()\n",
        "\n",
        "print(f'Best Logistic stacker C={best_C} | OOF(prob,time-mask) AUC: {best_auc:.5f}')\n",
        "\n",
        "# Promote only if >= +0.002 over S14 constrained blend\n",
        "s14_oof_auc = 0.67198\n",
        "improvement = best_auc - s14_oof_auc\n",
        "print(f'Improvement over S14: {improvement:.5f}')\n",
        "\n",
        "sub_stack = pd.DataFrame({id_col: ids, target_col: best_test_scores.astype(np.float32)})\n",
        "sub_stack.to_csv('submission_time_stacker_logreg.csv', index=False)\n",
        "if improvement >= 0.002:\n",
        "    sub_stack.to_csv('submission.csv', index=False)\n",
        "    print('Promoted Logistic stacker to primary submission.csv')\n",
        "else:\n",
        "    print('Kept S14 primary; saved submission_time_stacker_logreg.csv')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacker(LR) Time-CV: 5 folds; validated 2398/2878\nStacker raw feature shapes: (2878, 5) (1162, 5)\n[C=0.1] Fold 1/5 done\n[C=0.1] Fold 2/5 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[C=0.1] Fold 3/5 done\n[C=0.1] Fold 4/5 done\n[C=0.1] Fold 5/5 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=0.1 | OOF(prob,time-mask) AUC: 0.55112 | time 0.7s\n[C=0.3] Fold 1/5 done\n[C=0.3] Fold 2/5 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[C=0.3] Fold 3/5 done\n[C=0.3] Fold 4/5 done\n[C=0.3] Fold 5/5 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=0.3 | OOF(prob,time-mask) AUC: 0.56067 | time 0.7s\n[C=1.0] Fold 1/5 done\n[C=1.0] Fold 2/5 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[C=1.0] Fold 3/5 done\n[C=1.0] Fold 4/5 done\n[C=1.0] Fold 5/5 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=1.0 | OOF(prob,time-mask) AUC: 0.56459 | time 0.7s\n[C=3.0] Fold 1/5 done\n[C=3.0] Fold 2/5 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[C=3.0] Fold 3/5 done\n[C=3.0] Fold 4/5 done\n[C=3.0] Fold 5/5 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=3.0 | OOF(prob,time-mask) AUC: 0.56140 | time 0.7s\n[C=10.0] Fold 1/5 done\n[C=10.0] Fold 2/5 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[C=10.0] Fold 3/5 done\n[C=10.0] Fold 4/5 done\n[C=10.0] Fold 5/5 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=10.0 | OOF(prob,time-mask) AUC: 0.55787 | time 0.7s\nBest Logistic stacker C=1.0 | OOF(prob,time-mask) AUC: 0.56459\nImprovement over S14: -0.10739\nKept S14 primary; saved submission_time_stacker_logreg.csv\n"
          ]
        }
      ]
    },
    {
      "id": "b2f25af4-fe98-4da9-ba83-65194de3b540",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S20: Meta-only XGB v2 with enriched domain features (time-aware 6-block CV), 3-seed bag; cache OOF/test\n",
        "import numpy as np, pandas as pd, time, gc, re\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import xgboost as xgb\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body(df):\n",
        "    return df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "\n",
        "def build_meta_v2(df):\n",
        "    title = get_title(df); body = get_body(df)\n",
        "    txt = (title + ' ' + body).str.lower()\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    # Base lengths\n",
        "    out['title_len'] = title.str.len().astype(np.float32)\n",
        "    out['body_len'] = body.str.len().astype(np.float32)\n",
        "    out['title_body_ratio'] = (out['title_len'] / (1.0 + out['body_len'])).astype(np.float32)\n",
        "    # URL / IMG flags\n",
        "    out['has_url'] = body.str.contains(r'https?://', regex=True).astype(np.float32)\n",
        "    out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n",
        "    # Counts/ratios\n",
        "    out['exclam_count'] = body.str.count('!').astype(np.float32)\n",
        "    out['question_count'] = body.str.count('\\?').astype(np.float32)\n",
        "    out['ellipsis_count'] = body.str.count(r'\\.{3,}').astype(np.float32)\n",
        "    out['url_count'] = body.str.count(r'https?://').astype(np.float32)\n",
        "    out['img_count'] = body.str.count(r'imgur|jpg|jpeg|png|gif').astype(np.float32)\n",
        "    out['number_count'] = body.str.count(r'\\d+').astype(np.float32)\n",
        "    # ALLCAPS ratio and word_allcaps_count\n",
        "    def caps_ratio_func(s):\n",
        "        if not isinstance(s, str) or len(s)==0: return 0.0\n",
        "        caps = sum(1 for ch in s if ch.isupper())\n",
        "        return caps / max(len(s), 1)\n",
        "    out['caps_ratio'] = body.apply(caps_ratio_func).astype(np.float32)\n",
        "    out['word_allcaps_count'] = body.str.findall(r'\\b[A-Z]{2,}\\b').apply(lambda x: len(x) if isinstance(x, list) else 0).astype(np.float32)\n",
        "    # Pronouns\n",
        "    out['i_count'] = txt.str.count(r'\\bi\\b').astype(np.float32)\n",
        "    out['we_count'] = txt.str.count(r'\\bwe\\b').astype(np.float32)\n",
        "    # Gratitude / reciprocity\n",
        "    out['grat_count'] = txt.str.count(r'thank you|thanks|grateful|appreciate').astype(np.float32)\n",
        "    out['recip_count'] = txt.str.count(r'pay it forward|give back|return favor|promise').astype(np.float32)\n",
        "    # Hardship / urgency\n",
        "    out['hard_count'] = txt.str.count(r'broke|rent|bill|student|homeless|hungry|kids|family|unemployed|job').astype(np.float32)\n",
        "    out['urg_count'] = txt.str.count(r'today|tonight|asap|emergency').astype(np.float32)\n",
        "    # Evidence\n",
        "    out['evid_count'] = txt.str.count(r'proof|pic|photo|verify|receipt').astype(np.float32)\n",
        "    # Pizza brands\n",
        "    out['brand_count'] = txt.str.count(r'domino|pizza hut|papa john|little caesars').astype(np.float32)\n",
        "    # Length/structure\n",
        "    token_count = txt.str.count(r'\\w+').astype(np.float32)\n",
        "    sent_count = body.str.count(r'[\\.!?]').astype(np.float32)\n",
        "    out['token_count'] = token_count\n",
        "    out['sentence_count'] = sent_count\n",
        "    # Temporal\n",
        "    if 'unix_timestamp_of_request' in df.columns:\n",
        "        dt = pd.to_datetime(df['unix_timestamp_of_request'], unit='s', utc=True, errors='coerce')\n",
        "    else:\n",
        "        dt = pd.to_datetime(0, unit='s', utc=True) + pd.to_timedelta(np.zeros(len(df)), unit='s')\n",
        "    hour = dt.dt.hour.fillna(0).astype(np.float32)\n",
        "    day = dt.dt.day.fillna(1).astype(np.float32)\n",
        "    dow = dt.dt.dayofweek.fillna(0).astype(np.float32)\n",
        "    out['dayofweek'] = dow\n",
        "    out['is_weekend'] = dow.isin([5, 6]).astype(np.float32)\n",
        "    out['end_of_month'] = (day >= 27).astype(np.float32)\n",
        "    out['is_month_start'] = (day <= 5).astype(np.float32)\n",
        "    out['end_of_week'] = dow.isin([4, 5]).astype(np.float32)\n",
        "    out['hour_sin'] = np.sin(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    out['hour_cos'] = np.cos(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    # Account proxies\n",
        "    for c in [\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_posts_at_request'\n",
        "    ]:\n",
        "        if c in df.columns:\n",
        "            out[c] = pd.to_numeric(df[c], errors='coerce').astype(np.float32)\n",
        "        else:\n",
        "            out[c] = 0.0\n",
        "    # log1p heavy tails\n",
        "    for c in out.columns:\n",
        "        if out[c].dtype != np.float32: continue\n",
        "        out[c] = out[c].replace([np.inf, -np.inf], 0).fillna(0)\n",
        "    heavy = ['title_len','body_len','title_body_ratio','exclam_count','question_count','ellipsis_count','url_count','img_count','number_count','word_allcaps_count','i_count','we_count','grat_count','recip_count','hard_count','urg_count','evid_count','brand_count','token_count','sentence_count','requester_upvotes_minus_downvotes_at_request','requester_upvotes_plus_downvotes_at_request','requester_number_of_comments_at_request','requester_number_of_posts_at_request']\n",
        "    for c in heavy:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(out[c].clip(lower=0)).astype(np.float32)\n",
        "    out = out.replace([np.inf, -np.inf], 0).fillna(0).astype(np.float32)\n",
        "    return out\n",
        "\n",
        "# 6-block forward-chaining folds and mask\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'Meta v2 Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}')\n",
        "\n",
        "meta_te_full = build_meta_v2(test).astype(np.float32).values\n",
        "\n",
        "base_params = dict(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='auc',\n",
        "    max_depth=3,\n",
        "    eta=0.05,\n",
        "    min_child_weight=8,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.8,\n",
        "    reg_lambda=4.0,\n",
        "    gamma=0.0,\n",
        "    device='cuda',\n",
        "    tree_method='hist'\n",
        ")\n",
        "num_boost_round = 5000\n",
        "early_stopping_rounds = 300\n",
        "seeds = [42, 1337, 2025]\n",
        "\n",
        "oof_sum = np.zeros(n, dtype=np.float64)\n",
        "oof_cnt = np.zeros(n, dtype=np.float64)\n",
        "test_seed_preds = []\n",
        "\n",
        "for si, seed in enumerate(seeds, 1):\n",
        "    print(f'=== Meta v2 Seed {seed} ({si}/{len(seeds)}) ===')\n",
        "    params = dict(base_params); params['seed'] = seed\n",
        "    oof_seed = np.zeros(n, dtype=np.float32)\n",
        "    test_folds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "        t0 = time.time()\n",
        "        M_tr = build_meta_v2(train.iloc[tr_idx]).astype(np.float32).values\n",
        "        M_va = build_meta_v2(train.iloc[va_idx]).astype(np.float32).values\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "        Xtr = scaler.fit_transform(M_tr).astype(np.float32)\n",
        "        Xva = scaler.transform(M_va).astype(np.float32)\n",
        "        Xte = scaler.transform(meta_te_full).astype(np.float32)\n",
        "        pos = float((y[tr_idx] == 1).sum()); neg = float((y[tr_idx] == 0).sum())\n",
        "        params['scale_pos_weight'] = (neg / max(pos, 1.0)) if pos > 0 else 1.0\n",
        "        dtrain = xgb.DMatrix(Xtr, label=y[tr_idx])\n",
        "        dvalid = xgb.DMatrix(Xva, label=y[va_idx])\n",
        "        dtest  = xgb.DMatrix(Xte)\n",
        "        booster = xgb.train(params, dtrain, num_boost_round=num_boost_round, evals=[(dvalid, 'valid')], early_stopping_rounds=early_stopping_rounds, verbose_eval=False)\n",
        "        va_pred = booster.predict(dvalid, iteration_range=(0, booster.best_iteration+1)).astype(np.float32)\n",
        "        te_pred = booster.predict(dtest, iteration_range=(0, booster.best_iteration+1)).astype(np.float32)\n",
        "        oof_seed[va_idx] = va_pred; test_folds.append(te_pred)\n",
        "        auc = roc_auc_score(y[va_idx], va_pred)\n",
        "        print(f'[Meta v2 Seed {seed} Fold {fi}] best_iter={booster.best_iteration} | spw={params[\"scale_pos_weight\"]:.2f} | AUC: {auc:.5f} | {time.time()-t0:.1f}s')\n",
        "        del M_tr, M_va, scaler, Xtr, Xva, Xte, dtrain, dvalid, dtest, booster\n",
        "        gc.collect()\n",
        "    seed_auc = roc_auc_score(y[mask], oof_seed[mask])\n",
        "    print(f'[Meta v2 Seed {seed}] OOF AUC (validated only): {seed_auc:.5f}')\n",
        "    oof_sum[mask] += oof_seed[mask]; oof_cnt[mask] += 1.0\n",
        "    test_seed_preds.append(np.mean(test_folds, axis=0).astype(np.float64))\n",
        "    del oof_seed, test_folds; gc.collect()\n",
        "\n",
        "oof_avg = np.zeros(n, dtype=np.float32)\n",
        "oof_avg[mask] = (oof_sum[mask] / np.maximum(oof_cnt[mask], 1.0)).astype(np.float32)\n",
        "test_avg = np.mean(test_seed_preds, axis=0).astype(np.float32)\n",
        "auc_oof = roc_auc_score(y[mask], oof_avg[mask])\n",
        "print(f'Meta v2 Time-CV OOF AUC (validated only, 3-seed avg): {auc_oof:.5f}')\n",
        "np.save('oof_xgb_meta_time_v2.npy', oof_avg.astype(np.float32))\n",
        "np.save('test_xgb_meta_time_v2.npy', test_avg)\n",
        "print('Saved oof_xgb_meta_time_v2.npy and test_xgb_meta_time_v2.npy')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta v2 Time-CV: 5 folds; validated 2398/2878\n=== Meta v2 Seed 42 (1/3) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta v2 Seed 42 Fold 1] best_iter=32 | spw=1.94 | AUC: 0.68917 | 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta v2 Seed 42 Fold 2] best_iter=70 | spw=2.33 | AUC: 0.67833 | 0.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta v2 Seed 42 Fold 3] best_iter=13 | spw=2.49 | AUC: 0.61383 | 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta v2 Seed 42 Fold 4] best_iter=3 | spw=2.79 | AUC: 0.62771 | 0.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta v2 Seed 42 Fold 5] best_iter=427 | spw=2.83 | AUC: 0.62892 | 1.6s\n[Meta v2 Seed 42] OOF AUC (validated only): 0.64297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Meta v2 Seed 1337 (2/3) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta v2 Seed 1337 Fold 1] best_iter=2 | spw=1.94 | AUC: 0.70038 | 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta v2 Seed 1337 Fold 2] best_iter=42 | spw=2.33 | AUC: 0.68153 | 0.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta v2 Seed 1337 Fold 3] best_iter=1 | spw=2.49 | AUC: 0.61514 | 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta v2 Seed 1337 Fold 4] best_iter=6 | spw=2.79 | AUC: 0.63437 | 0.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta v2 Seed 1337 Fold 5] best_iter=391 | spw=2.83 | AUC: 0.62453 | 1.7s\n[Meta v2 Seed 1337] OOF AUC (validated only): 0.64275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Meta v2 Seed 2025 (3/3) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta v2 Seed 2025 Fold 1] best_iter=71 | spw=1.94 | AUC: 0.68500 | 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta v2 Seed 2025 Fold 2] best_iter=30 | spw=2.33 | AUC: 0.66752 | 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta v2 Seed 2025 Fold 3] best_iter=168 | spw=2.49 | AUC: 0.61635 | 1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta v2 Seed 2025 Fold 4] best_iter=55 | spw=2.79 | AUC: 0.62501 | 1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1904270908.py:27: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Meta v2 Seed 2025 Fold 5] best_iter=753 | spw=2.83 | AUC: 0.61686 | 2.1s\n[Meta v2 Seed 2025] OOF AUC (validated only): 0.64438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta v2 Time-CV OOF AUC (validated only, 3-seed avg): 0.64897\nSaved oof_xgb_meta_time_v2.npy and test_xgb_meta_time_v2.npy\n"
          ]
        }
      ]
    },
    {
      "id": "dcc4c945-4b3f-4066-b21e-28cf3b33b49b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S21: NB-SVM (counts-based, binary=True) under 6-block forward-chaining CV; grid C in [2,4,8]; cache OOF/test\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body(df):\n",
        "    return df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "def combine_text(df):\n",
        "    return (get_title(df) + ' \\n ' + get_body(df)).astype(str)\n",
        "def clean_text_series(s):\n",
        "    s = s.str.lower()\n",
        "    s = s.str.replace(r'https?://\\S+', ' url ', regex=True)\n",
        "    s = s.str.replace(r'\\d+', ' number ', regex=True)\n",
        "    s = s.str.replace(r'\\s+', ' ', regex=True)\n",
        "    return s\n",
        "\n",
        "txt_tr_raw = combine_text(train); txt_te_raw = combine_text(test)\n",
        "txt_tr = clean_text_series(txt_tr_raw); txt_te = clean_text_series(txt_te_raw)\n",
        "\n",
        "# 6-block forward-chaining folds\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'NB-SVM (counts) Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}')\n",
        "\n",
        "# Count vectorizers (binary=True); word 1-2 and char_wb 3-5\n",
        "cnt_word_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=100000, binary=True)\n",
        "cnt_char_params = dict(analyzer='char_wb', ngram_range=(3,5), lowercase=True, min_df=3, max_features=80000, binary=True)\n",
        "\n",
        "def log_count_ratio(X_counts, y_bin, alpha=1.0):\n",
        "    yb = y_bin.astype(bool)\n",
        "    pos_sum = (X_counts[yb].sum(axis=0) + alpha).A1\n",
        "    neg_sum = (X_counts[~yb].sum(axis=0) + alpha).A1\n",
        "    return np.log(pos_sum / neg_sum).astype(np.float32)\n",
        "\n",
        "C_grid = [2.0, 4.0, 8.0]\n",
        "best_auc, best_C = -1.0, None\n",
        "best_oof = None; best_test = None\n",
        "\n",
        "for Ci, C in enumerate(C_grid, 1):\n",
        "    tC = time.time()\n",
        "    oof = np.zeros(n, dtype=np.float32)\n",
        "    test_fold_preds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "        t0 = time.time()\n",
        "        y_tr = y[tr_idx]\n",
        "        # Fit count vectorizers on train fold\n",
        "        cnt_w = CountVectorizer(**cnt_word_params)\n",
        "        Xw_tr = cnt_w.fit_transform(txt_tr.iloc[tr_idx]); Xw_va = cnt_w.transform(txt_tr.iloc[va_idx]); Xw_te = cnt_w.transform(txt_te)\n",
        "        cnt_c = CountVectorizer(**cnt_char_params)\n",
        "        Xc_tr = cnt_c.fit_transform(txt_tr.iloc[tr_idx]); Xc_va = cnt_c.transform(txt_tr.iloc[va_idx]); Xc_te = cnt_c.transform(txt_te)\n",
        "        # Compute log-count ratios\n",
        "        r_w = log_count_ratio(Xw_tr, y_tr, alpha=1.0)\n",
        "        r_c = log_count_ratio(Xc_tr, y_tr, alpha=1.0)\n",
        "        # NB transform: multiply columns by r\n",
        "        Xtr_nb = hstack([Xw_tr.multiply(r_w), Xc_tr.multiply(r_c)], format='csr')\n",
        "        Xva_nb = hstack([Xw_va.multiply(r_w), Xc_va.multiply(r_c)], format='csr')\n",
        "        Xte_nb = hstack([Xw_te.multiply(r_w), Xc_te.multiply(r_c)], format='csr')\n",
        "        # Train LogisticRegression on NB features\n",
        "        clf = LogisticRegression(solver='saga', penalty='l2', C=C, max_iter=4000, n_jobs=-1, random_state=42)\n",
        "        clf.fit(Xtr_nb, y_tr)\n",
        "        va_pred = clf.predict_proba(Xva_nb)[:,1].astype(np.float32)\n",
        "        te_pred = clf.predict_proba(Xte_nb)[:,1].astype(np.float32)\n",
        "        oof[va_idx] = va_pred; test_fold_preds.append(te_pred)\n",
        "        auc = roc_auc_score(y[va_idx], va_pred)\n",
        "        print(f'[NB-Counts C={C}] Fold {fi}/{len(folds)} AUC: {auc:.5f} | {time.time()-t0:.1f}s | shapes tr:{Xtr_nb.shape} va:{Xva_nb.shape}')\n",
        "        del (cnt_w, cnt_c, Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, r_w, r_c, Xtr_nb, Xva_nb, Xte_nb, clf); gc.collect()\n",
        "    auc_oof = roc_auc_score(y[mask], oof[mask])\n",
        "    test_avg = np.mean(test_fold_preds, axis=0).astype(np.float32)\n",
        "    print(f'[NB-Counts] C={C} | OOF AUC (validated only): {auc_oof:.5f} | total {time.time()-tC:.1f}s')\n",
        "    if auc_oof > best_auc:\n",
        "        best_auc, best_C = auc_oof, C\n",
        "        best_oof = oof.copy(); best_test = test_avg.copy()\n",
        "\n",
        "print(f'NB-Counts best C={best_C} | OOF(time-mask) AUC: {best_auc:.5f}')\n",
        "np.save('oof_nb_counts_time.npy', best_oof.astype(np.float32))\n",
        "np.save('test_nb_counts_time.npy', best_test.astype(np.float32))\n",
        "print('Saved oof_nb_counts_time.npy and test_nb_counts_time.npy')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM (counts) Time-CV: 5 folds; validated 2398/2878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-Counts C=2.0] Fold 1/5 AUC: 0.55299 | 5.8s | shapes tr:(480, 17453) va:(480, 17453)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-Counts C=2.0] Fold 2/5 AUC: 0.52408 | 12.8s | shapes tr:(960, 27201) va:(480, 27201)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-Counts C=2.0] Fold 3/5 AUC: 0.51316 | 25.9s | shapes tr:(1440, 34444) va:(480, 34444)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-Counts C=2.0] Fold 4/5 AUC: 0.53523 | 37.9s | shapes tr:(1920, 39905) va:(479, 39905)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-Counts C=2.0] Fold 5/5 AUC: 0.58091 | 43.7s | shapes tr:(2399, 44848) va:(479, 44848)\n[NB-Counts] C=2.0 | OOF AUC (validated only): 0.54553 | total 126.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-Counts C=4.0] Fold 1/5 AUC: 0.55274 | 6.6s | shapes tr:(480, 17453) va:(480, 17453)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-Counts C=4.0] Fold 2/5 AUC: 0.52372 | 15.7s | shapes tr:(960, 27201) va:(480, 27201)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-Counts C=4.0] Fold 3/5 AUC: 0.51249 | 30.5s | shapes tr:(1440, 34444) va:(480, 34444)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-Counts C=4.0] Fold 4/5 AUC: 0.53560 | 43.5s | shapes tr:(1920, 39905) va:(479, 39905)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-Counts C=4.0] Fold 5/5 AUC: 0.58109 | 50.2s | shapes tr:(2399, 44848) va:(479, 44848)\n[NB-Counts] C=4.0 | OOF AUC (validated only): 0.54506 | total 147.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-Counts C=8.0] Fold 1/5 AUC: 0.55259 | 7.9s | shapes tr:(480, 17453) va:(480, 17453)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-Counts C=8.0] Fold 2/5 AUC: 0.52293 | 18.6s | shapes tr:(960, 27201) va:(480, 27201)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-Counts C=8.0] Fold 3/5 AUC: 0.51241 | 35.2s | shapes tr:(1440, 34444) va:(480, 34444)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-Counts C=8.0] Fold 4/5 AUC: 0.53553 | 48.2s | shapes tr:(1920, 39905) va:(479, 39905)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-Counts C=8.0] Fold 5/5 AUC: 0.58138 | 55.9s | shapes tr:(2399, 44848) va:(479, 44848)\n[NB-Counts] C=8.0 | OOF AUC (validated only): 0.54450 | total 166.4s\nNB-Counts best C=2.0 | OOF(time-mask) AUC: 0.54553\nSaved oof_nb_counts_time.npy and test_nb_counts_time.npy\n"
          ]
        }
      ]
    },
    {
      "id": "9aa5024b-99fe-4964-bc1c-3939edfd1577",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S14b: Wider-grid hedge blend (allow denser total and meta up to 0.22); do not overwrite primary unless better\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Time-mask\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    mask[np.array(blocks[i])] = True\n",
        "print(f'Time-CV (6 blocks) validated count: {mask.sum()}/{n}')\n",
        "\n",
        "# Load bases\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy'); t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "o_lr_ns = np.load('oof_lr_time_nosub_meta.npy'); t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy'); t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_time_v2.npy'); t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta_time.npy'); t_meta = np.load('test_xgb_meta_time.npy')\n",
        "\n",
        "z_lr_w, z_lr_ns = to_logit(o_lr_w), to_logit(o_lr_ns)\n",
        "z_d1, z_d2, z_meta = to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)\n",
        "tz_lr_w, tz_lr_ns = to_logit(t_lr_w), to_logit(t_lr_ns)\n",
        "tz_d1, tz_d2, tz_meta = to_logit(t_d1), to_logit(t_d2), to_logit(t_meta)\n",
        "\n",
        "# Wider grid (hedge):\n",
        "g_grid = np.arange(0.50, 0.90 + 1e-12, 0.025)\n",
        "meta_grid = [0.08, 0.10, 0.12, 0.15, 0.18, 0.20, 0.22]\n",
        "dense_tot_grid = np.arange(0.15, 0.45 + 1e-12, 0.05)  # extended to 0.45\n",
        "alpha_grid = [0.2, 0.35, 0.5, 0.65, 0.8]\n",
        "\n",
        "best_auc, best_cfg = -1.0, None\n",
        "tried = 0\n",
        "for g in g_grid:\n",
        "    z_lr_mix = (1.0 - g)*z_lr_w + g*z_lr_ns\n",
        "    tz_lr_mix = (1.0 - g)*tz_lr_w + g*tz_lr_ns\n",
        "    for meta_w in meta_grid:\n",
        "        for d_tot in dense_tot_grid:\n",
        "            w_lr = 1.0 - meta_w - d_tot\n",
        "            if w_lr <= 0 or w_lr >= 1:\n",
        "                continue\n",
        "            for a in alpha_grid:\n",
        "                w_d2 = d_tot * a\n",
        "                w_d1 = d_tot - w_d2\n",
        "                if w_d1 < 0 or w_d2 < 0:\n",
        "                    continue\n",
        "                z_oof = w_lr*z_lr_mix + w_d1*z_d1 + w_d2*z_d2 + meta_w*z_meta\n",
        "                auc = roc_auc_score(y[mask], z_oof[mask])\n",
        "                tried += 1\n",
        "                if auc > best_auc:\n",
        "                    best_auc = auc\n",
        "                    best_cfg = dict(g=float(g), w_lr=float(w_lr), w_d1=float(w_d1), w_d2=float(w_d2), w_meta=float(meta_w), tz_lr_mix=tz_lr_mix)\n",
        "cfg_print = {k: v for k, v in best_cfg.items() if k != 'tz_lr_mix'} if best_cfg is not None else {}\n",
        "print(f'S14b wider grid tried {tried} | Best OOF(z,time-mask) AUC: {best_auc:.5f} | cfg={cfg_print}')\n",
        "\n",
        "# Build alt test submission\n",
        "g = best_cfg['g']; w_lr = best_cfg['w_lr']; w_d1 = best_cfg['w_d1']; w_d2 = best_cfg['w_d2']; w_meta = best_cfg['w_meta']\n",
        "tz_lr_mix = best_cfg['tz_lr_mix']\n",
        "zt = w_lr*tz_lr_mix + w_d1*tz_d1 + w_d2*tz_d2 + w_meta*tz_meta\n",
        "pt = sigmoid(zt).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: pt}).to_csv('submission_time_blend_constrained_wide.csv', index=False)\n",
        "print('Wrote submission_time_blend_constrained_wide.csv (not promoted).')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV (6 blocks) validated count: 2398/2878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S14b wider grid tried 4165 | Best OOF(z,time-mask) AUC: 0.67378 | cfg={'g': 0.9000000000000004, 'w_lr': 0.32999999999999996, 'w_d1': 0.36000000000000004, 'w_d2': 0.09000000000000002, 'w_meta': 0.22}\nWrote submission_time_blend_constrained_wide.csv (not promoted).\n"
          ]
        }
      ]
    },
    {
      "id": "906de14e-5756-455c-a823-15a9d2792852",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Promote S14b wider-grid blend to primary submission\n",
        "import pandas as pd\n",
        "src = 'submission_time_blend_constrained_wide.csv'\n",
        "dst = 'submission.csv'\n",
        "sub = pd.read_csv(src)\n",
        "sub.to_csv(dst, index=False)\n",
        "print(f'Promoted {src} to {dst}. Head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promoted submission_time_blend_constrained_wide.csv to submission.csv. Head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.537033\n1   t3_roiuw                  0.511641\n2   t3_mjnbq                  0.395971\n3   t3_t8wd1                  0.442418\n4  t3_1m4zxu                  0.515684\n"
          ]
        }
      ]
    },
    {
      "id": "8a0fabb4-1e06-4617-acd3-b4f5888366ae",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Refit-on-full for 5 core time-consistent bases and final S14b-weighted blend\n",
        "import numpy as np, pandas as pd, time, gc, re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body(df):\n",
        "    return df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "def combine_text(df):\n",
        "    return (get_title(df) + ' \\n ' + get_body(df)).astype(str)\n",
        "def clean_text_series(s):\n",
        "    s = s.str.lower()\n",
        "    s = s.str.replace(r'https?://\\S+', ' url ', regex=True)\n",
        "    s = s.str.replace(r'\\d+', ' number ', regex=True)\n",
        "    s = s.str.replace(r'\\s+', ' ', regex=True)\n",
        "    return s\n",
        "def build_subs(df):\n",
        "    if 'requester_subreddits_at_request' not in df.columns:\n",
        "        return pd.Series(['']*len(df))\n",
        "    sr = df['requester_subreddits_at_request']\n",
        "    return sr.apply(lambda x: ' '.join([str(s).lower() for s in x]) if isinstance(x, (list, tuple)) else '')\n",
        "def build_meta_v1(df):\n",
        "    title = get_title(df); body = get_body(df)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    out['title_len'] = title.str.len().astype(np.float32)\n",
        "    out['body_len'] = body.str.len().astype(np.float32)\n",
        "    out['title_body_ratio'] = (out['title_len'] / (1.0 + out['body_len'])).astype(np.float32)\n",
        "    out['has_url'] = body.str.contains(r'https?://', regex=True).astype(np.float32)\n",
        "    out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n",
        "    if 'unix_timestamp_of_request' in df.columns:\n",
        "        dt = pd.to_datetime(df['unix_timestamp_of_request'], unit='s', utc=True, errors='coerce')\n",
        "    else:\n",
        "        dt = pd.to_datetime(0, unit='s', utc=True) + pd.to_timedelta(np.zeros(len(df)), unit='s')\n",
        "    hour = dt.dt.hour.fillna(0).astype(np.float32)\n",
        "    out['hour'] = hour\n",
        "    out['dayofweek'] = dt.dt.dayofweek.fillna(0).astype(np.float32)\n",
        "    out['is_weekend'] = out['dayofweek'].isin([5,6]).astype(np.float32)\n",
        "    out['hour_sin'] = np.sin(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    out['hour_cos'] = np.cos(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    for c in [\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_posts_at_request'\n",
        "    ]:\n",
        "        if c in df.columns:\n",
        "            out[c] = pd.to_numeric(df[c], errors='coerce').astype(np.float32)\n",
        "        else:\n",
        "            out[c] = 0.0\n",
        "    for c in ['title_len','body_len','title_body_ratio',\n",
        "              'requester_upvotes_minus_downvotes_at_request',\n",
        "              'requester_upvotes_plus_downvotes_at_request',\n",
        "              'requester_number_of_comments_at_request',\n",
        "              'requester_number_of_posts_at_request']:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(out[c].clip(lower=0)).astype(np.float32)\n",
        "    out = out.replace([np.inf,-np.inf], 0).fillna(0).astype(np.float32)\n",
        "    return out\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Common text data\n",
        "txt_tr_raw = combine_text(train); txt_te_raw = combine_text(test)\n",
        "txt_tr = clean_text_series(txt_tr_raw); txt_te = clean_text_series(txt_te_raw)\n",
        "subs_tr = build_subs(train); subs_te = build_subs(test)\n",
        "meta_tr_v1 = build_meta_v1(train).astype(np.float32).values\n",
        "meta_te_v1 = build_meta_v1(test).astype(np.float32).values\n",
        "\n",
        "# Vectorizer params (match time-CV models)\n",
        "word_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=60000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "char_params = dict(analyzer='char_wb', ngram_range=(3,5), lowercase=True, min_df=3, max_features=60000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "subs_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=20000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "\n",
        "print('Refit-on-full: LR_withsub_meta')\n",
        "t0 = time.time()\n",
        "tfidf_w = TfidfVectorizer(**word_params)\n",
        "Xw_tr = tfidf_w.fit_transform(txt_tr); Xw_te = tfidf_w.transform(txt_te)\n",
        "tfidf_c = TfidfVectorizer(**char_params)\n",
        "Xc_tr = tfidf_c.fit_transform(txt_tr); Xc_te = tfidf_c.transform(txt_te)\n",
        "tfidf_s = TfidfVectorizer(**subs_params)\n",
        "Xs_tr = tfidf_s.fit_transform(subs_tr); Xs_te = tfidf_s.transform(subs_te)\n",
        "scaler_meta = StandardScaler(with_mean=True, with_std=True)\n",
        "Mtr_s = scaler_meta.fit_transform(meta_tr_v1).astype(np.float32)\n",
        "Mte_s = scaler_meta.transform(meta_te_v1).astype(np.float32)\n",
        "X_tr_with = hstack([Xw_tr, Xc_tr, Xs_tr, csr_matrix(Mtr_s)], format='csr')\n",
        "X_te_with = hstack([Xw_te, Xc_te, Xs_te, csr_matrix(Mte_s)], format='csr')\n",
        "lr_with = LogisticRegression(solver='saga', penalty='l2', C=0.8, max_iter=4000, n_jobs=-1, random_state=42)\n",
        "lr_with.fit(X_tr_with, y)\n",
        "p_te_lr_with = lr_with.predict_proba(X_te_with)[:,1].astype(np.float32)\n",
        "print(f'LR_withsub_meta trained in {time.time()-t0:.1f}s | shapes tr:{X_tr_with.shape} te:{X_te_with.shape}')\n",
        "\n",
        "print('Refit-on-full: LR_nosub_meta')\n",
        "t1 = time.time()\n",
        "scaler_meta2 = StandardScaler(with_mean=True, with_std=True)\n",
        "Mtr2_s = scaler_meta2.fit_transform(meta_tr_v1).astype(np.float32)\n",
        "Mte2_s = scaler_meta2.transform(meta_te_v1).astype(np.float32)\n",
        "X_tr_nosub = hstack([Xw_tr, Xc_tr, csr_matrix(Mtr2_s)], format='csr')\n",
        "X_te_nosub = hstack([Xw_te, Xc_te, csr_matrix(Mte2_s)], format='csr')\n",
        "lr_ns = LogisticRegression(solver='saga', penalty='l2', C=0.8, max_iter=4000, n_jobs=-1, random_state=42)\n",
        "lr_ns.fit(X_tr_nosub, y)\n",
        "p_te_lr_ns = lr_ns.predict_proba(X_te_nosub)[:,1].astype(np.float32)\n",
        "print(f'LR_nosub_meta trained in {time.time()-t1:.1f}s | shapes tr:{X_tr_nosub.shape} te:{X_te_nosub.shape}')\n",
        "\n",
        "# Dense v1 refit (TF-IDF word/char/subs -> SVD 150/150/50 + meta_v1 -> XGB)\n",
        "print('Refit-on-full: Dense v1 (SVD 150/150/50 + meta) XGB')\n",
        "t2 = time.time()\n",
        "svd_w_v1, svd_c_v1, svd_s_v1 = TruncatedSVD(n_components=150, random_state=42), TruncatedSVD(n_components=150, random_state=42), TruncatedSVD(n_components=50, random_state=42)\n",
        "Zw_tr = svd_w_v1.fit_transform(Xw_tr); Zw_te = svd_w_v1.transform(Xw_te)\n",
        "Zc_tr = svd_c_v1.fit_transform(Xc_tr); Zc_te = svd_c_v1.transform(Xc_te)\n",
        "Zs_tr = svd_s_v1.fit_transform(Xs_tr); Zs_te = svd_s_v1.transform(Xs_te)\n",
        "Xtr_dense_v1 = np.hstack([Zw_tr, Zc_tr, Zs_tr, Mtr_s]).astype(np.float32)\n",
        "Xte_dense_v1 = np.hstack([Zw_te, Zc_te, Zs_te, Mte_s]).astype(np.float32)\n",
        "sc_v1 = StandardScaler(with_mean=True, with_std=True)\n",
        "Xtr_v1 = sc_v1.fit_transform(Xtr_dense_v1); Xte_v1 = sc_v1.transform(Xte_dense_v1)\n",
        "pos = float((y==1).sum()); neg = float((y==0).sum()); spw = (neg/max(pos,1.0)) if pos>0 else 1.0\n",
        "params_v1 = dict(objective='binary:logistic', eval_metric='auc', max_depth=3, eta=0.035, subsample=0.8, colsample_bytree=0.6, min_child_weight=8, reg_alpha=1.0, reg_lambda=4.0, gamma=0.1, device='cuda', tree_method='hist', seed=42, scale_pos_weight=spw)\n",
        "# Use conservative rounds approximating median best_iteration from CV\n",
        "nrounds_v1 = 200\n",
        "dtr_v1 = xgb.DMatrix(Xtr_v1, label=y); dte_v1 = xgb.DMatrix(Xte_v1)\n",
        "booster_v1 = xgb.train(params_v1, dtr_v1, num_boost_round=nrounds_v1, verbose_eval=False)\n",
        "p_te_d1 = booster_v1.predict(dte_v1).astype(np.float32)\n",
        "print(f'Dense v1 trained in {time.time()-t2:.1f}s | nrounds={nrounds_v1} | spw={spw:.2f} | shapes tr:{Xtr_v1.shape}')\n",
        "\n",
        "# Dense v2 refit (no-subs; SVD 250/120 + meta_v1 -> XGB)\n",
        "print('Refit-on-full: Dense v2 (SVD 250/120 + meta) XGB')\n",
        "t3 = time.time()\n",
        "svd_w_v2, svd_c_v2 = TruncatedSVD(n_components=250, random_state=42), TruncatedSVD(n_components=120, random_state=42)\n",
        "Zw2_tr = svd_w_v2.fit_transform(Xw_tr); Zw2_te = svd_w_v2.transform(Xw_te)\n",
        "Zc2_tr = svd_c_v2.fit_transform(Xc_tr); Zc2_te = svd_c_v2.transform(Xc_te)\n",
        "Xtr_dense_v2 = np.hstack([Zw2_tr, Zc2_tr, Mtr_s]).astype(np.float32)\n",
        "Xte_dense_v2 = np.hstack([Zw2_te, Zc2_te, Mte_s]).astype(np.float32)\n",
        "sc_v2 = StandardScaler(with_mean=True, with_std=True)\n",
        "Xtr_v2 = sc_v2.fit_transform(Xtr_dense_v2); Xte_v2 = sc_v2.transform(Xte_dense_v2)\n",
        "pos = float((y==1).sum()); neg = float((y==0).sum()); spw2 = (neg/max(pos,1.0)) if pos>0 else 1.0\n",
        "params_v2 = dict(objective='binary:logistic', eval_metric='auc', max_depth=3, eta=0.03, min_child_weight=10, subsample=0.75, colsample_bytree=0.7, reg_alpha=1.5, reg_lambda=5.0, gamma=0.2, device='cuda', tree_method='hist', seed=42, scale_pos_weight=spw2)\n",
        "nrounds_v2 = 120\n",
        "dtr_v2 = xgb.DMatrix(Xtr_v2, label=y); dte_v2 = xgb.DMatrix(Xte_v2)\n",
        "booster_v2 = xgb.train(params_v2, dtr_v2, num_boost_round=nrounds_v2, verbose_eval=False)\n",
        "p_te_d2 = booster_v2.predict(dte_v2).astype(np.float32)\n",
        "print(f'Dense v2 trained in {time.time()-t3:.1f}s | nrounds={nrounds_v2} | spw={spw2:.2f} | shapes tr:{Xtr_v2.shape}')\n",
        "\n",
        "# Meta-only XGB refit\n",
        "print('Refit-on-full: Meta-only XGB')\n",
        "t4 = time.time()\n",
        "sc_meta = StandardScaler(with_mean=True, with_std=True)\n",
        "Xtr_meta = sc_meta.fit_transform(meta_tr_v1).astype(np.float32)\n",
        "Xte_meta = sc_meta.transform(meta_te_v1).astype(np.float32)\n",
        "pos = float((y==1).sum()); neg = float((y==0).sum()); spw3 = (neg/max(pos,1.0)) if pos>0 else 1.0\n",
        "params_meta = dict(objective='binary:logistic', eval_metric='auc', max_depth=3, eta=0.05, min_child_weight=8, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.5, reg_lambda=3.0, gamma=0.0, device='cuda', tree_method='hist', seed=42, scale_pos_weight=spw3)\n",
        "nrounds_meta = 80\n",
        "dtr_m = xgb.DMatrix(Xtr_meta, label=y); dte_m = xgb.DMatrix(Xte_meta)\n",
        "booster_m = xgb.train(params_meta, dtr_m, num_boost_round=nrounds_meta, verbose_eval=False)\n",
        "p_te_meta = booster_m.predict(dte_m).astype(np.float32)\n",
        "print(f'Meta-only trained in {time.time()-t4:.1f}s | nrounds={nrounds_meta} | spw={spw3:.2f} | feats:{Xtr_meta.shape[1]}')\n",
        "\n",
        "# Apply S14b best weights to refit-on-full predictions\n",
        "print('Blending refit-on-full predictions with S14b weights...')\n",
        "z_lr_w = to_logit(p_te_lr_with); z_lr_ns = to_logit(p_te_lr_ns)\n",
        "z_d1 = to_logit(p_te_d1); z_d2 = to_logit(p_te_d2); z_meta = to_logit(p_te_meta)\n",
        "g = 0.90\n",
        "z_lr_mix = (1.0 - g)*z_lr_w + g*z_lr_ns\n",
        "w_lr, w_d1, w_d2, w_meta = 0.33, 0.36, 0.09, 0.22\n",
        "z_blend = w_lr*z_lr_mix + w_d1*z_d1 + w_d2*z_d2 + w_meta*z_meta\n",
        "p_final = sigmoid(z_blend).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: ids, target_col: p_final})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "sub.to_csv('submission_time_blend_constrained_wide_refit.csv', index=False)\n",
        "print('Saved submission.csv (refit-on-full S14b blend). Head:')\n",
        "print(sub.head())\n",
        "\n",
        "# Cache refit test preds for record\n",
        "np.save('test_refit_lr_withsub_meta.npy', p_te_lr_with)\n",
        "np.save('test_refit_lr_nosub_meta.npy', p_te_lr_ns)\n",
        "np.save('test_refit_xgb_dense_v1.npy', p_te_d1)\n",
        "np.save('test_refit_xgb_dense_v2.npy', p_te_d2)\n",
        "np.save('test_refit_xgb_meta.npy', p_te_meta)\n",
        "print('Refit-on-full artifacts saved.')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3704428602.py:41: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3704428602.py:41: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refit-on-full: LR_withsub_meta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR_withsub_meta trained in 53.9s | shapes tr:(2878, 54043) te:(1162, 54043)\nRefit-on-full: LR_nosub_meta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR_nosub_meta trained in 49.7s | shapes tr:(2878, 49515) te:(1162, 49515)\nRefit-on-full: Dense v1 (SVD 150/150/50 + meta) XGB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense v1 trained in 7.2s | nrounds=200 | spw=3.03 | shapes tr:(2878, 364)\nRefit-on-full: Dense v2 (SVD 250/120 + meta) XGB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense v2 trained in 7.4s | nrounds=120 | spw=3.03 | shapes tr:(2878, 384)\nRefit-on-full: Meta-only XGB\nMeta-only trained in 0.1s | nrounds=80 | spw=3.03 | feats:14\nBlending refit-on-full predictions with S14b weights...\nSaved submission.csv (refit-on-full S14b blend). Head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.506584\n1   t3_roiuw                  0.436435\n2   t3_mjnbq                  0.373261\n3   t3_t8wd1                  0.372974\n4  t3_1m4zxu                  0.387110\nRefit-on-full artifacts saved.\n"
          ]
        }
      ]
    },
    {
      "id": "5b5c066b-4fa4-483f-a419-dce49a429724",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Hedge submissions from refit-on-full predictions: 15% shrunk logit blend and equal-prob 5-base\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "test = pd.read_json('test.json')\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Load refit-on-full test predictions\n",
        "p_lr_w = np.load('test_refit_lr_withsub_meta.npy')\n",
        "p_lr_ns = np.load('test_refit_lr_nosub_meta.npy')\n",
        "p_d1 = np.load('test_refit_xgb_dense_v1.npy')\n",
        "p_d2 = np.load('test_refit_xgb_dense_v2.npy')\n",
        "p_meta = np.load('test_refit_xgb_meta.npy')\n",
        "\n",
        "# Primary weights (S14b) for reference\n",
        "g = 0.90\n",
        "w_vec = np.array([0.33, 0.36, 0.09, 0.22], dtype=np.float64)  # (LRmix, D1, D2, Meta)\n",
        "\n",
        "# Hedge 1: 15% shrink toward equal on logit components\n",
        "z_lr_w = to_logit(p_lr_w); z_lr_ns = to_logit(p_lr_ns)\n",
        "z_lr_mix = (1.0 - g)*z_lr_w + g*z_lr_ns\n",
        "z_d1 = to_logit(p_d1); z_d2 = to_logit(p_d2); z_meta = to_logit(p_meta)\n",
        "w_eq = np.ones_like(w_vec) / 4.0\n",
        "alpha = 0.15\n",
        "w_shrunk = (1.0 - alpha)*w_vec + alpha*w_eq\n",
        "w_shrunk = (w_shrunk / w_shrunk.sum()).astype(np.float64)\n",
        "z_shrunk = w_shrunk[0]*z_lr_mix + w_shrunk[1]*z_d1 + w_shrunk[2]*z_d2 + w_shrunk[3]*z_meta\n",
        "p_shrunk = sigmoid(z_shrunk).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: p_shrunk}).to_csv('submission_time_blend_constrained_wide_refit_shrunk.csv', index=False)\n",
        "\n",
        "# Hedge 2: equal-probability average over all 5 bases with clipping\n",
        "p_eq5 = np.clip((p_lr_w + p_lr_ns + p_d1 + p_d2 + p_meta) / 5.0, 0.01, 0.99).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: p_eq5}).to_csv('submission_time_equal5_refit.csv', index=False)\n",
        "\n",
        "print('Wrote hedges:')\n",
        "print(' - submission_time_blend_constrained_wide_refit_shrunk.csv')\n",
        "print(' - submission_time_equal5_refit.csv')\n",
        "print('Primary submission.csv remains the S14b refit blend.')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote hedges:\n - submission_time_blend_constrained_wide_refit_shrunk.csv\n - submission_time_equal5_refit.csv\nPrimary submission.csv remains the S14b refit blend.\n"
          ]
        }
      ]
    },
    {
      "id": "47ec6835-142b-4cce-a085-f148dd57f5d8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Refit-on-full XGB bagging (3 seeds) for Dense v1, Dense v2, Meta; blend with S14b weights and promote to submission.csv\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "import xgboost as xgb\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body(df):\n",
        "    return df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "def combine_text(df):\n",
        "    return (get_title(df) + ' \\n ' + get_body(df)).astype(str)\n",
        "def clean_text_series(s):\n",
        "    s = s.str.lower()\n",
        "    s = s.str.replace(r'https?://\\S+', ' url ', regex=True)\n",
        "    s = s.str.replace(r'\\d+', ' number ', regex=True)\n",
        "    s = s.str.replace(r'\\s+', ' ', regex=True)\n",
        "    return s\n",
        "def build_subs(df):\n",
        "    if 'requester_subreddits_at_request' not in df.columns:\n",
        "        return pd.Series(['']*len(df))\n",
        "    sr = df['requester_subreddits_at_request']\n",
        "    return sr.apply(lambda x: ' '.join([str(s).lower() for s in x]) if isinstance(x, (list, tuple)) else '')\n",
        "def build_meta_v1(df):\n",
        "    title = get_title(df); body = get_body(df)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    out['title_len'] = title.str.len().astype(np.float32)\n",
        "    out['body_len'] = body.str.len().astype(np.float32)\n",
        "    out['title_body_ratio'] = (out['title_len'] / (1.0 + out['body_len'])).astype(np.float32)\n",
        "    out['has_url'] = body.str.contains(r'https?://', regex=True).astype(np.float32)\n",
        "    out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n",
        "    if 'unix_timestamp_of_request' in df.columns:\n",
        "        dt = pd.to_datetime(df['unix_timestamp_of_request'], unit='s', utc=True, errors='coerce')\n",
        "    else:\n",
        "        dt = pd.to_datetime(0, unit='s', utc=True) + pd.to_timedelta(np.zeros(len(df)), unit='s')\n",
        "    hour = dt.dt.hour.fillna(0).astype(np.float32)\n",
        "    out['hour'] = hour\n",
        "    out['dayofweek'] = dt.dt.dayofweek.fillna(0).astype(np.float32)\n",
        "    out['is_weekend'] = out['dayofweek'].isin([5,6]).astype(np.float32)\n",
        "    out['hour_sin'] = np.sin(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    out['hour_cos'] = np.cos(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    for c in [\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_posts_at_request'\n",
        "    ]:\n",
        "        if c in df.columns:\n",
        "            out[c] = pd.to_numeric(df[c], errors='coerce').astype(np.float32)\n",
        "        else:\n",
        "            out[c] = 0.0\n",
        "    for c in ['title_len','body_len','title_body_ratio',\n",
        "              'requester_upvotes_minus_downvotes_at_request',\n",
        "              'requester_upvotes_plus_downvotes_at_request',\n",
        "              'requester_number_of_comments_at_request',\n",
        "              'requester_number_of_posts_at_request']:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(out[c].clip(lower=0)).astype(np.float32)\n",
        "    out = out.replace([np.inf,-np.inf], 0).fillna(0).astype(np.float32)\n",
        "    return out\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Prepare text/meta\n",
        "txt_tr = clean_text_series(combine_text(train)); txt_te = clean_text_series(combine_text(test))\n",
        "subs_tr = build_subs(train); subs_te = build_subs(test)\n",
        "meta_tr = build_meta_v1(train).astype(np.float32).values\n",
        "meta_te = build_meta_v1(test).astype(np.float32).values\n",
        "\n",
        "word_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=60000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "char_params = dict(analyzer='char_wb', ngram_range=(3,5), lowercase=True, min_df=3, max_features=60000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "subs_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=20000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "\n",
        "print('Vectorizing (word/char/subs)...')\n",
        "t0 = time.time()\n",
        "tfidf_w = TfidfVectorizer(**word_params); Xw_tr = tfidf_w.fit_transform(txt_tr); Xw_te = tfidf_w.transform(txt_te)\n",
        "tfidf_c = TfidfVectorizer(**char_params); Xc_tr = tfidf_c.fit_transform(txt_tr); Xc_te = tfidf_c.transform(txt_te)\n",
        "tfidf_s = TfidfVectorizer(**subs_params); Xs_tr = tfidf_s.fit_transform(subs_tr); Xs_te = tfidf_s.transform(subs_te)\n",
        "print(f'TF-IDF done in {time.time()-t0:.1f}s')\n",
        "\n",
        "# Build Dense v1 features (SVD 150/150/50 + meta)\n",
        "svd_w_v1, svd_c_v1, svd_s_v1 = TruncatedSVD(n_components=150, random_state=42), TruncatedSVD(n_components=150, random_state=42), TruncatedSVD(n_components=50, random_state=42)\n",
        "Zw_tr = svd_w_v1.fit_transform(Xw_tr); Zw_te = svd_w_v1.transform(Xw_te)\n",
        "Zc_tr = svd_c_v1.fit_transform(Xc_tr); Zc_te = svd_c_v1.transform(Xc_te)\n",
        "Zs_tr = svd_s_v1.fit_transform(Xs_tr); Zs_te = svd_s_v1.transform(Xs_te)\n",
        "sc_meta = StandardScaler(with_mean=True, with_std=True)\n",
        "Mtr_s = sc_meta.fit_transform(meta_tr).astype(np.float32); Mte_s = sc_meta.transform(meta_te).astype(np.float32)\n",
        "Xtr_v1 = StandardScaler(with_mean=True, with_std=True).fit_transform(np.hstack([Zw_tr, Zc_tr, Zs_tr, Mtr_s]).astype(np.float32))\n",
        "Xte_v1 = StandardScaler(with_mean=True, with_std=True).fit(np.zeros((1, Xtr_v1.shape[1]), dtype=np.float32)).__class__(with_mean=True, with_std=True)\n",
        "# Refit scaler properly for test using same stats as train for v1\n",
        "sc_all_v1 = StandardScaler(with_mean=True, with_std=True)\n",
        "Xtr_all_v1 = sc_all_v1.fit_transform(np.hstack([Zw_tr, Zc_tr, Zs_tr, Mtr_s]).astype(np.float32))\n",
        "Xte_all_v1 = sc_all_v1.transform(np.hstack([Zw_te, Zc_te, Zs_te, Mte_s]).astype(np.float32))\n",
        "\n",
        "# Build Dense v2 features (SVD 250/120 + meta, no subs)\n",
        "svd_w_v2, svd_c_v2 = TruncatedSVD(n_components=250, random_state=42), TruncatedSVD(n_components=120, random_state=42)\n",
        "Zw2_tr = svd_w_v2.fit_transform(Xw_tr); Zw2_te = svd_w_v2.transform(Xw_te)\n",
        "Zc2_tr = svd_c_v2.fit_transform(Xc_tr); Zc2_te = svd_c_v2.transform(Xc_te)\n",
        "sc_meta2 = StandardScaler(with_mean=True, with_std=True)\n",
        "Mtr2_s = sc_meta2.fit_transform(meta_tr).astype(np.float32); Mte2_s = sc_meta2.transform(meta_te).astype(np.float32)\n",
        "sc_all_v2 = StandardScaler(with_mean=True, with_std=True)\n",
        "Xtr_all_v2 = sc_all_v2.fit_transform(np.hstack([Zw2_tr, Zc2_tr, Mtr2_s]).astype(np.float32))\n",
        "Xte_all_v2 = sc_all_v2.transform(np.hstack([Zw2_te, Zc2_te, Mte2_s]).astype(np.float32))\n",
        "\n",
        "# Meta-only features\n",
        "sc_m = StandardScaler(with_mean=True, with_std=True)\n",
        "Xtr_meta = sc_m.fit_transform(meta_tr).astype(np.float32)\n",
        "Xte_meta = sc_m.transform(meta_te).astype(np.float32)\n",
        "\n",
        "pos = float((y==1).sum()); neg = float((y==0).sum()); spw = (neg/max(pos,1.0)) if pos>0 else 1.0\n",
        "params_v1 = dict(objective='binary:logistic', eval_metric='auc', max_depth=3, eta=0.035, subsample=0.8, colsample_bytree=0.6, min_child_weight=8, reg_alpha=1.0, reg_lambda=4.0, gamma=0.1, device='cuda', tree_method='hist', scale_pos_weight=spw)\n",
        "params_v2 = dict(objective='binary:logistic', eval_metric='auc', max_depth=3, eta=0.03, min_child_weight=10, subsample=0.75, colsample_bytree=0.7, reg_alpha=1.5, reg_lambda=5.0, gamma=0.2, device='cuda', tree_method='hist', scale_pos_weight=spw)\n",
        "params_meta = dict(objective='binary:logistic', eval_metric='auc', max_depth=3, eta=0.05, min_child_weight=8, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.5, reg_lambda=3.0, gamma=0.0, device='cuda', tree_method='hist', scale_pos_weight=spw)\n",
        "seeds = [42, 1337, 2025]\n",
        "nrounds_v1, nrounds_v2, nrounds_meta = 200, 120, 80\n",
        "\n",
        "print('Training XGB bag (3 seeds)...')\n",
        "p_d1_seeds, p_d2_seeds, p_meta_seeds = [], [], []\n",
        "dtr_v1 = xgb.DMatrix(Xtr_all_v1, label=y); dte_v1 = xgb.DMatrix(Xte_all_v1)\n",
        "dtr_v2 = xgb.DMatrix(Xtr_all_v2, label=y); dte_v2 = xgb.DMatrix(Xte_all_v2)\n",
        "dtr_m  = xgb.DMatrix(Xtr_meta, label=y);  dte_m  = xgb.DMatrix(Xte_meta)\n",
        "t1 = time.time()\n",
        "for si, sd in enumerate(seeds, 1):\n",
        "    params1 = dict(params_v1); params1['seed'] = sd\n",
        "    params2 = dict(params_v2); params2['seed'] = sd\n",
        "    paramsm = dict(params_meta); paramsm['seed'] = sd\n",
        "    bst1 = xgb.train(params1, dtr_v1, num_boost_round=nrounds_v1, verbose_eval=False)\n",
        "    bst2 = xgb.train(params2, dtr_v2, num_boost_round=nrounds_v2, verbose_eval=False)\n",
        "    bstm = xgb.train(paramsm, dtr_m,  num_boost_round=nrounds_meta, verbose_eval=False)\n",
        "    p_d1_seeds.append(bst1.predict(dte_v1).astype(np.float32))\n",
        "    p_d2_seeds.append(bst2.predict(dte_v2).astype(np.float32))\n",
        "    p_meta_seeds.append(bstm.predict(dte_m).astype(np.float32))\n",
        "    print(f'  Seed {sd} done')\n",
        "print(f'XGB bagging done in {time.time()-t1:.1f}s')\n",
        "\n",
        "p_te_d1_bag = np.mean(p_d1_seeds, axis=0).astype(np.float32)\n",
        "p_te_d2_bag = np.mean(p_d2_seeds, axis=0).astype(np.float32)\n",
        "p_te_meta_bag = np.mean(p_meta_seeds, axis=0).astype(np.float32)\n",
        "\n",
        "# Load LR refit predictions from previous cell to avoid recomputation\n",
        "p_te_lr_with = np.load('test_refit_lr_withsub_meta.npy')\n",
        "p_te_lr_ns = np.load('test_refit_lr_nosub_meta.npy')\n",
        "\n",
        "# Blend with exact S14b weights\n",
        "g = 0.90\n",
        "z_lr_mix = (1.0 - g)*to_logit(p_te_lr_with) + g*to_logit(p_te_lr_ns)\n",
        "z_d1 = to_logit(p_te_d1_bag); z_d2 = to_logit(p_te_d2_bag); z_meta = to_logit(p_te_meta_bag)\n",
        "w_lr, w_d1, w_d2, w_meta = 0.33, 0.36, 0.09, 0.22\n",
        "z_blend = w_lr*z_lr_mix + w_d1*z_d1 + w_d2*z_d2 + w_meta*z_meta\n",
        "p_final = sigmoid(z_blend).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: ids, target_col: p_final})\n",
        "sub.to_csv('submission_time_blend_constrained_wide_refit_bagged.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Promoted bagged refit S14b blend to submission.csv. Head:')\n",
        "print(sub.head())\n",
        "\n",
        "# Cache bagged components\n",
        "np.save('test_refit_xgb_dense_v1_bag.npy', p_te_d1_bag)\n",
        "np.save('test_refit_xgb_dense_v2_bag.npy', p_te_d2_bag)\n",
        "np.save('test_refit_xgb_meta_bag.npy', p_te_meta_bag)\n",
        "print('Saved bagged XGB prediction artifacts.')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3461756310.py:39: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3461756310.py:39: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorizing (word/char/subs)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF done in 3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training XGB bag (3 seeds)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Seed 42 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Seed 1337 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Seed 2025 done\nXGB bagging done in 2.1s\nPromoted bagged refit S14b blend to submission.csv. Head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.495084\n1   t3_roiuw                  0.437231\n2   t3_mjnbq                  0.366789\n3   t3_t8wd1                  0.381403\n4  t3_1m4zxu                  0.382439\nSaved bagged XGB prediction artifacts.\n"
          ]
        }
      ]
    },
    {
      "id": "11b8c566-2dea-4123-a34c-2abeed3c0af9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S22: Build MiniLM sentence embeddings (title + body) and cache with meta_v1\n",
        "import sys, subprocess, time, gc, numpy as np, pandas as pd\n",
        "\n",
        "def ensure(pkg):\n",
        "    try:\n",
        "        __import__(pkg)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "print('Ensuring sentence-transformers and torch...')\n",
        "if not ensure('torch'):\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'torch'], check=True)\n",
        "if not ensure('sentence_transformers'):\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'sentence-transformers'], check=True)\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body(df):\n",
        "    return df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "def build_text(df):\n",
        "    return (get_title(df) + '\\n' + get_body(df)).astype(str)\n",
        "def build_meta_v1(df):\n",
        "    title = get_title(df); body = get_body(df)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    out['title_len'] = title.str.len().astype(np.float32)\n",
        "    out['body_len'] = body.str.len().astype(np.float32)\n",
        "    out['title_body_ratio'] = (out['title_len'] / (1.0 + out['body_len'])).astype(np.float32)\n",
        "    out['has_url'] = body.str.contains(r'https?://', regex=True).astype(np.float32)\n",
        "    out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n",
        "    if 'unix_timestamp_of_request' in df.columns:\n",
        "        dt = pd.to_datetime(df['unix_timestamp_of_request'], unit='s', utc=True, errors='coerce')\n",
        "    else:\n",
        "        dt = pd.to_datetime(0, unit='s', utc=True) + pd.to_timedelta(np.zeros(len(df)), unit='s')\n",
        "    hour = dt.dt.hour.fillna(0).astype(np.float32)\n",
        "    out['hour'] = hour\n",
        "    out['dayofweek'] = dt.dt.dayofweek.fillna(0).astype(np.float32)\n",
        "    out['is_weekend'] = out['dayofweek'].isin([5,6]).astype(np.float32)\n",
        "    out['hour_sin'] = np.sin(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    out['hour_cos'] = np.cos(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    for c in [\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_posts_at_request'\n",
        "    ]:\n",
        "        if c in df.columns:\n",
        "            out[c] = pd.to_numeric(df[c], errors='coerce').astype(np.float32)\n",
        "        else:\n",
        "            out[c] = 0.0\n",
        "    for c in ['title_len','body_len','title_body_ratio',\n",
        "              'requester_upvotes_minus_downvotes_at_request',\n",
        "              'requester_upvotes_plus_downvotes_at_request',\n",
        "              'requester_number_of_comments_at_request',\n",
        "              'requester_number_of_posts_at_request']:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(out[c].clip(lower=0)).astype(np.float32)\n",
        "    out = out.replace([np.inf,-np.inf], 0).fillna(0).astype(np.float32)\n",
        "    return out\n",
        "\n",
        "texts_tr = build_text(train).tolist()\n",
        "texts_te = build_text(test).tolist()\n",
        "print(f'Texts prepared: train {len(texts_tr)}, test {len(texts_te)}')\n",
        "\n",
        "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "print(f'Loading SentenceTransformer: {model_name}')\n",
        "model = SentenceTransformer(model_name, device='cuda')\n",
        "print('Encoding embeddings on GPU...')\n",
        "t0 = time.time()\n",
        "emb_tr = model.encode(texts_tr, batch_size=256, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True, device='cuda')\n",
        "emb_te = model.encode(texts_te, batch_size=256, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True, device='cuda')\n",
        "print(f'Embeddings done in {time.time()-t0:.1f}s | shapes tr:{emb_tr.shape} te:{emb_te.shape}')\n",
        "\n",
        "meta_tr = build_meta_v1(train).astype(np.float32).values\n",
        "meta_te = build_meta_v1(test).astype(np.float32).values\n",
        "print(f'Meta_v1 shapes tr:{meta_tr.shape} te:{meta_te.shape}')\n",
        "\n",
        "np.save('emb_minilm_tr.npy', emb_tr.astype(np.float32))\n",
        "np.save('emb_minilm_te.npy', emb_te.astype(np.float32))\n",
        "np.save('meta_v1_tr.npy', meta_tr.astype(np.float32))\n",
        "np.save('meta_v1_te.npy', meta_te.astype(np.float32))\n",
        "print('Saved emb_minilm_tr.npy, emb_minilm_te.npy, meta_v1_tr.npy, meta_v1_te.npy')\n",
        "\n",
        "del emb_tr, emb_te, meta_tr, meta_te, texts_tr, texts_te, model\n",
        "gc.collect()\n",
        "print('S22 complete.')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensuring sentence-transformers and torch...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/torch-2.8.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchgen already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/functorch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusolver_cu12-11.7.3.90.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/jinja2-3.1.6.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cudnn_cu12-9.10.2.21.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cufft_cu12-11.3.3.83.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusparse_cu12-12.5.8.93.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/requests-2.32.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/requests already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/triton-3.4.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/certifi-2025.8.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/certifi already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/charset_normalizer-3.4.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/charset_normalizer already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock-3.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec-2025.9.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/idna-3.10.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/idna already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/MarkupSafe-3.0.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx-3.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/nvidia_cublas_cu12-12.8.4.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_cupti_cu12-12.8.90.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_nvrtc_cu12-12.8.93.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_runtime_cu12-12.8.90.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cufile_cu12-1.13.1.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_curand_cu12-10.3.9.90.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nccl_cu12-2.27.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvjitlink_cu12-12.8.93.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvtx_cu12-12.8.90.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/setuptools-80.9.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/setuptools already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pkg_resources already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/_distutils_hack already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/distutils-precedence.pth already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy-1.14.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/isympy.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/urllib3-2.5.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/urllib3 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath-1.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusparselt_cu12-0.7.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/share already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No sentence-transformers model found with name sentence-transformers/all-MiniLM-L6-v2. Creating a new one with mean pooling.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texts prepared: train 2878, test 1162\nLoading SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 30] Read-only file system: '/app/.cache'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[Errno 30] Read-only file system: '/app/.cache'",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mEntryNotFoundError\u001b[39m                        Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/huggingface_hub/file_download.py:1546\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1546\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1547\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\n\u001b[32m   1548\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/huggingface_hub/file_download.py:1463\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[39m\n\u001b[32m   1462\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1463\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1471\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1472\u001b[39m hf_raise_for_status(r)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/huggingface_hub/file_download.py:286\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/huggingface_hub/file_download.py:310\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    309\u001b[39m response = http_backoff(method=method, url=url, **params, retry_on_exceptions=(), retry_on_status_codes=(\u001b[32m429\u001b[39m,))\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/huggingface_hub/utils/_http.py:420\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    419\u001b[39m     message = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEntry Not Found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(EntryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_code == \u001b[33m\"\u001b[39m\u001b[33mGatedRepo\u001b[39m\u001b[33m\"\u001b[39m:\n",
            "\u001b[31mEntryNotFoundError\u001b[39m: 404 Client Error. (Request ID: Root=1-68c415b6-005514f051393759779c4325;3e4211be-80b5-4dad-9363-9f96a8244f32)\n\nEntry Not Found for url: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/pathlib.py:1117\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     os.mkdir(\u001b[38;5;28mself\u001b[39m, mode)\n\u001b[32m   1118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/app/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/refs'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/pathlib.py:1117\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     os.mkdir(\u001b[38;5;28mself\u001b[39m, mode)\n\u001b[32m   1118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/app/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/pathlib.py:1117\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     os.mkdir(\u001b[38;5;28mself\u001b[39m, mode)\n\u001b[32m   1118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/app/.cache/huggingface/hub'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/pathlib.py:1117\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     os.mkdir(\u001b[38;5;28mself\u001b[39m, mode)\n\u001b[32m   1118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/app/.cache/huggingface'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     71\u001b[39m model_name = \u001b[33m'\u001b[39m\u001b[33msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mLoading SentenceTransformer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m model = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mEncoding embeddings on GPU...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     75\u001b[39m t0 = time.time()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sentence_transformers/SentenceTransformer.py:339\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    327\u001b[39m         modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28mself\u001b[39m._load_sbert_model(\n\u001b[32m    328\u001b[39m             model_name_or_path,\n\u001b[32m    329\u001b[39m             token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    336\u001b[39m             config_kwargs=config_kwargs,\n\u001b[32m    337\u001b[39m         )\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m         modules = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_auto_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhas_modules\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modules, OrderedDict):\n\u001b[32m    353\u001b[39m     modules = OrderedDict([(\u001b[38;5;28mstr\u001b[39m(idx), module) \u001b[38;5;28;01mfor\u001b[39;00m idx, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules)])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sentence_transformers/SentenceTransformer.py:2060\u001b[39m, in \u001b[36mSentenceTransformer._load_auto_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs, has_modules)\u001b[39m\n\u001b[32m   2057\u001b[39m tokenizer_kwargs = shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {**shared_kwargs, **tokenizer_kwargs}\n\u001b[32m   2058\u001b[39m config_kwargs = shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m config_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {**shared_kwargs, **config_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m2060\u001b[39m transformer_model = \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2061\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2062\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2067\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2068\u001b[39m pooling_model = Pooling(transformer_model.get_word_embedding_dimension(), \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2069\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files_only:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sentence_transformers/models/Transformer.py:86\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     84\u001b[39m     config_args = {}\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m config, is_peft_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m._load_model(model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sentence_transformers/models/Transformer.py:127\u001b[39m, in \u001b[36mTransformer._load_config\u001b[39m\u001b[34m(self, model_name_or_path, cache_dir, backend, config_args)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load_config\u001b[39m(\n\u001b[32m    112\u001b[39m     \u001b[38;5;28mself\u001b[39m, model_name_or_path: \u001b[38;5;28mstr\u001b[39m, cache_dir: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m, backend: \u001b[38;5;28mstr\u001b[39m, config_args: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]\n\u001b[32m    113\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[PeftConfig | PretrainedConfig, \u001b[38;5;28mbool\u001b[39m]:\n\u001b[32m    114\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Loads the transformers or PEFT configuration\u001b[39;00m\n\u001b[32m    115\u001b[39m \n\u001b[32m    116\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    124\u001b[39m \u001b[33;03m        tuple[PretrainedConfig, bool]: The model configuration and a boolean indicating whether the model is a PEFT model.\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m         \u001b[43mfind_adapter_config_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtoken\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrevision\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal_files_only\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m         \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    135\u001b[39m     ):\n\u001b[32m    136\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_peft_available():\n\u001b[32m    137\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[32m    138\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mLoading a PEFT model requires installing the `peft` package. You can install it via `pip install peft`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    139\u001b[39m             )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/transformers/utils/peft_utils.py:88\u001b[39m, in \u001b[36mfind_adapter_config_file\u001b[39m\u001b[34m(model_id, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, _commit_hash)\u001b[39m\n\u001b[32m     86\u001b[39m         adapter_cached_filename = os.path.join(model_id, ADAPTER_CONFIG_NAME)\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     adapter_cached_filename = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mADAPTER_CONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m adapter_cached_filename\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/transformers/utils/hub.py:321\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    264\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    265\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    266\u001b[39m     **kwargs,\n\u001b[32m    267\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    268\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    269\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    270\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/transformers/utils/hub.py:566\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    563\u001b[39m     \u001b[38;5;66;03m# Any other Exception type should now be re-raised, in order to provide helpful error messages and break the execution flow\u001b[39;00m\n\u001b[32m    564\u001b[39m     \u001b[38;5;66;03m# (EntryNotFoundError will be treated outside this block and correctly re-raised if needed)\u001b[39;00m\n\u001b[32m    565\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, EntryNotFoundError):\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    568\u001b[39m resolved_files = [\n\u001b[32m    569\u001b[39m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[32m    570\u001b[39m ]\n\u001b[32m    571\u001b[39m \u001b[38;5;66;03m# If there are any missing file and the flag is active, raise\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/transformers/utils/hub.py:478\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    477\u001b[39m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    493\u001b[39m         snapshot_download(\n\u001b[32m    494\u001b[39m             path_or_repo_id,\n\u001b[32m    495\u001b[39m             allow_patterns=full_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    504\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    505\u001b[39m         )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/huggingface_hub/file_download.py:1010\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    990\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    991\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    992\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1007\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1008\u001b[39m     )\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/huggingface_hub/file_download.py:1073\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1069\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[32m   1071\u001b[39m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[32m   1072\u001b[39m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1073\u001b[39m (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) = \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[32m   1089\u001b[39m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[32m   1090\u001b[39m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1096\u001b[39m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[32m   1097\u001b[39m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[32m   1098\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1099\u001b[39m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/huggingface_hub/file_download.py:1562\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1558\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1559\u001b[39m                 logger.error(\n\u001b[32m   1560\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not cache non-existence of file. Will ignore error and continue. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1561\u001b[39m                 )\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m             \u001b[43m_cache_commit_hash_for_specific_revision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1563\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m   1565\u001b[39m \u001b[38;5;66;03m# Commit hash must exist\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/huggingface_hub/file_download.py:766\u001b[39m, in \u001b[36m_cache_commit_hash_for_specific_revision\u001b[39m\u001b[34m(storage_folder, revision, commit_hash)\u001b[39m\n\u001b[32m    764\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m revision != commit_hash:\n\u001b[32m    765\u001b[39m     ref_path = Path(storage_folder) / \u001b[33m\"\u001b[39m\u001b[33mrefs\u001b[39m\u001b[33m\"\u001b[39m / revision\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m     \u001b[43mref_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    767\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ref_path.exists() \u001b[38;5;129;01mor\u001b[39;00m commit_hash != ref_path.read_text():\n\u001b[32m    768\u001b[39m         \u001b[38;5;66;03m# Update ref only if has been updated. Could cause useless error in case\u001b[39;00m\n\u001b[32m    769\u001b[39m         \u001b[38;5;66;03m# repo is already cached and user doesn't have write access to cache folder.\u001b[39;00m\n\u001b[32m    770\u001b[39m         \u001b[38;5;66;03m# See https://github.com/huggingface/huggingface_hub/issues/1216.\u001b[39;00m\n\u001b[32m    771\u001b[39m         ref_path.write_text(commit_hash)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/pathlib.py:1121\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parent == \u001b[38;5;28mself\u001b[39m:\n\u001b[32m   1120\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1122\u001b[39m     \u001b[38;5;28mself\u001b[39m.mkdir(mode, parents=\u001b[38;5;28;01mFalse\u001b[39;00m, exist_ok=exist_ok)\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   1124\u001b[39m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[32m   1125\u001b[39m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/pathlib.py:1121\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parent == \u001b[38;5;28mself\u001b[39m:\n\u001b[32m   1120\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1122\u001b[39m     \u001b[38;5;28mself\u001b[39m.mkdir(mode, parents=\u001b[38;5;28;01mFalse\u001b[39;00m, exist_ok=exist_ok)\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   1124\u001b[39m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[32m   1125\u001b[39m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n",
            "    \u001b[31m[... skipping similar frames: Path.mkdir at line 1121 (1 times)]\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/pathlib.py:1121\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parent == \u001b[38;5;28mself\u001b[39m:\n\u001b[32m   1120\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1122\u001b[39m     \u001b[38;5;28mself\u001b[39m.mkdir(mode, parents=\u001b[38;5;28;01mFalse\u001b[39;00m, exist_ok=exist_ok)\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   1124\u001b[39m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[32m   1125\u001b[39m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/pathlib.py:1117\u001b[39m, in \u001b[36mPath.mkdir\u001b[39m\u001b[34m(self, mode, parents, exist_ok)\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1114\u001b[39m \u001b[33;03mCreate a new directory at this given path.\u001b[39;00m\n\u001b[32m   1115\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     os.mkdir(\u001b[38;5;28mself\u001b[39m, mode)\n\u001b[32m   1118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m   1119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parent == \u001b[38;5;28mself\u001b[39m:\n",
            "\u001b[31mOSError\u001b[39m: [Errno 30] Read-only file system: '/app/.cache'"
          ]
        }
      ]
    },
    {
      "id": "5cb7f893-2a68-43d1-b0f6-25cf323ebaa3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S22 recovery: set HF caches to writable path and rebuild MiniLM embeddings\n",
        "import os, time, gc, numpy as np, pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Route HF/transformers caches to local writable folder\n",
        "cache_dir = os.path.abspath('hf_cache')\n",
        "os.makedirs(cache_dir, exist_ok=True)\n",
        "os.environ['HF_HOME'] = cache_dir\n",
        "os.environ['HUGGINGFACE_HUB_CACHE'] = cache_dir\n",
        "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
        "os.environ['SENTENCE_TRANSFORMERS_HOME'] = cache_dir\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body(df):\n",
        "    return df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "def build_text(df):\n",
        "    return (get_title(df) + '\\n' + get_body(df)).astype(str)\n",
        "def build_meta_v1(df):\n",
        "    title = get_title(df); body = get_body(df)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    out['title_len'] = title.str.len().astype(np.float32)\n",
        "    out['body_len'] = body.str.len().astype(np.float32)\n",
        "    out['title_body_ratio'] = (out['title_len'] / (1.0 + out['body_len'])).astype(np.float32)\n",
        "    out['has_url'] = body.str.contains(r'https?://', regex=True).astype(np.float32)\n",
        "    out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n",
        "    if 'unix_timestamp_of_request' in df.columns:\n",
        "        dt = pd.to_datetime(df['unix_timestamp_of_request'], unit='s', utc=True, errors='coerce')\n",
        "    else:\n",
        "        dt = pd.to_datetime(0, unit='s', utc=True) + pd.to_timedelta(np.zeros(len(df)), unit='s')\n",
        "    hour = dt.dt.hour.fillna(0).astype(np.float32)\n",
        "    out['hour'] = hour\n",
        "    out['dayofweek'] = dt.dt.dayofweek.fillna(0).astype(np.float32)\n",
        "    out['is_weekend'] = out['dayofweek'].isin([5,6]).astype(np.float32)\n",
        "    out['hour_sin'] = np.sin(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    out['hour_cos'] = np.cos(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    for c in [\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_posts_at_request'\n",
        "    ]:\n",
        "        if c in df.columns:\n",
        "            out[c] = pd.to_numeric(df[c], errors='coerce').astype(np.float32)\n",
        "        else:\n",
        "            out[c] = 0.0\n",
        "    for c in ['title_len','body_len','title_body_ratio',\n",
        "              'requester_upvotes_minus_downvotes_at_request',\n",
        "              'requester_upvotes_plus_downvotes_at_request',\n",
        "              'requester_number_of_comments_at_request',\n",
        "              'requester_number_of_posts_at_request']:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(out[c].clip(lower=0)).astype(np.float32)\n",
        "    out = out.replace([np.inf,-np.inf], 0).fillna(0).astype(np.float32)\n",
        "    return out\n",
        "\n",
        "texts_tr = build_text(train).tolist()\n",
        "texts_te = build_text(test).tolist()\n",
        "print(f'Texts prepared: train {len(texts_tr)}, test {len(texts_te)}')\n",
        "\n",
        "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "print(f'Loading SentenceTransformer with cache_folder={cache_dir}')\n",
        "model = SentenceTransformer(model_name, device='cuda', cache_folder=cache_dir)\n",
        "\n",
        "print('Encoding on GPU...')\n",
        "t0 = time.time()\n",
        "emb_tr = model.encode(texts_tr, batch_size=256, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True, device='cuda')\n",
        "emb_te = model.encode(texts_te, batch_size=256, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True, device='cuda')\n",
        "print(f'Embeddings done in {time.time()-t0:.1f}s | shapes tr:{emb_tr.shape} te:{emb_te.shape}')\n",
        "\n",
        "meta_tr = build_meta_v1(train).astype(np.float32).values\n",
        "meta_te = build_meta_v1(test).astype(np.float32).values\n",
        "np.save('emb_minilm_tr.npy', emb_tr.astype(np.float32))\n",
        "np.save('emb_minilm_te.npy', emb_te.astype(np.float32))\n",
        "np.save('meta_v1_tr.npy', meta_tr.astype(np.float32))\n",
        "np.save('meta_v1_te.npy', meta_te.astype(np.float32))\n",
        "print('Saved emb_minilm_tr.npy, emb_minilm_te.npy, meta_v1_tr.npy, meta_v1_te.npy')\n",
        "del emb_tr, emb_te, meta_tr, meta_te, texts_tr, texts_te, model\n",
        "gc.collect()\n",
        "print('S22 recovery complete.')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texts prepared: train 2878, test 1162\nLoading SentenceTransformer with cache_folder=/app/agent_run_states/random-acts-of-pizza-spray-20250912-053053/hf_cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding on GPU...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:   0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:   8%|\u258a         | 1/12 [00:01<00:11,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  17%|\u2588\u258b        | 2/12 [00:01<00:07,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  25%|\u2588\u2588\u258c       | 3/12 [00:02<00:05,  1.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  33%|\u2588\u2588\u2588\u258e      | 4/12 [00:02<00:04,  1.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  42%|\u2588\u2588\u2588\u2588\u258f     | 5/12 [00:02<00:03,  2.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  50%|\u2588\u2588\u2588\u2588\u2588     | 6/12 [00:03<00:02,  2.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 7/12 [00:03<00:01,  2.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 8/12 [00:03<00:01,  3.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 9/12 [00:03<00:00,  3.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 10/12 [00:03<00:00,  4.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 11/12 [00:03<00:00,  4.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:03<00:00,  3.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  20%|\u2588\u2588        | 1/5 [00:00<00:02,  1.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:00<00:01,  2.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:01<00:00,  2.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:01<00:00,  3.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:01<00:00,  3.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n/tmp/ipykernel_11141/3042838769.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3042838769.py:30: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings done in 5.6s | shapes tr:(2878, 384) te:(1162, 384)\nSaved emb_minilm_tr.npy, emb_minilm_te.npy, meta_v1_tr.npy, meta_v1_te.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S22 recovery complete.\n"
          ]
        }
      ]
    },
    {
      "id": "72b200f8-854f-4df2-8d73-53c0c64b76fa",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S23: Time-aware XGB on MiniLM embeddings + meta_v1 (6-block CV, 3-seed bag); cache OOF/test\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import xgboost as xgb\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "# Load cached embeddings and meta_v1\n",
        "Emb_tr = np.load('emb_minilm_tr.npy').astype(np.float32)\n",
        "Emb_te = np.load('emb_minilm_te.npy').astype(np.float32)\n",
        "Meta_tr = np.load('meta_v1_tr.npy').astype(np.float32)\n",
        "Meta_te = np.load('meta_v1_te.npy').astype(np.float32)\n",
        "print('Loaded features:', Emb_tr.shape, Emb_te.shape, Meta_tr.shape, Meta_te.shape)\n",
        "\n",
        "# 6-block forward-chaining folds and mask\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'Emb+Meta Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}')\n",
        "\n",
        "# XGB params (per expert)\n",
        "base_params = dict(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='auc',\n",
        "    max_depth=3,\n",
        "    eta=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.6,\n",
        "    min_child_weight=8,\n",
        "    reg_alpha=0.5,\n",
        "    reg_lambda=3.0,\n",
        "    gamma=0.0,\n",
        "    device='cuda',\n",
        "    tree_method='hist'\n",
        ")\n",
        "num_boost_round = 8000\n",
        "early_stopping_rounds = 100\n",
        "seeds = [42, 1337, 2025]\n",
        "\n",
        "oof_sum = np.zeros(n, dtype=np.float64)\n",
        "oof_cnt = np.zeros(n, dtype=np.float64)\n",
        "test_seed_preds = []\n",
        "\n",
        "for si, seed in enumerate(seeds, 1):\n",
        "    print(f'=== Emb+Meta Seed {seed} ({si}/{len(seeds)}) ===')\n",
        "    params = dict(base_params); params['seed'] = seed\n",
        "    oof_seed = np.zeros(n, dtype=np.float32)\n",
        "    test_folds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "        t0 = time.time()\n",
        "        Xtr_raw = np.hstack([Emb_tr[tr_idx], Meta_tr[tr_idx]]).astype(np.float32)\n",
        "        Xva_raw = np.hstack([Emb_tr[va_idx], Meta_tr[va_idx]]).astype(np.float32)\n",
        "        Xte_raw = np.hstack([Emb_te, Meta_te]).astype(np.float32)\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "        Xtr = scaler.fit_transform(Xtr_raw).astype(np.float32)\n",
        "        Xva = scaler.transform(Xva_raw).astype(np.float32)\n",
        "        Xte = scaler.transform(Xte_raw).astype(np.float32)\n",
        "        pos = float((y[tr_idx] == 1).sum()); neg = float((y[tr_idx] == 0).sum())\n",
        "        params['scale_pos_weight'] = (neg / max(pos, 1.0)) if pos > 0 else 1.0\n",
        "        dtr = xgb.DMatrix(Xtr, label=y[tr_idx])\n",
        "        dva = xgb.DMatrix(Xva, label=y[va_idx])\n",
        "        dte = xgb.DMatrix(Xte)\n",
        "        booster = xgb.train(params, dtr, num_boost_round=num_boost_round, evals=[(dva, 'valid')], early_stopping_rounds=early_stopping_rounds, verbose_eval=False)\n",
        "        va_pred = booster.predict(dva, iteration_range=(0, booster.best_iteration+1)).astype(np.float32)\n",
        "        te_pred = booster.predict(dte, iteration_range=(0, booster.best_iteration+1)).astype(np.float32)\n",
        "        oof_seed[va_idx] = va_pred; test_folds.append(te_pred)\n",
        "        auc = roc_auc_score(y[va_idx], va_pred)\n",
        "        print(f'[Emb Seed {seed} Fold {fi}] best_iter={booster.best_iteration} | spw={params[\"scale_pos_weight\"]:.2f} | AUC: {auc:.5f} | {time.time()-t0:.1f}s | shapes tr:{Xtr.shape}')\n",
        "        del Xtr_raw, Xva_raw, Xte_raw, Xtr, Xva, Xte, dtr, dva, dte, booster, scaler; gc.collect()\n",
        "    seed_auc = roc_auc_score(y[mask], oof_seed[mask])\n",
        "    print(f'[Emb Seed {seed}] OOF AUC (validated only): {seed_auc:.5f}')\n",
        "    oof_sum[mask] += oof_seed[mask]; oof_cnt[mask] += 1.0\n",
        "    test_seed_preds.append(np.mean(test_folds, axis=0).astype(np.float64))\n",
        "    del oof_seed, test_folds; gc.collect()\n",
        "\n",
        "oof_avg = np.zeros(n, dtype=np.float32)\n",
        "oof_avg[mask] = (oof_sum[mask] / np.maximum(oof_cnt[mask], 1.0)).astype(np.float32)\n",
        "test_avg = np.mean(test_seed_preds, axis=0).astype(np.float32)\n",
        "auc_oof = roc_auc_score(y[mask], oof_avg[mask])\n",
        "print(f'Emb+Meta Time-CV OOF AUC (validated only, 3-seed avg): {auc_oof:.5f}')\n",
        "np.save('oof_xgb_emb_meta_time.npy', oof_avg.astype(np.float32))\n",
        "np.save('test_xgb_emb_meta_time.npy', test_avg)\n",
        "print('Saved oof_xgb_emb_meta_time.npy and test_xgb_emb_meta_time.npy')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded features: (2878, 384) (1162, 384) (2878, 14) (1162, 14)\nEmb+Meta Time-CV: 5 folds; validated 2398/2878\n=== Emb+Meta Seed 42 (1/3) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Emb Seed 42 Fold 1] best_iter=68 | spw=1.94 | AUC: 0.64868 | 0.5s | shapes tr:(480, 398)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Emb Seed 42 Fold 2] best_iter=41 | spw=2.33 | AUC: 0.70075 | 0.3s | shapes tr:(960, 398)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Emb Seed 42 Fold 3] best_iter=12 | spw=2.49 | AUC: 0.60800 | 0.3s | shapes tr:(1440, 398)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Emb Seed 42 Fold 4] best_iter=30 | spw=2.79 | AUC: 0.64451 | 0.3s | shapes tr:(1920, 398)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Emb Seed 42 Fold 5] best_iter=103 | spw=2.83 | AUC: 0.64119 | 0.5s | shapes tr:(2399, 398)\n[Emb Seed 42] OOF AUC (validated only): 0.64339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Emb+Meta Seed 1337 (2/3) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Emb Seed 1337 Fold 1] best_iter=240 | spw=1.94 | AUC: 0.64989 | 0.7s | shapes tr:(480, 398)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Emb Seed 1337 Fold 2] best_iter=16 | spw=2.33 | AUC: 0.68545 | 0.3s | shapes tr:(960, 398)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Emb Seed 1337 Fold 3] best_iter=8 | spw=2.49 | AUC: 0.62842 | 0.3s | shapes tr:(1440, 398)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Emb Seed 1337 Fold 4] best_iter=8 | spw=2.79 | AUC: 0.64566 | 0.3s | shapes tr:(1920, 398)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Emb Seed 1337 Fold 5] best_iter=77 | spw=2.83 | AUC: 0.63445 | 0.4s | shapes tr:(2399, 398)\n[Emb Seed 1337] OOF AUC (validated only): 0.62448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Emb+Meta Seed 2025 (3/3) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Emb Seed 2025 Fold 1] best_iter=260 | spw=1.94 | AUC: 0.65203 | 0.7s | shapes tr:(480, 398)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Emb Seed 2025 Fold 2] best_iter=79 | spw=2.33 | AUC: 0.69104 | 0.4s | shapes tr:(960, 398)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Emb Seed 2025 Fold 3] best_iter=4 | spw=2.49 | AUC: 0.59918 | 0.3s | shapes tr:(1440, 398)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Emb Seed 2025 Fold 4] best_iter=59 | spw=2.79 | AUC: 0.63145 | 0.4s | shapes tr:(1920, 398)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Emb Seed 2025 Fold 5] best_iter=175 | spw=2.83 | AUC: 0.65415 | 0.7s | shapes tr:(2399, 398)\n[Emb Seed 2025] OOF AUC (validated only): 0.63028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emb+Meta Time-CV OOF AUC (validated only, 3-seed avg): 0.64020\nSaved oof_xgb_emb_meta_time.npy and test_xgb_emb_meta_time.npy\n"
          ]
        }
      ]
    },
    {
      "id": "ff220de2-b1ed-4932-9f58-e36bf32a7e1f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S24: Constrained time-consistent logit blend adding Emb+Meta base (6th model); write primary + hedges\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# 6-block forward-chaining mask\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    mask[np.array(blocks[i])] = True\n",
        "print(f'Time-CV (6 blocks) validated count: {mask.sum()}/{n}')\n",
        "\n",
        "# Load bases (time-consistent OOF/test)\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy'); t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "o_lr_ns = np.load('oof_lr_time_nosub_meta.npy'); t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy'); t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_time_v2.npy'); t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta_time.npy'); t_meta = np.load('test_xgb_meta_time.npy')\n",
        "o_emb = np.load('oof_xgb_emb_meta_time.npy'); t_emb = np.load('test_xgb_emb_meta_time.npy')\n",
        "\n",
        "# Convert to logits\n",
        "z_lr_w, z_lr_ns = to_logit(o_lr_w), to_logit(o_lr_ns)\n",
        "z_d1, z_d2, z_meta, z_emb = to_logit(o_d1), to_logit(o_d2), to_logit(o_meta), to_logit(o_emb)\n",
        "tz_lr_w, tz_lr_ns = to_logit(t_lr_w), to_logit(t_lr_ns)\n",
        "tz_d1, tz_d2, tz_meta, tz_emb = to_logit(t_d1), to_logit(t_d2), to_logit(t_meta), to_logit(t_emb)\n",
        "\n",
        "# Grid per expert guidance\n",
        "g_grid = [0.85, 0.90, 0.95]\n",
        "meta_grid = [0.18, 0.20, 0.22]\n",
        "dense_tot_grid = [0.30, 0.35, 0.40, 0.45]\n",
        "alpha_grid = [0.65, 0.80]  # split of dense_total -> favor v1\n",
        "emb_grid = [0.05, 0.08, 0.10, 0.12, 0.15, 0.18, 0.20]\n",
        "\n",
        "best_auc, best_cfg = -1.0, None\n",
        "tried = 0\n",
        "for g in g_grid:\n",
        "    z_lr_mix = (1.0 - g)*z_lr_w + g*z_lr_ns\n",
        "    tz_lr_mix = (1.0 - g)*tz_lr_w + g*tz_lr_ns\n",
        "    for w_emb in emb_grid:\n",
        "        rem1 = 1.0 - w_emb\n",
        "        for meta_w in meta_grid:\n",
        "            for d_tot in dense_tot_grid:\n",
        "                w_lr = rem1 - meta_w - d_tot\n",
        "                if w_lr <= 0 or w_lr >= 1:\n",
        "                    continue\n",
        "                for a in alpha_grid:\n",
        "                    w_d2 = d_tot * a\n",
        "                    w_d1 = d_tot - w_d2\n",
        "                    if w_d1 < 0 or w_d2 < 0:\n",
        "                        continue\n",
        "                    z_oof = w_lr*z_lr_mix + w_d1*z_d1 + w_d2*z_d2 + meta_w*z_meta + w_emb*z_emb\n",
        "                    auc = roc_auc_score(y[mask], z_oof[mask])\n",
        "                    tried += 1\n",
        "                    if auc > best_auc:\n",
        "                        best_auc = auc\n",
        "                        best_cfg = dict(g=float(g), w_emb=float(w_emb), w_lr=float(w_lr), w_d1=float(w_d1), w_d2=float(w_d2), w_meta=float(meta_w), tz_lr_mix=tz_lr_mix)\n",
        "cfg_print = {k: v for k, v in best_cfg.items() if k != 'tz_lr_mix'} if best_cfg is not None else {}\n",
        "print(f'6-way (with Emb) grid tried {tried} | Best OOF(z,time-mask) AUC: {best_auc:.5f} | cfg={cfg_print}')\n",
        "\n",
        "# Build primary test prediction\n",
        "g = best_cfg['g']; w_emb = best_cfg['w_emb']; w_lr = best_cfg['w_lr']; w_d1 = best_cfg['w_d1']; w_d2 = best_cfg['w_d2']; w_meta = best_cfg['w_meta']\n",
        "tz_lr_mix = best_cfg['tz_lr_mix']\n",
        "zt_best = w_lr*tz_lr_mix + w_d1*tz_d1 + w_d2*tz_d2 + w_meta*tz_meta + w_emb*tz_emb\n",
        "pt_best = sigmoid(zt_best).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: ids, target_col: pt_best})\n",
        "sub.to_csv('submission_time_blend_with_emb.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Promoted submission_time_blend_with_emb.csv to submission.csv. Head:')\n",
        "print(sub.head())\n",
        "\n",
        "# Hedge 1: 15% shrink toward equal on the 5 logit components (LRmix, D1, D2, Meta, Emb)\n",
        "w_vec = np.array([w_lr, w_d1, w_d2, w_meta, w_emb], dtype=np.float64)\n",
        "w_eq = np.ones_like(w_vec) / len(w_vec)\n",
        "alpha = 0.15\n",
        "w_shr = (1.0 - alpha)*w_vec + alpha*w_eq\n",
        "w_shr = (w_shr / w_shr.sum()).astype(np.float64)\n",
        "zt_shr = w_shr[0]*tz_lr_mix + w_shr[1]*tz_d1 + w_shr[2]*tz_d2 + w_shr[3]*tz_meta + w_shr[4]*tz_emb\n",
        "pt_shr = sigmoid(zt_shr).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: pt_shr}).to_csv('submission_time_blend_with_emb_shrunk.csv', index=False)\n",
        "\n",
        "# Hedge 2: equal-probability average over all 6 bases in prob space with clipping\n",
        "p_eq6 = np.clip((sigmoid(tz_lr_w) + sigmoid(tz_lr_ns) + t_d1 + t_d2 + t_meta + t_emb) / 6.0, 0.01, 0.99).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: p_eq6}).to_csv('submission_time_equal6_prob.csv', index=False)\n",
        "print('Wrote hedges: submission_time_blend_with_emb_shrunk.csv and submission_time_equal6_prob.csv')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV (6 blocks) validated count: 2398/2878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6-way (with Emb) grid tried 504 | Best OOF(z,time-mask) AUC: 0.67750 | cfg={'g': 0.95, 'w_emb': 0.2, 'w_lr': 0.2800000000000001, 'w_d1': 0.10499999999999998, 'w_d2': 0.195, 'w_meta': 0.22}\nPromoted submission_time_blend_with_emb.csv to submission.csv. Head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.517004\n1   t3_roiuw                  0.526068\n2   t3_mjnbq                  0.432309\n3   t3_t8wd1                  0.434144\n4  t3_1m4zxu                  0.516772\nWrote hedges: submission_time_blend_with_emb_shrunk.csv and submission_time_equal6_prob.csv\n"
          ]
        }
      ]
    },
    {
      "id": "14850455-8668-41d3-8f00-e73e4267c010",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S25: Refit-on-full 5-seed bag for XGB bases (Dense v1/v2, Meta, Emb+Meta) and 6-way blend using S24 best weights\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.sparse import hstack\n",
        "import xgboost as xgb\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body(df):\n",
        "    return df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "def combine_text(df):\n",
        "    return (get_title(df) + ' \\n ' + get_body(df)).astype(str)\n",
        "def clean_text_series(s):\n",
        "    s = s.str.lower()\n",
        "    s = s.str.replace(r'https?://\\S+', ' url ', regex=True)\n",
        "    s = s.str.replace(r'\\d+', ' number ', regex=True)\n",
        "    s = s.str.replace(r'\\s+', ' ', regex=True)\n",
        "    return s\n",
        "def build_subs(df):\n",
        "    if 'requester_subreddits_at_request' not in df.columns:\n",
        "        return pd.Series(['']*len(df))\n",
        "    sr = df['requester_subreddits_at_request']\n",
        "    return sr.apply(lambda x: ' '.join([str(s).lower() for s in x]) if isinstance(x, (list, tuple)) else '')\n",
        "def build_meta_v1(df):\n",
        "    title = get_title(df); body = get_body(df)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    out['title_len'] = title.str.len().astype(np.float32)\n",
        "    out['body_len'] = body.str.len().astype(np.float32)\n",
        "    out['title_body_ratio'] = (out['title_len'] / (1.0 + out['body_len'])).astype(np.float32)\n",
        "    out['has_url'] = body.str.contains(r'https?://', regex=True).astype(np.float32)\n",
        "    out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n",
        "    if 'unix_timestamp_of_request' in df.columns:\n",
        "        dt = pd.to_datetime(df['unix_timestamp_of_request'], unit='s', utc=True, errors='coerce')\n",
        "    else:\n",
        "        dt = pd.to_datetime(0, unit='s', utc=True) + pd.to_timedelta(np.zeros(len(df)), unit='s')\n",
        "    hour = dt.dt.hour.fillna(0).astype(np.float32)\n",
        "    out['hour'] = hour\n",
        "    out['dayofweek'] = dt.dt.dayofweek.fillna(0).astype(np.float32)\n",
        "    out['is_weekend'] = out['dayofweek'].isin([5,6]).astype(np.float32)\n",
        "    out['hour_sin'] = np.sin(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    out['hour_cos'] = np.cos(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    for c in [\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_posts_at_request'\n",
        "    ]:\n",
        "        if c in df.columns:\n",
        "            out[c] = pd.to_numeric(df[c], errors='coerce').astype(np.float32)\n",
        "        else:\n",
        "            out[c] = 0.0\n",
        "    for c in ['title_len','body_len','title_body_ratio',\n",
        "              'requester_upvotes_minus_downvotes_at_request',\n",
        "              'requester_upvotes_plus_downvotes_at_request',\n",
        "              'requester_number_of_comments_at_request',\n",
        "              'requester_number_of_posts_at_request']:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(out[c].clip(lower=0)).astype(np.float32)\n",
        "    out = out.replace([np.inf,-np.inf], 0).fillna(0).astype(np.float32)\n",
        "    return out\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Seeds and rounds per expert\n",
        "seeds = [42, 1337, 2025, 614, 2718]\n",
        "nrounds_v1, nrounds_v2, nrounds_meta, nrounds_emb = 200, 120, 80, 150\n",
        "\n",
        "# Refit Dense v1/v2 + Meta XGB on full train with 5-seed bag\n",
        "print('Refit 5-seed bag: Dense v1/v2 + Meta ...')\n",
        "txt_tr = clean_text_series(combine_text(train)); txt_te = clean_text_series(combine_text(test))\n",
        "subs_tr = build_subs(train); subs_te = build_subs(test)\n",
        "meta_tr = build_meta_v1(train).astype(np.float32).values\n",
        "meta_te = build_meta_v1(test).astype(np.float32).values\n",
        "\n",
        "word_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=60000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "char_params = dict(analyzer='char_wb', ngram_range=(3,5), lowercase=True, min_df=3, max_features=60000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "subs_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=20000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "\n",
        "t0 = time.time()\n",
        "tfidf_w = TfidfVectorizer(**word_params); Xw_tr = tfidf_w.fit_transform(txt_tr); Xw_te = tfidf_w.transform(txt_te)\n",
        "tfidf_c = TfidfVectorizer(**char_params); Xc_tr = tfidf_c.fit_transform(txt_tr); Xc_te = tfidf_c.transform(txt_te)\n",
        "tfidf_s = TfidfVectorizer(**subs_params); Xs_tr = tfidf_s.fit_transform(subs_tr); Xs_te = tfidf_s.transform(subs_te)\n",
        "print(f'TF-IDF built in {time.time()-t0:.1f}s')\n",
        "\n",
        "# Dense v1 features (SVD 150/150/50 + meta)\n",
        "svd_w_v1, svd_c_v1, svd_s_v1 = TruncatedSVD(n_components=150, random_state=42), TruncatedSVD(n_components=150, random_state=42), TruncatedSVD(n_components=50, random_state=42)\n",
        "Zw_tr = svd_w_v1.fit_transform(Xw_tr); Zw_te = svd_w_v1.transform(Xw_te)\n",
        "Zc_tr = svd_c_v1.fit_transform(Xc_tr); Zc_te = svd_c_v1.transform(Xc_te)\n",
        "Zs_tr = svd_s_v1.fit_transform(Xs_tr); Zs_te = svd_s_v1.transform(Xs_te)\n",
        "sc_meta = StandardScaler(with_mean=True, with_std=True)\n",
        "Mtr_s = sc_meta.fit_transform(meta_tr).astype(np.float32); Mte_s = sc_meta.transform(meta_te).astype(np.float32)\n",
        "sc_all_v1 = StandardScaler(with_mean=True, with_std=True)\n",
        "Xtr_v1 = sc_all_v1.fit_transform(np.hstack([Zw_tr, Zc_tr, Zs_tr, Mtr_s]).astype(np.float32))\n",
        "Xte_v1 = sc_all_v1.transform(np.hstack([Zw_te, Zc_te, Zs_te, Mte_s]).astype(np.float32))\n",
        "\n",
        "# Dense v2 features (SVD 250/120 + meta, no subs)\n",
        "svd_w_v2, svd_c_v2 = TruncatedSVD(n_components=250, random_state=42), TruncatedSVD(n_components=120, random_state=42)\n",
        "Zw2_tr = svd_w_v2.fit_transform(Xw_tr); Zw2_te = svd_w_v2.transform(Xw_te)\n",
        "Zc2_tr = svd_c_v2.fit_transform(Xc_tr); Zc2_te = svd_c_v2.transform(Xc_te)\n",
        "sc_meta2 = StandardScaler(with_mean=True, with_std=True)\n",
        "Mtr2_s = sc_meta2.fit_transform(meta_tr).astype(np.float32); Mte2_s = sc_meta2.transform(meta_te).astype(np.float32)\n",
        "sc_all_v2 = StandardScaler(with_mean=True, with_std=True)\n",
        "Xtr_v2 = sc_all_v2.fit_transform(np.hstack([Zw2_tr, Zc2_tr, Mtr2_s]).astype(np.float32))\n",
        "Xte_v2 = sc_all_v2.transform(np.hstack([Zw2_te, Zc2_te, Mte2_s]).astype(np.float32))\n",
        "\n",
        "# Meta-only features\n",
        "sc_m = StandardScaler(with_mean=True, with_std=True)\n",
        "Xtr_m = sc_m.fit_transform(meta_tr).astype(np.float32)\n",
        "Xte_m = sc_m.transform(meta_te).astype(np.float32)\n",
        "\n",
        "neg = float((y==0).sum()); pos = float((y==1).sum()); spw = (neg/max(pos,1.0)) if pos>0 else 1.0\n",
        "params_v1 = dict(objective='binary:logistic', eval_metric='auc', max_depth=3, eta=0.035, subsample=0.8, colsample_bytree=0.6, min_child_weight=8, reg_alpha=1.0, reg_lambda=4.0, gamma=0.1, device='cuda', tree_method='hist', scale_pos_weight=spw)\n",
        "params_v2 = dict(objective='binary:logistic', eval_metric='auc', max_depth=3, eta=0.03, min_child_weight=10, subsample=0.75, colsample_bytree=0.7, reg_alpha=1.5, reg_lambda=5.0, gamma=0.2, device='cuda', tree_method='hist', scale_pos_weight=spw)\n",
        "params_meta = dict(objective='binary:logistic', eval_metric='auc', max_depth=3, eta=0.05, min_child_weight=8, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.5, reg_lambda=3.0, gamma=0.0, device='cuda', tree_method='hist', scale_pos_weight=spw)\n",
        "\n",
        "dtr_v1 = xgb.DMatrix(Xtr_v1, label=y); dte_v1 = xgb.DMatrix(Xte_v1)\n",
        "dtr_v2 = xgb.DMatrix(Xtr_v2, label=y); dte_v2 = xgb.DMatrix(Xte_v2)\n",
        "dtr_m  = xgb.DMatrix(Xtr_m,  label=y); dte_m  = xgb.DMatrix(Xte_m)\n",
        "\n",
        "p_d1_seeds, p_d2_seeds, p_meta_seeds = [], [], []\n",
        "for sd in seeds:\n",
        "    b1 = xgb.train({**params_v1, 'seed': sd}, dtr_v1, num_boost_round=nrounds_v1, verbose_eval=False)\n",
        "    b2 = xgb.train({**params_v2, 'seed': sd}, dtr_v2, num_boost_round=nrounds_v2, verbose_eval=False)\n",
        "    bm = xgb.train({**params_meta, 'seed': sd}, dtr_m, num_boost_round=nrounds_meta, verbose_eval=False)\n",
        "    p_d1_seeds.append(b1.predict(dte_v1).astype(np.float32))\n",
        "    p_d2_seeds.append(b2.predict(dte_v2).astype(np.float32))\n",
        "    p_meta_seeds.append(bm.predict(dte_m).astype(np.float32))\n",
        "print('Dense v1/v2/Meta 5-seed bagging complete')\n",
        "\n",
        "p_te_d1_bag = np.mean(p_d1_seeds, axis=0).astype(np.float32)\n",
        "p_te_d2_bag = np.mean(p_d2_seeds, axis=0).astype(np.float32)\n",
        "p_te_meta_bag = np.mean(p_meta_seeds, axis=0).astype(np.float32)\n",
        "\n",
        "# Refit Emb+Meta on full train with 5-seed bag\n",
        "print('Refit 5-seed bag: Emb+Meta ...')\n",
        "Emb_tr = np.load('emb_minilm_tr.npy').astype(np.float32)\n",
        "Emb_te = np.load('emb_minilm_te.npy').astype(np.float32)\n",
        "Meta_tr = np.load('meta_v1_tr.npy').astype(np.float32)\n",
        "Meta_te = np.load('meta_v1_te.npy').astype(np.float32)\n",
        "sc_e = StandardScaler(with_mean=True, with_std=True)\n",
        "Xtr_e = sc_e.fit_transform(np.hstack([Emb_tr, Meta_tr]).astype(np.float32))\n",
        "Xte_e = sc_e.transform(np.hstack([Emb_te, Meta_te]).astype(np.float32))\n",
        "params_emb = dict(objective='binary:logistic', eval_metric='auc', max_depth=3, eta=0.05, subsample=0.8, colsample_bytree=0.6, min_child_weight=8, reg_alpha=0.5, reg_lambda=3.0, gamma=0.0, device='cuda', tree_method='hist', scale_pos_weight=spw)\n",
        "dtr_e = xgb.DMatrix(Xtr_e, label=y); dte_e = xgb.DMatrix(Xte_e)\n",
        "p_emb_seeds = []\n",
        "for sd in seeds:\n",
        "    be = xgb.train({**params_emb, 'seed': sd}, dtr_e, num_boost_round=nrounds_emb, verbose_eval=False)\n",
        "    p_emb_seeds.append(be.predict(dte_e).astype(np.float32))\n",
        "p_te_emb_bag = np.mean(p_emb_seeds, axis=0).astype(np.float32)\n",
        "print('Emb+Meta 5-seed bagging complete')\n",
        "\n",
        "# Load LR refit predictions (from earlier refit cell)\n",
        "p_te_lr_with = np.load('test_refit_lr_withsub_meta.npy')\n",
        "p_te_lr_ns = np.load('test_refit_lr_nosub_meta.npy')\n",
        "\n",
        "# Blend using S24 best 6-way weights on logits\n",
        "g = 0.95  # LR mix weight from S24 best cfg\n",
        "w_emb = 0.20; w_lr = 0.28; w_d1 = 0.105; w_d2 = 0.195; w_meta = 0.22\n",
        "z_lr_mix = (1.0 - g)*to_logit(p_te_lr_with) + g*to_logit(p_te_lr_ns)\n",
        "z_d1 = to_logit(p_te_d1_bag); z_d2 = to_logit(p_te_d2_bag); z_meta = to_logit(p_te_meta_bag); z_emb = to_logit(p_te_emb_bag)\n",
        "z_blend = w_lr*z_lr_mix + w_d1*z_d1 + w_d2*z_d2 + w_meta*z_meta + w_emb*z_emb\n",
        "p_final = sigmoid(z_blend).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: ids, target_col: p_final})\n",
        "sub.to_csv('submission_time_blend_with_emb_refit5bag.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Promoted 6-way refit 5-seed bag blend to submission.csv. Head:')\n",
        "print(sub.head())\n",
        "\n",
        "# Hedges: 15% shrink over 5 logit components (LRmix,D1,D2,Meta,Emb) and equal-prob 6-way\n",
        "w_vec = np.array([w_lr, w_d1, w_d2, w_meta, w_emb], dtype=np.float64)\n",
        "w_eq = np.ones_like(w_vec) / len(w_vec)\n",
        "alpha = 0.15\n",
        "w_shr = ((1.0 - alpha)*w_vec + alpha*w_eq); w_shr = (w_shr / w_shr.sum()).astype(np.float64)\n",
        "z_shr = w_shr[0]*z_lr_mix + w_shr[1]*z_d1 + w_shr[2]*z_d2 + w_shr[3]*z_meta + w_shr[4]*z_emb\n",
        "p_shr = sigmoid(z_shr).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: p_shr}).to_csv('submission_time_blend_with_emb_refit5bag_shrunk.csv', index=False)\n",
        "p_eq6 = np.clip((sigmoid((1.0 - g)*to_logit(p_te_lr_with) + g*to_logit(p_te_lr_ns)) + p_te_d1_bag + p_te_d2_bag + p_te_meta_bag + p_te_emb_bag) / 5.0, 0.01, 0.99).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: p_eq6}).to_csv('submission_time_equal5_refit5bag_prob.csv', index=False)\n",
        "print('Wrote hedges for refit5bag blend.')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refit 5-seed bag: Dense v1/v2 + Meta ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/1787563273.py:39: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/1787563273.py:39: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF built in 2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense v1/v2/Meta 5-seed bagging complete\nRefit 5-seed bag: Emb+Meta ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emb+Meta 5-seed bagging complete\nPromoted 6-way refit 5-seed bag blend to submission.csv. Head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.498361\n1   t3_roiuw                  0.468186\n2   t3_mjnbq                  0.396775\n3   t3_t8wd1                  0.376720\n4  t3_1m4zxu                  0.421771\nWrote hedges for refit5bag blend.\n"
          ]
        }
      ]
    },
    {
      "id": "bf349d96-3a72-46bd-9a17-74642711c285",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S26: Upgraded LR (word 1-3, char_wb 2-6, min_df=2, 250k) + meta_v1+{token_count,sentence_count}; models: main/title/body; L2 vs ElasticNet per expert\n",
        "import numpy as np, pandas as pd, time, gc, json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body(df):\n",
        "    return df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "def txt_main(df):\n",
        "    return (get_title(df) + ' \\n ' + get_body(df)).astype(str)\n",
        "\n",
        "def build_meta_up(df):\n",
        "    title = get_title(df); body = get_body(df)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    out['title_len'] = title.str.len().astype(np.float32)\n",
        "    out['body_len'] = body.str.len().astype(np.float32)\n",
        "    out['title_body_ratio'] = (out['title_len'] / (1.0 + out['body_len'])).astype(np.float32)\n",
        "    out['has_url'] = body.str.contains(r'https?://', regex=True).astype(np.float32)\n",
        "    out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n",
        "    # add token_count and sentence_count (log1p later)\n",
        "    out['token_count'] = (title.str.count(r'\\w+') + body.str.count(r'\\w+')).astype(np.float32)\n",
        "    out['sentence_count'] = body.str.count(r'[\\.!?]').astype(np.float32)\n",
        "    if 'unix_timestamp_of_request' in df.columns:\n",
        "        dt = pd.to_datetime(df['unix_timestamp_of_request'], unit='s', utc=True, errors='coerce')\n",
        "    else:\n",
        "        dt = pd.to_datetime(0, unit='s', utc=True) + pd.to_timedelta(np.zeros(len(df)), unit='s')\n",
        "    hour = dt.dt.hour.fillna(0).astype(np.float32)\n",
        "    out['dayofweek'] = dt.dt.dayofweek.fillna(0).astype(np.float32)\n",
        "    out['is_weekend'] = out['dayofweek'].isin([5,6]).astype(np.float32)\n",
        "    out['hour_sin'] = np.sin(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    out['hour_cos'] = np.cos(2*np.pi*hour/24.0).astype(np.float32)\n",
        "    for c in [\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_posts_at_request'\n",
        "    ]:\n",
        "        if c in df.columns:\n",
        "            out[c] = pd.to_numeric(df[c], errors='coerce').astype(np.float32)\n",
        "        else:\n",
        "            out[c] = 0.0\n",
        "    for c in ['title_len','body_len','title_body_ratio','token_count','sentence_count',\n",
        "              'requester_upvotes_minus_downvotes_at_request','requester_upvotes_plus_downvotes_at_request',\n",
        "              'requester_number_of_comments_at_request','requester_number_of_posts_at_request']:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(out[c].clip(lower=0)).astype(np.float32)\n",
        "    out = out.replace([np.inf,-np.inf], 0).fillna(0).astype(np.float32)\n",
        "    return out\n",
        "\n",
        "# 6-block forward-chaining folds and mask\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'LR upgraded Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}')\n",
        "\n",
        "# Vectorizers per spec\n",
        "word_params = dict(analyzer='word', ngram_range=(1,3), lowercase=True, min_df=2, max_features=250_000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "char_params = dict(analyzer='char_wb', ngram_range=(2,6), lowercase=True, min_df=2, max_features=250_000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "\n",
        "texts_main_tr, texts_main_te = txt_main(train), txt_main(test)\n",
        "texts_title_tr, texts_title_te = get_title(train), get_title(test)\n",
        "texts_body_tr, texts_body_te = get_body(train), get_body(test)\n",
        "meta_te_full = build_meta_up(test).astype(np.float32).values\n",
        "\n",
        "def run_lr_view(view_name: str, tr_text: pd.Series, te_text: pd.Series, tag_out: str):\n",
        "    # Returns best OOF/test and config dict\n",
        "    l2_C_grid = [0.3, 0.5, 0.8, 1.2, 2.0]\n",
        "    en_l1_grid = [0.05, 0.10, 0.20]\n",
        "    en_C_grid = [0.5, 0.8, 1.2]\n",
        "    best_auc, best_kind, best_cfg = -1.0, None, None\n",
        "    best_oof, best_test = None, None\n",
        "    # Helper to train one LR config across folds, score OOF on mask, return test avg\n",
        "    def train_cfg(penalty_kind: str, C_val: float, l1_ratio: float|None):\n",
        "        oof = np.zeros(n, dtype=np.float32)\n",
        "        test_fold_preds = []\n",
        "        for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "            t0 = time.time()\n",
        "            tfidf_w = TfidfVectorizer(**word_params)\n",
        "            Xw_tr = tfidf_w.fit_transform(tr_text.iloc[tr_idx]); Xw_va = tfidf_w.transform(tr_text.iloc[va_idx]); Xw_te = tfidf_w.transform(te_text)\n",
        "            tfidf_c = TfidfVectorizer(**char_params)\n",
        "            Xc_tr = tfidf_c.fit_transform(tr_text.iloc[tr_idx]); Xc_va = tfidf_c.transform(tr_text.iloc[va_idx]); Xc_te = tfidf_c.transform(te_text)\n",
        "            # Meta per fold and scale\n",
        "            M_tr = build_meta_up(train.iloc[tr_idx]).astype(np.float32).values\n",
        "            M_va = build_meta_up(train.iloc[va_idx]).astype(np.float32).values\n",
        "            scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "            M_tr_s = scaler.fit_transform(M_tr).astype(np.float32)\n",
        "            M_va_s = scaler.transform(M_va).astype(np.float32)\n",
        "            M_te_s = scaler.transform(meta_te_full).astype(np.float32)\n",
        "            X_tr = hstack([Xw_tr, Xc_tr, csr_matrix(M_tr_s)], format='csr')\n",
        "            X_va = hstack([Xw_va, Xc_va, csr_matrix(M_va_s)], format='csr')\n",
        "            X_te = hstack([Xw_te, Xc_te, csr_matrix(M_te_s)], format='csr')\n",
        "            if penalty_kind == 'l2':\n",
        "                clf = LogisticRegression(solver='saga', penalty='l2', C=C_val, max_iter=4000, n_jobs=-1, random_state=42, verbose=0)\n",
        "            else:\n",
        "                clf = LogisticRegression(solver='saga', penalty='elasticnet', l1_ratio=l1_ratio, C=C_val, max_iter=4000, n_jobs=-1, random_state=42, verbose=0)\n",
        "            clf.fit(X_tr, y[tr_idx])\n",
        "            va_pred = clf.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "            te_pred = clf.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "            oof[va_idx] = va_pred; test_fold_preds.append(te_pred)\n",
        "            auc = roc_auc_score(y[va_idx], va_pred)\n",
        "            print(f'[{view_name} {penalty_kind} C={C_val} l1={l1_ratio}] Fold {fi} AUC: {auc:.5f} | {time.time()-t0:.1f}s | tr:{X_tr.shape}')\n",
        "            del tfidf_w, tfidf_c, Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, M_tr, M_va, scaler, M_tr_s, M_va_s, M_te_s, X_tr, X_va, X_te, clf; gc.collect()\n",
        "        auc_oof = roc_auc_score(y[mask], oof[mask])\n",
        "        te_avg = np.mean(test_fold_preds, axis=0).astype(np.float32)\n",
        "        return auc_oof, oof, te_avg\n",
        "    # L2 sweep\n",
        "    for C_val in l2_C_grid:\n",
        "        auc_here, oof_here, te_here = train_cfg('l2', C_val, None)\n",
        "        print(f'[{view_name}] L2 C={C_val} | OOF(time-mask) AUC: {auc_here:.5f}')\n",
        "        if auc_here > best_auc:\n",
        "            best_auc, best_kind = auc_here, ('l2', C_val, None)\n",
        "            best_oof, best_test = oof_here, te_here\n",
        "    l2_best_auc = best_auc\n",
        "    # ElasticNet sweep; keep only if >= l2_best + 0.002\n",
        "    en_best_auc, en_best = -1.0, None\n",
        "    en_best_oof, en_best_test = None, None\n",
        "    for l1 in en_l1_grid:\n",
        "        for C_val in en_C_grid:\n",
        "            auc_here, oof_here, te_here = train_cfg('en', C_val, l1)\n",
        "            print(f'[{view_name}] EN C={C_val} l1={l1} | OOF(time-mask) AUC: {auc_here:.5f}')\n",
        "            if auc_here > en_best_auc:\n",
        "                en_best_auc, en_best = auc_here, ('en', C_val, l1)\n",
        "                en_best_oof, en_best_test = oof_here, te_here\n",
        "    if en_best_auc >= l2_best_auc + 0.002:\n",
        "        best_auc, best_kind = en_best_auc, en_best\n",
        "        best_oof, best_test = en_best_oof, en_best_test\n",
        "    # Save\n",
        "    tag = f'lr_time_up_{tag_out}'\n",
        "    np.save(f'oof_{tag}.npy', best_oof.astype(np.float32))\n",
        "    np.save(f'test_{tag}.npy', best_test.astype(np.float32))\n",
        "    print(f'[{view_name}] BEST {best_kind} | OOF(time-mask) AUC: {best_auc:.5f} | saved as {tag}')\n",
        "    return dict(view=view_name, tag=tag, best=best_kind, auc=best_auc)\n",
        "\n",
        "t_start = time.time()\n",
        "res_main = run_lr_view('LR_main', texts_main_tr, texts_main_te, 'main')\n",
        "res_title = run_lr_view('LR_title', texts_title_tr, texts_title_te, 'title')\n",
        "res_body = run_lr_view('LR_body', texts_body_tr, texts_body_te, 'body')\n",
        "print('S26 finished in', f'{time.time()-t_start:.1f}s')\n",
        "with open('lr_upgraded_results.json','w') as f:\n",
        "    json.dump({'main':res_main,'title':res_title,'body':res_body}, f, indent=2)\n",
        "print('Saved lr_upgraded_results.json')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR upgraded Time-CV: 5 folds; validated 2398/2878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main l2 C=0.3 l1=None] Fold 1 AUC: 0.72635 | 9.2s | tr:(480, 35930)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main l2 C=0.3 l1=None] Fold 2 AUC: 0.67470 | 19.9s | tr:(960, 58405)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main l2 C=0.3 l1=None] Fold 3 AUC: 0.63685 | 26.2s | tr:(1440, 75903)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main l2 C=0.3 l1=None] Fold 4 AUC: 0.63168 | 36.8s | tr:(1920, 90003)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main l2 C=0.3 l1=None] Fold 5 AUC: 0.63657 | 45.3s | tr:(2399, 102312)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main] L2 C=0.3 | OOF(time-mask) AUC: 0.66097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main l2 C=0.5 l1=None] Fold 1 AUC: 0.72832 | 9.2s | tr:(480, 35930)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main l2 C=0.5 l1=None] Fold 2 AUC: 0.67576 | 22.3s | tr:(960, 58405)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main l2 C=0.5 l1=None] Fold 3 AUC: 0.63483 | 30.0s | tr:(1440, 75903)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main l2 C=0.5 l1=None] Fold 4 AUC: 0.63357 | 45.6s | tr:(1920, 90003)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main l2 C=0.5 l1=None] Fold 5 AUC: 0.64093 | 56.0s | tr:(2399, 102312)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main] L2 C=0.5 | OOF(time-mask) AUC: 0.66262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main l2 C=0.8 l1=None] Fold 1 AUC: 0.73005 | 8.5s | tr:(480, 35930)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main l2 C=0.8 l1=None] Fold 2 AUC: 0.67464 | 24.6s | tr:(960, 58405)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main l2 C=0.8 l1=None] Fold 3 AUC: 0.63094 | 34.9s | tr:(1440, 75903)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main l2 C=0.8 l1=None] Fold 4 AUC: 0.63415 | 56.0s | tr:(1920, 90003)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main l2 C=0.8 l1=None] Fold 5 AUC: 0.64223 | 72.0s | tr:(2399, 102312)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main] L2 C=0.8 | OOF(time-mask) AUC: 0.66257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_main l2 C=1.2 l1=None] Fold 1 AUC: 0.73050 | 6.5s | tr:(480, 35930)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n/tmp/ipykernel_11141/3939276029.py:28: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['has_img'] = body.str.contains(r'(imgur|jpg|jpeg|png|gif)', regex=True).astype(np.float32)\n"
          ]
        }
      ]
    },
    {
      "id": "fa876abf-c733-4318-a6b3-184e3e420326",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S27: Build all-mpnet-base-v2 sentence embeddings (title+body) with local HF cache; save .npy\n",
        "import os, time, gc, numpy as np, pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Ensure HF caches are writable (reuse same cache dir as S22 recovery)\n",
        "cache_dir = os.path.abspath('hf_cache')\n",
        "os.makedirs(cache_dir, exist_ok=True)\n",
        "os.environ['HF_HOME'] = cache_dir\n",
        "os.environ['HUGGINGFACE_HUB_CACHE'] = cache_dir\n",
        "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
        "os.environ['SENTENCE_TRANSFORMERS_HOME'] = cache_dir\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "\n",
        "def get_title(df):\n",
        "    return df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "def get_body(df):\n",
        "    return df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "def build_text(df):\n",
        "    return (get_title(df) + '\\n' + get_body(df)).astype(str)\n",
        "\n",
        "texts_tr = build_text(train).tolist()\n",
        "texts_te = build_text(test).tolist()\n",
        "print(f'Texts prepared: train {len(texts_tr)}, test {len(texts_te)}')\n",
        "\n",
        "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
        "print(f'Loading SentenceTransformer {model_name} with cache_folder={cache_dir}')\n",
        "model = SentenceTransformer(model_name, device='cuda', cache_folder=cache_dir)\n",
        "\n",
        "print('Encoding MPNet embeddings on GPU...')\n",
        "t0 = time.time()\n",
        "emb_tr = model.encode(texts_tr, batch_size=128, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True, device='cuda')\n",
        "emb_te = model.encode(texts_te, batch_size=128, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True, device='cuda')\n",
        "print(f'MPNet embeddings done in {time.time()-t0:.1f}s | shapes tr:{emb_tr.shape} te:{emb_te.shape}')\n",
        "\n",
        "np.save('emb_mpnet_tr.npy', emb_tr.astype(np.float32))\n",
        "np.save('emb_mpnet_te.npy', emb_te.astype(np.float32))\n",
        "print('Saved emb_mpnet_tr.npy and emb_mpnet_te.npy')\n",
        "\n",
        "del model, emb_tr, emb_te, texts_tr, texts_te\n",
        "gc.collect()\n",
        "print('S27 complete.')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texts prepared: train 2878, test 1162\nLoading SentenceTransformer sentence-transformers/all-mpnet-base-v2 with cache_folder=/app/agent_run_states/random-acts-of-pizza-spray-20250912-053053/hf_cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding MPNet embeddings on GPU...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:   0%|          | 0/23 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:   4%|\u258d         | 1/23 [00:02<01:04,  2.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:   9%|\u258a         | 2/23 [00:05<00:56,  2.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  13%|\u2588\u258e        | 3/23 [00:07<00:47,  2.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  17%|\u2588\u258b        | 4/23 [00:09<00:40,  2.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  22%|\u2588\u2588\u258f       | 5/23 [00:10<00:35,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  26%|\u2588\u2588\u258c       | 6/23 [00:12<00:29,  1.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  30%|\u2588\u2588\u2588       | 7/23 [00:13<00:24,  1.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  35%|\u2588\u2588\u2588\u258d      | 8/23 [00:14<00:20,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  39%|\u2588\u2588\u2588\u2589      | 9/23 [00:15<00:17,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  43%|\u2588\u2588\u2588\u2588\u258e     | 10/23 [00:16<00:14,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  48%|\u2588\u2588\u2588\u2588\u258a     | 11/23 [00:17<00:12,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 12/23 [00:17<00:10,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 13/23 [00:18<00:08,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 14/23 [00:19<00:08,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 15/23 [00:20<00:06,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 16/23 [00:20<00:05,  1.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 17/23 [00:21<00:04,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 18/23 [00:22<00:03,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 19/23 [00:22<00:02,  1.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 20/23 [00:23<00:01,  1.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 21/23 [00:23<00:01,  1.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 22/23 [00:23<00:00,  2.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 23/23 [00:23<00:00,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 23/23 [00:23<00:00,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  10%|\u2588         | 1/10 [00:02<00:26,  2.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  20%|\u2588\u2588        | 2/10 [00:04<00:18,  2.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  30%|\u2588\u2588\u2588       | 3/10 [00:06<00:13,  1.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  40%|\u2588\u2588\u2588\u2588      | 4/10 [00:07<00:10,  1.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:08<00:07,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:09<00:05,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:10<00:03,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:11<00:01,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:11<00:00,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:11<00:00,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MPNet embeddings done in 35.5s | shapes tr:(2878, 768) te:(1162, 768)\nSaved emb_mpnet_tr.npy and emb_mpnet_te.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S27 complete.\n"
          ]
        }
      ]
    },
    {
      "id": "a7056c97-129b-49e4-a86e-2d2094741e9d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S28: Time-aware XGB on MPNet embeddings + meta_v1 (6-block CV, 3-seed bag); cache OOF/test\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import xgboost as xgb\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "# Load cached MPNet embeddings and meta_v1\n",
        "Emb_tr = np.load('emb_mpnet_tr.npy').astype(np.float32)\n",
        "Emb_te = np.load('emb_mpnet_te.npy').astype(np.float32)\n",
        "Meta_tr = np.load('meta_v1_tr.npy').astype(np.float32)\n",
        "Meta_te = np.load('meta_v1_te.npy').astype(np.float32)\n",
        "print('Loaded MPNet features:', Emb_tr.shape, Emb_te.shape, Meta_tr.shape, Meta_te.shape)\n",
        "\n",
        "# 6-block forward-chaining folds and mask\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "folds = []; mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    va_idx = np.array(blocks[i]); tr_idx = np.concatenate(blocks[:i])\n",
        "    folds.append((tr_idx, va_idx)); mask[va_idx] = True\n",
        "print(f'MPNet+Meta Time-CV: {len(folds)} folds; validated {mask.sum()}/{n}')\n",
        "\n",
        "# XGB params (as in S23, with slight colsample room for larger dims)\n",
        "base_params = dict(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='auc',\n",
        "    max_depth=3,\n",
        "    eta=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.6,\n",
        "    min_child_weight=8,\n",
        "    reg_alpha=0.5,\n",
        "    reg_lambda=3.0,\n",
        "    gamma=0.0,\n",
        "    device='cuda',\n",
        "    tree_method='hist'\n",
        ")\n",
        "num_boost_round = 8000\n",
        "early_stopping_rounds = 100\n",
        "seeds = [42, 1337, 2025]\n",
        "\n",
        "oof_sum = np.zeros(n, dtype=np.float64)\n",
        "oof_cnt = np.zeros(n, dtype=np.float64)\n",
        "test_seed_preds = []\n",
        "\n",
        "for si, seed in enumerate(seeds, 1):\n",
        "    print(f'=== MPNet+Meta Seed {seed} ({si}/{len(seeds)}) ===')\n",
        "    params = dict(base_params); params['seed'] = seed\n",
        "    oof_seed = np.zeros(n, dtype=np.float32)\n",
        "    test_folds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "        t0 = time.time()\n",
        "        Xtr_raw = np.hstack([Emb_tr[tr_idx], Meta_tr[tr_idx]]).astype(np.float32)\n",
        "        Xva_raw = np.hstack([Emb_tr[va_idx], Meta_tr[va_idx]]).astype(np.float32)\n",
        "        Xte_raw = np.hstack([Emb_te, Meta_te]).astype(np.float32)\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "        Xtr = scaler.fit_transform(Xtr_raw).astype(np.float32)\n",
        "        Xva = scaler.transform(Xva_raw).astype(np.float32)\n",
        "        Xte = scaler.transform(Xte_raw).astype(np.float32)\n",
        "        pos = float((y[tr_idx] == 1).sum()); neg = float((y[tr_idx] == 0).sum())\n",
        "        params['scale_pos_weight'] = (neg / max(pos, 1.0)) if pos > 0 else 1.0\n",
        "        dtr = xgb.DMatrix(Xtr, label=y[tr_idx])\n",
        "        dva = xgb.DMatrix(Xva, label=y[va_idx])\n",
        "        dte = xgb.DMatrix(Xte)\n",
        "        booster = xgb.train(params, dtr, num_boost_round=num_boost_round, evals=[(dva, 'valid')], early_stopping_rounds=early_stopping_rounds, verbose_eval=False)\n",
        "        va_pred = booster.predict(dva, iteration_range=(0, booster.best_iteration+1)).astype(np.float32)\n",
        "        te_pred = booster.predict(dte, iteration_range=(0, booster.best_iteration+1)).astype(np.float32)\n",
        "        oof_seed[va_idx] = va_pred; test_folds.append(te_pred)\n",
        "        auc = roc_auc_score(y[va_idx], va_pred)\n",
        "        print(f'[MPNet Seed {seed} Fold {fi}] best_iter={booster.best_iteration} | spw={params[\"scale_pos_weight\"]:.2f} | AUC: {auc:.5f} | {time.time()-t0:.1f}s | shapes tr:{Xtr.shape}')\n",
        "        del Xtr_raw, Xva_raw, Xte_raw, Xtr, Xva, Xte, dtr, dva, dte, booster, scaler; gc.collect()\n",
        "    seed_auc = roc_auc_score(y[mask], oof_seed[mask])\n",
        "    print(f'[MPNet Seed {seed}] OOF AUC (validated only): {seed_auc:.5f}')\n",
        "    oof_sum[mask] += oof_seed[mask]; oof_cnt[mask] += 1.0\n",
        "    test_seed_preds.append(np.mean(test_folds, axis=0).astype(np.float64))\n",
        "    del oof_seed, test_folds; gc.collect()\n",
        "\n",
        "oof_avg = np.zeros(n, dtype=np.float32)\n",
        "oof_avg[mask] = (oof_sum[mask] / np.maximum(oof_cnt[mask], 1.0)).astype(np.float32)\n",
        "test_avg = np.mean(test_seed_preds, axis=0).astype(np.float32)\n",
        "auc_oof = roc_auc_score(y[mask], oof_avg[mask])\n",
        "print(f'MPNet+Meta Time-CV OOF AUC (validated only, 3-seed avg): {auc_oof:.5f}')\n",
        "np.save('oof_xgb_emb_mpnet_time.npy', oof_avg.astype(np.float32))\n",
        "np.save('test_xgb_emb_mpnet_time.npy', test_avg)\n",
        "print('Saved oof_xgb_emb_mpnet_time.npy and test_xgb_emb_mpnet_time.npy')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded MPNet features: (2878, 768) (1162, 768) (2878, 14) (1162, 14)\nMPNet+Meta Time-CV: 5 folds; validated 2398/2878\n=== MPNet+Meta Seed 42 (1/3) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MPNet Seed 42 Fold 1] best_iter=99 | spw=1.94 | AUC: 0.67585 | 0.8s | shapes tr:(480, 782)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MPNet Seed 42 Fold 2] best_iter=161 | spw=2.33 | AUC: 0.67840 | 0.8s | shapes tr:(960, 782)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MPNet Seed 42 Fold 3] best_iter=10 | spw=2.49 | AUC: 0.63069 | 0.4s | shapes tr:(1440, 782)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MPNet Seed 42 Fold 4] best_iter=7 | spw=2.79 | AUC: 0.61982 | 0.4s | shapes tr:(1920, 782)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MPNet Seed 42 Fold 5] best_iter=29 | spw=2.83 | AUC: 0.61483 | 0.5s | shapes tr:(2399, 782)\n[MPNet Seed 42] OOF AUC (validated only): 0.62090\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== MPNet+Meta Seed 1337 (2/3) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MPNet Seed 1337 Fold 1] best_iter=149 | spw=1.94 | AUC: 0.67626 | 0.7s | shapes tr:(480, 782)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MPNet Seed 1337 Fold 2] best_iter=67 | spw=2.33 | AUC: 0.68780 | 0.5s | shapes tr:(960, 782)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MPNet Seed 1337 Fold 3] best_iter=7 | spw=2.49 | AUC: 0.60389 | 0.4s | shapes tr:(1440, 782)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MPNet Seed 1337 Fold 4] best_iter=13 | spw=2.79 | AUC: 0.62401 | 0.4s | shapes tr:(1920, 782)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MPNet Seed 1337 Fold 5] best_iter=22 | spw=2.83 | AUC: 0.60226 | 0.4s | shapes tr:(2399, 782)\n[MPNet Seed 1337] OOF AUC (validated only): 0.62912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== MPNet+Meta Seed 2025 (3/3) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MPNet Seed 2025 Fold 1] best_iter=113 | spw=1.94 | AUC: 0.68658 | 0.6s | shapes tr:(480, 782)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MPNet Seed 2025 Fold 2] best_iter=81 | spw=2.33 | AUC: 0.69627 | 0.6s | shapes tr:(960, 782)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MPNet Seed 2025 Fold 3] best_iter=65 | spw=2.49 | AUC: 0.61451 | 0.6s | shapes tr:(1440, 782)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MPNet Seed 2025 Fold 4] best_iter=28 | spw=2.79 | AUC: 0.63609 | 0.5s | shapes tr:(1920, 782)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MPNet Seed 2025 Fold 5] best_iter=14 | spw=2.83 | AUC: 0.63984 | 0.4s | shapes tr:(2399, 782)\n[MPNet Seed 2025] OOF AUC (validated only): 0.63786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MPNet+Meta Time-CV OOF AUC (validated only, 3-seed avg): 0.63820\nSaved oof_xgb_emb_mpnet_time.npy and test_xgb_emb_mpnet_time.npy\n"
          ]
        }
      ]
    },
    {
      "id": "b1a50c85-cefe-43c3-b3bd-5af16b3c679f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S29: 7-way constrained time-consistent logit blend (LRmix, D1, D2, Meta, Emb_MiniLM, Emb_MPNet); promote if best\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# 6-block forward-chaining time mask\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "n = len(train); k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "mask = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    mask[np.array(blocks[i])] = True\n",
        "print(f'Time-CV (6 blocks) validated count: {mask.sum()}/{n}')\n",
        "\n",
        "# Load 5 core time-bases + 2 embedding bases (MiniLM and MPNet) OOF/test\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy'); t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "o_lr_ns = np.load('oof_lr_time_nosub_meta.npy');  t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy');         t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_time_v2.npy');      t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta_time.npy');         t_meta = np.load('test_xgb_meta_time.npy')\n",
        "o_emb_minilm = np.load('oof_xgb_emb_meta_time.npy'); t_emb_minilm = np.load('test_xgb_emb_meta_time.npy')\n",
        "o_emb_mpnet  = np.load('oof_xgb_emb_mpnet_time.npy'); t_emb_mpnet  = np.load('test_xgb_emb_mpnet_time.npy')\n",
        "\n",
        "# Convert to logits\n",
        "z_lr_w, z_lr_ns = to_logit(o_lr_w), to_logit(o_lr_ns)\n",
        "z_d1, z_d2, z_meta = to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)\n",
        "z_emb_minilm, z_emb_mpnet = to_logit(o_emb_minilm), to_logit(o_emb_mpnet)\n",
        "tz_lr_w, tz_lr_ns = to_logit(t_lr_w), to_logit(t_lr_ns)\n",
        "tz_d1, tz_d2, tz_meta = to_logit(t_d1), to_logit(t_d2), to_logit(t_meta)\n",
        "tz_emb_minilm, tz_emb_mpnet = to_logit(t_emb_minilm), to_logit(t_emb_mpnet)\n",
        "\n",
        "# Grids (kept tight for speed/robustness)\n",
        "g_grid = [0.90, 0.95]\n",
        "meta_grid = [0.18, 0.20, 0.22]\n",
        "dense_tot_grid = [0.30, 0.35, 0.40]\n",
        "alpha_grid = [0.65, 0.80]  # split dense total into v1/v2: w_d2 = d_tot * alpha; w_d1 = d_tot - w_d2\n",
        "emb_minilm_grid = [0.08, 0.12, 0.16]\n",
        "emb_mpnet_grid = [0.04, 0.08, 0.12]\n",
        "\n",
        "best_auc, best_cfg = -1.0, None\n",
        "tried = 0\n",
        "for g in g_grid:\n",
        "    z_lr_mix = (1.0 - g)*z_lr_w + g*z_lr_ns\n",
        "    tz_lr_mix = (1.0 - g)*tz_lr_w + g*tz_lr_ns\n",
        "    for w_emb_min in emb_minilm_grid:\n",
        "        for w_emb_mp in emb_mpnet_grid:\n",
        "            for meta_w in meta_grid:\n",
        "                for d_tot in dense_tot_grid:\n",
        "                    # Remaining for LR after allocating meta, dense, embeddings\n",
        "                    w_lr_rem = 1.0 - (meta_w + d_tot + w_emb_min + w_emb_mp)\n",
        "                    if w_lr_rem <= 0 or w_lr_rem >= 1:\n",
        "                        continue\n",
        "                    for a in alpha_grid:\n",
        "                        w_d2 = d_tot * a\n",
        "                        w_d1 = d_tot - w_d2\n",
        "                        if w_d1 < 0 or w_d2 < 0:\n",
        "                            continue\n",
        "                        z_oof = (w_lr_rem*z_lr_mix +\n",
        "                                 w_d1*z_d1 + w_d2*z_d2 +\n",
        "                                 meta_w*z_meta +\n",
        "                                 w_emb_min*z_emb_minilm +\n",
        "                                 w_emb_mp*z_emb_mpnet)\n",
        "                        auc = roc_auc_score(y[mask], z_oof[mask])\n",
        "                        tried += 1\n",
        "                        if auc > best_auc:\n",
        "                            best_auc = auc\n",
        "                            best_cfg = dict(g=float(g), w_lr=float(w_lr_rem), w_d1=float(w_d1), w_d2=float(w_d2), w_meta=float(meta_w),\n",
        "                                            w_emb_min=float(w_emb_min), w_emb_mp=float(w_emb_mp), tz_lr_mix=tz_lr_mix)\n",
        "cfg_print = {k: v for k, v in best_cfg.items() if k != 'tz_lr_mix'} if best_cfg is not None else {}\n",
        "print(f'7-way grid tried {tried} | Best OOF(z,time-mask) AUC: {best_auc:.5f} | cfg={cfg_print}')\n",
        "\n",
        "# Build primary test prediction\n",
        "g = best_cfg['g']; w_lr = best_cfg['w_lr']; w_d1 = best_cfg['w_d1']; w_d2 = best_cfg['w_d2'];\n",
        "w_meta = best_cfg['w_meta']; w_emb_min = best_cfg['w_emb_min']; w_emb_mp = best_cfg['w_emb_mp']\n",
        "tz_lr_mix = best_cfg['tz_lr_mix']\n",
        "zt_best = (w_lr*tz_lr_mix + w_d1*tz_d1 + w_d2*tz_d2 + w_meta*tz_meta + w_emb_min*tz_emb_minilm + w_emb_mp*tz_emb_mpnet)\n",
        "pt_best = sigmoid(zt_best).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: ids, target_col: pt_best})\n",
        "sub.to_csv('submission_time_blend_with_two_emb.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Promoted 7-way (with MiniLM+MPNet) blend to submission.csv. Head:')\n",
        "print(sub.head())\n",
        "\n",
        "# Hedge: 15% shrink toward equal across 6 components (LRmix, D1, D2, Meta, Emb_min, Emb_mp) in logit space\n",
        "w_vec = np.array([w_lr, w_d1, w_d2, w_meta, w_emb_min, w_emb_mp], dtype=np.float64)\n",
        "w_eq = np.ones_like(w_vec) / len(w_vec)\n",
        "alpha = 0.15\n",
        "w_shr = ((1.0 - alpha)*w_vec + alpha*w_eq); w_shr = (w_shr / w_shr.sum()).astype(np.float64)\n",
        "zt_shr = (w_shr[0]*tz_lr_mix + w_shr[1]*tz_d1 + w_shr[2]*tz_d2 + w_shr[3]*tz_meta + w_shr[4]*tz_emb_minilm + w_shr[5]*tz_emb_mpnet)\n",
        "pt_shr = sigmoid(zt_shr).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: pt_shr}).to_csv('submission_time_blend_with_two_emb_shrunk.csv', index=False)\n",
        "print('Wrote hedge: submission_time_blend_with_two_emb_shrunk.csv')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV (6 blocks) validated count: 2398/2878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7-way grid tried 324 | Best OOF(z,time-mask) AUC: 0.68094 | cfg={'g': 0.95, 'w_lr': 0.24, 'w_d1': 0.10499999999999998, 'w_d2': 0.195, 'w_meta': 0.22, 'w_emb_min': 0.12, 'w_emb_mp': 0.12}\nPromoted 7-way (with MiniLM+MPNet) blend to submission.csv. Head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.517362\n1   t3_roiuw                  0.523317\n2   t3_mjnbq                  0.439774\n3   t3_t8wd1                  0.435404\n4  t3_1m4zxu                  0.509780\nWrote hedge: submission_time_blend_with_two_emb_shrunk.csv\n"
          ]
        }
      ]
    },
    {
      "id": "115472e7-4f94-4458-99fc-ad06d1248e6c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S30: Recency-weighted 7-way logit blends: full-mask, last-2 blocks, and gamma-decayed (gamma in {0.90,0.95,0.98})\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# 6-block forward-chaining blocks and masks\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "n = len(train)\n",
        "mask_full = np.zeros(n, dtype=bool)\n",
        "for i in range(1, k):\n",
        "    mask_full[np.array(blocks[i])] = True\n",
        "# last-2 validated blocks only (blocks[4], blocks[5])\n",
        "mask_last2 = np.zeros(n, dtype=bool)\n",
        "for i in [4,5]:\n",
        "    mask_last2[np.array(blocks[i])] = True\n",
        "print(f'Time-CV validated full: {mask_full.sum()}/{n} | last2: {mask_last2.sum()}')\n",
        "\n",
        "# Load bases (same as S29)\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy'); t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "o_lr_ns = np.load('oof_lr_time_nosub_meta.npy');  t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy');         t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_time_v2.npy');      t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta_time.npy');        t_meta = np.load('test_xgb_meta_time.npy')\n",
        "o_emb_min = np.load('oof_xgb_emb_meta_time.npy'); t_emb_min = np.load('test_xgb_emb_meta_time.npy')\n",
        "o_emb_mp  = np.load('oof_xgb_emb_mpnet_time.npy');t_emb_mp  = np.load('test_xgb_emb_mpnet_time.npy')\n",
        "\n",
        "# Convert to logits\n",
        "z_lr_w, z_lr_ns = to_logit(o_lr_w), to_logit(o_lr_ns)\n",
        "z_d1, z_d2, z_meta = to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)\n",
        "z_emn, z_emp = to_logit(o_emb_min), to_logit(o_emb_mp)\n",
        "tz_lr_w, tz_lr_ns = to_logit(t_lr_w), to_logit(t_lr_ns)\n",
        "tz_d1, tz_d2, tz_meta = to_logit(t_d1), to_logit(t_d2), to_logit(t_meta)\n",
        "tz_emn, tz_emp = to_logit(t_emb_min), to_logit(t_emb_mp)\n",
        "\n",
        "# Weight grids per expert priors\n",
        "g_grid = [0.90, 0.95, 0.97]\n",
        "meta_grid = [0.18, 0.20, 0.22]\n",
        "dense_tot_grid = [0.22, 0.30, 0.35, 0.40]\n",
        "alpha_grid = [0.50, 0.65, 0.80]  # split dense_total into v1/v2\n",
        "emn_grid = [0.10, 0.12, 0.15]\n",
        "emp_grid = [0.08, 0.10, 0.12]\n",
        "\n",
        "def search_best(mask, sample_weight=None):\n",
        "    best_auc, best_cfg = -1.0, None\n",
        "    tried = 0\n",
        "    for g in g_grid:\n",
        "        z_lr_mix = (1.0 - g)*z_lr_w + g*z_lr_ns\n",
        "        tz_lr_mix = (1.0 - g)*tz_lr_w + g*tz_lr_ns\n",
        "        for w_emn in emn_grid:\n",
        "            for w_emp in emp_grid:\n",
        "                for w_meta in meta_grid:\n",
        "                    for d_tot in dense_tot_grid:\n",
        "                        w_lr = 1.0 - (w_emn + w_emp + w_meta + d_tot)\n",
        "                        if w_lr <= 0 or w_lr >= 1:\n",
        "                            continue\n",
        "                        for a in alpha_grid:\n",
        "                            w_d2 = d_tot * a\n",
        "                            w_d1 = d_tot - w_d2\n",
        "                            if w_d1 < 0 or w_d2 < 0: continue\n",
        "                            z_oof = (w_lr*z_lr_mix + w_d1*z_d1 + w_d2*z_d2 + w_meta*z_meta + w_emn*z_emn + w_emp*z_emp)\n",
        "                            auc = roc_auc_score(y[mask], z_oof[mask], sample_weight=(sample_weight[mask] if sample_weight is not None else None))\n",
        "                            tried += 1\n",
        "                            if auc > best_auc:\n",
        "                                best_auc = auc\n",
        "                                best_cfg = dict(g=float(g), w_lr=float(w_lr), w_d1=float(w_d1), w_d2=float(w_d2), w_meta=float(w_meta),\n",
        "                                                w_emn=float(w_emn), w_emp=float(w_emp), tz_lr_mix=tz_lr_mix)\n",
        "    return best_auc, best_cfg, tried\n",
        "\n",
        "# 1) Full-mask (reconfirm best on all validated indices)\n",
        "auc_full, cfg_full, tried_full = search_best(mask_full)\n",
        "print(f'[Full] tried={tried_full} | best OOF(z) AUC={auc_full:.5f} | cfg={ {k:v for k,v in cfg_full.items() if k!=\"tz_lr_mix\"} }')\n",
        "\n",
        "# 2) Last-2 blocks only\n",
        "auc_last2, cfg_last2, tried_last2 = search_best(mask_last2)\n",
        "print(f'[Last2] tried={tried_last2} | best OOF(z,last2) AUC={auc_last2:.5f} | cfg={ {k:v for k,v in cfg_last2.items() if k!=\"tz_lr_mix\"} }')\n",
        "\n",
        "# 3) Time-decayed objective over validated (gamma in {0.90,0.95,0.98})\n",
        "best_gamma, best_auc_g, best_cfg_g = None, -1.0, None\n",
        "for gamma in [0.90, 0.95, 0.98]:\n",
        "    # assign weights by block recency: later blocks higher weight\n",
        "    w = np.zeros(n, dtype=np.float64)\n",
        "    for bi in range(1, k):\n",
        "        # older blocks get smaller weight; newer larger\n",
        "        age = (k - 1) - bi  # bi in 1..5; age 4..0\n",
        "        w[np.array(blocks[bi])] = (gamma ** age)\n",
        "    auc_g, cfg_g, _ = search_best(mask_full, sample_weight=w)\n",
        "    print(f'[Gamma {gamma}] best OOF(z,weighted) AUC={auc_g:.5f}')\n",
        "    if auc_g > best_auc_g:\n",
        "        best_auc_g, best_cfg_g, best_gamma = auc_g, cfg_g, gamma\n",
        "print(f'[Gamma-best] gamma={best_gamma} | AUC={best_auc_g:.5f} | cfg={ {k:v for k,v in best_cfg_g.items() if k!=\"tz_lr_mix\"} }')\n",
        "\n",
        "# Build submissions for each variant + 15% shrink hedges\n",
        "def build_and_save(tag, cfg):\n",
        "    g = cfg['g']; tz_lr_mix = cfg['tz_lr_mix']\n",
        "    w_lr, w_d1, w_d2, w_meta, w_emn, w_emp = cfg['w_lr'], cfg['w_d1'], cfg['w_d2'], cfg['w_meta'], cfg['w_emn'], cfg['w_emp']\n",
        "    zt = (w_lr*tz_lr_mix + w_d1*tz_d1 + w_d2*tz_d2 + w_meta*tz_meta + w_emn*tz_emn + w_emp*tz_emp)\n",
        "    pt = sigmoid(zt).astype(np.float32)\n",
        "    pd.DataFrame({id_col: ids, target_col: pt}).to_csv(f'submission_7way_{tag}.csv', index=False)\n",
        "    # Shrink hedge\n",
        "    w_vec = np.array([w_lr, w_d1, w_d2, w_meta, w_emn, w_emp], dtype=np.float64)\n",
        "    w_eq = np.ones_like(w_vec)/len(w_vec)\n",
        "    alpha = 0.15\n",
        "    w_shr = ((1.0 - alpha)*w_vec + alpha*w_eq); w_shr = (w_shr / w_shr.sum()).astype(np.float64)\n",
        "    zt_shr = (w_shr[0]*tz_lr_mix + w_shr[1]*tz_d1 + w_shr[2]*tz_d2 + w_shr[3]*tz_meta + w_shr[4]*tz_emn + w_shr[5]*tz_emp)\n",
        "    pt_shr = sigmoid(zt_shr).astype(np.float32)\n",
        "    pd.DataFrame({id_col: ids, target_col: pt_shr}).to_csv(f'submission_7way_{tag}_shrunk.csv', index=False)\n",
        "\n",
        "build_and_save('full', cfg_full)\n",
        "build_and_save('last2', cfg_last2)\n",
        "build_and_save(f'gamma{best_gamma:.2f}'.replace('.','p'), best_cfg_g)\n",
        "\n",
        "# Promote gamma-best as primary per expert advice\n",
        "prim = f'submission_7way_gamma{best_gamma:.2f}'.replace('.','p') + '.csv'\n",
        "pd.read_csv(prim).to_csv('submission.csv', index=False)\n",
        "print(f'Promoted {prim} to submission.csv')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time-CV validated full: 2398/2878 | last2: 958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Full] tried=972 | best OOF(z) AUC=0.68130 | cfg={'g': 0.97, 'w_lr': 0.24, 'w_d1': 0.15, 'w_d2': 0.15, 'w_meta': 0.22, 'w_emn': 0.12, 'w_emp': 0.12}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Last2] tried=972 | best OOF(z,last2) AUC=0.64682 | cfg={'g': 0.97, 'w_lr': 0.22999999999999998, 'w_d1': 0.15, 'w_d2': 0.15, 'w_meta': 0.2, 'w_emn': 0.15, 'w_emp': 0.12}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gamma 0.95] best OOF(z,weighted) AUC=0.67821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gamma 0.98] best OOF(z,weighted) AUC=0.68007\n[Gamma-best] gamma=0.98 | AUC=0.68007 | cfg={'g': 0.97, 'w_lr': 0.24, 'w_d1': 0.15, 'w_d2': 0.15, 'w_meta': 0.22, 'w_emn': 0.12, 'w_emp': 0.12}\nPromoted submission_7way_gamma0p98.csv to submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "1c608fac-012b-4dbf-9ef0-f5cf20032614",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S31: Block-5 optimizer + per-model recent interpolation (r in {0.24,0.30}) + gamma-weighted; hedged submissions (compact)\n",
        "import numpy as np, pandas as pd, os\n",
        "from sklearn.metrics import roc_auc_score\n",
        "id_col='request_id'; target_col='requester_received_pizza'\n",
        "train=pd.read_json('train.json'); test=pd.read_json('test.json')\n",
        "y=train[target_col].astype(int).values; ids=test[id_col].values\n",
        "def to_logit(p,eps=1e-6): p=np.clip(p.astype(np.float64),eps,1-eps); return np.log(p/(1-p))\n",
        "def sigmoid(z): return 1/(1+np.exp(-z))\n",
        "# 6-block forward-chaining, use last validated block (blocks[5])\n",
        "order=np.argsort(train['unix_timestamp_of_request'].values); k=6; blocks=np.array_split(order,k); n=len(train)\n",
        "mask_blk5=np.zeros(n,bool); mask_blk5[np.array(blocks[5])]=True\n",
        "print('Block-5 size:', int(mask_blk5.sum()), '/', n)\n",
        "# Load base OOF/test\n",
        "o_lr_w=np.load('oof_lr_time_withsub_meta.npy'); t_lr_w=np.load('test_lr_time_withsub_meta.npy')\n",
        "o_lr_ns=np.load('oof_lr_time_nosub_meta.npy'); t_lr_ns=np.load('test_lr_time_nosub_meta.npy')\n",
        "o_d1=np.load('oof_xgb_dense_time.npy'); t_d1=np.load('test_xgb_dense_time.npy')\n",
        "o_d2=np.load('oof_xgb_dense_time_v2.npy'); t_d2=np.load('test_xgb_dense_time_v2.npy')\n",
        "o_meta=np.load('oof_xgb_meta_time.npy'); t_meta=np.load('test_xgb_meta_time.npy')\n",
        "o_emn=np.load('oof_xgb_emb_meta_time.npy'); t_emn=np.load('test_xgb_emb_meta_time.npy')\n",
        "o_emp=np.load('oof_xgb_emb_mpnet_time.npy'); t_emp=np.load('test_xgb_emb_mpnet_time.npy')\n",
        "# Logits\n",
        "z_lr_w,z_lr_ns=to_logit(o_lr_w),to_logit(o_lr_ns); z_d1,z_d2,z_meta=to_logit(o_d1),to_logit(o_d2),to_logit(o_meta)\n",
        "z_emn,z_emp=to_logit(o_emn),to_logit(o_emp)\n",
        "tz_lr_w,tz_lr_ns=to_logit(t_lr_w),to_logit(t_lr_ns); tz_d1,tz_d2,tz_meta=to_logit(t_d1),to_logit(t_d2),to_logit(t_meta)\n",
        "tz_emn,tz_emp=to_logit(t_emn),to_logit(t_emp)\n",
        "# Recent test-only (avg of recent35/45) for eligible models\n",
        "def load_recent_avg_logit(prefix):\n",
        "    arrs=[]\n",
        "    for suf in ['_recent35.npy','_recent45.npy']:\n",
        "        p=prefix+suf\n",
        "        if os.path.exists(p):\n",
        "            try: arrs.append(to_logit(np.load(p)))\n",
        "            except: pass\n",
        "    return (np.mean(arrs,axis=0).astype(np.float64)) if arrs else None\n",
        "tz_lr_w_r=load_recent_avg_logit('test_lr_time_withsub_meta')\n",
        "tz_lr_ns_r=load_recent_avg_logit('test_lr_time_nosub_meta')\n",
        "tz_emn_r=load_recent_avg_logit('test_xgb_emb_meta_time')\n",
        "tz_emp_r=load_recent_avg_logit('test_xgb_emb_mpnet_time')\n",
        "print('Recent availability:', {'lr_w':tz_lr_w_r is not None,'lr_ns':tz_lr_ns_r is not None,'emn':tz_emn_r is not None,'emp':tz_emp_r is not None})\n",
        "# Grids per expert\n",
        "g_grid=[0.92,0.94,0.96,0.98]\n",
        "d_tot_grid=[0.10,0.14,0.18,0.22]; v1_frac_grid=[0.5,0.6,0.7]\n",
        "meta_grid=[0.16,0.18,0.20,0.22]; emn_grid=[0.10,0.12,0.14]; emp_grid=[0.08,0.10,0.12]\n",
        "def search_blk5():\n",
        "    best_auc=-1.0; best=None; tried=0\n",
        "    for g in g_grid:\n",
        "        z_lr_mix=(1-g)*z_lr_w+g*z_lr_ns\n",
        "        for d_tot in d_tot_grid:\n",
        "            for v1f in v1_frac_grid:\n",
        "                w_d1=d_tot*v1f; w_d2=d_tot-w_d1\n",
        "                if w_d1<0 or w_d2<0: continue\n",
        "                for w_meta in meta_grid:\n",
        "                    for w_emn in emn_grid:\n",
        "                        for w_emp in emp_grid:\n",
        "                            w_sum=d_tot+w_meta+w_emn+w_emp; w_lr=1.0-w_sum\n",
        "                            if w_lr<=0 or w_lr>=1: continue\n",
        "                            if not (0.24<=w_lr<=0.50): continue\n",
        "                            z=w_lr*z_lr_mix+w_d1*z_d1+w_d2*z_d2+w_meta*z_meta+w_emn*z_emn+w_emp*z_emp\n",
        "                            auc=roc_auc_score(y[mask_blk5], z[mask_blk5]); tried+=1\n",
        "                            if auc>best_auc:\n",
        "                                best_auc=auc; best=dict(g=float(g),w_lr=float(w_lr),w_d1=float(w_d1),w_d2=float(w_d2),w_meta=float(w_meta),w_emn=float(w_emn),w_emp=float(w_emp))\n",
        "    print('Blk5 tried', tried, '| best AUC', f'{best_auc:.5f}', '|', best); return best\n",
        "cfg=search_blk5()\n",
        "# Alpha grids (test-only interpolation); set to [0] if no recent\n",
        "aLR=[0.15,0.25,0.35] if (tz_lr_w_r is not None or tz_lr_ns_r is not None) else [0.0]\n",
        "aMN=[0.20,0.30,0.40] if (tz_emn_r is not None) else [0.0]\n",
        "aMP=[0.10,0.20,0.30] if (tz_emp_r is not None) else [0.0]\n",
        "def pick_alphas(cfg,r,tol=0.02):\n",
        "    wl,wmn,wmp=cfg['w_lr'],cfg['w_emn'],cfg['w_emp']\n",
        "    best=None; best_err=1e9; best_sum=9e9\n",
        "    for al in aLR:\n",
        "        for amn in aMN:\n",
        "            for amp in aMP:\n",
        "                s=wl*al+wmn*amn+wmp*amp; err=abs(s-r); sm=al+amn+amp\n",
        "                if (err<best_err) or (abs(err-best_err)<1e-12 and sm<best_sum):\n",
        "                    best_err=err; best_sum=sm; best=(al,amn,amp)\n",
        "    return best\n",
        "def build_sub(tag,cfg,alphas):\n",
        "    g=cfg['g']\n",
        "    tz_lr_mix_full=(1-g)*tz_lr_w+g*tz_lr_ns\n",
        "    z_w_r=tz_lr_w_r if tz_lr_w_r is not None else tz_lr_w\n",
        "    z_ns_r=tz_lr_ns_r if tz_lr_ns_r is not None else tz_lr_ns\n",
        "    tz_lr_mix_recent=(1-g)*z_w_r+g*z_ns_r\n",
        "    a_lr,a_mn,a_mp=alphas\n",
        "    tz_lr_mix=(1-a_lr)*tz_lr_mix_full+a_lr*tz_lr_mix_recent\n",
        "    tz_mn=(1-a_mn)*tz_emn + a_mn*(tz_emn_r if tz_emn_r is not None else tz_emn)\n",
        "    tz_mp=(1-a_mp)*tz_emp + a_mp*(tz_emp_r if tz_emp_r is not None else tz_emp)\n",
        "    zt=cfg['w_lr']*tz_lr_mix+cfg['w_d1']*tz_d1+cfg['w_d2']*tz_d2+cfg['w_meta']*tz_meta+cfg['w_emn']*tz_mn+cfg['w_emp']*tz_mp\n",
        "    pt=sigmoid(zt).astype(np.float32)\n",
        "    fn=f'submission_blk5_{tag}.csv'; pd.DataFrame({id_col:ids,target_col:pt}).to_csv(fn,index=False); print('Wrote',fn,'mean',float(pt.mean()))\n",
        "alphas24=pick_alphas(cfg,0.24); alphas30=pick_alphas(cfg,0.30)\n",
        "print('alphas r24/r30:', alphas24, alphas30)\n",
        "build_sub('r24', cfg, alphas24)\n",
        "build_sub('r30', cfg, alphas30)\n",
        "# Gamma-weighted variant over validated blocks (no recent interpolation)\n",
        "def search_gamma():\n",
        "    best=None; best_auc=-1; best_gm=None\n",
        "    for gamma in [0.995,0.998,0.9995]:\n",
        "        w=np.zeros(n,np.float64)\n",
        "        for bi in range(1,k):\n",
        "            age=(k-1)-bi; w[np.array(blocks[bi])]=gamma**age\n",
        "        best_auc_g=-1; best_cfg_g=None\n",
        "        for g in g_grid:\n",
        "            z_lr_mix=(1-g)*z_lr_w+g*z_lr_ns\n",
        "            for d_tot in d_tot_grid:\n",
        "                for v1f in v1_frac_grid:\n",
        "                    w_d1=d_tot*v1f; w_d2=d_tot-w_d1\n",
        "                    if w_d1<0 or w_d2<0: continue\n",
        "                    for w_meta in meta_grid:\n",
        "                        for w_emn in emn_grid:\n",
        "                            for w_emp in emp_grid:\n",
        "                                w_sum=d_tot+w_meta+w_emn+w_emp; w_lr=1-w_sum\n",
        "                                if w_lr<=0 or w_lr>=1 or not (0.24<=w_lr<=0.50): continue\n",
        "                                z=w_lr*z_lr_mix+w_d1*z_d1+w_d2*z_d2+w_meta*z_meta+w_emn*z_emn+w_emp*z_emp\n",
        "                                auc=roc_auc_score(y, z, sample_weight=w)\n",
        "                                if auc>best_auc_g: best_auc_g=auc; best_cfg_g=dict(g=float(g),w_lr=float(w_lr),w_d1=float(w_d1),w_d2=float(w_d2),w_meta=float(w_meta),w_emn=float(w_emn),w_emp=float(w_emp))\n",
        "        print('[gamma',gamma,'] best',f'{best_auc_g:.5f}',best_cfg_g)\n",
        "        if best_auc_g>best_auc: best_auc=best_auc_g; best=best_cfg_g; best_gm=gamma\n",
        "    return best_gm,best_auc,best\n",
        "gm,aucg,cfg_g=search_gamma()\n",
        "print('Gamma-best:', gm, aucg, cfg_g)\n",
        "g=cfg_g['g']; tz_lr_mix=(1-g)*tz_lr_w+g*tz_lr_ns\n",
        "zt=cfg_g['w_lr']*tz_lr_mix+cfg_g['w_d1']*tz_d1+cfg_g['w_d2']*tz_d2+cfg_g['w_meta']*tz_meta+cfg_g['w_emn']*tz_emn+cfg_g['w_emp']*tz_emp\n",
        "pt=sigmoid(zt).astype(np.float32)\n",
        "fn=f'submission_blk5_gamma{str(gm).replace(\".\",\"p\")}.csv'; pd.DataFrame({id_col:ids,target_col:pt}).to_csv(fn,index=False); print('Wrote',fn)\n",
        "# Hedged logit-averages\n",
        "def load_sub(path): return pd.read_csv(path)[target_col].values.astype(np.float64)\n",
        "def logit_avg(paths,out):\n",
        "    z=np.mean([to_logit(load_sub(p)) for p in paths],axis=0); p=sigmoid(z).astype(np.float32)\n",
        "    pd.DataFrame({id_col:ids,target_col:p}).to_csv(out,index=False); print('Wrote',out,'mean',float(p.mean()))\n",
        "s_r24='submission_blk5_r24.csv'; s_r30='submission_blk5_r30.csv'; s_g=f'submission_blk5_gamma{str(gm).replace(\".\",\"p\")}.csv'\n",
        "logit_avg([s_r24,s_g],'submission_logitavg_r24_gamma.csv')\n",
        "logit_avg([s_r24,s_r30,s_g],'submission_logitavg_r24_r30_gamma.csv')\n",
        "pd.read_csv('submission_logitavg_r24_gamma.csv').to_csv('submission.csv', index=False)\n",
        "print('Promoted submission_logitavg_r24_gamma.csv to submission.csv')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Block-5 size: 479 / 2878\nRecent availability: {'lr_w': False, 'lr_ns': False, 'emn': False, 'emp': False}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blk5 tried 1596 | best AUC 0.65235 | {'g': 0.98, 'w_lr': 0.45999999999999996, 'w_d1': 0.05, 'w_d2': 0.05, 'w_meta': 0.22, 'w_emn': 0.14, 'w_emp': 0.08}\nalphas r24/r30: (0.0, 0.0, 0.0) (0.0, 0.0, 0.0)\nWrote submission_blk5_r24.csv mean 0.38267284631729126\nWrote submission_blk5_r30.csv mean 0.38267284631729126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[gamma 0.995 ] best 0.68111 {'g': 0.98, 'w_lr': 0.29999999999999993, 'w_d1': 0.154, 'w_d2': 0.066, 'w_meta': 0.22, 'w_emn': 0.14, 'w_emp': 0.12}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[gamma 0.998 ] best 0.68129 {'g': 0.98, 'w_lr': 0.29999999999999993, 'w_d1': 0.154, 'w_d2': 0.066, 'w_meta': 0.22, 'w_emn': 0.14, 'w_emp': 0.12}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[gamma 0.9995 ] best 0.68138 {'g': 0.98, 'w_lr': 0.29999999999999993, 'w_d1': 0.154, 'w_d2': 0.066, 'w_meta': 0.22, 'w_emn': 0.14, 'w_emp': 0.12}\nGamma-best: 0.9995 0.6813808736215854 {'g': 0.98, 'w_lr': 0.29999999999999993, 'w_d1': 0.154, 'w_d2': 0.066, 'w_meta': 0.22, 'w_emn': 0.14, 'w_emp': 0.12}\nWrote submission_blk5_gamma0p9995.csv\nWrote submission_logitavg_r24_gamma.csv mean 0.3966662585735321\nWrote submission_logitavg_r24_r30_gamma.csv mean 0.3919568359851837\nPromoted submission_logitavg_r24_gamma.csv to submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "3b5c3143-8c2f-4709-adc8-08753fb14274",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S32: Inspect available recent* artifacts to wire correct filenames\n",
        "import os, re, json, glob\n",
        "cands = sorted([f for f in os.listdir('.') if f.endswith('.npy') and ('recent' in f.lower() or re.search(r'recent\\d+', f.lower()))])\n",
        "print('Found recent-like npy files:', len(cands))\n",
        "for f in cands[:200]:\n",
        "    print(f)\n",
        "# Also show any test_*_recent* explicitly\n",
        "print('\\nExplicit test_*_recent* matches:')\n",
        "for f in sorted(glob.glob('test_*_recent*.npy')):\n",
        "    print(f)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found recent-like npy files: 6\ntest_catboost_textmeta_v2_recent35.npy\ntest_catboost_textmeta_v2_recent45.npy\ntest_lr_nosub_meta_recent35.npy\ntest_lr_nosub_meta_recent45.npy\ntest_xgb_minilm_meta_recent35.npy\ntest_xgb_minilm_meta_recent45.npy\n\nExplicit test_*_recent* matches:\ntest_catboost_textmeta_v2_recent35.npy\ntest_catboost_textmeta_v2_recent45.npy\ntest_lr_nosub_meta_recent35.npy\ntest_lr_nosub_meta_recent45.npy\ntest_xgb_minilm_meta_recent35.npy\ntest_xgb_minilm_meta_recent45.npy\n"
          ]
        }
      ]
    },
    {
      "id": "5d11c821-12af-4147-ac44-127c3a01b018",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S33: Reload recent test-only using discovered filenames and rebuild r24/r30 + hedges\n",
        "import numpy as np, pandas as pd, os\n",
        "def to_logit(p,eps=1e-6): p=np.clip(p.astype(np.float64),eps,1-eps); return np.log(p/(1-p))\n",
        "def sigmoid(z): return 1/(1+np.exp(-z))\n",
        "def load_recent_avg_logit(prefix):\n",
        "    arrs=[]\n",
        "    for suf in ['_recent35.npy','_recent45.npy']:\n",
        "        p=prefix+suf\n",
        "        if os.path.exists(p):\n",
        "            try: arrs.append(to_logit(np.load(p)))\n",
        "            except: pass\n",
        "    return (np.mean(arrs,axis=0).astype(np.float64)) if arrs else None\n",
        "# Try alternative recent prefixes discovered by S32\n",
        "tz_lr_ns_r_alt = load_recent_avg_logit('test_lr_nosub_meta')\n",
        "tz_emn_r_alt   = load_recent_avg_logit('test_xgb_minilm_meta')\n",
        "# Override globals from S31 if better recent found\n",
        "try:\n",
        "    tz_lr_ns_r = tz_lr_ns_r_alt if (tz_lr_ns_r_alt is not None) else tz_lr_ns_r\n",
        "except NameError:\n",
        "    tz_lr_ns_r = tz_lr_ns_r_alt\n",
        "try:\n",
        "    tz_emn_r = tz_emn_r_alt if (tz_emn_r_alt is not None) else tz_emn_r\n",
        "except NameError:\n",
        "    tz_emn_r = tz_emn_r_alt\n",
        "print('Updated recent availability:', {\n",
        "    'lr_ns': tz_lr_ns_r is not None,\n",
        "    'minilm': tz_emn_r is not None\n",
        "})\n",
        "# Recompute alphas for r=0.24 and r=0.30 using existing cfg and pick_alphas from S31\n",
        "alphas24 = pick_alphas(cfg, 0.24); alphas30 = pick_alphas(cfg, 0.30)\n",
        "print('New alphas r24/r30:', alphas24, alphas30)\n",
        "# Rebuild submissions using existing build_sub from S31\n",
        "build_sub('r24_alt', cfg, alphas24)\n",
        "build_sub('r30_alt', cfg, alphas30)\n",
        "# Rebuild hedges with gamma-best from S31 and new r24_alt/r30_alt\n",
        "def load_sub(path): return pd.read_csv(path)['requester_received_pizza'].values.astype(np.float64)\n",
        "def logit_avg(paths,out, ids_col):\n",
        "    z=np.mean([to_logit(load_sub(p)) for p in paths],axis=0); p=sigmoid(z).astype(np.float32)\n",
        "    pd.DataFrame({ids_col: pd.read_json('test.json')['request_id'].values, 'requester_received_pizza': p}).to_csv(out,index=False)\n",
        "    print('Wrote',out,'mean',float(p.mean()))\n",
        "s_r24_alt='submission_blk5_r24_alt.csv'; s_r30_alt='submission_blk5_r30_alt.csv'; s_g=f'submission_blk5_gamma{str(gm).replace(\".\",\"p\")}.csv'\n",
        "logit_avg([s_r24_alt,s_g],'submission_logitavg_r24alt_gamma.csv','request_id')\n",
        "logit_avg([s_r24_alt,s_r30_alt,s_g],'submission_logitavg_r24alt_r30alt_gamma.csv','request_id')\n",
        "# Promote the 2-way hedge\n",
        "pd.read_csv('submission_logitavg_r24alt_gamma.csv').to_csv('submission.csv', index=False)\n",
        "print('Promoted submission_logitavg_r24alt_gamma.csv to submission.csv')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated recent availability: {'lr_ns': True, 'minilm': True}\nNew alphas r24/r30: (0.0, 0.0, 0.0) (0.0, 0.0, 0.0)\nWrote submission_blk5_r24_alt.csv mean 0.38267284631729126\nWrote submission_blk5_r30_alt.csv mean 0.38267284631729126\nWrote submission_logitavg_r24alt_gamma.csv mean 0.3966662585735321\nWrote submission_logitavg_r24alt_r30alt_gamma.csv mean 0.3919568359851837\nPromoted submission_logitavg_r24alt_gamma.csv to submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "b5e4021a-4fe8-4448-a63f-32d1123eb93e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S34: Recompute alpha grids based on updated recent availability and rebuild submissions/hedges\n",
        "import numpy as np, pandas as pd\n",
        "def to_logit(p,eps=1e-6): p=np.clip(p.astype(np.float64),eps,1-eps); return np.log(p/(1-p))\n",
        "def sigmoid(z): return 1/(1+np.exp(-z))\n",
        "# Recompute alpha grids now that tz_*_r were updated in S33\n",
        "aLR=[0.15,0.25,0.35] if (('tz_lr_w_r' in globals() and tz_lr_w_r is not None) or ('tz_lr_ns_r' in globals() and tz_lr_ns_r is not None)) else [0.0]\n",
        "aMN=[0.20,0.30,0.40] if ('tz_emn_r' in globals() and tz_emn_r is not None) else [0.0]\n",
        "aMP=[0.10,0.20,0.30] if ('tz_emp_r' in globals() and tz_emp_r is not None) else [0.0]\n",
        "print('Alpha grids set:', {'LR':aLR, 'MiniLM':aMN, 'MPNet':aMP})\n",
        "def pick_alphas(cfg,r,tol=0.02):\n",
        "    wl,wmn,wmp=cfg['w_lr'],cfg['w_emn'],cfg['w_emp']\n",
        "    best=None; best_err=1e9; best_sum=9e9\n",
        "    for al in aLR:\n",
        "        for amn in aMN:\n",
        "            for amp in aMP:\n",
        "                s=wl*al+wmn*amn+wmp*amp; err=abs(s-r); sm=al+amn+amp\n",
        "                if (err<best_err) or (abs(err-best_err)<1e-12 and sm<best_sum):\n",
        "                    best_err=err; best_sum=sm; best=(al,amn,amp)\n",
        "    return best\n",
        "def build_sub(tag,cfg,alphas):\n",
        "    g=cfg['g']\n",
        "    tz_lr_mix_full=(1-g)*tz_lr_w+g*tz_lr_ns\n",
        "    z_w_r=tz_lr_w_r if ('tz_lr_w_r' in globals() and tz_lr_w_r is not None) else tz_lr_w\n",
        "    z_ns_r=tz_lr_ns_r if ('tz_lr_ns_r' in globals() and tz_lr_ns_r is not None) else tz_lr_ns\n",
        "    tz_lr_mix_recent=(1-g)*z_w_r+g*z_ns_r\n",
        "    a_lr,a_mn,a_mp=alphas\n",
        "    tz_lr_mix=(1-a_lr)*tz_lr_mix_full+a_lr*tz_lr_mix_recent\n",
        "    tz_mn=(1-a_mn)*tz_emn + a_mn*((tz_emn_r if ('tz_emn_r' in globals()) else None) if (('tz_emn_r' in globals()) and (tz_emn_r is not None)) else tz_emn)\n",
        "    tz_mp=(1-a_mp)*tz_emp + a_mp*((tz_emp_r if ('tz_emp_r' in globals()) else None) if (('tz_emp_r' in globals()) and (tz_emp_r is not None)) else tz_emp)\n",
        "    zt=cfg['w_lr']*tz_lr_mix+cfg['w_d1']*tz_d1+cfg['w_d2']*tz_d2+cfg['w_meta']*tz_meta+cfg['w_emn']*tz_mn+cfg['w_emp']*tz_mp\n",
        "    pt=sigmoid(zt).astype(np.float32)\n",
        "    fn=f'submission_blk5_{tag}.csv'; pd.DataFrame({'request_id':pd.read_json('test.json')['request_id'].values,'requester_received_pizza':pt}).to_csv(fn,index=False); print('Wrote',fn,'mean',float(pt.mean()))\n",
        "# Use cfg from S31 (block-5 best) for alpha targeting\n",
        "alphas24=pick_alphas(cfg,0.24); alphas30=pick_alphas(cfg,0.30)\n",
        "print('Recomputed alphas r24/r30:', alphas24, alphas30)\n",
        "build_sub('r24_alt2', cfg, alphas24)\n",
        "build_sub('r30_alt2', cfg, alphas30)\n",
        "# Hedge with existing gamma-best\n",
        "def load_sub(path): return pd.read_csv(path)['requester_received_pizza'].values.astype(np.float64)\n",
        "def logit_avg(paths,out):\n",
        "    z=np.mean([to_logit(load_sub(p)) for p in paths],axis=0); p=sigmoid(z).astype(np.float32)\n",
        "    pd.DataFrame({'request_id':pd.read_json('test.json')['request_id'].values,'requester_received_pizza':p}).to_csv(out,index=False); print('Wrote',out,'mean',float(p.mean()))\n",
        "s_r24='submission_blk5_r24_alt2.csv'; s_r30='submission_blk5_r30_alt2.csv'; s_g=f'submission_blk5_gamma{str(gm).replace(\".\",\"p\")}.csv'\n",
        "logit_avg([s_r24,s_g],'submission_logitavg_r24alt2_gamma.csv')\n",
        "logit_avg([s_r24,s_r30,s_g],'submission_logitavg_r24alt2_r30alt2_gamma.csv')\n",
        "pd.read_csv('submission_logitavg_r24alt2_gamma.csv').to_csv('submission.csv', index=False)\n",
        "print('Promoted submission_logitavg_r24alt2_gamma.csv to submission.csv')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha grids set: {'LR': [0.15, 0.25, 0.35], 'MiniLM': [0.2, 0.3, 0.4], 'MPNet': [0.0]}\nRecomputed alphas r24/r30: (0.35, 0.4, 0.0) (0.35, 0.4, 0.0)\nWrote submission_blk5_r24_alt2.csv mean 0.34550267457962036\nWrote submission_blk5_r30_alt2.csv mean 0.34550267457962036\nWrote submission_logitavg_r24alt2_gamma.csv mean 0.37762686610221863\nWrote submission_logitavg_r24alt2_r30alt2_gamma.csv mean 0.3667590320110321\nPromoted submission_logitavg_r24alt2_gamma.csv to submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "cba12029-4d46-42b4-ac96-5cc8ef26cf17",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S35: Promote safer 2-way hedge (r24 + gamma) to submission.csv per expert advice\n",
        "import pandas as pd\n",
        "src = 'submission_logitavg_r24_gamma.csv'\n",
        "df = pd.read_csv(src)\n",
        "df.to_csv('submission.csv', index=False)\n",
        "print('Promoted', src, 'to submission.csv | mean p =', float(df['requester_received_pizza'].mean()))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promoted submission_logitavg_r24_gamma.csv to submission.csv | mean p = 0.3966662772977625\n"
          ]
        }
      ]
    },
    {
      "id": "bc7e74a7-7fc8-41b1-a948-99b42a942d55",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S36: Conservative recent interpolation using gamma-best weights; build r24/r30 small-alpha variants and hedges\n",
        "import numpy as np, pandas as pd, os\n",
        "def to_logit(p,eps=1e-6): p=np.clip(p.astype(np.float64),eps,1-eps); return np.log(p/(1-p))\n",
        "def sigmoid(z): return 1/(1+np.exp(-z))\n",
        "id_col='request_id'; target_col='requester_received_pizza'\n",
        "test_ids = pd.read_json('test.json')[id_col].values\n",
        "# Ensure gamma-best cfg is available from S31\n",
        "assert 'cfg_g' in globals(), 'gamma-best cfg not found; run S31 first'\n",
        "g = cfg_g['g']\n",
        "# Use existing full-history logits from S31\n",
        "# tz_lr_w, tz_lr_ns, tz_d1, tz_d2, tz_meta, tz_emn, tz_emp must exist\n",
        "tz_lr_mix_full = (1-g)*tz_lr_w + g*tz_lr_ns\n",
        "# Recent logits loaded in S31/S33: tz_lr_w_r, tz_lr_ns_r, tz_emn_r (MPNet none)\n",
        "z_w_r = tz_lr_w_r if ('tz_lr_w_r' in globals() and tz_lr_w_r is not None) else tz_lr_w\n",
        "z_ns_r = tz_lr_ns_r if ('tz_lr_ns_r' in globals() and tz_lr_ns_r is not None) else tz_lr_ns\n",
        "tz_lr_mix_recent = (1-g)*z_w_r + g*z_ns_r\n",
        "tz_mn_recent = tz_emn_r if ('tz_emn_r' in globals() and tz_emn_r is not None) else tz_emn\n",
        "# Small alpha grids per expert\n",
        "aLR = [0.05, 0.10, 0.15] if (('tz_lr_w_r' in globals() and tz_lr_w_r is not None) or ('tz_lr_ns_r' in globals() and tz_lr_ns_r is not None)) else [0.0]\n",
        "aMN = [0.05, 0.10, 0.15] if ('tz_emn_r' in globals() and tz_emn_r is not None) else [0.0]\n",
        "aMP = [0.0]  # no MPNet recent\n",
        "print('Small alpha grids:', {'LR': aLR, 'MiniLM': aMN, 'MPNet': aMP})\n",
        "def pick_alphas_gamma(cfg, r, tol=0.02):\n",
        "    wl, wmn, wmp = cfg['w_lr'], cfg['w_emn'], cfg['w_emp']\n",
        "    best = None; best_err = 1e9; best_sum = 1e9\n",
        "    for al in aLR:\n",
        "        for amn in aMN:\n",
        "            for amp in aMP:\n",
        "                s = wl*al + wmn*amn + wmp*amp\n",
        "                err = abs(s - r); sm = al + amn + amp\n",
        "                if (err < best_err) or (abs(err - best_err) < 1e-12 and sm < best_sum):\n",
        "                    best_err = err; best_sum = sm; best = (al, amn, amp)\n",
        "    return best\n",
        "def build_with_cfg(tag, cfg, alphas):\n",
        "    a_lr, a_mn, a_mp = alphas\n",
        "    tz_lr_mix = (1-a_lr)*tz_lr_mix_full + a_lr*tz_lr_mix_recent\n",
        "    tz_mn = (1-a_mn)*tz_emn + a_mn*tz_mn_recent\n",
        "    # MPNet unchanged (no recent)\n",
        "    zt = (cfg['w_lr']*tz_lr_mix + cfg['w_d1']*tz_d1 + cfg['w_d2']*tz_d2 + cfg['w_meta']*tz_meta + cfg['w_emn']*tz_mn + cfg['w_emp']*tz_emp)\n",
        "    pt = sigmoid(zt).astype(np.float32)\n",
        "    fn = f'submission_blk5_{tag}.csv'; pd.DataFrame({id_col: test_ids, target_col: pt}).to_csv(fn, index=False); print('Wrote', fn, 'mean', float(pt.mean()))\n",
        "# Compute alphas against gamma-best weights\n",
        "alphas_r24 = pick_alphas_gamma(cfg_g, 0.24); alphas_r30 = pick_alphas_gamma(cfg_g, 0.30)\n",
        "print('Conservative alphas r24/r30 (gamma-weights):', alphas_r24, alphas_r30)\n",
        "build_with_cfg('r24_small', cfg_g, alphas_r24)\n",
        "build_with_cfg('r30_small', cfg_g, alphas_r30)\n",
        "# Build hedges with gamma-best submission (no recent interpolation)\n",
        "def load_sub(path): return pd.read_csv(path)[target_col].values.astype(np.float64)\n",
        "def logit_avg(paths, out):\n",
        "    z = np.mean([to_logit(load_sub(p)) for p in paths], axis=0); p = sigmoid(z).astype(np.float32)\n",
        "    pd.DataFrame({id_col: test_ids, target_col: p}).to_csv(out, index=False); print('Wrote', out, 'mean', float(p.mean()))\n",
        "gamma_path = f'submission_blk5_gamma{str(gm).replace(\".\",\"p\")}.csv'\n",
        "logit_avg(['submission_blk5_r24_small.csv', gamma_path], 'submission_logitavg_r24small_gamma.csv')\n",
        "logit_avg(['submission_blk5_r24_small.csv', 'submission_blk5_r30_small.csv', gamma_path], 'submission_logitavg_r24small_r30small_gamma.csv')\n",
        "print('Note: submission.csv unchanged; primary remains submission_logitavg_r24_gamma.csv')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small alpha grids: {'LR': [0.05, 0.1, 0.15], 'MiniLM': [0.05, 0.1, 0.15], 'MPNet': [0.0]}\nConservative alphas r24/r30 (gamma-weights): (0.15, 0.15, 0.0) (0.15, 0.15, 0.0)\nWrote submission_blk5_r24_small.csv mean 0.3995879590511322\nWrote submission_blk5_r30_small.csv mean 0.3995879590511322\nWrote submission_logitavg_r24small_gamma.csv mean 0.4053078293800354\nWrote submission_logitavg_r24small_r30small_gamma.csv mean 0.4033985137939453\nNote: submission.csv unchanged; primary remains submission_logitavg_r24_gamma.csv\n"
          ]
        }
      ]
    },
    {
      "id": "449bf20a-72d1-42e2-9c67-f22e1420aecd",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S37: Promote conservative small-alpha 2-way hedge to submission.csv\n",
        "import pandas as pd\n",
        "src = 'submission_logitavg_r24small_gamma.csv'\n",
        "df = pd.read_csv(src)\n",
        "df.to_csv('submission.csv', index=False)\n",
        "print('Promoted', src, 'to submission.csv | mean p =', float(df['requester_received_pizza'].mean()))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promoted submission_logitavg_r24small_gamma.csv to submission.csv | mean p = 0.4053078408777969\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}