{
  "cells": [
    {
      "id": "0800bb68-2172-4fd2-aa2d-833221d4499d",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Random Acts of Pizza: Medal Plan\n",
        "\n",
        "Objective: Achieve medal via strong, reliable text+meta baseline with robust CV and fast iteration.\n",
        "\n",
        "Data:\n",
        "- train.json, test.json. Target: requester_received_pizza (bool).\n",
        "- Text fields: request_text, title, combined text. Meta: requester_account_age_in_days_at_request, requester_days_since_first_post_on_raop_at_request, requester_number_of_comments_at_request, requester_number_of_comments_in_raop_at_request, requester_number_of_posts_at_request, requester_number_of_posts_on_raop_at_request, requester_upvotes_plus_downvotes_at_request, requester_upvotes_minus_downvotes_at_request, etc. IDs: request_id.\n",
        "\n",
        "Metric: ROC AUC (maximize).\n",
        "\n",
        "Validation:\n",
        "- Stratified KFold (n_splits=5, shuffle, fixed seed).\n",
        "- Single fold source saved and reused; transforms fit per-fold to prevent leakage.\n",
        "- Report OOF AUC mean/std; log per-fold times.\n",
        "\n",
        "Baseline v1 (fast):\n",
        "- Text preprocessing: concatenate title + request_text.\n",
        "- TF-IDF word + char n-grams (word 1-2, char 3-5), sublinear_tf, min_df tuned lightly; limit features via max_features (e.g., 200k).\n",
        "- Linear Model: LogisticRegression (liblinear/saga) or LinearSVC with calibrated probabilities; start with LogisticRegression(saga, class_weight='balanced', max_iter=4000).\n",
        "- Blend word and char TF-IDF by hstack.\n",
        "\n",
        "Meta Features v1:\n",
        "- Numeric: account age, activity counts, karma-like features, raop history, hour/day from unix_timestamp if present, text length metrics (len, word count, title len), punctuation/exclamation count, presence of keywords (e.g., \"pizza\", \"thanks\").\n",
        "- Binary flags: includes images, has_url, has_code, mentions_student/job/money? (simple keyword lists).\n",
        "\n",
        "Pipeline:\n",
        "- Build single ColumnTransformer: TF-IDF on text; passthrough standardized numeric; binary keywords.\n",
        "- Model: LogisticRegression or XGBoost (gpu) on sparse; start with LR for speed/strong baseline on text.\n",
        "- Cache vectorizers and sparse matrices to .npz for reuse; cache OOF/test preds .npy.\n",
        "\n",
        "Iteration Plan:\n",
        "1) Implement data loader + EDA-lite (shapes, target rate, nulls).\n",
        "2) Baseline TF-IDF+LR 5-fold; get OOF AUC; produce submission.csv.\n",
        "3) Add meta features; re-evaluate.\n",
        "4) Try LinearSVC + CalibratedClassifierCV; compare.\n",
        "5) Try XGBoost (gpu_hist) on sparse CSR; tune shallow depth/ETA; compare.\n",
        "6) Simple blend of top-2 via weighted average on OOF to set weights; apply to test.\n",
        "7) Error analysis on OOF bins; targeted keyword features.\n",
        "\n",
        "Efficiency:\n",
        "- Log fold indices and elapsed times.\n",
        "- Use subsample (e.g., 1000 rows, 2 folds) for smoke tests.\n",
        "- Save folds and results; one change per run.\n",
        "\n",
        "Deliverables:\n",
        "- submission.csv with columns: request_id, requester_received_pizza.\n",
        "- Reproducible notebook with saved CV.\n",
        "\n",
        "Next: Implement data loading, schema print, and CV scaffolding."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "520d7b3c-2199-4df3-a556-dcfb602f71fe",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Data loading and schema/EDA scaffold\n",
        "import json, os, time, sys, gc, math, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "t0 = time.time()\n",
        "print('Reading train/test JSON...')\n",
        "train_path = 'train.json'\n",
        "test_path = 'test.json'\n",
        "train = pd.read_json(train_path)\n",
        "test = pd.read_json(test_path)\n",
        "print(f'Loaded train: {train.shape}, test: {test.shape}')\n",
        "\n",
        "# Peek columns and target\n",
        "print('\\nTrain columns:', len(train.columns))\n",
        "print(sorted(train.columns.tolist())[:40], '...')\n",
        "print('\\nTest columns:', len(test.columns))\n",
        "print(sorted(test.columns.tolist())[:40], '...')\n",
        "\n",
        "target_col = 'requester_received_pizza'\n",
        "id_col = 'request_id'\n",
        "assert target_col in train.columns, f'Missing target {target_col}'\n",
        "assert id_col in train.columns and id_col in test.columns, 'Missing request_id in data'\n",
        "\n",
        "# Target stats\n",
        "print('\\nTarget distribution:')\n",
        "print(train[target_col].value_counts(dropna=False).rename('count'))\n",
        "pos_rate = train[target_col].mean()\n",
        "print(f'Positive rate: {pos_rate:.4f}')\n",
        "\n",
        "# Identify likely text fields and username for grouping\n",
        "possible_text_cols = [c for c in ['request_text_edit_aware','request_text','title'] if c in train.columns]\n",
        "group_col = 'requester_username' if 'requester_username' in train.columns else None\n",
        "print('\\nText columns found:', possible_text_cols)\n",
        "print('Group column:', group_col)\n",
        "\n",
        "# Check overlap of users between train/test to assess leakage risk and choose CV\n",
        "if group_col is not None:\n",
        "    tr_users = set(train[group_col].dropna().astype(str))\n",
        "    te_users = set(test[group_col].dropna().astype(str))\n",
        "    inter = tr_users & te_users\n",
        "    print(f'Unique users train: {len(tr_users)}, test: {len(te_users)}, overlap: {len(inter)}')\n",
        "else:\n",
        "    print('No requester_username available; will use StratifiedKFold.')\n",
        "\n",
        "# List features ending with _at_request to whitelist meta\n",
        "request_time_feats = sorted([c for c in train.columns if c.endswith('_at_request')])\n",
        "print(f'\\nRequest-time features (count={len(request_time_feats)}):')\n",
        "print(request_time_feats[:30], '...')\n",
        "\n",
        "# Known leaky fields to drop if present\n",
        "leaky_prefixes = ['giver_', 'post_was_']\n",
        "leaky_suffixes = ['_at_retrieval']\n",
        "leaky_cols = [c for c in train.columns if any(c.startswith(p) for p in leaky_prefixes) or any(c.endswith(s) for s in leaky_suffixes)]\n",
        "if leaky_cols:\n",
        "    print('Potentially leaky columns found (will exclude):', leaky_cols)\n",
        "else:\n",
        "    print('No obvious leaky columns found by prefix/suffix scan.')\n",
        "\n",
        "# Basic nulls and sample rows\n",
        "print('\\nNull rates (top 20 by null %):')\n",
        "nulls = train.isnull().mean().sort_values(ascending=False)\n",
        "print(nulls.head(20))\n",
        "\n",
        "print('\\nSample rows (id, title snippet):')\n",
        "title_col = 'title' if 'title' in train.columns else None\n",
        "if title_col:\n",
        "    print(train[[id_col, title_col]].head(3))\n",
        "else:\n",
        "    print(train[[id_col]].head(3))\n",
        "\n",
        "# Confirm submission format\n",
        "sub_example = pd.read_csv('sampleSubmission.csv')\n",
        "print('\\nSample submission head:')\n",
        "print(sub_example.head())\n",
        "assert list(sub_example.columns) == [id_col, target_col], 'Submission columns mismatch'\n",
        "\n",
        "elapsed = time.time() - t0\n",
        "print(f'\\nData load + EDA scaffold done in {elapsed:.2f}s')\n",
        "\n",
        "# Prepare combined text preview function (no fitting yet)\n",
        "def make_text(df: pd.DataFrame) -> pd.Series:\n",
        "    parts = []\n",
        "    if 'title' in df.columns:\n",
        "        parts.append(df['title'].fillna(''))\n",
        "    if 'request_text_edit_aware' in df.columns:\n",
        "        parts.append(df['request_text_edit_aware'].fillna(''))\n",
        "    elif 'request_text' in df.columns:\n",
        "        parts.append(df['request_text'].fillna(''))\n",
        "    if parts:\n",
        "        return (parts[0].astype(str) + ' \\n ' + parts[1].astype(str)) if len(parts) > 1 else parts[0].astype(str)\n",
        "    return pd.Series([''] * len(df))\n",
        "\n",
        "train['_combined_text'] = make_text(train)\n",
        "test['_combined_text'] = make_text(test)\n",
        "print('\\nCombined text examples:')\n",
        "print(train['_combined_text'].head(2).tolist())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading train/test JSON...\nLoaded train: (2878, 32), test: (1162, 17)\n\nTrain columns: 32\n['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc'] ...\n\nTest columns: 17\n['giver_username_if_known', 'request_id', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_days_since_first_post_on_raop_at_request', 'requester_number_of_comments_at_request', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_posts_at_request', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_subreddits_at_request', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_request', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc'] ...\n\nTarget distribution:\nrequester_received_pizza\nFalse    2163\nTrue      715\nName: count, dtype: int64\nPositive rate: 0.2484\n\nText columns found: ['request_text_edit_aware', 'request_text']\nGroup column: requester_username\nUnique users train: 2878, test: 1162, overlap: 0\n\nRequest-time features (count=10):\n['requester_account_age_in_days_at_request', 'requester_days_since_first_post_on_raop_at_request', 'requester_number_of_comments_at_request', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_posts_at_request', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_subreddits_at_request', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_request'] ...\nPotentially leaky columns found (will exclude): ['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_number_of_comments_at_retrieval', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_retrieval']\n\nNull rates (top 20 by null %):\nrequester_user_flair                                 0.751564\ngiver_username_if_known                              0.000000\nnumber_of_downvotes_of_request_at_retrieval          0.000000\nunix_timestamp_of_request                            0.000000\nrequester_username                                   0.000000\nrequester_upvotes_plus_downvotes_at_retrieval        0.000000\nrequester_upvotes_plus_downvotes_at_request          0.000000\nrequester_upvotes_minus_downvotes_at_retrieval       0.000000\nrequester_upvotes_minus_downvotes_at_request         0.000000\nrequester_subreddits_at_request                      0.000000\nrequester_received_pizza                             0.000000\nrequester_number_of_subreddits_at_request            0.000000\nrequester_number_of_posts_on_raop_at_retrieval       0.000000\nrequester_number_of_posts_on_raop_at_request         0.000000\nrequester_number_of_posts_at_retrieval               0.000000\nrequester_number_of_posts_at_request                 0.000000\nrequester_number_of_comments_in_raop_at_retrieval    0.000000\nrequester_number_of_comments_in_raop_at_request      0.000000\nrequester_number_of_comments_at_retrieval            0.000000\nrequester_number_of_comments_at_request              0.000000\ndtype: float64\n\nSample rows (id, title snippet):\n  request_id\n0   t3_q8ycf\n1   t3_ixnia\n2   t3_ndy6g\n\nSample submission head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                         0\n1   t3_roiuw                         0\n2   t3_mjnbq                         0\n3   t3_t8wd1                         0\n4  t3_1m4zxu                         0\n\nData load + EDA scaffold done in 0.13s\n\nCombined text examples:\n[\"I will soon be going on a long deployment which I'm not aloud to discuss but willing to give some info if you ask. Just wanna eat some of the stuff America has to offer before I leave for a long time to Afganistan.\", \"We would all really appreciate it, and would even send a picture of the three of us enjoying the said pizza (if that's your thing).  Just curious to see if this works!!!\"]\n"
          ]
        }
      ]
    },
    {
      "id": "63019bfd-86f6-4b01-8ac1-68eead41e5eb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TF-IDF + Meta + Logistic Regression CV and Submission (grid: C x class_weight)\n",
        "import time\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "def safe_log1p_signed(arr):\n",
        "    a = pd.to_numeric(arr, errors='coerce').astype(float)\n",
        "    return np.sign(a) * np.log1p(np.abs(a))\n",
        "\n",
        "def extract_subreddit_text(series):\n",
        "    def joiner(x):\n",
        "        if isinstance(x, (list, tuple)):\n",
        "            return ' '.join([str(s).lower() for s in x])\n",
        "        return ''\n",
        "    return series.apply(joiner)\n",
        "\n",
        "# Ensure VADER is available and create a global analyzer\n",
        "try:\n",
        "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "    _vader = SentimentIntensityAnalyzer()\n",
        "except Exception:\n",
        "    import subprocess, sys as _sys\n",
        "    subprocess.run([_sys.executable, '-m', 'pip', 'install', '-q', 'vaderSentiment'])\n",
        "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "    _vader = SentimentIntensityAnalyzer()\n",
        "\n",
        "def build_meta_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df2 = pd.DataFrame(index=df.index)\n",
        "    base = [\n",
        "        'requester_account_age_in_days_at_request',\n",
        "        'requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request',\n",
        "        'requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request',\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "    ]\n",
        "    for c in base:\n",
        "        if c in df.columns:\n",
        "            df2[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "    if 'unix_timestamp_of_request' in df.columns:\n",
        "        ts = pd.to_datetime(df['unix_timestamp_of_request'], unit='s', errors='coerce')\n",
        "        df2['req_hour'] = ts.dt.hour.fillna(0).astype(np.int16)\n",
        "        df2['req_wday'] = ts.dt.weekday.fillna(0).astype(np.int16)\n",
        "        df2['req_is_weekend'] = df2['req_wday'].isin([5,6]).astype(np.int8)\n",
        "        df2['req_month'] = ts.dt.month.fillna(0).astype(np.int16)\n",
        "        month = df2['req_month'].fillna(0).astype(int)\n",
        "        season = pd.Series(np.zeros(len(df2), dtype=np.int16), index=df2.index)\n",
        "        season[(month==12)|(month<=2)] = 1\n",
        "        season[(month>=3)&(month<=5)] = 2\n",
        "        season[(month>=6)&(month<=8)] = 3\n",
        "        season[(month>=9)&(month<=11)] = 4\n",
        "        df2['req_season'] = season.astype(np.int16)\n",
        "    txt = df['_combined_text'].fillna('').astype(str)\n",
        "    df2['text_len'] = txt.str.len().astype(np.int32)\n",
        "    raw_wc = txt.str.split().map(len)\n",
        "    df2['word_count'] = raw_wc.astype(np.int32)\n",
        "    # Length buckets (simple bins)\n",
        "    df2['wc_bin_small'] = (raw_wc < 200).astype(np.int8)\n",
        "    df2['wc_bin_medium'] = ((raw_wc >= 200) & (raw_wc <= 600)).astype(np.int8)\n",
        "    df2['wc_bin_large'] = (raw_wc > 600).astype(np.int8)\n",
        "    df2['exclaim_count'] = txt.str.count('!').astype(np.int16)\n",
        "    df2['question_count'] = txt.str.count('\\?').astype(np.int16)\n",
        "    upper_ratio = txt.map(lambda s: (sum(ch.isupper() for ch in s) / max(1, len(s)))).astype(np.float32)\n",
        "    df2['upper_ratio'] = upper_ratio.clip(0, 0.7)\n",
        "    # URL/image flags\n",
        "    df2['has_url'] = txt.str.contains('http://', regex=False) | txt.str.contains('https://', regex=False)\n",
        "    df2['has_url'] = df2['has_url'].astype(np.int8)\n",
        "    df2['url_count'] = txt.str.count('http://') + txt.str.count('https://')\n",
        "    df2['url_count'] = df2['url_count'].astype(np.int16)\n",
        "    df2['has_image'] = txt.str.contains('imgur', case=False, regex=False) | txt.str.contains('.jpg', case=False, regex=False) | txt.str.contains('.jpeg', case=False, regex=False) | txt.str.contains('.png', case=False, regex=False) | txt.str.contains('.gif', case=False, regex=False)\n",
        "    df2['has_image'] = df2['has_image'].astype(np.int8)\n",
        "    # Sentiment (VADER compound)\n",
        "    try:\n",
        "        df2['sent_compound'] = txt.map(lambda s: _vader.polarity_scores(s)['compound']).astype(np.float32)\n",
        "    except Exception:\n",
        "        df2['sent_compound'] = 0.0\n",
        "    lexicons = {\n",
        "        'kw_student': ['student','college','university','school','tuition'],\n",
        "        'kw_job': ['job','unemployed','laid off','hired','interview'],\n",
        "        'kw_money': ['rent','bill','bills','broke','money','paycheck','payment'],\n",
        "        'kw_family': ['family','kids','child','children','wife','husband','mom','dad'],\n",
        "        'kw_emergency': ['emergency','medical','hospital','doctor'],\n",
        "        'kw_gratitude': ['please','thank','thanks','appreciate','grateful'],\n",
        "        'kw_reciprocity': ['pay it forward','return the favor','pay it back','return favor'],\n",
        "        'kw_pizza': ['pizza','pepperoni','cheese','dominos','pizza hut','papa john']\n",
        "    }\n",
        "    low_txt = txt.str.lower()\n",
        "    for name, toks in lexicons.items():\n",
        "        pat = '|'.join([re.escape(t) for t in toks])\n",
        "        df2[name] = low_txt.str.contains(pat).astype(np.int8)\n",
        "    if 'requester_subreddits_at_request' in df.columns:\n",
        "        subs = df['requester_subreddits_at_request']\n",
        "        df2['subs_len'] = subs.apply(lambda x: len(x) if isinstance(x, (list, tuple)) else 0).astype(np.int32)\n",
        "        df2['subs_has_raop'] = subs.apply(lambda x: any(isinstance(s, str) and 'random_acts_of_pizza' in s.lower() for s in x) if isinstance(x, (list, tuple)) else False).astype(np.int8)\n",
        "    if 'requester_number_of_posts_on_raop_at_request' in df2.columns:\n",
        "        df2['has_raop_post_hist'] = (pd.to_numeric(df['requester_number_of_posts_on_raop_at_request'], errors='coerce') > 0).astype(np.int8)\n",
        "    if 'requester_number_of_comments_in_raop_at_request' in df2.columns:\n",
        "        df2['has_raop_comment_hist'] = (pd.to_numeric(df['requester_number_of_comments_in_raop_at_request'], errors='coerce') > 0).astype(np.int8)\n",
        "    age = pd.to_numeric(df.get('requester_account_age_in_days_at_request', np.nan), errors='coerce')\n",
        "    comments = pd.to_numeric(df.get('requester_number_of_comments_at_request', np.nan), errors='coerce')\n",
        "    posts = pd.to_numeric(df.get('requester_number_of_posts_at_request', np.nan), errors='coerce')\n",
        "    df2['comments_per_day'] = comments / np.maximum(1.0, age)\n",
        "    df2['posts_per_day'] = posts / np.maximum(1.0, age)\n",
        "    # Account age buckets (one-hot): [0-30], (30-90], (90-365], >365 days\n",
        "    age_days = pd.to_numeric(df.get('requester_account_age_in_days_at_request', np.nan), errors='coerce').fillna(0).astype(float)\n",
        "    df2['age_bin_0_30'] = (age_days <= 30).astype(np.int8)\n",
        "    df2['age_bin_30_90'] = ((age_days > 30) & (age_days <= 90)).astype(np.int8)\n",
        "    df2['age_bin_90_365'] = ((age_days > 90) & (age_days <= 365)).astype(np.int8)\n",
        "    df2['age_bin_365p'] = (age_days > 365).astype(np.int8)\n",
        "    for c in ['requester_number_of_comments_at_request','requester_number_of_posts_at_request','requester_number_of_comments_in_raop_at_request','requester_number_of_posts_on_raop_at_request','requester_upvotes_plus_downvotes_at_request','text_len','word_count','exclaim_count','question_count','url_count','subs_len','comments_per_day','posts_per_day']:\n",
        "        if c in df2.columns:\n",
        "            df2[c] = np.log1p(pd.to_numeric(df2[c], errors='coerce').clip(lower=0))\n",
        "    if 'requester_upvotes_minus_downvotes_at_request' in df2.columns:\n",
        "        df2['requester_upvotes_minus_downvotes_at_request'] = safe_log1p_signed(df2['requester_upvotes_minus_downvotes_at_request'])\n",
        "    df2 = df2.apply(pd.to_numeric, errors='coerce')\n",
        "    df2 = df2.replace([np.inf, -np.inf], np.nan)\n",
        "    return df2\n",
        "\n",
        "def rebuild_combined_text(df: pd.DataFrame):\n",
        "    parts = []\n",
        "    if 'request_title' in df.columns:\n",
        "        parts.append(df['request_title'].fillna(''))\n",
        "    elif 'request_title' not in df.columns and 'title' in df.columns:\n",
        "        parts.append(df['title'].fillna(''))\n",
        "    if 'request_text_edit_aware' in df.columns:\n",
        "        parts.append(df['request_text_edit_aware'].fillna(''))\n",
        "    elif 'request_text' in df.columns:\n",
        "        parts.append(df['request_text'].fillna(''))\n",
        "    if parts:\n",
        "        return (parts[0].astype(str) + ' \\n ' + parts[1].astype(str)) if len(parts) > 1 else parts[0].astype(str)\n",
        "    return pd.Series([''] * len(df))\n",
        "\n",
        "# Rebuild combined text\n",
        "train['_combined_text'] = rebuild_combined_text(train)\n",
        "test['_combined_text'] = rebuild_combined_text(test)\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "n_splits = 5\n",
        "cv = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42).split(train, y))\n",
        "print(f'Prepared {n_splits}-fold StratifiedKFold CV (shuffled).')\n",
        "\n",
        "# Vectorizer parameters (reduced capacity, no stopwords, min_df=3)\n",
        "word_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=50000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "char_params = dict(analyzer='char_wb', ngram_range=(3,5), lowercase=True, min_df=3, max_features=50000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "\n",
        "# Precompute test meta once\n",
        "meta_te_full = build_meta_features(test).replace([np.inf, -np.inf], np.nan).fillna(0).astype(np.float32)\n",
        "te_text = test['_combined_text'].astype(str)\n",
        "\n",
        "# Prepare OOF/test containers for grid\n",
        "Cs = [0.5, 1.0, 2.0]\n",
        "class_weights = [None, 'balanced']\n",
        "oof_by_cfg = {(C,cw): np.zeros(len(train), dtype=np.float32) for C in Cs for cw in class_weights}\n",
        "test_preds_by_cfg = {(C,cw): [] for C in Cs for cw in class_weights}\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(cv):\n",
        "    t_fold = time.time()\n",
        "    print(f'Fold {fold+1}/{n_splits} - train {len(tr_idx)} va {len(va_idx)}')\n",
        "    sys.stdout.flush()\n",
        "    tr_text = train.loc[tr_idx, '_combined_text'].astype(str)\n",
        "    va_text = train.loc[va_idx, '_combined_text'].astype(str)\n",
        "\n",
        "    # Vectorize train split only\n",
        "    tfidf_w = TfidfVectorizer(**word_params)\n",
        "    Xw_tr = tfidf_w.fit_transform(tr_text)\n",
        "    Xw_va = tfidf_w.transform(va_text)\n",
        "    Xw_te = tfidf_w.transform(te_text)\n",
        "\n",
        "    tfidf_c = TfidfVectorizer(**char_params)\n",
        "    Xc_tr = tfidf_c.fit_transform(tr_text)\n",
        "    Xc_va = tfidf_c.transform(va_text)\n",
        "    Xc_te = tfidf_c.transform(te_text)\n",
        "\n",
        "    # Meta features\n",
        "    meta_tr = build_meta_features(train.loc[tr_idx])\n",
        "    meta_va = build_meta_features(train.loc[va_idx])\n",
        "    meta_tr = meta_tr.replace([np.inf, -np.inf], np.nan).fillna(0).astype(np.float32)\n",
        "    meta_va = meta_va.replace([np.inf, -np.inf], np.nan).fillna(0).astype(np.float32)\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    Xm_tr = scaler.fit_transform(meta_tr)\n",
        "    Xm_va = scaler.transform(meta_va)\n",
        "    Xm_te = scaler.transform(meta_te_full)\n",
        "\n",
        "    # Stack sparse matrices\n",
        "    X_tr = hstack([Xw_tr, Xc_tr, Xm_tr], format='csr')\n",
        "    X_va = hstack([Xw_va, Xc_va, Xm_va], format='csr')\n",
        "    X_te = hstack([Xw_te, Xc_te, Xm_te], format='csr')\n",
        "\n",
        "    # Train/eval per config\n",
        "    for C in Cs:\n",
        "        for cw in class_weights:\n",
        "            clf = LogisticRegression(solver='saga', penalty='l2', C=C, max_iter=4000, n_jobs=-1, class_weight=cw, random_state=42, verbose=0)\n",
        "            clf.fit(X_tr, y[tr_idx])\n",
        "            va_pred = clf.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "            oof_by_cfg[(C,cw)][va_idx] = va_pred\n",
        "            te_pred = clf.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "            test_preds_by_cfg[(C,cw)].append(te_pred)\n",
        "\n",
        "    print(f'Fold {fold+1} done in {time.time()-t_fold:.1f}s')\n",
        "    sys.stdout.flush()\n",
        "    # Cleanup per fold\n",
        "    del Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, Xm_tr, Xm_va, Xm_te, X_tr, X_va, X_te\n",
        "    gc.collect()\n",
        "\n",
        "# Evaluate OOF per config and pick best\n",
        "auc_per_cfg = {}\n",
        "for key in oof_by_cfg:\n",
        "    auc = roc_auc_score(y, oof_by_cfg[key])\n",
        "    auc_per_cfg[key] = auc\n",
        "    C, cw = key\n",
        "    print(f'C={C}, class_weight={cw} OOF AUC: {auc:.5f}')\n",
        "best_cfg = max(auc_per_cfg, key=auc_per_cfg.get)\n",
        "print(f'Best cfg: C={best_cfg[0]}, class_weight={best_cfg[1]} with OOF AUC {auc_per_cfg[best_cfg]:.5f}')\n",
        "\n",
        "# Average test preds for best cfg\n",
        "best_test_stack = np.mean(test_preds_by_cfg[best_cfg], axis=0).astype(np.float32)\n",
        "\n",
        "# Build submission\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: best_test_stack})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv; head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 5-fold StratifiedKFold CV (shuffled).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 done in 270.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 done in 271.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 done in 369.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 done in 286.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 done in 384.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=0.5, class_weight=None OOF AUC: 0.67348\nC=0.5, class_weight=balanced OOF AUC: 0.67366\nC=1.0, class_weight=None OOF AUC: 0.67357\nC=1.0, class_weight=balanced OOF AUC: 0.67261\nC=2.0, class_weight=None OOF AUC: 0.67009\nC=2.0, class_weight=balanced OOF AUC: 0.66902\nBest cfg: C=0.5, class_weight=balanced with OOF AUC 0.67366\nSaved submission.csv; head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.492938\n1   t3_roiuw                  0.341476\n2   t3_mjnbq                  0.313049\n3   t3_t8wd1                  0.514664\n4  t3_1m4zxu                  0.515675\n"
          ]
        }
      ]
    },
    {
      "id": "5461c400-c130-436e-a5be-77c741da09e3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Alternate model: TF-IDF (50k+50k) + Meta with LR and Calibrated LinearSVC; add NB-SVM and OOF blend\n",
        "import time, gc, sys\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "assert '_combined_text' in train.columns and '_combined_text' in test.columns, 'Run data load cell first'\n",
        "\n",
        "# Params matching earlier better run\n",
        "word_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=50000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "char_params = dict(analyzer='char_wb', ngram_range=(3,5), lowercase=True, min_df=3, max_features=50000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "n_splits = 5\n",
        "cv = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42).split(train, y))\n",
        "print(f'Prepared {n_splits}-fold StratifiedKFold CV (shuffled).')\n",
        "\n",
        "# Precompute test meta\n",
        "meta_te_full = build_meta_features(test).replace([np.inf, -np.inf], np.nan).fillna(0).astype(np.float32)\n",
        "te_text = test['_combined_text'].astype(str)\n",
        "\n",
        "oof_lr = np.zeros(len(train), dtype=np.float32)\n",
        "oof_svc = np.zeros(len(train), dtype=np.float32)\n",
        "oof_nbsvm = np.zeros(len(train), dtype=np.float32)\n",
        "test_lr_folds, test_svc_folds, test_nbsvm_folds = [], [], []\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(cv):\n",
        "    t0 = time.time()\n",
        "    print(f'Fold {fold+1}/{n_splits} - train {len(tr_idx)} va {len(va_idx)}')\n",
        "    tr_text = train.loc[tr_idx, '_combined_text'].astype(str)\n",
        "    va_text = train.loc[va_idx, '_combined_text'].astype(str)\n",
        "\n",
        "    # Vectorizers fit on train split only\n",
        "    tfidf_w = TfidfVectorizer(**word_params)\n",
        "    Xw_tr = tfidf_w.fit_transform(tr_text)\n",
        "    Xw_va = tfidf_w.transform(va_text)\n",
        "    Xw_te = tfidf_w.transform(te_text)\n",
        "\n",
        "    tfidf_c = TfidfVectorizer(**char_params)\n",
        "    Xc_tr = tfidf_c.fit_transform(tr_text)\n",
        "    Xc_va = tfidf_c.transform(va_text)\n",
        "    Xc_te = tfidf_c.transform(te_text)\n",
        "\n",
        "    # Meta features\n",
        "    meta_tr = build_meta_features(train.loc[tr_idx]).replace([np.inf, -np.inf], np.nan).fillna(0).astype(np.float32)\n",
        "    meta_va = build_meta_features(train.loc[va_idx]).replace([np.inf, -np.inf], np.nan).fillna(0).astype(np.float32)\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    Xm_tr = scaler.fit_transform(meta_tr)\n",
        "    Xm_va = scaler.transform(meta_va)\n",
        "    Xm_te = scaler.transform(meta_te_full)\n",
        "\n",
        "    # Stack\n",
        "    X_tr = hstack([Xw_tr, Xc_tr, Xm_tr], format='csr')\n",
        "    X_va = hstack([Xw_va, Xc_va, Xm_va], format='csr')\n",
        "    X_te = hstack([Xw_te, Xc_te, Xm_te], format='csr')\n",
        "\n",
        "    # Model 1: Logistic Regression (class_weight=None per expert advice)\n",
        "    lr = LogisticRegression(solver='saga', penalty='l2', C=1.0, max_iter=4000, n_jobs=-1, class_weight=None, random_state=42, verbose=0)\n",
        "    lr.fit(X_tr, y[tr_idx])\n",
        "    va_lr = lr.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "    oof_lr[va_idx] = va_lr\n",
        "    te_lr = lr.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "    test_lr_folds.append(te_lr)\n",
        "\n",
        "    # Model 2: LinearSVC + calibration (sigmoid, cv=3)\n",
        "    svc_base = LinearSVC(C=1.0, max_iter=5000, dual=True, random_state=42)\n",
        "    svc = CalibratedClassifierCV(svc_base, method='sigmoid', cv=3)\n",
        "    svc.fit(X_tr, y[tr_idx])\n",
        "    va_svc = svc.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "    oof_svc[va_idx] = va_svc\n",
        "    te_svc = svc.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "    test_svc_folds.append(te_svc)\n",
        "\n",
        "    # Model 3: NB-SVM on word (1,2) counts only\n",
        "    cv_counts = CountVectorizer(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=50000)\n",
        "    Xc_tr_counts = cv_counts.fit_transform(tr_text)\n",
        "    Xc_va_counts = cv_counts.transform(va_text)\n",
        "    Xc_te_counts = cv_counts.transform(te_text)\n",
        "    # Compute log-count ratios\n",
        "    y_tr = y[tr_idx]\n",
        "    alpha = 1.0\n",
        "    pos_mask = (y_tr == 1)\n",
        "    neg_mask = (y_tr == 0)\n",
        "    p_pos = Xc_tr_counts[pos_mask].sum(axis=0) + alpha\n",
        "    p_neg = Xc_tr_counts[neg_mask].sum(axis=0) + alpha\n",
        "    r = np.log((p_pos / p_neg)).A1  # 1D array\n",
        "    # Transform\n",
        "    Xnb_tr = Xc_tr_counts.multiply(r)\n",
        "    Xnb_va = Xc_va_counts.multiply(r)\n",
        "    Xnb_te = Xc_te_counts.multiply(r)\n",
        "    nb_lr = LogisticRegression(solver='liblinear', penalty='l2', C=0.5, max_iter=2000, class_weight=None, random_state=42)\n",
        "    nb_lr.fit(Xnb_tr, y_tr)\n",
        "    va_nb = nb_lr.predict_proba(Xnb_va)[:,1].astype(np.float32)\n",
        "    oof_nbsvm[va_idx] = va_nb\n",
        "    te_nb = nb_lr.predict_proba(Xnb_te)[:,1].astype(np.float32)\n",
        "    test_nbsvm_folds.append(te_nb)\n",
        "\n",
        "    auc_lr = roc_auc_score(y[va_idx], va_lr)\n",
        "    auc_svc = roc_auc_score(y[va_idx], va_svc)\n",
        "    auc_nb = roc_auc_score(y[va_idx], va_nb)\n",
        "    print(f'Fold {fold+1} AUCs | LR: {auc_lr:.5f} | SVC: {auc_svc:.5f} | NB-SVM: {auc_nb:.5f} | time {time.time()-t0:.1f}s')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    del Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, Xm_tr, Xm_va, Xm_te, X_tr, X_va, X_te, lr, svc, cv_counts, Xc_tr_counts, Xc_va_counts, Xc_te_counts, nb_lr, Xnb_tr, Xnb_va, Xnb_te\n",
        "    gc.collect()\n",
        "\n",
        "# OOF scores\n",
        "auc_lr_oof = roc_auc_score(y, oof_lr)\n",
        "auc_svc_oof = roc_auc_score(y, oof_svc)\n",
        "auc_nb_oof = roc_auc_score(y, oof_nbsvm)\n",
        "print(f'OOF AUC | LR: {auc_lr_oof:.5f} | SVC: {auc_svc_oof:.5f} | NB-SVM: {auc_nb_oof:.5f}')\n",
        "\n",
        "# 3-model blend weight search on OOF (w1,w2,w3 sum=1; step=0.02)\n",
        "best_auc, best_w = -1.0, (1.0, 0.0, 0.0)\n",
        "grid = np.linspace(0.0, 1.0, 51)\n",
        "for w1 in grid:\n",
        "    for w2 in grid:\n",
        "        if w1 + w2 > 1.0:\n",
        "            continue\n",
        "        w3 = 1.0 - w1 - w2\n",
        "        blend = w1*oof_lr + w2*oof_svc + w3*oof_nbsvm\n",
        "        auc = roc_auc_score(y, blend)\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_w = auc, (float(w1), float(w2), float(w3))\n",
        "print(f'Best blend w(LR,SVC,NB)={best_w} OOF AUC: {best_auc:.5f}')\n",
        "\n",
        "# Build test preds using best weights\n",
        "test_lr = np.mean(test_lr_folds, axis=0).astype(np.float32)\n",
        "test_svc = np.mean(test_svc_folds, axis=0).astype(np.float32)\n",
        "test_nb = np.mean(test_nbsvm_folds, axis=0).astype(np.float32)\n",
        "test_pred = best_w[0]*test_lr + best_w[1]*test_svc + best_w[2]*test_nb\n",
        "\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: test_pred})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv; head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 5-fold StratifiedKFold CV (shuffled).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 AUCs | LR: 0.67317 | SVC: 0.64932 | NB-SVM: 0.57840 | time 73.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 AUCs | LR: 0.68112 | SVC: 0.66406 | NB-SVM: 0.57874 | time 74.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 AUCs | LR: 0.68007 | SVC: 0.66934 | NB-SVM: 0.57948 | time 101.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 AUCs | LR: 0.65351 | SVC: 0.63831 | NB-SVM: 0.51811 | time 79.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 AUCs | LR: 0.68030 | SVC: 0.65694 | NB-SVM: 0.58832 | time 92.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF AUC | LR: 0.67357 | SVC: 0.65506 | NB-SVM: 0.56847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best blend w(LR,SVC,NB)=(1.0, 0.0, 0.0) OOF AUC: 0.67357\nSaved submission.csv; head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.271767\n1   t3_roiuw                  0.159797\n2   t3_mjnbq                  0.145530\n3   t3_t8wd1                  0.303928\n4  t3_1m4zxu                  0.274813\n"
          ]
        }
      ]
    },
    {
      "id": "3ea0bbc9-91fa-42b3-8418-505cf32806a1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Strong LR baseline: split title/body TF-IDF, upweight title, char_wb(3,6), meta with age buckets; grid C\n",
        "import time, gc, sys, numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "assert 'request_title' in train.columns and ('request_text_edit_aware' in train.columns or 'request_text' in train.columns)\n",
        "\n",
        "def get_title_series(df):\n",
        "    return df['request_title'].fillna('').astype(str) if 'request_title' in df.columns else pd.Series(['']*len(df), index=df.index)\n",
        "\n",
        "def get_body_series(df):\n",
        "    if 'request_text_edit_aware' in df.columns:\n",
        "        return df['request_text_edit_aware'].fillna('').astype(str)\n",
        "    elif 'request_text' in df.columns:\n",
        "        return df['request_text'].fillna('').astype(str)\n",
        "    else:\n",
        "        return pd.Series(['']*len(df), index=df.index)\n",
        "\n",
        "# Params per expert prescription\n",
        "title_word_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=30000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "body_word_params  = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=70000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "char_params       = dict(analyzer='char_wb', ngram_range=(3,6), lowercase=True, min_df=3, max_features=50000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "n_splits = 5\n",
        "cv = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42).split(train, y))\n",
        "print(f'Prepared {n_splits}-fold StratifiedKFold CV (shuffled).')\n",
        "\n",
        "# Precompute test texts and meta\n",
        "te_title = get_title_series(test)\n",
        "te_body = get_body_series(test)\n",
        "te_concat = (te_title + ' ' + te_body).astype(str)\n",
        "meta_te_full = build_meta_features(test).replace([np.inf, -np.inf], np.nan).fillna(0).astype(np.float32)\n",
        "\n",
        "Cs = [0.5, 1.0, 2.0, 4.0]\n",
        "oof_by_C = {C: np.zeros(len(train), dtype=np.float32) for C in Cs}\n",
        "test_preds_by_C = {C: [] for C in Cs}\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(cv):\n",
        "    t0 = time.time()\n",
        "    print(f'Fold {fold+1}/{n_splits} - train {len(tr_idx)} va {len(va_idx)}')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    tr_title = get_title_series(train.loc[tr_idx])\n",
        "    va_title = get_title_series(train.loc[va_idx])\n",
        "    tr_body  = get_body_series(train.loc[tr_idx])\n",
        "    va_body  = get_body_series(train.loc[va_idx])\n",
        "    tr_concat = (tr_title + ' ' + tr_body).astype(str)\n",
        "    va_concat = (va_title + ' ' + va_body).astype(str)\n",
        "\n",
        "    # Fit vectorizers on train split only\n",
        "    tfidf_title = TfidfVectorizer(**title_word_params)\n",
        "    Xt_tr = tfidf_title.fit_transform(tr_title)\n",
        "    Xt_va = tfidf_title.transform(va_title)\n",
        "    Xt_te = tfidf_title.transform(te_title)\n",
        "\n",
        "    tfidf_body = TfidfVectorizer(**body_word_params)\n",
        "    Xb_tr = tfidf_body.fit_transform(tr_body)\n",
        "    Xb_va = tfidf_body.transform(va_body)\n",
        "    Xb_te = tfidf_body.transform(te_body)\n",
        "\n",
        "    tfidf_char = TfidfVectorizer(**char_params)\n",
        "    Xc_tr = tfidf_char.fit_transform(tr_concat)\n",
        "    Xc_va = tfidf_char.transform(va_concat)\n",
        "    Xc_te = tfidf_char.transform(te_concat)\n",
        "\n",
        "    # Meta features per fold\n",
        "    meta_tr = build_meta_features(train.loc[tr_idx]).replace([np.inf, -np.inf], np.nan).fillna(0).astype(np.float32)\n",
        "    meta_va = build_meta_features(train.loc[va_idx]).replace([np.inf, -np.inf], np.nan).fillna(0).astype(np.float32)\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    Xm_tr = scaler.fit_transform(meta_tr)\n",
        "    Xm_va = scaler.transform(meta_va)\n",
        "    Xm_te = scaler.transform(meta_te_full)\n",
        "\n",
        "    # Upweight title by 2.0 before stacking\n",
        "    Xt_tr = Xt_tr.multiply(2.0)\n",
        "    Xt_va = Xt_va.multiply(2.0)\n",
        "    Xt_te = Xt_te.multiply(2.0)\n",
        "\n",
        "    X_tr = hstack([Xt_tr, Xb_tr, Xc_tr, Xm_tr], format='csr')\n",
        "    X_va = hstack([Xt_va, Xb_va, Xc_va, Xm_va], format='csr')\n",
        "    X_te = hstack([Xt_te, Xb_te, Xc_te, Xm_te], format='csr')\n",
        "\n",
        "    # Train LR for each C; class_weight=None\n",
        "    for C in Cs:\n",
        "        lr = LogisticRegression(solver='saga', penalty='l2', C=C, class_weight=None, max_iter=5000, n_jobs=-1, random_state=42, verbose=0)\n",
        "        lr.fit(X_tr, y[tr_idx])\n",
        "        va_pred = lr.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "        oof_by_C[C][va_idx] = va_pred\n",
        "        te_pred = lr.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "        test_preds_by_C[C].append(te_pred)\n",
        "\n",
        "    # Per-fold diagnostics at C=1.0\n",
        "    fold_auc = roc_auc_score(y[va_idx], oof_by_C[1.0][va_idx])\n",
        "    print(f'Fold {fold+1} done in {time.time()-t0:.1f}s | AUC@C=1.0: {fold_auc:.5f}')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    del Xt_tr, Xt_va, Xt_te, Xb_tr, Xb_va, Xb_te, Xc_tr, Xc_va, Xc_te, Xm_tr, Xm_va, Xm_te, X_tr, X_va, X_te, lr\n",
        "    gc.collect()\n",
        "\n",
        "# Evaluate OOF per C and pick best\n",
        "auc_per_C = {}\n",
        "for C in Cs:\n",
        "    auc = roc_auc_score(y, oof_by_C[C])\n",
        "    auc_per_C[C] = auc\n",
        "    print(f'C={C} OOF AUC: {auc:.5f}')\n",
        "best_C = max(auc_per_C, key=auc_per_C.get)\n",
        "print(f'Best C: {best_C} with OOF AUC {auc_per_C[best_C]:.5f}')\n",
        "\n",
        "# Average test preds for best C\n",
        "best_test = np.mean(test_preds_by_C[best_C], axis=0).astype(np.float32)\n",
        "\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: best_test})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv; head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 5-fold StratifiedKFold CV (shuffled).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 done in 228.5s | AUC@C=1.0: 0.67845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 done in 238.2s | AUC@C=1.0: 0.68102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 done in 300.9s | AUC@C=1.0: 0.68588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 done in 231.2s | AUC@C=1.0: 0.66409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 done in 278.8s | AUC@C=1.0: 0.64261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=0.5 OOF AUC: 0.67283\nC=1.0 OOF AUC: 0.66993\nC=2.0 OOF AUC: 0.66629\nC=4.0 OOF AUC: 0.66307\nBest C: 0.5 with OOF AUC 0.67283\nSaved submission.csv; head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.223368\n1   t3_roiuw                  0.154929\n2   t3_mjnbq                  0.110799\n3   t3_t8wd1                  0.213140\n4  t3_1m4zxu                  0.211933\n"
          ]
        }
      ]
    },
    {
      "id": "75f608f6-4fe9-4546-8c4c-28c8403194ab",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Two-view LR: word-only (title*2 + body) and char-only; OOF blend\n",
        "import time, gc, sys, numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "def get_title_series(df):\n",
        "    return df['request_title'].fillna('').astype(str) if 'request_title' in df.columns else pd.Series(['']*len(df), index=df.index)\n",
        "\n",
        "def get_body_series(df):\n",
        "    if 'request_text_edit_aware' in df.columns:\n",
        "        return df['request_text_edit_aware'].fillna('').astype(str)\n",
        "    elif 'request_text' in df.columns:\n",
        "        return df['request_text'].fillna('').astype(str)\n",
        "    else:\n",
        "        return pd.Series(['']*len(df), index=df.index)\n",
        "\n",
        "# Vectorizer params\n",
        "title_word_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=30000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "body_word_params  = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=70000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "char_params       = dict(analyzer='char_wb', ngram_range=(3,6), lowercase=True, min_df=3, max_features=50000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "n_splits = 5\n",
        "cv = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42).split(train, y))\n",
        "print(f'Prepared {n_splits}-fold StratifiedKFold CV (shuffled).')\n",
        "\n",
        "te_title = get_title_series(test)\n",
        "te_body = get_body_series(test)\n",
        "te_concat = (te_title + ' ' + te_body).astype(str)\n",
        "\n",
        "Cs = [0.5, 1.0, 2.0]\n",
        "best_auc_overall = -1.0\n",
        "best_C_word = None\n",
        "best_C_char = None\n",
        "best_w = 0.0\n",
        "best_test_pred = None\n",
        "\n",
        "# We'll evaluate grid over C for word and char separately and blend OOF with weight w\n",
        "for Cw in Cs:\n",
        "    for Cc in Cs:\n",
        "        oof_word = np.zeros(len(train), dtype=np.float32)\n",
        "        oof_char = np.zeros(len(train), dtype=np.float32)\n",
        "        te_word_folds, te_char_folds = [], []\n",
        "\n",
        "        for fold, (tr_idx, va_idx) in enumerate(cv):\n",
        "            t0 = time.time()\n",
        "            tr_title = get_title_series(train.loc[tr_idx])\n",
        "            va_title = get_title_series(train.loc[va_idx])\n",
        "            tr_body  = get_body_series(train.loc[tr_idx])\n",
        "            va_body  = get_body_series(train.loc[va_idx])\n",
        "            tr_concat = (tr_title + ' ' + tr_body).astype(str)\n",
        "            va_concat = (va_title + ' ' + va_body).astype(str)\n",
        "\n",
        "            # Word view\n",
        "            tfidf_title = TfidfVectorizer(**title_word_params)\n",
        "            Xt_tr = tfidf_title.fit_transform(tr_title)\n",
        "            Xt_va = tfidf_title.transform(va_title)\n",
        "            Xt_te = tfidf_title.transform(te_title)\n",
        "            tfidf_body = TfidfVectorizer(**body_word_params)\n",
        "            Xb_tr = tfidf_body.fit_transform(tr_body)\n",
        "            Xb_va = tfidf_body.transform(va_body)\n",
        "            Xb_te = tfidf_body.transform(te_body)\n",
        "            Xt_tr = Xt_tr.multiply(2.0); Xt_va = Xt_va.multiply(2.0); Xt_te = Xt_te.multiply(2.0)\n",
        "            Xw_tr = hstack([Xt_tr, Xb_tr], format='csr')\n",
        "            Xw_va = hstack([Xt_va, Xb_va], format='csr')\n",
        "            Xw_te = hstack([Xt_te, Xb_te], format='csr')\n",
        "            lr_w = LogisticRegression(solver='saga', penalty='l2', C=Cw, class_weight=None, max_iter=5000, n_jobs=-1, random_state=42, verbose=0)\n",
        "            lr_w.fit(Xw_tr, y[tr_idx])\n",
        "            oof_word[va_idx] = lr_w.predict_proba(Xw_va)[:,1].astype(np.float32)\n",
        "            te_word_folds.append(lr_w.predict_proba(Xw_te)[:,1].astype(np.float32))\n",
        "\n",
        "            # Char view\n",
        "            tfidf_char = TfidfVectorizer(**char_params)\n",
        "            Xc_tr = tfidf_char.fit_transform(tr_concat)\n",
        "            Xc_va = tfidf_char.transform(va_concat)\n",
        "            Xc_te = tfidf_char.transform(te_concat)\n",
        "            lr_c = LogisticRegression(solver='saga', penalty='l2', C=Cc, class_weight=None, max_iter=5000, n_jobs=-1, random_state=42, verbose=0)\n",
        "            lr_c.fit(Xc_tr, y[tr_idx])\n",
        "            oof_char[va_idx] = lr_c.predict_proba(Xc_va)[:,1].astype(np.float32)\n",
        "            te_char_folds.append(lr_c.predict_proba(Xc_te)[:,1].astype(np.float32))\n",
        "\n",
        "            print(f'Fold {fold+1}/{n_splits} Cw={Cw} Cc={Cc} done in {time.time()-t0:.1f}s')\n",
        "            sys.stdout.flush()\n",
        "            del Xt_tr, Xt_va, Xt_te, Xb_tr, Xb_va, Xb_te, Xw_tr, Xw_va, Xw_te, lr_w, tfidf_title, tfidf_body, Xc_tr, Xc_va, Xc_te, lr_c, tfidf_char\n",
        "            gc.collect()\n",
        "\n",
        "        # Blend weight search\n",
        "        local_best_auc, local_best_w = -1.0, 0.0\n",
        "        for w in np.linspace(0.0, 1.0, 101):\n",
        "            blend = w*oof_word + (1.0-w)*oof_char\n",
        "            auc = roc_auc_score(y, blend)\n",
        "            if auc > local_best_auc:\n",
        "                local_best_auc, local_best_w = auc, float(w)\n",
        "        print(f'Cw={Cw} Cc={Cc} best blend w={local_best_w:.2f} OOF AUC: {local_best_auc:.5f}')\n",
        "\n",
        "        if local_best_auc > best_auc_overall:\n",
        "            best_auc_overall = local_best_auc\n",
        "            best_C_word, best_C_char, best_w = Cw, Cc, local_best_w\n",
        "            te_word = np.mean(te_word_folds, axis=0).astype(np.float32)\n",
        "            te_char = np.mean(te_char_folds, axis=0).astype(np.float32)\n",
        "            best_test_pred = best_w*te_word + (1.0-best_w)*te_char\n",
        "\n",
        "print(f'Best setting: Cw={best_C_word}, Cc={best_C_char}, w={best_w:.2f} with OOF AUC {best_auc_overall:.5f}')\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: best_test_pred})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv; head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 5-fold StratifiedKFold CV (shuffled).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5 Cw=0.5 Cc=0.5 done in 10.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 Cw=0.5 Cc=0.5 done in 10.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 Cw=0.5 Cc=0.5 done in 10.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 Cw=0.5 Cc=0.5 done in 9.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 Cw=0.5 Cc=0.5 done in 10.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cw=0.5 Cc=0.5 best blend w=0.20 OOF AUC: 0.64917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5 Cw=0.5 Cc=1.0 done in 11.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 Cw=0.5 Cc=1.0 done in 11.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 Cw=0.5 Cc=1.0 done in 11.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 Cw=0.5 Cc=1.0 done in 10.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 Cw=0.5 Cc=1.0 done in 11.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cw=0.5 Cc=1.0 best blend w=0.28 OOF AUC: 0.64521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5 Cw=0.5 Cc=2.0 done in 12.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 Cw=0.5 Cc=2.0 done in 12.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 Cw=0.5 Cc=2.0 done in 12.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 Cw=0.5 Cc=2.0 done in 11.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 Cw=0.5 Cc=2.0 done in 12.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cw=0.5 Cc=2.0 best blend w=0.37 OOF AUC: 0.63815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5 Cw=1.0 Cc=0.5 done in 10.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 Cw=1.0 Cc=0.5 done in 10.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 Cw=1.0 Cc=0.5 done in 10.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 Cw=1.0 Cc=0.5 done in 9.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 Cw=1.0 Cc=0.5 done in 10.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cw=1.0 Cc=0.5 best blend w=0.17 OOF AUC: 0.64890\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5 Cw=1.0 Cc=1.0 done in 11.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 Cw=1.0 Cc=1.0 done in 11.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 Cw=1.0 Cc=1.0 done in 11.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 Cw=1.0 Cc=1.0 done in 10.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 Cw=1.0 Cc=1.0 done in 11.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cw=1.0 Cc=1.0 best blend w=0.23 OOF AUC: 0.64471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5 Cw=1.0 Cc=2.0 done in 13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 Cw=1.0 Cc=2.0 done in 12.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 Cw=1.0 Cc=2.0 done in 12.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 Cw=1.0 Cc=2.0 done in 11.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 Cw=1.0 Cc=2.0 done in 12.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cw=1.0 Cc=2.0 best blend w=0.30 OOF AUC: 0.63743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5 Cw=2.0 Cc=0.5 done in 10.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 Cw=2.0 Cc=0.5 done in 10.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 Cw=2.0 Cc=0.5 done in 10.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 Cw=2.0 Cc=0.5 done in 9.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 Cw=2.0 Cc=0.5 done in 10.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cw=2.0 Cc=0.5 best blend w=0.12 OOF AUC: 0.64832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5 Cw=2.0 Cc=1.0 done in 11.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 Cw=2.0 Cc=1.0 done in 11.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 Cw=2.0 Cc=1.0 done in 11.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 Cw=2.0 Cc=1.0 done in 10.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 Cw=2.0 Cc=1.0 done in 11.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cw=2.0 Cc=1.0 best blend w=0.18 OOF AUC: 0.64385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5 Cw=2.0 Cc=2.0 done in 13.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 Cw=2.0 Cc=2.0 done in 12.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 Cw=2.0 Cc=2.0 done in 13.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 Cw=2.0 Cc=2.0 done in 12.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 Cw=2.0 Cc=2.0 done in 13.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cw=2.0 Cc=2.0 best blend w=0.24 OOF AUC: 0.63643\nBest setting: Cw=0.5, Cc=0.5, w=0.20 with OOF AUC 0.64917\nSaved submission.csv; head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.256258\n1   t3_roiuw                  0.245750\n2   t3_mjnbq                  0.225583\n3   t3_t8wd1                  0.235965\n4  t3_1m4zxu                  0.203067\n"
          ]
        }
      ]
    },
    {
      "id": "65c27d7e-9435-4d54-844c-8ec1679be443",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Text-only LR: title*2 + body word TF-IDF and char_wb(3,6); grid C; no meta\n",
        "import time, gc, sys, numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "def _get_title(df):\n",
        "    return df['request_title'].fillna('').astype(str) if 'request_title' in df.columns else pd.Series(['']*len(df), index=df.index)\n",
        "\n",
        "def _get_body(df):\n",
        "    if 'request_text_edit_aware' in df.columns:\n",
        "        return df['request_text_edit_aware'].fillna('').astype(str)\n",
        "    elif 'request_text' in df.columns:\n",
        "        return df['request_text'].fillna('').astype(str)\n",
        "    else:\n",
        "        return pd.Series(['']*len(df), index=df.index)\n",
        "\n",
        "# Vectorizer params\n",
        "title_word_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=30000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "body_word_params  = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=70000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "char_params       = dict(analyzer='char_wb', ngram_range=(3,6), lowercase=True, min_df=3, max_features=50000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "n_splits = 5\n",
        "cv = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42).split(train, y))\n",
        "print(f'Prepared {n_splits}-fold StratifiedKFold CV (shuffled).')\n",
        "\n",
        "te_title = _get_title(test)\n",
        "te_body = _get_body(test)\n",
        "te_concat = (te_title + ' ' + te_body).astype(str)\n",
        "\n",
        "Cs = [0.5, 1.0, 2.0, 4.0]\n",
        "oof_by_C = {C: np.zeros(len(train), dtype=np.float32) for C in Cs}\n",
        "test_preds_by_C = {C: [] for C in Cs}\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(cv):\n",
        "    t0 = time.time()\n",
        "    print(f'Fold {fold+1}/{n_splits} - train {len(tr_idx)} va {len(va_idx)}')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    tr_title = _get_title(train.loc[tr_idx])\n",
        "    va_title = _get_title(train.loc[va_idx])\n",
        "    tr_body  = _get_body(train.loc[tr_idx])\n",
        "    va_body  = _get_body(train.loc[va_idx])\n",
        "    tr_concat = (tr_title + ' ' + tr_body).astype(str)\n",
        "    va_concat = (va_title + ' ' + va_body).astype(str)\n",
        "\n",
        "    # Fit vectorizers on train split only\n",
        "    tfidf_title = TfidfVectorizer(**title_word_params)\n",
        "    Xt_tr = tfidf_title.fit_transform(tr_title)\n",
        "    Xt_va = tfidf_title.transform(va_title)\n",
        "    Xt_te = tfidf_title.transform(te_title)\n",
        "\n",
        "    tfidf_body = TfidfVectorizer(**body_word_params)\n",
        "    Xb_tr = tfidf_body.fit_transform(tr_body)\n",
        "    Xb_va = tfidf_body.transform(va_body)\n",
        "    Xb_te = tfidf_body.transform(te_body)\n",
        "\n",
        "    tfidf_char = TfidfVectorizer(**char_params)\n",
        "    Xc_tr = tfidf_char.fit_transform(tr_concat)\n",
        "    Xc_va = tfidf_char.transform(va_concat)\n",
        "    Xc_te = tfidf_char.transform(te_concat)\n",
        "\n",
        "    # Upweight title by 2.0 and stack (no meta)\n",
        "    Xt_tr = Xt_tr.multiply(2.0); Xt_va = Xt_va.multiply(2.0); Xt_te = Xt_te.multiply(2.0)\n",
        "    X_tr = hstack([Xt_tr, Xb_tr, Xc_tr], format='csr')\n",
        "    X_va = hstack([Xt_va, Xb_va, Xc_va], format='csr')\n",
        "    X_te = hstack([Xt_te, Xb_te, Xc_te], format='csr')\n",
        "\n",
        "    for C in Cs:\n",
        "        lr = LogisticRegression(solver='saga', penalty='l2', C=C, class_weight=None, max_iter=5000, n_jobs=-1, random_state=42, verbose=0)\n",
        "        lr.fit(X_tr, y[tr_idx])\n",
        "        oof_by_C[C][va_idx] = lr.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "        test_preds_by_C[C].append(lr.predict_proba(X_te)[:,1].astype(np.float32))\n",
        "\n",
        "    print(f'Fold {fold+1} done in {time.time()-t0:.1f}s')\n",
        "    sys.stdout.flush()\n",
        "    del Xt_tr, Xt_va, Xt_te, Xb_tr, Xb_va, Xb_te, Xc_tr, Xc_va, Xc_te, X_tr, X_va, X_te, lr, tfidf_title, tfidf_body, tfidf_char\n",
        "    gc.collect()\n",
        "\n",
        "# Evaluate OOF per C and pick best\n",
        "auc_per_C = {}\n",
        "for C in Cs:\n",
        "    auc = roc_auc_score(y, oof_by_C[C])\n",
        "    auc_per_C[C] = auc\n",
        "    print(f'C={C} OOF AUC: {auc:.5f}')\n",
        "best_C = max(auc_per_C, key=auc_per_C.get)\n",
        "print(f'Best C: {best_C} with OOF AUC {auc_per_C[best_C]:.5f}')\n",
        "\n",
        "# Average test preds for best C\n",
        "best_test = np.mean(test_preds_by_C[best_C], axis=0).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: best_test})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv; head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 5-fold StratifiedKFold CV (shuffled).\nFold 1/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 done in 70.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 done in 66.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 done in 65.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 done in 66.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 done in 65.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=0.5 OOF AUC: 0.62205\nC=1.0 OOF AUC: 0.61834\nC=2.0 OOF AUC: 0.61370\nC=4.0 OOF AUC: 0.60944\nBest C: 0.5 with OOF AUC 0.62205\nSaved submission.csv; head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.205925\n1   t3_roiuw                  0.243738\n2   t3_mjnbq                  0.193898\n3   t3_t8wd1                  0.156261\n4  t3_1m4zxu                  0.154038\n"
          ]
        }
      ]
    },
    {
      "id": "19242186-c3cd-4c0b-859f-c9d462be0890",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Pivot run: cleaned combined text + high-cap TF-IDF (word, char), subreddit TF-IDF, minimal meta; LR(saga) C grid\n",
        "import time, gc, sys, re, numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "def combine_raw_text(df):\n",
        "    title = df['request_title'].fillna('').astype(str) if 'request_title' in df.columns else pd.Series(['']*len(df), index=df.index)\n",
        "    if 'request_text_edit_aware' in df.columns:\n",
        "        body = df['request_text_edit_aware'].fillna('').astype(str)\n",
        "    elif 'request_text' in df.columns:\n",
        "        body = df['request_text'].fillna('').astype(str)\n",
        "    else:\n",
        "        body = pd.Series(['']*len(df), index=df.index)\n",
        "    return (title + '\\n' + body).astype(str)\n",
        "\n",
        "url_pat = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "num_pat = re.compile(r'\\d+')\n",
        "repeat_pat = re.compile(r'(.)\\1{3,}')\n",
        "def clean_text_series(s: pd.Series) -> pd.Series:\n",
        "    def _clean(t: str) -> str:\n",
        "        t = t.lower()\n",
        "        t = url_pat.sub(' url_token ', t)\n",
        "        t = num_pat.sub(' num_token ', t)\n",
        "        t = repeat_pat.sub(lambda m: m.group(1)*3, t)\n",
        "        return t\n",
        "    return s.fillna('').astype(str).map(_clean)\n",
        "\n",
        "def build_subreddit_text(df: pd.DataFrame) -> pd.Series:\n",
        "    if 'requester_subreddits_at_request' not in df.columns:\n",
        "        return pd.Series(['']*len(df), index=df.index)\n",
        "    def joiner(x):\n",
        "        if isinstance(x, (list, tuple)):\n",
        "            return ' '.join([str(s).lower() for s in x if isinstance(s, str)])\n",
        "        return ''\n",
        "    return df['requester_subreddits_at_request'].apply(joiner)\n",
        "\n",
        "def safe_log1p_signed(arr):\n",
        "    a = pd.to_numeric(arr, errors='coerce').astype(float)\n",
        "    return np.sign(a) * np.log1p(np.abs(a))\n",
        "\n",
        "def build_meta_minimal(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    # Core numerics\n",
        "    cols = [\n",
        "        'requester_account_age_in_days_at_request',\n",
        "        'requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_posts_at_request',\n",
        "        'requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "    ]\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            out[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "        else:\n",
        "            out[c] = 0.0\n",
        "    # Time features\n",
        "    if 'unix_timestamp_of_request' in df.columns:\n",
        "        ts = pd.to_datetime(df['unix_timestamp_of_request'], unit='s', errors='coerce')\n",
        "    elif 'unix_timestamp_of_request_utc' in df.columns:\n",
        "        ts = pd.to_datetime(df['unix_timestamp_of_request_utc'], unit='s', errors='coerce')\n",
        "    else:\n",
        "        ts = pd.Series(pd.NaT, index=df.index)\n",
        "    out['req_hour'] = ts.dt.hour.fillna(0).astype(np.int16)\n",
        "    out['req_wday'] = ts.dt.weekday.fillna(0).astype(np.int16)\n",
        "    out['req_is_weekend'] = out['req_wday'].isin([5,6]).astype(np.int8)\n",
        "    # Text-derived simple counts (from combined text computed outside per fold)\n",
        "    combo = combine_raw_text(df)\n",
        "    combo_len = combo.str.len().astype(np.int32)\n",
        "    wc = combo.str.split().map(len)\n",
        "    out['text_len'] = combo_len\n",
        "    out['word_count'] = wc.astype(np.int32)\n",
        "    out['exclaim_count'] = combo.str.count('!').astype(np.int16)\n",
        "    out['question_count'] = combo.str.count('\\?').astype(np.int16)\n",
        "    out['upper_ratio'] = combo.map(lambda s: (sum(ch.isupper() for ch in s)/max(1, len(s)))).astype(np.float32).clip(0, 0.7)\n",
        "    # Derived rates\n",
        "    age = pd.to_numeric(df.get('requester_account_age_in_days_at_request', np.nan), errors='coerce')\n",
        "    comments = pd.to_numeric(df.get('requester_number_of_comments_at_request', np.nan), errors='coerce')\n",
        "    posts = pd.to_numeric(df.get('requester_number_of_posts_at_request', np.nan), errors='coerce')\n",
        "    out['comments_per_day'] = comments / np.maximum(1.0, age)\n",
        "    out['posts_per_day'] = posts / np.maximum(1.0, age)\n",
        "    # Age bins\n",
        "    age_days = pd.to_numeric(df.get('requester_account_age_in_days_at_request', np.nan), errors='coerce').fillna(0).astype(float)\n",
        "    out['age_bin_0_30'] = (age_days <= 30).astype(np.int8)\n",
        "    out['age_bin_30_90'] = ((age_days > 30) & (age_days <= 90)).astype(np.int8)\n",
        "    out['age_bin_90_365'] = ((age_days > 90) & (age_days <= 365)).astype(np.int8)\n",
        "    out['age_bin_365p'] = (age_days > 365).astype(np.int8)\n",
        "    # RAOP history flags\n",
        "    out['has_raop_post_hist'] = (pd.to_numeric(df.get('requester_number_of_posts_on_raop_at_request', 0), errors='coerce') > 0).astype(np.int8)\n",
        "    out['has_raop_comment_hist'] = (pd.to_numeric(df.get('requester_number_of_comments_in_raop_at_request', 0), errors='coerce') > 0).astype(np.int8)\n",
        "    # Transforms\n",
        "    for c in ['requester_number_of_comments_at_request','requester_number_of_posts_at_request','requester_number_of_comments_in_raop_at_request','requester_number_of_posts_on_raop_at_request','requester_upvotes_plus_downvotes_at_request','text_len','word_count','exclaim_count','question_count','comments_per_day','posts_per_day']:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(pd.to_numeric(out[c], errors='coerce').clip(lower=0))\n",
        "    if 'requester_upvotes_minus_downvotes_at_request' in out.columns:\n",
        "        out['requester_upvotes_minus_downvotes_at_request'] = safe_log1p_signed(out['requester_upvotes_minus_downvotes_at_request'])\n",
        "    out = out.apply(pd.to_numeric, errors='coerce').replace([np.inf, -np.inf], np.nan).fillna(0).astype(np.float32)\n",
        "    return out\n",
        "\n",
        "# Vectorizer configurations\n",
        "word_params = dict(analyzer='word', ngram_range=(1,2), lowercase=False, min_df=3, max_df=0.95, max_features=100000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "char_params = dict(analyzer='char_wb', ngram_range=(3,6), lowercase=False, min_df=2, max_features=120000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "subs_params = dict(analyzer='word', ngram_range=(1,1), lowercase=True, min_df=2, max_features=10000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "n_splits = 5\n",
        "cv = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42).split(train, y))\n",
        "print(f'Prepared {n_splits}-fold StratifiedKFold CV (shuffled).')\n",
        "\n",
        "# Precompute raw sources for test\n",
        "raw_te_text = combine_raw_text(test)\n",
        "clean_te_text = clean_text_series(raw_te_text)\n",
        "subs_te_text = build_subreddit_text(test)\n",
        "meta_te_full = build_meta_minimal(test)\n",
        "\n",
        "Cs = [0.5, 1.0, 2.0]\n",
        "oof_by_C = {C: np.zeros(len(train), dtype=np.float32) for C in Cs}\n",
        "test_preds_by_C = {C: [] for C in Cs}\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(cv):\n",
        "    t0 = time.time()\n",
        "    print(f'Fold {fold+1}/{n_splits} - train {len(tr_idx)} va {len(va_idx)}')\n",
        "    sys.stdout.flush()\n",
        "    tr_text_raw = combine_raw_text(train.loc[tr_idx])\n",
        "    va_text_raw = combine_raw_text(train.loc[va_idx])\n",
        "    tr_text = clean_text_series(tr_text_raw)\n",
        "    va_text = clean_text_series(va_text_raw)\n",
        "    tr_subs = build_subreddit_text(train.loc[tr_idx])\n",
        "    va_subs = build_subreddit_text(train.loc[va_idx])\n",
        "\n",
        "    # Fit vectorizers on train split only\n",
        "    tfidf_w = TfidfVectorizer(**word_params)\n",
        "    Xw_tr = tfidf_w.fit_transform(tr_text)\n",
        "    Xw_va = tfidf_w.transform(va_text)\n",
        "    Xw_te = tfidf_w.transform(clean_te_text)\n",
        "\n",
        "    tfidf_c = TfidfVectorizer(**char_params)\n",
        "    Xc_tr = tfidf_c.fit_transform(tr_text)\n",
        "    Xc_va = tfidf_c.transform(va_text)\n",
        "    Xc_te = tfidf_c.transform(clean_te_text)\n",
        "\n",
        "    tfidf_s = TfidfVectorizer(**subs_params)\n",
        "    Xs_tr = tfidf_s.fit_transform(tr_subs)\n",
        "    Xs_va = tfidf_s.transform(va_subs)\n",
        "    Xs_te = tfidf_s.transform(subs_te_text)\n",
        "\n",
        "    # Minimal meta per fold\n",
        "    meta_tr = build_meta_minimal(train.loc[tr_idx])\n",
        "    meta_va = build_meta_minimal(train.loc[va_idx])\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    Xm_tr = scaler.fit_transform(meta_tr)\n",
        "    Xm_va = scaler.transform(meta_va)\n",
        "    Xm_te = scaler.transform(meta_te_full)\n",
        "\n",
        "    # Stack\n",
        "    X_tr = hstack([Xw_tr, Xc_tr, Xs_tr, Xm_tr], format='csr')\n",
        "    X_va = hstack([Xw_va, Xc_va, Xs_va, Xm_va], format='csr')\n",
        "    X_te = hstack([Xw_te, Xc_te, Xs_te, Xm_te], format='csr')\n",
        "\n",
        "    for C in Cs:\n",
        "        lr = LogisticRegression(solver='saga', penalty='l2', C=C, class_weight=None, max_iter=5000, n_jobs=-1, random_state=42, verbose=0)\n",
        "        lr.fit(X_tr, y[tr_idx])\n",
        "        va_pred = lr.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "        oof_by_C[C][va_idx] = va_pred\n",
        "        te_pred = lr.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "        test_preds_by_C[C].append(te_pred)\n",
        "\n",
        "    fold_auc = roc_auc_score(y[va_idx], oof_by_C[1.0][va_idx])\n",
        "    print(f'Fold {fold+1} done in {time.time()-t0:.1f}s | AUC@C=1.0: {fold_auc:.5f}')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    del Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, Xs_tr, Xs_va, Xs_te, Xm_tr, Xm_va, Xm_te, X_tr, X_va, X_te, lr\n",
        "    gc.collect()\n",
        "\n",
        "# Evaluate OOF per C and pick best\n",
        "auc_per_C = {}\n",
        "for C in Cs:\n",
        "    auc = roc_auc_score(y, oof_by_C[C])\n",
        "    auc_per_C[C] = auc\n",
        "    print(f'C={C} OOF AUC: {auc:.5f}')\n",
        "best_C = max(auc_per_C, key=auc_per_C.get)\n",
        "print(f'Best C: {best_C} with OOF AUC {auc_per_C[best_C]:.5f}')\n",
        "\n",
        "# Average test preds for best C\n",
        "best_test = np.mean(test_preds_by_C[best_C], axis=0).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: best_test})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv; head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 5-fold StratifiedKFold CV (shuffled).\nFold 1/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 done in 92.3s | AUC@C=1.0: 0.67984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 done in 87.7s | AUC@C=1.0: 0.67359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 done in 91.9s | AUC@C=1.0: 0.69048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 done in 82.6s | AUC@C=1.0: 0.64510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 done in 88.3s | AUC@C=1.0: 0.69493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=0.5 OOF AUC: 0.67860\nC=1.0 OOF AUC: 0.67634\nC=2.0 OOF AUC: 0.67050\nBest C: 0.5 with OOF AUC 0.67860\nSaved submission.csv; head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.280687\n1   t3_roiuw                  0.177380\n2   t3_mjnbq                  0.194645\n3   t3_t8wd1                  0.227205\n4  t3_1m4zxu                  0.280125\n"
          ]
        }
      ]
    },
    {
      "id": "905efeac-24d8-4fdf-80e5-8aa4b72e146a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Add calibrated RidgeClassifier and blend with LR on pivot features\n",
        "import time, gc, sys, numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Reuse helper funcs and params from pivot cell 7: combine_raw_text, clean_text_series, build_subreddit_text, build_meta_minimal,\n",
        "# and word_params, char_params, subs_params should be in scope from cell 7.\n",
        "\n",
        "assert 'combine_raw_text' in globals() and 'clean_text_series' in globals() and 'build_subreddit_text' in globals() and 'build_meta_minimal' in globals(), 'Run pivot cell first'\n",
        "assert 'word_params' in globals() and 'char_params' in globals() and 'subs_params' in globals(), 'Run pivot cell first'\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "n_splits = 5\n",
        "cv = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42).split(train, y))\n",
        "print(f'Prepared {n_splits}-fold StratifiedKFold CV (shuffled).')\n",
        "\n",
        "# Precompute test sources via same preprocessing as pivot\n",
        "raw_te_text = combine_raw_text(test)\n",
        "clean_te_text = clean_text_series(raw_te_text)\n",
        "subs_te_text = build_subreddit_text(test)\n",
        "meta_te_full = build_meta_minimal(test)\n",
        "\n",
        "oof_lr = np.zeros(len(train), dtype=np.float32)\n",
        "oof_rc = np.zeros(len(train), dtype=np.float32)\n",
        "test_lr_folds, test_rc_folds = [], []\n",
        "\n",
        "LR_C = 0.5\n",
        "ridge_alphas = [0.5, 1.0, 2.0]\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(cv):\n",
        "    t0 = time.time()\n",
        "    print(f'Fold {fold+1}/{n_splits} - train {len(tr_idx)} va {len(va_idx)}')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    tr_text_raw = combine_raw_text(train.loc[tr_idx])\n",
        "    va_text_raw = combine_raw_text(train.loc[va_idx])\n",
        "    tr_text = clean_text_series(tr_text_raw)\n",
        "    va_text = clean_text_series(va_text_raw)\n",
        "    tr_subs = build_subreddit_text(train.loc[tr_idx])\n",
        "    va_subs = build_subreddit_text(train.loc[va_idx])\n",
        "\n",
        "    # Fit vectorizers on train split only\n",
        "    tfidf_w = TfidfVectorizer(**word_params)\n",
        "    Xw_tr = tfidf_w.fit_transform(tr_text)\n",
        "    Xw_va = tfidf_w.transform(va_text)\n",
        "    Xw_te = tfidf_w.transform(clean_te_text)\n",
        "\n",
        "    tfidf_c = TfidfVectorizer(**char_params)\n",
        "    Xc_tr = tfidf_c.fit_transform(tr_text)\n",
        "    Xc_va = tfidf_c.transform(va_text)\n",
        "    Xc_te = tfidf_c.transform(clean_te_text)\n",
        "\n",
        "    tfidf_s = TfidfVectorizer(**subs_params)\n",
        "    Xs_tr = tfidf_s.fit_transform(tr_subs)\n",
        "    Xs_va = tfidf_s.transform(va_subs)\n",
        "    Xs_te = tfidf_s.transform(subs_te_text)\n",
        "\n",
        "    meta_tr = build_meta_minimal(train.loc[tr_idx])\n",
        "    meta_va = build_meta_minimal(train.loc[va_idx])\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    Xm_tr = scaler.fit_transform(meta_tr)\n",
        "    Xm_va = scaler.transform(meta_va)\n",
        "    Xm_te = scaler.transform(meta_te_full)\n",
        "\n",
        "    # Stack\n",
        "    X_tr = hstack([Xw_tr, Xc_tr, Xs_tr, Xm_tr], format='csr')\n",
        "    X_va = hstack([Xw_va, Xc_va, Xs_va, Xm_va], format='csr')\n",
        "    X_te = hstack([Xw_te, Xc_te, Xs_te, Xm_te], format='csr')\n",
        "\n",
        "    # Model 1: Logistic Regression (best C from pivot: 0.5), class_weight=None\n",
        "    lr = LogisticRegression(solver='saga', penalty='l2', C=LR_C, class_weight=None, max_iter=6000, n_jobs=-1, random_state=42, verbose=0)\n",
        "    lr.fit(X_tr, y[tr_idx])\n",
        "    va_lr = lr.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "    oof_lr[va_idx] = va_lr\n",
        "    te_lr = lr.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "    test_lr_folds.append(te_lr)\n",
        "\n",
        "    # Model 2: RidgeClassifier with sigmoid calibration; select alpha by val AUC per fold\n",
        "    best_rc_auc, best_rc_te, best_rc_va = -1.0, None, None\n",
        "    for alpha in ridge_alphas:\n",
        "        rc_base = RidgeClassifier(alpha=alpha, random_state=42)\n",
        "        rc_cal = CalibratedClassifierCV(rc_base, method='sigmoid', cv=3)\n",
        "        rc_cal.fit(X_tr, y[tr_idx])\n",
        "        va_rc = rc_cal.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "        auc_rc = roc_auc_score(y[va_idx], va_rc)\n",
        "        if auc_rc > best_rc_auc:\n",
        "            best_rc_auc = auc_rc\n",
        "            best_rc_va = va_rc\n",
        "            best_rc_te = rc_cal.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "    oof_rc[va_idx] = best_rc_va\n",
        "    test_rc_folds.append(best_rc_te)\n",
        "\n",
        "    auc_lr = roc_auc_score(y[va_idx], va_lr)\n",
        "    print(f'Fold {fold+1} AUCs | LR(C=0.5): {auc_lr:.5f} | Best RidgeCal: {best_rc_auc:.5f} | time {time.time()-t0:.1f}s')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    del Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, Xs_tr, Xs_va, Xs_te, Xm_tr, Xm_va, Xm_te, X_tr, X_va, X_te, lr, rc_base, rc_cal\n",
        "    gc.collect()\n",
        "\n",
        "# OOF scores\n",
        "auc_lr_oof = roc_auc_score(y, oof_lr)\n",
        "auc_rc_oof = roc_auc_score(y, oof_rc)\n",
        "print(f'OOF AUC | LR: {auc_lr_oof:.5f} | RidgeCal: {auc_rc_oof:.5f}')\n",
        "\n",
        "# Blend weight search\n",
        "best_w, best_auc = 0.0, -1.0\n",
        "for w in np.linspace(0.0, 1.0, 101):\n",
        "    blend = (1-w)*oof_lr + w*oof_rc\n",
        "    auc = roc_auc_score(y, blend)\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_w = auc, float(w)\n",
        "print(f'Best blend w(Ridge)={best_w:.2f} OOF AUC: {best_auc:.5f}')\n",
        "\n",
        "# Build test preds using best weight\n",
        "test_lr = np.mean(test_lr_folds, axis=0).astype(np.float32)\n",
        "test_rc = np.mean(test_rc_folds, axis=0).astype(np.float32)\n",
        "test_pred = (1-best_w)*test_lr + best_w*test_rc\n",
        "\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: test_pred})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv; head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 5-fold StratifiedKFold CV (shuffled).\nFold 1/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 AUCs | LR(C=0.5): 0.68299 | Best RidgeCal: 0.66707 | time 28.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 AUCs | LR(C=0.5): 0.66915 | Best RidgeCal: 0.68084 | time 26.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 AUCs | LR(C=0.5): 0.69675 | Best RidgeCal: 0.68388 | time 28.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 AUCs | LR(C=0.5): 0.64766 | Best RidgeCal: 0.63491 | time 25.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 AUCs | LR(C=0.5): 0.69875 | Best RidgeCal: 0.67991 | time 27.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF AUC | LR: 0.67860 | RidgeCal: 0.66922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best blend w(Ridge)=0.00 OOF AUC: 0.67860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv; head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.280687\n1   t3_roiuw                  0.177380\n2   t3_mjnbq                  0.194645\n3   t3_t8wd1                  0.227205\n4  t3_1m4zxu                  0.280125\n"
          ]
        }
      ]
    },
    {
      "id": "2a0126a5-5988-474a-b692-0242f070b413",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# XGBoost on pivot features + blend with LR (OOF-driven)\n",
        "import time, gc, sys, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    import subprocess, sys as _sys\n",
        "    subprocess.run([_sys.executable, '-m', 'pip', 'install', '-q', 'xgboost'])\n",
        "    import xgboost as xgb\n",
        "\n",
        "assert 'combine_raw_text' in globals() and 'clean_text_series' in globals() and 'build_subreddit_text' in globals() and 'build_meta_minimal' in globals(), 'Run pivot cell first'\n",
        "assert 'word_params' in globals() and 'char_params' in globals() and 'subs_params' in globals(), 'Run pivot cell first'\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "n_splits = 5\n",
        "cv = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42).split(train, y))\n",
        "print(f'Prepared {n_splits}-fold StratifiedKFold CV (shuffled).')\n",
        "\n",
        "# Precompute test sources via same preprocessing as pivot\n",
        "raw_te_text = combine_raw_text(test)\n",
        "clean_te_text = clean_text_series(raw_te_text)\n",
        "subs_te_text = build_subreddit_text(test)\n",
        "meta_te_full = build_meta_minimal(test)\n",
        "\n",
        "oof_lr = np.zeros(len(train), dtype=np.float32)\n",
        "oof_xgb = np.zeros(len(train), dtype=np.float32)\n",
        "test_lr_folds, test_xgb_folds = [], []\n",
        "\n",
        "# LR baseline params (from pivot best)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "LR_C = 0.5\n",
        "\n",
        "# Prefer GPU if available\n",
        "use_gpu = True\n",
        "\n",
        "# XGB params (regularized; no early stopping due to API limitations)\n",
        "xgb_params = dict(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='auc',\n",
        "    max_depth=3,\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=1200,\n",
        "    subsample=0.7,\n",
        "    colsample_bytree=0.5,\n",
        "    min_child_weight=10,\n",
        "    reg_alpha=1.0,\n",
        "    reg_lambda=1.8,\n",
        "    gamma=0.1,\n",
        "    tree_method='gpu_hist' if use_gpu else 'hist',\n",
        "    predictor='gpu_predictor' if use_gpu else 'auto',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(cv):\n",
        "    t0 = time.time()\n",
        "    print(f'Fold {fold+1}/{n_splits} - train {len(tr_idx)} va {len(va_idx)}')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    tr_text_raw = combine_raw_text(train.loc[tr_idx])\n",
        "    va_text_raw = combine_raw_text(train.loc[va_idx])\n",
        "    tr_text = clean_text_series(tr_text_raw)\n",
        "    va_text = clean_text_series(va_text_raw)\n",
        "    tr_subs = build_subreddit_text(train.loc[tr_idx])\n",
        "    va_subs = build_subreddit_text(train.loc[va_idx])\n",
        "\n",
        "    # Vectorizers on train split only (reuse pivot params)\n",
        "    tfidf_w = TfidfVectorizer(**word_params)\n",
        "    Xw_tr = tfidf_w.fit_transform(tr_text)\n",
        "    Xw_va = tfidf_w.transform(va_text)\n",
        "    Xw_te = tfidf_w.transform(clean_te_text)\n",
        "\n",
        "    tfidf_c = TfidfVectorizer(**char_params)\n",
        "    Xc_tr = tfidf_c.fit_transform(tr_text)\n",
        "    Xc_va = tfidf_c.transform(va_text)\n",
        "    Xc_te = tfidf_c.transform(clean_te_text)\n",
        "\n",
        "    tfidf_s = TfidfVectorizer(**subs_params)\n",
        "    Xs_tr = tfidf_s.fit_transform(tr_subs)\n",
        "    Xs_va = tfidf_s.transform(va_subs)\n",
        "    Xs_te = tfidf_s.transform(subs_te_text)\n",
        "\n",
        "    meta_tr = build_meta_minimal(train.loc[tr_idx])\n",
        "    meta_va = build_meta_minimal(train.loc[va_idx])\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    Xm_tr = scaler.fit_transform(meta_tr)\n",
        "    Xm_va = scaler.transform(meta_va)\n",
        "    Xm_te = scaler.transform(meta_te_full)\n",
        "\n",
        "    # Stack\n",
        "    X_tr = hstack([Xw_tr, Xc_tr, Xs_tr, Xm_tr], format='csr')\n",
        "    X_va = hstack([Xw_va, Xc_va, Xs_va, Xm_va], format='csr')\n",
        "    X_te = hstack([Xw_te, Xc_te, Xs_te, Xm_te], format='csr')\n",
        "\n",
        "    # Model 1: LR\n",
        "    lr = LogisticRegression(solver='saga', penalty='l2', C=LR_C, class_weight=None, max_iter=6000, n_jobs=-1, random_state=42, verbose=0)\n",
        "    lr.fit(X_tr, y[tr_idx])\n",
        "    va_lr = lr.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "    oof_lr[va_idx] = va_lr\n",
        "    te_lr = lr.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "    test_lr_folds.append(te_lr)\n",
        "\n",
        "    # Model 2: XGBoost without early stopping (version compatibility)\n",
        "    xgb_clf = xgb.XGBClassifier(**xgb_params)\n",
        "    xgb_clf.fit(\n",
        "        X_tr, y[tr_idx],\n",
        "        eval_set=[(X_va, y[va_idx])],\n",
        "        verbose=False\n",
        "    )\n",
        "    va_xgb = xgb_clf.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "    oof_xgb[va_idx] = va_xgb\n",
        "    te_xgb = xgb_clf.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "    test_xgb_folds.append(te_xgb)\n",
        "\n",
        "    auc_lr = roc_auc_score(y[va_idx], va_lr)\n",
        "    auc_x = roc_auc_score(y[va_idx], va_xgb)\n",
        "    print(f'Fold {fold+1} AUCs | LR: {auc_lr:.5f} | XGB: {auc_x:.5f} | time {time.time()-t0:.1f}s')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    del Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, Xs_tr, Xs_va, Xs_te, Xm_tr, Xm_va, Xm_te, X_tr, X_va, X_te, lr, xgb_clf\n",
        "    gc.collect()\n",
        "\n",
        "# OOF scores\n",
        "auc_lr_oof = roc_auc_score(y, oof_lr)\n",
        "auc_xgb_oof = roc_auc_score(y, oof_xgb)\n",
        "print(f'OOF AUC | LR: {auc_lr_oof:.5f} | XGB: {auc_xgb_oof:.5f}')\n",
        "\n",
        "# Blend LR + XGB by OOF weight grid\n",
        "best_w, best_auc = 0.0, -1.0\n",
        "for w in np.linspace(0.0, 1.0, 101):\n",
        "    blend = (1-w)*oof_lr + w*oof_xgb\n",
        "    auc = roc_auc_score(y, blend)\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_w = auc, float(w)\n",
        "print(f'Best blend w(XGB)={best_w:.2f} OOF AUC: {best_auc:.5f}')\n",
        "\n",
        "# Build test preds using best weight\n",
        "test_lr = np.mean(test_lr_folds, axis=0).astype(np.float32)\n",
        "test_xg = np.mean(test_xgb_folds, axis=0).astype(np.float32)\n",
        "test_pred = (1-best_w)*test_lr + best_w*test_xg\n",
        "\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: test_pred})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv; head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 5-fold StratifiedKFold CV (shuffled).\nFold 1/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:05:10] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:05:10] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"predictor\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 AUCs | LR: 0.68299 | XGB: 0.62196 | time 45.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:05:30] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:05:30] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:05:52] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:05:52] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"predictor\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 AUCs | LR: 0.66915 | XGB: 0.60482 | time 42.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:06:13] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:06:36] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:06:36] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"predictor\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 AUCs | LR: 0.69675 | XGB: 0.63090 | time 43.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:06:56] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:07:16] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:07:16] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"predictor\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 AUCs | LR: 0.64766 | XGB: 0.58929 | time 40.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:07:36] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:07:58] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:07:58] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"predictor\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 AUCs | LR: 0.69875 | XGB: 0.61814 | time 41.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:08:18] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF AUC | LR: 0.67860 | XGB: 0.61282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best blend w(XGB)=0.00 OOF AUC: 0.67860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv; head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.280687\n1   t3_roiuw                  0.177380\n2   t3_mjnbq                  0.194645\n3   t3_t8wd1                  0.227205\n4  t3_1m4zxu                  0.280125\n"
          ]
        }
      ]
    },
    {
      "id": "d6cb2e69-7cd7-4604-a635-44ca479a75ee",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Enhanced meta features + Pivot TF-IDF LR (target: push OOF >= 0.692)\n",
        "import time, gc, sys, re, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "assert 'combine_raw_text' in globals() and 'clean_text_series' in globals() and 'build_subreddit_text' in globals(), 'Run pivot cell 7 first'\n",
        "assert 'word_params' in globals() and 'char_params' in globals() and 'subs_params' in globals(), 'Run pivot cell 7 first'\n",
        "\n",
        "def build_meta_enhanced(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = build_meta_minimal(df).copy()  # start from minimal core\n",
        "    # Text for regex-based features\n",
        "    title = df['request_title'].fillna('').astype(str) if 'request_title' in df.columns else pd.Series(['']*len(df), index=df.index)\n",
        "    body = df['request_text_edit_aware'].fillna('').astype(str) if 'request_text_edit_aware' in df.columns else df['request_text'].fillna('').astype(str) if 'request_text' in df.columns else pd.Series(['']*len(df), index=df.index)\n",
        "    txt = (title + ' ' + body).str.lower()\n",
        "    # Reciprocity / return-the-favor\n",
        "    out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n",
        "    # Payday / timeframe\n",
        "    out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n",
        "    out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n",
        "    # Concrete amount ($ or N dollars)\n",
        "    out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n",
        "    # Gratitude density\n",
        "    wc = np.maximum(1, out.get('word_count', (title + ' ' + body).str.split().map(len)).astype(np.int32))\n",
        "    grat_cnt = txt.str.count(r'\\b(thank|thanks|thank you|appreciate|grateful)\\b')\n",
        "    out['grat_density'] = (grat_cnt / wc).astype(np.float32).clip(0, 0.3)\n",
        "    # Pronoun ratios\n",
        "    fp_cnt = txt.str.count(r'\\b(i|me|my|we|our|us)\\b')\n",
        "    you_cnt = txt.str.count(r'\\b(you|your)\\b')\n",
        "    out['first_person_ratio'] = (fp_cnt / wc).astype(np.float32).clip(0, 1.0)\n",
        "    out['you_ratio'] = (you_cnt / wc).astype(np.float32).clip(0, 1.0)\n",
        "    # Time-of-day bins + sin/cos hour\n",
        "    if 'unix_timestamp_of_request' in df.columns:\n",
        "        ts = pd.to_datetime(df['unix_timestamp_of_request'], unit='s', errors='coerce')\n",
        "    elif 'unix_timestamp_of_request_utc' in df.columns:\n",
        "        ts = pd.to_datetime(df['unix_timestamp_of_request_utc'], unit='s', errors='coerce')\n",
        "    else:\n",
        "        ts = pd.Series(pd.NaT, index=df.index)\n",
        "    hour = ts.dt.hour.fillna(0).astype(int)\n",
        "    out['tod_morning'] = ((hour>=6)&(hour<=11)).astype(np.int8)\n",
        "    out['tod_afternoon'] = ((hour>=12)&(hour<=17)).astype(np.int8)\n",
        "    out['tod_evening'] = ((hour>=18)&(hour<=23)).astype(np.int8)\n",
        "    out['tod_night'] = ((hour>=0)&(hour<=5)).astype(np.int8)\n",
        "    out['hour_sin'] = np.sin(2*np.pi*hour/24).astype(np.float32)\n",
        "    out['hour_cos'] = np.cos(2*np.pi*hour/24).astype(np.float32)\n",
        "    # Mentions kids\n",
        "    out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n",
        "    # Title features\n",
        "    tl = title.str.len().astype(np.int32)\n",
        "    twc = title.str.split().map(len).astype(np.int32).replace(0, 1)\n",
        "    out['title_word_count'] = np.log1p(twc).astype(np.float32)\n",
        "    out['title_has_question'] = title.str.contains('\\?', regex=True).astype(np.int8)\n",
        "    out['title_has_please'] = title.str.contains(r'\\bplease\\b', case=False, regex=True).astype(np.int8)\n",
        "    # Avg word length\n",
        "    total_chars = (title + ' ' + body).str.replace('\\s+', '', regex=True).str.len().astype(np.int32)\n",
        "    out['avg_word_len'] = (total_chars / wc).astype(np.float32).replace([np.inf, -np.inf], 0).clip(0, 15)\n",
        "    # requester_number_of_subreddits_at_request transforms\n",
        "    if 'requester_number_of_subreddits_at_request' in df.columns:\n",
        "        val = pd.to_numeric(df['requester_number_of_subreddits_at_request'], errors='coerce').fillna(0)\n",
        "        out['subs_count_log1p'] = np.log1p(val).astype(np.float32)\n",
        "        out['subs_high_bin'] = (val >= 50).astype(np.int8)\n",
        "    return out.replace([np.inf, -np.inf], np.nan).fillna(0).astype(np.float32)\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "n_splits = 5\n",
        "cv = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42).split(train, y))\n",
        "print(f'Prepared {n_splits}-fold StratifiedKFold CV (shuffled).')\n",
        "\n",
        "# Precompute test sources via same preprocessing as pivot\n",
        "raw_te_text = combine_raw_text(test)\n",
        "clean_te_text = clean_text_series(raw_te_text)\n",
        "subs_te_text = build_subreddit_text(test)\n",
        "meta_te_full = build_meta_enhanced(test)\n",
        "\n",
        "Cs = [0.3, 0.5, 0.8]\n",
        "oof_by_C = {C: np.zeros(len(train), dtype=np.float32) for C in Cs}\n",
        "test_preds_by_C = {C: [] for C in Cs}\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(cv):\n",
        "    t0 = time.time()\n",
        "    print(f'Fold {fold+1}/{n_splits} - train {len(tr_idx)} va {len(va_idx)}')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    tr_text_raw = combine_raw_text(train.loc[tr_idx])\n",
        "    va_text_raw = combine_raw_text(train.loc[va_idx])\n",
        "    tr_text = clean_text_series(tr_text_raw)\n",
        "    va_text = clean_text_series(va_text_raw)\n",
        "    tr_subs = build_subreddit_text(train.loc[tr_idx])\n",
        "    va_subs = build_subreddit_text(train.loc[va_idx])\n",
        "\n",
        "    # Fit vectorizers on train split only (pivot params)\n",
        "    tfidf_w = TfidfVectorizer(**word_params)\n",
        "    Xw_tr = tfidf_w.fit_transform(tr_text)\n",
        "    Xw_va = tfidf_w.transform(va_text)\n",
        "    Xw_te = tfidf_w.transform(clean_te_text)\n",
        "\n",
        "    tfidf_c = TfidfVectorizer(**char_params)\n",
        "    Xc_tr = tfidf_c.fit_transform(tr_text)\n",
        "    Xc_va = tfidf_c.transform(va_text)\n",
        "    Xc_te = tfidf_c.transform(clean_te_text)\n",
        "\n",
        "    tfidf_s = TfidfVectorizer(**subs_params)\n",
        "    Xs_tr = tfidf_s.fit_transform(tr_subs)\n",
        "    Xs_va = tfidf_s.transform(va_subs)\n",
        "    Xs_te = tfidf_s.transform(subs_te_text)\n",
        "\n",
        "    meta_tr = build_meta_enhanced(train.loc[tr_idx])\n",
        "    meta_va = build_meta_enhanced(train.loc[va_idx])\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    Xm_tr = scaler.fit_transform(meta_tr)\n",
        "    Xm_va = scaler.transform(meta_va)\n",
        "    Xm_te = scaler.transform(meta_te_full)\n",
        "\n",
        "    # Stack\n",
        "    X_tr = hstack([Xw_tr, Xc_tr, Xs_tr, Xm_tr], format='csr')\n",
        "    X_va = hstack([Xw_va, Xc_va, Xs_va, Xm_va], format='csr')\n",
        "    X_te = hstack([Xw_te, Xc_te, Xs_te, Xm_te], format='csr')\n",
        "\n",
        "    for C in Cs:\n",
        "        lr = LogisticRegression(solver='saga', penalty='l2', C=C, class_weight=None, max_iter=6000, n_jobs=-1, random_state=42, verbose=0)\n",
        "        lr.fit(X_tr, y[tr_idx])\n",
        "        va_pred = lr.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "        oof_by_C[C][va_idx] = va_pred\n",
        "        te_pred = lr.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "        test_preds_by_C[C].append(te_pred)\n",
        "\n",
        "    fold_auc = roc_auc_score(y[va_idx], oof_by_C[0.5][va_idx])\n",
        "    print(f'Fold {fold+1} done in {time.time()-t0:.1f}s | AUC@C=0.5: {fold_auc:.5f}')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    del Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, Xs_tr, Xs_va, Xs_te, Xm_tr, Xm_va, Xm_te, X_tr, X_va, X_te, lr\n",
        "    gc.collect()\n",
        "\n",
        "# Evaluate OOF per C and pick best\n",
        "auc_per_C = {}\n",
        "for C in Cs:\n",
        "    auc = roc_auc_score(y, oof_by_C[C])\n",
        "    auc_per_C[C] = auc\n",
        "    print(f'C={C} OOF AUC: {auc:.5f}')\n",
        "best_C = max(auc_per_C, key=auc_per_C.get)\n",
        "print(f'Best C: {best_C} with OOF AUC {auc_per_C[best_C]:.5f}')\n",
        "\n",
        "# Average test preds for best C\n",
        "best_test = np.mean(test_preds_by_C[best_C], axis=0).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: best_test})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv; head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 5-fold StratifiedKFold CV (shuffled).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 done in 81.9s | AUC@C=0.5: 0.69215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 done in 78.3s | AUC@C=0.5: 0.67601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 done in 78.9s | AUC@C=0.5: 0.69970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 done in 73.8s | AUC@C=0.5: 0.63280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 done in 78.5s | AUC@C=0.5: 0.68046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=0.3 OOF AUC: 0.67501\nC=0.5 OOF AUC: 0.67569\nC=0.8 OOF AUC: 0.67463\nBest C: 0.5 with OOF AUC 0.67569\nSaved submission.csv; head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.292437\n1   t3_roiuw                  0.178725\n2   t3_mjnbq                  0.216726\n3   t3_t8wd1                  0.227825\n4  t3_1m4zxu                  0.388530\n"
          ]
        }
      ]
    },
    {
      "id": "b52fc345-6e60-4c6f-9b53-f30a9ee600ef",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Dense view: SVD(word, char, subs) + enhanced meta -> XGBoost (DMatrix, device=cuda) with early stopping; seed-bag; build OOF and submission\n",
        "import time, gc, sys, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    import subprocess, sys as _sys\n",
        "    subprocess.run([_sys.executable, '-m', 'pip', 'install', '-q', 'xgboost'])\n",
        "    import xgboost as xgb\n",
        "\n",
        "assert 'combine_raw_text' in globals() and 'clean_text_series' in globals() and 'build_subreddit_text' in globals(), 'Run pivot cell 7 first'\n",
        "assert 'word_params' in globals() and 'char_params' in globals() and 'subs_params' in globals(), 'Run pivot cell 7 first'\n",
        "assert 'build_meta_enhanced' in globals(), 'Run cell 10 to define build_meta_enhanced'\n",
        "\n",
        "# SVD component sizes (per expert advice)\n",
        "svd_word_n = 250\n",
        "svd_char_n = 250\n",
        "svd_subs_n = 80\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "n_splits = 5\n",
        "cv = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42).split(train, y))\n",
        "print(f'Prepared {n_splits}-fold StratifiedKFold CV (shuffled).')\n",
        "\n",
        "# Precompute test sources\n",
        "raw_te_text = combine_raw_text(test)\n",
        "clean_te_text = clean_text_series(raw_te_text)\n",
        "subs_te_text = build_subreddit_text(test)\n",
        "meta_te = build_meta_enhanced(test).astype(np.float32)\n",
        "\n",
        "# Containers for seed-bagging (expanded seeds for more stability)\n",
        "seeds = [42, 2025, 7, 11, 77]\n",
        "oof_bag = np.zeros((len(train), len(seeds)), dtype=np.float32)\n",
        "test_bag = np.zeros((len(test), len(seeds)), dtype=np.float32)\n",
        "\n",
        "# Fixed XGB params (dense view, slightly deeper trees)\n",
        "base_params = dict(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='auc',\n",
        "    max_depth=4,\n",
        "    eta=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.7,\n",
        "    min_child_weight=4,\n",
        "    reg_alpha=0.5,\n",
        "    reg_lambda=2.0,\n",
        "    gamma=0.0,\n",
        "    device='cuda'\n",
        ")\n",
        "\n",
        "for si, seed in enumerate(seeds):\n",
        "    print(f'=== Seed {seed} / {len(seeds)} ===')\n",
        "    params = dict(base_params)\n",
        "    params['seed'] = seed\n",
        "    oof = np.zeros(len(train), dtype=np.float32)\n",
        "    test_fold_preds = []\n",
        "\n",
        "    for fold, (tr_idx, va_idx) in enumerate(cv):\n",
        "        t0 = time.time()\n",
        "        print(f'Seed {seed} | Fold {fold+1}/{n_splits} - train {len(tr_idx)} va {len(va_idx)}')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Text prep\n",
        "        tr_text_raw = combine_raw_text(train.loc[tr_idx])\n",
        "        va_text_raw = combine_raw_text(train.loc[va_idx])\n",
        "        tr_text = clean_text_series(tr_text_raw)\n",
        "        va_text = clean_text_series(va_text_raw)\n",
        "        tr_subs = build_subreddit_text(train.loc[tr_idx])\n",
        "        va_subs = build_subreddit_text(train.loc[va_idx])\n",
        "\n",
        "        # TF-IDF (fit on train split)\n",
        "        tfidf_w = TfidfVectorizer(**word_params)\n",
        "        Xw_tr = tfidf_w.fit_transform(tr_text)\n",
        "        Xw_va = tfidf_w.transform(va_text)\n",
        "        Xw_te = tfidf_w.transform(clean_te_text)\n",
        "\n",
        "        tfidf_c = TfidfVectorizer(**char_params)\n",
        "        Xc_tr = tfidf_c.fit_transform(tr_text)\n",
        "        Xc_va = tfidf_c.transform(va_text)\n",
        "        Xc_te = tfidf_c.transform(clean_te_text)\n",
        "\n",
        "        tfidf_s = TfidfVectorizer(**subs_params)\n",
        "        Xs_tr = tfidf_s.fit_transform(tr_subs)\n",
        "        Xs_va = tfidf_s.transform(va_subs)\n",
        "        Xs_te = tfidf_s.transform(subs_te_text)\n",
        "\n",
        "        # SVD to dense\n",
        "        svd_w = TruncatedSVD(n_components=svd_word_n, random_state=seed)\n",
        "        Zw_tr = svd_w.fit_transform(Xw_tr)\n",
        "        Zw_va = svd_w.transform(Xw_va)\n",
        "        Zw_te = svd_w.transform(Xw_te)\n",
        "\n",
        "        svd_c = TruncatedSVD(n_components=svd_char_n, random_state=seed)\n",
        "        Zc_tr = svd_c.fit_transform(Xc_tr)\n",
        "        Zc_va = svd_c.transform(Xc_va)\n",
        "        Zc_te = svd_c.transform(Xc_te)\n",
        "\n",
        "        svd_s = TruncatedSVD(n_components=svd_subs_n, random_state=seed)\n",
        "        Zs_tr = svd_s.fit_transform(Xs_tr)\n",
        "        Zs_va = svd_s.transform(Xs_va)\n",
        "        Zs_te = svd_s.transform(Xs_te)\n",
        "\n",
        "        # Enhanced meta (dense)\n",
        "        meta_tr = build_meta_enhanced(train.loc[tr_idx]).astype(np.float32)\n",
        "        meta_va = build_meta_enhanced(train.loc[va_idx]).astype(np.float32)\n",
        "\n",
        "        # Concatenate dense blocks\n",
        "        Xtr_dense = np.hstack([Zw_tr, Zc_tr, Zs_tr, meta_tr.values]).astype(np.float32)\n",
        "        Xva_dense = np.hstack([Zw_va, Zc_va, Zs_va, meta_va.values]).astype(np.float32)\n",
        "        Xte_dense = np.hstack([Zw_te, Zc_te, Zs_te, meta_te.values]).astype(np.float32)\n",
        "\n",
        "        # Scale dense features\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "        Xtr_d = scaler.fit_transform(Xtr_dense)\n",
        "        Xva_d = scaler.transform(Xva_dense)\n",
        "        Xte_d = scaler.transform(Xte_dense)\n",
        "\n",
        "        # XGB training via DMatrix with early stopping\n",
        "        dtrain = xgb.DMatrix(Xtr_d, label=y[tr_idx])\n",
        "        dvalid = xgb.DMatrix(Xva_d, label=y[va_idx])\n",
        "        dtest  = xgb.DMatrix(Xte_d)\n",
        "        evals = [(dtrain, 'train'), (dvalid, 'valid')]\n",
        "        booster = xgb.train(\n",
        "            params,\n",
        "            dtrain,\n",
        "            num_boost_round=4000,\n",
        "            evals=evals,\n",
        "            early_stopping_rounds=200,\n",
        "            verbose_eval=False\n",
        "        )\n",
        "        va_pred = booster.predict(dvalid, iteration_range=(0, booster.best_iteration+1)).astype(np.float32)\n",
        "        te_pred = booster.predict(dtest, iteration_range=(0, booster.best_iteration+1)).astype(np.float32)\n",
        "        oof[va_idx] = va_pred\n",
        "        test_fold_preds.append(te_pred)\n",
        "\n",
        "        auc = roc_auc_score(y[va_idx], va_pred)\n",
        "        print(f'Seed {seed} | Fold {fold+1} AUC: {auc:.5f} | best_iter={booster.best_iteration} | time {time.time()-t0:.1f}s')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        del Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, Xs_tr, Xs_va, Xs_te, Zw_tr, Zw_va, Zw_te, Zc_tr, Zc_va, Zc_te, Zs_tr, Zs_va, Zs_te, meta_tr, meta_va, Xtr_dense, Xva_dense, Xte_dense, Xtr_d, Xva_d, Xte_d, dtrain, dvalid, dtest, booster, tfidf_w, tfidf_c, tfidf_s, svd_w, svd_c, svd_s, scaler\n",
        "        gc.collect()\n",
        "\n",
        "    # Store bagged predictions\n",
        "    oof_bag[:, si] = oof\n",
        "    test_bag[:, si] = np.mean(test_fold_preds, axis=0).astype(np.float32)\n",
        "    auc_seed = roc_auc_score(y, oof)\n",
        "    print(f'Seed {seed} OOF AUC: {auc_seed:.5f}')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "# Average across seeds\n",
        "oof_mean = oof_bag.mean(axis=1)\n",
        "test_mean = test_bag.mean(axis=1)\n",
        "auc_oof = roc_auc_score(y, oof_mean)\n",
        "print(f'Bagged XGB (SVD+dense) OOF AUC: {auc_oof:.5f}')\n",
        "\n",
        "# Save submission from dense XGB alone (will blend with LR later if needed)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: test_mean.astype(np.float32)})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv; head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 5-fold StratifiedKFold CV (shuffled).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Seed 42 / 5 ===\nSeed 42 | Fold 1/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 1 AUC: 0.68139 | best_iter=98 | time 16.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 2/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 2 AUC: 0.64557 | best_iter=291 | time 17.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 3/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 3 AUC: 0.67763 | best_iter=58 | time 16.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 4/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 4 AUC: 0.63253 | best_iter=49 | time 16.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 5/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 5 AUC: 0.70090 | best_iter=58 | time 16.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 OOF AUC: 0.65929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Seed 2025 / 5 ===\nSeed 2025 | Fold 1/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 1 AUC: 0.66244 | best_iter=91 | time 16.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 2/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 2 AUC: 0.65729 | best_iter=40 | time 16.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 3/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 3 AUC: 0.69851 | best_iter=228 | time 19.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 4/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 4 AUC: 0.62073 | best_iter=126 | time 16.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 5/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 5 AUC: 0.70964 | best_iter=21 | time 16.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 OOF AUC: 0.65686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Seed 7 / 5 ===\nSeed 7 | Fold 1/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 1 AUC: 0.67655 | best_iter=196 | time 17.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 2/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 2 AUC: 0.66259 | best_iter=19 | time 16.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 3/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 3 AUC: 0.68034 | best_iter=99 | time 16.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 4/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 4 AUC: 0.63693 | best_iter=211 | time 17.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 5/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 5 AUC: 0.70461 | best_iter=161 | time 17.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 OOF AUC: 0.66390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Seed 11 / 5 ===\nSeed 11 | Fold 1/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 11 | Fold 1 AUC: 0.64743 | best_iter=86 | time 16.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 11 | Fold 2/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 11 | Fold 2 AUC: 0.66429 | best_iter=61 | time 16.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 11 | Fold 3/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 11 | Fold 3 AUC: 0.65905 | best_iter=80 | time 16.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 11 | Fold 4/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 11 | Fold 4 AUC: 0.62500 | best_iter=624 | time 18.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 11 | Fold 5/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 11 | Fold 5 AUC: 0.71021 | best_iter=8 | time 16.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 11 OOF AUC: 0.63608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Seed 77 / 5 ===\nSeed 77 | Fold 1/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 77 | Fold 1 AUC: 0.67007 | best_iter=88 | time 16.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 77 | Fold 2/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 77 | Fold 2 AUC: 0.65960 | best_iter=55 | time 16.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 77 | Fold 3/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 77 | Fold 3 AUC: 0.69084 | best_iter=47 | time 16.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 77 | Fold 4/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 77 | Fold 4 AUC: 0.64407 | best_iter=196 | time 17.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 77 | Fold 5/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 77 | Fold 5 AUC: 0.70803 | best_iter=10 | time 16.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 77 OOF AUC: 0.65565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagged XGB (SVD+dense) OOF AUC: 0.66977\nSaved submission.csv; head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.350501\n1   t3_roiuw                  0.218400\n2   t3_mjnbq                  0.233026\n3   t3_t8wd1                  0.197108\n4  t3_1m4zxu                  0.148176\n"
          ]
        }
      ]
    },
    {
      "id": "ceb475c9-fe04-476c-a140-d10881a23847",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Blend pivot LR (sparse) with bagged XGB (SVD+dense) using OOF to tune weight; write submission\n",
        "import time, gc, sys, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "assert 'combine_raw_text' in globals() and 'clean_text_series' in globals() and 'build_subreddit_text' in globals() and 'build_meta_minimal' in globals(), 'Run pivot cell 7 first'\n",
        "assert 'word_params' in globals() and 'char_params' in globals() and 'subs_params' in globals(), 'Run pivot cell 7 first'\n",
        "assert 'oof_mean' in globals() and 'test_mean' in globals(), 'Run cell 11 (dense XGB) to get oof_mean/test_mean'\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "n_splits = 5\n",
        "cv = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42).split(train, y))\n",
        "print(f'Prepared {n_splits}-fold StratifiedKFold CV (shuffled).')\n",
        "\n",
        "# Precompute test sources via same preprocessing as pivot\n",
        "raw_te_text = combine_raw_text(test)\n",
        "clean_te_text = clean_text_series(raw_te_text)\n",
        "subs_te_text = build_subreddit_text(test)\n",
        "meta_te_full = build_meta_minimal(test)\n",
        "\n",
        "# Containers for LR (pivot) OOF/test\n",
        "oof_lr = np.zeros(len(train), dtype=np.float32)\n",
        "test_lr_folds = []\n",
        "LR_C = 0.5\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(cv):\n",
        "    t0 = time.time()\n",
        "    print(f'Fold {fold+1}/{n_splits} - train {len(tr_idx)} va {len(va_idx)}')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    tr_text_raw = combine_raw_text(train.loc[tr_idx])\n",
        "    va_text_raw = combine_raw_text(train.loc[va_idx])\n",
        "    tr_text = clean_text_series(tr_text_raw)\n",
        "    va_text = clean_text_series(va_text_raw)\n",
        "    tr_subs = build_subreddit_text(train.loc[tr_idx])\n",
        "    va_subs = build_subreddit_text(train.loc[va_idx])\n",
        "\n",
        "    tfidf_w = TfidfVectorizer(**word_params)\n",
        "    Xw_tr = tfidf_w.fit_transform(tr_text)\n",
        "    Xw_va = tfidf_w.transform(va_text)\n",
        "    Xw_te = tfidf_w.transform(clean_te_text)\n",
        "\n",
        "    tfidf_c = TfidfVectorizer(**char_params)\n",
        "    Xc_tr = tfidf_c.fit_transform(tr_text)\n",
        "    Xc_va = tfidf_c.transform(va_text)\n",
        "    Xc_te = tfidf_c.transform(clean_te_text)\n",
        "\n",
        "    tfidf_s = TfidfVectorizer(**subs_params)\n",
        "    Xs_tr = tfidf_s.fit_transform(tr_subs)\n",
        "    Xs_va = tfidf_s.transform(va_subs)\n",
        "    Xs_te = tfidf_s.transform(subs_te_text)\n",
        "\n",
        "    meta_tr = build_meta_minimal(train.loc[tr_idx])\n",
        "    meta_va = build_meta_minimal(train.loc[va_idx])\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    Xm_tr = scaler.fit_transform(meta_tr)\n",
        "    Xm_va = scaler.transform(meta_va)\n",
        "    Xm_te = scaler.transform(meta_te_full)\n",
        "\n",
        "    X_tr = hstack([Xw_tr, Xc_tr, Xs_tr, Xm_tr], format='csr')\n",
        "    X_va = hstack([Xw_va, Xc_va, Xs_va, Xm_va], format='csr')\n",
        "    X_te = hstack([Xw_te, Xc_te, Xs_te, Xm_te], format='csr')\n",
        "\n",
        "    lr = LogisticRegression(solver='saga', penalty='l2', C=LR_C, class_weight=None, max_iter=6000, n_jobs=-1, random_state=42, verbose=0)\n",
        "    lr.fit(X_tr, y[tr_idx])\n",
        "    va_lr = lr.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "    oof_lr[va_idx] = va_lr\n",
        "    te_lr = lr.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "    test_lr_folds.append(te_lr)\n",
        "\n",
        "    auc = roc_auc_score(y[va_idx], va_lr)\n",
        "    print(f'Fold {fold+1} LR AUC: {auc:.5f} | time {time.time()-t0:.1f}s')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    del Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, Xs_tr, Xs_va, Xs_te, Xm_tr, Xm_va, Xm_te, X_tr, X_va, X_te, lr\n",
        "    gc.collect()\n",
        "\n",
        "# OOF diagnostics\n",
        "auc_lr_oof = roc_auc_score(y, oof_lr)\n",
        "print(f'LR (pivot) OOF AUC: {auc_lr_oof:.5f}')\n",
        "auc_xgb_dense_oof = roc_auc_score(y, oof_mean)\n",
        "print(f'Dense XGB OOF AUC: {auc_xgb_dense_oof:.5f}')\n",
        "\n",
        "# Blend weight search with fine grid\n",
        "best_auc, best_w = -1.0, 0.0\n",
        "for w in np.linspace(0.0, 1.0, 201):\n",
        "    blend = (1.0 - w)*oof_lr + w*oof_mean\n",
        "    auc = roc_auc_score(y, blend)\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_w = auc, float(w)\n",
        "print(f'Best blend w(XGB_dense)={best_w:.3f} OOF AUC: {best_auc:.5f}')\n",
        "\n",
        "# Build blended test predictions\n",
        "test_lr = np.mean(test_lr_folds, axis=0).astype(np.float32)\n",
        "test_pred = (1.0 - best_w)*test_lr + best_w*test_mean.astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: test_pred})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv; head:')\n",
        "print(sub.head())\n",
        "\n",
        "# Optionally cache OOF/test for reuse\n",
        "np.save('oof_lr_pivot.npy', oof_lr)\n",
        "np.save('oof_xgb_dense.npy', oof_mean)\n",
        "np.save('test_lr_pivot.npy', test_lr)\n",
        "np.save('test_xgb_dense.npy', test_mean.astype(np.float32))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 5-fold StratifiedKFold CV (shuffled).\nFold 1/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 LR AUC: 0.68299 | time 21.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 LR AUC: 0.66915 | time 20.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 LR AUC: 0.69675 | time 22.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 LR AUC: 0.64766 | time 19.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 LR AUC: 0.69875 | time 20.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR (pivot) OOF AUC: 0.67860\nDense XGB OOF AUC: 0.67561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best blend w(XGB_dense)=0.510 OOF AUC: 0.68999\nSaved submission.csv; head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.319026\n1   t3_roiuw                  0.200970\n2   t3_mjnbq                  0.210793\n3   t3_t8wd1                  0.202225\n4  t3_1m4zxu                  0.209998\n"
          ]
        }
      ]
    },
    {
      "id": "09ea26a6-db44-4d8f-b9ea-10e7c3ec8416",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Elastic-net LR on pivot sparse features; blend with dense XGB OOF (aim >= 0.692)\n",
        "import time, gc, sys, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "assert 'combine_raw_text' in globals() and 'clean_text_series' in globals() and 'build_subreddit_text' in globals() and 'build_meta_minimal' in globals(), 'Run pivot cell 7 first'\n",
        "assert 'word_params' in globals() and 'char_params' in globals() and 'subs_params' in globals(), 'Run pivot cell 7 first'\n",
        "assert 'oof_mean' in globals() and 'test_mean' in globals(), 'Run cell 11 (dense XGB) first'\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "n_splits = 5\n",
        "cv = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42).split(train, y))\n",
        "print(f'Prepared {n_splits}-fold StratifiedKFold CV (shuffled).')\n",
        "\n",
        "# Precompute test sources via same preprocessing as pivot\n",
        "raw_te_text = combine_raw_text(test)\n",
        "clean_te_text = clean_text_series(raw_te_text)\n",
        "subs_te_text = build_subreddit_text(test)\n",
        "meta_te_full = build_meta_minimal(test)\n",
        "\n",
        "# EN grid\n",
        "Cs = [0.2, 0.5, 1.0]\n",
        "l1s = [0.1, 0.3, 0.5]\n",
        "oof_grid = {(C,l1): np.zeros(len(train), dtype=np.float32) for C in Cs for l1 in l1s}\n",
        "test_grid = {(C,l1): [] for C in Cs for l1 in l1s}\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(cv):\n",
        "    t0 = time.time()\n",
        "    print(f'Fold {fold+1}/{n_splits} - train {len(tr_idx)} va {len(va_idx)}')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    tr_text_raw = combine_raw_text(train.loc[tr_idx])\n",
        "    va_text_raw = combine_raw_text(train.loc[va_idx])\n",
        "    tr_text = clean_text_series(tr_text_raw)\n",
        "    va_text = clean_text_series(va_text_raw)\n",
        "    tr_subs = build_subreddit_text(train.loc[tr_idx])\n",
        "    va_subs = build_subreddit_text(train.loc[va_idx])\n",
        "\n",
        "    tfidf_w = TfidfVectorizer(**word_params)\n",
        "    Xw_tr = tfidf_w.fit_transform(tr_text)\n",
        "    Xw_va = tfidf_w.transform(va_text)\n",
        "    Xw_te = tfidf_w.transform(clean_te_text)\n",
        "\n",
        "    tfidf_c = TfidfVectorizer(**char_params)\n",
        "    Xc_tr = tfidf_c.fit_transform(tr_text)\n",
        "    Xc_va = tfidf_c.transform(va_text)\n",
        "    Xc_te = tfidf_c.transform(clean_te_text)\n",
        "\n",
        "    tfidf_s = TfidfVectorizer(**subs_params)\n",
        "    Xs_tr = tfidf_s.fit_transform(tr_subs)\n",
        "    Xs_va = tfidf_s.transform(va_subs)\n",
        "    Xs_te = tfidf_s.transform(subs_te_text)\n",
        "\n",
        "    meta_tr = build_meta_minimal(train.loc[tr_idx])\n",
        "    meta_va = build_meta_minimal(train.loc[va_idx])\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    Xm_tr = scaler.fit_transform(meta_tr)\n",
        "    Xm_va = scaler.transform(meta_va)\n",
        "    Xm_te = scaler.transform(meta_te_full)\n",
        "\n",
        "    X_tr = hstack([Xw_tr, Xc_tr, Xs_tr, Xm_tr], format='csr')\n",
        "    X_va = hstack([Xw_va, Xc_va, Xs_va, Xm_va], format='csr')\n",
        "    X_te = hstack([Xw_te, Xc_te, Xs_te, Xm_te], format='csr')\n",
        "\n",
        "    for C in Cs:\n",
        "        for l1 in l1s:\n",
        "            clf = LogisticRegression(solver='saga', penalty='elasticnet', l1_ratio=l1, C=C, class_weight=None, max_iter=6000, n_jobs=-1, random_state=42, verbose=0)\n",
        "            clf.fit(X_tr, y[tr_idx])\n",
        "            va_pred = clf.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "            oof_grid[(C,l1)][va_idx] = va_pred\n",
        "            te_pred = clf.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "            test_grid[(C,l1)].append(te_pred)\n",
        "\n",
        "    ref_auc = roc_auc_score(y[va_idx], oof_grid[(0.5,0.3)][va_idx]) if (0.5,0.3) in oof_grid else roc_auc_score(y[va_idx], list(oof_grid.values())[0][va_idx])\n",
        "    print(f'Fold {fold+1} done in {time.time()-t0:.1f}s | AUC@C=0.5,l1=0.3: {ref_auc:.5f}')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    del Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, Xs_tr, Xs_va, Xs_te, Xm_tr, Xm_va, Xm_te, X_tr, X_va, X_te, clf\n",
        "    gc.collect()\n",
        "\n",
        "# Select best EN config\n",
        "auc_per = {}\n",
        "for key, oof in oof_grid.items():\n",
        "    auc = roc_auc_score(y, oof)\n",
        "    auc_per[key] = auc\n",
        "    print(f'EN C={key[0]} l1={key[1]} OOF AUC: {auc:.5f}')\n",
        "best_key = max(auc_per, key=auc_per.get)\n",
        "print(f'Best EN: C={best_key[0]} l1={best_key[1]} with OOF {auc_per[best_key]:.5f}')\n",
        "\n",
        "# Build EN test preds\n",
        "test_en = np.mean(test_grid[best_key], axis=0).astype(np.float32)\n",
        "oof_en = oof_grid[best_key]\n",
        "\n",
        "# Blend EN with dense XGB OOF to tune weight\n",
        "best_auc, best_w = -1.0, 0.0\n",
        "for w in np.linspace(0.0, 1.0, 201):\n",
        "    blend = (1.0 - w)*oof_en + w*oof_mean\n",
        "    auc = roc_auc_score(y, blend)\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_w = auc, float(w)\n",
        "print(f'Best EN+XGBdense w={best_w:.3f} OOF AUC: {best_auc:.5f}')\n",
        "\n",
        "# Build blended test predictions and save submission\n",
        "test_pred = (1.0 - best_w)*test_en + best_w*test_mean.astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: test_pred})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv; head:')\n",
        "print(sub.head())\n",
        "\n",
        "# Cache EN OOF/test\n",
        "np.save('oof_en_pivot.npy', oof_en)\n",
        "np.save('test_en_pivot.npy', test_en)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 5-fold StratifiedKFold CV (shuffled).\nFold 1/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 done in 496.2s | AUC@C=0.5,l1=0.3: 0.65938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m l1 \u001b[38;5;129;01min\u001b[39;00m l1s:\n\u001b[32m     71\u001b[39m     clf = LogisticRegression(solver=\u001b[33m'\u001b[39m\u001b[33msaga\u001b[39m\u001b[33m'\u001b[39m, penalty=\u001b[33m'\u001b[39m\u001b[33melasticnet\u001b[39m\u001b[33m'\u001b[39m, l1_ratio=l1, C=C, class_weight=\u001b[38;5;28;01mNone\u001b[39;00m, max_iter=\u001b[32m6000\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m, random_state=\u001b[32m42\u001b[39m, verbose=\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtr_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m     va_pred = clf.predict_proba(X_va)[:,\u001b[32m1\u001b[39m].astype(np.float32)\n\u001b[32m     74\u001b[39m     oof_grid[(C,l1)][va_idx] = va_pred\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1473\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1466\u001b[39m     estimator._validate_params()\n\u001b[32m   1468\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1469\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1470\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1471\u001b[39m     )\n\u001b[32m   1472\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1350\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1347\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1348\u001b[39m     n_threads = \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1350\u001b[39m fold_coefs_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1353\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1364\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1368\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m fold_coefs_, _, n_iter_ = \u001b[38;5;28mzip\u001b[39m(*fold_coefs_)\n\u001b[32m   1376\u001b[39m \u001b[38;5;28mself\u001b[39m.n_iter_ = np.asarray(n_iter_, dtype=np.int32)[:, \u001b[32m0\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py:74\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     69\u001b[39m config = get_config()\n\u001b[32m     70\u001b[39m iterable_with_config = (\n\u001b[32m     71\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     73\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py:1952\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1946\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   1947\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   1948\u001b[39m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[32m   1949\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   1950\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1952\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py:1595\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1592\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1594\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1595\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1597\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1598\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1599\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1600\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1601\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py:1707\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1702\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1703\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[32m   1704\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1705\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n\u001b[32m   1706\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n\u001b[32m-> \u001b[39m\u001b[32m1707\u001b[39m     time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1708\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1710\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[32m   1711\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[32m   1712\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "620e0152-33dc-4075-9faf-b06b6815eb16",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Meta-only XGB (GPU, early stopping, seed bag) + 3-way blend with LR(pivot) and SVD+XGB(dense)\n",
        "import time, gc, sys, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    import subprocess, sys as _sys\n",
        "    subprocess.run([_sys.executable, '-m', 'pip', 'install', '-q', 'xgboost'])\n",
        "    import xgboost as xgb\n",
        "\n",
        "assert 'build_meta_enhanced' in globals(), 'Run cell 10 to define build_meta_enhanced'\n",
        "\n",
        "# Load or access OOF/test for existing models\n",
        "def _safe_load(name, fallback=None):\n",
        "    import os\n",
        "    if os.path.exists(name):\n",
        "        return np.load(name)\n",
        "    return fallback\n",
        "\n",
        "oof_lr = globals().get('oof_lr', _safe_load('oof_lr_pivot.npy'))\n",
        "test_lr = globals().get('test_lr', _safe_load('test_lr_pivot.npy'))\n",
        "oof_dense = globals().get('oof_mean', _safe_load('oof_xgb_dense.npy'))\n",
        "test_dense = globals().get('test_mean', _safe_load('test_xgb_dense.npy'))\n",
        "assert oof_lr is not None and oof_dense is not None, 'Missing base OOF preds; run cells 11 and 12'\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "n_splits = 5\n",
        "cv = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42).split(train, y))\n",
        "print(f'Prepared {n_splits}-fold StratifiedKFold CV (shuffled).')\n",
        "\n",
        "# Precompute meta features\n",
        "meta_tr_full = build_meta_enhanced(train).astype(np.float32)\n",
        "meta_te_full = build_meta_enhanced(test).astype(np.float32)\n",
        "\n",
        "seeds = [42, 2025, 7]\n",
        "oof_meta_bag = np.zeros((len(train), len(seeds)), dtype=np.float32)\n",
        "test_meta_bag = np.zeros((len(test), len(seeds)), dtype=np.float32)\n",
        "\n",
        "# XGB params for dense/meta-only view\n",
        "base_params = dict(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='auc',\n",
        "    max_depth=5,\n",
        "    eta=0.04,\n",
        "    subsample=0.7,\n",
        "    colsample_bytree=0.7,\n",
        "    min_child_weight=4,\n",
        "    reg_alpha=1.0,\n",
        "    reg_lambda=2.0,\n",
        "    gamma=0.0,\n",
        "    device='cuda'\n",
        ")\n",
        "\n",
        "for si, seed in enumerate(seeds):\n",
        "    params = dict(base_params); params['seed'] = seed\n",
        "    oof_meta = np.zeros(len(train), dtype=np.float32)\n",
        "    test_fold_preds = []\n",
        "    for fold, (tr_idx, va_idx) in enumerate(cv):\n",
        "        t0 = time.time()\n",
        "        Xtr = meta_tr_full.iloc[tr_idx].values\n",
        "        Xva = meta_tr_full.iloc[va_idx].values\n",
        "        Xte = meta_te_full.values\n",
        "        dtrain = xgb.DMatrix(Xtr, label=y[tr_idx])\n",
        "        dvalid = xgb.DMatrix(Xva, label=y[va_idx])\n",
        "        dtest  = xgb.DMatrix(Xte)\n",
        "        booster = xgb.train(\n",
        "            params, dtrain, num_boost_round=2000, evals=[(dtrain,'train'),(dvalid,'valid')],\n",
        "            early_stopping_rounds=150, verbose_eval=False\n",
        "        )\n",
        "        va_pred = booster.predict(dvalid, iteration_range=(0, booster.best_iteration+1)).astype(np.float32)\n",
        "        te_pred = booster.predict(dtest, iteration_range=(0, booster.best_iteration+1)).astype(np.float32)\n",
        "        oof_meta[va_idx] = va_pred\n",
        "        test_fold_preds.append(te_pred)\n",
        "        auc = roc_auc_score(y[va_idx], va_pred)\n",
        "        print(f'Seed {seed} | Fold {fold+1} AUC: {auc:.5f} | best_iter={booster.best_iteration} | {time.time()-t0:.1f}s')\n",
        "        del Xtr, Xva, Xte, dtrain, dvalid, dtest, booster\n",
        "        gc.collect()\n",
        "    oof_meta_bag[:, si] = oof_meta\n",
        "    test_meta_bag[:, si] = np.mean(test_fold_preds, axis=0).astype(np.float32)\n",
        "    print(f'Seed {seed} OOF AUC (meta-only): {roc_auc_score(y, oof_meta):.5f}')\n",
        "\n",
        "oof_meta = oof_meta_bag.mean(axis=1)\n",
        "test_meta = test_meta_bag.mean(axis=1)\n",
        "print(f'Bagged meta-only XGB OOF AUC: {roc_auc_score(y, oof_meta):.5f}')\n",
        "\n",
        "# 3-way blend on OOF: w1*LR + w2*DenseXGB + w3*MetaXGB, w sum=1\n",
        "best_auc, best_weights = -1.0, (1.0, 0.0, 0.0)\n",
        "grid = np.linspace(0.0, 1.0, 51)\n",
        "for w1 in grid:\n",
        "    for w2 in grid:\n",
        "        if w1 + w2 > 1.0:\n",
        "            continue\n",
        "        w3 = 1.0 - w1 - w2\n",
        "        blend = w1*oof_lr + w2*oof_dense + w3*oof_meta\n",
        "        auc = roc_auc_score(y, blend)\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_weights = auc, (float(w1), float(w2), float(w3))\n",
        "print(f'Best 3-way weights (LR, DenseXGB, MetaXGB)={best_weights} OOF AUC: {best_auc:.5f}')\n",
        "\n",
        "# Build and save submission\n",
        "if test_lr is None:\n",
        "    test_lr = _safe_load('test_lr_pivot.npy')\n",
        "if test_dense is None:\n",
        "    test_dense = _safe_load('test_xgb_dense.npy')\n",
        "assert test_lr is not None and test_dense is not None, 'Missing base test preds; rerun cells 11 and 12'\n",
        "w1, w2, w3 = best_weights\n",
        "test_blend = (w1*test_lr + w2*test_dense + w3*test_meta).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: test_blend})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv; head:')\n",
        "print(sub.head())\n",
        "\n",
        "# Cache meta-only OOF/test\n",
        "np.save('oof_xgb_meta.npy', oof_meta)\n",
        "np.save('test_xgb_meta.npy', test_meta)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 5-fold StratifiedKFold CV (shuffled).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 1 AUC: 0.67961 | best_iter=120 | 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 2 AUC: 0.66006 | best_iter=21 | 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 3 AUC: 0.70261 | best_iter=97 | 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 4 AUC: 0.60509 | best_iter=58 | 0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 5 AUC: 0.68038 | best_iter=35 | 0.5s\nSeed 42 OOF AUC (meta-only): 0.65991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 1 AUC: 0.67210 | best_iter=81 | 0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 2 AUC: 0.65923 | best_iter=139 | 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 3 AUC: 0.69231 | best_iter=94 | 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 4 AUC: 0.60857 | best_iter=23 | 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 5 AUC: 0.68572 | best_iter=39 | 0.5s\nSeed 2025 OOF AUC (meta-only): 0.66264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 1 AUC: 0.67170 | best_iter=176 | 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 2 AUC: 0.66174 | best_iter=93 | 0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 3 AUC: 0.69701 | best_iter=95 | 0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 4 AUC: 0.61009 | best_iter=53 | 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 5 AUC: 0.67805 | best_iter=80 | 0.6s\nSeed 7 OOF AUC (meta-only): 0.66368\nBagged meta-only XGB OOF AUC: 0.66619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best 3-way weights (LR, DenseXGB, MetaXGB)=(0.34, 0.42, 0.23999999999999994) OOF AUC: 0.69182\nSaved submission.csv; head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.330313\n1   t3_roiuw                  0.214858\n2   t3_mjnbq                  0.215433\n3   t3_t8wd1                  0.206821\n4  t3_1m4zxu                  0.225135\n"
          ]
        }
      ]
    },
    {
      "id": "abbf3251-e317-43c7-925c-7b4b412448da",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Local fine-grid 3-way blend around coarse best (w1=0.34, w2=0.42, w3=0.24) with 0.001 step; build submission\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load cached OOF/test preds\n",
        "oof_lr = np.load('oof_lr_pivot.npy')\n",
        "oof_dense = np.load('oof_xgb_dense.npy')\n",
        "oof_meta = np.load('oof_xgb_meta.npy')\n",
        "test_lr = np.load('test_lr_pivot.npy')\n",
        "test_dense = np.load('test_xgb_dense.npy')\n",
        "test_meta = np.load('test_xgb_meta.npy')\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "# Coarse best from prior step\n",
        "w1_c, w2_c = 0.34, 0.42\n",
        "step = 0.001\n",
        "w1_min, w1_max = 0.26, 0.42\n",
        "w2_min, w2_max = 0.34, 0.50\n",
        "\n",
        "best_auc, best_w = -1.0, (w1_c, w2_c, 1.0 - w1_c - w2_c)\n",
        "t0 = time.time()\n",
        "w1_grid = np.arange(w1_min, w1_max + 1e-12, step)\n",
        "w2_grid = np.arange(w2_min, w2_max + 1e-12, step)\n",
        "cnt = 0\n",
        "for w1 in w1_grid:\n",
        "    for w2 in w2_grid:\n",
        "        w3 = 1.0 - w1 - w2\n",
        "        if w3 < 0.0 or w3 > 1.0:\n",
        "            continue\n",
        "        blend = w1*oof_lr + w2*oof_dense + w3*oof_meta\n",
        "        auc = roc_auc_score(y, blend)\n",
        "        cnt += 1\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_w = auc, (float(w1), float(w2), float(w3))\n",
        "print(f'Local fine-grid ({cnt} combos) best weights (LR, DenseXGB, MetaXGB)={best_w} OOF AUC: {best_auc:.5f} | search {time.time()-t0:.1f}s')\n",
        "\n",
        "# Build submission with best weights\n",
        "w1, w2, w3 = best_w\n",
        "test_blend = (w1*test_lr + w2*test_dense + w3*test_meta).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: test_blend})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv; head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local fine-grid (25921 combos) best weights (LR, DenseXGB, MetaXGB)=(0.3420000000000001, 0.4180000000000001, 0.23999999999999982) OOF AUC: 0.69184 | search 51.9s\nSaved submission.csv; head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.330162\n1   t3_roiuw                  0.214765\n2   t3_mjnbq                  0.215370\n3   t3_t8wd1                  0.206919\n4  t3_1m4zxu                  0.225410\n"
          ]
        }
      ]
    },
    {
      "id": "ff30ce46-1e4f-4b44-953b-a799a5c767f7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Rank-averaged 3-way blend (robust for ROC AUC); global 0.01 simplex grid; build submission\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def rank_normalize(arr):\n",
        "    # Tie-aware rank to [0,1]\n",
        "    order = np.argsort(arr)\n",
        "    ranks = np.empty_like(order, dtype=np.float64)\n",
        "    ranks[order] = np.arange(len(arr), dtype=np.float64)\n",
        "    return ranks / (len(arr) - 1.0)\n",
        "\n",
        "# Load cached OOF/test preds\n",
        "oof_lr = np.load('oof_lr_pivot.npy')\n",
        "oof_dense = np.load('oof_xgb_dense.npy')\n",
        "oof_meta = np.load('oof_xgb_meta.npy')\n",
        "test_lr = np.load('test_lr_pivot.npy')\n",
        "test_dense = np.load('test_xgb_dense.npy')\n",
        "test_meta = np.load('test_xgb_meta.npy')\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "# Rank-normalize\n",
        "o1 = rank_normalize(oof_lr); o2 = rank_normalize(oof_dense); o3 = rank_normalize(oof_meta)\n",
        "t1 = rank_normalize(test_lr); t2 = rank_normalize(test_dense); t3 = rank_normalize(test_meta)\n",
        "\n",
        "# Global simplex search with step=0.01\n",
        "best_auc, best_w = -1.0, (1.0, 0.0, 0.0)\n",
        "grid = np.linspace(0.0, 1.0, 101)\n",
        "t0 = time.time()\n",
        "cnt = 0\n",
        "for w1 in grid:\n",
        "    for w2 in grid:\n",
        "        if w1 + w2 > 1.0:\n",
        "            continue\n",
        "        w3 = 1.0 - w1 - w2\n",
        "        blend = w1*o1 + w2*o2 + w3*o3\n",
        "        auc = roc_auc_score(y, blend)\n",
        "        cnt += 1\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_w = auc, (float(w1), float(w2), float(w3))\n",
        "print(f'Rank blend ({cnt} combos) best weights (LR, DenseXGB, MetaXGB)={best_w} OOF AUC: {best_auc:.5f} | search {time.time()-t0:.1f}s')\n",
        "\n",
        "# Build submission with best rank-averaged weights\n",
        "w1, w2, w3 = best_w\n",
        "test_rank_blend = (w1*t1 + w2*t2 + w3*t3).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: test_rank_blend})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv; head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank blend (5151 combos) best weights (LR, DenseXGB, MetaXGB)=(0.43, 0.34, 0.23000000000000004) OOF AUC: 0.69104 | search 10.3s\nSaved submission.csv; head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.799543\n1   t3_roiuw                  0.489905\n2   t3_mjnbq                  0.488717\n3   t3_t8wd1                  0.472972\n4  t3_1m4zxu                  0.532257\n"
          ]
        }
      ]
    },
    {
      "id": "26e8adb8-47d3-401a-b21c-737cb5a954cf",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ultra-local fine-grid refinement around best prob-blend (w1=0.342, w2=0.418); step=0.0002\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load cached OOF/test preds\n",
        "oof_lr = np.load('oof_lr_pivot.npy')\n",
        "oof_dense = np.load('oof_xgb_dense.npy')\n",
        "oof_meta = np.load('oof_xgb_meta.npy')\n",
        "test_lr = np.load('test_lr_pivot.npy')\n",
        "test_dense = np.load('test_xgb_dense.npy')\n",
        "test_meta = np.load('test_xgb_meta.npy')\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "# Center around previous best (0.342, 0.418); +/- 0.01 window with 0.0002 step\n",
        "w1_c, w2_c = 0.342, 0.418\n",
        "step = 0.0002\n",
        "w1_min, w1_max = w1_c - 0.010, w1_c + 0.010\n",
        "w2_min, w2_max = w2_c - 0.010, w2_c + 0.010\n",
        "\n",
        "best_auc, best_w = -1.0, (w1_c, w2_c, 1.0 - w1_c - w2_c)\n",
        "t0 = time.time()\n",
        "w1_grid = np.arange(w1_min, w1_max + 1e-12, step)\n",
        "w2_grid = np.arange(w2_min, w2_max + 1e-12, step)\n",
        "cnt = 0\n",
        "for w1 in w1_grid:\n",
        "    for w2 in w2_grid:\n",
        "        w3 = 1.0 - w1 - w2\n",
        "        if w3 < 0.0 or w3 > 1.0:\n",
        "            continue\n",
        "        blend = w1*oof_lr + w2*oof_dense + w3*oof_meta\n",
        "        auc = roc_auc_score(y, blend)\n",
        "        cnt += 1\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_w = auc, (float(w1), float(w2), float(w3))\n",
        "print(f'Ultra-local fine-grid ({cnt} combos) best weights (LR, DenseXGB, MetaXGB)={best_w} OOF AUC: {best_auc:.5f} | search {time.time()-t0:.1f}s')\n",
        "\n",
        "# Build submission with best weights\n",
        "w1, w2, w3 = best_w\n",
        "test_blend = (w1*test_lr + w2*test_dense + w3*test_meta).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: test_blend})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv; head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultra-local fine-grid (10201 combos) best weights (LR, DenseXGB, MetaXGB)=(0.341399999999999, 0.40919999999999984, 0.24940000000000112) OOF AUC: 0.69185 | search 20.4s\nSaved submission.csv; head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.330208\n1   t3_roiuw                  0.215065\n2   t3_mjnbq                  0.215385\n3   t3_t8wd1                  0.207358\n4  t3_1m4zxu                  0.226728\n"
          ]
        }
      ]
    },
    {
      "id": "2ef0bec1-5fd6-489a-9777-e21bd6fa5a0a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Move 1: Advanced weight optimization using cached strong OOF/test (3-way blend)\n",
        "import numpy as np, pandas as pd, time, sys, gc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "print('Loading cached OOF/test predictions (LR Pivot, Dense XGB 3-seed, Meta XGB 3-seed)...')\n",
        "oof_lr = np.load('oof_lr_pivot.npy')\n",
        "oof_dense = np.load('oof_xgb_dense.npy')\n",
        "oof_meta = np.load('oof_xgb_meta.npy')\n",
        "test_lr = np.load('test_lr_pivot.npy')\n",
        "test_dense = np.load('test_xgb_dense.npy')\n",
        "test_meta = np.load('test_xgb_meta.npy')\n",
        "\n",
        "y = train['requester_received_pizza'].astype(int).values\n",
        "n = len(y)\n",
        "assert oof_lr.shape[0] == n and oof_dense.shape[0] == n and oof_meta.shape[0] == n, 'OOF length mismatch'\n",
        "\n",
        "def eval_auc(w1, w2):\n",
        "    w3 = 1.0 - w1 - w2\n",
        "    if w3 < 0 or w3 > 1:\n",
        "        return -1.0\n",
        "    blend = w1*oof_lr + w2*oof_dense + w3*oof_meta\n",
        "    return roc_auc_score(y, blend)\n",
        "\n",
        "# Start around prior best (from Cell 17): (w1, w2, w3) = (0.3414, 0.4092, 0.2494)\n",
        "w1_c, w2_c = 0.3414, 0.4092\n",
        "\n",
        "# Coarse-to-fine global search\n",
        "best_auc, best_w = -1.0, (w1_c, w2_c, 1.0 - w1_c - w2_c)\n",
        "def grid_search(w1_center, w2_center, window, step):\n",
        "    global best_auc, best_w\n",
        "    w1_min, w1_max = w1_center - window, w1_center + window\n",
        "    w2_min, w2_max = w2_center - window, w2_center + window\n",
        "    w1_grid = np.arange(max(0.0, w1_min), min(1.0, w1_max) + 1e-12, step)\n",
        "    w2_grid = np.arange(max(0.0, w2_min), min(1.0, w2_max) + 1e-12, step)\n",
        "    t0 = time.time()\n",
        "    cnt = 0\n",
        "    for w1 in w1_grid:\n",
        "        for w2 in w2_grid:\n",
        "            if w1 + w2 > 1.0:\n",
        "                continue\n",
        "            auc = eval_auc(w1, w2)\n",
        "            cnt += 1\n",
        "            if auc > best_auc:\n",
        "                best_auc = auc\n",
        "                best_w = (float(w1), float(w2), float(1.0 - w1 - w2))\n",
        "    print(f'Grid window={window} step={step} tried {cnt} combos | best weights={best_w} OOF AUC: {best_auc:.5f} | {time.time()-t0:.1f}s')\n",
        "\n",
        "# Pass 1: broader window, coarse step\n",
        "grid_search(w1_c, w2_c, window=0.03, step=0.002)\n",
        "# Pass 2: refine around current best\n",
        "grid_search(best_w[0], best_w[1], window=0.01, step=0.001)\n",
        "# Pass 3: ultra-fine around current best\n",
        "grid_search(best_w[0], best_w[1], window=0.004, step=0.0005)\n",
        "\n",
        "global_best_auc, global_best_w = best_auc, best_w\n",
        "print(f'Global best after coarse-to-fine: weights={global_best_w} OOF AUC: {global_best_auc:.5f}')\n",
        "\n",
        "# Per-fold weight optimization with averaged weights\n",
        "cv = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(np.arange(n), y))\n",
        "\n",
        "def per_fold_averaged_weights(step=0.02):\n",
        "    w_list = []\n",
        "    oof_blend = np.zeros(n, dtype=np.float32)\n",
        "    for k, (tr_idx, va_idx) in enumerate(cv):\n",
        "        y_tr = y[tr_idx]\n",
        "        # OOF preds restricted to training part\n",
        "        a1, a2, a3 = oof_lr[tr_idx], oof_dense[tr_idx], oof_meta[tr_idx]\n",
        "        best_auc_k, best_w_k = -1.0, (1.0, 0.0, 0.0)\n",
        "        grid = np.linspace(0.0, 1.0, int(1.0/step)+1)\n",
        "        for w1 in grid:\n",
        "            for w2 in grid:\n",
        "                if w1 + w2 > 1.0:\n",
        "                    continue\n",
        "                w3 = 1.0 - w1 - w2\n",
        "                blend_tr = w1*a1 + w2*a2 + w3*a3\n",
        "                auc_k = roc_auc_score(y_tr, blend_tr)\n",
        "                if auc_k > best_auc_k:\n",
        "                    best_auc_k = auc_k\n",
        "                    best_w_k = (float(w1), float(w2), float(w3))\n",
        "        w_list.append(best_w_k)\n",
        "        # Apply to validation fold\n",
        "        b1, b2, b3 = oof_lr[va_idx], oof_dense[va_idx], oof_meta[va_idx]\n",
        "        w1k, w2k, w3k = best_w_k\n",
        "        oof_blend[va_idx] = (w1k*b1 + w2k*b2 + w3k*b3).astype(np.float32)\n",
        "        print(f'Fold {k+1} best fold-weights={best_w_k} (train-fold AUC={best_auc_k:.5f})')\n",
        "    # Average weights\n",
        "    w_arr = np.array(w_list)\n",
        "    w_avg = w_arr.mean(axis=0)\n",
        "    # Renormalize to simplex in case of tiny drift\n",
        "    s = w_avg.sum()\n",
        "    if s <= 0:\n",
        "        w_avg = np.array([1.0, 0.0, 0.0], dtype=np.float64)\n",
        "    else:\n",
        "        w_avg = w_avg / s\n",
        "    auc_oof_avg = roc_auc_score(y, oof_blend)\n",
        "    print(f'Per-fold averaged weights={tuple(w_avg.tolist())} | blended OOF (using per-fold weights): {auc_oof_avg:.5f}')\n",
        "    # Also evaluate using the averaged weights applied globally (for stability)\n",
        "    w1a, w2a, w3a = w_avg.tolist()\n",
        "    auc_global_avg = eval_auc(w1a, w2a)\n",
        "    print(f'Applying averaged weights globally gives OOF AUC: {auc_global_avg:.5f}')\n",
        "    return tuple(w_avg.tolist()), auc_oof_avg, auc_global_avg\n",
        "\n",
        "w_avg, auc_pf_oof, auc_pf_global = per_fold_averaged_weights(step=0.02)\n",
        "\n",
        "# Choose the best approach\n",
        "cand = [\n",
        "    ('global', global_best_auc, global_best_w),\n",
        "    ('per_fold_oof', auc_pf_oof, w_avg),\n",
        "    ('per_fold_global', auc_pf_global, w_avg)\n",
        "]\n",
        "cand.sort(key=lambda x: x[1], reverse=True)\n",
        "choice_name, choice_auc, choice_w = cand[0]\n",
        "print(f'Chosen blend: {choice_name} with weights={choice_w} OOF AUC: {choice_auc:.5f}')\n",
        "\n",
        "# Build submission with chosen weights\n",
        "w1, w2, w3 = choice_w\n",
        "test_blend = (w1*test_lr + w2*test_dense + w3*test_meta).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: test_blend})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv; head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached OOF/test predictions (LR Pivot, Dense XGB 3-seed, Meta XGB 3-seed)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid window=0.03 step=0.002 tried 961 combos | best weights=(0.34140000000000004, 0.4092, 0.24939999999999996) OOF AUC: 0.69185 | 1.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid window=0.01 step=0.001 tried 441 combos | best weights=(0.34140000000000004, 0.4092, 0.24939999999999996) OOF AUC: 0.69185 | 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid window=0.004 step=0.0005 tried 289 combos | best weights=(0.34140000000000004, 0.4092, 0.24939999999999996) OOF AUC: 0.69185 | 0.6s\nGlobal best after coarse-to-fine: weights=(0.34140000000000004, 0.4092, 0.24939999999999996) OOF AUC: 0.69185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 best fold-weights=(0.34, 0.46, 0.1999999999999999) (train-fold AUC=0.69164)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 best fold-weights=(0.34, 0.46, 0.1999999999999999) (train-fold AUC=0.69532)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 best fold-weights=(0.36, 0.42, 0.22000000000000003) (train-fold AUC=0.68401)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 best fold-weights=(0.28, 0.46, 0.25999999999999995) (train-fold AUC=0.70388)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 best fold-weights=(0.4, 0.34, 0.25999999999999995) (train-fold AUC=0.68499)\nPer-fold averaged weights=(0.34400000000000003, 0.42800000000000005, 0.22799999999999998) | blended OOF (using per-fold weights): 0.69104\nApplying averaged weights globally gives OOF AUC: 0.69176\nChosen blend: global with weights=(0.34140000000000004, 0.4092, 0.24939999999999996) OOF AUC: 0.69185\nSaved submission.csv; head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.330208\n1   t3_roiuw                  0.215065\n2   t3_mjnbq                  0.215385\n3   t3_t8wd1                  0.207358\n4  t3_1m4zxu                  0.226728\n"
          ]
        }
      ]
    },
    {
      "id": "f465a520-8a90-4dcc-b727-72fb04515456",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Move 2: Simple level-2 stacking (LogisticRegression) on 3 base OOF preds; fold-safe; bag test\n",
        "import numpy as np, pandas as pd, time, sys, gc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "print('Loading cached base OOF/test predictions...')\n",
        "oof_lr = np.load('oof_lr_pivot.npy')\n",
        "oof_dense = np.load('oof_xgb_dense.npy')\n",
        "oof_meta = np.load('oof_xgb_meta.npy')\n",
        "test_lr = np.load('test_lr_pivot.npy')\n",
        "test_dense = np.load('test_xgb_dense.npy')\n",
        "test_meta = np.load('test_xgb_meta.npy')\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "n = len(y)\n",
        "X_base = np.vstack([oof_lr, oof_dense, oof_meta]).T.astype(np.float32)\n",
        "X_test = np.vstack([test_lr, test_dense, test_meta]).T.astype(np.float32)\n",
        "assert X_base.shape == (n, 3) and X_test.shape[1] == 3, 'Shape mismatch in base features'\n",
        "\n",
        "cv = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(np.arange(n), y))\n",
        "oof_meta_lr = np.zeros(n, dtype=np.float32)\n",
        "test_bag = []\n",
        "\n",
        "t0 = time.time()\n",
        "for k, (tr_idx, va_idx) in enumerate(cv):\n",
        "    Xtr, ytr = X_base[tr_idx], y[tr_idx]\n",
        "    Xva, yva = X_base[va_idx], y[va_idx]\n",
        "    clf = LogisticRegression(solver='lbfgs', C=1.0, penalty='l2', max_iter=1000, n_jobs=-1, random_state=42)\n",
        "    clf.fit(Xtr, ytr)\n",
        "    va_pred = clf.predict_proba(Xva)[:,1].astype(np.float32)\n",
        "    oof_meta_lr[va_idx] = va_pred\n",
        "    test_pred = clf.predict_proba(X_test)[:,1].astype(np.float32)\n",
        "    test_bag.append(test_pred)\n",
        "    fold_auc = roc_auc_score(yva, va_pred)\n",
        "    print(f'Fold {k+1}/5 meta AUC: {fold_auc:.5f}')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "auc_oof = roc_auc_score(y, oof_meta_lr)\n",
        "print(f'Stacker OOF AUC (LR on 3 base preds): {auc_oof:.5f} | time {time.time()-t0:.1f}s')\n",
        "\n",
        "# Build test via averaging fold meta-model predictions\n",
        "test_stack = np.mean(test_bag, axis=0).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: test_stack})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv; head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached base OOF/test predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5 meta AUC: 0.69457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 meta AUC: 0.67785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 meta AUC: 0.72156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 meta AUC: 0.64475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 meta AUC: 0.72157\n"
          ]
        }
      ]
    },
    {
      "id": "bf573af8-1a9f-468f-ac49-e39a1dd5f2ee",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional Move: Per-model Platt calibration (fold-safe) + refined weight search using cached OOF/test\n",
        "import numpy as np, pandas as pd, time, sys, gc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "print('Loading cached base OOF/test predictions...')\n",
        "o1 = np.load('oof_lr_pivot.npy')\n",
        "o2 = np.load('oof_xgb_dense.npy')\n",
        "o3 = np.load('oof_xgb_meta.npy')\n",
        "t1 = np.load('test_lr_pivot.npy')\n",
        "t2 = np.load('test_xgb_dense.npy')\n",
        "t3 = np.load('test_xgb_meta.npy')\n",
        "y = train[target_col].astype(int).values\n",
        "n = len(y)\n",
        "\n",
        "def platt_calibrate_foldwise(oof_pred, test_pred, y, seed=42):\n",
        "    cv = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=seed).split(np.arange(n), y))\n",
        "    oof_cal = np.zeros(n, dtype=np.float32)\n",
        "    test_cal_folds = []\n",
        "    for k, (tr_idx, va_idx) in enumerate(cv):\n",
        "        Xtr = oof_pred[tr_idx].reshape(-1,1).astype(np.float32)\n",
        "        ytr = y[tr_idx]\n",
        "        Xva = oof_pred[va_idx].reshape(-1,1).astype(np.float32)\n",
        "        Xt  = test_pred.reshape(-1,1).astype(np.float32)\n",
        "        clf = LogisticRegression(solver='lbfgs', C=1.0, penalty='l2', max_iter=1000, n_jobs=-1, random_state=seed)\n",
        "        clf.fit(Xtr, ytr)\n",
        "        oof_cal[va_idx] = clf.predict_proba(Xva)[:,1].astype(np.float32)\n",
        "        test_cal_folds.append(clf.predict_proba(Xt)[:,1].astype(np.float32))\n",
        "    test_cal = np.mean(test_cal_folds, axis=0).astype(np.float32)\n",
        "    return oof_cal, test_cal\n",
        "\n",
        "t0 = time.time()\n",
        "o1c, t1c = platt_calibrate_foldwise(o1, t1, y, seed=42)\n",
        "o2c, t2c = platt_calibrate_foldwise(o2, t2, y, seed=42)\n",
        "o3c, t3c = platt_calibrate_foldwise(o3, t3, y, seed=42)\n",
        "print(f'Calibrated single-model OOF AUCs | LR:{roc_auc_score(y,o1c):.5f} Dense:{roc_auc_score(y,o2c):.5f} Meta:{roc_auc_score(y,o3c):.5f} | {time.time()-t0:.1f}s')\n",
        "\n",
        "def eval_auc(w1, w2, a1, a2, a3):\n",
        "    w3 = 1.0 - w1 - w2\n",
        "    if w3 < 0 or w3 > 1:\n",
        "        return -1.0\n",
        "    blend = w1*a1 + w2*a2 + w3*a3\n",
        "    return roc_auc_score(y, blend)\n",
        "\n",
        "# Start from prior best weights\n",
        "w1_c, w2_c = 0.3414, 0.4092\n",
        "best_auc, best_w = -1.0, (w1_c, w2_c, 1.0 - w1_c - w2_c)\n",
        "\n",
        "def grid_search(a1, a2, a3, w1_center, w2_center, window, step):\n",
        "    global best_auc, best_w\n",
        "    w1_min, w1_max = w1_center - window, w1_center + window\n",
        "    w2_min, w2_max = w2_center - window, w2_center + window\n",
        "    w1_grid = np.arange(max(0.0, w1_min), min(1.0, w1_max) + 1e-12, step)\n",
        "    w2_grid = np.arange(max(0.0, w2_min), min(1.0, w2_max) + 1e-12, step)\n",
        "    t0 = time.time(); cnt = 0\n",
        "    local_best_auc, local_best_w = -1.0, None\n",
        "    for w1 in w1_grid:\n",
        "        for w2 in w2_grid:\n",
        "            if w1 + w2 > 1.0:\n",
        "                continue\n",
        "            auc = eval_auc(w1, w2, a1, a2, a3)\n",
        "            cnt += 1\n",
        "            if auc > local_best_auc:\n",
        "                local_best_auc = auc\n",
        "                local_best_w = (float(w1), float(w2), float(1.0 - w1 - w2))\n",
        "    print(f'Calib grid window={window} step={step} tried {cnt} | best={local_best_w} OOF AUC: {local_best_auc:.5f} | {time.time()-t0:.1f}s')\n",
        "    return local_best_auc, local_best_w\n",
        "\n",
        "# Run coarse-to-fine on calibrated OOF\n",
        "auc1, w1 = grid_search(o1c, o2c, o3c, w1_c, w2_c, window=0.03, step=0.002)\n",
        "auc2, w2 = grid_search(o1c, o2c, o3c, w1[0], w1[1], window=0.01, step=0.001)\n",
        "auc3, w3 = grid_search(o1c, o2c, o3c, w2[0], w2[1], window=0.004, step=0.0005)\n",
        "final_auc, final_w = auc3, w3\n",
        "print(f'Final calibrated best weights={final_w} OOF AUC: {final_auc:.5f}')\n",
        "\n",
        "# Build calibrated-blend submission\n",
        "w1b, w2b, w3b = final_w\n",
        "test_blend = (w1b*t1c + w2b*t2c + w3b*t3c).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: test_blend})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv; head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached base OOF/test predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calibrated single-model OOF AUCs | LR:0.67810 Dense:0.67329 Meta:0.66537 | 13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calib grid window=0.03 step=0.002 tried 961 | best=(0.36740000000000006, 0.3832, 0.24939999999999996) OOF AUC: 0.69032 | 2.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calib grid window=0.01 step=0.001 tried 441 | best=(0.37640000000000007, 0.3732, 0.25039999999999996) OOF AUC: 0.69035 | 0.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calib grid window=0.004 step=0.0005 tried 289 | best=(0.37590000000000007, 0.3737, 0.2503999999999999) OOF AUC: 0.69035 | 0.6s\nFinal calibrated best weights=(0.37590000000000007, 0.3737, 0.2503999999999999) OOF AUC: 0.69035\nSaved submission.csv; head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.325981\n1   t3_roiuw                  0.219363\n2   t3_mjnbq                  0.218908\n3   t3_t8wd1                  0.212368\n4  t3_1m4zxu                  0.231804\n"
          ]
        }
      ]
    },
    {
      "id": "309ff221-d4cf-4033-82cb-49b66a5d7c49",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Restore best 3-way prob-blend submission using cached strong OOF/test and best weights from Cell 18/17\n",
        "import numpy as np, pandas as pd\n",
        "best_w = (0.3414, 0.4092, 0.2494)  # (LR, DenseXGB 3-seed, MetaXGB 3-seed)\n",
        "oof_lr = np.load('oof_lr_pivot.npy')  # not used here, but sanity if needed\n",
        "oof_dense = np.load('oof_xgb_dense.npy')\n",
        "oof_meta = np.load('oof_xgb_meta.npy')\n",
        "test_lr = np.load('test_lr_pivot.npy')\n",
        "test_dense = np.load('test_xgb_dense.npy')\n",
        "test_meta = np.load('test_xgb_meta.npy')\n",
        "w1, w2, w3 = best_w\n",
        "test_blend = (w1*test_lr + w2*test_dense + w3*test_meta).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: test_blend})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Rewrote submission.csv with best prob-blend weights', best_w)\n",
        "print(sub.head())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rewrote submission.csv with best prob-blend weights (0.3414, 0.4092, 0.2494)\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.330208\n1   t3_roiuw                  0.215065\n2   t3_mjnbq                  0.215385\n3   t3_t8wd1                  0.207358\n4  t3_1m4zxu                  0.226728\n"
          ]
        }
      ]
    },
    {
      "id": "358d713b-3a3f-4c30-a509-820c7cf29337",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Move 2b: Fast level-2 stacking with Ridge regression on 3 base OOF preds (fold-safe); build submission only if improves\n",
        "import numpy as np, pandas as pd, time, sys\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "print('Loading cached base OOF/test predictions...')\n",
        "oof_lr = np.load('oof_lr_pivot.npy')\n",
        "oof_dense = np.load('oof_xgb_dense.npy')\n",
        "oof_meta = np.load('oof_xgb_meta.npy')\n",
        "test_lr = np.load('test_lr_pivot.npy')\n",
        "test_dense = np.load('test_xgb_dense.npy')\n",
        "test_meta = np.load('test_xgb_meta.npy')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "X_base = np.vstack([oof_lr, oof_dense, oof_meta]).T.astype(np.float32)\n",
        "X_test = np.vstack([test_lr, test_dense, test_meta]).T.astype(np.float32)\n",
        "assert X_base.shape[1] == 3 and X_test.shape[1] == 3\n",
        "\n",
        "cv = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(np.arange(len(y)), y))\n",
        "oof_stack = np.zeros(len(y), dtype=np.float32)\n",
        "test_bag = []\n",
        "\n",
        "alphas = [0.5, 1.0, 2.0, 5.0]\n",
        "t0 = time.time()\n",
        "for k, (tr_idx, va_idx) in enumerate(cv):\n",
        "    Xtr, ytr = X_base[tr_idx], y[tr_idx]\n",
        "    Xva, yva = X_base[va_idx], y[va_idx]\n",
        "    best_auc_k, best_pred_va, best_pred_te = -1.0, None, None\n",
        "    for a in alphas:\n",
        "        mdl = Ridge(alpha=a, random_state=42)\n",
        "        mdl.fit(Xtr, ytr)\n",
        "        va_pred = mdl.predict(Xva).astype(np.float32)\n",
        "        # Ridge outputs unbounded; for AUC ranking is fine. Clip to [0,1] for submission stability.\n",
        "        auc_k = roc_auc_score(yva, va_pred)\n",
        "        if auc_k > best_auc_k:\n",
        "            best_auc_k = auc_k\n",
        "            best_pred_va = va_pred\n",
        "            best_pred_te = mdl.predict(X_test).astype(np.float32)\n",
        "    oof_stack[va_idx] = best_pred_va\n",
        "    test_bag.append(best_pred_te)\n",
        "    print(f'Fold {k+1}/5 ridge-stack best AUC: {best_auc_k:.5f}')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "auc_oof = roc_auc_score(y, oof_stack)\n",
        "print(f'Ridge stacker OOF AUC: {auc_oof:.5f} | time {time.time()-t0:.1f}s')\n",
        "\n",
        "test_stack = np.mean(test_bag, axis=0).astype(np.float32)\n",
        "# Only write submission if this beats our best OOF 0.69185\n",
        "if auc_oof >= 0.69185:\n",
        "    # Map to [0,1] via rank normalization to avoid scale issues while preserving order\n",
        "    order = np.argsort(test_stack)\n",
        "    ranks = np.empty_like(order, dtype=np.float64); ranks[order] = np.arange(len(test_stack), dtype=np.float64)\n",
        "    test_scores = (ranks / (len(test_stack) - 1.0)).astype(np.float32)\n",
        "    sub = pd.DataFrame({id_col: test[id_col].values, target_col: test_scores})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv from ridge stacker; head:')\n",
        "    print(sub.head())\n",
        "else:\n",
        "    print('Ridge stacker did not beat best OOF 0.69185; keeping existing submission.csv')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached base OOF/test predictions...\nFold 1/5 ridge-stack best AUC: 0.69471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 ridge-stack best AUC: 0.67771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 ridge-stack best AUC: 0.72172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 ridge-stack best AUC: 0.64525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 ridge-stack best AUC: 0.72245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge stacker OOF AUC: 0.69059 | time 0.1s\nRidge stacker did not beat best OOF 0.69185; keeping existing submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "aafff151-f028-4821-9eea-b79443caf972",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Move 1c: Logit-space blending (coarse-to-fine) using cached OOF/test; may slightly improve AUC synergy\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "print('Loading cached OOF/test predictions...')\n",
        "o1 = np.load('oof_lr_pivot.npy')\n",
        "o2 = np.load('oof_xgb_dense.npy')\n",
        "o3 = np.load('oof_xgb_meta.npy')\n",
        "t1 = np.load('test_lr_pivot.npy')\n",
        "t2 = np.load('test_xgb_dense.npy')\n",
        "t3 = np.load('test_xgb_meta.npy')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "# Logit-transform\n",
        "z1, z2, z3 = to_logit(o1), to_logit(o2), to_logit(o3)\n",
        "tz1, tz2, tz3 = to_logit(t1), to_logit(t2), to_logit(t3)\n",
        "\n",
        "def eval_auc_logit(w1, w2):\n",
        "    w3 = 1.0 - w1 - w2\n",
        "    if w3 < 0 or w3 > 1:\n",
        "        return -1.0\n",
        "    z = w1*z1 + w2*z2 + w3*z3\n",
        "    # AUC is rank-based; z is monotonic with probs, so use z directly\n",
        "    return roc_auc_score(y, z)\n",
        "\n",
        "# Start at prior prob-best\n",
        "w1_c, w2_c = 0.3414, 0.4092\n",
        "best_auc, best_w = -1.0, (w1_c, w2_c, 1.0 - w1_c - w2_c)\n",
        "\n",
        "def grid_search_logit(w1_center, w2_center, window, step):\n",
        "    global best_auc, best_w\n",
        "    w1_min, w1_max = max(0.0, w1_center - window), min(1.0, w1_center + window)\n",
        "    w2_min, w2_max = max(0.0, w2_center - window), min(1.0, w2_center + window)\n",
        "    w1_grid = np.arange(w1_min, w1_max + 1e-12, step)\n",
        "    w2_grid = np.arange(w2_min, w2_max + 1e-12, step)\n",
        "    t0 = time.time(); cnt = 0\n",
        "    for w1 in w1_grid:\n",
        "        for w2 in w2_grid:\n",
        "            if w1 + w2 > 1.0:\n",
        "                continue\n",
        "            auc = eval_auc_logit(w1, w2)\n",
        "            cnt += 1\n",
        "            if auc > best_auc:\n",
        "                best_auc = auc\n",
        "                best_w = (float(w1), float(w2), float(1.0 - w1 - w2))\n",
        "    print(f'Logit grid window={window} step={step} tried {cnt} | best={best_w} OOF AUC(z): {best_auc:.5f}')\n",
        "\n",
        "# Coarse-to-fine\n",
        "grid_search_logit(w1_c, w2_c, window=0.05, step=0.005)\n",
        "grid_search_logit(best_w[0], best_w[1], window=0.02, step=0.001)\n",
        "grid_search_logit(best_w[0], best_w[1], window=0.008, step=0.0005)\n",
        "print(f'Final logit-blend weights={best_w} | OOF AUC(z): {best_auc:.5f}')\n",
        "\n",
        "# Build test with logit blending (convert back to probs for submission)\n",
        "w1b, w2b, w3b = best_w\n",
        "zt = w1b*tz1 + w2b*tz2 + w3b*tz3\n",
        "pt = sigmoid(zt).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (logit blend); head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached OOF/test predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logit grid window=0.05 step=0.005 tried 441 | best=(0.35140000000000005, 0.41420000000000007, 0.2343999999999999) OOF AUC(z): 0.69199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logit grid window=0.02 step=0.001 tried 1681 | best=(0.35040000000000004, 0.4192000000000001, 0.23039999999999988) OOF AUC(z): 0.69201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logit grid window=0.008 step=0.0005 tried 1089 | best=(0.35040000000000004, 0.4192000000000001, 0.23039999999999988) OOF AUC(z): 0.69201\nFinal logit-blend weights=(0.35040000000000004, 0.4192000000000001, 0.23039999999999988) | OOF AUC(z): 0.69201\nSaved submission.csv (logit blend); head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.328460\n1   t3_roiuw                  0.212609\n2   t3_mjnbq                  0.214714\n3   t3_t8wd1                  0.205759\n4  t3_1m4zxu                  0.216342\n"
          ]
        }
      ]
    },
    {
      "id": "2e14ccf2-c0f1-448b-aabb-7fc2c8005a29",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ultra-fine logit blend refinement around (0.3504, 0.4192)\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "o1 = np.load('oof_lr_pivot.npy'); o2 = np.load('oof_xgb_dense.npy'); o3 = np.load('oof_xgb_meta.npy')\n",
        "t1 = np.load('test_lr_pivot.npy'); t2 = np.load('test_xgb_dense.npy'); t3 = np.load('test_xgb_meta.npy')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "z1, z2, z3 = to_logit(o1), to_logit(o2), to_logit(o3)\n",
        "tz1, tz2, tz3 = to_logit(t1), to_logit(t2), to_logit(t3)\n",
        "\n",
        "def eval_auc_logit(w1, w2):\n",
        "    w3 = 1.0 - w1 - w2\n",
        "    if w3 < 0 or w3 > 1:\n",
        "        return -1.0\n",
        "    z = w1*z1 + w2*z2 + w3*z3\n",
        "    return roc_auc_score(y, z)\n",
        "\n",
        "w1_c, w2_c = 0.3504, 0.4192\n",
        "best_auc, best_w = -1.0, (w1_c, w2_c, 1.0 - w1_c - w2_c)\n",
        "\n",
        "def refine(center_w1, center_w2, window, step):\n",
        "    global best_auc, best_w\n",
        "    w1_min, w1_max = max(0.0, center_w1 - window), min(1.0, center_w1 + window)\n",
        "    w2_min, w2_max = max(0.0, center_w2 - window), min(1.0, center_w2 + window)\n",
        "    w1_grid = np.arange(w1_min, w1_max + 1e-12, step)\n",
        "    w2_grid = np.arange(w2_min, w2_max + 1e-12, step)\n",
        "    t0 = time.time(); cnt = 0\n",
        "    for w1 in w1_grid:\n",
        "        for w2 in w2_grid:\n",
        "            if w1 + w2 > 1.0:\n",
        "                continue\n",
        "            auc = eval_auc_logit(w1, w2)\n",
        "            cnt += 1\n",
        "            if auc > best_auc:\n",
        "                best_auc = auc\n",
        "                best_w = (float(w1), float(w2), float(1.0 - w1 - w2))\n",
        "    print(f'Refine window={window} step={step} tried {cnt} | best={best_w} OOF AUC(z): {best_auc:.5f} | {time.time()-t0:.1f}s')\n",
        "\n",
        "# Two refinement passes\n",
        "refine(w1_c, w2_c, window=0.006, step=0.0002)\n",
        "refine(best_w[0], best_w[1], window=0.003, step=0.0001)\n",
        "print(f'Ultra-fine final logit weights={best_w} | OOF AUC(z): {best_auc:.5f}')\n",
        "\n",
        "# Build submission with refined logit blend\n",
        "w1b, w2b, w3b = best_w\n",
        "zt = w1b*tz1 + w2b*tz2 + w3b*tz3\n",
        "pt = sigmoid(zt).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (ultra-fine logit blend); head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refine window=0.006 step=0.0002 tried 3721 | best=(0.35019999999999935, 0.41999999999999926, 0.22980000000000134) OOF AUC(z): 0.69201 | 7.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refine window=0.003 step=0.0001 tried 3721 | best=(0.350099999999999, 0.41979999999999895, 0.23010000000000208) OOF AUC(z): 0.69201 | 7.5s\nUltra-fine final logit weights=(0.350099999999999, 0.41979999999999895, 0.23010000000000208) | OOF AUC(z): 0.69201\nSaved submission.csv (ultra-fine logit blend); head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.328483\n1   t3_roiuw                  0.212616\n2   t3_mjnbq                  0.214724\n3   t3_t8wd1                  0.205729\n4  t3_1m4zxu                  0.216253\n"
          ]
        }
      ]
    },
    {
      "id": "f0ccbf85-4865-4926-92e9-5896b3562989",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Move 1d: Logit scaling (per-model temperature) + weight search to squeeze +0.0001\u20130.0003\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "print('Loading cached OOF/test predictions...')\n",
        "o1 = np.load('oof_lr_pivot.npy')\n",
        "o2 = np.load('oof_xgb_dense.npy')\n",
        "o3 = np.load('oof_xgb_meta.npy')\n",
        "t1 = np.load('test_lr_pivot.npy')\n",
        "t2 = np.load('test_xgb_dense.npy')\n",
        "t3 = np.load('test_xgb_meta.npy')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "# Base logits\n",
        "z1_base, z2_base, z3_base = to_logit(o1), to_logit(o2), to_logit(o3)\n",
        "tz1_base, tz2_base, tz3_base = to_logit(t1), to_logit(t2), to_logit(t3)\n",
        "\n",
        "def eval_auc_logit_combo(s1, s2, s3, w1c, w2c):\n",
        "    z1 = s1 * z1_base; z2 = s2 * z2_base; z3 = s3 * z3_base\n",
        "    def auc_for_weights(w1, w2):\n",
        "        w3 = 1.0 - w1 - w2\n",
        "        if w3 < 0 or w3 > 1:\n",
        "            return -1.0\n",
        "        z = w1*z1 + w2*z2 + w3*z3\n",
        "        return roc_auc_score(y, z)\n",
        "    # Coarse-to-fine around provided center weights\n",
        "    best_auc, best_w = -1.0, (w1c, w2c, 1.0 - w1c - w2c)\n",
        "    def grid_pass(center, window, step):\n",
        "        nonlocal best_auc, best_w\n",
        "        cw1, cw2 = center\n",
        "        w1_min, w1_max = max(0.0, cw1 - window), min(1.0, cw1 + window)\n",
        "        w2_min, w2_max = max(0.0, cw2 - window), min(1.0, cw2 + window)\n",
        "        w1_grid = np.arange(w1_min, w1_max + 1e-12, step)\n",
        "        w2_grid = np.arange(w2_min, w2_max + 1e-12, step)\n",
        "        for w1 in w1_grid:\n",
        "            for w2 in w2_grid:\n",
        "                if w1 + w2 > 1.0: continue\n",
        "                auc = auc_for_weights(w1, w2)\n",
        "                if auc > best_auc:\n",
        "                    best_auc = auc; best_w = (float(w1), float(w2), float(1.0 - w1 - w2))\n",
        "        return (best_w[0], best_w[1])\n",
        "    c = (w1c, w2c)\n",
        "    c = grid_pass(c, window=0.02, step=0.001)\n",
        "    c = grid_pass(c, window=0.008, step=0.0005)\n",
        "    c = grid_pass(c, window=0.004, step=0.0002)\n",
        "    return best_auc, best_w\n",
        "\n",
        "# Center from best logit blend found earlier\n",
        "w1_center, w2_center = 0.3504, 0.4192\n",
        "\n",
        "# Search small temperature scales per model\n",
        "scales = [0.85, 0.9, 0.95, 1.0, 1.05, 1.1, 1.15]\n",
        "best_global_auc, best_cfg = -1.0, None\n",
        "t0 = time.time(); combos = 0\n",
        "for s1 in scales:\n",
        "    for s2 in scales:\n",
        "        for s3 in scales:\n",
        "            auc, w = eval_auc_logit_combo(s1, s2, s3, w1_center, w2_center)\n",
        "            combos += 1\n",
        "            if auc > best_global_auc:\n",
        "                best_global_auc, best_cfg = auc, (s1, s2, s3, w)\n",
        "print(f'Tried {combos} temperature triples; best AUC(z): {best_global_auc:.5f} with scales={best_cfg[:3]} weights={best_cfg[3]} | {time.time()-t0:.1f}s')\n",
        "\n",
        "# If improvement, build submission with best scales+weights\n",
        "s1, s2, s3, (bw1, bw2, bw3) = best_cfg\n",
        "tz1 = s1 * tz1_base; tz2 = s2 * tz2_base; tz3 = s3 * tz3_base\n",
        "zt = bw1*tz1 + bw2*tz2 + bw3*tz3\n",
        "pt = sigmoid(zt).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (logit scale+blend); head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached OOF/test predictions...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s2 \u001b[38;5;129;01min\u001b[39;00m scales:\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m s3 \u001b[38;5;129;01min\u001b[39;00m scales:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m         auc, w = \u001b[43meval_auc_logit_combo\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw1_center\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw2_center\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m         combos += \u001b[32m1\u001b[39m\n\u001b[32m     67\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m auc > best_global_auc:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36meval_auc_logit_combo\u001b[39m\u001b[34m(s1, s2, s3, w1c, w2c)\u001b[39m\n\u001b[32m     50\u001b[39m c = grid_pass(c, window=\u001b[32m0.02\u001b[39m, step=\u001b[32m0.001\u001b[39m)\n\u001b[32m     51\u001b[39m c = grid_pass(c, window=\u001b[32m0.008\u001b[39m, step=\u001b[32m0.0005\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m c = \u001b[43mgrid_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.004\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0002\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m best_auc, best_w\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36meval_auc_logit_combo.<locals>.grid_pass\u001b[39m\u001b[34m(center, window, step)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w2 \u001b[38;5;129;01min\u001b[39;00m w2_grid:\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m w1 + w2 > \u001b[32m1.0\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     auc = \u001b[43mauc_for_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m auc > best_auc:\n\u001b[32m     47\u001b[39m         best_auc = auc; best_w = (\u001b[38;5;28mfloat\u001b[39m(w1), \u001b[38;5;28mfloat\u001b[39m(w2), \u001b[38;5;28mfloat\u001b[39m(\u001b[32m1.0\u001b[39m - w1 - w2))\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36meval_auc_logit_combo.<locals>.auc_for_weights\u001b[39m\u001b[34m(w1, w2)\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[32m1.0\u001b[39m\n\u001b[32m     31\u001b[39m z = w1*z1 + w2*z2 + w3*z3\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py:213\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    209\u001b[39m         skip_parameter_validation=(\n\u001b[32m    210\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    211\u001b[39m         )\n\u001b[32m    212\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    215\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    216\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    217\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    219\u001b[39m     msg = re.sub(\n\u001b[32m    220\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    221\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    222\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    223\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:638\u001b[39m, in \u001b[36mroc_auc_score\u001b[39m\u001b[34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[39m\n\u001b[32m    634\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _multiclass_roc_auc_score(\n\u001b[32m    635\u001b[39m         y_true, y_score, labels, multi_class, average, sample_weight\n\u001b[32m    636\u001b[39m     )\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m y_type == \u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m     labels = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    639\u001b[39m     y_true = label_binarize(y_true, classes=labels)[:, \u001b[32m0\u001b[39m]\n\u001b[32m    640\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _average_binary_score(\n\u001b[32m    641\u001b[39m         partial(_binary_roc_auc_score, max_fpr=max_fpr),\n\u001b[32m    642\u001b[39m         y_true,\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         sample_weight=sample_weight,\n\u001b[32m    646\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/numpy/lib/arraysetops.py:274\u001b[39m, in \u001b[36munique\u001b[39m\u001b[34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[39m\n\u001b[32m    272\u001b[39m ar = np.asanyarray(ar)\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     ret = \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/numpy/lib/arraysetops.py:328\u001b[39m, in \u001b[36m_unique1d\u001b[39m\u001b[34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_unique1d\u001b[39m(ar, return_index=\u001b[38;5;28;01mFalse\u001b[39;00m, return_inverse=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    324\u001b[39m               return_counts=\u001b[38;5;28;01mFalse\u001b[39;00m, *, equal_nan=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    325\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    326\u001b[39m \u001b[33;03m    Find the unique elements of an array, ignoring shape.\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     ar = np.asanyarray(ar).flatten()\n\u001b[32m    330\u001b[39m     optional_indices = return_index \u001b[38;5;129;01mor\u001b[39;00m return_inverse\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m optional_indices:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "cdd2729e-61cf-46e3-841c-b65653dd3044",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Move 2c: Simple level-2 stacking with LogisticRegression (liblinear) on 3 base OOF preds; fold-safe; bag test\n",
        "import numpy as np, pandas as pd, time, sys\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "print('Loading cached base OOF/test predictions...')\n",
        "oof_lr = np.load('oof_lr_pivot.npy')\n",
        "oof_dense = np.load('oof_xgb_dense.npy')\n",
        "oof_meta = np.load('oof_xgb_meta.npy')\n",
        "test_lr = np.load('test_lr_pivot.npy')\n",
        "test_dense = np.load('test_xgb_dense.npy')\n",
        "test_meta = np.load('test_xgb_meta.npy')\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "n = len(y)\n",
        "X_base = np.vstack([oof_lr, oof_dense, oof_meta]).T.astype(np.float32)\n",
        "X_test = np.vstack([test_lr, test_dense, test_meta]).T.astype(np.float32)\n",
        "assert X_base.shape == (n, 3) and X_test.shape[1] == 3, 'Shape mismatch in base features'\n",
        "\n",
        "cv = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(np.arange(n), y))\n",
        "oof_meta_lr = np.zeros(n, dtype=np.float32)\n",
        "test_bag = []\n",
        "\n",
        "t0 = time.time()\n",
        "for k, (tr_idx, va_idx) in enumerate(cv):\n",
        "    Xtr, ytr = X_base[tr_idx], y[tr_idx]\n",
        "    Xva, yva = X_base[va_idx], y[va_idx]\n",
        "    # Use liblinear for reliability on tiny 3-dim data\n",
        "    clf = LogisticRegression(solver='liblinear', C=1.0, penalty='l2', max_iter=1000, random_state=42)\n",
        "    clf.fit(Xtr, ytr)\n",
        "    va_pred = clf.predict_proba(Xva)[:,1].astype(np.float32)\n",
        "    oof_meta_lr[va_idx] = va_pred\n",
        "    test_pred = clf.predict_proba(X_test)[:,1].astype(np.float32)\n",
        "    test_bag.append(test_pred)\n",
        "    fold_auc = roc_auc_score(yva, va_pred)\n",
        "    print(f'Fold {k+1}/5 meta AUC: {fold_auc:.5f}')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "auc_oof = roc_auc_score(y, oof_meta_lr)\n",
        "print(f'Stacker OOF AUC (LR-liblinear on 3 base preds): {auc_oof:.5f} | time {time.time()-t0:.1f}s')\n",
        "\n",
        "# Build test via averaging fold meta-model predictions\n",
        "test_stack = np.mean(test_bag, axis=0).astype(np.float32)\n",
        "\n",
        "# Only write submission if this beats our current best OOF baseline (0.69201 logit-blend) or hits bronze threshold\n",
        "if auc_oof >= 0.69210 or auc_oof > 0.69201:\n",
        "    sub = pd.DataFrame({id_col: test[id_col].values, target_col: test_stack})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv from LR-liblinear stacker; head:')\n",
        "    print(sub.head())\n",
        "else:\n",
        "    print('Stacker did not beat best OOF 0.69201; keeping existing submission.csv')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached base OOF/test predictions...\nFold 1/5 meta AUC: 0.69473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 meta AUC: 0.67761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 meta AUC: 0.72151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 meta AUC: 0.64499\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 meta AUC: 0.72185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacker OOF AUC (LR-liblinear on 3 base preds): 0.69015 | time 0.0s\nStacker did not beat best OOF 0.69201; keeping existing submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "a4553879-43d3-45f0-8dca-1b2639b44b4d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Dense XGB v2: smaller SVD (200/200/60) + 3 seeds; blend with LR+Meta using cached OOF; aim to exceed 0.69210\n",
        "import time, gc, sys, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    import subprocess, sys as _sys\n",
        "    subprocess.run([_sys.executable, '-m', 'pip', 'install', '-q', 'xgboost'])\n",
        "    import xgboost as xgb\n",
        "\n",
        "assert 'combine_raw_text' in globals() and 'clean_text_series' in globals() and 'build_subreddit_text' in globals(), 'Run pivot cell 7 first'\n",
        "assert 'word_params' in globals() and 'char_params' in globals() and 'subs_params' in globals(), 'Run pivot cell 7 first'\n",
        "assert 'build_meta_enhanced' in globals(), 'Run cell 10 to define build_meta_enhanced'\n",
        "\n",
        "# SVD component sizes (smaller per expert coach recommendation)\n",
        "svd_word_n = 200\n",
        "svd_char_n = 200\n",
        "svd_subs_n = 60\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "n_splits = 5\n",
        "cv = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42).split(train, y))\n",
        "print(f'Prepared {n_splits}-fold StratifiedKFold CV (shuffled).')\n",
        "\n",
        "# Precompute test sources\n",
        "raw_te_text = combine_raw_text(test)\n",
        "clean_te_text = clean_text_series(raw_te_text)\n",
        "subs_te_text = build_subreddit_text(test)\n",
        "meta_te = build_meta_enhanced(test).astype(np.float32)\n",
        "\n",
        "# Containers for seed-bagging\n",
        "seeds = [42, 2025, 7]\n",
        "oof_bag = np.zeros((len(train), len(seeds)), dtype=np.float32)\n",
        "test_bag = np.zeros((len(test), len(seeds)), dtype=np.float32)\n",
        "\n",
        "# XGB params (slightly regularized, device=cuda, early stopping)\n",
        "base_params = dict(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='auc',\n",
        "    max_depth=4,\n",
        "    eta=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.7,\n",
        "    min_child_weight=6,\n",
        "    reg_alpha=1.0,\n",
        "    reg_lambda=2.5,\n",
        "    gamma=0.0,\n",
        "    device='cuda'\n",
        ")\n",
        "\n",
        "for si, seed in enumerate(seeds):\n",
        "    print(f'=== Seed {seed} / {len(seeds)} ===')\n",
        "    params = dict(base_params); params['seed'] = seed\n",
        "    oof = np.zeros(len(train), dtype=np.float32)\n",
        "    test_fold_preds = []\n",
        "\n",
        "    for fold, (tr_idx, va_idx) in enumerate(cv):\n",
        "        t0 = time.time()\n",
        "        print(f'Seed {seed} | Fold {fold+1}/{n_splits} - train {len(tr_idx)} va {len(va_idx)}')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Text prep\n",
        "        tr_text_raw = combine_raw_text(train.loc[tr_idx])\n",
        "        va_text_raw = combine_raw_text(train.loc[va_idx])\n",
        "        tr_text = clean_text_series(tr_text_raw)\n",
        "        va_text = clean_text_series(va_text_raw)\n",
        "        tr_subs = build_subreddit_text(train.loc[tr_idx])\n",
        "        va_subs = build_subreddit_text(train.loc[va_idx])\n",
        "\n",
        "        # TF-IDF (fit on train split)\n",
        "        tfidf_w = TfidfVectorizer(**word_params)\n",
        "        Xw_tr = tfidf_w.fit_transform(tr_text)\n",
        "        Xw_va = tfidf_w.transform(va_text)\n",
        "        Xw_te = tfidf_w.transform(clean_te_text)\n",
        "\n",
        "        tfidf_c = TfidfVectorizer(**char_params)\n",
        "        Xc_tr = tfidf_c.fit_transform(tr_text)\n",
        "        Xc_va = tfidf_c.transform(va_text)\n",
        "        Xc_te = tfidf_c.transform(clean_te_text)\n",
        "\n",
        "        tfidf_s = TfidfVectorizer(**subs_params)\n",
        "        Xs_tr = tfidf_s.fit_transform(tr_subs)\n",
        "        Xs_va = tfidf_s.transform(va_subs)\n",
        "        Xs_te = tfidf_s.transform(subs_te_text)\n",
        "\n",
        "        # SVD to dense\n",
        "        svd_w = TruncatedSVD(n_components=svd_word_n, random_state=seed)\n",
        "        Zw_tr = svd_w.fit_transform(Xw_tr)\n",
        "        Zw_va = svd_w.transform(Xw_va)\n",
        "        Zw_te = svd_w.transform(Xw_te)\n",
        "\n",
        "        svd_c = TruncatedSVD(n_components=svd_char_n, random_state=seed)\n",
        "        Zc_tr = svd_c.fit_transform(Xc_tr)\n",
        "        Zc_va = svd_c.transform(Xc_va)\n",
        "        Zc_te = svd_c.transform(Xc_te)\n",
        "\n",
        "        svd_s = TruncatedSVD(n_components=svd_subs_n, random_state=seed)\n",
        "        Zs_tr = svd_s.fit_transform(Xs_tr)\n",
        "        Zs_va = svd_s.transform(Xs_va)\n",
        "        Zs_te = svd_s.transform(Xs_te)\n",
        "\n",
        "        # Enhanced meta (dense)\n",
        "        meta_tr = build_meta_enhanced(train.loc[tr_idx]).astype(np.float32)\n",
        "        meta_va = build_meta_enhanced(train.loc[va_idx]).astype(np.float32)\n",
        "\n",
        "        # Concatenate dense blocks\n",
        "        Xtr_dense = np.hstack([Zw_tr, Zc_tr, Zs_tr, meta_tr.values]).astype(np.float32)\n",
        "        Xva_dense = np.hstack([Zw_va, Zc_va, Zs_va, meta_va.values]).astype(np.float32)\n",
        "        Xte_dense = np.hstack([Zw_te, Zc_te, Zs_te, meta_te.values]).astype(np.float32)\n",
        "\n",
        "        # Scale dense features\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "        Xtr_d = scaler.fit_transform(Xtr_dense)\n",
        "        Xva_d = scaler.transform(Xva_dense)\n",
        "        Xte_d = scaler.transform(Xte_dense)\n",
        "\n",
        "        # XGB training via DMatrix with early stopping\n",
        "        dtrain = xgb.DMatrix(Xtr_d, label=y[tr_idx])\n",
        "        dvalid = xgb.DMatrix(Xva_d, label=y[va_idx])\n",
        "        dtest  = xgb.DMatrix(Xte_d)\n",
        "        evals = [(dtrain, 'train'), (dvalid, 'valid')]\n",
        "        booster = xgb.train(\n",
        "            params,\n",
        "            dtrain,\n",
        "            num_boost_round=4000,\n",
        "            evals=evals,\n",
        "            early_stopping_rounds=200,\n",
        "            verbose_eval=False\n",
        "        )\n",
        "        va_pred = booster.predict(dvalid, iteration_range=(0, booster.best_iteration+1)).astype(np.float32)\n",
        "        te_pred = booster.predict(dtest, iteration_range=(0, booster.best_iteration+1)).astype(np.float32)\n",
        "        oof[va_idx] = va_pred\n",
        "        test_fold_preds.append(te_pred)\n",
        "\n",
        "        auc = roc_auc_score(y[va_idx], va_pred)\n",
        "        print(f'Seed {seed} | Fold {fold+1} AUC: {auc:.5f} | best_iter={booster.best_iteration} | time {time.time()-t0:.1f}s')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        del Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, Xs_tr, Xs_va, Xs_te, Zw_tr, Zw_va, Zw_te, Zc_tr, Zc_va, Zc_te, Zs_tr, Zs_va, Zs_te, meta_tr, meta_va, Xtr_dense, Xva_dense, Xte_dense, Xtr_d, Xva_d, Xte_d, dtrain, dvalid, dtest, booster, tfidf_w, tfidf_c, tfidf_s, svd_w, svd_c, svd_s, scaler\n",
        "        gc.collect()\n",
        "\n",
        "    # Store bagged predictions\n",
        "    oof_bag[:, si] = oof\n",
        "    test_bag[:, si] = np.mean(test_fold_preds, axis=0).astype(np.float32)\n",
        "    auc_seed = roc_auc_score(y, oof)\n",
        "    print(f'Seed {seed} OOF AUC: {auc_seed:.5f}')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "# Average across seeds\n",
        "oof_dense_v2 = oof_bag.mean(axis=1)\n",
        "test_dense_v2 = test_bag.mean(axis=1)\n",
        "auc_oof_v2 = roc_auc_score(y, oof_dense_v2)\n",
        "print(f'Bagged XGB (SVD 200/200/60 + enhanced meta) OOF AUC: {auc_oof_v2:.5f}')\n",
        "\n",
        "# Cache v2 OOF/test\n",
        "np.save('oof_xgb_dense_v2.npy', oof_dense_v2.astype(np.float32))\n",
        "np.save('test_xgb_dense_v2.npy', test_dense_v2.astype(np.float32))\n",
        "\n",
        "# Blend v2 dense with cached LR + Meta OOF to tune weights (prob space)\n",
        "oof_lr = np.load('oof_lr_pivot.npy')\n",
        "oof_meta = np.load('oof_xgb_meta.npy')\n",
        "test_lr = np.load('test_lr_pivot.npy')\n",
        "test_meta = np.load('test_xgb_meta.npy')\n",
        "\n",
        "def eval_auc_prob(w1, w2):\n",
        "    w3 = 1.0 - w1 - w2\n",
        "    if w3 < 0 or w3 > 1: return -1.0\n",
        "    blend = w1*oof_lr + w2*oof_dense_v2 + w3*oof_meta\n",
        "    return roc_auc_score(y, blend)\n",
        "\n",
        "# Coarse-to-fine around prior best center (0.3414, 0.4092)\n",
        "w1_c, w2_c = 0.3414, 0.4092\n",
        "best_auc, best_w = -1.0, (w1_c, w2_c, 1.0 - w1_c - w2_c)\n",
        "def grid_pass(center_w1, center_w2, window, step):\n",
        "    global best_auc, best_w\n",
        "    w1_min, w1_max = max(0.0, center_w1 - window), min(1.0, center_w1 + window)\n",
        "    w2_min, w2_max = max(0.0, center_w2 - window), min(1.0, center_w2 + window)\n",
        "    w1_grid = np.arange(w1_min, w1_max + 1e-12, step)\n",
        "    w2_grid = np.arange(w2_min, w2_max + 1e-12, step)\n",
        "    t0s = time.time(); cnt = 0\n",
        "    for w1 in w1_grid:\n",
        "        for w2 in w2_grid:\n",
        "            if w1 + w2 > 1.0: continue\n",
        "            auc = eval_auc_prob(w1, w2)\n",
        "            cnt += 1\n",
        "            if auc > best_auc:\n",
        "                best_auc = auc; best_w = (float(w1), float(w2), float(1.0 - w1 - w2))\n",
        "    print(f'Blend grid window={window} step={step} tried {cnt} | best={best_w} OOF AUC: {best_auc:.5f} | {time.time()-t0s:.1f}s')\n",
        "    return best_w[0], best_w[1]\n",
        "\n",
        "w1a, w2a = grid_pass(w1_c, w2_c, window=0.06, step=0.002)\n",
        "w1b, w2b = grid_pass(w1a, w2a, window=0.02, step=0.001)\n",
        "print(f'Final prob-blend weights(v2)={best_w} | OOF AUC: {best_auc:.5f}')\n",
        "\n",
        "# If improved vs 0.69201, write submission with v2 dense\n",
        "if best_auc >= 0.69210 or best_auc > 0.69201:\n",
        "    w1, w2, w3 = best_w\n",
        "    test_blend = (w1*test_lr + w2*test_dense_v2 + w3*test_meta).astype(np.float32)\n",
        "    sub = pd.DataFrame({id_col: test[id_col].values, target_col: test_blend})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (3-way blend with dense v2); head:')\n",
        "    print(sub.head())\n",
        "else:\n",
        "    print('Dense v2 blend did not beat 0.69201; keeping current best submission.')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 5-fold StratifiedKFold CV (shuffled).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Seed 42 / 3 ===\nSeed 42 | Fold 1/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 1 AUC: 0.68480 | best_iter=125 | time 14.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 2/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 2 AUC: 0.65718 | best_iter=108 | time 14.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 3/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 3 AUC: 0.68315 | best_iter=49 | time 14.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 4/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 4 AUC: 0.64337 | best_iter=296 | time 15.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 5/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 5 AUC: 0.69807 | best_iter=120 | time 14.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 OOF AUC: 0.66976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Seed 2025 / 3 ===\nSeed 2025 | Fold 1/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 1 AUC: 0.65807 | best_iter=66 | time 13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 2/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 2 AUC: 0.66279 | best_iter=641 | time 15.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 3/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 3 AUC: 0.67685 | best_iter=45 | time 13.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 4/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 4 AUC: 0.63028 | best_iter=74 | time 13.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 5/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 | Fold 5 AUC: 0.71652 | best_iter=61 | time 13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 2025 OOF AUC: 0.65146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Seed 7 / 3 ===\nSeed 7 | Fold 1/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 1 AUC: 0.66718 | best_iter=80 | time 14.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 2/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 2 AUC: 0.65757 | best_iter=15 | time 13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 3/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 3 AUC: 0.68273 | best_iter=26 | time 13.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 4/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 4 AUC: 0.62948 | best_iter=189 | time 14.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 5/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 | Fold 5 AUC: 0.70696 | best_iter=56 | time 14.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 7 OOF AUC: 0.65787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagged XGB (SVD 200/200/60 + enhanced meta) OOF AUC: 0.67490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blend grid window=0.06 step=0.002 tried 3721 | best=(0.37940000000000007, 0.40120000000000006, 0.21939999999999987) OOF AUC: 0.69116 | 7.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blend grid window=0.02 step=0.001 tried 1681 | best=(0.38040000000000007, 0.40020000000000006, 0.21939999999999987) OOF AUC: 0.69116 | 3.2s\nFinal prob-blend weights(v2)=(0.38040000000000007, 0.40020000000000006, 0.21939999999999987) | OOF AUC: 0.69116\nDense v2 blend did not beat 0.69201; keeping current best submission.\n"
          ]
        }
      ]
    },
    {
      "id": "a9ffd35e-f68c-4b58-8161-215058bf9016",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick checks: logit geometric mean and tiny temperature sweep with small-window weight search\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "print('Loading cached OOF/test predictions (3-seed dense)...')\n",
        "o1 = np.load('oof_lr_pivot.npy')\n",
        "o2 = np.load('oof_xgb_dense.npy')\n",
        "o3 = np.load('oof_xgb_meta.npy')\n",
        "t1 = np.load('test_lr_pivot.npy')\n",
        "t2 = np.load('test_xgb_dense.npy')\n",
        "t3 = np.load('test_xgb_meta.npy')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "# Logits\n",
        "z1, z2, z3 = to_logit(o1), to_logit(o2), to_logit(o3)\n",
        "tz1, tz2, tz3 = to_logit(t1), to_logit(t2), to_logit(t3)\n",
        "\n",
        "# 1) Equal-weight logit geometric mean (average logits) -> check OOF\n",
        "z_avg = (z1 + z2 + z3) / 3.0\n",
        "auc_geom = roc_auc_score(y, z_avg)\n",
        "print(f'Equal-weight logit geometric mean OOF AUC(z): {auc_geom:.5f}')\n",
        "if auc_geom >= 0.69210:\n",
        "    pt = sigmoid((tz1 + tz2 + tz3) / 3.0).astype(np.float32)\n",
        "    sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv from logit geometric mean; head:')\n",
        "    print(sub.head())\n",
        "else:\n",
        "    # 2) Tiny temperature sweep {0.95,1.00,1.05} with local weight search around prior best logit weights\n",
        "    scales = [0.95, 1.00, 1.05]\n",
        "    # Prior best logit weights center from cell 24\n",
        "    w1c, w2c = 0.3501, 0.4198\n",
        "    window, step = 0.015, 0.001\n",
        "    best_auc, best_cfg = -1.0, None\n",
        "    t0 = time.time(); tried = 0\n",
        "    for s1 in scales:\n",
        "        for s2 in scales:\n",
        "            for s3 in scales:\n",
        "                zz1, zz2, zz3 = s1*z1, s2*z2, s3*z3\n",
        "                w1_min, w1_max = max(0.0, w1c - window), min(1.0, w1c + window)\n",
        "                w2_min, w2_max = max(0.0, w2c - window), min(1.0, w2c + window)\n",
        "                w1_grid = np.arange(w1_min, w1_max + 1e-12, step)\n",
        "                w2_grid = np.arange(w2_min, w2_max + 1e-12, step)\n",
        "                local_best_auc, local_best_w = -1.0, None\n",
        "                for w1 in w1_grid:\n",
        "                    for w2 in w2_grid:\n",
        "                        if w1 + w2 > 1.0: continue\n",
        "                        w3 = 1.0 - w1 - w2\n",
        "                        z = w1*zz1 + w2*zz2 + w3*zz3\n",
        "                        auc = roc_auc_score(y, z)\n",
        "                        tried += 1\n",
        "                        if auc > local_best_auc:\n",
        "                            local_best_auc, local_best_w = auc, (float(w1), float(w2), float(w3))\n",
        "                if local_best_auc > best_auc:\n",
        "                    best_auc, best_cfg = local_best_auc, (s1, s2, s3, local_best_w)\n",
        "    print(f'Tiny temp sweep tried {tried} combos | best AUC(z): {best_auc:.5f} with scales={best_cfg[:3]} weights={best_cfg[3]} | {time.time()-t0:.1f}s')\n",
        "    # If improved to threshold, write submission\n",
        "    if best_auc >= 0.69210:\n",
        "        s1, s2, s3, (bw1, bw2, bw3) = best_cfg\n",
        "        pt = sigmoid(bw1*(s1*tz1) + bw2*(s2*tz2) + bw3*(s3*tz3)).astype(np.float32)\n",
        "        sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "        sub.to_csv('submission.csv', index=False)\n",
        "        print('Saved submission.csv from tiny temperature + logit blend; head:')\n",
        "        print(sub.head())\n",
        "    else:\n",
        "        print('No improvement beyond 0.69210; keeping current best submission.')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached OOF/test predictions (3-seed dense)...\nEqual-weight logit geometric mean OOF AUC(z): 0.69148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiny temp sweep tried 25947 combos | best AUC(z): 0.69202 with scales=(0.95, 0.95, 1.05) weights=(0.35810000000000003, 0.4288, 0.2130999999999999) | 50.0s\nNo improvement beyond 0.69210; keeping current best submission.\n"
          ]
        }
      ]
    },
    {
      "id": "bc255e3f-c550-44f4-828b-4a8ca5b4fde0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Add a diverse sparse LR variant (word 1-3 + char_wb 2-5, min_df=2, no URL/NUM tokenization) and 4-way logit blend\n",
        "import time, gc, sys, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "assert 'combine_raw_text' in globals(), 'Run pivot cell 7 first for combine_raw_text'\n",
        "\n",
        "# Alternate text: simple lowercase without url/num replacement for diversity\n",
        "def simple_clean_series(s: pd.Series) -> pd.Series:\n",
        "    return s.fillna('').astype(str).str.lower()\n",
        "\n",
        "# Vectorizer params for diverse LR\n",
        "word_params_alt = dict(analyzer='word', ngram_range=(1,3), lowercase=True, min_df=2, max_features=80000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "char_params_alt = dict(analyzer='char_wb', ngram_range=(2,5), lowercase=True, min_df=2, max_features=100000, sublinear_tf=True, smooth_idf=True, norm='l2')\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "n_splits = 5\n",
        "cv = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42).split(train, y))\n",
        "print(f'Prepared {n_splits}-fold StratifiedKFold CV (shuffled).')\n",
        "\n",
        "# Precompute texts\n",
        "tr_text_full = combine_raw_text(train)\n",
        "te_text_full = combine_raw_text(test)\n",
        "tr_text_alt = simple_clean_series(tr_text_full)\n",
        "te_text_alt = simple_clean_series(te_text_full)\n",
        "\n",
        "# Train alternate LR (no subreddit vectorizer to save time; include minimal meta for stability)\n",
        "def build_meta_min_fast(df):\n",
        "    return build_meta_minimal(df).astype(np.float32) if 'build_meta_minimal' in globals() else pd.DataFrame(index=df.index)\n",
        "\n",
        "oof_lr_alt = np.zeros(len(train), dtype=np.float32)\n",
        "test_lr_alt_folds = []\n",
        "LR_C_alt = 0.5\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(cv):\n",
        "    t0 = time.time()\n",
        "    print(f'Alt LR | Fold {fold+1}/{n_splits} - train {len(tr_idx)} va {len(va_idx)}')\n",
        "    sys.stdout.flush()\n",
        "    tr_txt = tr_text_alt.iloc[tr_idx]\n",
        "    va_txt = tr_text_alt.iloc[va_idx]\n",
        "\n",
        "    tfidf_w = TfidfVectorizer(**word_params_alt)\n",
        "    Xw_tr = tfidf_w.fit_transform(tr_txt)\n",
        "    Xw_va = tfidf_w.transform(va_txt)\n",
        "    Xw_te = tfidf_w.transform(te_text_alt)\n",
        "\n",
        "    tfidf_c = TfidfVectorizer(**char_params_alt)\n",
        "    Xc_tr = tfidf_c.fit_transform(tr_txt)\n",
        "    Xc_va = tfidf_c.transform(va_txt)\n",
        "    Xc_te = tfidf_c.transform(te_text_alt)\n",
        "\n",
        "    # Minimal meta\n",
        "    meta_tr = build_meta_min_fast(train.loc[tr_idx])\n",
        "    meta_va = build_meta_min_fast(train.loc[va_idx])\n",
        "    meta_te = build_meta_min_fast(test)\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    Xm_tr = scaler.fit_transform(meta_tr)\n",
        "    Xm_va = scaler.transform(meta_va)\n",
        "    Xm_te = scaler.transform(meta_te)\n",
        "\n",
        "    X_tr = hstack([Xw_tr, Xc_tr, Xm_tr], format='csr')\n",
        "    X_va = hstack([Xw_va, Xc_va, Xm_va], format='csr')\n",
        "    X_te = hstack([Xw_te, Xc_te, Xm_te], format='csr')\n",
        "\n",
        "    lr_alt = LogisticRegression(solver='saga', penalty='l2', C=LR_C_alt, class_weight=None, max_iter=6000, n_jobs=-1, random_state=42, verbose=0)\n",
        "    lr_alt.fit(X_tr, y[tr_idx])\n",
        "    va_pred = lr_alt.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "    oof_lr_alt[va_idx] = va_pred\n",
        "    te_pred = lr_alt.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "    test_lr_alt_folds.append(te_pred)\n",
        "\n",
        "    auc = roc_auc_score(y[va_idx], va_pred)\n",
        "    print(f'Alt LR | Fold {fold+1} AUC: {auc:.5f} | time {time.time()-t0:.1f}s')\n",
        "    sys.stdout.flush()\n",
        "    del Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, Xm_tr, Xm_va, Xm_te, X_tr, X_va, X_te, lr_alt, tfidf_w, tfidf_c, scaler\n",
        "    gc.collect()\n",
        "\n",
        "auc_alt = roc_auc_score(y, oof_lr_alt)\n",
        "print(f'Alt LR OOF AUC: {auc_alt:.5f}')\n",
        "test_lr_alt = np.mean(test_lr_alt_folds, axis=0).astype(np.float32)\n",
        "np.save('oof_lr_alt.npy', oof_lr_alt)\n",
        "np.save('test_lr_alt.npy', test_lr_alt)\n",
        "\n",
        "# 4-way logit blend with tiny weight for alt (0..0.15), local search for others around prior best\n",
        "o1 = np.load('oof_lr_pivot.npy')\n",
        "o2 = np.load('oof_xgb_dense.npy')\n",
        "o3 = np.load('oof_xgb_meta.npy')\n",
        "o4 = oof_lr_alt\n",
        "t1 = np.load('test_lr_pivot.npy')\n",
        "t2 = np.load('test_xgb_dense.npy')\n",
        "t3 = np.load('test_xgb_meta.npy')\n",
        "t4 = test_lr_alt\n",
        "\n",
        "z1, z2, z3, z4 = to_logit(o1), to_logit(o2), to_logit(o3), to_logit(o4)\n",
        "tz1, tz2, tz3, tz4 = to_logit(t1), to_logit(t2), to_logit(t3), to_logit(t4)\n",
        "\n",
        "# Prior best 3-way logit center\n",
        "base_w1, base_w2, base_w3 = 0.3501, 0.4198, 0.2301\n",
        "best_auc, best_w = -1.0, (base_w1, base_w2, base_w3, 0.0)\n",
        "t0 = time.time(); tried = 0\n",
        "for w4 in np.arange(0.0, 0.15001, 0.005):\n",
        "    rem = 1.0 - w4\n",
        "    cw1, cw2 = base_w1 * rem, base_w2 * rem\n",
        "    w1_min, w1_max = max(0.0, cw1 - 0.02*rem), min(rem, cw1 + 0.02*rem)\n",
        "    w2_min, w2_max = max(0.0, cw2 - 0.02*rem), min(rem, cw2 + 0.02*rem)\n",
        "    w1_grid = np.arange(w1_min, w1_max + 1e-12, 0.001)\n",
        "    w2_grid = np.arange(w2_min, w2_max + 1e-12, 0.001)\n",
        "    for w1 in w1_grid:\n",
        "        for w2 in w2_grid:\n",
        "            w3 = rem - w1 - w2\n",
        "            if w3 < 0.0:\n",
        "                continue\n",
        "            z = w1*z1 + w2*z2 + w3*z3 + w4*z4\n",
        "            auc = roc_auc_score(y, z)\n",
        "            tried += 1\n",
        "            if auc > best_auc:\n",
        "                best_auc = auc; best_w = (float(w1/rem) if rem>0 else 0.0, float(w2/rem) if rem>0 else 0.0, float(w3/rem) if rem>0 else 0.0, float(w4))\n",
        "print(f'4-way logit blend tried {tried} combos | best normalized weights (LR,Dense,Meta,Alt)={best_w} | OOF AUC(z): {best_auc:.5f} | {time.time()-t0:.1f}s')\n",
        "\n",
        "# Build submission if threshold met\n",
        "w1n, w2n, w3n, w4n = best_w\n",
        "if best_auc >= 0.69210:\n",
        "    zt = (w1n*tz1 + w2n*tz2 + w3n*tz3) * (1.0 - w4n) + w4n*tz4\n",
        "    pt = sigmoid(zt).astype(np.float32)\n",
        "    sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (4-way logit blend incl. alt LR); head:')\n",
        "    print(sub.head())\n",
        "else:\n",
        "    print('4-way blend did not reach 0.69210; keeping current best submission.')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 5-fold StratifiedKFold CV (shuffled).\nAlt LR | Fold 1/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alt LR | Fold 1 AUC: 0.67561 | time 22.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alt LR | Fold 2/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alt LR | Fold 2 AUC: 0.66758 | time 20.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alt LR | Fold 3/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alt LR | Fold 3 AUC: 0.68796 | time 22.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alt LR | Fold 4/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alt LR | Fold 4 AUC: 0.64426 | time 19.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alt LR | Fold 5/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alt LR | Fold 5 AUC: 0.69882 | time 21.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alt LR OOF AUC: 0.67415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4-way logit blend tried 43956 combos | best normalized weights (LR,Dense,Meta,Alt)=(0.33115820105820104, 0.41990582010582017, 0.24893597883597882, 0.055) | OOF AUC(z): 0.69208 | 88.0s\n4-way blend did not reach 0.69210; keeping current best submission.\n"
          ]
        }
      ]
    },
    {
      "id": "999a4d55-468f-43a8-b445-fe42baa55285",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Logit-space blend using Dense XGB v2 (oof_xgb_dense_v2.npy) + tiny temperature sweep; aim to exceed 0.69210\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "print('Loading cached OOF/test predictions with dense v2...')\n",
        "o1 = np.load('oof_lr_pivot.npy')\n",
        "o2 = np.load('oof_xgb_dense_v2.npy')\n",
        "o3 = np.load('oof_xgb_meta.npy')\n",
        "t1 = np.load('test_lr_pivot.npy')\n",
        "t2 = np.load('test_xgb_dense_v2.npy')\n",
        "t3 = np.load('test_xgb_meta.npy')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "# Logits\n",
        "z1, z2, z3 = to_logit(o1), to_logit(o2), to_logit(o3)\n",
        "tz1, tz2, tz3 = to_logit(t1), to_logit(t2), to_logit(t3)\n",
        "\n",
        "def eval_auc_logit(w1, w2):\n",
        "    w3 = 1.0 - w1 - w2\n",
        "    if w3 < 0 or w3 > 1:\n",
        "        return -1.0\n",
        "    z = w1*z1 + w2*z2 + w3*z3\n",
        "    return roc_auc_score(y, z)\n",
        "\n",
        "# Start near prior best logit center from v1\n",
        "w1_c, w2_c = 0.3501, 0.4198\n",
        "best_auc, best_w = -1.0, (w1_c, w2_c, 1.0 - w1_c - w2_c)\n",
        "\n",
        "def grid_search_logit(w1_center, w2_center, window, step):\n",
        "    global best_auc, best_w\n",
        "    w1_min, w1_max = max(0.0, w1_center - window), min(1.0, w1_center + window)\n",
        "    w2_min, w2_max = max(0.0, w2_center - window), min(1.0, w2_center + window)\n",
        "    w1_grid = np.arange(w1_min, w1_max + 1e-12, step)\n",
        "    w2_grid = np.arange(w2_min, w2_max + 1e-12, step)\n",
        "    t0 = time.time(); cnt = 0\n",
        "    for w1 in w1_grid:\n",
        "        for w2 in w2_grid:\n",
        "            if w1 + w2 > 1.0:\n",
        "                continue\n",
        "            auc = eval_auc_logit(w1, w2)\n",
        "            cnt += 1\n",
        "            if auc > best_auc:\n",
        "                best_auc = auc\n",
        "                best_w = (float(w1), float(w2), float(1.0 - w1 - w2))\n",
        "    print(f'v2 logit grid window={window} step={step} tried {cnt} | best={best_w} OOF AUC(z): {best_auc:.5f}')\n",
        "\n",
        "# Coarse-to-fine around the center\n",
        "grid_search_logit(w1_c, w2_c, window=0.05, step=0.005)\n",
        "grid_search_logit(best_w[0], best_w[1], window=0.02, step=0.001)\n",
        "grid_search_logit(best_w[0], best_w[1], window=0.008, step=0.0005)\n",
        "print(f'v2 logit-blend preliminary weights={best_w} | OOF AUC(z): {best_auc:.5f}')\n",
        "\n",
        "# Tiny temperature sweep around 1.0 for v2 (fast)\n",
        "scales = [0.95, 1.00, 1.05]\n",
        "w1c, w2c = best_w[0], best_w[1]\n",
        "window, step = 0.015, 0.001\n",
        "best_auc_t, best_cfg = best_auc, (1.0, 1.0, 1.0, best_w)\n",
        "t0 = time.time(); tried = 0\n",
        "for s1 in scales:\n",
        "    for s2 in scales:\n",
        "        for s3 in scales:\n",
        "            zz1, zz2, zz3 = s1*z1, s2*z2, s3*z3\n",
        "            w1_min, w1_max = max(0.0, w1c - window), min(1.0, w1c + window)\n",
        "            w2_min, w2_max = max(0.0, w2c - window), min(1.0, w2c + window)\n",
        "            w1_grid = np.arange(w1_min, w1_max + 1e-12, step)\n",
        "            w2_grid = np.arange(w2_min, w2_max + 1e-12, step)\n",
        "            local_best_auc, local_best_w = -1.0, None\n",
        "            for w1 in w1_grid:\n",
        "                for w2 in w2_grid:\n",
        "                    if w1 + w2 > 1.0: continue\n",
        "                    w3 = 1.0 - w1 - w2\n",
        "                    z = w1*zz1 + w2*zz2 + w3*zz3\n",
        "                    auc = roc_auc_score(y, z)\n",
        "                    tried += 1\n",
        "                    if auc > local_best_auc:\n",
        "                        local_best_auc, local_best_w = auc, (float(w1), float(w2), float(w3))\n",
        "            if local_best_auc > best_auc_t:\n",
        "                best_auc_t, best_cfg = local_best_auc, (s1, s2, s3, local_best_w)\n",
        "print(f'v2 tiny temp sweep tried {tried} combos | best AUC(z): {best_auc_t:.5f} with scales={best_cfg[:3]} weights={best_cfg[3]} | {time.time()-t0:.1f}s')\n",
        "\n",
        "# Choose the best between plain v2 logit-blend and temp-swept v2\n",
        "use_auc = best_auc_t if best_auc_t > best_auc else best_auc\n",
        "use_w = best_cfg[3] if best_auc_t > best_auc else best_w\n",
        "use_scales = best_cfg[:3] if best_auc_t > best_auc else (1.0, 1.0, 1.0)\n",
        "print(f'Chosen v2 config: scales={use_scales} weights={use_w} OOF AUC(z): {use_auc:.5f}')\n",
        "\n",
        "# If threshold reached, write submission with dense v2 logits\n",
        "if use_auc >= 0.69210:\n",
        "    s1, s2, s3 = use_scales\n",
        "    w1b, w2b, w3b = use_w\n",
        "    pt = sigmoid(w1b*(s1*tz1) + w2b*(s2*tz2) + w3b*(s3*tz3)).astype(np.float32)\n",
        "    sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (v2 logit blend); head:')\n",
        "    print(sub.head())\n",
        "else:\n",
        "    print('v2 logit blend < 0.69210; keeping current best submission.')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached OOF/test predictions with dense v2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v2 logit grid window=0.05 step=0.005 tried 441 | best=(0.33010000000000006, 0.41980000000000006, 0.2500999999999999) OOF AUC(z): 0.69153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v2 logit grid window=0.02 step=0.001 tried 1681 | best=(0.33510000000000006, 0.42280000000000006, 0.24209999999999987) OOF AUC(z): 0.69155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v2 logit grid window=0.008 step=0.0005 tried 1089 | best=(0.33460000000000006, 0.42280000000000006, 0.24259999999999993) OOF AUC(z): 0.69155\nv2 logit-blend preliminary weights=(0.33460000000000006, 0.42280000000000006, 0.24259999999999993) | OOF AUC(z): 0.69155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v2 tiny temp sweep tried 25947 combos | best AUC(z): 0.69155 with scales=(1.0, 1.05, 1.05) weights=(0.3466000000000001, 0.41480000000000006, 0.23859999999999992) | 51.2s\nChosen v2 config: scales=(1.0, 1.05, 1.05) weights=(0.3466000000000001, 0.41480000000000006, 0.23859999999999992) OOF AUC(z): 0.69155\nv2 logit blend < 0.69210; keeping current best submission.\n"
          ]
        }
      ]
    },
    {
      "id": "3f13755f-1e45-4bf2-9bd6-c2fb23fd48db",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 4-way logit blend: LR + Dense v1 + Dense v2 + Meta (split dense weight); aim to exceed 0.69210\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "print('Loading cached OOF/test predictions (LR, Dense v1, Dense v2, Meta)...')\n",
        "o1 = np.load('oof_lr_pivot.npy')\n",
        "o2 = np.load('oof_xgb_dense.npy')\n",
        "o2b = np.load('oof_xgb_dense_v2.npy')\n",
        "o3 = np.load('oof_xgb_meta.npy')\n",
        "t1 = np.load('test_lr_pivot.npy')\n",
        "t2 = np.load('test_xgb_dense.npy')\n",
        "t2b = np.load('test_xgb_dense_v2.npy')\n",
        "t3 = np.load('test_xgb_meta.npy')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "# Logits\n",
        "z1, z2, z2b, z3 = to_logit(o1), to_logit(o2), to_logit(o2b), to_logit(o3)\n",
        "tz1, tz2, tz2b, tz3 = to_logit(t1), to_logit(t2), to_logit(t2b), to_logit(t3)\n",
        "\n",
        "# Start near prior best 3-way logit weights (v1): (0.3501, 0.4198, 0.2301)\n",
        "w1c, wdc = 0.3501, 0.4198  # LR, total dense\n",
        "w3c = 1.0 - w1c - wdc\n",
        "w1_min, w1_max = max(0.0, w1c - 0.02), min(1.0, w1c + 0.02)\n",
        "wd_min, wd_max = max(0.0, wdc - 0.02), min(1.0, wdc + 0.02)\n",
        "step = 0.001\n",
        "\n",
        "best_auc, best_cfg = -1.0, None\n",
        "t0 = time.time(); cnt = 0\n",
        "alphas = np.arange(0.0, 0.4001, 0.05)  # fraction of total dense weight assigned to v2\n",
        "w1_grid = np.arange(w1_min, w1_max + 1e-12, step)\n",
        "wd_grid = np.arange(wd_min, wd_max + 1e-12, step)\n",
        "for w1 in w1_grid:\n",
        "    for wd in wd_grid:\n",
        "        w3 = 1.0 - w1 - wd\n",
        "        if w3 < 0.0 or w3 > 1.0:\n",
        "            continue\n",
        "        for a in alphas:\n",
        "            w2b = wd * a\n",
        "            w2 = wd - w2b\n",
        "            z = w1*z1 + w2*z2 + w2b*z2b + w3*z3\n",
        "            auc = roc_auc_score(y, z)\n",
        "            cnt += 1\n",
        "            if auc > best_auc:\n",
        "                best_auc = auc; best_cfg = (float(w1), float(w2), float(w2b), float(w3))\n",
        "print(f'4-way split-dense search tried {cnt} combos | best weights (LR, DenseV1, DenseV2, Meta)={best_cfg} OOF AUC(z): {best_auc:.5f} | {time.time()-t0:.1f}s')\n",
        "\n",
        "# Optional micro-refine around best\n",
        "w1b, w2b1, w2b2, w3b = best_cfg\n",
        "def refine(w1c, w2c1, w2c2, w3c, window=0.006, step=0.0005):\n",
        "    best_local_auc, best_local_w = -1.0, None\n",
        "    # Reparam via (w1, wd, alpha), with wd = w2c1+w2c2, alpha = w2c2/wd (guard wd=0)\n",
        "    wd_c = w2c1 + w2c2\n",
        "    alpha_c = (w2c2 / wd_c) if wd_c > 0 else 0.0\n",
        "    w1_min = max(0.0, w1c - window); w1_max = min(1.0, w1c + window)\n",
        "    wd_min = max(0.0, wd_c - window); wd_max = min(1.0, wd_c + window)\n",
        "    a_min = max(0.0, alpha_c - 0.05); a_max = min(0.6, alpha_c + 0.05)\n",
        "    w1_grid = np.arange(w1_min, w1_max + 1e-12, step)\n",
        "    wd_grid = np.arange(wd_min, wd_max + 1e-12, step)\n",
        "    a_grid = np.arange(a_min, a_max + 1e-12, step*5)\n",
        "    t1s = time.time(); cnt2 = 0\n",
        "    for w1 in w1_grid:\n",
        "        for wd in wd_grid:\n",
        "            w3 = 1.0 - w1 - wd\n",
        "            if w3 < 0.0 or w3 > 1.0:\n",
        "                continue\n",
        "            for a in a_grid:\n",
        "                w2v2 = wd * a\n",
        "                w2v1 = wd - w2v2\n",
        "                z = w1*z1 + w2v1*z2 + w2v2*z2b + w3*z3\n",
        "                auc = roc_auc_score(y, z)\n",
        "                cnt2 += 1\n",
        "                if auc > best_local_auc:\n",
        "                    best_local_auc = auc; best_local_w = (float(w1), float(w2v1), float(w2v2), float(w3))\n",
        "    print(f'Refine ({cnt2} combos) best weights={best_local_w} OOF AUC(z): {best_local_auc:.5f} | {time.time()-t1s:.1f}s')\n",
        "    return best_local_auc, best_local_w\n",
        "\n",
        "auc_ref, w_ref = refine(w1b, w2b1, w2b2, w3b, window=0.006, step=0.0005)\n",
        "use_auc = auc_ref if auc_ref > best_auc else best_auc\n",
        "use_w = w_ref if auc_ref > best_auc else best_cfg\n",
        "print(f'Chosen 4-way split-dense config: weights={use_w} OOF AUC(z): {use_auc:.5f}')\n",
        "\n",
        "# If threshold reached, write submission\n",
        "if use_auc >= 0.69210:\n",
        "    w1f, w2f1, w2f2, w3f = use_w\n",
        "    zt = w1f*tz1 + w2f1*tz2 + w2f2*tz2b + w3f*tz3\n",
        "    pt = sigmoid(zt).astype(np.float32)\n",
        "    sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (4-way split-dense logit blend); head:')\n",
        "    print(sub.head())\n",
        "else:\n",
        "    print('4-way split-dense logit blend < 0.69210; keeping current best submission.')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached OOF/test predictions (LR, Dense v1, Dense v2, Meta)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4-way split-dense search tried 15129 combos | best weights (LR, DenseV1, DenseV2, Meta)=(0.3441, 0.28002, 0.15078000000000003, 0.2250999999999999) OOF AUC(z): 0.69240 | 29.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refine (25625 combos) best weights=(0.3381, 0.26678599999999997, 0.16351400000000005, 0.23159999999999992) OOF AUC(z): 0.69242 | 49.9s\nChosen 4-way split-dense config: weights=(0.3381, 0.26678599999999997, 0.16351400000000005, 0.23159999999999992) OOF AUC(z): 0.69242\nSaved submission.csv (4-way split-dense logit blend); head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.331806\n1   t3_roiuw                  0.217080\n2   t3_mjnbq                  0.213391\n3   t3_t8wd1                  0.209035\n4  t3_1m4zxu                  0.215080\n"
          ]
        }
      ]
    },
    {
      "id": "f01918d2-fe7e-45b2-a8ad-6560725fa41f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Per-fold logit standardization + per-fold weight optimization (4-way: LR, Dense v1, Dense v2, Meta); average test over folds\n",
        "import numpy as np, pandas as pd, time, sys\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "print('Loading cached OOF/test predictions for 4-way (LR, Dense v1, Dense v2, Meta)...')\n",
        "o1 = np.load('oof_lr_pivot.npy')\n",
        "o2 = np.load('oof_xgb_dense.npy')\n",
        "o2b = np.load('oof_xgb_dense_v2.npy')\n",
        "o3 = np.load('oof_xgb_meta.npy')\n",
        "t1 = np.load('test_lr_pivot.npy')\n",
        "t2 = np.load('test_xgb_dense.npy')\n",
        "t2b = np.load('test_xgb_dense_v2.npy')\n",
        "t3 = np.load('test_xgb_meta.npy')\n",
        "y = train['requester_received_pizza'].astype(int).values\n",
        "n = len(y)\n",
        "\n",
        "# Convert to logits once\n",
        "z1_all, z2_all, z2b_all, z3_all = to_logit(o1), to_logit(o2), to_logit(o2b), to_logit(o3)\n",
        "tz1_all, tz2_all, tz2b_all, tz3_all = to_logit(t1), to_logit(t2), to_logit(t2b), to_logit(t3)\n",
        "\n",
        "# Global best 4-way logit weights found earlier (Cell 31):\n",
        "w_best_global = (0.3381, 0.266786, 0.163514, 0.2316)  # (LR, Dense v1, Dense v2, Meta)\n",
        "print('Starting per-fold search around global best weights:', w_best_global)\n",
        "\n",
        "cv = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(np.arange(n), y))\n",
        "oof_blend = np.zeros(n, dtype=np.float64)\n",
        "test_fold_preds = []\n",
        "\n",
        "t_start = time.time()\n",
        "for k, (tr_idx, va_idx) in enumerate(cv):\n",
        "    t0 = time.time()\n",
        "    # Per-fold standardization (z-score) using train indices stats\n",
        "    def zscore(arr, mean, std):\n",
        "        std = np.where(std == 0, 1.0, std)\n",
        "        return (arr - mean) / std\n",
        "\n",
        "    z1_tr, z2_tr, z2b_tr, z3_tr = z1_all[tr_idx], z2_all[tr_idx], z2b_all[tr_idx], z3_all[tr_idx]\n",
        "    z1_va, z2_va, z2b_va, z3_va = z1_all[va_idx], z2_all[va_idx], z2b_all[va_idx], z3_all[va_idx]\n",
        "\n",
        "    m1, s1 = z1_tr.mean(), z1_tr.std(ddof=0)\n",
        "    m2, s2 = z2_tr.mean(), z2_tr.std(ddof=0)\n",
        "    m2b, s2b = z2b_tr.mean(), z2b_tr.std(ddof=0)\n",
        "    m3, s3 = z3_tr.mean(), z3_tr.std(ddof=0)\n",
        "\n",
        "    z1_tr_s = zscore(z1_tr, m1, s1); z1_va_s = zscore(z1_va, m1, s1); tz1_s = zscore(tz1_all, m1, s1)\n",
        "    z2_tr_s = zscore(z2_tr, m2, s2); z2_va_s = zscore(z2_va, m2, s2); tz2_s = zscore(tz2_all, m2, s2)\n",
        "    z2b_tr_s = zscore(z2b_tr, m2b, s2b); z2b_va_s = zscore(z2b_va, m2b, s2b); tz2b_s = zscore(tz2b_all, m2b, s2b)\n",
        "    z3_tr_s = zscore(z3_tr, m3, s3); z3_va_s = zscore(z3_va, m3, s3); tz3_s = zscore(tz3_all, m3, s3)\n",
        "\n",
        "    # Per-fold tiny grid search around global best (window=0.02, step=0.001); ensure non-neg and sum=1\n",
        "    base_w1, base_w2, base_w2b, base_w3 = w_best_global\n",
        "    w1_min, w1_max = max(0.0, base_w1 - 0.02), min(1.0, base_w1 + 0.02)\n",
        "    w2_min, w2_max = max(0.0, base_w2 - 0.02), min(1.0, base_w2 + 0.02)\n",
        "    w2b_min, w2b_max = max(0.0, base_w2b - 0.02), min(1.0, base_w2b + 0.02)\n",
        "    step = 0.001\n",
        "    w1_grid = np.arange(w1_min, w1_max + 1e-12, step)\n",
        "    w2_grid = np.arange(w2_min, w2_max + 1e-12, step)\n",
        "    w2b_grid = np.arange(w2b_min, w2b_max + 1e-12, step)\n",
        "\n",
        "    best_auc_k, best_w_k = -1.0, w_best_global\n",
        "    y_tr = y[tr_idx]\n",
        "    tried = 0\n",
        "    for w1 in w1_grid:\n",
        "        for w2 in w2_grid:\n",
        "            for w2b in w2b_grid:\n",
        "                w3 = 1.0 - (w1 + w2 + w2b)\n",
        "                if w3 < 0.0 or w3 > 1.0:\n",
        "                    continue\n",
        "                z_tr_combo = w1*z1_tr_s + w2*z2_tr_s + w2b*z2b_tr_s + w3*z3_tr_s\n",
        "                auc_k = roc_auc_score(y_tr, z_tr_combo)\n",
        "                tried += 1\n",
        "                if auc_k > best_auc_k:\n",
        "                    best_auc_k = auc_k; best_w_k = (float(w1), float(w2), float(w2b), float(w3))\n",
        "    # Apply to validation and test (fold-normalized)\n",
        "    w1k, w2k, w2bk, w3k = best_w_k\n",
        "    z_va_combo = w1k*z1_va_s + w2k*z2_va_s + w2bk*z2b_va_s + w3k*z3_va_s\n",
        "    z_te_combo = w1k*tz1_s + w2k*tz2_s + w2bk*tz2b_s + w3k*tz3_s\n",
        "    oof_blend[va_idx] = z_va_combo\n",
        "    test_fold_preds.append(z_te_combo.astype(np.float64))\n",
        "    print(f'Fold {k+1}/5 | tried={tried} | best fold-train AUC(z)={best_auc_k:.5f} | weights={best_w_k} | time {time.time()-t0:.1f}s')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "# Evaluate OOF AUC using z (monotonic wrt prob) and also prob for completeness\n",
        "oof_auc_z = roc_auc_score(y, oof_blend)\n",
        "oof_auc_p = roc_auc_score(y, sigmoid(oof_blend))\n",
        "print(f'Per-fold standardized 4-way blend OOF AUC(z)={oof_auc_z:.5f} | AUC(prob)={oof_auc_p:.5f} | total {time.time()-t_start:.1f}s')\n",
        "\n",
        "# Average test logits across folds, convert to prob, and write submission\n",
        "test_mean_z = np.mean(test_fold_preds, axis=0)\n",
        "pt = sigmoid(test_mean_z).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (per-fold z-std 4-way blend); head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached OOF/test predictions for 4-way (LR, Dense v1, Dense v2, Meta)...\nStarting per-fold search around global best weights: (0.3381, 0.266786, 0.163514, 0.2316)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5 | tried=68921 | best fold-train AUC(z)=0.69202 | weights=(0.3461, 0.2857860000000001, 0.143514, 0.2245999999999998) | time 128.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 | tried=68921 | best fold-train AUC(z)=0.69544 | weights=(0.35810000000000003, 0.25178600000000007, 0.17251400000000003, 0.2175999999999998) | time 123.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 | tried=68921 | best fold-train AUC(z)=0.68510 | weights=(0.35810000000000003, 0.24778600000000003, 0.18351400000000004, 0.2105999999999999) | time 123.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 | tried=68921 | best fold-train AUC(z)=0.70456 | weights=(0.3541, 0.2597860000000001, 0.16351400000000002, 0.2225999999999998) | time 124.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 | tried=68921 | best fold-train AUC(z)=0.68544 | weights=(0.3241, 0.24678600000000003, 0.147514, 0.28159999999999996) | time 124.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-fold standardized 4-way blend OOF AUC(z)=0.69143 | AUC(prob)=0.69143 | total 622.8s\nSaved submission.csv (per-fold z-std 4-way blend); head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.716866\n1   t3_roiuw                  0.498893\n2   t3_mjnbq                  0.485197\n3   t3_t8wd1                  0.468800\n4  t3_1m4zxu                  0.477995\n"
          ]
        }
      ]
    },
    {
      "id": "210e0ca6-beaa-4539-8eb6-d867a3cfd731",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Weight bagging of nearby 4-way logit optima + constrained pruning checks; write best submission\n",
        "import numpy as np, pandas as pd, time, sys\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "print('Loading cached OOF/test predictions (LR, Dense v1, Dense v2, Meta)...')\n",
        "o1 = np.load('oof_lr_pivot.npy'); t1 = np.load('test_lr_pivot.npy')\n",
        "o2 = np.load('oof_xgb_dense.npy'); t2 = np.load('test_xgb_dense.npy')\n",
        "o2b = np.load('oof_xgb_dense_v2.npy'); t2b = np.load('test_xgb_dense_v2.npy')\n",
        "o3 = np.load('oof_xgb_meta.npy'); t3 = np.load('test_xgb_meta.npy')\n",
        "y = train['requester_received_pizza'].astype(int).values\n",
        "\n",
        "# Convert to logits\n",
        "z1, z2, z2b, z3 = to_logit(o1), to_logit(o2), to_logit(o2b), to_logit(o3)\n",
        "tz1, tz2, tz2b, tz3 = to_logit(t1), to_logit(t2), to_logit(t2b), to_logit(t3)\n",
        "\n",
        "# Two nearby strong 4-way configs from Cell 31\n",
        "w_ref = (0.3381, 0.266786, 0.163514, 0.2316)  # best refine\n",
        "w_alt = (0.3441, 0.28002, 0.15078, 0.22510)  # pre-refine top\n",
        "\n",
        "def auc_for_weights(w):\n",
        "    w1, w2, w2b, w3 = w\n",
        "    z = w1*z1 + w2*z2 + w2b*z2b + w3*z3\n",
        "    return roc_auc_score(y, z)\n",
        "\n",
        "def test_probs_for_weights(w):\n",
        "    w1, w2, w2b, w3 = w\n",
        "    zt = w1*tz1 + w2*tz2 + w2b*tz2b + w3*tz3\n",
        "    return sigmoid(zt).astype(np.float32)\n",
        "\n",
        "auc_ref = auc_for_weights(w_ref); auc_alt = auc_for_weights(w_alt)\n",
        "print(f'Config REF OOF AUC(z)={auc_ref:.5f} weights={w_ref}')\n",
        "print(f'Config ALT OOF AUC(z)={auc_alt:.5f} weights={w_alt}')\n",
        "\n",
        "# Bag the two configs: average test probs; OOF via averaging logits (safer for AUC ranking)\n",
        "z_ref = w_ref[0]*z1 + w_ref[1]*z2 + w_ref[2]*z2b + w_ref[3]*z3\n",
        "z_alt = w_alt[0]*z1 + w_alt[1]*z2 + w_alt[2]*z2b + w_alt[3]*z3\n",
        "z_bag = 0.5*(z_ref + z_alt)\n",
        "auc_bag = roc_auc_score(y, z_bag)\n",
        "print(f'Bagged (2-config) OOF AUC(z)={auc_bag:.5f}')\n",
        "\n",
        "# Constrained pruning checks around w_ref:\n",
        "def constrained_search(prune='meta', window=0.01, step=0.001):\n",
        "    base = w_ref\n",
        "    best_auc, best_w = -1.0, None\n",
        "    w1c, w2c, w2bc, w3c = base\n",
        "    if prune == 'meta':\n",
        "        # set w3=0, search w1,w2,w2b s.t. sum=1\n",
        "        w1_grid = np.arange(max(0, w1c-window), min(1, w1c+window)+1e-12, step)\n",
        "        w2_grid = np.arange(max(0, w2c-window), min(1, w2c+window)+1e-12, step)\n",
        "        w2b_grid= np.arange(max(0, w2bc-window),min(1, w2bc+window)+1e-12, step)\n",
        "        tried=0\n",
        "        for w1v in w1_grid:\n",
        "            for w2v in w2_grid:\n",
        "                for w2bv in w2b_grid:\n",
        "                    s = w1v + w2v + w2bv\n",
        "                    if s <= 0 or s > 1.0: continue\n",
        "                    z = w1v*z1 + w2v*z2 + w2bv*z2b  # w3=0\n",
        "                    auc = roc_auc_score(y, z)\n",
        "                    tried += 1\n",
        "                    if auc > best_auc:\n",
        "                        best_auc, best_w = auc, (float(w1v), float(w2v), float(w2bv), 0.0)\n",
        "        print(f'Prune meta tried={tried} best OOF AUC(z)={best_auc:.5f} weights={best_w}')\n",
        "        return best_auc, best_w\n",
        "    elif prune == 'dense_v2':\n",
        "        # set w2b=0, search w1,w2,w3 sum=1\n",
        "        w1_grid = np.arange(max(0, w1c-window), min(1, w1c+window)+1e-12, step)\n",
        "        w2_grid = np.arange(max(0, w2c-window), min(1, w2c+window)+1e-12, step)\n",
        "        tried=0\n",
        "        for w1v in w1_grid:\n",
        "            for w2v in w2_grid:\n",
        "                w3v = 1.0 - w1v - w2v\n",
        "                if w3v < 0 or w3v > 1: continue\n",
        "                z = w1v*z1 + w2v*z2 + w3v*z3  # w2b=0\n",
        "                auc = roc_auc_score(y, z)\n",
        "                tried += 1\n",
        "                if auc > best_auc:\n",
        "                    best_auc, best_w = auc, (float(w1v), float(w2v), 0.0, float(w3v))\n",
        "        print(f'Prune dense_v2 tried={tried} best OOF AUC(z)={best_auc:.5f} weights={best_w}')\n",
        "        return best_auc, best_w\n",
        "    else:\n",
        "        return -1.0, None\n",
        "\n",
        "auc_prune_meta, w_prune_meta = constrained_search('meta', window=0.01, step=0.001)\n",
        "auc_prune_dv2,  w_prune_dv2  = constrained_search('dense_v2', window=0.01, step=0.001)\n",
        "\n",
        "# Choose best among ref, alt, bag, prune variants\n",
        "candidates = [\n",
        "    ('ref', auc_ref, w_ref),\n",
        "    ('alt', auc_alt, w_alt),\n",
        "    ('bag2', auc_bag, None),\n",
        "    ('prune_meta', auc_prune_meta, w_prune_meta),\n",
        "    ('prune_dense_v2', auc_prune_dv2, w_prune_dv2),\n",
        "]\n",
        "candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "print('Candidates ranked (name, OOF AUC(z)):', [(n, round(a,5)) for n,a,_ in candidates])\n",
        "\n",
        "# Build submission for the top candidate\n",
        "top_name, top_auc, top_w = candidates[0]\n",
        "if top_name == 'bag2':\n",
        "    pt = 0.5*sigmoid(w_ref[0]*tz1 + w_ref[1]*tz2 + w_ref[2]*tz2b + w_ref[3]*tz3) + \\\n",
        "         0.5*sigmoid(w_alt[0]*tz1 + w_alt[1]*tz2 + w_alt[2]*tz2b + w_alt[3]*tz3)\n",
        "    pt = pt.astype(np.float32)\n",
        "    sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print(f'Saved submission.csv (bagged two 4-way configs); top OOF AUC(z)={top_auc:.5f}; head:')\n",
        "    print(sub.head())\n",
        "else:\n",
        "    w1, w2, w2b, w3 = top_w\n",
        "    zt = w1*tz1 + w2*tz2 + w2b*tz2b + w3*tz3\n",
        "    pt = sigmoid(zt).astype(np.float32)\n",
        "    sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print(f'Saved submission.csv ({top_name}); OOF AUC(z)={top_auc:.5f}; weights={top_w}; head:')\n",
        "    print(sub.head())"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached OOF/test predictions (LR, Dense v1, Dense v2, Meta)...\nConfig REF OOF AUC(z)=0.69242 weights=(0.3381, 0.266786, 0.163514, 0.2316)\nConfig ALT OOF AUC(z)=0.69240 weights=(0.3441, 0.28002, 0.15078, 0.2251)\nBagged (2-config) OOF AUC(z)=0.69238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prune meta tried=9261 best OOF AUC(z)=0.69076 weights=(0.3441, 0.258786, 0.160514, 0.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prune dense_v2 tried=441 best OOF AUC(z)=0.69107 weights=(0.3471, 0.27678600000000003, 0.0, 0.376114)\nCandidates ranked (name, OOF AUC(z)): [('ref', 0.69242), ('alt', 0.6924), ('bag2', 0.69238), ('prune_dense_v2', 0.69107), ('prune_meta', 0.69076)]\nSaved submission.csv (ref); OOF AUC(z)=0.69242; weights=(0.3381, 0.266786, 0.163514, 0.2316); head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.331806\n1   t3_roiuw                  0.217080\n2   t3_mjnbq                  0.213391\n3   t3_t8wd1                  0.209035\n4  t3_1m4zxu                  0.215080\n"
          ]
        }
      ]
    },
    {
      "id": "d9eb2b54-756f-4fc7-86aa-75a62167ee3b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Add fast NB counts model and 5-way logit blend with small NB weight cap\n",
        "import numpy as np, pandas as pd, time, sys, gc\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "assert 'combine_raw_text' in globals(), 'Need combine_raw_text from earlier pivot cell'\n",
        "\n",
        "print('Training fast MultinomialNB on word(1,2) counts (min_df=3, max_features=40000)...')\n",
        "y = train['requester_received_pizza'].astype(int).values\n",
        "texts_tr = combine_raw_text(train).fillna('').astype(str)\n",
        "texts_te = combine_raw_text(test).fillna('').astype(str)\n",
        "\n",
        "# CV for OOF NB preds\n",
        "cv = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(np.arange(len(y)), y))\n",
        "oof_nb = np.zeros(len(y), dtype=np.float32)\n",
        "te_nb_folds = []\n",
        "alphas = [0.5, 1.0, 2.0]\n",
        "vec = CountVectorizer(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_features=40000, binary=False)\n",
        "\n",
        "for k, (tr_idx, va_idx) in enumerate(cv):\n",
        "    t0 = time.time()\n",
        "    tr_txt = texts_tr.iloc[tr_idx]\n",
        "    va_txt = texts_tr.iloc[va_idx]\n",
        "    Xtr = vec.fit_transform(tr_txt)\n",
        "    Xva = vec.transform(va_txt)\n",
        "    Xte = vec.transform(texts_te)\n",
        "    best_auc_k, best_va, best_te = -1.0, None, None\n",
        "    for a in alphas:\n",
        "        nb = MultinomialNB(alpha=a)\n",
        "        nb.fit(Xtr, y[tr_idx])\n",
        "        va_p = nb.predict_proba(Xva)[:,1].astype(np.float32)\n",
        "        auc_k = roc_auc_score(y[va_idx], va_p)\n",
        "        if auc_k > best_auc_k:\n",
        "            best_auc_k = auc_k\n",
        "            best_va = va_p\n",
        "            best_te = nb.predict_proba(Xte)[:,1].astype(np.float32)\n",
        "    oof_nb[va_idx] = best_va\n",
        "    te_nb_folds.append(best_te)\n",
        "    print(f'Fold {k+1}/5 NB best AUC: {best_auc_k:.5f} | time {time.time()-t0:.1f}s')\n",
        "    sys.stdout.flush()\n",
        "    del Xtr, Xva, Xte, nb\n",
        "    gc.collect()\n",
        "\n",
        "auc_nb = roc_auc_score(y, oof_nb)\n",
        "print(f'NB counts OOF AUC: {auc_nb:.5f}')\n",
        "test_nb = np.mean(te_nb_folds, axis=0).astype(np.float32)\n",
        "np.save('oof_nb_counts.npy', oof_nb)\n",
        "np.save('test_nb_counts.npy', test_nb)\n",
        "\n",
        "# 5-way logit blend: base 4-way weights from Cell 31; small NB weight cap\n",
        "print('5-way logit blend search with small NB cap...')\n",
        "o1 = np.load('oof_lr_pivot.npy')\n",
        "o2 = np.load('oof_xgb_dense.npy')\n",
        "o2b = np.load('oof_xgb_dense_v2.npy')\n",
        "o3 = np.load('oof_xgb_meta.npy')\n",
        "o4 = oof_nb\n",
        "t1 = np.load('test_lr_pivot.npy')\n",
        "t2 = np.load('test_xgb_dense.npy')\n",
        "t2b = np.load('test_xgb_dense_v2.npy')\n",
        "t3 = np.load('test_xgb_meta.npy')\n",
        "t4 = test_nb\n",
        "\n",
        "z1, z2, z2b, z3, z4 = to_logit(o1), to_logit(o2), to_logit(o2b), to_logit(o3), to_logit(o4)\n",
        "tz1, tz2, tz2b, tz3, tz4 = to_logit(t1), to_logit(t2), to_logit(t2b), to_logit(t3), to_logit(t4)\n",
        "\n",
        "# Base best 4-way weights from Cell 31 refine:\n",
        "w_lr0, w_d10, w_d20, w_meta0 = 0.3381, 0.266786, 0.163514, 0.2316\n",
        "\n",
        "# Grids\n",
        "w1_grid = np.arange(max(0.0, w_lr0 - 0.01), min(1.0, w_lr0 + 0.01) + 1e-12, 0.001)\n",
        "w2_grid = np.arange(max(0.0, w_d10 - 0.01), min(1.0, w_d10 + 0.01) + 1e-12, 0.001)\n",
        "w2b_grid= np.arange(max(0.0, w_d20 - 0.01), min(1.0, w_d20 + 0.01) + 1e-12, 0.001)\n",
        "w4_grid = np.arange(0.0, 0.08001, 0.005)  # NB cap 8%\n",
        "\n",
        "best_auc, best_w = -1.0, None\n",
        "t0 = time.time(); tried = 0\n",
        "for w4 in w4_grid:\n",
        "    rem = 1.0 - w4\n",
        "    for w1 in w1_grid:\n",
        "        for w2 in w2_grid:\n",
        "            for w2b in w2b_grid:\n",
        "                s = w1 + w2 + w2b\n",
        "                if s > rem: continue\n",
        "                w3 = rem - s\n",
        "                z = w1*z1 + w2*z2 + w2b*z2b + w3*z3 + w4*z4\n",
        "                auc = roc_auc_score(y, z)\n",
        "                tried += 1\n",
        "                if auc > best_auc:\n",
        "                    best_auc = auc\n",
        "                    # store normalized weights (sum to 1 across all 5)\n",
        "                    best_w = (float(w1), float(w2), float(w2b), float(w3), float(w4))\n",
        "print(f'5-way search tried {tried} combos | best OOF AUC(z)={best_auc:.5f} | weights(LR,D1,D2,Meta,NB)={best_w} | {time.time()-t0:.1f}s')\n",
        "\n",
        "if best_w is not None and best_auc >= 0.69210:\n",
        "    w1, w2, w2b, w3, w4 = best_w\n",
        "    zt = w1*tz1 + w2*tz2 + w2b*tz2b + w3*tz3 + w4*tz4\n",
        "    pt = sigmoid(zt).astype(np.float32)\n",
        "    sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (5-way logit blend with NB); head:')\n",
        "    print(sub.head())\n",
        "else:\n",
        "    print('5-way blend did not reach threshold; keeping current best submission.')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fast MultinomialNB on word(1,2) counts (min_df=3, max_features=40000)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5 NB best AUC: 0.57436 | time 0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 NB best AUC: 0.56471 | time 0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 NB best AUC: 0.55392 | time 0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 NB best AUC: 0.54240 | time 0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 NB best AUC: 0.57940 | time 0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB counts OOF AUC: 0.56205\n5-way logit blend search with small NB cap...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-way search tried 157437 combos | best OOF AUC(z)=0.69600 | weights(LR,D1,D2,Meta,NB)=(0.3281, 0.259786, 0.156514, 0.23560000000000003, 0.02) | 313.6s\nSaved submission.csv (5-way logit blend with NB); head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.294011\n1   t3_roiuw                  0.217789\n2   t3_mjnbq                  0.192705\n3   t3_t8wd1                  0.188907\n4  t3_1m4zxu                  0.206851\n"
          ]
        }
      ]
    },
    {
      "id": "0dfa17a2-84cc-4baa-9fc4-ce6a198cda9f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Per-fold logit weight optimization without standardization (4-way); average test over folds\n",
        "import numpy as np, pandas as pd, time, sys\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "print('Loading cached OOF/test predictions (LR, Dense v1, Dense v2, Meta)...')\n",
        "o1 = np.load('oof_lr_pivot.npy')\n",
        "o2 = np.load('oof_xgb_dense.npy')\n",
        "o2b = np.load('oof_xgb_dense_v2.npy')\n",
        "o3 = np.load('oof_xgb_meta.npy')\n",
        "t1 = np.load('test_lr_pivot.npy')\n",
        "t2 = np.load('test_xgb_dense.npy')\n",
        "t2b = np.load('test_xgb_dense_v2.npy')\n",
        "t3 = np.load('test_xgb_meta.npy')\n",
        "y = train['requester_received_pizza'].astype(int).values\n",
        "n = len(y)\n",
        "\n",
        "# Convert to logits once\n",
        "z1_all, z2_all, z2b_all, z3_all = to_logit(o1), to_logit(o2), to_logit(o2b), to_logit(o3)\n",
        "tz1_all, tz2_all, tz2b_all, tz3_all = to_logit(t1), to_logit(t2), to_logit(t2b), to_logit(t3)\n",
        "\n",
        "# Global best 4-way logit weights (from Cell 31 refine)\n",
        "w_best_global = (0.3381, 0.266786, 0.163514, 0.2316)  # (LR, Dense v1, Dense v2, Meta)\n",
        "print('Per-fold search around global best weights:', w_best_global)\n",
        "\n",
        "cv = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(np.arange(n), y))\n",
        "oof_blend = np.zeros(n, dtype=np.float64)\n",
        "test_fold_logits = []\n",
        "\n",
        "t_start = time.time()\n",
        "for k, (tr_idx, va_idx) in enumerate(cv):\n",
        "    t0 = time.time()\n",
        "    z1_tr, z2_tr, z2b_tr, z3_tr = z1_all[tr_idx], z2_all[tr_idx], z2b_all[tr_idx], z3_all[tr_idx]\n",
        "    z1_va, z2_va, z2b_va, z3_va = z1_all[va_idx], z2_all[va_idx], z3_all[va_idx], z3_all[va_idx]  # placeholder\n",
        "    # fix z2b_va, z3_va assignment (typo guard)\n",
        "    z2b_va = z2b_all[va_idx]; z3_va = z3_all[va_idx]\n",
        "    tz1, tz2, tz2b, tz3 = tz1_all, tz2_all, tz2b_all, tz3_all\n",
        "\n",
        "    # Tiny window around global best (no standardization), step 0.001\n",
        "    base_w1, base_w2, base_w2b, base_w3 = w_best_global\n",
        "    w1_min, w1_max = max(0.0, base_w1 - 0.01), min(1.0, base_w1 + 0.01)\n",
        "    w2_min, w2_max = max(0.0, base_w2 - 0.01), min(1.0, base_w2 + 0.01)\n",
        "    w2b_min, w2b_max = max(0.0, base_w2b - 0.01), min(1.0, base_w2b + 0.01)\n",
        "    step = 0.001\n",
        "    w1_grid = np.arange(w1_min, w1_max + 1e-12, step)\n",
        "    w2_grid = np.arange(w2_min, w2_max + 1e-12, step)\n",
        "    w2b_grid = np.arange(w2b_min, w2b_max + 1e-12, step)\n",
        "\n",
        "    best_auc_k, best_w_k = -1.0, w_best_global\n",
        "    y_tr = y[tr_idx]\n",
        "    tried = 0\n",
        "    for w1 in w1_grid:\n",
        "        for w2 in w2_grid:\n",
        "            for w2b in w2b_grid:\n",
        "                w3 = 1.0 - (w1 + w2 + w2b)\n",
        "                if w3 < 0.0 or w3 > 1.0:\n",
        "                    continue\n",
        "                z_tr_combo = w1*z1_tr + w2*z2_tr + w2b*z2b_tr + w3*z3_tr\n",
        "                auc_k = roc_auc_score(y_tr, z_tr_combo)\n",
        "                tried += 1\n",
        "                if auc_k > best_auc_k:\n",
        "                    best_auc_k = auc_k; best_w_k = (float(w1), float(w2), float(w2b), float(w3))\n",
        "\n",
        "    w1k, w2k, w2bk, w3k = best_w_k\n",
        "    z_va_combo = w1k*z1_va + w2k*z2_va + w2bk*z2b_va + w3k*z3_va\n",
        "    z_te_combo = w1k*tz1 + w2k*tz2 + w2bk*tz2b + w3k*tz3\n",
        "    oof_blend[va_idx] = z_va_combo\n",
        "    test_fold_logits.append(z_te_combo.astype(np.float64))\n",
        "    print(f'Fold {k+1}/5 | tried={tried} | best fold-train AUC(z)={best_auc_k:.5f} | weights={best_w_k} | time {time.time()-t0:.1f}s')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "# Evaluate OOF\n",
        "oof_auc_z = roc_auc_score(y, oof_blend)\n",
        "oof_auc_p = roc_auc_score(y, sigmoid(oof_blend))\n",
        "print(f'Per-fold 4-way (no z-std) OOF AUC(z)={oof_auc_z:.5f} | AUC(prob)={oof_auc_p:.5f} | total {time.time()-t_start:.1f}s')\n",
        "\n",
        "# Average test logits across folds\n",
        "test_mean_z = np.mean(test_fold_logits, axis=0)\n",
        "pt = sigmoid(test_mean_z).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (per-fold 4-way no-zstd); head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached OOF/test predictions (LR, Dense v1, Dense v2, Meta)...\nPer-fold search around global best weights: (0.3381, 0.266786, 0.163514, 0.2316)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5 | tried=9261 | best fold-train AUC(z)=0.69213 | weights=(0.3451, 0.27578600000000003, 0.160514, 0.2185999999999999) | time 16.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 | tried=9261 | best fold-train AUC(z)=0.69554 | weights=(0.3451, 0.27578600000000003, 0.159514, 0.21960000000000002) | time 17.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 | tried=9261 | best fold-train AUC(z)=0.68520 | weights=(0.3441, 0.263786, 0.173514, 0.2185999999999999) | time 17.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 | tried=9261 | best fold-train AUC(z)=0.70448 | weights=(0.3291, 0.27478600000000003, 0.159514, 0.23659999999999992) | time 16.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 | tried=9261 | best fold-train AUC(z)=0.68542 | weights=(0.3341, 0.265786, 0.167514, 0.23259999999999992) | time 17.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-fold 4-way (no z-std) OOF AUC(z)=0.69225 | AUC(prob)=0.69225 | total 85.3s\nSaved submission.csv (per-fold 4-way no-zstd); head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.331706\n1   t3_roiuw                  0.216851\n2   t3_mjnbq                  0.213342\n3   t3_t8wd1                  0.208793\n4  t3_1m4zxu                  0.214304\n"
          ]
        }
      ]
    },
    {
      "id": "08324201-ac73-4154-b48d-d8f22f877228",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Tiny 4-way logit temperature scaling + narrow weight refinement around best ref weights; write submission if improves\n",
        "import numpy as np, pandas as pd, time, sys\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "print('Loading cached OOF/test predictions (LR, Dense v1, Dense v2, Meta)...')\n",
        "o1 = np.load('oof_lr_pivot.npy')\n",
        "o2 = np.load('oof_xgb_dense.npy')\n",
        "o2b = np.load('oof_xgb_dense_v2.npy')\n",
        "o3 = np.load('oof_xgb_meta.npy')\n",
        "t1 = np.load('test_lr_pivot.npy')\n",
        "t2 = np.load('test_xgb_dense.npy')\n",
        "t2b = np.load('test_xgb_dense_v2.npy')\n",
        "t3 = np.load('test_xgb_meta.npy')\n",
        "y = train['requester_received_pizza'].astype(int).values\n",
        "\n",
        "# Base logits\n",
        "z1b, z2b_, z2bb, z3b = to_logit(o1), to_logit(o2), to_logit(o2b), to_logit(o3)\n",
        "tz1b, tz2b_, tz2bb, tz3b = to_logit(t1), to_logit(t2), to_logit(t2b), to_logit(t3)\n",
        "\n",
        "# Reference best 4-way weights from split-dense refine\n",
        "w_ref = (0.3381, 0.266786, 0.163514, 0.2316)  # (LR, Dense v1, Dense v2, Meta)\n",
        "\n",
        "scales = [0.95, 1.00, 1.05]\n",
        "w_window = 0.01\n",
        "w_step = 0.001\n",
        "\n",
        "best_auc, best_cfg = -1.0, None\n",
        "t0 = time.time(); tried = 0\n",
        "for s1 in scales:\n",
        "    for s2 in scales:\n",
        "        for s2v in scales:\n",
        "            for s3 in scales:\n",
        "                z1 = s1 * z1b; z2 = s2 * z2b_; z2v2 = s2v * z2bb; z3 = s3 * z3b\n",
        "                w1c, w2c, w2vc, w3c = w_ref\n",
        "                # Search over w1, w2, w2v in a tiny window; w3 = 1 - (w1+w2+w2v)\n",
        "                w1_min, w1_max = max(0.0, w1c - w_window), min(1.0, w1c + w_window)\n",
        "                w2_min, w2_max = max(0.0, w2c - w_window), min(1.0, w2c + w_window)\n",
        "                w2v_min, w2v_max = max(0.0, w2vc - w_window), min(1.0, w2vc + w_window)\n",
        "                w1_grid = np.arange(w1_min, w1_max + 1e-12, w_step)\n",
        "                w2_grid = np.arange(w2_min, w2_max + 1e-12, w_step)\n",
        "                w2v_grid= np.arange(w2v_min, w2v_max + 1e-12, w_step)\n",
        "                local_best_auc, local_best_w = -1.0, None\n",
        "                for w1 in w1_grid:\n",
        "                    for w2 in w2_grid:\n",
        "                        for w2v_w in w2v_grid:\n",
        "                            w3 = 1.0 - (w1 + w2 + w2v_w)\n",
        "                            if w3 < 0.0 or w3 > 1.0:\n",
        "                                continue\n",
        "                            z = w1*z1 + w2*z2 + w2v_w*z2v2 + w3*z3\n",
        "                            auc = roc_auc_score(y, z)\n",
        "                            tried += 1\n",
        "                            if auc > local_best_auc:\n",
        "                                local_best_auc = auc\n",
        "                                local_best_w = (float(w1), float(w2), float(w2v_w), float(w3))\n",
        "                if local_best_auc > best_auc:\n",
        "                    best_auc = local_best_auc\n",
        "                    best_cfg = ((s1, s2, s2v, s3), local_best_w)\n",
        "print(f'Tiny 4-way temp+weights tried ~{tried} combos | best OOF AUC(z)={best_auc:.5f} | scales={best_cfg[0]} | weights={best_cfg[1]} | {time.time()-t0:.1f}s')\n",
        "\n",
        "# Build submission with best config\n",
        "s1, s2, s2v, s3 = best_cfg[0]\n",
        "bw1, bw2, bw2v, bw3 = best_cfg[1]\n",
        "zt = bw1*(s1*tz1b) + bw2*(s2*tz2b_) + bw2v*(s2v*tz2bb) + bw3*(s3*tz3b)\n",
        "pt = sigmoid(zt).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (4-way tiny temp-scaled logit blend); head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached OOF/test predictions (LR, Dense v1, Dense v2, Meta)...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     56\u001b[39m z = w1*z1 + w2*z2 + w2v_w*z2v2 + w3*z3\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m auc = \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m tried += \u001b[32m1\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m auc > local_best_auc:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py:213\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    209\u001b[39m         skip_parameter_validation=(\n\u001b[32m    210\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    211\u001b[39m         )\n\u001b[32m    212\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    215\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    216\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    217\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    219\u001b[39m     msg = re.sub(\n\u001b[32m    220\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    221\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    222\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    223\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:640\u001b[39m, in \u001b[36mroc_auc_score\u001b[39m\u001b[34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[39m\n\u001b[32m    638\u001b[39m     labels = np.unique(y_true)\n\u001b[32m    639\u001b[39m     y_true = label_binarize(y_true, classes=labels)[:, \u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_average_binary_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_binary_roc_auc_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_fpr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_fpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# multilabel-indicator\u001b[39;00m\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _average_binary_score(\n\u001b[32m    649\u001b[39m         partial(_binary_roc_auc_score, max_fpr=max_fpr),\n\u001b[32m    650\u001b[39m         y_true,\n\u001b[32m   (...)\u001b[39m\u001b[32m    653\u001b[39m         sample_weight=sample_weight,\n\u001b[32m    654\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_base.py:76\u001b[39m, in \u001b[36m_average_binary_score\u001b[39m\u001b[34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[39m\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m format is not supported\u001b[39m\u001b[33m\"\u001b[39m.format(y_type))\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_type == \u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[32m     79\u001b[39m y_true = check_array(y_true)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:387\u001b[39m, in \u001b[36m_binary_roc_auc_score\u001b[39m\u001b[34m(y_true, y_score, sample_weight, max_fpr)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np.unique(y_true)) != \u001b[32m2\u001b[39m:\n\u001b[32m    382\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    383\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    384\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mis not defined in that case.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    385\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m fpr, tpr, _ = \u001b[43mroc_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_fpr == \u001b[32m1\u001b[39m:\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m auc(fpr, tpr)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py:186\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    184\u001b[39m global_skip_validation = get_config()[\u001b[33m\"\u001b[39m\u001b[33mskip_parameter_validation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m func_sig = signature(func)\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:1145\u001b[39m, in \u001b[36mroc_curve\u001b[39m\u001b[34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[39m\n\u001b[32m   1043\u001b[39m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[32m   1044\u001b[39m     {\n\u001b[32m   1045\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33marray-like\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1054\u001b[39m     y_true, y_score, *, pos_label=\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, drop_intermediate=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1055\u001b[39m ):\n\u001b[32m   1056\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[32m   1057\u001b[39m \n\u001b[32m   1058\u001b[39m \u001b[33;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1143\u001b[39m \u001b[33;03m    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[32m   1144\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1145\u001b[39m     fps, tps, thresholds = \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1149\u001b[39m     \u001b[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[32m   1150\u001b[39m     \u001b[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[32m   1151\u001b[39m     \u001b[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1156\u001b[39m     \u001b[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[32m   1157\u001b[39m     \u001b[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[32m   1158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) > \u001b[32m2\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:815\u001b[39m, in \u001b[36m_binary_clf_curve\u001b[39m\u001b[34m(y_true, y_score, pos_label, sample_weight)\u001b[39m\n\u001b[32m    781\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Calculate true and false positives per binary classification threshold.\u001b[39;00m\n\u001b[32m    782\u001b[39m \n\u001b[32m    783\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    812\u001b[39m \u001b[33;03m    Decreasing score values.\u001b[39;00m\n\u001b[32m    813\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    814\u001b[39m \u001b[38;5;66;03m# Check to make sure y_true is valid\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m815\u001b[39m y_type = \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43my_true\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    816\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (y_type == \u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y_type == \u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[32m    817\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m format is not supported\u001b[39m\u001b[33m\"\u001b[39m.format(y_type))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/multiclass.py:404\u001b[39m, in \u001b[36mtype_of_target\u001b[39m\u001b[34m(y, input_name)\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(first_row_or_val):\n\u001b[32m    403\u001b[39m     first_row_or_val = first_row_or_val.data\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m.shape[\u001b[32m0\u001b[39m] > \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(first_row_or_val) > \u001b[32m1\u001b[39m):\n\u001b[32m    405\u001b[39m     \u001b[38;5;66;03m# [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\u001b[39;00m\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m + suffix\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_array_api.py:407\u001b[39m, in \u001b[36m_NumPyAPIWrapper.unique_values\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munique_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/numpy/lib/arraysetops.py:274\u001b[39m, in \u001b[36munique\u001b[39m\u001b[34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[39m\n\u001b[32m    272\u001b[39m ar = np.asanyarray(ar)\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     ret = \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/numpy/lib/arraysetops.py:336\u001b[39m, in \u001b[36m_unique1d\u001b[39m\u001b[34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[39m\n\u001b[32m    334\u001b[39m     aux = ar[perm]\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     ar.sort()\n\u001b[32m    337\u001b[39m     aux = ar\n\u001b[32m    338\u001b[39m mask = np.empty(aux.shape, dtype=np.bool_)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "52492630-55f5-4bd9-858c-98103fd61eed",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ablation: LR pivot without subreddit TF-IDF; re-blend with Dense v1 + Meta in logit space\n",
        "import time, gc, sys, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "assert 'combine_raw_text' in globals() and 'clean_text_series' in globals() and 'build_meta_minimal' in globals(), 'Run pivot cell 7 first'\n",
        "assert 'word_params' in globals() and 'char_params' in globals(), 'Run pivot cell 7 first (params)'\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "n_splits = 5\n",
        "cv = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42).split(train, y))\n",
        "print(f'Prepared {n_splits}-fold StratifiedKFold CV (shuffled).')\n",
        "\n",
        "# Precompute test sources via same preprocessing as pivot\n",
        "clean_te_text = clean_text_series(combine_raw_text(test))\n",
        "meta_te_full = build_meta_minimal(test)\n",
        "\n",
        "# Containers for LR (no-subreddit) OOF/test\n",
        "oof_lr_ns = np.zeros(len(train), dtype=np.float32)\n",
        "test_lr_ns_folds = []\n",
        "LR_C = 0.5\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(cv):\n",
        "    t0 = time.time()\n",
        "    print(f'Fold {fold+1}/{n_splits} - train {len(tr_idx)} va {len(va_idx)}')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    tr_text = clean_text_series(combine_raw_text(train.loc[tr_idx]))\n",
        "    va_text = clean_text_series(combine_raw_text(train.loc[va_idx]))\n",
        "\n",
        "    # Fit only word + char TF-IDF (no subreddit vectorizer)\n",
        "    tfidf_w = TfidfVectorizer(**word_params)\n",
        "    Xw_tr = tfidf_w.fit_transform(tr_text)\n",
        "    Xw_va = tfidf_w.transform(va_text)\n",
        "    Xw_te = tfidf_w.transform(clean_te_text)\n",
        "\n",
        "    tfidf_c = TfidfVectorizer(**char_params)\n",
        "    Xc_tr = tfidf_c.fit_transform(tr_text)\n",
        "    Xc_va = tfidf_c.transform(va_text)\n",
        "    Xc_te = tfidf_c.transform(clean_te_text)\n",
        "\n",
        "    # Minimal meta\n",
        "    meta_tr = build_meta_minimal(train.loc[tr_idx])\n",
        "    meta_va = build_meta_minimal(train.loc[va_idx])\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    Xm_tr = scaler.fit_transform(meta_tr)\n",
        "    Xm_va = scaler.transform(meta_va)\n",
        "    Xm_te = scaler.transform(meta_te_full)\n",
        "\n",
        "    # Stack\n",
        "    X_tr = hstack([Xw_tr, Xc_tr, Xm_tr], format='csr')\n",
        "    X_va = hstack([Xw_va, Xc_va, Xm_va], format='csr')\n",
        "    X_te = hstack([Xw_te, Xc_te, Xm_te], format='csr')\n",
        "\n",
        "    lr = LogisticRegression(solver='saga', penalty='l2', C=LR_C, class_weight=None, max_iter=6000, n_jobs=-1, random_state=42, verbose=0)\n",
        "    lr.fit(X_tr, y[tr_idx])\n",
        "    va_lr = lr.predict_proba(X_va)[:,1].astype(np.float32)\n",
        "    oof_lr_ns[va_idx] = va_lr\n",
        "    te_lr = lr.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "    test_lr_ns_folds.append(te_lr)\n",
        "\n",
        "    auc = roc_auc_score(y[va_idx], va_lr)\n",
        "    print(f'Fold {fold+1} LR(no-subs) AUC: {auc:.5f} | time {time.time()-t0:.1f}s')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    del Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, Xm_tr, Xm_va, Xm_te, X_tr, X_va, X_te, lr, tfidf_w, tfidf_c, scaler\n",
        "    gc.collect()\n",
        "\n",
        "auc_ns = roc_auc_score(y, oof_lr_ns)\n",
        "print(f'LR(no-subreddit) OOF AUC: {auc_ns:.5f}')\n",
        "test_lr_ns = np.mean(test_lr_ns_folds, axis=0).astype(np.float32)\n",
        "np.save('oof_lr_nosub.npy', oof_lr_ns)\n",
        "np.save('test_lr_nosub.npy', test_lr_ns)\n",
        "\n",
        "# Re-blend with Dense v1 and Meta in logit space\n",
        "print('Loading cached Dense v1 and Meta OOF/test...')\n",
        "o_dense = np.load('oof_xgb_dense.npy')\n",
        "o_meta = np.load('oof_xgb_meta.npy')\n",
        "t_dense = np.load('test_xgb_dense.npy')\n",
        "t_meta = np.load('test_xgb_meta.npy')\n",
        "\n",
        "# Convert to logits\n",
        "z_lr = to_logit(oof_lr_ns); z_d = to_logit(o_dense); z_m = to_logit(o_meta)\n",
        "tz_lr = to_logit(test_lr_ns); tz_d = to_logit(t_dense); tz_m = to_logit(t_meta)\n",
        "\n",
        "def eval_auc_logit(w1, w2):\n",
        "    w3 = 1.0 - w1 - w2\n",
        "    if w3 < 0 or w3 > 1: return -1.0\n",
        "    z = w1*z_lr + w2*z_d + w3*z_m\n",
        "    return roc_auc_score(y, z)\n",
        "\n",
        "# Start around prior best (0.3504, 0.4192) but allow adaptation\n",
        "w1_c, w2_c = 0.3504, 0.4192\n",
        "best_auc, best_w = -1.0, (w1_c, w2_c, 1.0 - w1_c - w2_c)\n",
        "\n",
        "def grid_search_logit(w1_center, w2_center, window, step):\n",
        "    global best_auc, best_w\n",
        "    w1_min, w1_max = max(0.0, w1_center - window), min(1.0, w1_center + window)\n",
        "    w2_min, w2_max = max(0.0, w2_center - window), min(1.0, w2_center + window)\n",
        "    w1_grid = np.arange(w1_min, w1_max + 1e-12, step)\n",
        "    w2_grid = np.arange(w2_min, w2_max + 1e-12, step)\n",
        "    t0s = time.time(); cnt = 0\n",
        "    for w1 in w1_grid:\n",
        "        for w2 in w2_grid:\n",
        "            if w1 + w2 > 1.0: continue\n",
        "            auc = eval_auc_logit(w1, w2)\n",
        "            cnt += 1\n",
        "            if auc > best_auc:\n",
        "                best_auc = auc; best_w = (float(w1), float(w2), float(1.0 - w1 - w2))\n",
        "    print(f'Logit grid window={window} step={step} tried {cnt} | best={best_w} OOF AUC(z): {best_auc:.5f} | {time.time()-t0s:.1f}s')\n",
        "\n",
        "grid_search_logit(w1_c, w2_c, window=0.06, step=0.002)\n",
        "grid_search_logit(best_w[0], best_w[1], window=0.02, step=0.001)\n",
        "grid_search_logit(best_w[0], best_w[1], window=0.008, step=0.0005)\n",
        "print(f'Final logit weights (LR_noSubs, Dense, Meta)={best_w} | OOF AUC(z): {best_auc:.5f}')\n",
        "\n",
        "# Build submission from LR(no-subs) + Dense + Meta\n",
        "w1b, w2b, w3b = best_w\n",
        "pt = sigmoid(w1b*tz_lr + w2b*tz_d + w3b*tz_m).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (LR_noSubs + Dense + Meta logit blend); head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 5-fold StratifiedKFold CV (shuffled).\nFold 1/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 LR(no-subs) AUC: 0.67816 | time 20.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 LR(no-subs) AUC: 0.66422 | time 18.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 LR(no-subs) AUC: 0.68971 | time 19.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 LR(no-subs) AUC: 0.64905 | time 18.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 LR(no-subs) AUC: 0.70163 | time 20.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR(no-subreddit) OOF AUC: 0.67573\nLoading cached Dense v1 and Meta OOF/test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logit grid window=0.06 step=0.002 tried 3721 | best=(0.3044, 0.4372000000000001, 0.2583999999999999) OOF AUC(z): 0.69127 | 7.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logit grid window=0.02 step=0.001 tried 1681 | best=(0.2994, 0.4282000000000001, 0.2723999999999999) OOF AUC(z): 0.69128 | 3.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logit grid window=0.008 step=0.0005 tried 1089 | best=(0.2999, 0.4277000000000001, 0.27239999999999986) OOF AUC(z): 0.69128 | 2.1s\nFinal logit weights (LR_noSubs, Dense, Meta)=(0.2999, 0.4277000000000001, 0.27239999999999986) | OOF AUC(z): 0.69128\nSaved submission.csv (LR_noSubs + Dense + Meta logit blend); head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.336776\n1   t3_roiuw                  0.215132\n2   t3_mjnbq                  0.205367\n3   t3_t8wd1                  0.210039\n4  t3_1m4zxu                  0.218076\n"
          ]
        }
      ]
    },
    {
      "id": "4461083c-09f8-4526-97c5-682a2b4f679a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Per-fold 3-way logit weight optimization (LR, Dense v1, Meta) without standardization; average test logits across folds\n",
        "import numpy as np, pandas as pd, time, sys\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "print('Loading cached OOF/test predictions (LR, Dense v1, Meta)...')\n",
        "o1 = np.load('oof_lr_pivot.npy')\n",
        "o2 = np.load('oof_xgb_dense.npy')\n",
        "o3 = np.load('oof_xgb_meta.npy')\n",
        "t1 = np.load('test_lr_pivot.npy')\n",
        "t2 = np.load('test_xgb_dense.npy')\n",
        "t3 = np.load('test_xgb_meta.npy')\n",
        "y = train['requester_received_pizza'].astype(int).values\n",
        "n = len(y)\n",
        "\n",
        "# Convert to logits once\n",
        "z1_all, z2_all, z3_all = to_logit(o1), to_logit(o2), to_logit(o3)\n",
        "tz1_all, tz2_all, tz3_all = to_logit(t1), to_logit(t2), to_logit(t3)\n",
        "\n",
        "# Start around prior best 3-way logit weights (from Cell 23/24):\n",
        "w_center = (0.3501, 0.4198, 0.2301)  # (LR, Dense v1, Meta)\n",
        "window = 0.015\n",
        "step = 0.001\n",
        "print('Per-fold search around center weights:', w_center, 'window=', window, 'step=', step)\n",
        "\n",
        "cv = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(np.arange(n), y))\n",
        "oof_blend = np.zeros(n, dtype=np.float64)\n",
        "test_fold_logits = []\n",
        "\n",
        "t_start = time.time()\n",
        "for k, (tr_idx, va_idx) in enumerate(cv):\n",
        "    t0 = time.time()\n",
        "    z1_tr, z2_tr, z3_tr = z1_all[tr_idx], z2_all[tr_idx], z3_all[tr_idx]\n",
        "    z1_va, z2_va, z3_va = z1_all[va_idx], z2_all[va_idx], z3_all[va_idx]\n",
        "    tz1, tz2, tz3 = tz1_all, tz2_all, tz3_all\n",
        "\n",
        "    base_w1, base_w2, base_w3 = w_center\n",
        "    w1_min, w1_max = max(0.0, base_w1 - window), min(1.0, base_w1 + window)\n",
        "    w2_min, w2_max = max(0.0, base_w2 - window), min(1.0, base_w2 + window)\n",
        "    w1_grid = np.arange(w1_min, w1_max + 1e-12, step)\n",
        "    w2_grid = np.arange(w2_min, w2_max + 1e-12, step)\n",
        "\n",
        "    best_auc_k, best_w_k = -1.0, w_center\n",
        "    y_tr = y[tr_idx]\n",
        "    tried = 0\n",
        "    for w1 in w1_grid:\n",
        "        for w2 in w2_grid:\n",
        "            w3 = 1.0 - (w1 + w2)\n",
        "            if w3 < 0.0 or w3 > 1.0:\n",
        "                continue\n",
        "            z_tr_combo = w1*z1_tr + w2*z2_tr + w3*z3_tr\n",
        "            auc_k = roc_auc_score(y_tr, z_tr_combo)\n",
        "            tried += 1\n",
        "            if auc_k > best_auc_k:\n",
        "                best_auc_k = auc_k; best_w_k = (float(w1), float(w2), float(w3))\n",
        "\n",
        "    w1k, w2k, w3k = best_w_k\n",
        "    z_va_combo = w1k*z1_va + w2k*z2_va + w3k*z3_va\n",
        "    z_te_combo = w1k*tz1 + w2k*tz2 + w3k*tz3\n",
        "    oof_blend[va_idx] = z_va_combo\n",
        "    test_fold_logits.append(z_te_combo.astype(np.float64))\n",
        "    print(f'Fold {k+1}/5 | tried={tried} | best fold-train AUC(z)={best_auc_k:.5f} | weights={best_w_k} | time {time.time()-t0:.1f}s')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "# Evaluate OOF AUC\n",
        "oof_auc_z = roc_auc_score(y, oof_blend)\n",
        "oof_auc_p = roc_auc_score(y, sigmoid(oof_blend))\n",
        "print(f'Per-fold 3-way (no z-std) OOF AUC(z)={oof_auc_z:.5f} | AUC(prob)={oof_auc_p:.5f} | total {time.time()-t_start:.1f}s')\n",
        "\n",
        "# Average test logits across folds, convert to prob, and write submission\n",
        "test_mean_z = np.mean(test_fold_logits, axis=0)\n",
        "pt = sigmoid(test_mean_z).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (per-fold 3-way no-zstd); head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached OOF/test predictions (LR, Dense v1, Meta)...\nPer-fold search around center weights: (0.3501, 0.4198, 0.2301) window= 0.015 step= 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5 | tried=961 | best fold-train AUC(z)=0.69180 | weights=(0.3501, 0.4198, 0.23009999999999997) | time 1.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 | tried=961 | best fold-train AUC(z)=0.69551 | weights=(0.3501, 0.4198, 0.23009999999999997) | time 1.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 | tried=961 | best fold-train AUC(z)=0.68416 | weights=(0.36510000000000004, 0.4068, 0.22809999999999997) | time 1.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 | tried=961 | best fold-train AUC(z)=0.70382 | weights=(0.3411, 0.4198, 0.23909999999999998) | time 1.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 | tried=961 | best fold-train AUC(z)=0.68526 | weights=(0.3351, 0.4058, 0.2591) | time 1.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-fold 3-way (no z-std) OOF AUC(z)=0.69182 | AUC(prob)=0.69182 | total 8.6s\nSaved submission.csv (per-fold 3-way no-zstd); head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.328622\n1   t3_roiuw                  0.212896\n2   t3_mjnbq                  0.214778\n3   t3_t8wd1                  0.206003\n4  t3_1m4zxu                  0.217101\n"
          ]
        }
      ]
    },
    {
      "id": "6174790e-543c-43c1-8a1e-81747bc26ad7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Bagging two blends: global 4-way ref logit blend + per-fold 3-way logit blend; optimize alpha on OOF and average test logits\n",
        "import numpy as np, pandas as pd, time, sys\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "print('Loading cached base OOF/test preds...')\n",
        "o1 = np.load('oof_lr_pivot.npy'); t1 = np.load('test_lr_pivot.npy')\n",
        "o2 = np.load('oof_xgb_dense.npy'); t2 = np.load('test_xgb_dense.npy')\n",
        "o2b= np.load('oof_xgb_dense_v2.npy'); t2b= np.load('test_xgb_dense_v2.npy')\n",
        "o3 = np.load('oof_xgb_meta.npy'); t3 = np.load('test_xgb_meta.npy')\n",
        "y = train['requester_received_pizza'].astype(int).values\n",
        "n = len(y)\n",
        "\n",
        "# Base logits\n",
        "z1_all, z2_all, z2b_all, z3_all = to_logit(o1), to_logit(o2), to_logit(o2b), to_logit(o3)\n",
        "tz1_all, tz2_all, tz2b_all, tz3_all = to_logit(t1), to_logit(t2), to_logit(t2b), to_logit(t3)\n",
        "\n",
        "# 1) Global 4-way ref logit blend (from Cell 31 refine)\n",
        "w4_ref = (0.3381, 0.266786, 0.163514, 0.2316)\n",
        "z_ref4 = w4_ref[0]*z1_all + w4_ref[1]*z2_all + w4_ref[2]*z2b_all + w4_ref[3]*z3_all\n",
        "tz_ref4 = w4_ref[0]*tz1_all + w4_ref[1]*tz2_all + w4_ref[2]*tz2b_all + w4_ref[3]*tz3_all\n",
        "auc_ref4 = roc_auc_score(y, z_ref4)\n",
        "print(f'Ref 4-way logit blend OOF AUC(z): {auc_ref4:.5f}')\n",
        "\n",
        "# 2) Per-fold 3-way logit blend (LR, Dense v1, Meta) without standardization; rebuild OOF/test logits\n",
        "cv = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(np.arange(n), y))\n",
        "w_center = (0.3501, 0.4198, 0.2301)  # center from prior best\n",
        "window, step = 0.015, 0.001\n",
        "z_pf3 = np.zeros(n, dtype=np.float64)\n",
        "test_fold_logits = []\n",
        "t0 = time.time()\n",
        "for k, (tr_idx, va_idx) in enumerate(cv):\n",
        "    z1_tr, z2_tr, z3_tr = z1_all[tr_idx], z2_all[tr_idx], z3_all[tr_idx]\n",
        "    z1_va, z2_va, z3_va = z1_all[va_idx], z2_all[va_idx], z3_all[va_idx]\n",
        "    tz1, tz2, tz3 = tz1_all, tz2_all, tz3_all\n",
        "    w1c, w2c, _ = w_center\n",
        "    w1_min, w1_max = max(0.0, w1c - window), min(1.0, w1c + window)\n",
        "    w2_min, w2_max = max(0.0, w2c - window), min(1.0, w2c + window)\n",
        "    w1_grid = np.arange(w1_min, w1_max + 1e-12, step)\n",
        "    w2_grid = np.arange(w2_min, w2_max + 1e-12, step)\n",
        "    best_auc_k, best_w_k = -1.0, w_center\n",
        "    y_tr = y[tr_idx]\n",
        "    for w1 in w1_grid:\n",
        "        for w2 in w2_grid:\n",
        "            w3 = 1.0 - (w1 + w2)\n",
        "            if w3 < 0.0 or w3 > 1.0: continue\n",
        "            z_tr_combo = w1*z1_tr + w2*z2_tr + w3*z3_tr\n",
        "            auc_k = roc_auc_score(y_tr, z_tr_combo)\n",
        "            if auc_k > best_auc_k:\n",
        "                best_auc_k, best_w_k = auc_k, (float(w1), float(w2), float(w3))\n",
        "    w1k, w2k, w3k = best_w_k\n",
        "    z_pf3[va_idx] = w1k*z1_va + w2k*z2_va + w3k*z3_va\n",
        "    test_fold_logits.append((w1k*tz1 + w2k*tz2 + w3k*tz3).astype(np.float64))\n",
        "    print(f'Fold {k+1}/5 per-fold 3-way best weights={best_w_k} | time fold')\n",
        "auc_pf3 = roc_auc_score(y, z_pf3)\n",
        "print(f'Per-fold 3-way logit blend OOF AUC(z): {auc_pf3:.5f} | time {time.time()-t0:.1f}s')\n",
        "tz_pf3 = np.mean(test_fold_logits, axis=0)\n",
        "\n",
        "# 3) Bag the two blends in logit space with alpha tuned on OOF\n",
        "best_auc, best_alpha = -1.0, 0.0\n",
        "for a in np.linspace(0.0, 1.0, 101):\n",
        "    z_mix = (1.0 - a)*z_ref4 + a*z_pf3\n",
        "    auc = roc_auc_score(y, z_mix)\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_alpha = auc, float(a)\n",
        "print(f'Alpha search best alpha={best_alpha:.3f} | OOF AUC(z)={best_auc:.5f}')\n",
        "\n",
        "# Build submission from mixed test logits\n",
        "tz_mix = (1.0 - best_alpha)*tz_ref4 + best_alpha*tz_pf3\n",
        "pt = sigmoid(tz_mix).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (bagged 4-way ref + per-fold 3-way, logit space); head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached base OOF/test preds...\nRef 4-way logit blend OOF AUC(z): 0.69242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5 per-fold 3-way best weights=(0.3501, 0.4198, 0.23009999999999997) | time fold\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 per-fold 3-way best weights=(0.3501, 0.4198, 0.23009999999999997) | time fold\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 per-fold 3-way best weights=(0.36510000000000004, 0.4068, 0.22809999999999997) | time fold\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 per-fold 3-way best weights=(0.3411, 0.4198, 0.23909999999999998) | time fold\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 per-fold 3-way best weights=(0.3351, 0.4058, 0.2591) | time fold\nPer-fold 3-way logit blend OOF AUC(z): 0.69182 | time 8.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha search best alpha=0.000 | OOF AUC(z)=0.69242\nSaved submission.csv (bagged 4-way ref + per-fold 3-way, logit space); head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.331806\n1   t3_roiuw                  0.217080\n2   t3_mjnbq                  0.213391\n3   t3_t8wd1                  0.209035\n4  t3_1m4zxu                  0.215080\n"
          ]
        }
      ]
    },
    {
      "id": "c262866c-12f7-4dd6-aca6-f0bf1248667b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Bag 4-way global-ref and per-fold 4-way (no z-std) blends in logit space; optimize alpha on OOF; write submission\n",
        "import numpy as np, pandas as pd, time, sys\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "print('Loading cached OOF/test preds for 4-way (LR, Dense v1, Dense v2, Meta)...')\n",
        "o1 = np.load('oof_lr_pivot.npy'); t1 = np.load('test_lr_pivot.npy')\n",
        "o2 = np.load('oof_xgb_dense.npy'); t2 = np.load('test_xgb_dense.npy')\n",
        "o2b= np.load('oof_xgb_dense_v2.npy'); t2b= np.load('test_xgb_dense_v2.npy')\n",
        "o3 = np.load('oof_xgb_meta.npy'); t3 = np.load('test_xgb_meta.npy')\n",
        "y = train['requester_received_pizza'].astype(int).values\n",
        "n = len(y)\n",
        "\n",
        "# Logits\n",
        "z1_all, z2_all, z2b_all, z3_all = to_logit(o1), to_logit(o2), to_logit(o2b), to_logit(o3)\n",
        "tz1_all, tz2_all, tz2b_all, tz3_all = to_logit(t1), to_logit(t2), to_logit(t2b), to_logit(t3)\n",
        "\n",
        "# 4-way global reference blend (best from Cell 31 refine)\n",
        "w_ref = (0.3381, 0.266786, 0.163514, 0.2316)\n",
        "z_ref4 = w_ref[0]*z1_all + w_ref[1]*z2_all + w_ref[2]*z2b_all + w_ref[3]*z3_all\n",
        "tz_ref4 = w_ref[0]*tz1_all + w_ref[1]*tz2_all + w_ref[2]*tz2b_all + w_ref[3]*tz3_all\n",
        "auc_ref4 = roc_auc_score(y, z_ref4)\n",
        "print(f'Ref 4-way OOF AUC(z): {auc_ref4:.5f}')\n",
        "\n",
        "# Rebuild fast per-fold 4-way (no z-std) with a tiny window (\u00b10.01, step=0.002) for speed\n",
        "cv = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(np.arange(n), y))\n",
        "base_w = w_ref\n",
        "w1c, w2c, w2bc, w3c = base_w\n",
        "w1_grid = np.arange(max(0.0, w1c-0.01), min(1.0, w1c+0.01)+1e-12, 0.002)\n",
        "w2_grid = np.arange(max(0.0, w2c-0.01), min(1.0, w2c+0.01)+1e-12, 0.002)\n",
        "w2b_grid= np.arange(max(0.0, w2bc-0.01),min(1.0, w2bc+0.01)+1e-12, 0.002)\n",
        "z_pf4 = np.zeros(n, dtype=np.float64)\n",
        "test_fold_logits = []\n",
        "t0 = time.time()\n",
        "for k, (tr_idx, va_idx) in enumerate(cv):\n",
        "    z1_tr, z2_tr, z2b_tr, z3_tr = z1_all[tr_idx], z2_all[tr_idx], z2b_all[tr_idx], z3_all[tr_idx]\n",
        "    z1_va, z2_va, z2b_va, z3_va = z1_all[va_idx], z2_all[va_idx], z2b_all[va_idx], z3_all[va_idx]\n",
        "    y_tr = y[tr_idx]\n",
        "    best_auc_k, best_w_k = -1.0, base_w\n",
        "    for w1 in w1_grid:\n",
        "        for w2 in w2_grid:\n",
        "            for w2b in w2b_grid:\n",
        "                w3 = 1.0 - (w1 + w2 + w2b)\n",
        "                if w3 < 0.0 or w3 > 1.0: continue\n",
        "                z_tr = w1*z1_tr + w2*z2_tr + w2b*z2b_tr + w3*z3_tr\n",
        "                auc_k = roc_auc_score(y_tr, z_tr)\n",
        "                if auc_k > best_auc_k:\n",
        "                    best_auc_k = auc_k; best_w_k = (float(w1), float(w2), float(w2b), float(w3))\n",
        "    w1k, w2k, w2bk, w3k = best_w_k\n",
        "    z_pf4[va_idx] = w1k*z1_va + w2k*z2_va + w2bk*z2b_va + w3k*z3_va\n",
        "    tz_fold = w1k*tz1_all + w2k*tz2_all + w2bk*tz2b_all + w3k*tz3_all\n",
        "    test_fold_logits.append(tz_fold.astype(np.float64))\n",
        "    print(f'Fold {k+1}/5 per-fold 4-way best weights={best_w_k} | time {time.time()-t0:.1f}s'); sys.stdout.flush()\n",
        "auc_pf4 = roc_auc_score(y, z_pf4)\n",
        "print(f'Per-fold 4-way (no z-std) OOF AUC(z): {auc_pf4:.5f} | total {time.time()-t0:.1f}s')\n",
        "tz_pf4 = np.mean(test_fold_logits, axis=0)\n",
        "\n",
        "# Alpha bagging of z_ref4 and z_pf4\n",
        "best_auc, best_alpha = -1.0, 0.0\n",
        "for a in np.linspace(0.0, 1.0, 101):\n",
        "    z_mix = (1.0 - a)*z_ref4 + a*z_pf4\n",
        "    auc = roc_auc_score(y, z_mix)\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_alpha = auc, float(a)\n",
        "print(f'Alpha bag best alpha={best_alpha:.3f} | OOF AUC(z)={best_auc:.5f}')\n",
        "\n",
        "# Build submission from mixed test logits\n",
        "tz_mix = (1.0 - best_alpha)*tz_ref4 + best_alpha*tz_pf4\n",
        "pt = sigmoid(tz_mix).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (bagged 4-way ref + per-fold 4-way, logit space); head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached OOF/test preds for 4-way (LR, Dense v1, Dense v2, Meta)...\nRef 4-way OOF AUC(z): 0.69242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5 per-fold 4-way best weights=(0.3441, 0.27678600000000003, 0.157514, 0.22160000000000002) | time 2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5 per-fold 4-way best weights=(0.3441, 0.27678600000000003, 0.159514, 0.21960000000000002) | time 4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5 per-fold 4-way best weights=(0.3281, 0.260786, 0.173514, 0.23760000000000003) | time 7.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5 per-fold 4-way best weights=(0.3281, 0.27678600000000003, 0.157514, 0.23760000000000003) | time 9.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5 per-fold 4-way best weights=(0.3281, 0.264786, 0.167514, 0.23960000000000004) | time 11.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-fold 4-way (no z-std) OOF AUC(z): 0.69221 | total 11.8s\nAlpha bag best alpha=0.010 | OOF AUC(z)=0.69242\nSaved submission.csv (bagged 4-way ref + per-fold 4-way, logit space); head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.331808\n1   t3_roiuw                  0.217081\n2   t3_mjnbq                  0.213392\n3   t3_t8wd1                  0.209032\n4  t3_1m4zxu                  0.215074\n"
          ]
        }
      ]
    },
    {
      "id": "b040b041-ba4b-4478-a2bc-365faca1d787",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Lightweight tiny temperature scaling for 4-way logit blend with very narrow weight window\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "print('Loading cached OOF/test predictions (LR, Dense v1, Dense v2, Meta)...')\n",
        "o1 = np.load('oof_lr_pivot.npy'); t1 = np.load('test_lr_pivot.npy')\n",
        "o2 = np.load('oof_xgb_dense.npy'); t2 = np.load('test_xgb_dense.npy')\n",
        "o2b= np.load('oof_xgb_dense_v2.npy'); t2b= np.load('test_xgb_dense_v2.npy')\n",
        "o3 = np.load('oof_xgb_meta.npy'); t3 = np.load('test_xgb_meta.npy')\n",
        "y = train['requester_received_pizza'].astype(int).values\n",
        "\n",
        "z1b, z2b, z2bb, z3b = to_logit(o1), to_logit(o2), to_logit(o2b), to_logit(o3)\n",
        "tz1b, tz2b, tz2bb, tz3b = to_logit(t1), to_logit(t2), to_logit(t2b), to_logit(t3)\n",
        "\n",
        "# Base best 4-way weights\n",
        "w_ref = (0.3381, 0.266786, 0.163514, 0.2316)\n",
        "\n",
        "scales = [0.95, 1.00, 1.05]\n",
        "w_window = 0.006  # tighter window\n",
        "w_step = 0.002    # coarser step\n",
        "\n",
        "best_auc, best_cfg = -1.0, None\n",
        "t0 = time.time(); tried = 0\n",
        "for s1 in scales:\n",
        "    for s2 in scales:\n",
        "        for s2v in scales:\n",
        "            for s3 in scales:\n",
        "                z1 = s1*z1b; z2 = s2*z2b; z2v2 = s2v*z2bb; z3 = s3*z3b\n",
        "                w1c, w2c, w2vc, w3c = w_ref\n",
        "                w1_min, w1_max = max(0.0, w1c - w_window), min(1.0, w1c + w_window)\n",
        "                w2_min, w2_max = max(0.0, w2c - w_window), min(1.0, w2c + w_window)\n",
        "                w2v_min, w2v_max = max(0.0, w2vc - w_window), min(1.0, w2vc + w_window)\n",
        "                for w1 in np.arange(w1_min, w1_max + 1e-12, w_step):\n",
        "                    for w2 in np.arange(w2_min, w2_max + 1e-12, w_step):\n",
        "                        for w2v_w in np.arange(w2v_min, w2v_max + 1e-12, w_step):\n",
        "                            w3 = 1.0 - (w1 + w2 + w2v_w)\n",
        "                            if w3 < 0.0 or w3 > 1.0:\n",
        "                                continue\n",
        "                            z = w1*z1 + w2*z2 + w2v_w*z2v2 + w3*z3\n",
        "                            auc = roc_auc_score(y, z)\n",
        "                            tried += 1\n",
        "                            if auc > best_auc:\n",
        "                                best_auc = auc\n",
        "                                best_cfg = ((s1, s2, s2v, s3), (float(w1), float(w2), float(w2v_w), float(w3)))\n",
        "print(f'Light temp+weights tried {tried} combos | best OOF AUC(z)={best_auc:.5f} | scales={best_cfg[0]} | weights={best_cfg[1]} | {time.time()-t0:.1f}s')\n",
        "\n",
        "# Build submission with best config\n",
        "s1, s2, s2v, s3 = best_cfg[0]\n",
        "bw1, bw2, bw2v, bw3 = best_cfg[1]\n",
        "zt = bw1*(s1*tz1b) + bw2*(s2*tz2b) + bw2v*(s2v*tz2bb) + bw3*(s3*tz3b)\n",
        "pt = sigmoid(zt).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (light 4-way temp-scaled logit blend); head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached OOF/test predictions (LR, Dense v1, Dense v2, Meta)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Light temp+weights tried 27783 combos | best OOF AUC(z)=0.69242 | scales=(0.95, 0.95, 0.95, 0.95) | weights=(0.3381, 0.266786, 0.163514, 0.23160000000000003) | 53.3s\nSaved submission.csv (light 4-way temp-scaled logit blend); head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.339611\n1   t3_roiuw                  0.228178\n2   t3_mjnbq                  0.224545\n3   t3_t8wd1                  0.220249\n4  t3_1m4zxu                  0.226209\n"
          ]
        }
      ]
    },
    {
      "id": "fcce0aa4-f984-4519-b85c-0b4277e324b9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Blend-of-blends: 3-way logit blend of Ref4, PerFold4(no-zstd), Ref3-way; fine grid around (0.35,0.42,0.23)\n",
        "import numpy as np, pandas as pd, time, sys\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "print('Loading cached OOF/test preds...')\n",
        "o1 = np.load('oof_lr_pivot.npy'); t1 = np.load('test_lr_pivot.npy')\n",
        "o2 = np.load('oof_xgb_dense.npy'); t2 = np.load('test_xgb_dense.npy')\n",
        "o2b= np.load('oof_xgb_dense_v2.npy'); t2b= np.load('test_xgb_dense_v2.npy')\n",
        "o3 = np.load('oof_xgb_meta.npy'); t3 = np.load('test_xgb_meta.npy')\n",
        "y = train['requester_received_pizza'].astype(int).values\n",
        "n = len(y)\n",
        "\n",
        "# Base logits\n",
        "z1_all, z2_all, z2b_all, z3_all = to_logit(o1), to_logit(o2), to_logit(o2b), to_logit(o3)\n",
        "tz1_all, tz2_all, tz2b_all, tz3_all = to_logit(t1), to_logit(t2), to_logit(t2b), to_logit(t3)\n",
        "\n",
        "# A) Ref4: global 4-way split-dense best\n",
        "w4_ref = (0.3381, 0.266786, 0.163514, 0.2316)\n",
        "z_ref4 = w4_ref[0]*z1_all + w4_ref[1]*z2_all + w4_ref[2]*z2b_all + w4_ref[3]*z3_all\n",
        "tz_ref4 = w4_ref[0]*tz1_all + w4_ref[1]*tz2_all + w4_ref[2]*tz2b_all + w4_ref[3]*tz3_all\n",
        "print(f'Ref4 OOF AUC(z): {roc_auc_score(y, z_ref4):.5f}')\n",
        "\n",
        "# B) PerFold4 (no z-std) fast rebuild with tiny window (\u00b10.01, step=0.002)\n",
        "cv = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(np.arange(n), y))\n",
        "w1c, w2c, w2bc, w3c = w4_ref\n",
        "w1_grid = np.arange(max(0.0, w1c-0.01), min(1.0, w1c+0.01)+1e-12, 0.002)\n",
        "w2_grid = np.arange(max(0.0, w2c-0.01), min(1.0, w2c+0.01)+1e-12, 0.002)\n",
        "w2b_grid= np.arange(max(0.0, w2bc-0.01),min(1.0, w2bc+0.01)+1e-12, 0.002)\n",
        "z_pf4 = np.zeros(n, dtype=np.float64)\n",
        "test_fold_logits = []\n",
        "t0 = time.time()\n",
        "for k, (tr_idx, va_idx) in enumerate(cv):\n",
        "    z1_tr, z2_tr, z2b_tr, z3_tr = z1_all[tr_idx], z2_all[tr_idx], z2b_all[tr_idx], z3_all[tr_idx]\n",
        "    z1_va, z2_va, z2b_va, z3_va = z1_all[va_idx], z2_all[va_idx], z2b_all[va_idx], z3_all[va_idx]\n",
        "    y_tr = y[tr_idx]\n",
        "    best_auc_k, best_w_k = -1.0, w4_ref\n",
        "    for w1 in w1_grid:\n",
        "        for w2 in w2_grid:\n",
        "            for w2b in w2b_grid:\n",
        "                w3 = 1.0 - (w1 + w2 + w2b)\n",
        "                if w3 < 0.0 or w3 > 1.0: continue\n",
        "                z_tr = w1*z1_tr + w2*z2_tr + w2b*z2b_tr + w3*z3_tr\n",
        "                auc_k = roc_auc_score(y_tr, z_tr)\n",
        "                if auc_k > best_auc_k:\n",
        "                    best_auc_k = auc_k; best_w_k = (float(w1), float(w2), float(w2b), float(w3))\n",
        "    w1k, w2k, w2bk, w3k = best_w_k\n",
        "    z_pf4[va_idx] = w1k*z1_va + w2k*z2_va + w2bk*z2b_va + w3k*z3_va\n",
        "    tz_fold = w1k*tz1_all + w2k*tz2_all + w2bk*tz2b_all + w3k*tz3_all\n",
        "    test_fold_logits.append(tz_fold.astype(np.float64))\n",
        "    print(f'PerFold4 Fold {k+1} best_w={best_w_k}')\n",
        "auc_pf4 = roc_auc_score(y, z_pf4)\n",
        "tz_pf4 = np.mean(test_fold_logits, axis=0)\n",
        "print(f'PerFold4 OOF AUC(z): {auc_pf4:.5f} | {time.time()-t0:.1f}s')\n",
        "\n",
        "# C) Ref3-way logit blend (LR, Dense v1, Meta) with best weights from cell 24\n",
        "w3_ref = (0.3501, 0.4198, 0.2301)  # from ultra-fine logit best vicinity\n",
        "z_ref3 = w3_ref[0]*z1_all + w3_ref[1]*z2_all + w3_ref[2]*z3_all\n",
        "tz_ref3 = w3_ref[0]*tz1_all + w3_ref[1]*tz2_all + w3_ref[2]*tz3_all\n",
        "print(f'Ref3 OOF AUC(z): {roc_auc_score(y, z_ref3):.5f}')\n",
        "\n",
        "# 3-way blend of z_ref4, z_pf4, z_ref3 in logit space; fine grid around (0.35,0.42,0.23)\n",
        "wA_c, wB_c = 0.35, 0.42  # C=1-wA-wB\n",
        "window, step = 0.02, 0.001\n",
        "wA_min, wA_max = max(0.0, wA_c-window), min(1.0, wA_c+window)\n",
        "wB_min, wB_max = max(0.0, wB_c-window), min(1.0, wB_c+window)\n",
        "wA_grid = np.arange(wA_min, wA_max + 1e-12, step)\n",
        "wB_grid = np.arange(wB_min, wB_max + 1e-12, step)\n",
        "best_auc, best_w = -1.0, (wA_c, wB_c, 1.0 - wA_c - wB_c)\n",
        "t1s = time.time(); tried = 0\n",
        "for wA in wA_grid:\n",
        "    for wB in wB_grid:\n",
        "        wC = 1.0 - wA - wB\n",
        "        if wC < 0.0 or wC > 1.0: continue\n",
        "        z_mix = wA*z_ref4 + wB*z_pf4 + wC*z_ref3\n",
        "        auc = roc_auc_score(y, z_mix)\n",
        "        tried += 1\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_w = auc, (float(wA), float(wB), float(wC))\n",
        "print(f'Blend-of-blends tried {tried} combos | best weights(Ref4,PerFold4,Ref3)={best_w} OOF AUC(z): {best_auc:.5f} | {time.time()-t1s:.1f}s')\n",
        "\n",
        "# Build submission with best weights\n",
        "wA, wB, wC = best_w\n",
        "tz_mix = wA*tz_ref4 + wB*tz_pf4 + wC*tz_ref3\n",
        "pt = sigmoid(tz_mix).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (blend-of-blends 3-way logit); head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached OOF/test preds...\nRef4 OOF AUC(z): 0.69242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PerFold4 Fold 1 best_w=(0.3441, 0.27678600000000003, 0.157514, 0.22160000000000002)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PerFold4 Fold 2 best_w=(0.3441, 0.27678600000000003, 0.159514, 0.21960000000000002)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PerFold4 Fold 3 best_w=(0.3281, 0.260786, 0.173514, 0.23760000000000003)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PerFold4 Fold 4 best_w=(0.3281, 0.27678600000000003, 0.157514, 0.23760000000000003)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PerFold4 Fold 5 best_w=(0.3281, 0.264786, 0.167514, 0.23960000000000004)\nPerFold4 OOF AUC(z): 0.69221 | 11.6s\nRef3 OOF AUC(z): 0.69201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blend-of-blends tried 1681 combos | best weights(Ref4,PerFold4,Ref3)=(0.366, 0.432, 0.202) OOF AUC(z): 0.69229 | 3.3s\nSaved submission.csv (blend-of-blends 3-way logit); head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.331250\n1   t3_roiuw                  0.216240\n2   t3_mjnbq                  0.213711\n3   t3_t8wd1                  0.208273\n4  t3_1m4zxu                  0.215067\n"
          ]
        }
      ]
    },
    {
      "id": "5f6bad1a-d553-4f9e-a863-5a5d6a47967f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Meta-flag tweak (+throwaway, +military) only for dense XGB v1; retrain single-seed and re-run 4-way split-dense logit blend\n",
        "import time, gc, sys, re, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    import subprocess, sys as _sys\n",
        "    subprocess.run([_sys.executable, '-m', 'pip', 'install', '-q', 'xgboost'])\n",
        "    import xgboost as xgb\n",
        "\n",
        "assert 'combine_raw_text' in globals() and 'clean_text_series' in globals() and 'build_subreddit_text' in globals(), 'Run pivot cell 7 first'\n",
        "assert 'word_params' in globals() and 'char_params' in globals() and 'subs_params' in globals(), 'Run pivot cell 7 first'\n",
        "assert 'build_meta_enhanced' in globals(), 'Run cell 10 first'\n",
        "\n",
        "def build_meta_enhanced_plus(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = build_meta_enhanced(df).copy()\n",
        "    # f_throwaway_user from username\n",
        "    if 'requester_username' in df.columns:\n",
        "        un = df['requester_username'].fillna('').astype(str)\n",
        "        out['f_throwaway_user'] = un.str.contains(r'(throwaway|temp|anon)', case=False, regex=True).astype(np.int8)\n",
        "    else:\n",
        "        out['f_throwaway_user'] = 0\n",
        "    # f_military from title+body\n",
        "    title = df['request_title'].fillna('').astype(str) if 'request_title' in df.columns else pd.Series(['']*len(df), index=df.index)\n",
        "    body = df['request_text_edit_aware'].fillna('').astype(str) if 'request_text_edit_aware' in df.columns else df['request_text'].fillna('').astype(str) if 'request_text' in df.columns else pd.Series(['']*len(df), index=df.index)\n",
        "    txt = (title + ' ' + body).str.lower()\n",
        "    out['f_military'] = txt.str.contains(r'\\b(military|army|navy|marine|air\\s*force|veteran|deployment)\\b', regex=True).astype(np.int8)\n",
        "    return out.astype(np.float32)\n",
        "\n",
        "# 1) Retrain Dense XGB v1 (single seed=42) with enhanced_plus meta; SVD dims same as v1 (250/250/80)\n",
        "svd_word_n, svd_char_n, svd_subs_n = 250, 250, 80\n",
        "y = train[target_col].astype(int).values\n",
        "cv = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(train, y))\n",
        "print('Prepared 5-fold CV; retraining Dense v1 (single seed=42) with meta+flags')\n",
        "\n",
        "raw_te_text = combine_raw_text(test)\n",
        "clean_te_text = clean_text_series(raw_te_text)\n",
        "subs_te_text = build_subreddit_text(test)\n",
        "meta_te_plus = build_meta_enhanced_plus(test).astype(np.float32)\n",
        "\n",
        "seed = 42\n",
        "oof_dense_mf = np.zeros(len(train), dtype=np.float32)\n",
        "test_dense_mf_folds = []\n",
        "\n",
        "base_params = dict(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='auc',\n",
        "    max_depth=4,\n",
        "    eta=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.7,\n",
        "    min_child_weight=4,\n",
        "    reg_alpha=0.5,\n",
        "    reg_lambda=2.0,\n",
        "    gamma=0.0,\n",
        "    device='cuda',\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(cv):\n",
        "    t0 = time.time()\n",
        "    print(f'Seed {seed} | Fold {fold+1}/5 - train {len(tr_idx)} va {len(va_idx)}')\n",
        "    tr_text_raw = combine_raw_text(train.loc[tr_idx])\n",
        "    va_text_raw = combine_raw_text(train.loc[va_idx])\n",
        "    tr_text = clean_text_series(tr_text_raw)\n",
        "    va_text = clean_text_series(va_text_raw)\n",
        "    tr_subs = build_subreddit_text(train.loc[tr_idx])\n",
        "    va_subs = build_subreddit_text(train.loc[va_idx])\n",
        "\n",
        "    tfidf_w = TfidfVectorizer(**word_params)\n",
        "    Xw_tr = tfidf_w.fit_transform(tr_text); Xw_va = tfidf_w.transform(va_text); Xw_te = tfidf_w.transform(clean_te_text)\n",
        "    tfidf_c = TfidfVectorizer(**char_params)\n",
        "    Xc_tr = tfidf_c.fit_transform(tr_text); Xc_va = tfidf_c.transform(va_text); Xc_te = tfidf_c.transform(clean_te_text)\n",
        "    tfidf_s = TfidfVectorizer(**subs_params)\n",
        "    Xs_tr = tfidf_s.fit_transform(tr_subs); Xs_va = tfidf_s.transform(va_subs); Xs_te = tfidf_s.transform(subs_te_text)\n",
        "\n",
        "    svd_w = TruncatedSVD(n_components=svd_word_n, random_state=seed)\n",
        "    Zw_tr = svd_w.fit_transform(Xw_tr); Zw_va = svd_w.transform(Xw_va); Zw_te = svd_w.transform(Xw_te)\n",
        "    svd_c = TruncatedSVD(n_components=svd_char_n, random_state=seed)\n",
        "    Zc_tr = svd_c.fit_transform(Xc_tr); Zc_va = svd_c.transform(Xc_va); Zc_te = svd_c.transform(Xc_te)\n",
        "    svd_s = TruncatedSVD(n_components=svd_subs_n, random_state=seed)\n",
        "    Zs_tr = svd_s.fit_transform(Xs_tr); Zs_va = svd_s.transform(Xs_va); Zs_te = svd_s.transform(Xs_te)\n",
        "\n",
        "    meta_tr_plus = build_meta_enhanced_plus(train.loc[tr_idx]).astype(np.float32)\n",
        "    meta_va_plus = build_meta_enhanced_plus(train.loc[va_idx]).astype(np.float32)\n",
        "\n",
        "    Xtr_dense = np.hstack([Zw_tr, Zc_tr, Zs_tr, meta_tr_plus.values]).astype(np.float32)\n",
        "    Xva_dense = np.hstack([Zw_va, Zc_va, Zs_va, meta_va_plus.values]).astype(np.float32)\n",
        "    Xte_dense = np.hstack([Zw_te, Zc_te, Zs_te, meta_te_plus.values]).astype(np.float32)\n",
        "\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    Xtr_d = scaler.fit_transform(Xtr_dense)\n",
        "    Xva_d = scaler.transform(Xva_dense)\n",
        "    Xte_d = scaler.transform(Xte_dense)\n",
        "\n",
        "    dtrain = xgb.DMatrix(Xtr_d, label=y[tr_idx])\n",
        "    dvalid = xgb.DMatrix(Xva_d, label=y[va_idx])\n",
        "    dtest  = xgb.DMatrix(Xte_d)\n",
        "    booster = xgb.train(base_params, dtrain, num_boost_round=4000, evals=[(dtrain,'train'),(dvalid,'valid')], early_stopping_rounds=200, verbose_eval=False)\n",
        "    va_pred = booster.predict(dvalid, iteration_range=(0, booster.best_iteration+1)).astype(np.float32)\n",
        "    te_pred = booster.predict(dtest, iteration_range=(0, booster.best_iteration+1)).astype(np.float32)\n",
        "    oof_dense_mf[va_idx] = va_pred\n",
        "    test_dense_mf_folds.append(te_pred)\n",
        "    auc = roc_auc_score(y[va_idx], va_pred)\n",
        "    print(f'Fold {fold+1} AUC: {auc:.5f} | best_iter={booster.best_iteration} | time {time.time()-t0:.1f}s')\n",
        "    del Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, Xs_tr, Xs_va, Xs_te, Zw_tr, Zw_va, Zw_te, Zc_tr, Zc_va, Zc_te, Zs_tr, Zs_va, Zs_te, meta_tr_plus, meta_va_plus, Xtr_dense, Xva_dense, Xte_dense, Xtr_d, Xva_d, Xte_d, dtrain, dvalid, dtest, booster, tfidf_w, tfidf_c, tfidf_s, svd_w, svd_c, svd_s, scaler\n",
        "    gc.collect()\n",
        "\n",
        "auc_oof_mf = roc_auc_score(y, oof_dense_mf)\n",
        "print(f'Dense XGB v1 (meta+flags) single-seed OOF AUC: {auc_oof_mf:.5f}')\n",
        "test_dense_mf = np.mean(test_dense_mf_folds, axis=0).astype(np.float32)\n",
        "np.save('oof_xgb_dense_mf.npy', oof_dense_mf.astype(np.float32))\n",
        "np.save('test_xgb_dense_mf.npy', test_dense_mf)\n",
        "\n",
        "# 2) 4-way split-dense logit blend using LR + Dense v1 (meta+flags) + Dense v2 (old) + Meta\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "o1 = np.load('oof_lr_pivot.npy')\n",
        "o2 = np.load('oof_xgb_dense_mf.npy')  # new dense v1 with flags\n",
        "o2b = np.load('oof_xgb_dense_v2.npy')\n",
        "o3 = np.load('oof_xgb_meta.npy')\n",
        "t1 = np.load('test_lr_pivot.npy')\n",
        "t2 = np.load('test_xgb_dense_mf.npy')\n",
        "t2b = np.load('test_xgb_dense_v2.npy')\n",
        "t3 = np.load('test_xgb_meta.npy')\n",
        "z1, z2, z2b, z3 = to_logit(o1), to_logit(o2), to_logit(o2b), to_logit(o3)\n",
        "tz1, tz2, tz2b, tz3 = to_logit(t1), to_logit(t2), to_logit(t2b), to_logit(t3)\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "# Start near prior best 3-way logit (v1 baseline): (0.3501, 0.4198, 0.2301). We'll split dense between v1/v2 via alpha.\n",
        "w1c, wdc = 0.3501, 0.4198\n",
        "w3c = 1.0 - w1c - wdc\n",
        "w1_min, w1_max = max(0.0, w1c - 0.02), min(1.0, w1c + 0.02)\n",
        "wd_min, wd_max = max(0.0, wdc - 0.02), min(1.0, wdc + 0.02)\n",
        "alphas = np.arange(0.0, 0.4001, 0.05)\n",
        "step = 0.001\n",
        "w1_grid = np.arange(w1_min, w1_max + 1e-12, step)\n",
        "wd_grid = np.arange(wd_min, wd_max + 1e-12, step)\n",
        "best_auc, best_cfg = -1.0, None\n",
        "t0 = time.time(); cnt = 0\n",
        "for w1 in w1_grid:\n",
        "    for wd in wd_grid:\n",
        "        w3 = 1.0 - w1 - wd\n",
        "        if w3 < 0.0 or w3 > 1.0: continue\n",
        "        for a in alphas:\n",
        "            w2b = wd * a\n",
        "            w2 = wd - w2b\n",
        "            z = w1*z1 + w2*z2 + w2b*z2b + w3*z3\n",
        "            auc = roc_auc_score(y, z)\n",
        "            cnt += 1\n",
        "            if auc > best_auc:\n",
        "                best_auc = auc; best_cfg = (float(w1), float(w2), float(w2b), float(w3))\n",
        "print(f'4-way split-dense (dense_v1=flags) tried {cnt} combos | best weights={best_cfg} OOF AUC(z): {best_auc:.5f} | {time.time()-t0:.1f}s')\n",
        "\n",
        "# Optional micro-refine around best\n",
        "w1b, w2b1, w2b2, w3b = best_cfg\n",
        "def refine(w1c, w2c1, w2c2, w3c, window=0.006, step=0.0005):\n",
        "    best_local_auc, best_local_w = -1.0, None\n",
        "    wd_c = w2c1 + w2c2\n",
        "    alpha_c = (w2c2 / wd_c) if wd_c > 0 else 0.0\n",
        "    w1_min = max(0.0, w1c - window); w1_max = min(1.0, w1c + window)\n",
        "    wd_min = max(0.0, wd_c - window); wd_max = min(1.0, wd_c + window)\n",
        "    a_min = max(0.0, alpha_c - 0.05); a_max = min(0.6, alpha_c + 0.05)\n",
        "    w1_grid = np.arange(w1_min, w1_max + 1e-12, step)\n",
        "    wd_grid = np.arange(wd_min, wd_max + 1e-12, step)\n",
        "    a_grid = np.arange(a_min, a_max + 1e-12, step*5)\n",
        "    t1s = time.time(); cnt2 = 0\n",
        "    for w1 in w1_grid:\n",
        "        for wd in wd_grid:\n",
        "            w3 = 1.0 - w1 - wd\n",
        "            if w3 < 0.0 or w3 > 1.0: continue\n",
        "            for a in a_grid:\n",
        "                w2v2 = wd * a; w2v1 = wd - w2v2\n",
        "                z = w1*z1 + w2v1*z2 + w2v2*z2b + w3*z3\n",
        "                auc = roc_auc_score(y, z); cnt2 += 1\n",
        "                if auc > best_local_auc:\n",
        "                    best_local_auc, best_local_w = auc, (float(w1), float(w2v1), float(w2v2), float(w3))\n",
        "    print(f'Refine ({cnt2} combos) best weights={best_local_w} OOF AUC(z): {best_local_auc:.5f} | {time.time()-t1s:.1f}s')\n",
        "    return best_local_auc, best_local_w\n",
        "\n",
        "auc_ref, w_ref = refine(w1b, w2b1, w2b2, w3b, window=0.006, step=0.0005)\n",
        "use_auc = auc_ref if auc_ref > best_auc else best_auc\n",
        "use_w = w_ref if auc_ref > best_auc else best_cfg\n",
        "print(f'Chosen 4-way (dense_v1=flags) weights={use_w} OOF AUC(z): {use_auc:.5f}')\n",
        "\n",
        "# Build submission if at/above threshold (or if this is our best shot now)\n",
        "w1f, w2f1, w2f2, w3f = use_w\n",
        "zt = w1f*tz1 + w2f1*tz2 + w2f2*tz2b + w3f*tz3\n",
        "pt = (1.0/(1.0+np.exp(-zt))).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (4-way with Dense v1 meta+flags); head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 5-fold CV; retraining Dense v1 (single seed=42) with meta+flags\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/744578686.py:26: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_throwaway_user'] = un.str.contains(r'(throwaway|temp|anon)', case=False, regex=True).astype(np.int8)\n/tmp/ipykernel_60/744578686.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_military'] = txt.str.contains(r'\\b(military|army|navy|marine|air\\s*force|veteran|deployment)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed 42 | Fold 1/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/744578686.py:26: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_throwaway_user'] = un.str.contains(r'(throwaway|temp|anon)', case=False, regex=True).astype(np.int8)\n/tmp/ipykernel_60/744578686.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_military'] = txt.str.contains(r'\\b(military|army|navy|marine|air\\s*force|veteran|deployment)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/744578686.py:26: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_throwaway_user'] = un.str.contains(r'(throwaway|temp|anon)', case=False, regex=True).astype(np.int8)\n/tmp/ipykernel_60/744578686.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_military'] = txt.str.contains(r'\\b(military|army|navy|marine|air\\s*force|veteran|deployment)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 AUC: 0.67162 | best_iter=150 | time 16.9s\nSeed 42 | Fold 2/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/744578686.py:26: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_throwaway_user'] = un.str.contains(r'(throwaway|temp|anon)', case=False, regex=True).astype(np.int8)\n/tmp/ipykernel_60/744578686.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_military'] = txt.str.contains(r'\\b(military|army|navy|marine|air\\s*force|veteran|deployment)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/744578686.py:26: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_throwaway_user'] = un.str.contains(r'(throwaway|temp|anon)', case=False, regex=True).astype(np.int8)\n/tmp/ipykernel_60/744578686.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_military'] = txt.str.contains(r'\\b(military|army|navy|marine|air\\s*force|veteran|deployment)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 AUC: 0.64378 | best_iter=256 | time 17.3s\nSeed 42 | Fold 3/5 - train 2302 va 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/744578686.py:26: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_throwaway_user'] = un.str.contains(r'(throwaway|temp|anon)', case=False, regex=True).astype(np.int8)\n/tmp/ipykernel_60/744578686.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_military'] = txt.str.contains(r'\\b(military|army|navy|marine|air\\s*force|veteran|deployment)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/744578686.py:26: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_throwaway_user'] = un.str.contains(r'(throwaway|temp|anon)', case=False, regex=True).astype(np.int8)\n/tmp/ipykernel_60/744578686.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_military'] = txt.str.contains(r'\\b(military|army|navy|marine|air\\s*force|veteran|deployment)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 AUC: 0.67763 | best_iter=58 | time 16.5s\nSeed 42 | Fold 4/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/744578686.py:26: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_throwaway_user'] = un.str.contains(r'(throwaway|temp|anon)', case=False, regex=True).astype(np.int8)\n/tmp/ipykernel_60/744578686.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_military'] = txt.str.contains(r'\\b(military|army|navy|marine|air\\s*force|veteran|deployment)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/744578686.py:26: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_throwaway_user'] = un.str.contains(r'(throwaway|temp|anon)', case=False, regex=True).astype(np.int8)\n/tmp/ipykernel_60/744578686.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_military'] = txt.str.contains(r'\\b(military|army|navy|marine|air\\s*force|veteran|deployment)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 AUC: 0.63492 | best_iter=55 | time 16.5s\nSeed 42 | Fold 5/5 - train 2303 va 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_60/744578686.py:26: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_throwaway_user'] = un.str.contains(r'(throwaway|temp|anon)', case=False, regex=True).astype(np.int8)\n/tmp/ipykernel_60/744578686.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_military'] = txt.str.contains(r'\\b(military|army|navy|marine|air\\s*force|veteran|deployment)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_reciprocity'] = txt.str.contains(r'\\b(pay it forward|return the favor|pay you back|pay it back)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:22: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_payday'] = txt.str.contains(r'\\b(payday|paycheck|get paid|next check)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:23: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_day_words'] = txt.str.contains(r'\\b(monday|tuesday|wednesday|thursday|friday|tomorrow|tonight|weekend)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:25: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_amount'] = txt.str.contains(r'(\\$\\s?\\d+)|(\\b\\d+\\s*dollars?\\b)', regex=True).astype(np.int8)\n/tmp/ipykernel_60/1712808894.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['mentions_kids'] = txt.str.contains(r'\\b(kids?|children|son|daughter|baby)\\b', regex=True).astype(np.int8)\n/tmp/ipykernel_60/744578686.py:26: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_throwaway_user'] = un.str.contains(r'(throwaway|temp|anon)', case=False, regex=True).astype(np.int8)\n/tmp/ipykernel_60/744578686.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  out['f_military'] = txt.str.contains(r'\\b(military|army|navy|marine|air\\s*force|veteran|deployment)\\b', regex=True).astype(np.int8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 AUC: 0.70090 | best_iter=58 | time 16.4s\nDense XGB v1 (meta+flags) single-seed OOF AUC: 0.65899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4-way split-dense (dense_v1=flags) tried 15129 combos | best weights=(0.3521, 0.23987999999999998, 0.15992, 0.24809999999999993) OOF AUC(z): 0.69014 | 29.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refine (25625 combos) best weights=(0.3516, 0.21658999999999995, 0.17721000000000003, 0.2546) OOF AUC(z): 0.69045 | 50.3s\nChosen 4-way (dense_v1=flags) weights=(0.3516, 0.21658999999999995, 0.17721000000000003, 0.2546) OOF AUC(z): 0.69045\nSaved submission.csv (4-way with Dense v1 meta+flags); head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.337412\n1   t3_roiuw                  0.219214\n2   t3_mjnbq                  0.206461\n3   t3_t8wd1                  0.213465\n4  t3_1m4zxu                  0.224252\n"
          ]
        }
      ]
    },
    {
      "id": "8f1c85bc-a8b2-448e-b32c-631522b7ac83",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 5-way logit blend: LR Pivot + Dense v1 + Dense v2 + Meta + LR_alt (cap small weight); cached logits only\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "print('Loading cached OOF/test predictions (LR, Dense v1, Dense v2, Meta, LR_alt)...')\n",
        "o1 = np.load('oof_lr_pivot.npy'); t1 = np.load('test_lr_pivot.npy')\n",
        "o2 = np.load('oof_xgb_dense.npy'); t2 = np.load('test_xgb_dense.npy')\n",
        "o2b= np.load('oof_xgb_dense_v2.npy'); t2b= np.load('test_xgb_dense_v2.npy')\n",
        "o3 = np.load('oof_xgb_meta.npy'); t3 = np.load('test_xgb_meta.npy')\n",
        "o4 = np.load('oof_lr_alt.npy'); t4 = np.load('test_lr_alt.npy')\n",
        "y = train['requester_received_pizza'].astype(int).values\n",
        "\n",
        "# Convert to logits\n",
        "z1, z2, z2b, z3, z4 = to_logit(o1), to_logit(o2), to_logit(o2b), to_logit(o3), to_logit(o4)\n",
        "tz1, tz2, tz2b, tz3, tz4 = to_logit(t1), to_logit(t2), to_logit(t2b), to_logit(t3), to_logit(t4)\n",
        "\n",
        "# Reference 4-way weights (from Cell 31 refine)\n",
        "w_lr_ref, w_d1_ref, w_d2_ref, w_meta_ref = 0.3381, 0.266786, 0.163514, 0.2316\n",
        "wd_ref = w_d1_ref + w_d2_ref\n",
        "\n",
        "# Grids: small cap for LR_alt; tight windows around ref\n",
        "w_alt_grid = np.arange(0.0, 0.1001, 0.005)\n",
        "w1_grid   = np.arange(max(0.0, w_lr_ref - 0.01), min(1.0, w_lr_ref + 0.01) + 1e-12, 0.001)\n",
        "wd_grid   = np.arange(max(0.0, wd_ref - 0.01), min(1.0, wd_ref + 0.01) + 1e-12, 0.001)\n",
        "alpha_grid= np.arange(0.0, 0.4001, 0.05)  # fraction of wd to Dense v2\n",
        "\n",
        "best_auc, best_w = -1.0, None\n",
        "t0 = time.time(); tried = 0\n",
        "for w_alt in w_alt_grid:\n",
        "    rem = 1.0 - w_alt\n",
        "    for w1 in w1_grid:\n",
        "        for wd in wd_grid:\n",
        "            # remaining for meta\n",
        "            w3 = rem - w1 - wd\n",
        "            if w3 < 0.0 or w3 > 1.0:\n",
        "                continue\n",
        "            for a in alpha_grid:\n",
        "                w2b = wd * a\n",
        "                w2 = wd - w2b\n",
        "                z = w1*z1 + w2*z2 + w2b*z2b + w3*z3 + w_alt*z4\n",
        "                auc = roc_auc_score(y, z)\n",
        "                tried += 1\n",
        "                if auc > best_auc:\n",
        "                    best_auc = auc\n",
        "                    # store full 5-way weights\n",
        "                    best_w = (float(w1), float(w2), float(w2b), float(w3), float(w_alt))\n",
        "print(f'5-way (LR, D1, D2, Meta, LR_alt) tried {tried} combos | best OOF AUC(z)={best_auc:.5f} | weights={best_w} | {time.time()-t0:.1f}s')\n",
        "\n",
        "# Build submission from best 5-way config (even if OOF ~ same; diversity may help LB)\n",
        "w1b, w2b, w2bb, w3b, w4b = best_w\n",
        "zt = w1b*tz1 + w2b*tz2 + w2bb*tz2b + w3b*tz3 + w4b*tz4\n",
        "pt = sigmoid(zt).astype(np.float32)\n",
        "sub = pd.DataFrame({id_col: test[id_col].values, target_col: pt})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (5-way with small LR_alt cap); head:')\n",
        "print(sub.head())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}