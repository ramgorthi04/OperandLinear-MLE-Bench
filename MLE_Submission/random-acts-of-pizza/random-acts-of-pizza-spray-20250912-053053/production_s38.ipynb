{
  "cells": [
    {
      "id": "b42a45ba-3ff1-4773-b8b9-9e159615e248",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S38: Robust optimizer with larger objective (last-2 blocks and gamma-weighted full) + shrink-to-equal hedge\n",
        "import numpy as np, pandas as pd, time, os\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# 6-block forward-chaining blocks and masks\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "n = len(train)\n",
        "mask_full = np.zeros(n, dtype=bool)\n",
        "for bi in range(1, k):\n",
        "    mask_full[np.array(blocks[bi])] = True\n",
        "# last two validated blocks (blocks[4], blocks[5])\n",
        "mask_last2 = np.zeros(n, dtype=bool)\n",
        "for bi in [4, 5]:\n",
        "    mask_last2[np.array(blocks[bi])] = True\n",
        "print(f'Validated counts -> full: {mask_full.sum()}/{n}, last2: {mask_last2.sum()}')\n",
        "\n",
        "# Load time-consistent base OOF/test predictions (7 bases)\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy');   t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "o_lr_ns = np.load('oof_lr_time_nosub_meta.npy');    t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy');           t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_time_v2.npy');        t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta_time.npy');          t_meta = np.load('test_xgb_meta_time.npy')\n",
        "o_emn = np.load('oof_xgb_emb_meta_time.npy');       t_emn = np.load('test_xgb_emb_meta_time.npy')  # MiniLM\n",
        "o_emp = np.load('oof_xgb_emb_mpnet_time.npy');      t_emp = np.load('test_xgb_emb_mpnet_time.npy') # MPNet\n",
        "\n",
        "# Convert to logits\n",
        "z_lr_w, z_lr_ns = to_logit(o_lr_w), to_logit(o_lr_ns)\n",
        "z_d1, z_d2, z_meta = to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)\n",
        "z_emn, z_emp = to_logit(o_emn), to_logit(o_emp)\n",
        "tz_lr_w, tz_lr_ns = to_logit(t_lr_w), to_logit(t_lr_ns)\n",
        "tz_d1, tz_d2, tz_meta = to_logit(t_d1), to_logit(t_d2), to_logit(t_meta)\n",
        "tz_emn, tz_emp = to_logit(t_emn), to_logit(t_emp)\n",
        "\n",
        "# Search grids (robust, not too wide)\n",
        "g_grid = [0.94, 0.96, 0.98]  # LR withsub/nosub mix (weight on nosub)\n",
        "meta_grid = [0.18, 0.20, 0.22]\n",
        "dense_tot_grid = [0.24, 0.30, 0.35, 0.40]\n",
        "alpha_grid = [0.50, 0.65, 0.80]  # split dense_total into v2 (w_d2 = d_tot * alpha)\n",
        "emn_grid = [0.10, 0.12, 0.15]    # MiniLM\n",
        "emp_grid = [0.08, 0.10, 0.12]    # MPNet\n",
        "\n",
        "def search_best(mask, sample_weight=None):\n",
        "    best_auc, best_cfg, tried = -1.0, None, 0\n",
        "    for g in g_grid:\n",
        "        z_lr_mix = (1.0 - g)*z_lr_w + g*z_lr_ns\n",
        "        tz_lr_mix = (1.0 - g)*tz_lr_w + g*tz_lr_ns\n",
        "        for w_emn in emn_grid:\n",
        "            for w_emp in emp_grid:\n",
        "                for w_meta in meta_grid:\n",
        "                    for d_tot in dense_tot_grid:\n",
        "                        w_lr = 1.0 - (w_emn + w_emp + w_meta + d_tot)\n",
        "                        if w_lr <= 0 or w_lr >= 1: continue\n",
        "                        for a in alpha_grid:\n",
        "                            w_d2 = d_tot * a; w_d1 = d_tot - w_d2\n",
        "                            if w_d1 < 0 or w_d2 < 0: continue\n",
        "                            z_oof = (w_lr*z_lr_mix + w_d1*z_d1 + w_d2*z_d2 + w_meta*z_meta + w_emn*z_emn + w_emp*z_emp)\n",
        "                            auc = roc_auc_score(y[mask], z_oof[mask], sample_weight=(sample_weight[mask] if sample_weight is not None else None))\n",
        "                            tried += 1\n",
        "                            if auc > best_auc:\n",
        "                                best_auc = auc\n",
        "                                best_cfg = dict(g=float(g), w_lr=float(w_lr), w_d1=float(w_d1), w_d2=float(w_d2), w_meta=float(w_meta),\n",
        "                                                w_emn=float(w_emn), w_emp=float(w_emp), tz_lr_mix=tz_lr_mix)\n",
        "    return best_auc, best_cfg, tried\n",
        "\n",
        "# Objective A: Gamma-weighted over all validated blocks\n",
        "best_gamma, best_auc_g, best_cfg_g = None, -1.0, None\n",
        "for gamma in [0.94, 0.96, 0.98]:\n",
        "    w = np.zeros(n, dtype=np.float64)\n",
        "    for bi in range(1, k):\n",
        "        age = (k - 1) - bi  # newer blocks get higher weight\n",
        "        w[np.array(blocks[bi])] = (gamma ** age)\n",
        "    auc_g, cfg_g, tried_g = search_best(mask_full, sample_weight=w)\n",
        "    print(f'[Gamma {gamma}] tried={tried_g} | best AUC={auc_g:.5f}')\n",
        "    if auc_g > best_auc_g:\n",
        "        best_auc_g, best_cfg_g, best_gamma = auc_g, cfg_g, gamma\n",
        "print(f'[Gamma-best] gamma={best_gamma} | AUC={best_auc_g:.5f} | cfg={{k:v for k,v in best_cfg_g.items() if k!=\"tz_lr_mix\"}}')\n",
        "\n",
        "# Objective B: Larger validation = blocks 4-5 only\n",
        "auc_last2, cfg_last2, tried_l2 = search_best(mask_last2)\n",
        "print(f'[Last2] tried={tried_l2} | best AUC(last2)={auc_last2:.5f} | cfg={{k:v for k,v in cfg_last2.items() if k!=\"tz_lr_mix\"}}')\n",
        "\n",
        "# Choose primary objective: prefer gamma-best for robustness; keep last2 as alternate\n",
        "primary_tag = f'gamma{best_gamma:.2f}'.replace('.', 'p')\n",
        "primary_cfg = best_cfg_g\n",
        "alt_tag = 'last45'\n",
        "alt_cfg = cfg_last2\n",
        "\n",
        "def build_submission(tag, cfg, do_shrink=True, alphas=(0.10, 0.15, 0.20)):\n",
        "    tz_lr_mix = cfg['tz_lr_mix']\n",
        "    w_lr, w_d1, w_d2, w_meta, w_emn, w_emp = cfg['w_lr'], cfg['w_d1'], cfg['w_d2'], cfg['w_meta'], cfg['w_emn'], cfg['w_emp']\n",
        "    zt = (w_lr*tz_lr_mix + w_d1*tz_d1 + w_d2*tz_d2 + w_meta*tz_meta + w_emn*tz_emn + w_emp*tz_emp)\n",
        "    pt = sigmoid(zt).astype(np.float32)\n",
        "    pd.DataFrame({id_col: ids, target_col: pt}).to_csv(f'submission_s38_{tag}.csv', index=False)\n",
        "    print(f'Wrote submission_s38_{tag}.csv | mean={pt.mean():.6f}')\n",
        "    best_shr_auc, best_alpha, best_pt = -1.0, None, None\n",
        "    if do_shrink:\n",
        "        w_vec = np.array([w_lr, w_d1, w_d2, w_meta, w_emn, w_emp], dtype=np.float64)\n",
        "        w_eq = np.ones_like(w_vec) / len(w_vec)\n",
        "        # Evaluate shrink on gamma-weighted full mask for stability\n",
        "        w_sw = np.zeros(n, dtype=np.float64)\n",
        "        for bi in range(1, k):\n",
        "            age = (k - 1) - bi\n",
        "            w_sw[np.array(blocks[bi])] = (best_gamma ** age) if best_gamma is not None else 1.0\n",
        "        for a in alphas:\n",
        "            w_shr = ((1.0 - a)*w_vec + a*w_eq); w_shr = (w_shr / w_shr.sum()).astype(np.float64)\n",
        "            z_oof = (w_shr[0]*((1.0 - primary_cfg['g'])*z_lr_w + primary_cfg['g']*z_lr_ns) +\n",
        "                     w_shr[1]*z_d1 + w_shr[2]*z_d2 + w_shr[3]*z_meta + w_shr[4]*z_emn + w_shr[5]*z_emp)\n",
        "            auc = roc_auc_score(y[mask_full], z_oof[mask_full], sample_weight=w_sw[mask_full])\n",
        "            if auc > best_shr_auc:\n",
        "                best_shr_auc, best_alpha = auc, a\n",
        "                zt_shr = (w_shr[0]*tz_lr_mix + w_shr[1]*tz_d1 + w_shr[2]*tz_d2 + w_shr[3]*tz_meta + w_shr[4]*tz_emn + w_shr[5]*tz_emp)\n",
        "                best_pt = sigmoid(zt_shr).astype(np.float32)\n",
        "        if best_alpha is not None:\n",
        "            fn = f'submission_s38_{tag}_shrunk_a{int(best_alpha*100)}.csv'\n",
        "            pd.DataFrame({id_col: ids, target_col: best_pt}).to_csv(fn, index=False)\n",
        "            print(f'Wrote {fn} | alpha={best_alpha:.2f} | mean={best_pt.mean():.6f} | shr_AUC={best_shr_auc:.5f}')\n",
        "            return pt, best_pt, best_alpha\n",
        "    return pt, None, None\n",
        "\n",
        "# Build primary and alternate submissions\n",
        "pt_primary, pt_primary_shr, alpha_primary = build_submission(primary_tag, primary_cfg, do_shrink=True)\n",
        "pt_alt, pt_alt_shr, alpha_alt = build_submission(alt_tag, alt_cfg, do_shrink=True)\n",
        "\n",
        "# Promote conservative hedge: gamma-best shrunk if available, else gamma-best raw\n",
        "promote_path = f'submission_s38_{primary_tag}.csv'\n",
        "if pt_primary_shr is not None:\n",
        "    promote_path = f'submission_s38_{primary_tag}_shrunk_a{int(alpha_primary*100)}.csv'\n",
        "pd.read_csv(promote_path).to_csv('submission.csv', index=False)\n",
        "print('Promoted to submission.csv ->', promote_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "2cce8af3-4cf0-4f00-890c-4815d88704cc",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S39: Gamma-weighted optimizer with small recent alphas (LR_nosub + MiniLM) + optional CatBoost + shrink + hedges\n",
        "import numpy as np, pandas as pd, os, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json'); test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values; ids = test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Time blocks\n",
        "order = np.argsort(train['unix_timestamp_of_request'].values)\n",
        "k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "n = len(train)\n",
        "mask_full = np.zeros(n, bool)\n",
        "for bi in range(1, k):\n",
        "    mask_full[np.array(blocks[bi])] = True\n",
        "\n",
        "# Load base OOF/test predictions (same 7 as S38); optional CatBoost\n",
        "o_lr_w = np.load('oof_lr_time_withsub_meta.npy');   t_lr_w = np.load('test_lr_time_withsub_meta.npy')\n",
        "o_lr_ns = np.load('oof_lr_time_nosub_meta.npy');    t_lr_ns = np.load('test_lr_time_nosub_meta.npy')\n",
        "o_d1 = np.load('oof_xgb_dense_time.npy');           t_d1 = np.load('test_xgb_dense_time.npy')\n",
        "o_d2 = np.load('oof_xgb_dense_time_v2.npy');        t_d2 = np.load('test_xgb_dense_time_v2.npy')\n",
        "o_meta = np.load('oof_xgb_meta_time.npy');          t_meta = np.load('test_xgb_meta_time.npy')\n",
        "o_emn = np.load('oof_xgb_emb_meta_time.npy');       t_emn = np.load('test_xgb_emb_meta_time.npy')  # MiniLM\n",
        "o_emp = np.load('oof_xgb_emb_mpnet_time.npy');      t_emp = np.load('test_xgb_emb_mpnet_time.npy') # MPNet\n",
        "\n",
        "has_cat = os.path.exists('oof_catboost_textmeta_v2.npy') and os.path.exists('test_catboost_textmeta_v2.npy')\n",
        "if has_cat:\n",
        "    o_cat = np.load('oof_catboost_textmeta_v2.npy'); t_cat = np.load('test_catboost_textmeta_v2.npy')\n",
        "\n",
        "# Logits\n",
        "z_lr_w, z_lr_ns = to_logit(o_lr_w), to_logit(o_lr_ns)\n",
        "z_d1, z_d2, z_meta = to_logit(o_d1), to_logit(o_d2), to_logit(o_meta)\n",
        "z_emn, z_emp = to_logit(o_emn), to_logit(o_emp)\n",
        "tz_lr_w, tz_lr_ns = to_logit(t_lr_w), to_logit(t_lr_ns)\n",
        "tz_d1, tz_d2, tz_meta = to_logit(t_d1), to_logit(t_d2), to_logit(t_meta)\n",
        "tz_emn, tz_emp = to_logit(t_emn), to_logit(t_emp)\n",
        "if has_cat:\n",
        "    z_cat, tz_cat = to_logit(o_cat), to_logit(t_cat)\n",
        "\n",
        "# Load recent test-only logits (average of recent35/45) for LR_nosub and MiniLM only\n",
        "def load_recent_avg_logit(prefix):\n",
        "    arrs = []\n",
        "    for suf in ['_recent35.npy', '_recent45.npy']:\n",
        "        p = prefix + suf\n",
        "        if os.path.exists(p):\n",
        "            try:\n",
        "                arrs.append(to_logit(np.load(p)))\n",
        "            except Exception:\n",
        "                pass\n",
        "    return np.mean(arrs, axis=0).astype(np.float64) if arrs else None\n",
        "\n",
        "# Recent sources discovered previously\n",
        "tz_lr_ns_r = load_recent_avg_logit('test_lr_nosub_meta')  # LR no-sub recent\n",
        "tz_emn_r = load_recent_avg_logit('test_xgb_minilm_meta')  # MiniLM recent\n",
        "print('Recent availability:', {'lr_nosub': tz_lr_ns_r is not None, 'minilm': tz_emn_r is not None})\n",
        "\n",
        "# Grids per expert advice\n",
        "g_list = [0.96, 0.98, 0.99]  # LR mix\n",
        "dense_tot_grid = [0.18, 0.22, 0.26, 0.30]\n",
        "v2_frac_grid = [0.55, 0.65, 0.75]\n",
        "meta_grid = [0.18, 0.20, 0.22]\n",
        "emn_grid = [0.10, 0.12, 0.14]\n",
        "emp_grid = [0.08, 0.10, 0.12]\n",
        "cat_grid = [0.04, 0.06] if has_cat else [0.0]\n",
        "gamma_list = [0.96, 0.98, 0.995]\n",
        "shrink_list = [0.15, 0.20, 0.25]\n",
        "\n",
        "# Build gamma sample weights with optional double weight on newest validated block (block 5)\n",
        "def gamma_weights(gamma, double_last=True):\n",
        "    w = np.zeros(n, np.float64)\n",
        "    for bi in range(1, k):\n",
        "        age = (k - 1) - bi\n",
        "        w_block = (gamma ** age)\n",
        "        if double_last and bi == 5:\n",
        "            w_block *= 2.0\n",
        "        w[np.array(blocks[bi])] = w_block\n",
        "    return w\n",
        "\n",
        "def search_gamma_primary():\n",
        "    best_auc, best_cfg, best_gamma = -1.0, None, None\n",
        "    total_tried = 0\n",
        "    for gamma in gamma_list:\n",
        "        w = gamma_weights(gamma, double_last=True)\n",
        "        best_auc_g = -1.0; best_cfg_g = None; tried = 0\n",
        "        for g in g_list:\n",
        "            z_lr_mix = (1.0 - g) * z_lr_w + g * z_lr_ns\n",
        "            for d_tot in dense_tot_grid:\n",
        "                for v2f in v2_frac_grid:\n",
        "                    w_d2 = d_tot * v2f; w_d1 = d_tot - w_d2\n",
        "                    if w_d1 < 0 or w_d2 < 0: continue\n",
        "                    for w_meta in meta_grid:\n",
        "                        for w_emn in emn_grid:\n",
        "                            for w_emp in emp_grid:\n",
        "                                for w_cat in cat_grid:\n",
        "                                    w_sum = d_tot + w_meta + w_emn + w_emp + (w_cat if has_cat else 0.0)\n",
        "                                    w_lr = 1.0 - w_sum\n",
        "                                    if (w_lr <= 0) or (w_lr >= 1):\n",
        "                                        continue\n",
        "                                    # Enforce LRmix floor/ceiling\n",
        "                                    if not (0.28 <= w_lr <= 0.50):\n",
        "                                        continue\n",
        "                                    # Cap CatBoost (already grid-capped), ensure non-negative remaining weights\n",
        "                                    z = (w_lr * z_lr_mix + w_d1 * z_d1 + w_d2 * z_d2 + w_meta * z_meta +\n",
        "                                         w_emn * z_emn + w_emp * z_emp + ((w_cat * z_cat) if has_cat else 0.0))\n",
        "                                    auc = roc_auc_score(y[mask_full], z[mask_full], sample_weight=w[mask_full])\n",
        "                                    tried += 1; total_tried += 1\n",
        "                                    if auc > best_auc_g:\n",
        "                                        best_auc_g = auc\n",
        "                                        best_cfg_g = dict(g=float(g), w_lr=float(w_lr), w_d1=float(w_d1), w_d2=float(w_d2),\n",
        "                                                          w_meta=float(w_meta), w_emn=float(w_emn), w_emp=float(w_emp))\n",
        "                                        if has_cat: best_cfg_g['w_cat'] = float(w_cat)\n",
        "        print(f'[Gamma {gamma}] tried={tried} | best AUC={best_auc_g:.5f}')\n",
        "        if best_auc_g > best_auc:\n",
        "            best_auc, best_cfg, best_gamma = best_auc_g, best_cfg_g, gamma\n",
        "    print(f'[Gamma-primary] gamma={best_gamma} | AUC={best_auc:.5f} | cfg={{k:v for k,v in best_cfg.items()}}')\n",
        "    return best_gamma, best_auc, best_cfg\n",
        "\n",
        "best_gamma, best_auc, best_cfg = search_gamma_primary()\n",
        "\n",
        "# Build test logits for a given cfg, with optional recent interpolation\n",
        "def build_test_logits(cfg, a_lr_ns=0.0, a_mn=0.0):\n",
        "    g = cfg['g']\n",
        "    # Full-history logits\n",
        "    tz_lr_mix_full = (1.0 - g) * tz_lr_w + g * tz_lr_ns\n",
        "    tz_mn_full = tz_emn\n",
        "    # Recent targets\n",
        "    tz_lr_ns_recent = tz_lr_ns_r if tz_lr_ns_r is not None else tz_lr_ns\n",
        "    tz_lr_mix_recent = (1.0 - g) * tz_lr_w + g * tz_lr_ns_recent\n",
        "    tz_mn_recent = tz_emn_r if tz_emn_r is not None else tz_emn\n",
        "    # Apply small alphas\n",
        "    tz_lr_mix = (1.0 - a_lr_ns) * tz_lr_mix_full + a_lr_ns * tz_lr_mix_recent\n",
        "    tz_mn = (1.0 - a_mn) * tz_mn_full + a_mn * tz_mn_recent\n",
        "    # Combine\n",
        "    parts = [ (cfg['w_lr'], tz_lr_mix), (cfg['w_d1'], tz_d1), (cfg['w_d2'], tz_d2), (cfg['w_meta'], tz_meta),\n",
        "              (cfg['w_emn'], tz_mn), (cfg['w_emp'], tz_emp) ]\n",
        "    if has_cat:\n",
        "        parts.append((cfg.get('w_cat', 0.0), tz_cat))\n",
        "    zt = np.zeros_like(tz_d1, dtype=np.float64)\n",
        "    for wv, zz in parts:\n",
        "        zt += wv * zz\n",
        "    return zt\n",
        "\n",
        "# Evaluate shrink on OOF (no recents) under gamma weights; choose best alpha\n",
        "def apply_shrink_and_choose(cfg):\n",
        "    w_vec = np.array([cfg['w_lr'], cfg['w_d1'], cfg['w_d2'], cfg['w_meta'], cfg['w_emn'], cfg['w_emp']] + ([cfg['w_cat']] if has_cat else []), dtype=np.float64)\n",
        "    w_eq = np.ones_like(w_vec) / len(w_vec)\n",
        "    g = cfg['g']\n",
        "    z_lr_mix = (1.0 - g) * z_lr_w + g * z_lr_ns\n",
        "    parts = [z_lr_mix, z_d1, z_d2, z_meta, z_emn, z_emp] + ([z_cat] if has_cat else [])\n",
        "    W = gamma_weights(best_gamma, double_last=True)\n",
        "    best_a, best_auc = None, -1.0\n",
        "    for a in shrink_list:\n",
        "        w_shr = ((1.0 - a) * w_vec + a * w_eq)\n",
        "        w_shr = (w_shr / w_shr.sum()).astype(np.float64)\n",
        "        z = np.zeros_like(z_d1, dtype=np.float64)\n",
        "        for wi, zi in zip(w_shr, parts):\n",
        "            z += wi * zi\n",
        "        auc = roc_auc_score(y[mask_full], z[mask_full], sample_weight=W[mask_full])\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_a, best_w = auc, a, w_shr.copy()\n",
        "    return best_a, best_w, best_auc\n",
        "\n",
        "best_shr_alpha, best_shr_w, shr_auc = apply_shrink_and_choose(best_cfg)\n",
        "print(f'[Shrink] best alpha={best_shr_alpha:.2f} | shr_AUC={shr_auc:.5f}')\n",
        "\n",
        "# Build and save gamma-shrunk submission (no recent interpolation)\n",
        "def build_from_weights_vec(w_vec, cfg):\n",
        "    g = cfg['g']\n",
        "    tz_lr_mix = (1.0 - g) * tz_lr_w + g * tz_lr_ns\n",
        "    parts = [tz_lr_mix, tz_d1, tz_d2, tz_meta, tz_emn, tz_emp] + ([tz_cat] if has_cat else [])\n",
        "    zt = np.zeros_like(tz_d1, dtype=np.float64)\n",
        "    for wi, zi in zip(w_vec, parts):\n",
        "        zt += wi * zi\n",
        "    return zt\n",
        "\n",
        "# Map shrunk vector back to named cfg for convenience\n",
        "def named_cfg_from_shrunk(w_vec, cfg):\n",
        "    keys = ['w_lr','w_d1','w_d2','w_meta','w_emn','w_emp'] + (['w_cat'] if has_cat else [])\n",
        "    out = dict(cfg)\n",
        "    for key, val in zip(keys, w_vec):\n",
        "        out[key] = float(val)\n",
        "    return out\n",
        "\n",
        "# Save gamma-shrunk\n",
        "keys = ['w_lr','w_d1','w_d2','w_meta','w_emn','w_emp'] + (['w_cat'] if has_cat else [])\n",
        "base_w = np.array([best_cfg[k] if k in best_cfg else 0.0 for k in keys], dtype=np.float64)\n",
        "w_gamma_shr = best_shr_w\n",
        "zt_gamma_shr = build_from_weights_vec(w_gamma_shr, best_cfg)\n",
        "p_gamma_shr = sigmoid(zt_gamma_shr).astype(np.float32)\n",
        "fn_gamma = f'submission_s39_gamma_shrunk_a{int(best_shr_alpha*100)}.csv'\n",
        "pd.DataFrame({id_col: ids, target_col: p_gamma_shr}).to_csv(fn_gamma, index=False)\n",
        "print('Wrote', fn_gamma, '| mean', float(p_gamma_shr.mean()))\n",
        "\n",
        "# Recent alpha search: small alphas in {0, .05, .10, .15} for LR_nosub and MiniLM; enforce r\u22480.24 and r\u22480.30\n",
        "a_grid = [0.0, 0.05, 0.10, 0.15]\n",
        "def pick_recent_alphas(cfg, r_target, tol=0.02):\n",
        "    # Effective recent share r \u2248 w_lrmix * g * a_lr_ns + w_emn * a_mn\n",
        "    g = cfg['g']\n",
        "    w_lr = cfg['w_lr']; w_mn = cfg['w_emn']\n",
        "    best = (0.0, 0.0); best_err = 1e9; best_sum = 1e9\n",
        "    for a_lr_ns in a_grid:\n",
        "        for a_mn in a_grid:\n",
        "            # If recent artifact missing, force alpha=0 for that model\n",
        "            if tz_lr_ns_r is None and a_lr_ns > 0: continue\n",
        "            if tz_emn_r is None and a_mn > 0: continue\n",
        "            r_est = w_lr * g * a_lr_ns + w_mn * a_mn\n",
        "            err = abs(r_est - r_target); sm = a_lr_ns + a_mn\n",
        "            if (err < best_err) or (abs(err - best_err) < 1e-12 and sm < best_sum):\n",
        "                best_err, best_sum, best = err, sm, (a_lr_ns, a_mn)\n",
        "    return best\n",
        "\n",
        "# Build r~0.24 and r~0.30 with the gamma-shrunk weights\n",
        "cfg_gamma_shr = named_cfg_from_shrunk(w_gamma_shr, best_cfg)\n",
        "a24 = pick_recent_alphas(cfg_gamma_shr, 0.24)\n",
        "a30 = pick_recent_alphas(cfg_gamma_shr, 0.30)\n",
        "print('Chosen recent alphas: r24', a24, '| r30', a30)\n",
        "\n",
        "zt_r24 = build_test_logits(cfg_gamma_shr, a_lr_ns=a24[0], a_mn=a24[1])\n",
        "p_r24 = sigmoid(zt_r24).astype(np.float32)\n",
        "fn_r24 = f'submission_s39_r24_shrunk_a{int(best_shr_alpha*100)}.csv'\n",
        "pd.DataFrame({id_col: ids, target_col: p_r24}).to_csv(fn_r24, index=False)\n",
        "print('Wrote', fn_r24, '| mean', float(p_r24.mean()))\n",
        "\n",
        "zt_r30 = build_test_logits(cfg_gamma_shr, a_lr_ns=a30[0], a_mn=a30[1])\n",
        "p_r30 = sigmoid(zt_r30).astype(np.float32)\n",
        "fn_r30 = f'submission_s39_r30_shrunk_a{int(best_shr_alpha*100)}.csv'\n",
        "pd.DataFrame({id_col: ids, target_col: p_r30}).to_csv(fn_r30, index=False)\n",
        "print('Wrote', fn_r30, '| mean', float(p_r30.mean()))\n",
        "\n",
        "# Hedged submissions: 2-way (gamma_shrunk, r24_shrunk) and 3-way (+ r30_shrunk)\n",
        "def logit_avg(paths, out):\n",
        "    zs = [to_logit(pd.read_csv(p)[target_col].values.astype(np.float64)) for p in paths]\n",
        "    z = np.mean(zs, axis=0); p = sigmoid(z).astype(np.float32)\n",
        "    pd.DataFrame({id_col: ids, target_col: p}).to_csv(out, index=False)\n",
        "    print('Wrote', out, '| mean', float(p.mean()))\n",
        "\n",
        "fn_hedge2 = 'submission_s39_hedge2_gamma_r24.csv'\n",
        "logit_avg([fn_gamma, fn_r24], fn_hedge2)\n",
        "fn_hedge3 = 'submission_s39_hedge3_gamma_r24_r30.csv'\n",
        "logit_avg([fn_gamma, fn_r24, fn_r30], fn_hedge3)\n",
        "\n",
        "# Promote safest hedge (2-way) to submission.csv\n",
        "pd.read_csv(fn_hedge2).to_csv('submission.csv', index=False)\n",
        "print('Promoted', fn_hedge2, 'to submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "84802a94-1f1b-4a5b-b45b-9627f82044c5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S39-cat: Generate full-test predictions for CatBoost text+meta v2 (to enable has_cat in S39)\n",
        "import numpy as np, pandas as pd, os, sys, time\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json'); test = pd.read_json('test.json')\n",
        "\n",
        "def get_text(df: pd.DataFrame) -> pd.Series:\n",
        "    title = df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "    body = df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "    return (title + '\\n' + body)\n",
        "\n",
        "# Build feature frame: one text column + numeric metas present in BOTH train and test\n",
        "txt_tr = get_text(train); txt_te = get_text(test)\n",
        "drop_cols = {target_col, id_col, 'request_title', 'request_text', 'request_text_edit_aware', 'unix_timestamp_of_request'}\n",
        "train_num = [c for c in train.columns if (c not in drop_cols) and pd.api.types.is_numeric_dtype(train[c])]\n",
        "test_num = [c for c in test.columns if (c not in drop_cols) and pd.api.types.is_numeric_dtype(test[c])]\n",
        "num_cols = sorted(set(train_num).intersection(test_num))\n",
        "print('Numeric cols used (intersection):', len(num_cols))\n",
        "X_tr = pd.concat([txt_tr.rename('txt'), train[num_cols]], axis=1)\n",
        "X_te = pd.concat([txt_te.rename('txt'), test[num_cols]], axis=1)\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "text_features = [0]  # first column is 'txt'\n",
        "train_pool = Pool(X_tr, label=y, text_features=text_features)\n",
        "test_pool = Pool(X_te, text_features=text_features)\n",
        "\n",
        "params = dict(\n",
        "    loss_function='Logloss',\n",
        "    eval_metric='AUC',\n",
        "    learning_rate=0.05,\n",
        "    depth=6,\n",
        "    l2_leaf_reg=5.0,\n",
        "    random_seed=42,\n",
        "    iterations=700,\n",
        "    task_type='GPU',\n",
        "    devices='0',\n",
        "    verbose=100\n",
        ")\n",
        "\n",
        "print('Training CatBoost text+meta v2 on full train...')\n",
        "t0 = time.time()\n",
        "model = CatBoostClassifier(**params)\n",
        "model.fit(train_pool)\n",
        "print(f'Training done in {time.time()-t0:.1f}s')\n",
        "\n",
        "print('Predicting test probabilities...')\n",
        "proba = model.predict_proba(test_pool)[:,1].astype(np.float32)\n",
        "np.save('test_catboost_textmeta_v2.npy', proba)\n",
        "print('Saved test_catboost_textmeta_v2.npy | mean=', float(proba.mean()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "6dca0120-e961-40f6-a365-4edfc89919e0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S40: Time-aware L2 stacker on base logits + small recency alphas + hedges\n",
        "import numpy as np, pandas as pd, os, time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col='request_id'; target_col='requester_received_pizza'\n",
        "train=pd.read_json('train.json'); test=pd.read_json('test.json')\n",
        "y=train[target_col].astype(int).values; ids=test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0-eps)\n",
        "    return np.log(p/(1.0-p))\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "# 6-block forward-chaining masks and gamma weights\n",
        "order=np.argsort(train['unix_timestamp_of_request'].values); k=6; blocks=np.array_split(order,k); n=len(train)\n",
        "mask_full=np.zeros(n,bool)\n",
        "for bi in range(1,k): mask_full[np.array(blocks[bi])]=True\n",
        "def gamma_weights(gamma=0.995, double_last=True):\n",
        "    w=np.zeros(n,np.float64)\n",
        "    for bi in range(1,k):\n",
        "        age=(k-1)-bi; wb=(gamma**age)\n",
        "        if double_last and bi==5: wb*=2.0\n",
        "        w[np.array(blocks[bi])]=wb\n",
        "    return w\n",
        "W=gamma_weights(0.995, double_last=True)\n",
        "\n",
        "# Load base OOF/test probs, convert to logits; columns in fixed order\n",
        "paths_oof=[\n",
        " 'oof_lr_time_withsub_meta.npy',\n",
        " 'oof_lr_time_nosub_meta.npy',\n",
        " 'oof_xgb_dense_time.npy',\n",
        " 'oof_xgb_dense_time_v2.npy',\n",
        " 'oof_xgb_meta_time.npy',\n",
        " 'oof_xgb_emb_meta_time.npy',\n",
        " 'oof_xgb_emb_mpnet_time.npy'\n",
        "]\n",
        "paths_te=[\n",
        " 'test_lr_time_withsub_meta.npy',\n",
        " 'test_lr_time_nosub_meta.npy',\n",
        " 'test_xgb_dense_time.npy',\n",
        " 'test_xgb_dense_time_v2.npy',\n",
        " 'test_xgb_meta_time.npy',\n",
        " 'test_xgb_emb_meta_time.npy',\n",
        " 'test_xgb_emb_mpnet_time.npy'\n",
        "]\n",
        "names=['LR_withsub','LR_nosub','Dense_v1','Dense_v2','Meta','MiniLM','MPNet']\n",
        "\n",
        "# Optional CatBoost column if OOF exists\n",
        "has_cat=os.path.exists('oof_catboost_textmeta_v2.npy') and os.path.exists('test_catboost_textmeta_v2.npy')\n",
        "if has_cat:\n",
        "    paths_oof.append('oof_catboost_textmeta_v2.npy'); paths_te.append('test_catboost_textmeta_v2.npy'); names.append('CatBoost')\n",
        "\n",
        "oof_list=[to_logit(np.load(p)) for p in paths_oof]\n",
        "te_list=[to_logit(np.load(p)) for p in paths_te]\n",
        "X_oof=np.column_stack(oof_list).astype(np.float64)\n",
        "X_te_full=np.column_stack(te_list).astype(np.float64)\n",
        "print('Stack columns:', names)\n",
        "\n",
        "# Standardize columns\n",
        "sc=StandardScaler(with_mean=True, with_std=True)\n",
        "Xz=sc.fit_transform(X_oof)\n",
        "Xz_te=sc.transform(X_te_full)\n",
        "\n",
        "# Tune C via gamma-weighted OOF AUC on mask_full\n",
        "C_grid=[0.05, 0.1, 0.3, 0.5, 1.0, 1.5, 3.0]\n",
        "best_auc=-1.0; best_C=None; best_model=None\n",
        "t0=time.time()\n",
        "for C in C_grid:\n",
        "    lr=LogisticRegression(penalty='l2', solver='lbfgs', C=C, max_iter=2000, fit_intercept=True)\n",
        "    lr.fit(Xz[mask_full], y[mask_full], sample_weight=W[mask_full])\n",
        "    z=lr.decision_function(Xz[mask_full])\n",
        "    auc=roc_auc_score(y[mask_full], z, sample_weight=W[mask_full])\n",
        "    print(f'C={C} | gamma-weighted OOF AUC={auc:.5f}')\n",
        "    if auc>best_auc: best_auc, best_C, best_model=auc, C, lr\n",
        "print(f'Best C={best_C} | AUC={best_auc:.5f} | time={time.time()-t0:.1f}s')\n",
        "coef=best_model.coef_.ravel(); intercept=float(best_model.intercept_[0])\n",
        "print('Coef (name, coef):', list(zip(names, coef)))\n",
        "\n",
        "# Build meta_gamma test logits/probs (no recency)\n",
        "z_te_gamma = (Xz_te @ coef) + intercept\n",
        "p_meta_gamma = sigmoid(z_te_gamma).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: p_meta_gamma}).to_csv('submission_s40_meta_gamma.csv', index=False)\n",
        "print('Wrote submission_s40_meta_gamma.csv | mean', float(p_meta_gamma.mean()))\n",
        "\n",
        "# Load recent test-only logits for LR_nosub and MiniLM (average of 35/45) if available\n",
        "def load_recent_avg_logit(prefix):\n",
        "    arrs=[]\n",
        "    for suf in ['_recent35.npy','_recent45.npy']:\n",
        "        p=prefix+suf\n",
        "        if os.path.exists(p):\n",
        "            try: arrs.append(to_logit(np.load(p)))\n",
        "            except: pass\n",
        "    return np.mean(arrs,axis=0).astype(np.float64) if arrs else None\n",
        "\n",
        "# Identify column indices\n",
        "idx_lr_ns = names.index('LR_nosub') if 'LR_nosub' in names else None\n",
        "idx_minilm = names.index('MiniLM') if 'MiniLM' in names else None\n",
        "tz_lr_ns_r = load_recent_avg_logit('test_lr_nosub_meta') if idx_lr_ns is not None else None\n",
        "tz_emn_r   = load_recent_avg_logit('test_xgb_minilm_meta') if idx_minilm is not None else None\n",
        "print('Recent present:', {'LR_nosub': tz_lr_ns_r is not None, 'MiniLM': tz_emn_r is not None})\n",
        "\n",
        "# Helper to rebuild transformed X_test with small alphas on raw-logit columns, then re-standardize\n",
        "a_grid=[0.0, 0.05, 0.10, 0.15]\n",
        "def build_Xz_te_with_recency(a_lr_ns, a_mn):\n",
        "    X_mod = X_te_full.copy()\n",
        "    if (idx_lr_ns is not None) and (tz_lr_ns_r is not None):\n",
        "        X_mod[:, idx_lr_ns] = (1.0 - a_lr_ns)*X_te_full[:, idx_lr_ns] + a_lr_ns*tz_lr_ns_r\n",
        "    if (idx_minilm is not None) and (tz_emn_r is not None):\n",
        "        X_mod[:, idx_minilm] = (1.0 - a_mn)*X_te_full[:, idx_minilm] + a_mn*tz_emn_r\n",
        "    return sc.transform(X_mod)\n",
        "\n",
        "# Pick alphas to target r \u2248 0.24 and 0.30 using normalized |coef| shares on those two columns\n",
        "def pick_alphas(target_r):\n",
        "    s_lr = abs(coef[idx_lr_ns]) if (idx_lr_ns is not None) else 0.0\n",
        "    s_mn = abs(coef[idx_minilm]) if (idx_minilm is not None) else 0.0\n",
        "    if (s_lr+s_mn) <= 0: s_lr=s_mn=1.0\n",
        "    s_lr /= (s_lr+s_mn); s_mn = 1.0 - s_lr\n",
        "    best=(0.0,0.0); best_err=1e9; best_sum=1e9\n",
        "    for a_lr in a_grid:\n",
        "        for a_mn in a_grid:\n",
        "            if (tz_lr_ns_r is None) and a_lr>0: continue\n",
        "            if (tz_emn_r is None) and a_mn>0: continue\n",
        "            r_est = s_lr*a_lr + s_mn*a_mn\n",
        "            err = abs(r_est - target_r); sm=a_lr+a_mn\n",
        "            if (err<best_err) or (abs(err-best_err)<1e-12 and sm<best_sum):\n",
        "                best_err, best_sum, best = err, sm, (a_lr, a_mn)\n",
        "    return best\n",
        "\n",
        "a24 = pick_alphas(0.24)\n",
        "a30 = pick_alphas(0.30)\n",
        "print('Chosen alphas -> r24:', a24, '| r30:', a30)\n",
        "\n",
        "# Build meta_r24 and meta_r30\n",
        "Xz_te_r24 = build_Xz_te_with_recency(a24[0], a24[1])\n",
        "z_te_r24 = (Xz_te_r24 @ coef) + intercept\n",
        "p_meta_r24 = sigmoid(z_te_r24).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: p_meta_r24}).to_csv('submission_s40_meta_r24.csv', index=False)\n",
        "print('Wrote submission_s40_meta_r24.csv | mean', float(p_meta_r24.mean()))\n",
        "\n",
        "Xz_te_r30 = build_Xz_te_with_recency(a30[0], a30[1])\n",
        "z_te_r30 = (Xz_te_r30 @ coef) + intercept\n",
        "p_meta_r30 = sigmoid(z_te_r30).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: p_meta_r30}).to_csv('submission_s40_meta_r30.csv', index=False)\n",
        "print('Wrote submission_s40_meta_r30.csv | mean', float(p_meta_r30.mean()))\n",
        "\n",
        "# Hedged submissions: 2-way and 3-way logit averages\n",
        "def logit_avg(paths, out):\n",
        "    zs=[to_logit(pd.read_csv(p)[target_col].values.astype(np.float64)) for p in paths]\n",
        "    z=np.mean(zs,axis=0); p=sigmoid(z).astype(np.float32)\n",
        "    pd.DataFrame({id_col: ids, target_col: p}).to_csv(out, index=False)\n",
        "    print('Wrote', out, '| mean', float(p.mean()))\n",
        "\n",
        "logit_avg(['submission_s40_meta_gamma.csv','submission_s40_meta_r24.csv'], 'submission_s40_meta_hedge2.csv')\n",
        "logit_avg(['submission_s40_meta_gamma.csv','submission_s40_meta_r24.csv','submission_s40_meta_r30.csv'], 'submission_s40_meta_hedge3.csv')\n",
        "\n",
        "# Optional robustness hedge: 0.85*meta_gamma + 0.15*eq-weight base logits\n",
        "z_eq = np.mean(X_te_full, axis=1)  # equal-weight average of base logits\n",
        "z_meta = z_te_gamma\n",
        "p_robust = sigmoid(0.85*z_meta + 0.15*z_eq).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: p_robust}).to_csv('submission_s40_meta_robust.csv', index=False)\n",
        "print('Wrote submission_s40_meta_robust.csv | mean', float(p_robust.mean()))\n",
        "\n",
        "# Promote 2-way hedged stacker to submission.csv\n",
        "pd.read_csv('submission_s40_meta_hedge2.csv').to_csv('submission.csv', index=False)\n",
        "print('Promoted submission_s40_meta_hedge2.csv to submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "c56688a1-90ab-447a-8448-e7e81c6cb66f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Promote safer S39 hedge2 to submission.csv\n",
        "import pandas as pd\n",
        "src = 'submission_s39_hedge2_gamma_r24.csv'\n",
        "df = pd.read_csv(src)\n",
        "df.to_csv('submission.csv', index=False)\n",
        "print('Promoted', src, 'to submission.csv | mean =', float(df['requester_received_pizza'].mean()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "8cbc6da3-8e0e-4630-9c31-3dafb7bc16f2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S40b: Stacker variants (reduced search) -> pick best gamma-weighted OOF, rebuild submissions\n",
        "import numpy as np, pandas as pd, os, time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "id_col='request_id'; target_col='requester_received_pizza'\n",
        "train=pd.read_json('train.json'); test=pd.read_json('test.json')\n",
        "y=train[target_col].astype(int).values; ids=test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1.0-eps)\n",
        "    return np.log(p/(1.0-p))\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "# Time masks and gamma weights\n",
        "order=np.argsort(train['unix_timestamp_of_request'].values); k=6; blocks=np.array_split(order,k); n=len(train)\n",
        "mask_full=np.zeros(n,bool)\n",
        "for bi in range(1,k): mask_full[np.array(blocks[bi])]=True\n",
        "def gamma_weights(gamma=0.995, double_last=True):\n",
        "    w=np.zeros(n,np.float64)\n",
        "    for bi in range(1,k):\n",
        "        age=(k-1)-bi; wb=(gamma**age)\n",
        "        if double_last and bi==5: wb*=2.0\n",
        "        w[np.array(blocks[bi])]=wb\n",
        "    return w\n",
        "W=gamma_weights(0.995, double_last=True)\n",
        "\n",
        "# Base arrays (order fixed); exclude CatBoost to avoid instability\n",
        "base_oof_paths=[\n",
        " 'oof_lr_time_withsub_meta.npy',\n",
        " 'oof_lr_time_nosub_meta.npy',\n",
        " 'oof_xgb_dense_time.npy',\n",
        " 'oof_xgb_dense_time_v2.npy',\n",
        " 'oof_xgb_meta_time.npy',\n",
        " 'oof_xgb_emb_meta_time.npy',\n",
        " 'oof_xgb_emb_mpnet_time.npy'\n",
        "]\n",
        "base_te_paths=[\n",
        " 'test_lr_time_withsub_meta.npy',\n",
        " 'test_lr_time_nosub_meta.npy',\n",
        " 'test_xgb_dense_time.npy',\n",
        " 'test_xgb_dense_time_v2.npy',\n",
        " 'test_xgb_meta_time.npy',\n",
        " 'test_xgb_emb_meta_time.npy',\n",
        " 'test_xgb_emb_mpnet_time.npy'\n",
        "]\n",
        "base_names=['LR_withsub','LR_nosub','Dense_v1','Dense_v2','Meta','MiniLM','MPNet']\n",
        "\n",
        "X_oof_full=np.column_stack([to_logit(np.load(p)) for p in base_oof_paths]).astype(np.float64)\n",
        "X_te_full=np.column_stack([to_logit(np.load(p)) for p in base_te_paths]).astype(np.float64)\n",
        "print('Stack variants (reduced) over columns:', base_names)\n",
        "\n",
        "def fit_eval_stack(X_tr, y_tr, W_tr, use_scaler, C_grid):\n",
        "    if use_scaler:\n",
        "        sc=StandardScaler(with_mean=True, with_std=True); Xz=sc.fit_transform(X_tr)\n",
        "    else:\n",
        "        sc=None; Xz=X_tr\n",
        "    best_auc=-1; best_C=None; best_model=None\n",
        "    for C in C_grid:\n",
        "        lr=LogisticRegression(penalty='l2', solver='lbfgs', C=C, max_iter=2000, fit_intercept=True)\n",
        "        lr.fit(Xz, y_tr, sample_weight=W_tr)\n",
        "        z=lr.decision_function(Xz)\n",
        "        auc=roc_auc_score(y_tr, z, sample_weight=W_tr)\n",
        "        if auc>best_auc: best_auc, best_C, best_model=auc, C, lr\n",
        "    return best_auc, best_C, best_model, sc\n",
        "\n",
        "C_grid=[0.1,0.3,1.0,3.0]\n",
        "variants=[]\n",
        "for use_scaler in [True, False]:\n",
        "    auc, Cbest, model, sc = fit_eval_stack(X_oof_full[mask_full], y[mask_full], W[mask_full], use_scaler, C_grid)\n",
        "    variants.append(dict(auc=auc, scaler=use_scaler, C=Cbest, model=model, sc=sc))\n",
        "    print(f'Variant scaler={use_scaler} | AUC={auc:.5f} | C={Cbest}')\n",
        "\n",
        "best = max(variants, key=lambda d: d['auc'])\n",
        "print('Best variant:', {k: (v if k not in ['model','sc'] else '...') for k,v in best.items()})\n",
        "\n",
        "# Build submissions for best variant\n",
        "model=best['model']; sc=best['sc']\n",
        "Xz_te = sc.transform(X_te_full) if sc is not None else X_te_full\n",
        "coef = model.coef_.ravel(); intercept=float(model.intercept_[0])\n",
        "print('Chosen coef len:', len(coef), '| intercept:', intercept)\n",
        "\n",
        "# meta_gamma\n",
        "z_te_gamma = (Xz_te @ coef) + intercept\n",
        "p_meta_gamma = sigmoid(z_te_gamma).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: p_meta_gamma}).to_csv('submission_s40b_meta_gamma.csv', index=False)\n",
        "print('Wrote submission_s40b_meta_gamma.csv | mean', float(p_meta_gamma.mean()))\n",
        "\n",
        "# Recency (LR_nosub + MiniLM) on raw-logit space then transform with scaler if any\n",
        "def load_recent_avg_logit(prefix):\n",
        "    arrs=[]\n",
        "    for suf in ['_recent35.npy','_recent45.npy']:\n",
        "        p=prefix+suf\n",
        "        if os.path.exists(p):\n",
        "            try: arrs.append(to_logit(np.load(p)))\n",
        "            except: pass\n",
        "    return np.mean(arrs,axis=0).astype(np.float64) if arrs else None\n",
        "\n",
        "idx_lr_ns = base_names.index('LR_nosub')\n",
        "idx_minilm = base_names.index('MiniLM')\n",
        "tz_lr_ns_r = load_recent_avg_logit('test_lr_nosub_meta')\n",
        "tz_emn_r   = load_recent_avg_logit('test_xgb_minilm_meta')\n",
        "print('Recent present:', {'LR_nosub': tz_lr_ns_r is not None, 'MiniLM': tz_emn_r is not None})\n",
        "\n",
        "a_grid=[0.0,0.05,0.10,0.15]\n",
        "def build_Xz_te_with_recency(a_lr_ns, a_mn):\n",
        "    X_mod = X_te_full.copy()\n",
        "    if tz_lr_ns_r is not None:\n",
        "        X_mod[:, idx_lr_ns] = (1.0 - a_lr_ns)*X_te_full[:, idx_lr_ns] + a_lr_ns*tz_lr_ns_r\n",
        "    if tz_emn_r is not None:\n",
        "        X_mod[:, idx_minilm] = (1.0 - a_mn)*X_te_full[:, idx_minilm] + a_mn*tz_emn_r\n",
        "    return (sc.transform(X_mod) if sc is not None else X_mod)\n",
        "\n",
        "def pick_alphas(target_r):\n",
        "    s_lr = abs(coef[idx_lr_ns]); s_mn = abs(coef[idx_minilm])\n",
        "    if (s_lr+s_mn) <= 0: s_lr=s_mn=1.0\n",
        "    s_lr /= (s_lr+s_mn); s_mn = 1.0 - s_lr\n",
        "    best=(0.0,0.0); best_err=1e9; best_sum=1e9\n",
        "    for a_lr in a_grid:\n",
        "        for a_mn in a_grid:\n",
        "            if (tz_lr_ns_r is None) and a_lr>0: continue\n",
        "            if (tz_emn_r is None) and a_mn>0: continue\n",
        "            r_est = s_lr*a_lr + s_mn*a_mn\n",
        "            err = abs(r_est - target_r); sm=a_lr+a_mn\n",
        "            if (err<best_err) or (abs(err-best_err)<1e-12 and sm<best_sum):\n",
        "                best_err, best_sum, best = err, sm, (a_lr, a_mn)\n",
        "    return best\n",
        "\n",
        "a24 = pick_alphas(0.24); a30 = pick_alphas(0.30)\n",
        "print('Chosen alphas (S40b) -> r24:', a24, '| r30:', a30)\n",
        "\n",
        "Xz_te_r24 = build_Xz_te_with_recency(a24[0], a24[1])\n",
        "z_te_r24 = (Xz_te_r24 @ coef) + intercept\n",
        "p_meta_r24 = sigmoid(z_te_r24).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: p_meta_r24}).to_csv('submission_s40b_meta_r24.csv', index=False)\n",
        "print('Wrote submission_s40b_meta_r24.csv | mean', float(p_meta_r24.mean()))\n",
        "\n",
        "Xz_te_r30 = build_Xz_te_with_recency(a30[0], a30[1])\n",
        "z_te_r30 = (Xz_te_r30 @ coef) + intercept\n",
        "p_meta_r30 = sigmoid(z_te_r30).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: p_meta_r30}).to_csv('submission_s40b_meta_r30.csv', index=False)\n",
        "print('Wrote submission_s40b_meta_r30.csv | mean', float(p_meta_r30.mean()))\n",
        "\n",
        "def logit_avg(paths, out):\n",
        "    zs=[to_logit(pd.read_csv(p)[target_col].values.astype(np.float64)) for p in paths]\n",
        "    z=np.mean(zs,axis=0); p=sigmoid(z).astype(np.float32)\n",
        "    pd.DataFrame({id_col: ids, target_col: p}).to_csv(out, index=False)\n",
        "    print('Wrote', out, '| mean', float(p.mean()))\n",
        "\n",
        "logit_avg(['submission_s40b_meta_gamma.csv','submission_s40b_meta_r24.csv'], 'submission_s40b_meta_hedge2.csv')\n",
        "logit_avg(['submission_s40b_meta_gamma.csv','submission_s40b_meta_r24.csv','submission_s40b_meta_r30.csv'], 'submission_s40b_meta_hedge3.csv')\n",
        "\n",
        "print('Note: not auto-promoting S40b; compare hedges vs S39 hedge before promoting.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "1cdacfd3-9d73-47e4-893f-8c5522b0df64",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S41-LGBM-OOF: True forward-chaining LightGBM stacker (no test-only features) + recency hedges + bias portfolio\n",
        "import os, time, json, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import lightgbm as lgb\n",
        "\n",
        "t0 = time.time()\n",
        "print('S41-LGBM-OOF: starting true time-aware OOF LightGBM stacker ...')\n",
        "\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json'); test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(np.asarray(p, dtype=np.float64), eps, 1-eps)\n",
        "    return np.log(p/(1-p))\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "# 1) Time ordering and 6 contiguous blocks\n",
        "ts_col = 'unix_timestamp_of_request' if 'unix_timestamp_of_request' in train.columns else 'unix_timestamp_of_request_utc'\n",
        "order = np.argsort(train[ts_col].values)\n",
        "k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "ntr = len(train)\n",
        "nte = len(test)\n",
        "\n",
        "# 2) Load base OOF/test probabilities for selected bases; convert to logits\n",
        "base_specs = [\n",
        "    ('LR_nosub', 'oof_lr_time_nosub_meta.npy', 'test_lr_time_nosub_meta.npy'),\n",
        "    ('Dense_v1', 'oof_xgb_dense_time.npy', 'test_xgb_dense_time.npy'),\n",
        "    ('Meta',    'oof_xgb_meta_time.npy',  'test_xgb_meta_time.npy'),\n",
        "    ('MiniLM',  'oof_xgb_emb_meta_time.npy', 'test_xgb_emb_meta_time.npy'),\n",
        "    ('MPNet',   'oof_xgb_emb_mpnet_time.npy', 'test_xgb_emb_mpnet_time.npy'),\n",
        "    ('E5_meta', 'oof_xgb_e5_meta_time.npy', 'test_xgb_e5_meta_time.npy'),\n",
        "    # Optional text+meta base if available\n",
        "    ('CatBoost_v2', 'oof_catboost_textmeta_v2.npy', 'test_catboost_textmeta_v2.npy'),\n",
        "]\n",
        "\n",
        "base_raw = []  # list of (name, oof_prob, te_prob)\n",
        "for name, oof_fp, te_fp in base_specs:\n",
        "    if (not os.path.exists(oof_fp)) or (not os.path.exists(te_fp)):\n",
        "        continue\n",
        "    oof = np.load(oof_fp); tep = np.load(te_fp)\n",
        "    if oof.ndim>1: oof=oof.ravel()\n",
        "    if tep.ndim>1: tep=tep.ravel()\n",
        "    if len(oof) != ntr:\n",
        "        continue\n",
        "    base_raw.append((name, oof.astype(np.float64), tep.astype(np.float64)))\n",
        "\n",
        "if not base_raw:\n",
        "    raise RuntimeError('No base OOF/Test pairs found. Aborting S41-LGBM-OOF.')\n",
        "\n",
        "# 2b) Per-base last-block AUC and pruning: keep bases with AUC_last >= 0.60; optionally include CatBoost_v2 if >=0.65\n",
        "last_block_idx = blocks[4]  # block 5 indices\n",
        "def safe_auc(y_true, p):\n",
        "    try:\n",
        "        return float(roc_auc_score(y_true, p))\n",
        "    except Exception:\n",
        "        return float('nan')\n",
        "\n",
        "rows = []\n",
        "for (name, oof, te) in base_raw:\n",
        "    auc_last = safe_auc(y[last_block_idx], oof[last_block_idx])\n",
        "    rows.append((name, auc_last))\n",
        "rows_sorted = sorted(rows, key=lambda t: (-(t[1] if not np.isnan(t[1]) else -1)))\n",
        "print('Base last-block AUCs (desc):', rows_sorted)\n",
        "\n",
        "# Basic keep by threshold 0.60\n",
        "filtered = [(bn, o, t) for (bn, o, t) in base_raw if safe_auc(y[last_block_idx], o[last_block_idx]) >= 0.60]\n",
        "\n",
        "# If CatBoost_v2 present but <0.65, drop it\n",
        "filtered2 = []\n",
        "for (bn, o, t) in filtered:\n",
        "    if bn == 'CatBoost_v2':\n",
        "        auc_n = safe_auc(y[last_block_idx], o[last_block_idx])\n",
        "        if auc_n < 0.65:\n",
        "            continue\n",
        "    filtered2.append((bn, o, t))\n",
        "filtered = filtered2\n",
        "\n",
        "if len(filtered) > 7:\n",
        "    keep_names = set([nm for (nm, _) in rows_sorted[:7]])\n",
        "    filtered = [tpl for tpl in filtered if tpl[0] in keep_names]\n",
        "if len(filtered) < 4:\n",
        "    keep_names = set([nm for (nm, _) in rows_sorted[:6]])\n",
        "    filtered = [tpl for tpl in base_raw if tpl[0] in keep_names]\n",
        "\n",
        "names = [bn for (bn,_,_) in filtered]\n",
        "Z_oof_list = [to_logit(o) for (_,o,_) in filtered]\n",
        "Z_te_list  = [to_logit(t) for (_,_,t) in filtered]\n",
        "print('Included bases after pruning:', names)\n",
        "X_oof_b = np.column_stack(Z_oof_list).astype(np.float64)\n",
        "X_te_b  = np.column_stack(Z_te_list).astype(np.float64)\n",
        "\n",
        "# 3) Build meta features: log1p_text_len, account_age_days, time_rank01 (computed across train+test)\n",
        "def get_text(df):\n",
        "    title = df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "    body = df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "    return title + '\\n' + body\n",
        "tr_txt = get_text(train); te_txt = get_text(test)\n",
        "tr_len = tr_txt.str.len().values.astype(np.float64); te_len = te_txt.str.len().values.astype(np.float64)\n",
        "tr_log1p = np.log1p(tr_len); te_log1p = np.log1p(te_len)\n",
        "\n",
        "def get_numeric_col(df, col, length):\n",
        "    if col in df.columns:\n",
        "        s = pd.to_numeric(df[col], errors='coerce')\n",
        "    else:\n",
        "        s = pd.Series([np.nan]*length)\n",
        "    s = s.fillna(0.0).astype(np.float64)\n",
        "    return s.values\n",
        "\n",
        "tr_age = get_numeric_col(train, 'requester_account_age_in_days_at_request', ntr)\n",
        "te_age = get_numeric_col(test, 'requester_account_age_in_days_at_request', nte)\n",
        "\n",
        "ts_tr = train[ts_col].values.astype(np.int64); ts_te = (test[ts_col].values.astype(np.int64) if ts_col in test.columns else np.zeros(nte, np.int64))\n",
        "all_ts = np.concatenate([ts_tr, ts_te])\n",
        "ord_all = np.argsort(all_ts)\n",
        "rank_all = np.empty_like(ord_all)\n",
        "rank_all[ord_all] = np.arange(len(all_ts))\n",
        "rank01_all = rank_all.astype(np.float64) / max(1, (len(all_ts)-1))\n",
        "tr_rank = rank01_all[:ntr]; te_rank = rank01_all[ntr:]\n",
        "\n",
        "X_oof = np.column_stack([X_oof_b, tr_log1p, tr_age, tr_rank]).astype(np.float64)\n",
        "X_te  = np.column_stack([X_te_b, te_log1p, te_age, te_rank]).astype(np.float64)\n",
        "feat_cols = names + ['log1p_text_len','account_age_days','time_rank01']\n",
        "nb = len(names)\n",
        "print('Final meta columns:', feat_cols)\n",
        "\n",
        "# 4) True forward-chaining OOF for meta (validate on blocks 2..5)\n",
        "val_blocks = [2,3,4,5]\n",
        "oof_pred = np.full(ntr, np.nan, dtype=np.float64)\n",
        "gamma = 0.995\n",
        "\n",
        "params = dict(\n",
        "    learning_rate=0.03,\n",
        "    num_leaves=31,\n",
        "    max_depth=5,\n",
        "    min_data_in_leaf=80,\n",
        "    feature_fraction=1.0,\n",
        "    bagging_fraction=0.8,\n",
        "    bagging_freq=1,\n",
        "    lambda_l2=5.0,\n",
        "    min_gain_to_split=0.001,\n",
        "    n_estimators=1200,\n",
        "    objective='binary',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "for vb in val_blocks:\n",
        "    tr_idx = np.concatenate([blocks[i-1] for i in range(1, vb)])  # blocks < vb\n",
        "    va_idx = blocks[vb-1]  # block vb\n",
        "    gbm = lgb.LGBMClassifier(**params)\n",
        "    gbm.fit(\n",
        "        X_oof[tr_idx], y[tr_idx],\n",
        "        eval_set=[(X_oof[va_idx], y[va_idx])],\n",
        "        eval_metric='auc',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(50)]\n",
        "    )\n",
        "    oof_pred[va_idx] = gbm.predict_proba(X_oof[va_idx], num_iteration=gbm.best_iteration_)[:,1]\n",
        "    print(f'VB={vb} done | va_size={len(va_idx)} | elapsed={time.time()-t0:.1f}s | best_iter={gbm.best_iteration_}')\n",
        "\n",
        "# Report AUC_last (block 5) and gamma-weighted AUC over blocks 2..5 (double weight for block 5)\n",
        "mask_oof = np.isfinite(oof_pred)\n",
        "assert mask_oof.sum() == sum(len(blocks[i-1]) for i in val_blocks)\n",
        "auc_last = roc_auc_score(y[blocks[4-1]], oof_pred[blocks[4-1]])  # block 5 -> index 4\n",
        "W = np.zeros(ntr, dtype=np.float64)\n",
        "for bi in range(1, k):\n",
        "    age = (k - 1) - bi\n",
        "    w_block = (gamma ** age)\n",
        "    if bi == 5:  # double weight for last validated block\n",
        "        w_block *= 2.0\n",
        "    W[blocks[bi]] = w_block\n",
        "auc_full_gamma = roc_auc_score(y[mask_oof], oof_pred[mask_oof], sample_weight=W[mask_oof])\n",
        "print(f'AUC_last (block5)={auc_last:.5f} | AUC_full_gamma(b2..b5)={auc_full_gamma:.5f}')\n",
        "\n",
        "# 5) Final meta fit: train on blocks 1..4, early stop on block 5\n",
        "tr_final_idx = np.concatenate([blocks[i] for i in range(0,4)])  # 0..3\n",
        "va_final_idx = blocks[4]  # block 5\n",
        "final_model = lgb.LGBMClassifier(**params)\n",
        "final_model.fit(\n",
        "    X_oof[tr_final_idx], y[tr_final_idx],\n",
        "    eval_set=[(X_oof[va_final_idx], y[va_final_idx])],\n",
        "    eval_metric='auc',\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(50)]\n",
        ")\n",
        "print('Final meta trained | best_iter=', final_model.best_iteration_)\n",
        "\n",
        "# 6) meta_gamma prediction\n",
        "p_gamma = final_model.predict_proba(X_te, num_iteration=final_model.best_iteration_)[:,1].astype(np.float32)\n",
        "pd.DataFrame({id_col: test[id_col].values, target_col: p_gamma}).to_csv('submission_s41lgb_meta_gamma.csv', index=False)\n",
        "print('Wrote submission_s41lgb_meta_gamma.csv | mean', float(p_gamma.mean()))\n",
        "\n",
        "# 7) Safe recency injection at test only: build r_low and r_high feature variants, then reuse trained meta\n",
        "recent_map = {\n",
        "    'LR_nosub': [\n",
        "        'test_lr_time_nosub_meta_recent35.npy',\n",
        "        'test_lr_time_nosub_meta_recent45.npy',\n",
        "    ],\n",
        "    'MiniLM': [\n",
        "        'test_xgb_emb_meta_time_recent35.npy',\n",
        "        'test_xgb_emb_meta_time_recent45.npy',\n",
        "    ],\n",
        "    'MPNet': [\n",
        "        'test_xgb_emb_mpnet_time_recent35.npy',\n",
        "        'test_xgb_emb_mpnet_time_recent45.npy',\n",
        "    ],\n",
        "}\n",
        "name_to_col = {n:i for i,n in enumerate(names)}\n",
        "\n",
        "def load_recent_avg_logit(files):\n",
        "    arrs = []\n",
        "    for fp in files:\n",
        "        if os.path.exists(fp):\n",
        "            a = np.load(fp)\n",
        "            if a.ndim>1: a=a.ravel()\n",
        "            arrs.append(to_logit(a))\n",
        "    if not arrs:\n",
        "        return None\n",
        "    return np.mean(arrs, axis=0).astype(np.float64)\n",
        "\n",
        "def apply_recency_to_Xte(X_base, alphas):\n",
        "    # alphas: dict base_name -> alpha, interpolate in logit space\n",
        "    Xr = X_base.copy()\n",
        "    for bname, a in alphas.items():\n",
        "        if a <= 0: continue\n",
        "        if bname not in name_to_col: continue\n",
        "        rec = load_recent_avg_logit(recent_map.get(bname, []))\n",
        "        if rec is None: continue\n",
        "        j = name_to_col[bname]\n",
        "        z_full = X_base[:, j]\n",
        "        z_recent = rec\n",
        "        Xr[:, j] = (1.0 - a)*z_full + a*z_recent\n",
        "    return Xr\n",
        "\n",
        "# r_low and r_high per expert advice (no training-time recency)\n",
        "alphas_low = {'LR_nosub': 0.00, 'MiniLM': 0.15, 'MPNet': 0.20}\n",
        "alphas_high = {'LR_nosub': 0.05, 'MiniLM': 0.25, 'MPNet': 0.30}\n",
        "\n",
        "X_te_low = apply_recency_to_Xte(X_te, alphas_low)\n",
        "X_te_high = apply_recency_to_Xte(X_te, alphas_high)\n",
        "p_low = final_model.predict_proba(X_te_low, num_iteration=final_model.best_iteration_)[:,1].astype(np.float32)\n",
        "p_high = final_model.predict_proba(X_te_high, num_iteration=final_model.best_iteration_)[:,1].astype(np.float32)\n",
        "pd.DataFrame({id_col: test[id_col].values, target_col: p_low}).to_csv('submission_s41lgb_meta_r_low.csv', index=False)\n",
        "pd.DataFrame({id_col: test[id_col].values, target_col: p_high}).to_csv('submission_s41lgb_meta_r_high.csv', index=False)\n",
        "print('Wrote r_low/r_high | means ->', float(p_low.mean()), float(p_high.mean()))\n",
        "\n",
        "# 8) 3-way logit hedge: gamma, r_low, r_high\n",
        "def logit_clip(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1-eps)\n",
        "    return np.log(p/(1-p))\n",
        "z_g = logit_clip(p_gamma); z_l = logit_clip(p_low); z_h = logit_clip(p_high)\n",
        "p_hedge3 = sigmoid((z_g + z_l + z_h)/3.0).astype(np.float32)\n",
        "pd.DataFrame({id_col: test[id_col].values, target_col: p_hedge3}).to_csv('submission_s41lgb_meta_hedge3.csv', index=False)\n",
        "print('Wrote submission_s41lgb_meta_hedge3.csv | mean', float(p_hedge3.mean()))\n",
        "\n",
        "# 9) Bias portfolio: means 0.30 (promote), 0.32, 0.28 using monotone logit shift\n",
        "def bias_to_mean(probs, target, tol=1e-6, it=100):\n",
        "    z = logit_clip(probs); lo, hi = -10.0, 10.0\n",
        "    for _ in range(it):\n",
        "        mid = 0.5*(lo+hi); m = sigmoid(z+mid).mean()\n",
        "        if abs(m - target) < tol: return mid\n",
        "        if m < target: lo = mid\n",
        "        else: hi = mid\n",
        "    return 0.5*(lo+hi)\n",
        "\n",
        "for tm in [0.30, 0.32, 0.28]:\n",
        "    b = bias_to_mean(p_hedge3, tm)\n",
        "    pm = sigmoid(logit_clip(p_hedge3) + b).astype(np.float32)\n",
        "    outp = f'submission_s41lgb_meta_hedge3_m{int(round(tm*100)):03d}.csv'\n",
        "    pd.DataFrame({id_col: test[id_col].values, target_col: pm}).to_csv(outp, index=False)\n",
        "    print(f'Wrote {outp} | mean={pm.mean():.6f} | bias={b:.4f}')\n",
        "    if abs(tm - 0.30) < 1e-9:\n",
        "        pd.DataFrame({id_col: test[id_col].values, target_col: pm}).to_csv('submission.csv', index=False)\n",
        "        print('PROMOTED: submission.csv <-', outp)\n",
        "\n",
        "print(f'S41-LGBM-OOF done in {time.time()-t0:.1f}s')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S41-LGBM-OOF: starting true time-aware OOF LightGBM stacker ...\nBase last-block AUCs (desc): [('CatBoost_v2', 0.650664850639457), ('MiniLM', 0.6444665035320191), ('Dense_v1', 0.6396417193776259), ('MPNet', 0.6346091693984025), ('LR_nosub', 0.6235744955907474), ('Meta', 0.621023592963664), ('E5_meta', 0.6164642873632208)]\nIncluded bases after pruning: ['LR_nosub', 'Dense_v1', 'Meta', 'MiniLM', 'MPNet', 'E5_meta', 'CatBoost_v2']\nFinal meta columns: ['LR_nosub', 'Dense_v1', 'Meta', 'MiniLM', 'MPNet', 'E5_meta', 'CatBoost_v2', 'log1p_text_len', 'account_age_days', 'time_rank01']\n[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Info] Number of positive: 163, number of negative: 317\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000158 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 414\n[LightGBM] [Info] Number of data points in the train set: 480, number of used features: 3\n[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.339583 -> initscore=-0.665152\n[LightGBM] [Info] Start training from score -0.665152\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 100 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[50]\tvalid_0's auc: 0.703887\tvalid_0's binary_logloss: 0.543729\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[100]\tvalid_0's auc: 0.686175\tvalid_0's binary_logloss: 0.534058\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[4]\tvalid_0's auc: 0.713251\tvalid_0's binary_logloss: 0.582458\n[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\nVB=2 done | va_size=480 | elapsed=0.2s | best_iter=4\n[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Info] Number of positive: 288, number of negative: 672\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000210 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2499\n[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 10\n[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300000 -> initscore=-0.847298\n[LightGBM] [Info] Start training from score -0.847298\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 100 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[50]\tvalid_0's auc: 0.674783\tvalid_0's binary_logloss: 0.535931\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[100]\tvalid_0's auc: 0.676394\tvalid_0's binary_logloss: 0.539878\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[150]\tvalid_0's auc: 0.67111\tvalid_0's binary_logloss: 0.551556\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[59]\tvalid_0's auc: 0.675752\tvalid_0's binary_logloss: 0.534422\n[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\nVB=3 done | va_size=480 | elapsed=0.3s | best_iter=59\n[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Info] Number of positive: 413, number of negative: 1027\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000198 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 1440, number of used features: 10\n[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.286806 -> initscore=-0.910950\n[LightGBM] [Info] Start training from score -0.910950\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 100 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[50]\tvalid_0's auc: 0.625643\tvalid_0's binary_logloss: 0.496922\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[100]\tvalid_0's auc: 0.622378\tvalid_0's binary_logloss: 0.503836\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[21]\tvalid_0's auc: 0.628935\tvalid_0's binary_logloss: 0.496909\n[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\nVB=4 done | va_size=480 | elapsed=0.4s | best_iter=21\n[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Info] Number of positive: 506, number of negative: 1414\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000209 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 1920, number of used features: 10\n[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.263542 -> initscore=-1.027641\n[LightGBM] [Info] Start training from score -1.027641\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 100 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[50]\tvalid_0's auc: 0.64741\tvalid_0's binary_logloss: 0.544402\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/app/.pip-target/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/app/.pip-target/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/app/.pip-target/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[100]\tvalid_0's auc: 0.632693\tvalid_0's binary_logloss: 0.546232\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[32]\tvalid_0's auc: 0.650815\tvalid_0's binary_logloss: 0.546082\n[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\nVB=5 done | va_size=479 | elapsed=0.4s | best_iter=32\nAUC_last (block5)=0.62894 | AUC_full_gamma(b2..b5)=0.63168\n[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Info] Number of positive: 506, number of negative: 1414\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000222 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 1920, number of used features: 10\n[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.263542 -> initscore=-1.027641\n[LightGBM] [Info] Start training from score -1.027641\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 100 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[50]\tvalid_0's auc: 0.64741\tvalid_0's binary_logloss: 0.544402\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[100]\tvalid_0's auc: 0.632693\tvalid_0's binary_logloss: 0.546232\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[32]\tvalid_0's auc: 0.650815\tvalid_0's binary_logloss: 0.546082\nFinal meta trained | best_iter= 32\n[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\nWrote submission_s41lgb_meta_gamma.csv | mean 0.2563827335834503\n[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n[LightGBM] [Warning] min_gain_to_split is set=0.001, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.001\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\nWrote r_low/r_high | means -> 0.24745973944664001 0.2430056482553482\nWrote submission_s41lgb_meta_hedge3.csv | mean 0.24886150658130646\nWrote submission_s41lgb_meta_hedge3_m030.csv | mean=0.300000 | bias=0.2642\nPROMOTED: submission.csv <- submission_s41lgb_meta_hedge3_m030.csv\nWrote submission_s41lgb_meta_hedge3_m032.csv | mean=0.320000 | bias=0.3604\nWrote submission_s41lgb_meta_hedge3_m028.csv | mean=0.280000 | bias=0.1644\nS41-LGBM-OOF done in 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/app/.pip-target/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/app/.pip-target/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "id": "c82e5343-e3cd-4d0c-94d9-cf6c6f12b325",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S41d-bias: Monotonic logit bias shift to target mean=0.40 and 50/50 hedge\n",
        "import numpy as np, pandas as pd, math, os, sys, time\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1.0/(1.0+np.exp(-x))\n",
        "\n",
        "def logit(p):\n",
        "    p = np.clip(p, 1e-6, 1-1e-6)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def find_bias_for_target_mean(probs, target_mean, tol=1e-6, max_iter=100):\n",
        "    z = logit(probs)\n",
        "    # Bisection on bias b: f(b)=mean(sigmoid(z+b)) - target\n",
        "    lo, hi = -10.0, 10.0\n",
        "    for _ in range(max_iter):\n",
        "        mid = 0.5*(lo+hi)\n",
        "        m = sigmoid(z+mid).mean()\n",
        "        if abs(m - target_mean) < tol:\n",
        "            return mid\n",
        "        if m < target_mean:\n",
        "            lo = mid\n",
        "        else:\n",
        "            hi = mid\n",
        "    return 0.5*(lo+hi)\n",
        "\n",
        "def apply_bias(probs, bias):\n",
        "    return sigmoid(logit(probs) + bias)\n",
        "\n",
        "def load_sub(path):\n",
        "    df = pd.read_csv(path)\n",
        "    # Normalize column names\n",
        "    cols = [c.lower() for c in df.columns]\n",
        "    if 'request_id' in cols:\n",
        "        id_col = df.columns[cols.index('request_id')]\n",
        "    elif 'id' in cols:\n",
        "        id_col = df.columns[cols.index('id')]\n",
        "    else:\n",
        "        raise ValueError('No id/request_id column found in ' + path)\n",
        "    prob_col = [c for c in df.columns if c != id_col][0]\n",
        "    return df[[id_col, prob_col]].rename(columns={id_col:'request_id', prob_col:'requester_received_pizza'})\n",
        "\n",
        "def save_sub(df, path):\n",
        "    out = df.copy()\n",
        "    out.columns = ['request_id','requester_received_pizza']\n",
        "    out.to_csv(path, index=False)\n",
        "    print(f'Wrote {os.path.basename(path)} | mean={out.requester_received_pizza.mean():.6f}')\n",
        "\n",
        "t0 = time.time()\n",
        "gamma_path = 'submission_s41_meta_gamma.csv'\n",
        "r24_path = 'submission_s41_meta_r24.csv'\n",
        "assert os.path.exists(gamma_path), 'Missing ' + gamma_path\n",
        "assert os.path.exists(r24_path), 'Missing ' + r24_path\n",
        "sg = load_sub(gamma_path)\n",
        "sr = load_sub(r24_path)\n",
        "assert np.all(sg.request_id.values == sr.request_id.values), 'Mismatched ids between gamma and r24'\n",
        "\n",
        "target_mean = 0.40\n",
        "print(f'Original means: gamma={sg.requester_received_pizza.mean():.6f} | r24={sr.requester_received_pizza.mean():.6f}')\n",
        "b_g = find_bias_for_target_mean(sg.requester_received_pizza.values, target_mean)\n",
        "b_r = find_bias_for_target_mean(sr.requester_received_pizza.values, target_mean)\n",
        "pg = apply_bias(sg.requester_received_pizza.values, b_g)\n",
        "pr = apply_bias(sr.requester_received_pizza.values, b_r)\n",
        "sg_m = pd.DataFrame({'request_id': sg.request_id.values, 'requester_received_pizza': pg})\n",
        "sr_m = pd.DataFrame({'request_id': sr.request_id.values, 'requester_received_pizza': pr})\n",
        "save_sub(sg_m, 'submission_s41_meta_gamma_m040.csv')\n",
        "save_sub(sr_m, 'submission_s41_meta_r24_m040.csv')\n",
        "\n",
        "# Logit-hedge 50/50 after shifting each to mean 0.40\n",
        "lg = logit(pg)\n",
        "lr_ = logit(pr)\n",
        "hedge_logits = 0.5*(lg + lr_)\n",
        "ph = sigmoid(hedge_logits)\n",
        "sh_m = pd.DataFrame({'request_id': sg.request_id.values, 'requester_received_pizza': ph})\n",
        "save_sub(sh_m, 'submission_s41_meta_hedge_m040.csv')\n",
        "\n",
        "# Promote hedge to submission.csv\n",
        "save_sub(sh_m, 'submission.csv')\n",
        "print(f'S41d-bias completed in {time.time()-t0:.2f}s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "43117d4a-260f-4525-9fc6-f7aa8373b5cb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S41d-bias-variants: Generate mean 0.38 and 0.42 hedged submissions\n",
        "import numpy as np, pandas as pd, os, time\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1.0/(1.0+np.exp(-x))\n",
        "def logit(p):\n",
        "    p = np.log(np.clip(p,1e-6,1-1e-6)/(1-np.clip(p,1e-6,1-1e-6)))\n",
        "def find_bias_for_target_mean(probs, target_mean, tol=1e-6, max_iter=100):\n",
        "    z = logit(probs)\n",
        "    lo, hi = -10.0, 10.0\n",
        "    for _ in range(max_iter):\n",
        "        mid = 0.5*(lo+hi)\n",
        "        m = sigmoid(z+mid).mean()\n",
        "        if abs(m - target_mean) < tol:\n",
        "            return mid\n",
        "        if m < target_mean: lo = mid\n",
        "        else: hi = mid\n",
        "    return 0.5*(lo+hi)\n",
        "def apply_bias(probs, bias):\n",
        "    return sigmoid(logit(probs)+bias)\n",
        "def load_sub(path):\n",
        "    df = pd.read_csv(path)\n",
        "    cols = [c.lower() for c in df.columns]\n",
        "    if 'request_id' in cols:\n",
        "        id_col = df.columns[cols.index('request_id')]\n",
        "    elif 'id' in cols:\n",
        "        id_col = df.columns[cols.index('id')]\n",
        "    else:\n",
        "        raise ValueError('No id/request_id column found in ' + path)\n",
        "    prob_col = [c for c in df.columns if c != id_col][0]\n",
        "    return df[[id_col, prob_col]].rename(columns={id_col:'request_id', prob_col:'requester_received_pizza'})\n",
        "def save_sub(df, path):\n",
        "    out = df.copy()\n",
        "    out.columns = ['request_id','requester_received_pizza']\n",
        "    out.to_csv(path, index=False)\n",
        "    print(f'Wrote {os.path.basename(path)} | mean={out.requester_received_pizza.mean():.6f}')\n",
        "\n",
        "gamma_path = 'submission_s41_meta_gamma.csv'\n",
        "r24_path = 'submission_s41_meta_r24.csv'\n",
        "sg = load_sub(gamma_path)\n",
        "sr = load_sub(r24_path)\n",
        "assert np.all(sg.request_id.values == sr.request_id.values)\n",
        "\n",
        "for tm in [0.38, 0.42]:\n",
        "    b_g = find_bias_for_target_mean(sg.requester_received_pizza.values, tm)\n",
        "    b_r = find_bias_for_target_mean(sr.requester_received_pizza.values, tm)\n",
        "    pg = apply_bias(sg.requester_received_pizza.values, b_g)\n",
        "    pr = apply_bias(sr.requester_received_pizza.values, b_r)\n",
        "    lg, lr_ = logit(pg), logit(pr)\n",
        "    ph = sigmoid(0.5*(lg+lr_))\n",
        "    dfh = pd.DataFrame({'request_id': sg.request_id.values, 'requester_received_pizza': ph})\n",
        "    save_sub(dfh, f'submission_s41_meta_hedge_m{int(tm*100):03d}.csv')\n",
        "\n",
        "# Promote 0.42 variant as next submission\n",
        "df42 = pd.read_csv('submission_s41_meta_hedge_m042.csv')\n",
        "df42.columns = ['request_id','requester_received_pizza']\n",
        "df42.to_csv('submission.csv', index=False)\n",
        "print(f'Promoted submission.csv | mean={df42.requester_received_pizza.mean():.6f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "b4c68219-5cfe-420f-9595-12b7c9ca6ca7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S41d-bias-variants (fixed): Use existing bias-shift helpers from Cell 7 to make 0.38 and 0.42 hedges\n",
        "import numpy as np, pandas as pd, os, time\n",
        "\n",
        "t0 = time.time()\n",
        "gamma_path = 'submission_s41_meta_gamma.csv'\n",
        "r24_path = 'submission_s41_meta_r24.csv'\n",
        "assert os.path.exists(gamma_path) and os.path.exists(r24_path), 'Missing gamma/r24 files'\n",
        "sg = load_sub(gamma_path)\n",
        "sr = load_sub(r24_path)\n",
        "assert np.all(sg.request_id.values == sr.request_id.values), 'ID mismatch'\n",
        "\n",
        "for tm in [0.38, 0.42]:\n",
        "    b_g = find_bias_for_target_mean(sg.requester_received_pizza.values, tm)\n",
        "    b_r = find_bias_for_target_mean(sr.requester_received_pizza.values, tm)\n",
        "    pg = apply_bias(sg.requester_received_pizza.values, b_g)\n",
        "    pr = apply_bias(sr.requester_received_pizza.values, b_r)\n",
        "    lg, lr_ = logit(pg), logit(pr)\n",
        "    ph = sigmoid(0.5*(lg + lr_))\n",
        "    dfh = pd.DataFrame({'request_id': sg.request_id.values, 'requester_received_pizza': ph})\n",
        "    out_path = f'submission_s41_meta_hedge_m{int(round(tm*100)):03d}.csv'\n",
        "    save_sub(dfh, out_path)\n",
        "\n",
        "# Promote 0.42 variant\n",
        "df42 = pd.read_csv('submission_s41_meta_hedge_m042.csv')\n",
        "df42.columns = ['request_id','requester_received_pizza']\n",
        "df42.to_csv('submission.csv', index=False)\n",
        "print(f'Promoted submission.csv | mean={df42.requester_received_pizza.mean():.6f} | took {time.time()-t0:.2f}s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "c9d91caa-fbb8-44cd-befe-2f653a88d6d5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Promote 0.38-mean hedge to submission.csv and report mean\n",
        "import pandas as pd\n",
        "df38 = pd.read_csv('submission_s41_meta_hedge_m038.csv')\n",
        "df38.columns = ['request_id','requester_received_pizza']\n",
        "df38.to_csv('submission.csv', index=False)\n",
        "print(f'Promoted submission.csv | mean={df38.requester_received_pizza.mean():.6f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "2b790504-7d02-437f-b29f-12a8b52ccb22",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S41c: Platt calibration on S41 outputs to fix underconfidence and re-hedge\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.special import expit\n",
        "\n",
        "t0 = time.time()\n",
        "print('S41c: starting Platt calibration...')\n",
        "\n",
        "def logit_clip(p, eps=1e-6):\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def sigmoid(x):\n",
        "    return expit(x)\n",
        "\n",
        "# Guard: ensure prior cell ran\n",
        "assert 'oof_pred' in globals() and 'y_tr' in globals() and 'W_tr' in globals(), 'S41 artifacts missing'\n",
        "\n",
        "# Fit Platt on logit(oof_pred) -> y_tr with sample weights\n",
        "z_tr = logit_clip(oof_pred).reshape(-1, 1)\n",
        "platt = LogisticRegression(C=10.0, solver='lbfgs', max_iter=1000, class_weight=None, random_state=42)\n",
        "platt.fit(z_tr, y_tr, sample_weight=W_tr)\n",
        "a = float(platt.coef_.ravel()[0])\n",
        "b = float(platt.intercept_.ravel()[0])\n",
        "print(f'Platt params: a={a:.4f}, b={b:.4f}')\n",
        "\n",
        "# Load S41 submissions and apply calibration\n",
        "sub_g = pd.read_csv('submission_s41_meta_gamma.csv')\n",
        "sub_r24_path = 'submission_s41_meta_r24.csv'\n",
        "sub_r20_path = 'submission_s41_meta_r20.csv'\n",
        "sub_r_path = sub_r24_path if os.path.exists(sub_r24_path) else sub_r20_path\n",
        "sub_r = pd.read_csv(sub_r_path)\n",
        "\n",
        "def apply_platt_to_series(p):\n",
        "    z = logit_clip(p.values)\n",
        "    return pd.Series(sigmoid(a*z + b), index=p.index)\n",
        "\n",
        "sub_g['requester_received_pizza'] = apply_platt_to_series(sub_g['requester_received_pizza'])\n",
        "sub_r['requester_received_pizza'] = apply_platt_to_series(sub_r['requester_received_pizza'])\n",
        "\n",
        "sub_g.to_csv('submission_s41c_meta_gamma.csv', index=False)\n",
        "sub_r.to_csv('submission_s41c_meta_r.csv', index=False)\n",
        "print(f'Calibrated means -> gamma: {sub_g.requester_received_pizza.mean():.6f} | r: {sub_r.requester_received_pizza.mean():.6f}')\n",
        "\n",
        "# 2-way logit hedge after calibration\n",
        "p1 = sub_g.requester_received_pizza.values\n",
        "p2 = sub_r.requester_received_pizza.values\n",
        "lz = 0.5*logit_clip(p1) + 0.5*logit_clip(p2)\n",
        "p_hedge = sigmoid(lz)\n",
        "sub_h = pd.DataFrame({'id': sub_g.id, 'requester_received_pizza': p_hedge})\n",
        "sub_h.to_csv('submission_s41c_meta_hedge2.csv', index=False)\n",
        "m_h = sub_h.requester_received_pizza.mean()\n",
        "print(f'Wrote submission_s41c_meta_hedge2.csv | mean {m_h:.6f}')\n",
        "\n",
        "# Promotion guardrail: AUC unchanged by monotone calibration; reuse S41 auc in memory\n",
        "auc_ok = ('auc' in globals()) and (auc >= 0.692)\n",
        "mean_ok = 0.39 <= m_h <= 0.43\n",
        "if auc_ok and mean_ok:\n",
        "    sub_h.to_csv('submission.csv', index=False)\n",
        "    print('PROMOTED: submission.csv <- submission_s41c_meta_hedge2.csv')\n",
        "else:\n",
        "    print(f'Not promoting S41c; auc_ok={auc_ok}, mean_ok={mean_ok}')\n",
        "\n",
        "print(f'S41c done in {time.time()-t0:.2f}s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "1c17e422-5713-4e43-a4f3-f4317046074c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S41d: Monotonic bias correction to target submission mean ~0.40 without changing ranking\n",
        "import numpy as np, pandas as pd, time, os\n",
        "from scipy.special import expit\n",
        "\n",
        "print('S41d: starting bias correction to target mean ~0.40')\n",
        "\n",
        "def logit_clip(p, eps=1e-6):\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def shift_to_target_mean(p, target=0.40):\n",
        "    z = logit_clip(p)\n",
        "    # bisection on delta so that mean(sigmoid(z+delta)) ~= target\n",
        "    lo, hi = -10.0, 10.0\n",
        "    for _ in range(50):\n",
        "        mid = 0.5*(lo+hi)\n",
        "        m = expit(z + mid).mean()\n",
        "        if m < target:\n",
        "            lo = mid\n",
        "        else:\n",
        "            hi = mid\n",
        "    delta = 0.5*(lo+hi)\n",
        "    p_new = expit(z + delta)\n",
        "    return p_new, float(delta), float(p_new.mean())\n",
        "\n",
        "# Load S41 base submissions\n",
        "sub_g = pd.read_csv('submission_s41_meta_gamma.csv')\n",
        "sub_r_path = 'submission_s41_meta_r24.csv' if os.path.exists('submission_s41_meta_r24.csv') else 'submission_s41_meta_r20.csv'\n",
        "sub_r = pd.read_csv(sub_r_path)\n",
        "\n",
        "# Target mean window per guardrail\n",
        "target_mean = 0.40\n",
        "p_g_new, d_g, m_g = shift_to_target_mean(sub_g.requester_received_pizza.values, target=target_mean)\n",
        "p_r_new, d_r, m_r = shift_to_target_mean(sub_r.requester_received_pizza.values, target=target_mean)\n",
        "print(f'Applied deltas -> gamma: {d_g:.4f} (mean {m_g:.6f}), recent: {d_r:.4f} (mean {m_r:.6f})')\n",
        "\n",
        "sub_g_cal = pd.DataFrame({'id': sub_g.id, 'requester_received_pizza': p_g_new})\n",
        "sub_r_cal = pd.DataFrame({'id': sub_r.id, 'requester_received_pizza': p_r_new})\n",
        "sub_g_cal.to_csv('submission_s41d_meta_gamma.csv', index=False)\n",
        "sub_r_cal.to_csv('submission_s41d_meta_r.csv', index=False)\n",
        "print(f'Wrote submission_s41d_meta_gamma.csv | mean {m_g:.6f}')\n",
        "print(f'Wrote submission_s41d_meta_r.csv | mean {m_r:.6f}')\n",
        "\n",
        "# 2-way logit-average hedge after bias correction\n",
        "lz1 = logit_clip(sub_g_cal.requester_received_pizza.values)\n",
        "lz2 = logit_clip(sub_r_cal.requester_received_pizza.values)\n",
        "p_hedge = expit(0.5*lz1 + 0.5*lz2)\n",
        "sub_h = pd.DataFrame({'id': sub_g_cal.id, 'requester_received_pizza': p_hedge})\n",
        "sub_h.to_csv('submission_s41d_meta_hedge2.csv', index=False)\n",
        "m_h = sub_h.requester_received_pizza.mean()\n",
        "print(f'Wrote submission_s41d_meta_hedge2.csv | mean {m_h:.6f}')\n",
        "\n",
        "# Promote if AUC from S41 was sufficient and mean within window\n",
        "auc_ok = ('auc' in globals()) and (auc >= 0.692)\n",
        "mean_ok = 0.39 <= m_h <= 0.43\n",
        "if auc_ok and mean_ok:\n",
        "    sub_h.to_csv('submission.csv', index=False)\n",
        "    print('PROMOTED: submission.csv <- submission_s41d_meta_hedge2.csv')\n",
        "else:\n",
        "    print(f'Not promoting S41d; auc_ok={auc_ok}, mean_ok={mean_ok}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "58da3b05-1857-4cbf-9b5f-32a644db6648",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S41e: Validate submission.csv format for Kaggle\n",
        "import pandas as pd, numpy as np\n",
        "import json, os\n",
        "print('Validating submission.csv ...')\n",
        "sub = pd.read_csv('submission.csv')\n",
        "print('Shape:', sub.shape)\n",
        "print('Columns:', list(sub.columns))\n",
        "print(sub.head(5))\n",
        "n_na = sub.isna().sum().to_dict()\n",
        "print('NA per column:', n_na)\n",
        "print('requester_received_pizza stats -> min/max/mean:', float(sub['requester_received_pizza'].min()), float(sub['requester_received_pizza'].max()), float(sub['requester_received_pizza'].mean()))\n",
        "print('Dtypes:', sub.dtypes.to_dict())\n",
        "print('Unique id count:', sub['id'].nunique())\n",
        "print('Any probs out of bounds:', bool(((sub['requester_received_pizza'] < 0) | (sub['requester_received_pizza'] > 1)).any()))\n",
        "with open('test.json', 'r') as f:\n",
        "    te_json = json.load(f)\n",
        "expected_n = len(te_json)\n",
        "print('Expected test rows:', expected_n)\n",
        "if sub.shape[0] != expected_n:\n",
        "    print('ERROR: Row count mismatch')\n",
        "else:\n",
        "    # Check id match set\n",
        "    te_ids = pd.Series([x.get('id') for x in te_json])\n",
        "    set_diff1 = set(sub['id']) - set(te_ids)\n",
        "    set_diff2 = set(te_ids) - set(sub['id'])\n",
        "    print('ID diff sizes -> sub-not-in-test:', len(set_diff1), ' test-not-in-sub:', len(set_diff2))\n",
        "    if len(set_diff1) or len(set_diff2):\n",
        "        print('Sample differences (up to 5):', list(sorted(list(set_diff1))[:5]), list(sorted(list(set_diff2))[:5]))\n",
        "print('Validation done.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "e1233a8e-f552-4676-81cc-7569c3b6f994",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S41f: Fix submission IDs from test.json\n",
        "import json, pandas as pd, numpy as np, os\n",
        "print('S41f: inspecting test.json keys to fix submission IDs...')\n",
        "with open('test.json', 'r') as f:\n",
        "    te_json = json.load(f)\n",
        "n_te = len(te_json)\n",
        "print('Test rows:', n_te)\n",
        "\n",
        "candidates = ['id', 'request_id', 'post_id', 'name']\n",
        "counts = {}\n",
        "for k in candidates:\n",
        "    vals = [x.get(k) for x in te_json]\n",
        "    counts[k] = sum(v is not None for v in vals)\n",
        "print('Non-null counts per key:', counts)\n",
        "\n",
        "# choose first key with full coverage\n",
        "chosen = None\n",
        "for k in candidates:\n",
        "    if counts.get(k, 0) == n_te:\n",
        "        chosen = k\n",
        "        break\n",
        "if chosen is None:\n",
        "    # fallback: prefer request_id if mostly present\n",
        "    chosen = max(counts, key=lambda k: counts[k])\n",
        "print('Chosen key for IDs:', chosen)\n",
        "\n",
        "ids = [x.get(chosen) for x in te_json]\n",
        "# ensure no None remains; if any, fill with sequential indices as last resort\n",
        "if any(v is None for v in ids):\n",
        "    print('Warning: some IDs missing; filling with index-based placeholders')\n",
        "    ids = [v if v is not None else i for i, v in enumerate(ids)]\n",
        "\n",
        "# Load best probs from S41d hedge (already promoted earlier)\n",
        "sub_path = 'submission_s41d_meta_hedge2.csv' if os.path.exists('submission_s41d_meta_hedge2.csv') else 'submission_s41_meta_hedge2.csv'\n",
        "sub = pd.read_csv(sub_path)\n",
        "print('Loaded probs from', sub_path, 'shape', sub.shape, 'mean', float(sub['requester_received_pizza'].mean()))\n",
        "\n",
        "# Overwrite id column with extracted IDs in same order as test.json\n",
        "sub_fixed = pd.DataFrame({'id': ids, 'requester_received_pizza': sub['requester_received_pizza'].values})\n",
        "print('Fixed NA in id:', int(sub_fixed['id'].isna().sum()))\n",
        "\n",
        "# Save fixed submission and promote\n",
        "sub_fixed.to_csv('submission_fixed.csv', index=False)\n",
        "sub_fixed.to_csv('submission.csv', index=False)\n",
        "print('Wrote submission_fixed.csv and promoted to submission.csv')\n",
        "print('Head:')\n",
        "print(sub_fixed.head(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "b554088d-3390-46bc-8096-86f5b13550cc",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S41g: Ensure submission has correct header 'request_id' per competition format\n",
        "import pandas as pd, json, os\n",
        "print('S41g: rewriting submission.csv with request_id header')\n",
        "sub = pd.read_csv('submission.csv')\n",
        "cols = list(sub.columns)\n",
        "print('Current columns:', cols)\n",
        "if 'request_id' not in sub.columns and 'id' in sub.columns:\n",
        "    sub = sub.rename(columns={'id': 'request_id'})\n",
        "    print('Renamed id -> request_id')\n",
        "sub = sub[['request_id', 'requester_received_pizza']]\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Wrote submission.csv with columns:', list(sub.columns))\n",
        "\n",
        "# Quick validation against test.json request_id keys\n",
        "with open('test.json', 'r') as f:\n",
        "    te_json = json.load(f)\n",
        "test_ids = [x.get('request_id') for x in te_json]\n",
        "print('Rows:', len(sub), ' expected:', len(test_ids))\n",
        "print('NA in request_id:', int(sub['request_id'].isna().sum()))\n",
        "print('Mean prob:', float(sub['requester_received_pizza'].mean()))\n",
        "print('Header OK:', list(sub.columns) == ['request_id', 'requester_received_pizza'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "c0faad2b-4412-4a50-af87-e713bac495ce",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S41-recent: Generate required recent35/45 test-only logits for LR_nosub and MiniLM\n",
        "import numpy as np, pandas as pd, os, time\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    HAS_XGB = True\n",
        "except Exception:\n",
        "    HAS_XGB = False\n",
        "\n",
        "t0 = time.time()\n",
        "id_col = 'request_id'\n",
        "target_col = 'requester_received_pizza'\n",
        "ts_keys = ['unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\n",
        "\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "n = len(train)\n",
        "\n",
        "# unix timestamp (with fallback)\n",
        "if ts_keys[0] in train.columns:\n",
        "    ts = train[ts_keys[0]].values\n",
        "else:\n",
        "    ts = train[ts_keys[1]].values\n",
        "\n",
        "# Time order and recent slices\n",
        "order = np.argsort(ts)  # ascending\n",
        "cut35 = int(np.floor(0.65 * n))  # last 35%\n",
        "cut45 = int(np.floor(0.55 * n))  # last 45%\n",
        "idx_r35 = order[cut35:]\n",
        "idx_r45 = order[cut45:]\n",
        "print(f'n={n} | r35 size={len(idx_r35)} | r45 size={len(idx_r45)}')\n",
        "\n",
        "# Cached features\n",
        "meta_tr = np.load('meta_v1_tr.npy')  # (n, m_meta)\n",
        "meta_te = np.load('meta_v1_te.npy')\n",
        "emb_mn_tr = np.load('emb_minilm_tr.npy')  # (n, d)\n",
        "emb_mn_te = np.load('emb_minilm_te.npy')\n",
        "\n",
        "# Text concat (title + body)\n",
        "def get_text(df: pd.DataFrame) -> pd.Series:\n",
        "    title = df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "    body = df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "    return title + ' ' + body\n",
        "\n",
        "txt_tr = get_text(train)\n",
        "txt_te = get_text(test)\n",
        "\n",
        "# 1) LR_nosub recent35/45 -> test_lr_time_nosub_meta_recentXX.npy\n",
        "tfidf_params = dict(\n",
        "    analyzer='word',\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=3,\n",
        "    max_features=100000,\n",
        "    lowercase=True,\n",
        "    strip_accents='unicode',\n",
        "    sublinear_tf=True\n",
        ")\n",
        "\n",
        "def fit_predict_lr_recent(idx_recent, out_path):\n",
        "    v = TfidfVectorizer(**tfidf_params)\n",
        "    X_txt_tr = v.fit_transform(txt_tr.iloc[idx_recent])\n",
        "    X_txt_te = v.transform(txt_te)\n",
        "    X_tr = sparse.hstack([X_txt_tr, sparse.csr_matrix(meta_tr[idx_recent])], format='csr')\n",
        "    X_te = sparse.hstack([X_txt_te, sparse.csr_matrix(meta_te)], format='csr')\n",
        "\n",
        "    lr = LogisticRegression(\n",
        "        penalty='l2',\n",
        "        solver='liblinear',\n",
        "        C=1.0,\n",
        "        max_iter=2000,\n",
        "        random_state=42\n",
        "    )\n",
        "    lr.fit(X_tr, y[idx_recent])\n",
        "    p = lr.predict_proba(X_te)[:, 1].astype(np.float32)\n",
        "    np.save(out_path, p)\n",
        "    print(f'Wrote {out_path} | mean={float(p.mean()):.6f}')\n",
        "\n",
        "fit_predict_lr_recent(idx_r35, 'test_lr_time_nosub_meta_recent35.npy')\n",
        "fit_predict_lr_recent(idx_r45, 'test_lr_time_nosub_meta_recent45.npy')\n",
        "\n",
        "# 2) MiniLM recent35/45 -> test_xgb_emb_meta_time_recentXX.npy\n",
        "def fit_predict_minilm_recent(idx_recent, out_path):\n",
        "    X_tr_slice = np.hstack([emb_mn_tr[idx_recent], meta_tr[idx_recent]])\n",
        "    X_te = np.hstack([emb_mn_te, meta_te])\n",
        "\n",
        "    if HAS_XGB:\n",
        "        idx_recent_sorted = np.sort(idx_recent)\n",
        "        m = len(idx_recent_sorted)\n",
        "        split = int(np.floor(0.80 * m))\n",
        "        tr_idx = idx_recent_sorted[:split]\n",
        "        va_idx = idx_recent_sorted[split:]\n",
        "\n",
        "        Xtr = np.hstack([emb_mn_tr[tr_idx], meta_tr[tr_idx]])\n",
        "        Xva = np.hstack([emb_mn_tr[va_idx], meta_tr[va_idx]])\n",
        "        ytr = y[tr_idx]\n",
        "        yva = y[va_idx]\n",
        "\n",
        "        dtr = xgb.DMatrix(Xtr, label=ytr)\n",
        "        dva = xgb.DMatrix(Xva, label=yva)\n",
        "        dte = xgb.DMatrix(X_te)\n",
        "\n",
        "        params = {\n",
        "            'objective': 'binary:logistic',\n",
        "            'eval_metric': 'auc',\n",
        "            'max_depth': 3,\n",
        "            'eta': 0.05,\n",
        "            'subsample': 0.9,\n",
        "            'colsample_bytree': 0.9,\n",
        "            'reg_lambda': 1.0,\n",
        "            'reg_alpha': 0.0,\n",
        "            'min_child_weight': 1.0,\n",
        "            'tree_method': 'hist',\n",
        "            'seed': 42\n",
        "        }\n",
        "        bst = xgb.train(params, dtr, num_boost_round=300, evals=[(dva, 'valid')], early_stopping_rounds=50, verbose_eval=False)\n",
        "        p = bst.predict(dte, iteration_range=(0, bst.best_iteration + 1)).astype(np.float32)\n",
        "    else:\n",
        "        lr = LogisticRegression(penalty='l2', solver='lbfgs', C=1.0, max_iter=1000, random_state=42)\n",
        "        lr.fit(X_tr_slice, y[idx_recent])\n",
        "        p = lr.predict_proba(X_te)[:, 1].astype(np.float32)\n",
        "\n",
        "    np.save(out_path, p)\n",
        "    print(f'Wrote {out_path} | mean={float(p.mean()):.6f}')\n",
        "\n",
        "fit_predict_minilm_recent(idx_r35, 'test_xgb_emb_meta_time_recent35.npy')\n",
        "fit_predict_minilm_recent(idx_r45, 'test_xgb_emb_meta_time_recent45.npy')\n",
        "\n",
        "print(f'Done generating 4 recent test-only files in {time.time()-t0:.1f}s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "924496b3-4e0b-4958-8aee-647db4714b93",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S41r30: Build stronger recency variant (alpha=0.20) and 3-way hedges; also bias-shift to mean 0.40\n",
        "import os, numpy as np, pandas as pd\n",
        "from scipy.special import expit\n",
        "\n",
        "# Guards: ensure S41-rev ran (clf and X_test exist)\n",
        "assert 'clf' in globals() and 'X_test' in globals() and 'all_cols' in globals(), 'Run Cell 6 (S41-rev) first.'\n",
        "\n",
        "def logit_clip(p, eps=1e-6):\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def try_load(fp):\n",
        "    if os.path.exists(fp):\n",
        "        arr = np.load(fp)\n",
        "        if arr.ndim > 1: arr = arr.ravel()\n",
        "        return arr\n",
        "    return None\n",
        "\n",
        "# Recent files map (same as S41)\n",
        "recent_map = {\n",
        "    'LR_nosub': [\n",
        "        'test_lr_time_nosub_meta_recent35.npy',\n",
        "        'test_lr_time_nosub_meta_recent45.npy',\n",
        "    ],\n",
        "    'MiniLM': [\n",
        "        'test_xgb_emb_meta_time_recent35.npy',\n",
        "        'test_xgb_emb_meta_time_recent45.npy',\n",
        "    ],\n",
        "}\n",
        "\n",
        "col_to_idx = {c: i for i, c in enumerate(all_cols)}\n",
        "\n",
        "def apply_recency(X_base, a_lr=0.20, a_mn=0.20):\n",
        "    Xr = X_base.copy()\n",
        "    # LR_nosub\n",
        "    if 'LR_nosub' in col_to_idx and a_lr > 0:\n",
        "        bi = col_to_idx['LR_nosub']\n",
        "        z_full = X_base[:, bi]\n",
        "        zs = [logit_clip(try_load(fp)) for fp in recent_map['LR_nosub'] if try_load(fp) is not None]\n",
        "        if zs:\n",
        "            z_recent = np.mean(zs, axis=0)\n",
        "            Xr[:, bi] = (1.0 - a_lr)*z_full + a_lr*z_recent\n",
        "    # MiniLM\n",
        "    if 'MiniLM' in col_to_idx and a_mn > 0:\n",
        "        bi = col_to_idx['MiniLM']\n",
        "        z_full = X_base[:, bi]\n",
        "        zs = [logit_clip(try_load(fp)) for fp in recent_map['MiniLM'] if try_load(fp) is not None]\n",
        "        if zs:\n",
        "            z_recent = np.mean(zs, axis=0)\n",
        "            Xr[:, bi] = (1.0 - a_mn)*z_full + a_mn*z_recent\n",
        "    return Xr\n",
        "\n",
        "# 1) Build r30 with alpha=0.20\n",
        "X_test_r30 = apply_recency(X_test, a_lr=0.20, a_mn=0.20)\n",
        "p_r30 = clf.predict_proba(X_test_r30)[:, 1].astype(np.float32)\n",
        "sub_r30 = pd.DataFrame({'request_id': pd.read_json('test.json')['request_id'].values, 'requester_received_pizza': p_r30})\n",
        "sub_r30.to_csv('submission_s41_meta_r30.csv', index=False)\n",
        "print('Wrote submission_s41_meta_r30.csv | mean', float(p_r30.mean()))\n",
        "\n",
        "# 2) 3-way logit hedge (gamma, r24, r30)\n",
        "sub_g = pd.read_csv('submission_s41_meta_gamma.csv')\n",
        "sub_r24 = pd.read_csv('submission_s41_meta_r24.csv')\n",
        "sub_r30 = pd.read_csv('submission_s41_meta_r30.csv')\n",
        "assert np.all(sub_g.request_id.values == sub_r24.request_id.values) and np.all(sub_g.request_id.values == sub_r30.request_id.values), 'ID mismatch across submissions'\n",
        "z_g = logit_clip(sub_g.requester_received_pizza.values)\n",
        "z_r24 = logit_clip(sub_r24.requester_received_pizza.values)\n",
        "z_r30 = logit_clip(sub_r30.requester_received_pizza.values)\n",
        "z3 = (z_g + z_r24 + z_r30) / 3.0\n",
        "p3 = expit(z3).astype(np.float32)\n",
        "sub_h3 = pd.DataFrame({'request_id': sub_g.request_id, 'requester_received_pizza': p3})\n",
        "sub_h3.to_csv('submission_s41_meta_hedge3.csv', index=False)\n",
        "print('Wrote submission_s41_meta_hedge3.csv | mean', float(p3.mean()))\n",
        "\n",
        "# 3) Bias-shift gamma, r24, r30 individually to mean 0.40, then 3-way hedge\n",
        "def find_bias_for_target_mean(probs, target_mean, tol=1e-6, max_iter=100):\n",
        "    z = logit_clip(probs)\n",
        "    lo, hi = -10.0, 10.0\n",
        "    for _ in range(max_iter):\n",
        "        mid = 0.5*(lo+hi)\n",
        "        m = expit(z+mid).mean()\n",
        "        if abs(m - target_mean) < tol:\n",
        "            return mid\n",
        "        if m < target_mean: lo = mid\n",
        "        else: hi = mid\n",
        "    return 0.5*(lo+hi)\n",
        "\n",
        "def apply_bias(probs, bias):\n",
        "    return expit(logit_clip(probs) + bias)\n",
        "\n",
        "tm = 0.40\n",
        "b_g = find_bias_for_target_mean(sub_g.requester_received_pizza.values, tm)\n",
        "b_r24 = find_bias_for_target_mean(sub_r24.requester_received_pizza.values, tm)\n",
        "b_r30 = find_bias_for_target_mean(sub_r30.requester_received_pizza.values, tm)\n",
        "pg = apply_bias(sub_g.requester_received_pizza.values, b_g)\n",
        "pr24 = apply_bias(sub_r24.requester_received_pizza.values, b_r24)\n",
        "pr30 = apply_bias(sub_r30.requester_received_pizza.values, b_r30)\n",
        "zg, zr24, zr30 = logit_clip(pg), logit_clip(pr24), logit_clip(pr30)\n",
        "p3m = expit((zg+zr24+zr30)/3.0).astype(np.float32)\n",
        "sub_h3m = pd.DataFrame({'request_id': sub_g.request_id, 'requester_received_pizza': p3m})\n",
        "sub_h3m.to_csv('submission_s41_meta_hedge3_m040.csv', index=False)\n",
        "print('Wrote submission_s41_meta_hedge3_m040.csv | mean', float(p3m.mean()))\n",
        "\n",
        "# Promote mean-0.40 3-way hedge to submission.csv\n",
        "sub_h3m.to_csv('submission.csv', index=False)\n",
        "print('Promoted submission.csv (3-way, mean~0.40)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "7e801020-d7ca-42cc-9086-6a69e487d316",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S41-recent-mpnet: Generate recent35/45 test-only logits for MPNet+meta\n",
        "import numpy as np, pandas as pd, os, time\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    HAS_XGB = True\n",
        "except Exception:\n",
        "    HAS_XGB = False\n",
        "\n",
        "t0 = time.time()\n",
        "id_col = 'request_id'\n",
        "target_col = 'requester_received_pizza'\n",
        "ts_keys = ['unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\n",
        "\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "n = len(train)\n",
        "\n",
        "# unix timestamp (with fallback)\n",
        "if ts_keys[0] in train.columns:\n",
        "    ts = train[ts_keys[0]].values\n",
        "else:\n",
        "    ts = train[ts_keys[1]].values\n",
        "\n",
        "# Time order and recent slices\n",
        "order = np.argsort(ts)  # ascending\n",
        "cut35 = int(np.floor(0.65 * n))  # last 35%\n",
        "cut45 = int(np.floor(0.55 * n))  # last 45%\n",
        "idx_r35 = order[cut35:]\n",
        "idx_r45 = order[cut45:]\n",
        "print(f'MPNET recent -> n={n} | r35 size={len(idx_r35)} | r45 size={len(idx_r45)}')\n",
        "\n",
        "# Cached features\n",
        "meta_tr = np.load('meta_v1_tr.npy')  # (n, m_meta)\n",
        "meta_te = np.load('meta_v1_te.npy')\n",
        "emb_mp_tr = np.load('emb_mpnet_tr.npy')  # (n, d)\n",
        "emb_mp_te = np.load('emb_mpnet_te.npy')\n",
        "\n",
        "def fit_predict_mpnet_recent(idx_recent, out_path):\n",
        "    X_tr_slice = np.hstack([emb_mp_tr[idx_recent], meta_tr[idx_recent]])\n",
        "    X_te = np.hstack([emb_mp_te, meta_te])\n",
        "    if HAS_XGB:\n",
        "        idx_recent_sorted = np.sort(idx_recent)\n",
        "        m = len(idx_recent_sorted)\n",
        "        split = int(np.floor(0.80 * m))\n",
        "        tr_idx = idx_recent_sorted[:split]\n",
        "        va_idx = idx_recent_sorted[split:]\n",
        "        Xtr = np.hstack([emb_mp_tr[tr_idx], meta_tr[tr_idx]])\n",
        "        Xva = np.hstack([emb_mp_tr[va_idx], meta_tr[va_idx]])\n",
        "        ytr = y[tr_idx]\n",
        "        yva = y[va_idx]\n",
        "        dtr = xgb.DMatrix(Xtr, label=ytr)\n",
        "        dva = xgb.DMatrix(Xva, label=yva)\n",
        "        dte = xgb.DMatrix(X_te)\n",
        "        params = {\n",
        "            'objective': 'binary:logistic',\n",
        "            'eval_metric': 'auc',\n",
        "            'max_depth': 3,\n",
        "            'eta': 0.05,\n",
        "            'subsample': 0.9,\n",
        "            'colsample_bytree': 0.9,\n",
        "            'reg_lambda': 1.0,\n",
        "            'reg_alpha': 0.0,\n",
        "            'min_child_weight': 1.0,\n",
        "            'tree_method': 'hist',\n",
        "            'seed': 42\n",
        "        }\n",
        "        bst = xgb.train(params, dtr, num_boost_round=300, evals=[(dva, 'valid')], early_stopping_rounds=50, verbose_eval=False)\n",
        "        p = bst.predict(dte, iteration_range=(0, bst.best_iteration + 1)).astype(np.float32)\n",
        "    else:\n",
        "        lr = LogisticRegression(penalty='l2', solver='lbfgs', C=1.0, max_iter=1000, random_state=42)\n",
        "        lr.fit(X_tr_slice, y[idx_recent])\n",
        "        p = lr.predict_proba(X_te)[:, 1].astype(np.float32)\n",
        "    np.save(out_path, p)\n",
        "    print(f'Wrote {out_path} | mean={float(p.mean()):.6f}')\n",
        "\n",
        "fit_predict_mpnet_recent(idx_r35, 'test_xgb_emb_mpnet_time_recent35.npy')\n",
        "fit_predict_mpnet_recent(idx_r45, 'test_xgb_emb_mpnet_time_recent45.npy')\n",
        "\n",
        "print(f'Done generating MPNet recent test-only files in {time.time()-t0:.1f}s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "164d9dda-c7ab-4461-9d6f-75739b1567d2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S41-mpnet-3way: Add MPNet recency to S41, build gamma/r_low/r_high and 3-way hedges, then bias to means 0.36 and 0.40\n",
        "import os, json, numpy as np, pandas as pd, time\n",
        "from scipy.special import expit\n",
        "\n",
        "assert 'clf' in globals() and 'X_test' in globals() and 'all_cols' in globals(), 'Run S41-rev (Cell 6) first.'\n",
        "\n",
        "def logit_clip(p, eps=1e-6):\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def find_bias_for_target_mean(probs, target_mean, tol=1e-6, max_iter=100):\n",
        "    z = logit_clip(probs)\n",
        "    lo, hi = -10.0, 10.0\n",
        "    for _ in range(max_iter):\n",
        "        mid = 0.5*(lo+hi)\n",
        "        m = expit(z+mid).mean()\n",
        "        if abs(m - target_mean) < tol:\n",
        "            return mid\n",
        "        if m < target_mean: lo = mid\n",
        "        else: hi = mid\n",
        "    return 0.5*(lo+hi)\n",
        "\n",
        "def apply_bias(probs, bias):\n",
        "    return expit(logit_clip(probs) + bias)\n",
        "\n",
        "with open('test.json', 'r') as f:\n",
        "    te_json = json.load(f)\n",
        "te_ids = [x.get('request_id') for x in te_json]\n",
        "\n",
        "# Recent files map including MPNet\n",
        "recent_map = {\n",
        "    'LR_nosub': [\n",
        "        'test_lr_time_nosub_meta_recent35.npy',\n",
        "        'test_lr_time_nosub_meta_recent45.npy',\n",
        "    ],\n",
        "    'MiniLM': [\n",
        "        'test_xgb_emb_meta_time_recent35.npy',\n",
        "        'test_xgb_emb_meta_time_recent45.npy',\n",
        "    ],\n",
        "    'MPNet': [\n",
        "        'test_xgb_emb_mpnet_time_recent35.npy',\n",
        "        'test_xgb_emb_mpnet_time_recent45.npy',\n",
        "    ],\n",
        "}\n",
        "\n",
        "col_to_idx = {c: i for i, c in enumerate(all_cols)}\n",
        "\n",
        "def load_recent_avg_probs(files):\n",
        "    arrs = []\n",
        "    for fp in files:\n",
        "        if os.path.exists(fp):\n",
        "            a = np.load(fp)\n",
        "            if a.ndim > 1: a = a.ravel()\n",
        "            arrs.append(a)\n",
        "    if not arrs:\n",
        "        return None\n",
        "    return np.mean(arrs, axis=0)\n",
        "\n",
        "def apply_recency_sym(X_base, a_lr=0.0, a_mn=0.0, a_mp=0.0):\n",
        "    Xr = X_base.copy()\n",
        "    # For each base, interpolate in logit space between full X_test column and recent avg\n",
        "    for name, alpha in [('LR_nosub', a_lr), ('MiniLM', a_mn), ('MPNet', a_mp)]:\n",
        "        if alpha <= 0: continue\n",
        "        if name not in col_to_idx: continue\n",
        "        rec_files = recent_map.get(name, [])\n",
        "        p_recent = load_recent_avg_probs(rec_files)\n",
        "        if p_recent is None: continue\n",
        "        idx = col_to_idx[name]\n",
        "        z_full = X_base[:, idx]\n",
        "        z_recent = logit_clip(p_recent)\n",
        "        Xr[:, idx] = (1.0 - alpha)*z_full + alpha*z_recent\n",
        "    return Xr\n",
        "\n",
        "def save_sub(path, probs):\n",
        "    df = pd.DataFrame({'request_id': te_ids, 'requester_received_pizza': probs.astype(np.float32)})\n",
        "    df.to_csv(path, index=False)\n",
        "    print(f'Wrote {os.path.basename(path)} | mean={float(df.requester_received_pizza.mean()):.6f}')\n",
        "\n",
        "t0 = time.time()\n",
        "# 1) Build three variants: gamma (0,0,0), r_low (0.10 each), r_high (0.20 each)\n",
        "Xg = X_test\n",
        "pr_g = clf.predict_proba(Xg)[:, 1]\n",
        "save_sub('submission_s41_final_gamma.csv', pr_g)\n",
        "\n",
        "X_low = apply_recency_sym(X_test, a_lr=0.10, a_mn=0.10, a_mp=0.10)\n",
        "pr_low = clf.predict_proba(X_low)[:, 1]\n",
        "save_sub('submission_s41_final_r_low.csv', pr_low)\n",
        "\n",
        "X_high = apply_recency_sym(X_test, a_lr=0.20, a_mn=0.20, a_mp=0.20)\n",
        "pr_high = clf.predict_proba(X_high)[:, 1]\n",
        "save_sub('submission_s41_final_r_high.csv', pr_high)\n",
        "\n",
        "# Guardrail: recency should shift mean vs gamma\n",
        "mg, ml, mh = float(pr_g.mean()), float(pr_low.mean()), float(pr_high.mean())\n",
        "print('Means -> gamma:', mg, '| r_low:', ml, '| r_high:', mh)\n",
        "if abs(ml - mg) <= 0.0005 or abs(mh - mg) <= 0.0005:\n",
        "    print('WARNING: Recency shift is tiny; check recent files presence.')\n",
        "\n",
        "# 2) 3-way logit hedge: gamma, r_low, r_high\n",
        "zg = logit_clip(pr_g); zl = logit_clip(pr_low); zh = logit_clip(pr_high)\n",
        "z3 = (zg + zl + zh) / 3.0\n",
        "p3 = expit(z3)\n",
        "save_sub('submission_s41_final_hedge3.csv', p3)\n",
        "\n",
        "# 3) Bias shift final 3-way hedge to target means 0.36 and 0.40, promote 0.36 first\n",
        "for tm in [0.36, 0.40]:\n",
        "    b = find_bias_for_target_mean(p3, tm)\n",
        "    p3m = apply_bias(p3, b)\n",
        "    outp = f'submission_s41_final_hedge3_m{int(tm*100):03d}.csv'\n",
        "    save_sub(outp, p3m)\n",
        "    if tm == 0.36:\n",
        "        pd.DataFrame({'request_id': te_ids, 'requester_received_pizza': p3m.astype(np.float32)}).to_csv('submission.csv', index=False)\n",
        "        print('PROMOTED: submission.csv <-', outp)\n",
        "\n",
        "print(f'S41-mpnet-3way done in {time.time()-t0:.2f}s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "4fe540d5-01dc-4064-ad97-acea15c6afc1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S41x: Blend S41 3-way hedge with S39 hedge2 (50/50 logit), then bias to mean 0.40 and promote\n",
        "import os, numpy as np, pandas as pd\n",
        "from scipy.special import expit\n",
        "\n",
        "def logit_clip(p, eps=1e-6):\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def find_bias_for_target_mean(probs, target_mean, tol=1e-6, max_iter=100):\n",
        "    z = logit_clip(probs)\n",
        "    lo, hi = -10.0, 10.0\n",
        "    for _ in range(max_iter):\n",
        "        mid = 0.5*(lo+hi)\n",
        "        m = expit(z+mid).mean()\n",
        "        if abs(m - target_mean) < tol:\n",
        "            return mid\n",
        "        if m < target_mean: lo = mid\n",
        "        else: hi = mid\n",
        "    return 0.5*(lo+hi)\n",
        "\n",
        "def apply_bias(probs, bias):\n",
        "    return expit(logit_clip(probs) + bias)\n",
        "\n",
        "def load_sub_norm(path):\n",
        "    df = pd.read_csv(path)\n",
        "    cols = [c.lower() for c in df.columns]\n",
        "    if 'request_id' in cols:\n",
        "        id_col = df.columns[cols.index('request_id')]\n",
        "    elif 'id' in cols:\n",
        "        id_col = df.columns[cols.index('id')]\n",
        "    else:\n",
        "        raise ValueError('No id/request_id in ' + path)\n",
        "    prob_col = [c for c in df.columns if c != id_col][0]\n",
        "    return df[[id_col, prob_col]].rename(columns={id_col:'request_id', prob_col:'requester_received_pizza'})\n",
        "\n",
        "s41_path = 'submission_s41_final_hedge3.csv'\n",
        "s39_path = 'submission_s39_hedge2_gamma_r24.csv'\n",
        "assert os.path.exists(s41_path), f'Missing {s41_path}'\n",
        "assert os.path.exists(s39_path), f'Missing {s39_path}'\n",
        "s41 = load_sub_norm(s41_path)\n",
        "s39 = load_sub_norm(s39_path)\n",
        "assert np.all(s41.request_id.values == s39.request_id.values), 'ID mismatch between S41 and S39 hedges'\n",
        "\n",
        "# 50/50 logit-average\n",
        "z41 = logit_clip(s41.requester_received_pizza.values)\n",
        "z39 = logit_clip(s39.requester_received_pizza.values)\n",
        "z_blend = 0.5*(z41 + z39)\n",
        "p_blend = expit(z_blend).astype(np.float32)\n",
        "df_blend = pd.DataFrame({'request_id': s41.request_id.values, 'requester_received_pizza': p_blend})\n",
        "df_blend.to_csv('submission_s41x_blend_s41_s39.csv', index=False)\n",
        "print('Wrote submission_s41x_blend_s41_s39.csv | mean=', float(p_blend.mean()))\n",
        "\n",
        "# Bias to mean 0.40\n",
        "b = find_bias_for_target_mean(p_blend, 0.40)\n",
        "p_bias = apply_bias(p_blend, b).astype(np.float32)\n",
        "df_bias = pd.DataFrame({'request_id': s41.request_id.values, 'requester_received_pizza': p_bias})\n",
        "df_bias.to_csv('submission_s41x_blend_s41_s39_m040.csv', index=False)\n",
        "print('Wrote submission_s41x_blend_s41_s39_m040.csv | mean=', float(p_bias.mean()), '| bias=', float(b))\n",
        "\n",
        "# Promote\n",
        "df_bias.to_csv('submission.csv', index=False)\n",
        "print('PROMOTED: submission.csv <- submission_s41x_blend_s41_s39_m040.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "8c95862b-7d98-495f-b7f4-265acfcee6e3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S41-recent3: Recent-only 3-base ensemble (LR_nosub, MiniLM, MPNet), 50/50 with S41 3-way hedge, bias to 0.36/0.40\n",
        "import os, numpy as np, pandas as pd\n",
        "from scipy.special import expit\n",
        "\n",
        "def logit_clip(p, eps=1e-6):\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def load_recent_avg(paths):\n",
        "    arrs = []\n",
        "    for fp in paths:\n",
        "        if os.path.exists(fp):\n",
        "            a = np.load(fp)\n",
        "            if a.ndim > 1: a = a.ravel()\n",
        "            arrs.append(a.astype(np.float64))\n",
        "    return (np.mean(arrs, axis=0) if arrs else None)\n",
        "\n",
        "def find_bias_for_target_mean(probs, target_mean, tol=1e-6, max_iter=100):\n",
        "    z = logit_clip(probs)\n",
        "    lo, hi = -10.0, 10.0\n",
        "    for _ in range(max_iter):\n",
        "        mid = 0.5*(lo+hi)\n",
        "        m = expit(z+mid).mean()\n",
        "        if abs(m - target_mean) < tol:\n",
        "            return mid\n",
        "        if m < target_mean: lo = mid\n",
        "        else: hi = mid\n",
        "    return 0.5*(lo+hi)\n",
        "\n",
        "def apply_bias(probs, bias):\n",
        "    return expit(logit_clip(probs) + bias)\n",
        "\n",
        "# Load recent35/45 for each base\n",
        "p_lr = load_recent_avg(['test_lr_time_nosub_meta_recent35.npy','test_lr_time_nosub_meta_recent45.npy'])\n",
        "p_mn = load_recent_avg(['test_xgb_emb_meta_time_recent35.npy','test_xgb_emb_meta_time_recent45.npy'])\n",
        "p_mp = load_recent_avg(['test_xgb_emb_mpnet_time_recent35.npy','test_xgb_emb_mpnet_time_recent45.npy'])\n",
        "assert p_lr is not None and p_mn is not None and p_mp is not None, 'Missing recent files for one of the bases'\n",
        "\n",
        "# 3-base recent-only logit average\n",
        "z_lr, z_mn, z_mp = logit_clip(p_lr), logit_clip(p_mn), logit_clip(p_mp)\n",
        "z_recent3 = (z_lr + z_mn + z_mp) / 3.0\n",
        "p_recent3 = expit(z_recent3).astype(np.float32)\n",
        "pd.DataFrame({'request_id': pd.read_json('test.json')['request_id'].values, 'requester_received_pizza': p_recent3}).to_csv('submission_recent3.csv', index=False)\n",
        "print('Wrote submission_recent3.csv | mean=', float(p_recent3.mean()))\n",
        "\n",
        "# Load best S41 3-way hedge (unbiased) for blending\n",
        "s41_3 = pd.read_csv('submission_s41_final_hedge3.csv') if os.path.exists('submission_s41_final_hedge3.csv') else pd.read_csv('submission_s41_meta_hedge3.csv')\n",
        "p_s41 = s41_3['requester_received_pizza'].values.astype(np.float64)\n",
        "assert len(p_s41) == len(p_recent3), 'Length mismatch S41 vs recent3'\n",
        "\n",
        "# 50/50 logit blend of recent3 and S41 3-way\n",
        "z_s41 = logit_clip(p_s41)\n",
        "z_blend = 0.5*z_recent3 + 0.5*z_s41\n",
        "p_blend = expit(z_blend).astype(np.float32)\n",
        "df_blend = pd.DataFrame({'request_id': s41_3['request_id'].values, 'requester_received_pizza': p_blend})\n",
        "df_blend.to_csv('submission_recent3_s41_blend.csv', index=False)\n",
        "print('Wrote submission_recent3_s41_blend.csv | mean=', float(p_blend.mean()))\n",
        "\n",
        "# Bias-shift to target means 0.36 and 0.40; promote 0.36\n",
        "for tm in [0.36, 0.40]:\n",
        "    b = find_bias_for_target_mean(p_blend, tm)\n",
        "    p_b = apply_bias(p_blend, b).astype(np.float32)\n",
        "    outp = f'submission_recent3_s41_blend_m{int(tm*100):03d}.csv'\n",
        "    pd.DataFrame({'request_id': s41_3['request_id'].values, 'requester_received_pizza': p_b}).to_csv(outp, index=False)\n",
        "    print(f'Wrote {outp} | mean={p_b.mean():.6f} | bias={b:.4f}')\n",
        "    if tm == 0.36:\n",
        "        pd.DataFrame({'request_id': s41_3['request_id'].values, 'requester_received_pizza': p_b}).to_csv('submission.csv', index=False)\n",
        "        print('PROMOTED: submission.csv <-', outp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "5a60e4b9-d147-42df-b15d-a75fd35a65d3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S41-submit-034: Bias S41 3-way hedge to mean 0.34 and promote\n",
        "import os, numpy as np, pandas as pd\n",
        "from scipy.special import expit\n",
        "\n",
        "def logit_clip(p, eps=1e-6):\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def find_bias_for_target_mean(probs, target_mean, tol=1e-6, it=100):\n",
        "    z = logit_clip(probs); lo, hi = -10.0, 10.0\n",
        "    for _ in range(it):\n",
        "        mid = 0.5*(lo+hi); m = expit(z+mid).mean()\n",
        "        if abs(m-target_mean) < tol: return mid\n",
        "        if m < target_mean: lo = mid\n",
        "        else: hi = mid\n",
        "    return 0.5*(lo+hi)\n",
        "\n",
        "def apply_bias(probs, b):\n",
        "    return expit(logit_clip(probs)+b)\n",
        "\n",
        "src = 'submission_s41_final_hedge3.csv' if os.path.exists('submission_s41_final_hedge3.csv') else 'submission_s41_meta_hedge3.csv'\n",
        "s = pd.read_csv(src)\n",
        "b = find_bias_for_target_mean(s.requester_received_pizza.values, 0.34)\n",
        "s['requester_received_pizza'] = apply_bias(s.requester_received_pizza.values, b).astype(np.float32)\n",
        "s.to_csv('submission.csv', index=False)\n",
        "print('Promoted submission.csv | mean=', float(s.requester_received_pizza.mean()), '| bias=', float(b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "5ef57161-e33b-4436-a1fd-731bf1063dcf",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S41-diagnostic: Last-block AUC per base vs gamma-weighted overall; flag degraders\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "required = ['X_oof','all_cols','block_id','train_idx','y_tr','W_tr','oof_pred']\n",
        "missing = [k for k in required if k not in globals()]\n",
        "assert not missing, f'Missing artifacts from S41-rev: {missing}. Re-run Cell 6.'\n",
        "\n",
        "# Define last validated block mask on training indices\n",
        "mask_last = (block_id[train_idx] == 5)\n",
        "y_last = y_tr[mask_last]\n",
        "X_oof_tr = X_oof[train_idx]\n",
        "\n",
        "rows = []\n",
        "for i, name in enumerate(all_cols):\n",
        "    if name in ['log1p_text_len','account_age_days']:\n",
        "        continue\n",
        "    z = X_oof_tr[:, i]\n",
        "    try:\n",
        "        auc_overall = roc_auc_score(y_tr, z, sample_weight=W_tr)\n",
        "        auc_last = roc_auc_score(y_last, z[mask_last])\n",
        "        rows.append((name, float(auc_overall), float(auc_last), float(auc_overall - auc_last)))\n",
        "    except Exception as e:\n",
        "        rows.append((name, np.nan, np.nan, np.nan))\n",
        "\n",
        "# Add stacker itself\n",
        "try:\n",
        "    auc_overall = roc_auc_score(y_tr, oof_pred, sample_weight=W_tr)\n",
        "    auc_last = roc_auc_score(y_last, oof_pred[mask_last])\n",
        "    rows.append(('S41_Stacker', float(auc_overall), float(auc_last), float(auc_overall - auc_last)))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "df = pd.DataFrame(rows, columns=['Model','AUC_overall','AUC_last','Drop']).sort_values(['Drop','AUC_last'], ascending=[False, True])\n",
        "print(df.to_string(index=False))\n",
        "df.to_csv('s41_last_block_diagnostic.csv', index=False)\n",
        "print('Saved s41_last_block_diagnostic.csv')\n",
        "\n",
        "# Heuristic flags: Drop > 0.03 or AUC_last < 0.60\n",
        "flags = df[(df['Model']!='S41_Stacker') & ((df['Drop']>0.03) | (df['AUC_last']<0.60))]\n",
        "print('\\nFlagged degraders (Drop>0.03 or AUC_last<0.60):')\n",
        "print(flags.to_string(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "6b88391d-9ca0-41a7-8829-0aebe749d9ed",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S41-mpnet-stronger: Add stronger symmetric recency (0.15, 0.25, 0.30), build 4-way hedge incl. gamma, bias to 0.34 and promote\n",
        "import os, json, numpy as np, pandas as pd, time\n",
        "from scipy.special import expit\n",
        "\n",
        "assert 'clf' in globals() and 'X_test' in globals() and 'all_cols' in globals(), 'Run S41-rev (Cell 6) first.'\n",
        "\n",
        "def logit_clip(p, eps=1e-6):\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def find_bias_for_target_mean(probs, target_mean, tol=1e-6, max_iter=100):\n",
        "    z = logit_clip(probs)\n",
        "    lo, hi = -10.0, 10.0\n",
        "    for _ in range(max_iter):\n",
        "        mid = 0.5*(lo+hi)\n",
        "        m = expit(z+mid).mean()\n",
        "        if abs(m - target_mean) < tol:\n",
        "            return mid\n",
        "        if m < target_mean: lo = mid\n",
        "        else: hi = mid\n",
        "    return 0.5*(lo+hi)\n",
        "\n",
        "def apply_bias(probs, bias):\n",
        "    return expit(logit_clip(probs) + bias)\n",
        "\n",
        "with open('test.json', 'r') as f:\n",
        "    te_json = json.load(f)\n",
        "te_ids = [x.get('request_id') for x in te_json]\n",
        "\n",
        "# Recent files map including MPNet\n",
        "recent_map = {\n",
        "    'LR_nosub': [\n",
        "        'test_lr_time_nosub_meta_recent35.npy',\n",
        "        'test_lr_time_nosub_meta_recent45.npy',\n",
        "    ],\n",
        "    'MiniLM': [\n",
        "        'test_xgb_emb_meta_time_recent35.npy',\n",
        "        'test_xgb_emb_meta_time_recent45.npy',\n",
        "    ],\n",
        "    'MPNet': [\n",
        "        'test_xgb_emb_mpnet_time_recent35.npy',\n",
        "        'test_xgb_emb_mpnet_time_recent45.npy',\n",
        "    ],\n",
        "}\n",
        "\n",
        "col_to_idx = {c: i for i, c in enumerate(all_cols)}\n",
        "\n",
        "def load_recent_avg_probs(files):\n",
        "    arrs = []\n",
        "    for fp in files:\n",
        "        if os.path.exists(fp):\n",
        "            a = np.load(fp)\n",
        "            if a.ndim > 1: a = a.ravel()\n",
        "            arrs.append(a)\n",
        "    if not arrs:\n",
        "        return None\n",
        "    return np.mean(arrs, axis=0)\n",
        "\n",
        "def apply_recency_sym(X_base, a_lr=0.0, a_mn=0.0, a_mp=0.0):\n",
        "    Xr = X_base.copy()\n",
        "    for name, alpha in [('LR_nosub', a_lr), ('MiniLM', a_mn), ('MPNet', a_mp)]:\n",
        "        if alpha <= 0: continue\n",
        "        if name not in col_to_idx: continue\n",
        "        p_recent = load_recent_avg_probs(recent_map.get(name, []))\n",
        "        if p_recent is None: continue\n",
        "        idx = col_to_idx[name]\n",
        "        z_full = X_base[:, idx]\n",
        "        z_recent = logit_clip(p_recent)\n",
        "        Xr[:, idx] = (1.0 - alpha)*z_full + alpha*z_recent\n",
        "    return Xr\n",
        "\n",
        "def save_sub(path, probs):\n",
        "    df = pd.DataFrame({'request_id': te_ids, 'requester_received_pizza': probs.astype(np.float32)})\n",
        "    df.to_csv(path, index=False)\n",
        "    print(f'Wrote {os.path.basename(path)} | mean={float(df.requester_received_pizza.mean()):.6f}')\n",
        "\n",
        "t0 = time.time()\n",
        "# gamma (no recency)\n",
        "pr_g = clf.predict_proba(X_test)[:, 1]\n",
        "save_sub('submission_s41_final_gamma.csv', pr_g)\n",
        "\n",
        "# Build symmetric recency variants\n",
        "variants = {\n",
        "    'r010': (0.10, 0.10, 0.10),\n",
        "    'r015': (0.15, 0.15, 0.15),\n",
        "    'r020': (0.20, 0.20, 0.20),\n",
        "    'r030': (0.30, 0.30, 0.30),\n",
        "}\n",
        "preds = {'gamma': pr_g}\n",
        "for tag, (alr, amn, amp) in variants.items():\n",
        "    Xv = apply_recency_sym(X_test, a_lr=alr, a_mn=amn, a_mp=amp)\n",
        "    pv = clf.predict_proba(Xv)[:, 1]\n",
        "    preds[tag] = pv\n",
        "    save_sub(f'submission_s41_final_{tag}.csv', pv)\n",
        "\n",
        "# Hedge: 4-way logit hedge across gamma + r010 + r020 + r030\n",
        "zg = logit_clip(preds['gamma'])\n",
        "z010 = logit_clip(preds['r010'])\n",
        "z020 = logit_clip(preds['r020'])\n",
        "z030 = logit_clip(preds['r030'])\n",
        "z4 = (zg + z010 + z020 + z030) / 4.0\n",
        "p4 = expit(z4)\n",
        "save_sub('submission_s41_final_hedge4.csv', p4)\n",
        "\n",
        "# Bias to mean 0.34 and promote\n",
        "b = find_bias_for_target_mean(p4, 0.34)\n",
        "p4m = apply_bias(p4, b)\n",
        "save_sub('submission_s41_final_hedge4_m034.csv', p4m)\n",
        "pd.DataFrame({'request_id': te_ids, 'requester_received_pizza': p4m.astype(np.float32)}).to_csv('submission.csv', index=False)\n",
        "print('PROMOTED: submission.csv <- submission_s41_final_hedge4_m034.csv | took', f'{time.time()-t0:.2f}s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "b1fd72da-9d47-4ac4-bd7e-12c226c967fb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Promote unshifted S41 3-way hedge to submission.csv\n",
        "import pandas as pd\n",
        "src = 'submission_s41_final_hedge3.csv'\n",
        "df = pd.read_csv(src)\n",
        "df.to_csv('submission.csv', index=False)\n",
        "print('Promoted', src, 'to submission.csv | mean=', float(df['requester_received_pizza'].mean()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "19fd1227-91f0-4c9a-b083-2851ef68490e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Promote r_high variant (single model) biased to mean 0.34\n",
        "import numpy as np, pandas as pd, os\n",
        "from scipy.special import expit\n",
        "\n",
        "def logit_clip(p, eps=1e-6):\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def find_bias_for_target_mean(probs, target_mean, tol=1e-6, it=100):\n",
        "    z = logit_clip(probs); lo, hi = -10.0, 10.0\n",
        "    for _ in range(it):\n",
        "        mid = 0.5*(lo+hi); m = expit(z+mid).mean()\n",
        "        if abs(m-target_mean) < tol: return mid\n",
        "        if m < target_mean: lo = mid\n",
        "        else: hi = mid\n",
        "    return 0.5*(lo+hi)\n",
        "\n",
        "def apply_bias(probs, b):\n",
        "    return expit(logit_clip(probs)+b)\n",
        "\n",
        "src = 'submission_s41_final_r_high.csv'\n",
        "assert os.path.exists(src), f'Missing {src}; run S41-mpnet-3way (Cell 19/24) first.'\n",
        "s = pd.read_csv(src)\n",
        "b = find_bias_for_target_mean(s.requester_received_pizza.values, 0.34)\n",
        "s['requester_received_pizza'] = apply_bias(s.requester_received_pizza.values, b).astype(np.float32)\n",
        "s.to_csv('submission_s41_final_r_high_m034.csv', index=False)\n",
        "s.to_csv('submission.csv', index=False)\n",
        "print('Promoted submission_s41_final_r_high_m034.csv to submission.csv | mean=', float(s.requester_received_pizza.mean()), '| bias=', float(b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "154e1611-fa01-46ff-8290-0f7df0c11334",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S41d-bias-032: Bias-shift submission_s41_meta_hedge2.csv to mean 0.32 and promote\n",
        "import numpy as np, pandas as pd, os\n",
        "from scipy.special import expit\n",
        "\n",
        "def logit_clip(p, eps=1e-6):\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def find_bias_for_target_mean(probs, target_mean, tol=1e-6, it=100):\n",
        "    z = logit_clip(probs); lo, hi = -10.0, 10.0\n",
        "    for _ in range(it):\n",
        "        mid = 0.5*(lo+hi); m = expit(z+mid).mean()\n",
        "        if abs(m - target_mean) < tol: return mid\n",
        "        if m < target_mean: lo = mid\n",
        "        else: hi = mid\n",
        "    return 0.5*(lo+hi)\n",
        "\n",
        "def apply_bias(probs, b):\n",
        "    return expit(logit_clip(probs)+b)\n",
        "\n",
        "src = 'submission_s41_meta_hedge2.csv'\n",
        "assert os.path.exists(src), f'Missing {src}; run S41-rev (Cell 6) first.'\n",
        "df = pd.read_csv(src)\n",
        "# Normalize columns\n",
        "cols = [c.lower() for c in df.columns]\n",
        "if 'request_id' in cols:\n",
        "    id_col = df.columns[cols.index('request_id')]\n",
        "elif 'id' in cols:\n",
        "    id_col = df.columns[cols.index('id')]\n",
        "else:\n",
        "    raise ValueError('No id/request_id column in source submission')\n",
        "prob_col = [c for c in df.columns if c != id_col][0]\n",
        "probs = df[prob_col].values.astype(float)\n",
        "\n",
        "target_mean = 0.32\n",
        "b = find_bias_for_target_mean(probs, target_mean)\n",
        "probs_b = apply_bias(probs, b).astype(np.float32)\n",
        "out = pd.DataFrame({'request_id': df[id_col].values, 'requester_received_pizza': probs_b})\n",
        "out.to_csv('submission_s41_meta_hedge2_m032.csv', index=False)\n",
        "out.to_csv('submission.csv', index=False)\n",
        "print(f'Promoted submission_s41_meta_hedge2_m032.csv -> submission.csv | mean={float(out.requester_received_pizza.mean()):.6f} | bias={float(b):.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "77d019a6-186e-4f1b-a0b1-9f6f66446c27",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S41-final-asym: Asymmetric recency (LR_nosub small, MiniLM/MPNet strong), 3-way hedge, bias to 0.30/0.32\n",
        "import os, json, numpy as np, pandas as pd, time\n",
        "from scipy.special import expit\n",
        "\n",
        "assert 'clf' in globals() and 'X_test' in globals() and 'all_cols' in globals(), 'Run S41-rev (Cell 6) first.'\n",
        "\n",
        "def logit_clip(p, eps=1e-6):\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def find_bias_for_target_mean(probs, target_mean, tol=1e-6, max_iter=100):\n",
        "    z = logit_clip(probs)\n",
        "    lo, hi = -10.0, 10.0\n",
        "    for _ in range(max_iter):\n",
        "        mid = 0.5*(lo+hi)\n",
        "        m = expit(z+mid).mean()\n",
        "        if abs(m - target_mean) < tol:\n",
        "            return mid\n",
        "        if m < target_mean: lo = mid\n",
        "        else: hi = mid\n",
        "    return 0.5*(lo+hi)\n",
        "\n",
        "def apply_bias(probs, bias):\n",
        "    return expit(logit_clip(probs) + bias)\n",
        "\n",
        "with open('test.json', 'r') as f:\n",
        "    te_json = json.load(f)\n",
        "te_ids = [x.get('request_id') for x in te_json]\n",
        "\n",
        "# Recent files map including MPNet\n",
        "recent_map = {\n",
        "    'LR_nosub': [\n",
        "        'test_lr_time_nosub_meta_recent35.npy',\n",
        "        'test_lr_time_nosub_meta_recent45.npy',\n",
        "    ],\n",
        "    'MiniLM': [\n",
        "        'test_xgb_emb_meta_time_recent35.npy',\n",
        "        'test_xgb_emb_meta_time_recent45.npy',\n",
        "    ],\n",
        "    'MPNet': [\n",
        "        'test_xgb_emb_mpnet_time_recent35.npy',\n",
        "        'test_xgb_emb_mpnet_time_recent45.npy',\n",
        "    ],\n",
        "}\n",
        "\n",
        "col_to_idx = {c: i for i, c in enumerate(all_cols)}\n",
        "\n",
        "def load_recent_avg_probs(files):\n",
        "    arrs = []\n",
        "    for fp in files:\n",
        "        if os.path.exists(fp):\n",
        "            a = np.load(fp)\n",
        "            if a.ndim > 1: a = a.ravel()\n",
        "            arrs.append(a)\n",
        "    if not arrs:\n",
        "        return None\n",
        "    return np.mean(arrs, axis=0)\n",
        "\n",
        "def apply_recency_asym(X_base, a_lr=0.0, a_mn=0.0, a_mp=0.0):\n",
        "    Xr = X_base.copy()\n",
        "    for name, alpha in [('LR_nosub', a_lr), ('MiniLM', a_mn), ('MPNet', a_mp)]:\n",
        "        if alpha <= 0: continue\n",
        "        if name not in col_to_idx: continue\n",
        "        p_recent = load_recent_avg_probs(recent_map.get(name, []))\n",
        "        if p_recent is None: continue\n",
        "        idx = col_to_idx[name]\n",
        "        z_full = X_base[:, idx]\n",
        "        z_recent = logit_clip(p_recent)\n",
        "        Xr[:, idx] = (1.0 - alpha)*z_full + alpha*z_recent\n",
        "    return Xr\n",
        "\n",
        "def save_sub(path, probs):\n",
        "    df = pd.DataFrame({'request_id': te_ids, 'requester_received_pizza': probs.astype(np.float32)})\n",
        "    df.to_csv(path, index=False)\n",
        "    print(f'Wrote {os.path.basename(path)} | mean={float(df.requester_received_pizza.mean()):.6f}')\n",
        "\n",
        "t0 = time.time()\n",
        "# 1) Build three variants with asymmetric alphas per expert advice:\n",
        "# gamma: (0, 0, 0) | r_low: (0.00, 0.15, 0.20) | r_high: (0.05, 0.25, 0.30)\n",
        "pr_g = clf.predict_proba(X_test)[:, 1]\n",
        "save_sub('submission_s41_final_gamma.csv', pr_g)\n",
        "\n",
        "X_low = apply_recency_asym(X_test, a_lr=0.00, a_mn=0.15, a_mp=0.20)\n",
        "pr_low = clf.predict_proba(X_low)[:, 1]\n",
        "save_sub('submission_s41_final_asym_r_low.csv', pr_low)\n",
        "\n",
        "X_high = apply_recency_asym(X_test, a_lr=0.05, a_mn=0.25, a_mp=0.30)\n",
        "pr_high = clf.predict_proba(X_high)[:, 1]\n",
        "save_sub('submission_s41_final_asym_r_high.csv', pr_high)\n",
        "\n",
        "mg, ml, mh = float(pr_g.mean()), float(pr_low.mean()), float(pr_high.mean())\n",
        "print('Means -> gamma:', mg, '| r_low:', ml, '| r_high:', mh)\n",
        "\n",
        "# 2) 3-way logit hedge: gamma, r_low, r_high\n",
        "zg, zl, zh = logit_clip(pr_g), logit_clip(pr_low), logit_clip(pr_high)\n",
        "z3 = (zg + zl + zh) / 3.0\n",
        "p3 = expit(z3)\n",
        "save_sub('submission_s41_final_hedge3_asym.csv', p3)\n",
        "\n",
        "# 3) Bias to 0.30 and 0.32; promote 0.30\n",
        "for tm in [0.30, 0.32]:\n",
        "    b = find_bias_for_target_mean(p3, tm)\n",
        "    p3m = apply_bias(p3, b)\n",
        "    outp = f'submission_s41_final_hedge3_asym_m{int(tm*100):03d}.csv'\n",
        "    save_sub(outp, p3m)\n",
        "    if abs(tm - 0.30) < 1e-9:\n",
        "        pd.DataFrame({'request_id': te_ids, 'requester_received_pizza': p3m.astype(np.float32)}).to_csv('submission.csv', index=False)\n",
        "        print('PROMOTED: submission.csv <-', outp)\n",
        "\n",
        "print(f'S41-final-asym done in {time.time()-t0:.2f}s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "1f66ee83-986b-4ec7-8719-a1fc985a37b0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S42-xgb: XGBoost stacker on pruned bases + asymmetric recency variants + 3-way hedge + bias to 0.30\n",
        "import os, json, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.special import expit\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    raise RuntimeError('XGBoost not installed; install xgboost to run S42-xgb')\n",
        "\n",
        "t0 = time.time()\n",
        "# Guards: require artifacts from S41 (features built in memory)\n",
        "required = ['X_oof','X_test','all_cols','mask_full','y','W','tr_json','te_json']\n",
        "missing = [k for k in required if k not in globals()]\n",
        "assert not missing, f'Missing artifacts from S41-rev: {missing}. Run Cell 6 first.'\n",
        "\n",
        "print('S42-xgb: starting XGBoost stacker on features:', all_cols)\n",
        "\n",
        "# Train on validated mask (blocks 1..5) with gamma weights\n",
        "train_idx = np.where(mask_full)[0]\n",
        "X_tr = X_oof[train_idx].astype(np.float32)\n",
        "y_tr = np.array([int(x.get('requester_received_pizza') or 0) for x in tr_json], dtype=int)[train_idx]\n",
        "W_tr = W[train_idx].astype(np.float32)\n",
        "\n",
        "dtr = xgb.DMatrix(X_tr, label=y_tr, weight=W_tr, feature_names=[str(c) for c in all_cols])\n",
        "dte_full = xgb.DMatrix(X_test.astype(np.float32), feature_names=[str(c) for c in all_cols])\n",
        "\n",
        "params = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'auc',\n",
        "    'eta': 0.03,\n",
        "    'max_depth': 6,\n",
        "    'min_child_weight': 100,\n",
        "    'subsample': 0.7,\n",
        "    'colsample_bytree': 0.7,\n",
        "    'reg_alpha': 1.0,\n",
        "    'reg_lambda': 10.0,\n",
        "    'tree_method': 'hist',\n",
        "    'seed': 42\n",
        "}\n",
        "bst = xgb.train(params, dtr, num_boost_round=1200, verbose_eval=False)\n",
        "print('XGB trained. Building variants...')\n",
        "\n",
        "def logit_clip(p, eps=1e-6):\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def find_bias_for_target_mean(probs, target_mean, tol=1e-6, max_iter=100):\n",
        "    z = logit_clip(probs); lo, hi = -10.0, 10.0\n",
        "    for _ in range(max_iter):\n",
        "        mid = 0.5*(lo+hi); m = expit(z+mid).mean()\n",
        "        if abs(m - target_mean) < tol: return mid\n",
        "        if m < target_mean: lo = mid\n",
        "        else: hi = mid\n",
        "    return 0.5*(lo+hi)\n",
        "\n",
        "def apply_bias(probs, b):\n",
        "    return expit(logit_clip(probs) + b)\n",
        "\n",
        "# Recent files map\n",
        "recent_map = {\n",
        "    'LR_nosub': [\n",
        "        'test_lr_time_nosub_meta_recent35.npy',\n",
        "        'test_lr_time_nosub_meta_recent45.npy',\n",
        "    ],\n",
        "    'MiniLM': [\n",
        "        'test_xgb_emb_meta_time_recent35.npy',\n",
        "        'test_xgb_emb_meta_time_recent45.npy',\n",
        "    ],\n",
        "    'MPNet': [\n",
        "        'test_xgb_emb_mpnet_time_recent35.npy',\n",
        "        'test_xgb_emb_mpnet_time_recent45.npy',\n",
        "    ],\n",
        "}\n",
        "col_to_idx = {c: i for i, c in enumerate(all_cols)}\n",
        "\n",
        "def load_recent_avg_probs(files):\n",
        "    arrs = []\n",
        "    for fp in files:\n",
        "        if os.path.exists(fp):\n",
        "            a = np.load(fp)\n",
        "            if a.ndim > 1: a = a.ravel()\n",
        "            arrs.append(a)\n",
        "    if not arrs: return None\n",
        "    return np.mean(arrs, axis=0)\n",
        "\n",
        "def apply_recency_asym_to_Xtest(X_base, a_lr=0.0, a_mn=0.0, a_mp=0.0):\n",
        "    Xr = X_base.copy()\n",
        "    for name, alpha in [('LR_nosub', a_lr), ('MiniLM', a_mn), ('MPNet', a_mp)]:\n",
        "        if alpha <= 0: continue\n",
        "        if name not in col_to_idx: continue\n",
        "        p_recent = load_recent_avg_probs(recent_map.get(name, []))\n",
        "        if p_recent is None: continue\n",
        "        idx = col_to_idx[name]\n",
        "        z_full = X_base[:, idx]\n",
        "        z_recent = logit_clip(p_recent)\n",
        "        Xr[:, idx] = (1.0 - alpha)*z_full + alpha*z_recent\n",
        "    return Xr\n",
        "\n",
        "te_ids = [x.get('request_id') for x in te_json]\n",
        "def save_sub(path, probs):\n",
        "    df = pd.DataFrame({'request_id': te_ids, 'requester_received_pizza': probs.astype(np.float32)})\n",
        "    df.to_csv(path, index=False)\n",
        "    print(f'Wrote {os.path.basename(path)} | mean={float(df.requester_received_pizza.mean()):.6f}')\n",
        "\n",
        "# 1) gamma (no recency)\n",
        "p_gamma = bst.predict(dte_full).astype(np.float32)\n",
        "save_sub('submission_s42x_meta_gamma.csv', p_gamma)\n",
        "\n",
        "# 2) Asymmetric recency variants: r_low (0.00,0.15,0.20), r_high (0.05,0.25,0.30)\n",
        "X_low = apply_recency_asym_to_Xtest(X_test, a_lr=0.00, a_mn=0.15, a_mp=0.20)\n",
        "p_low = bst.predict(xgb.DMatrix(X_low.astype(np.float32), feature_names=[str(c) for c in all_cols])).astype(np.float32)\n",
        "save_sub('submission_s42x_meta_asym_r_low.csv', p_low)\n",
        "\n",
        "X_high = apply_recency_asym_to_Xtest(X_test, a_lr=0.05, a_mn=0.25, a_mp=0.30)\n",
        "p_high = bst.predict(xgb.DMatrix(X_high.astype(np.float32), feature_names=[str(c) for c in all_cols])).astype(np.float32)\n",
        "save_sub('submission_s42x_meta_asym_r_high.csv', p_high)\n",
        "\n",
        "print('Means -> gamma:', float(p_gamma.mean()), '| r_low:', float(p_low.mean()), '| r_high:', float(p_high.mean()))\n",
        "\n",
        "# 3) 3-way logit hedge and bias to 0.30 (promote) and 0.32 (portfolio file)\n",
        "zg, zl, zh = logit_clip(p_gamma), logit_clip(p_low), logit_clip(p_high)\n",
        "p3 = expit((zg + zl + zh)/3.0).astype(np.float32)\n",
        "save_sub('submission_s42x_meta_hedge3.csv', p3)\n",
        "\n",
        "for tm in [0.30, 0.32]:\n",
        "    b = find_bias_for_target_mean(p3, tm)\n",
        "    p3m = apply_bias(p3, b).astype(np.float32)\n",
        "    outp = f'submission_s42x_meta_hedge3_m{int(tm*100):03d}.csv'\n",
        "    save_sub(outp, p3m)\n",
        "    if abs(tm - 0.30) < 1e-9:\n",
        "        pd.DataFrame({'request_id': te_ids, 'requester_received_pizza': p3m}).to_csv('submission.csv', index=False)\n",
        "        print('PROMOTED: submission.csv <-', outp)\n",
        "\n",
        "print(f'S42-xgb done in {time.time()-t0:.2f}s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "96442cd9-ab95-42fa-a0b0-60a2e518692f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S41-submit-028: Bias asymmetric 3-way hedge to mean 0.28 and promote\n",
        "import os, numpy as np, pandas as pd\n",
        "from scipy.special import expit\n",
        "\n",
        "def logit_clip(p, eps=1e-6):\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def find_bias_for_target_mean(probs, target_mean, tol=1e-6, it=100):\n",
        "    z = logit_clip(probs); lo, hi = -10.0, 10.0\n",
        "    for _ in range(it):\n",
        "        mid = 0.5*(lo+hi); m = expit(z+mid).mean()\n",
        "        if abs(m - target_mean) < tol: return mid\n",
        "        if m < target_mean: lo = mid\n",
        "        else: hi = mid\n",
        "    return 0.5*(lo+hi)\n",
        "\n",
        "def apply_bias(probs, b):\n",
        "    return expit(logit_clip(probs)+b)\n",
        "\n",
        "src = 'submission_s41_final_hedge3_asym.csv'\n",
        "if not os.path.exists(src):\n",
        "    # fallback to symmetric hedge if asym not present\n",
        "    src = 'submission_s41_final_hedge3.csv' if os.path.exists('submission_s41_final_hedge3.csv') else 'submission_s42x_meta_hedge3.csv'\n",
        "s = pd.read_csv(src)\n",
        "b = find_bias_for_target_mean(s['requester_received_pizza'].values, 0.28)\n",
        "s['requester_received_pizza'] = apply_bias(s['requester_received_pizza'].values, b).astype(np.float32)\n",
        "s.to_csv('submission_s41_final_hedge3_asym_m028.csv', index=False)\n",
        "s.to_csv('submission.csv', index=False)\n",
        "print('Promoted', src, '-> submission_s41_final_hedge3_asym_m028.csv -> submission.csv | mean=', float(s['requester_received_pizza'].mean()), '| bias=', float(b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "01f04bd6-e18c-46a6-9bd7-ffcd33249498",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S43-rank5: Diversified rank-average of top-5 bases -> bias to mean 0.35 and promote\n",
        "import os, json, numpy as np, pandas as pd\n",
        "from scipy.special import expit\n",
        "\n",
        "print('S43-rank5: building rank-average ensemble (LR_nosub, Dense_v1, Meta, MiniLM, MPNet) ...')\n",
        "with open('test.json', 'r') as f:\n",
        "    te_json = json.load(f)\n",
        "te_ids = [x.get('request_id') for x in te_json]\n",
        "\n",
        "def load_probs(fp):\n",
        "    a = np.load(fp)\n",
        "    if a.ndim > 1: a = a.ravel()\n",
        "    return a.astype(np.float64)\n",
        "\n",
        "bases = [\n",
        "    ('LR_nosub', 'test_lr_time_nosub_meta.npy'),\n",
        "    ('Dense_v1', 'test_xgb_dense_time.npy'),\n",
        "    ('Meta', 'test_xgb_meta_time.npy'),\n",
        "    ('MiniLM', 'test_xgb_emb_meta_time.npy'),\n",
        "    ('MPNet', 'test_xgb_emb_mpnet_time.npy'),\n",
        "]\n",
        "\n",
        "arrs = []\n",
        "for name, fp in bases:\n",
        "    if not os.path.exists(fp):\n",
        "        raise FileNotFoundError(f'Missing {fp} for base {name}')\n",
        "    p = load_probs(fp)\n",
        "    arrs.append(p)\n",
        "    print(f'Loaded {name}: mean={float(p.mean()):.6f}')\n",
        "\n",
        "arrs = np.stack(arrs, axis=1)  # (m, 5)\n",
        "m = arrs.shape[0]\n",
        "\n",
        "def rank01(x):\n",
        "    # deterministic rank to [0,1] without scipy; ties handled by average of positions of equal values\n",
        "    # For continuous preds ties are rare; fallback to simple order-based ranks\n",
        "    order = np.argsort(x, kind='mergesort')\n",
        "    ranks = np.empty_like(order, dtype=np.float64)\n",
        "    ranks[order] = np.arange(m, dtype=np.float64)\n",
        "    return ranks / (m - 1) if m > 1 else np.zeros_like(x, dtype=np.float64)\n",
        "\n",
        "rank_cols = np.column_stack([rank01(arrs[:, j]) for j in range(arrs.shape[1])])  # (m,5)\n",
        "rank_avg = rank_cols.mean(axis=1).astype(np.float64)  # in [0,1]\n",
        "\n",
        "df_rank = pd.DataFrame({'request_id': te_ids, 'requester_received_pizza': rank_avg.astype(np.float32)})\n",
        "df_rank.to_csv('submission_rank5.csv', index=False)\n",
        "print('Wrote submission_rank5.csv | mean=', float(df_rank.requester_received_pizza.mean()))\n",
        "\n",
        "def logit_clip(p, eps=1e-6):\n",
        "    p = np.asarray(p, dtype=np.float64)\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p/(1-p))\n",
        "def find_bias_for_target_mean(probs, target_mean, tol=1e-6, max_iter=100):\n",
        "    z = logit_clip(probs); lo, hi = -10.0, 10.0\n",
        "    for _ in range(max_iter):\n",
        "        mid = 0.5*(lo+hi); m = expit(z+mid).mean()\n",
        "        if abs(m - target_mean) < tol: return mid\n",
        "        if m < target_mean: lo = mid\n",
        "        else: hi = mid\n",
        "    return 0.5*(lo+hi)\n",
        "def apply_bias(probs, b):\n",
        "    return expit(logit_clip(probs) + b)\n",
        "\n",
        "target_mean = 0.35\n",
        "b = find_bias_for_target_mean(rank_avg, target_mean)\n",
        "rank_avg_b = apply_bias(rank_avg, b).astype(np.float32)\n",
        "df_b = pd.DataFrame({'request_id': te_ids, 'requester_received_pizza': rank_avg_b})\n",
        "df_b.to_csv('submission_rank5_m035.csv', index=False)\n",
        "df_b.to_csv('submission.csv', index=False)\n",
        "print(f'PROMOTED: submission.csv <- submission_rank5_m035.csv | mean={float(df_b.requester_received_pizza.mean()):.6f} | bias={float(b):.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "2aeac61f-d828-477e-af95-ed9d10910e43",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S44-hybrid: 50/50 logit hedge of S41 asym 3-way and rank-average; bias to 0.30 and 0.32\n",
        "import os, numpy as np, pandas as pd\n",
        "from scipy.special import expit\n",
        "\n",
        "def logit_clip(p, eps=1e-6):\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def load_sub_norm(path):\n",
        "    df = pd.read_csv(path)\n",
        "    cols = [c.lower() for c in df.columns]\n",
        "    if 'request_id' in cols:\n",
        "        id_col = df.columns[cols.index('request_id')]\n",
        "    elif 'id' in cols:\n",
        "        id_col = df.columns[cols.index('id')]\n",
        "    else:\n",
        "        raise ValueError('No id/request_id in ' + path)\n",
        "    prob_col = [c for c in df.columns if c != id_col][0]\n",
        "    out = df[[id_col, prob_col]].rename(columns={id_col:'request_id', prob_col:'requester_received_pizza'})\n",
        "    return out\n",
        "\n",
        "def find_bias_for_target_mean(probs, target_mean, tol=1e-6, max_iter=100):\n",
        "    z = logit_clip(probs); lo, hi = -10.0, 10.0\n",
        "    for _ in range(max_iter):\n",
        "        mid = 0.5*(lo+hi); m = expit(z+mid).mean()\n",
        "        if abs(m - target_mean) < tol: return mid\n",
        "        if m < target_mean: lo = mid\n",
        "        else: hi = mid\n",
        "    return 0.5*(lo+hi)\n",
        "\n",
        "def apply_bias(probs, b):\n",
        "    return expit(logit_clip(probs) + b)\n",
        "\n",
        "# Inputs: S41 asym 3-way hedge (unbiased) and rank-average (unbiased)\n",
        "path_stack = 'submission_s41_final_hedge3_asym.csv'\n",
        "path_rank = 'submission_rank5.csv'\n",
        "assert os.path.exists(path_stack), f'Missing {path_stack}; run Cell 28 first.'\n",
        "assert os.path.exists(path_rank), f'Missing {path_rank}; run Cell 31 first.'\n",
        "\n",
        "s_stack = load_sub_norm(path_stack)\n",
        "s_rank = load_sub_norm(path_rank)\n",
        "assert np.all(s_stack.request_id.values == s_rank.request_id.values), 'ID mismatch between stack and rank submissions'\n",
        "\n",
        "# 50/50 logit hedge\n",
        "z1 = logit_clip(s_stack.requester_received_pizza.values)\n",
        "z2 = logit_clip(s_rank.requester_received_pizza.values)\n",
        "z = 0.5*(z1 + z2)\n",
        "p = expit(z).astype(np.float32)\n",
        "df_h = pd.DataFrame({'request_id': s_stack.request_id.values, 'requester_received_pizza': p})\n",
        "df_h.to_csv('submission_s44_hybrid_50_50.csv', index=False)\n",
        "print('Wrote submission_s44_hybrid_50_50.csv | mean=', float(p.mean()))\n",
        "\n",
        "# Bias to 0.30 and 0.32; promote 0.30\n",
        "for tm in [0.30, 0.32]:\n",
        "    b = find_bias_for_target_mean(p, tm)\n",
        "    pm = apply_bias(p, b).astype(np.float32)\n",
        "    outp = f'submission_s44_hybrid_50_50_m{int(tm*100):03d}.csv'\n",
        "    pd.DataFrame({'request_id': s_stack.request_id.values, 'requester_received_pizza': pm}).to_csv(outp, index=False)\n",
        "    print(f'Wrote {outp} | mean={pm.mean():.6f} | bias={b:.4f}')\n",
        "    if abs(tm - 0.30) < 1e-9:\n",
        "        pd.DataFrame({'request_id': s_stack.request_id.values, 'requester_received_pizza': pm}).to_csv('submission.csv', index=False)\n",
        "        print('PROMOTED: submission.csv <-', outp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "220cff02-662a-43e3-994f-f177f169d09f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S45-eq5: Simple 5-base logit-average (LR_nosub, Dense_v1, Meta, MiniLM, MPNet) -> bias to 0.30 and 0.32\n",
        "import os, numpy as np, pandas as pd\n",
        "from scipy.special import expit\n",
        "\n",
        "def logit_clip(p, eps=1e-6):\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def find_bias_for_target_mean(probs, target_mean, tol=1e-6, max_iter=100):\n",
        "    z = logit_clip(probs); lo, hi = -10.0, 10.0\n",
        "    for _ in range(max_iter):\n",
        "        mid = 0.5*(lo+hi); m = expit(z+mid).mean()\n",
        "        if abs(m - target_mean) < tol: return mid\n",
        "        if m < target_mean: lo = mid\n",
        "        else: hi = mid\n",
        "    return 0.5*(lo+hi)\n",
        "\n",
        "def apply_bias(probs, b):\n",
        "    return expit(logit_clip(probs) + b)\n",
        "\n",
        "bases = [\n",
        "    ('LR_nosub', 'test_lr_time_nosub_meta.npy'),\n",
        "    ('Dense_v1', 'test_xgb_dense_time.npy'),\n",
        "    ('Meta', 'test_xgb_meta_time.npy'),\n",
        "    ('MiniLM', 'test_xgb_emb_meta_time.npy'),\n",
        "    ('MPNet', 'test_xgb_emb_mpnet_time.npy'),\n",
        "]\n",
        "\n",
        "arrs = []\n",
        "for name, fp in bases:\n",
        "    if not os.path.exists(fp):\n",
        "        raise FileNotFoundError(f'Missing {fp} for base {name}')\n",
        "    a = np.load(fp)\n",
        "    if a.ndim > 1: a = a.ravel()\n",
        "    arrs.append(a.astype(np.float64))\n",
        "\n",
        "Z = np.column_stack([logit_clip(a) for a in arrs])  # (m,5)\n",
        "z_mean = Z.mean(axis=1)\n",
        "p_eq5 = expit(z_mean).astype(np.float32)\n",
        "\n",
        "te_ids = pd.read_json('test.json')['request_id'].values\n",
        "df = pd.DataFrame({'request_id': te_ids, 'requester_received_pizza': p_eq5})\n",
        "df.to_csv('submission_eq5_logitavg.csv', index=False)\n",
        "print('Wrote submission_eq5_logitavg.csv | mean=', float(df.requester_received_pizza.mean()))\n",
        "\n",
        "for tm in [0.30, 0.32]:\n",
        "    b = find_bias_for_target_mean(p_eq5, tm)\n",
        "    pm = apply_bias(p_eq5, b).astype(np.float32)\n",
        "    outp = f'submission_eq5_logitavg_m{int(tm*100):03d}.csv'\n",
        "    pd.DataFrame({'request_id': te_ids, 'requester_received_pizza': pm}).to_csv(outp, index=False)\n",
        "    print(f'Wrote {outp} | mean={pm.mean():.6f} | bias={b:.4f}')\n",
        "    if abs(tm - 0.30) < 1e-9:\n",
        "        pd.DataFrame({'request_id': te_ids, 'requester_received_pizza': pm}).to_csv('submission.csv', index=False)\n",
        "        print('PROMOTED: submission.csv <-', outp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "fa0a71c2-6801-4522-9b46-10754592d88b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Promote time-aware stacker 0.32 mean variant to submission.csv\n",
        "import pandas as pd, os\n",
        "src = 'submission_s41_time_meta_gamma_m032.csv'\n",
        "assert os.path.exists(src), f'Missing {src}; run Cell 6 first.'\n",
        "df = pd.read_csv(src)\n",
        "df.to_csv('submission.csv', index=False)\n",
        "print('Promoted', src, 'to submission.csv | mean=', float(df['requester_received_pizza'].mean()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "1d2c28c7-958f-4a02-bfee-e493c8bfdac0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S43-rank5-033: Bias existing rank-average to mean 0.33 and promote\n",
        "import os, numpy as np, pandas as pd\n",
        "from scipy.special import expit\n",
        "\n",
        "def logit_clip(p, eps=1e-6):\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def find_bias_for_target_mean(probs, target_mean, tol=1e-6, max_iter=100):\n",
        "    z = logit_clip(probs); lo, hi = -10.0, 10.0\n",
        "    for _ in range(max_iter):\n",
        "        mid = 0.5*(lo+hi); m = expit(z+mid).mean()\n",
        "        if abs(m - target_mean) < tol: return mid\n",
        "        if m < target_mean: lo = mid\n",
        "        else: hi = mid\n",
        "    return 0.5*(lo+hi)\n",
        "\n",
        "def apply_bias(probs, b):\n",
        "    return expit(logit_clip(probs) + b)\n",
        "\n",
        "src = 'submission_rank5.csv'\n",
        "assert os.path.exists(src), 'Missing submission_rank5.csv; run Cell 31 first.'\n",
        "s = pd.read_csv(src)\n",
        "probs = s['requester_received_pizza'].values.astype(float)\n",
        "b = find_bias_for_target_mean(probs, 0.33)\n",
        "s['requester_received_pizza'] = apply_bias(probs, b).astype(np.float32)\n",
        "s.to_csv('submission_rank5_m033.csv', index=False)\n",
        "s.to_csv('submission.csv', index=False)\n",
        "print('PROMOTED: submission.csv <- submission_rank5_m033.csv | mean=', float(s['requester_received_pizza'].mean()), '| bias=', float(b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "29c669ff-118f-4edf-9ae6-9456de2fb191",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Promote time-aware stacker 0.28 mean variant to submission.csv\n",
        "import pandas as pd, os\n",
        "src = 'submission_s41_time_meta_gamma_m028.csv'\n",
        "assert os.path.exists(src), f'Missing {src}; run Cell 6 first.'\n",
        "df = pd.read_csv(src)\n",
        "df.to_csv('submission.csv', index=False)\n",
        "print('Promoted', src, 'to submission.csv | mean=', float(df['requester_received_pizza'].mean()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "59b02ee9-b2a6-4af4-b317-2cf62e569206",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S46-hybrid-time-rank5: 50/50 logit hedge of time-aware stacker (unbiased) and rank-average; bias to 0.30/0.32\n",
        "import os, numpy as np, pandas as pd\n",
        "from scipy.special import expit\n",
        "\n",
        "def logit_clip(p, eps=1e-6):\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def load_sub_norm(path):\n",
        "    df = pd.read_csv(path)\n",
        "    cols = [c.lower() for c in df.columns]\n",
        "    if 'request_id' in cols:\n",
        "        id_col = df.columns[cols.index('request_id')]\n",
        "    elif 'id' in cols:\n",
        "        id_col = df.columns[cols.index('id')]\n",
        "    else:\n",
        "        raise ValueError('No id/request_id in ' + path)\n",
        "    prob_col = [c for c in df.columns if c != id_col][0]\n",
        "    out = df[[id_col, prob_col]].rename(columns={id_col:'request_id', prob_col:'requester_received_pizza'})\n",
        "    return out\n",
        "\n",
        "def find_bias_for_target_mean(probs, target_mean, tol=1e-6, max_iter=100):\n",
        "    z = logit_clip(probs); lo, hi = -10.0, 10.0\n",
        "    for _ in range(max_iter):\n",
        "        mid = 0.5*(lo+hi); m = expit(z+mid).mean()\n",
        "        if abs(m - target_mean) < tol: return mid\n",
        "        if m < target_mean: lo = mid\n",
        "        else: hi = mid\n",
        "    return 0.5*(lo+hi)\n",
        "\n",
        "def apply_bias(probs, b):\n",
        "    return expit(logit_clip(probs) + b)\n",
        "\n",
        "# Inputs: time-aware stacker (unbiased) and rank-average (unbiased)\n",
        "path_time = 'submission_s41_time_meta_gamma.csv'\n",
        "path_rank = 'submission_rank5.csv'\n",
        "assert os.path.exists(path_time), f'Missing {path_time}; run Cell 6 first.'\n",
        "assert os.path.exists(path_rank), f'Missing {path_rank}; run Cell 31 first.'\n",
        "\n",
        "s_time = load_sub_norm(path_time)\n",
        "s_rank = load_sub_norm(path_rank)\n",
        "assert np.all(s_time.request_id.values == s_rank.request_id.values), 'ID mismatch between time-aware and rank submissions'\n",
        "\n",
        "# 50/50 logit hedge\n",
        "z1 = logit_clip(s_time.requester_received_pizza.values)\n",
        "z2 = logit_clip(s_rank.requester_received_pizza.values)\n",
        "z = 0.5*(z1 + z2)\n",
        "p = expit(z).astype(np.float32)\n",
        "df_h = pd.DataFrame({'request_id': s_time.request_id.values, 'requester_received_pizza': p})\n",
        "df_h.to_csv('submission_s46_hybrid_time_rank.csv', index=False)\n",
        "print('Wrote submission_s46_hybrid_time_rank.csv | mean=', float(p.mean()))\n",
        "\n",
        "# Bias to 0.30 and 0.32; promote 0.30\n",
        "for tm in [0.30, 0.32]:\n",
        "    b = find_bias_for_target_mean(p, tm)\n",
        "    pm = apply_bias(p, b).astype(np.float32)\n",
        "    outp = f'submission_s46_hybrid_time_rank_m{int(tm*100):03d}.csv'\n",
        "    pd.DataFrame({'request_id': s_time.request_id.values, 'requester_received_pizza': pm}).to_csv(outp, index=False)\n",
        "    print(f'Wrote {outp} | mean={pm.mean():.6f} | bias={b:.4f}')\n",
        "    if abs(tm - 0.30) < 1e-9:\n",
        "        pd.DataFrame({'request_id': s_time.request_id.values, 'requester_received_pizza': pm}).to_csv('submission.csv', index=False)\n",
        "        print('PROMOTED: submission.csv <-', outp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "7f6d7c94-eab6-4546-8d37-081200b2e05e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S47-LR-OOF: True forward-chaining LogisticRegression stacker with time interactions + recency hedges\n",
        "import os, time, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "t0 = time.time()\n",
        "print('S47-LR-OOF: starting...')\n",
        "id_col = 'request_id'; target_col = 'requester_received_pizza'\n",
        "train = pd.read_json('train.json'); test = pd.read_json('test.json')\n",
        "y = train[target_col].astype(int).values\n",
        "ntr, nte = len(train), len(test)\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p = np.clip(np.asarray(p, dtype=np.float64), eps, 1-eps)\n",
        "    return np.log(p/(1-p))\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "# Time ordering and contiguous blocks\n",
        "ts_col = 'unix_timestamp_of_request' if 'unix_timestamp_of_request' in train.columns else 'unix_timestamp_of_request_utc'\n",
        "order = np.argsort(train[ts_col].values)\n",
        "k = 6\n",
        "blocks = np.array_split(order, k)\n",
        "\n",
        "# Load base OOF/test probs and convert to logits\n",
        "bases = [\n",
        "    ('LR_nosub', 'oof_lr_time_nosub_meta.npy', 'test_lr_time_nosub_meta.npy'),\n",
        "    ('Dense_v1', 'oof_xgb_dense_time.npy', 'test_xgb_dense_time.npy'),\n",
        "    ('Meta',    'oof_xgb_meta_time.npy',  'test_xgb_meta_time.npy'),\n",
        "    ('MiniLM',  'oof_xgb_emb_meta_time.npy', 'test_xgb_emb_meta_time.npy'),\n",
        "    ('MPNet',   'oof_xgb_emb_mpnet_time.npy', 'test_xgb_emb_mpnet_time.npy'),\n",
        "    ('E5_meta', 'oof_xgb_e5_meta_time.npy', 'test_xgb_e5_meta_time.npy'),\n",
        "]\n",
        "names, Z_oof_list, Z_te_list = [], [], []\n",
        "for name, oof_fp, te_fp in bases:\n",
        "    if (not os.path.exists(oof_fp)) or (not os.path.exists(te_fp)):\n",
        "        print(f'Skipping {name}: missing files')\n",
        "        continue\n",
        "    oof = np.load(oof_fp); te = np.load(te_fp)\n",
        "    if oof.ndim>1: oof=oof.ravel()\n",
        "    if te.ndim>1: te=te.ravel()\n",
        "    if len(oof) != ntr:\n",
        "        print(f'Skipping {name}: OOF length mismatch {len(oof)} != {ntr}')\n",
        "        continue\n",
        "    names.append(name)\n",
        "    Z_oof_list.append(to_logit(oof)); Z_te_list.append(to_logit(te))\n",
        "print('Included bases:', names)\n",
        "Xb_tr = np.column_stack(Z_oof_list).astype(np.float64)\n",
        "Xb_te = np.column_stack(Z_te_list).astype(np.float64)\n",
        "nb = Xb_tr.shape[1]\n",
        "\n",
        "# Meta features\n",
        "def get_text(df):\n",
        "    title = df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "    body = df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "    return title + '\\n' + body\n",
        "tr_txt = get_text(train); te_txt = get_text(test)\n",
        "tr_len = tr_txt.str.len().values.astype(np.float64); te_len = te_txt.str.len().values.astype(np.float64)\n",
        "tr_log1p = np.log1p(tr_len); te_log1p = np.log1p(te_len)\n",
        "tr_age = train.get('requester_account_age_in_days_at_request', pd.Series([0]*ntr)).fillna(0).values.astype(np.float64)\n",
        "te_age = test.get('requester_account_age_in_days_at_request', pd.Series([0]*nte)).fillna(0).values.astype(np.float64)\n",
        "ts_tr = train[ts_col].values.astype(np.int64); ts_te = (test[ts_col].values.astype(np.int64) if ts_col in test.columns else np.zeros(nte, np.int64))\n",
        "all_ts = np.concatenate([ts_tr, ts_te])\n",
        "ord_all = np.argsort(all_ts)\n",
        "rank_all = np.empty_like(ord_all)\n",
        "rank_all[ord_all] = np.arange(len(all_ts))\n",
        "rank01_all = rank_all.astype(np.float64) / max(1, (len(all_ts)-1))\n",
        "tr_rank = rank01_all[:ntr]; te_rank = rank01_all[ntr:]\n",
        "\n",
        "# Build feature matrices with interactions: base logits + metas + base*time_rank\n",
        "def build_with_interactions(Z, log1p_len, age, trank):\n",
        "    inter = Z * trank[:, None]\n",
        "    return np.column_stack([Z, log1p_len, age, trank, inter]).astype(np.float64)\n",
        "X_tr_full = build_with_interactions(Xb_tr, tr_log1p, tr_age, tr_rank)\n",
        "X_te_full = build_with_interactions(Xb_te, te_log1p, te_age, te_rank)\n",
        "feat_cols = names + ['log1p_text_len','account_age_days','time_rank01'] + [f'{n}*time' for n in names]\n",
        "print('Final cols:', feat_cols[:min(10,len(feat_cols))], '... total', len(feat_cols))\n",
        "\n",
        "# Forward-chaining OOF for C grid; evaluate AUC_last and gamma-weighted over blocks 2..5\n",
        "val_blocks = [2,3,4,5]\n",
        "gamma = 0.995\n",
        "def gamma_weights_for_oof():\n",
        "    W = np.zeros(ntr, dtype=np.float64)\n",
        "    for bi in range(1, k):\n",
        "        age = (k - 1) - bi\n",
        "        w_block = (gamma ** age)\n",
        "        if bi == 5: w_block *= 2.0\n",
        "        W[blocks[bi]] = w_block\n",
        "    return W\n",
        "W_oof = gamma_weights_for_oof()\n",
        "C_grid = [0.2, 0.5, 1.0, 2.0, 5.0]\n",
        "best = None\n",
        "for C in C_grid:\n",
        "    oof = np.full(ntr, np.nan, dtype=np.float64)\n",
        "    for vb in val_blocks:\n",
        "        tr_idx = np.concatenate([blocks[i-1] for i in range(1, vb)])\n",
        "        va_idx = blocks[vb-1]\n",
        "        sc = StandardScaler(with_mean=True, with_std=True)\n",
        "        Xtr = sc.fit_transform(X_tr_full[tr_idx])\n",
        "        Xva = sc.transform(X_tr_full[va_idx])\n",
        "        lr = LogisticRegression(penalty='l2', solver='lbfgs', C=C, max_iter=2000, fit_intercept=True)\n",
        "        lr.fit(Xtr, y[tr_idx])\n",
        "        z = lr.decision_function(Xva)\n",
        "        p = sigmoid(z)\n",
        "        oof[va_idx] = p\n",
        "    mask = np.isfinite(oof)\n",
        "    auc_last = roc_auc_score(y[blocks[4-1]], oof[blocks[4-1]])\n",
        "    auc_gamma = roc_auc_score(y[mask], oof[mask], sample_weight=W_oof[mask])\n",
        "    print(f'C={C} | AUC_last={auc_last:.5f} | AUC_gamma={auc_gamma:.5f}')\n",
        "    if (best is None) or (auc_last > best['auc_last']) or (abs(auc_last - best['auc_last']) < 1e-12 and auc_gamma > best['auc_gamma']):\n",
        "        best = dict(C=C, auc_last=auc_last, auc_gamma=auc_gamma)\n",
        "print('Best C:', best)\n",
        "\n",
        "# Final fit on blocks 1..4, evaluate on block 5 for sanity, then predict test\n",
        "tr_final_idx = np.concatenate([blocks[i] for i in range(0,4)])\n",
        "va_final_idx = blocks[4]\n",
        "sc = StandardScaler(with_mean=True, with_std=True)\n",
        "Xtr = sc.fit_transform(X_tr_full[tr_final_idx])\n",
        "Xva = sc.transform(X_tr_full[va_final_idx])\n",
        "Xte = sc.transform(X_te_full)\n",
        "lr = LogisticRegression(penalty='l2', solver='lbfgs', C=best['C'], max_iter=2000, fit_intercept=True)\n",
        "lr.fit(Xtr, y[tr_final_idx])\n",
        "p_va = sigmoid(lr.decision_function(Xva))\n",
        "auc_va = roc_auc_score(y[va_final_idx], p_va)\n",
        "print(f'Final sanity AUC on block5={auc_va:.5f}')\n",
        "\n",
        "p_gamma = sigmoid(lr.decision_function(Xte)).astype(np.float32)\n",
        "pd.DataFrame({id_col: test[id_col].values, target_col: p_gamma}).to_csv('submission_s47lr_meta_gamma.csv', index=False)\n",
        "print('Wrote submission_s47lr_meta_gamma.csv | mean', float(p_gamma.mean()))\n",
        "\n",
        "# Safe test-time recency: interpolate selected base logits, rebuild features and predict\n",
        "recent_map = {\n",
        "    'LR_nosub': [\n",
        "        'test_lr_time_nosub_meta_recent35.npy',\n",
        "        'test_lr_time_nosub_meta_recent45.npy',\n",
        "    ],\n",
        "    'MiniLM': [\n",
        "        'test_xgb_emb_meta_time_recent35.npy',\n",
        "        'test_xgb_emb_meta_time_recent45.npy',\n",
        "    ],\n",
        "    'MPNet': [\n",
        "        'test_xgb_emb_mpnet_time_recent35.npy',\n",
        "        'test_xgb_emb_mpnet_time_recent45.npy',\n",
        "    ],\n",
        "}\n",
        "name_to_j = {n:i for i,n in enumerate(names)}\n",
        "def load_recent_avg_logit(files):\n",
        "    arrs = []\n",
        "    for fp in files:\n",
        "        if os.path.exists(fp):\n",
        "            a = np.load(fp)\n",
        "            if a.ndim>1: a=a.ravel()\n",
        "            arrs.append(to_logit(a))\n",
        "    if not arrs: return None\n",
        "    return np.mean(arrs, axis=0).astype(np.float64)\n",
        "\n",
        "def apply_recency_to_Zte(Z_base, alphas):\n",
        "    Zr = Z_base.copy()\n",
        "    for bname, a in alphas.items():\n",
        "        if a <= 0: continue\n",
        "        if bname not in name_to_j: continue\n",
        "        rec = load_recent_avg_logit(recent_map.get(bname, []))\n",
        "        if rec is None: continue\n",
        "        j = name_to_j[bname]\n",
        "        Zr[:, j] = (1.0 - a)*Z_base[:, j] + a*rec\n",
        "    return Zr\n",
        "\n",
        "alphas_low = {'LR_nosub': 0.00, 'MiniLM': 0.15, 'MPNet': 0.20}\n",
        "alphas_high = {'LR_nosub': 0.05, 'MiniLM': 0.25, 'MPNet': 0.30}\n",
        "\n",
        "def predict_with_Zte(Zte_mod):\n",
        "    Xte_mod = build_with_interactions(Zte_mod, te_log1p, te_age, te_rank)\n",
        "    Xte_mod = sc.transform(Xte_mod)\n",
        "    return sigmoid(lr.decision_function(Xte_mod)).astype(np.float32)\n",
        "\n",
        "Zte_low = apply_recency_to_Zte(Xb_te, alphas_low)\n",
        "Zte_high = apply_recency_to_Zte(Xb_te, alphas_high)\n",
        "p_low = predict_with_Zte(Zte_low)\n",
        "p_high = predict_with_Zte(Zte_high)\n",
        "pd.DataFrame({id_col: test[id_col].values, target_col: p_low}).to_csv('submission_s47lr_meta_r_low.csv', index=False)\n",
        "pd.DataFrame({id_col: test[id_col].values, target_col: p_high}).to_csv('submission_s47lr_meta_r_high.csv', index=False)\n",
        "print('Wrote r_low/r_high | means ->', float(p_low.mean()), float(p_high.mean()))\n",
        "\n",
        "# 3-way logit hedge and bias portfolio\n",
        "def logit_clip(p, eps=1e-6):\n",
        "    p = np.clip(p.astype(np.float64), eps, 1-eps)\n",
        "    return np.log(p/(1-p))\n",
        "z_g = logit_clip(p_gamma); z_l = logit_clip(p_low); z_h = logit_clip(p_high)\n",
        "p_hedge3 = sigmoid((z_g + z_l + z_h)/3.0).astype(np.float32)\n",
        "pd.DataFrame({id_col: test[id_col].values, target_col: p_hedge3}).to_csv('submission_s47lr_meta_hedge3.csv', index=False)\n",
        "print('Wrote submission_s47lr_meta_hedge3.csv | mean', float(p_hedge3.mean()))\n",
        "\n",
        "def bias_to_mean(probs, target, tol=1e-6, it=100):\n",
        "    z = logit_clip(probs); lo, hi = -10.0, 10.0\n",
        "    for _ in range(it):\n",
        "        mid = 0.5*(lo+hi); m = sigmoid(z+mid).mean()\n",
        "        if abs(m - target) < tol: return mid\n",
        "        if m < target: lo = mid\n",
        "        else: hi = mid\n",
        "    return 0.5*(lo+hi)\n",
        "\n",
        "for tm in [0.30, 0.32, 0.28]:\n",
        "    b = bias_to_mean(p_hedge3, tm)\n",
        "    pm = sigmoid(logit_clip(p_hedge3) + b).astype(np.float32)\n",
        "    outp = f'submission_s47lr_meta_hedge3_m{int(round(tm*100)):03d}.csv'\n",
        "    pd.DataFrame({id_col: test[id_col].values, target_col: pm}).to_csv(outp, index=False)\n",
        "    print(f'Wrote {outp} | mean={pm.mean():.6f} | bias={b:.4f}')\n",
        "    if abs(tm - 0.30) < 1e-9:\n",
        "        pd.DataFrame({id_col: test[id_col].values, target_col: pm}).to_csv('submission.csv', index=False)\n",
        "        print('PROMOTED: submission.csv <-', outp)\n",
        "\n",
        "print(f'S47-LR-OOF done in {time.time()-t0:.1f}s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "dd603c6d-d287-46ee-a271-b9d325b98adc",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S48-diagnostic: Validate alignment of base OOF arrays vs train indices; per-base AUCs by last block\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "print('S48: starting base OOF alignment diagnostic...')\n",
        "\n",
        "# Reuse variables from latest run if present, else reload minimal artifacts\n",
        "try:\n",
        "    _names = names; _Z_oof_list = Z_oof_list; _blocks = blocks; _y = y\n",
        "except NameError:\n",
        "    train = pd.read_json('train.json')\n",
        "    y = train['requester_received_pizza'].astype(int).values\n",
        "    ts_col = 'unix_timestamp_of_request' if 'unix_timestamp_of_request' in train.columns else 'unix_timestamp_of_request_utc'\n",
        "    order = np.argsort(train[ts_col].values)\n",
        "    k = 6\n",
        "    blocks = np.array_split(order, k)\n",
        "    bases = [\n",
        "        ('LR_nosub', 'oof_lr_time_nosub_meta.npy'),\n",
        "        ('Dense_v1', 'oof_xgb_dense_time.npy'),\n",
        "        ('Meta',    'oof_xgb_meta_time.npy'),\n",
        "        ('MiniLM',  'oof_xgb_emb_meta_time.npy'),\n",
        "        ('MPNet',   'oof_xgb_emb_mpnet_time.npy'),\n",
        "        ('E5_meta', 'oof_xgb_e5_meta_time.npy'),\n",
        "    ]\n",
        "    def to_logit(p, eps=1e-6):\n",
        "        p = np.clip(np.asarray(p, dtype=np.float64), eps, 1-eps)\n",
        "        return np.log(p/(1-p))\n",
        "    names = []\n",
        "    Z_oof_list = []\n",
        "    for name, fp in bases:\n",
        "        if not os.path.exists(fp):\n",
        "            print('Missing', fp, '-> skipping', name)\n",
        "            continue\n",
        "        a = np.load(fp); a = a.ravel()\n",
        "        if a.shape[0] != len(train):\n",
        "            print('Len mismatch for', name, a.shape[0], '!=', len(train))\n",
        "            continue\n",
        "        names.append(name); Z_oof_list.append(to_logit(a))\n",
        "    _names, _Z_oof_list, _blocks, _y = names, Z_oof_list, blocks, y\n",
        "\n",
        "n = len(_y)\n",
        "last_block_idx = _blocks[4]  # block 5 indices (original indexing)\n",
        "\n",
        "def safe_auc(y_true, y_score):\n",
        "    try:\n",
        "        return float(roc_auc_score(y_true, y_score))\n",
        "    except Exception:\n",
        "        return float('nan')\n",
        "\n",
        "rows = []\n",
        "for j, name in enumerate(_names):\n",
        "    z = _Z_oof_list[j]  # logit-oof\n",
        "    p = 1.0/(1.0+np.exp(-z))\n",
        "    auc_overall = safe_auc(_y, p)\n",
        "    auc_last = safe_auc(_y[last_block_idx], p[last_block_idx])\n",
        "    # Alternate hypothesis: if OOF arrays were saved in time-sorted order already, the last block would correspond to the tail slice\n",
        "    tail_len = len(last_block_idx)\n",
        "    tail_idx = np.arange(n - tail_len, n)\n",
        "    auc_tail = safe_auc(_y[tail_idx], p[tail_idx])\n",
        "    rows.append((name, auc_overall, auc_last, auc_tail))\n",
        "\n",
        "df = pd.DataFrame(rows, columns=['Base','AUC_overall','AUC_last(block5 by idx)','AUC_tail(last N rows)'])\n",
        "print(df.to_string(index=False))\n",
        "df.to_csv('s48_base_oof_alignment.csv', index=False)\n",
        "print('Saved s48_base_oof_alignment.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "3c9846f3-c6ec-4de5-804b-e82f05600a85",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S49-LR_STRONG: Time-aware TFIDF(word1-2 + char_wb3-6) + light metas -> LR with forward-chaining OOF\n",
        "import os, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "t0 = time.time()\n",
        "id_col='request_id'; ycol='requester_received_pizza'; ts='unix_timestamp_of_request'\n",
        "tr=pd.read_json('train.json'); te=pd.read_json('test.json')\n",
        "y=tr[ycol].astype(int).values; n=len(tr); m=len(te)\n",
        "ts_col = ts if ts in tr.columns else 'unix_timestamp_of_request_utc'\n",
        "order=np.argsort(tr[ts_col].values); k=6; blocks=np.array_split(order,k)\n",
        "\n",
        "def get_text(df):\n",
        "    t=df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "    b=df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "    return t+'\\n'+b\n",
        "tx_tr=get_text(tr); tx_te=get_text(te)\n",
        "\n",
        "# lightweight metas present in BOTH train/test\n",
        "meta_cands=['requester_account_age_in_days_at_request','requester_number_of_posts_at_request','requester_number_of_comments_at_request','requester_upvotes_minus_downvotes_at_request','requester_upvotes_plus_downvotes_at_request']\n",
        "meta_cols=[c for c in meta_cands if (c in tr.columns and c in te.columns)]\n",
        "log1p_len_tr=np.log1p(tx_tr.str.len().values)\n",
        "log1p_len_te=np.log1p(tx_te.str.len().values)\n",
        "Xmeta_tr=np.column_stack([log1p_len_tr]+[tr[c].fillna(0).astype(float).values for c in meta_cols])\n",
        "Xmeta_te=np.column_stack([log1p_len_te]+[te[c].fillna(0).astype(float).values for c in meta_cols])\n",
        "\n",
        "# Vectorizers\n",
        "v_w=TfidfVectorizer(analyzer='word', ngram_range=(1,2), min_df=2, max_df=0.95, max_features=400000, strip_accents='unicode', lowercase=True, sublinear_tf=True)\n",
        "v_c=TfidfVectorizer(analyzer='char_wb', ngram_range=(3,6), min_df=2, max_features=400000, sublinear_tf=True)\n",
        "Xw_tr=v_w.fit_transform(tx_tr); Xw_te=v_w.transform(tx_te)\n",
        "Xc_tr=v_c.fit_transform(tx_tr); Xc_te=v_c.transform(tx_te)\n",
        "Xtxt_tr=sparse.hstack([Xw_tr,Xc_tr],format='csr')\n",
        "Xtxt_te=sparse.hstack([Xw_te,Xc_te],format='csr')\n",
        "X_tr=sparse.hstack([Xtxt_tr, sparse.csr_matrix(Xmeta_tr)],format='csr')\n",
        "X_te=sparse.hstack([Xtxt_te, sparse.csr_matrix(Xmeta_te)],format='csr')\n",
        "print('Shapes -> X_tr:', X_tr.shape, 'X_te:', X_te.shape, '| metas used:', meta_cols)\n",
        "\n",
        "def fit_lr(C):\n",
        "    return LogisticRegression(solver='liblinear', penalty='l2', C=C, max_iter=3000, random_state=42)\n",
        "\n",
        "best=None\n",
        "C_list=[1.0,2.0,3.0,5.0]\n",
        "for C in C_list:\n",
        "    oof=np.full(n, np.nan, float)\n",
        "    for vb in [2,3,4,5]:\n",
        "        tr_idx=np.concatenate([blocks[i-1] for i in range(1,vb)])\n",
        "        va_idx=blocks[vb-1]\n",
        "        lr=fit_lr(C); lr.fit(X_tr[tr_idx], y[tr_idx])\n",
        "        oof[va_idx]=lr.predict_proba(X_tr[va_idx])[:,1]\n",
        "    auc_last=roc_auc_score(y[blocks[4-1]], oof[blocks[4-1]])\n",
        "    print(f'C={C} | AUC_last={auc_last:.5f}')\n",
        "    if (best is None) or (auc_last>best[0]): best=(auc_last,C,oof)\n",
        "print('BestC:', best[1], '| AUC_last=', f'{best[0]:.5f}')\n",
        "\n",
        "# Final fit on blocks 1..4 and predict test\n",
        "tr_final=np.concatenate([blocks[i] for i in range(0,4)])\n",
        "lr=fit_lr(best[1]); lr.fit(X_tr[tr_final], y[tr_final])\n",
        "p_te=lr.predict_proba(X_te)[:,1].astype(np.float32)\n",
        "np.save('oof_lr_wordchar_meta_time.npy', best[2].astype(np.float32))\n",
        "np.save('test_lr_wordchar_meta_time.npy', p_te)\n",
        "print('Saved oof_lr_wordchar_meta_time.npy / test_lr_wordchar_meta_time.npy | test mean=', float(p_te.mean()), '| took', f'{time.time()-t0:.1f}s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "80934ca9-ee68-4818-91a0-a2e049639278",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S49b-LR_STRONG_OOF_FOLDS: Per-fold vectorizers (no leakage) + richer metas -> TFIDF LR strong base\n",
        "import re, time, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "t0 = time.time()\n",
        "id_col='request_id'; ycol='requester_received_pizza'; ts='unix_timestamp_of_request'\n",
        "tr=pd.read_json('train.json'); te=pd.read_json('test.json')\n",
        "y=tr[ycol].astype(int).values; n=len(tr); m=len(te)\n",
        "ts_col = ts if ts in tr.columns else 'unix_timestamp_of_request_utc'\n",
        "order=np.argsort(tr[ts_col].values); k=6; blocks=np.array_split(order,k)\n",
        "\n",
        "def get_text(df):\n",
        "    t=df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "    b=df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "    return t+'\\n'+b\n",
        "tx_tr=get_text(tr); tx_te=get_text(te)\n",
        "\n",
        "# Lightweight engineered metas (present in both) + text-derived flags/counts\n",
        "def has_link(s): return 1 if re.search(r'(http|www\\.)', s, re.I) else 0\n",
        "def has_image(s): return 1 if re.search(r'\\.(jpg|jpeg|png|gif)\\b', s, re.I) else 0\n",
        "money_re = re.compile(r'(\\$|dollar|rent|bill|pay|cash)', re.I)\n",
        "urgency_re = re.compile(r'(urgent|emergency|asap|today|tonight)', re.I)\n",
        "def count_pat(p, s):\n",
        "    m = p.findall(s)\n",
        "    return len(m) if m else 0\n",
        "\n",
        "def build_meta(df, tx):\n",
        "    cols = []\n",
        "    # numeric requester stats if present\n",
        "    base_feats = ['requester_account_age_in_days_at_request','requester_number_of_posts_at_request','requester_number_of_comments_at_request','requester_upvotes_minus_downvotes_at_request','requester_upvotes_plus_downvotes_at_request']\n",
        "    arrs = []\n",
        "    for c in base_feats:\n",
        "        if c in df.columns: arrs.append(df[c].fillna(0).astype(float).values); cols.append(c)\n",
        "    # text length\n",
        "    log1p_len = np.log1p(tx.str.len().values); arrs.append(log1p_len); cols.append('log1p_text_len')\n",
        "    # binary/counts\n",
        "    link_f = np.array([has_link(s) for s in tx], dtype=np.float32); arrs.append(link_f); cols.append('has_link')\n",
        "    img_f = np.array([has_image(s) for s in tx], dtype=np.float32); arrs.append(img_f); cols.append('has_image')\n",
        "    money_c = np.array([count_pat(money_re, s) for s in tx], dtype=np.float32); arrs.append(money_c); cols.append('money_cnt')\n",
        "    urg_c = np.array([count_pat(urgency_re, s) for s in tx], dtype=np.float32); arrs.append(urg_c); cols.append('urgency_cnt')\n",
        "    excl_c = np.array([s.count('!') for s in tx], dtype=np.float32); arrs.append(excl_c); cols.append('exclam_cnt')\n",
        "    return np.column_stack(arrs).astype(np.float32), cols\n",
        "\n",
        "Xmeta_tr, meta_cols = build_meta(tr, tx_tr)\n",
        "Xmeta_te, _ = build_meta(te, tx_te)\n",
        "\n",
        "# Vectorizer params\n",
        "w_params=dict(analyzer='word', ngram_range=(1,2), min_df=2, max_df=0.95, max_features=300000, strip_accents='unicode', lowercase=True, sublinear_tf=True)\n",
        "c_params=dict(analyzer='char_wb', ngram_range=(3,6), min_df=2, max_features=300000, sublinear_tf=True)\n",
        "\n",
        "def fit_lr(C):\n",
        "    return LogisticRegression(solver='liblinear', penalty='l2', C=C, max_iter=3000, random_state=42)\n",
        "\n",
        "C_list=[1.0, 2.0, 3.0, 5.0]\n",
        "best=None\n",
        "for C in C_list:\n",
        "    oof=np.full(n, np.nan, dtype=np.float32)\n",
        "    for vb in [2,3,4,5]:\n",
        "        tr_idx=np.concatenate([blocks[i-1] for i in range(1,vb)])\n",
        "        va_idx=blocks[vb-1]\n",
        "        # Fit vectorizers on training folds only\n",
        "        vw=TfidfVectorizer(**w_params); vc=TfidfVectorizer(**c_params)\n",
        "        Xw_tr = vw.fit_transform(tx_tr.iloc[tr_idx]); Xw_va = vw.transform(tx_tr.iloc[va_idx])\n",
        "        Xc_tr = vc.fit_transform(tx_tr.iloc[tr_idx]); Xc_va = vc.transform(tx_tr.iloc[va_idx])\n",
        "        Xtr_txt = sparse.hstack([Xw_tr, Xc_tr], format='csr')\n",
        "        Xva_txt = sparse.hstack([Xw_va, Xc_va], format='csr')\n",
        "        Xtr = sparse.hstack([Xtr_txt, sparse.csr_matrix(Xmeta_tr[tr_idx])], format='csr')\n",
        "        Xva = sparse.hstack([Xva_txt, sparse.csr_matrix(Xmeta_tr[va_idx])], format='csr')\n",
        "        lr=fit_lr(C); lr.fit(Xtr, y[tr_idx])\n",
        "        oof[va_idx] = lr.predict_proba(Xva)[:,1].astype(np.float32)\n",
        "    auc_last=roc_auc_score(y[blocks[4-1]], oof[blocks[4-1]])\n",
        "    print(f'[Leak-free] C={C} | AUC_last={auc_last:.5f}')\n",
        "    if (best is None) or (auc_last>best[0]): best=(auc_last,C,oof)\n",
        "print('BestC (leak-free OOF):', best[1], '| AUC_last=', f'{best[0]:.5f}')\n",
        "\n",
        "# Final fit: train on blocks 1..4 with vectorizers fit on those texts, then predict test\n",
        "tr_final=np.concatenate([blocks[i] for i in range(0,4)])\n",
        "vw=TfidfVectorizer(**w_params); vc=TfidfVectorizer(**c_params)\n",
        "Xw_tr_f = vw.fit_transform(tx_tr.iloc[tr_final]); Xw_te_f = vw.transform(tx_te)\n",
        "Xc_tr_f = vc.fit_transform(tx_tr.iloc[tr_final]); Xc_te_f = vc.transform(tx_te)\n",
        "Xtr_txt_f = sparse.hstack([Xw_tr_f, Xc_tr_f], format='csr')\n",
        "Xte_txt_f = sparse.hstack([Xw_te_f, Xc_te_f], format='csr')\n",
        "Xtr_f = sparse.hstack([Xtr_txt_f, sparse.csr_matrix(Xmeta_tr[tr_final])], format='csr')\n",
        "Xte_f = sparse.hstack([Xte_txt_f, sparse.csr_matrix(Xmeta_te)], format='csr')\n",
        "lr=fit_lr(best[1]); lr.fit(Xtr_f, y[tr_final])\n",
        "p_te=lr.predict_proba(Xte_f)[:,1].astype(np.float32)\n",
        "\n",
        "# Save artifacts\n",
        "np.save('oof_lr_wordchar_meta_time.npy', best[2].astype(np.float32))\n",
        "np.save('test_lr_wordchar_meta_time.npy', p_te)\n",
        "print('Saved oof/test for LR_STRONG | test mean=', float(p_te.mean()), '| total time', f'{time.time()-t0:.1f}s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "ad1547a0-ec0f-47ff-972c-2f95dbd167e4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S50-CA-BLEND: Coordinate-ascent logit blend on block 5, recency hedges (with caps/floors), bias to 0.30/0.32\n",
        "import os, numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "t0=time.time()\n",
        "id_col='request_id'; target_col='requester_received_pizza'\n",
        "train=pd.read_json('train.json'); test=pd.read_json('test.json')\n",
        "y=train[target_col].astype(int).values; ids=test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p=np.clip(p.astype(np.float64), eps, 1-eps); return np.log(p/(1-p))\n",
        "def sigmoid(z): return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "# Time blocks and last validated block (block 5 = index 4)\n",
        "ts_col='unix_timestamp_of_request' if 'unix_timestamp_of_request' in train.columns else 'unix_timestamp_of_request_utc'\n",
        "order=np.argsort(train[ts_col].values); k=6; blocks=np.array_split(order,k)\n",
        "last_idx = blocks[4]\n",
        "\n",
        "# Candidate bases (OOF/test probs). Prefer tuned LR_strong if available.\n",
        "cands=[\n",
        " ('LR_nosub','oof_lr_time_nosub_meta.npy','test_lr_time_nosub_meta.npy'),\n",
        " ('Dense_v1','oof_xgb_dense_time.npy','test_xgb_dense_time.npy'),\n",
        " ('Meta','oof_xgb_meta_time.npy','test_xgb_meta_time.npy'),\n",
        " ('MiniLM','oof_xgb_emb_meta_time.npy','test_xgb_emb_meta_time.npy'),\n",
        " ('MPNet','oof_xgb_emb_mpnet_time.npy','test_xgb_emb_mpnet_time.npy'),\n",
        " ('CatBoost_v2','oof_catboost_textmeta_v2.npy','test_catboost_textmeta_v2.npy'),\n",
        " # LR_strong tuned fallback\n",
        " ('LR_strong_tuned','oof_lr_wordchar_meta_time_tuned.npy','test_lr_wordchar_meta_time_tuned.npy'),\n",
        " ('LR_strong','oof_lr_wordchar_meta_time.npy','test_lr_wordchar_meta_time.npy'),\n",
        "]\n",
        "\n",
        "def pick_lr_strong(entries):\n",
        "    # prefer tuned over untuned\n",
        "    prefer = None; fallback = None\n",
        "    for nm,oo,tt in entries:\n",
        "        if nm=='LR_strong_tuned' and os.path.exists(oo) and os.path.exists(tt):\n",
        "            prefer = ('LR_strong', oo, tt)  # normalize name to LR_strong\n",
        "        if nm=='LR_strong' and os.path.exists(oo) and os.path.exists(tt):\n",
        "            fallback = ('LR_strong', oo, tt)\n",
        "    return prefer if prefer is not None else fallback\n",
        "\n",
        "OOF_list=[]; TEST_list=[]; names=[]; auc_last_rows=[]\n",
        "tmp=[]\n",
        "for nm,oo,tt in cands:\n",
        "    if nm.startswith('LR_strong'):\n",
        "        tmp.append((nm,oo,tt))\n",
        "        continue\n",
        "    if os.path.exists(oo) and os.path.exists(tt):\n",
        "        o=np.load(oo).astype(np.float64).ravel(); t=np.load(tt).astype(np.float64).ravel()\n",
        "        if len(o)==len(y):\n",
        "            auc_last = roc_auc_score(y[last_idx], o[last_idx])\n",
        "            auc_last_rows.append((nm, auc_last))\n",
        "            OOF_list.append(to_logit(o)); TEST_list.append(to_logit(t)); names.append(nm)\n",
        "\n",
        "# handle LR_strong choice\n",
        "lr_choice = pick_lr_strong(tmp)\n",
        "if lr_choice is not None:\n",
        "    nm, oo, tt = lr_choice\n",
        "    o=np.load(oo).astype(np.float64).ravel(); t=np.load(tt).astype(np.float64).ravel()\n",
        "    if len(o)==len(y):\n",
        "        auc_last = roc_auc_score(y[last_idx], o[last_idx])\n",
        "        auc_last_rows.append((nm, auc_last))\n",
        "        OOF_list.append(to_logit(o)); TEST_list.append(to_logit(t)); names.append(nm)\n",
        "\n",
        "print('Loaded bases with AUC_last:', sorted(auc_last_rows, key=lambda x: -x[1]))\n",
        "\n",
        "if len(names)<3:\n",
        "    raise RuntimeError('Too few bases loaded for blending.')\n",
        "\n",
        "# Prune by last-block AUC >= 0.60 (keep at least top-5 if needed)\n",
        "by_name_auc = {nm:auc for nm,auc in auc_last_rows}\n",
        "keep = [nm for nm in names if by_name_auc.get(nm, 0.0) >= 0.60]\n",
        "if len(keep) < 5:\n",
        "    # fallback: keep top-5 by AUC_last\n",
        "    sorted_by = sorted([(nm, OOF_list[i], TEST_list[i], by_name_auc.get(nm, 0.0)) for i,nm in enumerate(names)], key=lambda x: -x[3])[:5]\n",
        "    names = [x[0] for x in sorted_by]\n",
        "    OOF_list = [x[1] for x in sorted_by]\n",
        "    TEST_list = [x[2] for x in sorted_by]\n",
        "else:\n",
        "    names_keep=[]; O_keep=[]; T_keep=[]\n",
        "    for i,nm in enumerate(names):\n",
        "        if nm in keep:\n",
        "            names_keep.append(nm); O_keep.append(OOF_list[i]); T_keep.append(TEST_list[i])\n",
        "    names, OOF_list, TEST_list = names_keep, O_keep, T_keep\n",
        "print('Bases kept:', names)\n",
        "\n",
        "OOF = np.column_stack(OOF_list)  # (n,k)\n",
        "TEST = np.column_stack(TEST_list) # (m,k)\n",
        "k_b = OOF.shape[1]\n",
        "\n",
        "def auc_last_w(w):\n",
        "    z = OOF @ w\n",
        "    return roc_auc_score(y[last_idx], z[last_idx])\n",
        "\n",
        "# Initialize weights\n",
        "w0 = np.ones(k_b, dtype=np.float64) / k_b\n",
        "init_map = {'LR_strong':0.36,'Dense_v1':0.14,'Meta':0.18,'MiniLM':0.16,'MPNet':0.16,'LR_nosub':0.0,'CatBoost_v2':0.05}\n",
        "w = np.array([init_map.get(n, 0.0) for n in names], dtype=np.float64)\n",
        "if w.sum() <= 0: w = w0.copy()\n",
        "else: w = w / w.sum()\n",
        "base_auc = auc_last_w(w)\n",
        "print('Init AUC_last=', f'{base_auc:.5f}', '| init w=', dict(zip(names, np.round(w,3))))\n",
        "\n",
        "# Caps and floors per expert advice\n",
        "caps = {\n",
        "    'CatBoost_v2': 0.50,\n",
        "    'LR_strong': 0.60,\n",
        "    'MiniLM': 0.40,\n",
        "    'MPNet': 0.40,\n",
        "    'Dense_v1': 0.30,\n",
        "    'Meta': 0.30,\n",
        "    'LR_nosub': 0.20,\n",
        "}\n",
        "global_cap = 0.55\n",
        "floors = {\n",
        "    'LR_strong': 0.18,\n",
        "    'MiniLM': 0.10,\n",
        "    'MPNet': 0.10,\n",
        "    'Dense_v1': 0.05,\n",
        "    'Meta': 0.05,\n",
        "}\n",
        "name_to_j = {n:i for i,n in enumerate(names)}\n",
        "\n",
        "def violates_caps(v):\n",
        "    # per-base caps and global single-base cap\n",
        "    if np.max(v) > global_cap + 1e-12: return True\n",
        "    for nm, cap in caps.items():\n",
        "        if nm in name_to_j and v[name_to_j[nm]] > cap + 1e-12:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def apply_floors(v):\n",
        "    v2 = v.copy()\n",
        "    for nm, fl in floors.items():\n",
        "        if nm in name_to_j:\n",
        "            j = name_to_j[nm]\n",
        "            if v2[j] < fl: v2[j] = fl\n",
        "    v2 = v2 / v2.sum()\n",
        "    return v2\n",
        "\n",
        "# Coordinate ascent with nonnegativity, sum=1, caps, and floors enforcement after each pass\n",
        "grid = np.linspace(0.0, 1.0, 21)\n",
        "for it in range(8):\n",
        "    improved=False\n",
        "    for j in range(k_b):\n",
        "        best_auc=base_auc; best_a = w[j]; best_w = w.copy()\n",
        "        for a in grid:\n",
        "            v = w.copy()\n",
        "            # set weight j = a, renormalize others to sum to 1-a\n",
        "            v_others = v.copy(); v_others[j]=0.0\n",
        "            if v_others.sum()>0:\n",
        "                v_others = (1.0 - a) * v_others / v_others.sum()\n",
        "            else:\n",
        "                v_others = np.zeros_like(v_others)\n",
        "            v = v_others; v[j]=a\n",
        "            if violates_caps(v):\n",
        "                continue\n",
        "            A = auc_last_w(v)\n",
        "            if A > best_auc + 1e-6:\n",
        "                best_auc, best_a, best_w = A, a, v.copy()\n",
        "        if best_auc > base_auc + 1e-6:\n",
        "            w = best_w; base_auc = best_auc; improved=True\n",
        "    # apply floors after each full pass and recompute auc\n",
        "    w = apply_floors(w)\n",
        "    base_auc = auc_last_w(w)\n",
        "    print(f'Iter {it}: AUC_last={base_auc:.5f} | w=', dict(zip(names, np.round(w,3))))\n",
        "    if not improved: break\n",
        "\n",
        "print('Final AUC_last:', f'{base_auc:.5f}', '| weights:', dict(zip(names, np.round(w,4))))\n",
        "\n",
        "# Build gamma (no recency) test probs\n",
        "z_te = TEST @ w\n",
        "p_gamma = sigmoid(z_te).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: p_gamma}).to_csv('submission_s50_ca_gamma.csv', index=False)\n",
        "print('Wrote submission_s50_ca_gamma.csv | mean', float(p_gamma.mean()))\n",
        "\n",
        "# Recency on embeddings only (MiniLM, MPNet); add tiny LR_nosub in high per advice\n",
        "def load_recent_avg(prefixes):\n",
        "    arr=[]\n",
        "    for fp in prefixes:\n",
        "        if os.path.exists(fp):\n",
        "            a=np.load(fp); a=a.ravel().astype(np.float64); arr.append(a)\n",
        "    return np.mean(arr,axis=0) if arr else None\n",
        "\n",
        "def apply_recency_to_TEST(TEST_base, alphas):\n",
        "    Z = TEST_base.copy()\n",
        "    # LR_nosub\n",
        "    if 'LR_nosub' in name_to_j and alphas.get('LR_nosub',0)>0:\n",
        "        pr = load_recent_avg(['test_lr_time_nosub_meta_recent35.npy','test_lr_time_nosub_meta_recent45.npy'])\n",
        "        if pr is not None:\n",
        "            j = name_to_j['LR_nosub']\n",
        "            Z[:,j] = (1.0 - alphas['LR_nosub'])*Z[:,j] + alphas['LR_nosub']*to_logit(pr)\n",
        "    # MiniLM\n",
        "    if 'MiniLM' in name_to_j and alphas.get('MiniLM',0)>0:\n",
        "        pr = load_recent_avg(['test_xgb_emb_meta_time_recent35.npy','test_xgb_emb_meta_time_recent45.npy'])\n",
        "        if pr is not None:\n",
        "            j = name_to_j['MiniLM']\n",
        "            Z[:,j] = (1.0 - alphas['MiniLM'])*Z[:,j] + alphas['MiniLM']*to_logit(pr)\n",
        "    # MPNet\n",
        "    if 'MPNet' in name_to_j and alphas.get('MPNet',0)>0:\n",
        "        pr = load_recent_avg(['test_xgb_emb_mpnet_time_recent35.npy','test_xgb_emb_mpnet_time_recent45.npy'])\n",
        "        if pr is not None:\n",
        "            j = name_to_j['MPNet']\n",
        "            Z[:,j] = (1.0 - alphas['MPNet'])*Z[:,j] + alphas['MPNet']*to_logit(pr)\n",
        "    return Z\n",
        "\n",
        "# Updated alphas per expert advice\n",
        "alphas_low = {'MiniLM': 0.18, 'MPNet': 0.22}\n",
        "alphas_high= {'MiniLM': 0.28, 'MPNet': 0.32, 'LR_nosub': 0.05}\n",
        "TEST_low  = apply_recency_to_TEST(TEST, alphas_low)\n",
        "TEST_high = apply_recency_to_TEST(TEST, alphas_high)\n",
        "p_low  = sigmoid(TEST_low @ w).astype(np.float32)\n",
        "p_high = sigmoid(TEST_high @ w).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: p_low}).to_csv('submission_s50_ca_r_low.csv', index=False)\n",
        "pd.DataFrame({id_col: ids, target_col: p_high}).to_csv('submission_s50_ca_r_high.csv', index=False)\n",
        "print('Wrote r_low/r_high means ->', float(p_low.mean()), float(p_high.mean()))\n",
        "\n",
        "# 3-way logit hedge: gamma, r_low, r_high\n",
        "zg, zl, zh = to_logit(p_gamma), to_logit(p_low), to_logit(p_high)\n",
        "p3 = sigmoid((zg+zl+zh)/3.0).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: p3}).to_csv('submission_s50_ca_hedge3.csv', index=False)\n",
        "print('Wrote submission_s50_ca_hedge3.csv | mean', float(p3.mean()))\n",
        "\n",
        "# Bias to 0.30 and 0.32; promote 0.30\n",
        "def bias_to_mean(probs, target, it=80):\n",
        "    z = to_logit(probs); lo,hi=-10.0,10.0\n",
        "    for _ in range(it):\n",
        "        mid=(lo+hi)/2; m=sigmoid(z+mid).mean()\n",
        "        if m<target: lo=mid\n",
        "        else: hi=mid\n",
        "    return (lo+hi)/2\n",
        "\n",
        "for tm in [0.30, 0.32]:\n",
        "    b = bias_to_mean(p3, tm)\n",
        "    pm = sigmoid(to_logit(p3)+b).astype(np.float32)\n",
        "    outp = f'submission_s50_ca_hedge3_m{int(tm*100):03d}.csv'\n",
        "    pd.DataFrame({id_col: ids, target_col: pm}).to_csv(outp, index=False)\n",
        "    print(f'Wrote {outp} | mean={pm.mean():.6f}')\n",
        "    if abs(tm-0.30)<1e-9:\n",
        "        pd.DataFrame({id_col: ids, target_col: pm}).to_csv('submission.csv', index=False)\n",
        "        print('PROMOTED: submission.csv <-', outp)\n",
        "\n",
        "print('S50-CA-BLEND done in', f'{time.time()-t0:.1f}s')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded bases with AUC_last: [('CatBoost_v2', 0.650664850639457), ('MiniLM', 0.6444665035320191), ('Dense_v1', 0.6396417193776259), ('MPNet', 0.6346091693984025), ('LR_nosub', 0.6235744955907474), ('Meta', 0.621023592963664), ('LR_strong', 0.6190498176277761)]\nBases kept: ['LR_nosub', 'Dense_v1', 'Meta', 'MiniLM', 'MPNet', 'CatBoost_v2', 'LR_strong']\nInit AUC_last= 0.64617 | init w= {'LR_nosub': 0.0, 'Dense_v1': 0.133, 'Meta': 0.171, 'MiniLM': 0.152, 'MPNet': 0.152, 'CatBoost_v2': 0.048, 'LR_strong': 0.343}\nIter 0: AUC_last=0.65398 | w= {'LR_nosub': 0.016, 'Dense_v1': 0.048, 'Meta': 0.072, 'MiniLM': 0.145, 'MPNet': 0.113, 'CatBoost_v2': 0.445, 'LR_strong': 0.16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 1: AUC_last=0.65564 | w= {'LR_nosub': 0.0, 'Dense_v1': 0.067, 'Meta': 0.044, 'MiniLM': 0.205, 'MPNet': 0.088, 'CatBoost_v2': 0.438, 'LR_strong': 0.158}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 2: AUC_last=0.65562 | w= {'LR_nosub': 0.0, 'Dense_v1': 0.065, 'Meta': 0.043, 'MiniLM': 0.143, 'MPNet': 0.165, 'CatBoost_v2': 0.429, 'LR_strong': 0.155}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 3: AUC_last=0.65550 | w= {'LR_nosub': 0.0, 'Dense_v1': 0.044, 'Meta': 0.044, 'MiniLM': 0.174, 'MPNet': 0.147, 'CatBoost_v2': 0.435, 'LR_strong': 0.157}\nIter 4: AUC_last=0.65569 | w= {'LR_nosub': 0.0, 'Dense_v1': 0.053, 'Meta': 0.044, 'MiniLM': 0.215, 'MPNet': 0.088, 'CatBoost_v2': 0.441, 'LR_strong': 0.159}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 5: AUC_last=0.65543 | w= {'LR_nosub': 0.0, 'Dense_v1': 0.064, 'Meta': 0.043, 'MiniLM': 0.143, 'MPNet': 0.165, 'CatBoost_v2': 0.43, 'LR_strong': 0.155}\nIter 6: AUC_last=0.65585 | w= {'LR_nosub': 0.0, 'Dense_v1': 0.055, 'Meta': 0.043, 'MiniLM': 0.23, 'MPNet': 0.086, 'CatBoost_v2': 0.431, 'LR_strong': 0.155}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 7: AUC_last=0.65548 | w= {'LR_nosub': 0.0, 'Dense_v1': 0.09, 'Meta': 0.043, 'MiniLM': 0.198, 'MPNet': 0.086, 'CatBoost_v2': 0.429, 'LR_strong': 0.154}\nFinal AUC_last: 0.65548 | weights: {'LR_nosub': 0.0, 'Dense_v1': 0.0898, 'Meta': 0.0429, 'MiniLM': 0.1982, 'MPNet': 0.0858, 'CatBoost_v2': 0.4289, 'LR_strong': 0.1544}\nWrote submission_s50_ca_gamma.csv | mean 0.33813750743865967\nWrote r_low/r_high means -> 0.3257971704006195 0.319436252117157\nWrote submission_s50_ca_hedge3.csv | mean 0.3277496099472046\nWrote submission_s50_ca_hedge3_m030.csv | mean=0.300000\nPROMOTED: submission.csv <- submission_s50_ca_hedge3_m030.csv\nWrote submission_s50_ca_hedge3_m032.csv | mean=0.320000\nS50-CA-BLEND done in 1.8s\n"
          ]
        }
      ]
    },
    {
      "id": "5c359fab-96b8-4f2e-b86e-b434a4fb02b0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S49c-LR_STRONG_TUNED: Per-fold TFIDF word(1-3)+char_wb(3-5), min_df=1, larger vocab, class_weight variants\n",
        "import re, time, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "t0 = time.time()\n",
        "id_col='request_id'; ycol='requester_received_pizza'; ts='unix_timestamp_of_request'\n",
        "tr=pd.read_json('train.json'); te=pd.read_json('test.json')\n",
        "y=tr[ycol].astype(int).values; n=len(tr); m=len(te)\n",
        "ts_col = ts if ts in tr.columns else 'unix_timestamp_of_request_utc'\n",
        "order=np.argsort(tr[ts_col].values); k=6; blocks=np.array_split(order,k)\n",
        "\n",
        "def get_text(df):\n",
        "    t=df.get('request_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "    b=df.get('request_text_edit_aware', df.get('request_text', pd.Series(['']*len(df)))).fillna('').astype(str)\n",
        "    return t+'\\n'+b\n",
        "tx_tr=get_text(tr); tx_te=get_text(te)\n",
        "\n",
        "# Lightweight engineered metas (numeric present in both) + simple text flags/counts\n",
        "def has_link(s): return 1 if re.search(r'(http|www\\.)', s, re.I) else 0\n",
        "def has_image(s): return 1 if re.search(r'\\.(jpg|jpeg|png|gif)\\b', s, re.I) else 0\n",
        "money_re = re.compile(r'(\\$|dollar|rent|bill|pay|cash)', re.I)\n",
        "urgency_re = re.compile(r'(urgent|emergency|asap|today|tonight)', re.I)\n",
        "def cnt(p, s):\n",
        "    m = p.findall(s); return len(m) if m else 0\n",
        "\n",
        "def build_meta(df, tx):\n",
        "    cols = []; arrs = []\n",
        "    base = ['requester_account_age_in_days_at_request','requester_number_of_posts_at_request','requester_number_of_comments_at_request','requester_upvotes_minus_downvotes_at_request','requester_upvotes_plus_downvotes_at_request']\n",
        "    for c in base:\n",
        "        if c in df.columns: arrs.append(df[c].fillna(0).astype(float).values); cols.append(c)\n",
        "    log1p_len = np.log1p(tx.str.len().values); arrs.append(log1p_len); cols.append('log1p_text_len')\n",
        "    link_f = np.array([has_link(s) for s in tx], dtype=np.float32); arrs.append(link_f); cols.append('has_link')\n",
        "    img_f = np.array([has_image(s) for s in tx], dtype=np.float32); arrs.append(img_f); cols.append('has_image')\n",
        "    money_c = np.array([cnt(money_re, s) for s in tx], dtype=np.float32); arrs.append(money_c); cols.append('money_cnt')\n",
        "    urg_c = np.array([cnt(urgency_re, s) for s in tx], dtype=np.float32); arrs.append(urg_c); cols.append('urgency_cnt')\n",
        "    excl_c = np.array([s.count('!') for s in tx], dtype=np.float32); arrs.append(excl_c); cols.append('exclam_cnt')\n",
        "    return np.column_stack(arrs).astype(np.float32), cols\n",
        "\n",
        "Xmeta_tr, meta_cols = build_meta(tr, tx_tr)\n",
        "Xmeta_te, _ = build_meta(te, tx_te)\n",
        "\n",
        "# Tuned vectorizers\n",
        "w_params=dict(analyzer='word', ngram_range=(1,3), min_df=1, max_df=0.98, max_features=600000, strip_accents='unicode', lowercase=True, sublinear_tf=True)\n",
        "c_params=dict(analyzer='char_wb', ngram_range=(3,5), min_df=1, max_features=400000, sublinear_tf=True)\n",
        "\n",
        "def fit_lr(C, cw=None):\n",
        "    return LogisticRegression(solver='liblinear', penalty='l2', C=C, max_iter=4000, random_state=42, class_weight=cw)\n",
        "\n",
        "C_list=[1.0, 2.0, 3.0, 5.0, 8.0, 12.0]\n",
        "cw_list=[None, 'balanced']\n",
        "best=None\n",
        "for cw in cw_list:\n",
        "    for C in C_list:\n",
        "        oof=np.full(n, np.nan, dtype=np.float32)\n",
        "        for vb in [2,3,4,5]:\n",
        "            tr_idx=np.concatenate([blocks[i-1] for i in range(1,vb)])\n",
        "            va_idx=blocks[vb-1]\n",
        "            vw=TfidfVectorizer(**w_params); vc=TfidfVectorizer(**c_params)\n",
        "            Xw_tr = vw.fit_transform(tx_tr.iloc[tr_idx]); Xw_va = vw.transform(tx_tr.iloc[va_idx])\n",
        "            Xc_tr = vc.fit_transform(tx_tr.iloc[tr_idx]); Xc_va = vc.transform(tx_tr.iloc[va_idx])\n",
        "            Xtr_txt = sparse.hstack([Xw_tr, Xc_tr], format='csr')\n",
        "            Xva_txt = sparse.hstack([Xw_va, Xc_va], format='csr')\n",
        "            Xtr = sparse.hstack([Xtr_txt, sparse.csr_matrix(Xmeta_tr[tr_idx])], format='csr')\n",
        "            Xva = sparse.hstack([Xva_txt, sparse.csr_matrix(Xmeta_tr[va_idx])], format='csr')\n",
        "            lr=fit_lr(C, cw); lr.fit(Xtr, y[tr_idx])\n",
        "            oof[va_idx] = lr.predict_proba(Xva)[:,1].astype(np.float32)\n",
        "        # Evaluate on true last validated block (block index 4)\n",
        "        auc_last=roc_auc_score(y[blocks[4]], oof[blocks[4]])\n",
        "        print(f'[Leak-free] cw={cw} C={C} | AUC_last={auc_last:.5f}')\n",
        "        if (best is None) or (auc_last>best[0]): best=(auc_last,(C,cw),oof)\n",
        "print('Best params (leak-free OOF):', best[1], '| AUC_last=', f'{best[0]:.5f}')\n",
        "\n",
        "# Final fit on blocks 1..4 with tuned vectorizers and best params; predict test\n",
        "tr_final=np.concatenate([blocks[i] for i in range(0,4)])\n",
        "vw=TfidfVectorizer(**w_params); vc=TfidfVectorizer(**c_params)\n",
        "Xw_tr_f = vw.fit_transform(tx_tr.iloc[tr_final]); Xw_te_f = vw.transform(tx_te)\n",
        "Xc_tr_f = vc.fit_transform(tx_tr.iloc[tr_final]); Xc_te_f = vc.transform(tx_te)\n",
        "Xtr_txt_f = sparse.hstack([Xw_tr_f, Xc_tr_f], format='csr')\n",
        "Xte_txt_f = sparse.hstack([Xw_te_f, Xc_te_f], format='csr')\n",
        "Xtr_f = sparse.hstack([Xtr_txt_f, sparse.csr_matrix(Xmeta_tr[tr_final])], format='csr')\n",
        "Xte_f = sparse.hstack([Xte_txt_f, sparse.csr_matrix(Xmeta_te)], format='csr')\n",
        "C_best, cw_best = best[1]\n",
        "lr=fit_lr(C_best, cw_best); lr.fit(Xtr_f, y[tr_final])\n",
        "p_te=lr.predict_proba(Xte_f)[:,1].astype(np.float32)\n",
        "\n",
        "# Save artifacts for blending\n",
        "np.save('oof_lr_wordchar_meta_time_tuned.npy', best[2].astype(np.float32))\n",
        "np.save('test_lr_wordchar_meta_time_tuned.npy', p_te)\n",
        "print('Saved tuned LR_STRONG OOF/test | test mean=', float(p_te.mean()), '| total time', f'{time.time()-t0:.1f}s')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Leak-free] cw=None C=1.0 | AUC_last=0.61573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Leak-free] cw=None C=2.0 | AUC_last=0.61589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Leak-free] cw=None C=3.0 | AUC_last=0.61166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Leak-free] cw=None C=5.0 | AUC_last=0.60797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Leak-free] cw=None C=8.0 | AUC_last=0.60963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Leak-free] cw=None C=12.0 | AUC_last=0.61049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Leak-free] cw=balanced C=1.0 | AUC_last=0.61889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Leak-free] cw=balanced C=2.0 | AUC_last=0.61905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Leak-free] cw=balanced C=3.0 | AUC_last=0.61547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Leak-free] cw=balanced C=5.0 | AUC_last=0.61106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Leak-free] cw=balanced C=8.0 | AUC_last=0.61169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Leak-free] cw=balanced C=12.0 | AUC_last=0.60635\nBest params (leak-free OOF): (2.0, 'balanced') | AUC_last= 0.61905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved tuned LR_STRONG OOF/test | test mean= 0.39611732959747314 | total time 258.1s\n"
          ]
        }
      ]
    },
    {
      "id": "f26c8e1a-58d5-45e2-a603-49a0110cee64",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# S51-LASTBLOCK-LR: Fit LR stacker on last block only (base logits), recency hedges, 3-way logit hedge, bias to 0.30/0.32\n",
        "import os, numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "t0=time.time()\n",
        "id_col='request_id'; target_col='requester_received_pizza'\n",
        "train=pd.read_json('train.json'); test=pd.read_json('test.json')\n",
        "y=train[target_col].astype(int).values; ids=test[id_col].values\n",
        "\n",
        "def to_logit(p, eps=1e-6):\n",
        "    p=np.clip(p.astype(np.float64), eps, 1-eps); return np.log(p/(1-p))\n",
        "def sigmoid(z): return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "# Time blocks and last validated block (block 5 = index 4)\n",
        "ts_col='unix_timestamp_of_request' if 'unix_timestamp_of_request' in train.columns else 'unix_timestamp_of_request_utc'\n",
        "order=np.argsort(train[ts_col].values); k=6; blocks=np.array_split(order,k)\n",
        "last_idx = blocks[4]\n",
        "\n",
        "# Bases to use (OOF/test probs). Prefer diverse, pruned set.\n",
        "bases=[\n",
        " ('LR_nosub','oof_lr_time_nosub_meta.npy','test_lr_time_nosub_meta.npy'),\n",
        " ('Dense_v1','oof_xgb_dense_time.npy','test_xgb_dense_time.npy'),\n",
        " ('Meta','oof_xgb_meta_time.npy','test_xgb_meta_time.npy'),\n",
        " ('MiniLM','oof_xgb_emb_meta_time.npy','test_xgb_emb_meta_time.npy'),\n",
        " ('MPNet','oof_xgb_emb_mpnet_time.npy','test_xgb_emb_mpnet_time.npy'),\n",
        " ('CatBoost_v2','oof_catboost_textmeta_v2.npy','test_catboost_textmeta_v2.npy'),\n",
        "]\n",
        "OOF_list=[]; TEST_list=[]; names=[]; auc_rows=[]\n",
        "for nm,oo,tt in bases:\n",
        "    if os.path.exists(oo) and os.path.exists(tt):\n",
        "        o=np.load(oo).astype(np.float64).ravel(); t=np.load(tt).astype(np.float64).ravel()\n",
        "        if len(o)==len(y):\n",
        "            OOF_list.append(to_logit(o)); TEST_list.append(to_logit(t)); names.append(nm)\n",
        "            try:\n",
        "                auc_rows.append((nm, float(roc_auc_score(y[last_idx], o[last_idx]))))\n",
        "            except Exception:\n",
        "                auc_rows.append((nm, np.nan))\n",
        "print('Loaded bases:', names)\n",
        "if len(names) < 3:\n",
        "    raise RuntimeError('Too few bases for last-block LR.')\n",
        "\n",
        "OOF=np.column_stack(OOF_list); TEST=np.column_stack(TEST_list)\n",
        "print('Shapes -> OOF:', OOF.shape, 'TEST:', TEST.shape)\n",
        "\n",
        "# Train LR on last block only (avoid overfitting to earlier distribution).\n",
        "sc=StandardScaler(with_mean=True, with_std=True)\n",
        "Xtr=sc.fit_transform(OOF[last_idx])\n",
        "ytr=y[last_idx]\n",
        "C_grid=[0.3, 1.0, 3.0]\n",
        "best=None\n",
        "for C in C_grid:\n",
        "    lr=LogisticRegression(penalty='l2', solver='lbfgs', C=C, max_iter=2000, fit_intercept=True)\n",
        "    lr.fit(Xtr, ytr)\n",
        "    z=lr.decision_function(Xtr)\n",
        "    auc=roc_auc_score(ytr, z)\n",
        "    print(f'C={C} | AUC_train_last={auc:.5f}')\n",
        "    if (best is None) or (auc>best[0]): best=(auc,C,lr)\n",
        "auc_best, C_best, lr_best = best\n",
        "print('Chosen C:', C_best, '| AUC_train_last=', f'{auc_best:.5f}')\n",
        "\n",
        "Xte=sc.transform(TEST)\n",
        "z_gamma = lr_best.decision_function(Xte)\n",
        "p_gamma = sigmoid(z_gamma).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: p_gamma}).to_csv('submission_s51_lastblock_gamma.csv', index=False)\n",
        "print('Wrote submission_s51_lastblock_gamma.csv | mean', float(p_gamma.mean()))\n",
        "\n",
        "# Recency on embeddings only (MiniLM, MPNet) using recent35/45; small asymmetric alphas\n",
        "def load_recent_avg(files):\n",
        "    arr=[]\n",
        "    for fp in files:\n",
        "        if os.path.exists(fp):\n",
        "            a=np.load(fp).astype(np.float64).ravel(); arr.append(a)\n",
        "    return np.mean(arr,axis=0) if arr else None\n",
        "name_to_j = {n:i for i,n in enumerate(names)}\n",
        "def apply_recency(TEST_base, alphas):\n",
        "    Z=TEST_base.copy()\n",
        "    if 'MiniLM' in name_to_j and alphas.get('MiniLM',0)>0:\n",
        "        pr = load_recent_avg(['test_xgb_emb_meta_time_recent35.npy','test_xgb_emb_meta_time_recent45.npy'])\n",
        "        if pr is not None:\n",
        "            j=name_to_j['MiniLM']; Z[:,j]=(1.0-alphas['MiniLM'])*Z[:,j] + alphas['MiniLM']*to_logit(pr)\n",
        "    if 'MPNet' in name_to_j and alphas.get('MPNet',0)>0:\n",
        "        pr = load_recent_avg(['test_xgb_emb_mpnet_time_recent35.npy','test_xgb_emb_mpnet_time_recent45.npy'])\n",
        "        if pr is not None:\n",
        "            j=name_to_j['MPNet']; Z[:,j]=(1.0-alphas['MPNet'])*Z[:,j] + alphas['MPNet']*to_logit(pr)\n",
        "    # Keep LR_nosub at 0\n",
        "    return Z\n",
        "\n",
        "alphas_low={'MiniLM':0.15,'MPNet':0.20}\n",
        "alphas_high={'MiniLM':0.25,'MPNet':0.30}\n",
        "Xte_low = sc.transform(apply_recency(TEST, alphas_low))\n",
        "Xte_high= sc.transform(apply_recency(TEST, alphas_high))\n",
        "p_low = sigmoid(lr_best.decision_function(Xte_low)).astype(np.float32)\n",
        "p_high= sigmoid(lr_best.decision_function(Xte_high)).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: p_low}).to_csv('submission_s51_lastblock_r_low.csv', index=False)\n",
        "pd.DataFrame({id_col: ids, target_col: p_high}).to_csv('submission_s51_lastblock_r_high.csv', index=False)\n",
        "print('Wrote r_low/r_high means ->', float(p_low.mean()), float(p_high.mean()))\n",
        "\n",
        "# 3-way logit hedge\n",
        "zg, zl, zh = to_logit(p_gamma), to_logit(p_low), to_logit(p_high)\n",
        "p3 = sigmoid((zg+zl+zh)/3.0).astype(np.float32)\n",
        "pd.DataFrame({id_col: ids, target_col: p3}).to_csv('submission_s51_lastblock_hedge3.csv', index=False)\n",
        "print('Wrote submission_s51_lastblock_hedge3.csv | mean', float(p3.mean()))\n",
        "\n",
        "# Bias to 0.30 and 0.32; promote 0.30\n",
        "def bias_to_mean(probs, target, it=80):\n",
        "    z = to_logit(probs); lo,hi=-10.0,10.0\n",
        "    for _ in range(it):\n",
        "        mid=(lo+hi)/2; m=sigmoid(z+mid).mean()\n",
        "        if m<target: lo=mid\n",
        "        else: hi=mid\n",
        "    return (lo+hi)/2\n",
        "for tm in [0.30, 0.32]:\n",
        "    b = bias_to_mean(p3, tm)\n",
        "    pm = sigmoid(to_logit(p3)+b).astype(np.float32)\n",
        "    outp = f'submission_s51_lastblock_hedge3_m{int(tm*100):03d}.csv'\n",
        "    pd.DataFrame({id_col: ids, target_col: pm}).to_csv(outp, index=False)\n",
        "    print(f'Wrote {outp} | mean={pm.mean():.6f}')\n",
        "    if abs(tm-0.30)<1e-9:\n",
        "        pd.DataFrame({id_col: ids, target_col: pm}).to_csv('submission.csv', index=False)\n",
        "        print('PROMOTED: submission.csv <-', outp)\n",
        "\n",
        "print('S51-LASTBLOCK-LR done in', f'{time.time()-t0:.1f}s')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}