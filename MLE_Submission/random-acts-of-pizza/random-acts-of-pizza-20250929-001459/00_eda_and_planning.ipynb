{
  "cells": [
    {
      "id": "71f643df-0954-4119-8aa5-918713d8469e",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Random Acts of Pizza \u2014 Plan\n",
        "\n",
        "Objective: Achieve medal-level AUC-ROC via robust text modeling with disciplined CV and fast iteration.\n",
        "\n",
        "Workflow\n",
        "- Environment sanity: verify GPU; if unavailable, exit early.\n",
        "- Data audit:\n",
        "  - Inspect train.json/test.json: schema, text fields, meta-data, target.\n",
        "  - Identify potential features: request_text, title, gratitude, politeness markers, narrative structure, user history proxies if present (e.g., account age, karma fields), timing features.\n",
        "- Validation:\n",
        "  - Stratified K-fold (e.g., 5 folds) on target; ensure transformations fit within folds.\n",
        "  - Save folds to disk for reuse.\n",
        "  - Track OOF AUC; cache OOF/test logits for blending.\n",
        "- Baselines (fast):\n",
        "  1) TF-IDF (word+char) -> Logistic Regression (liblinear/saga) with class_weight='balanced'.\n",
        "  2) TF-IDF -> Linear SVM (Calibrated) if needed.\n",
        "  3) Naive Bayes-SVM style (NB-SVM) linear model.\n",
        "- Feature Engineering v1:\n",
        "  - Text cleaning: lower, basic normalization; preserve punctuation for char n-grams.\n",
        "  - Separate fields: title vs body; combine with weighted concatenation.\n",
        "  - Meta features: text length, word counts, uppercase ratio, sentiment (VADER), presence of images/links, mention of money/\"student\", location, reciprocity cues, offer-of-return, gratitude terms, politeness markers.\n",
        "  - Temporal if available (weekday/hour).\n",
        "- Models for improvement:\n",
        "  - Linear baseline tuning (C, n-gram ranges, min_df).\n",
        "  - XGBoost/CatBoost on meta + dense text embeddings (e.g., SIF average of word vectors) \u2014 GPU if helpful.\n",
        "  - Light neural option only if quick (e.g., DistilBERT fine-tune with early stopping) but only after strong CV established.\n",
        "- Ensembling:\n",
        "  - Blend diverse linear models (word vs char TF-IDF, different seeds/params).\n",
        "  - Simple weighted average based on OOF.\n",
        "- Error analysis:\n",
        "  - Bucket by confidence, length, presence of key phrases; iterate features.\n",
        "\n",
        "Milestones & Expert Checkpoints\n",
        "1) Plan review (this).\n",
        "2) Data schema + baseline TF-IDF-LR OOF AUC and first submission.\n",
        "3) Feature engineering v1 + tuned linear models.\n",
        "4) Small ensemble of best models.\n",
        "5) Optional transformer fine-tune if time.\n",
        "\n",
        "Next actions\n",
        "- Run environment check (nvidia-smi), load data, inspect columns/target distribution.\n",
        "- Implement deterministic folds and TF-IDF+LR baseline with solid logging and OOF caching.\n",
        "- Use expert review after baseline results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "62c8a269-1b4e-4e2f-b5a8-187e7dcbb327",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, json, time, subprocess, shutil, math, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print('=== GPU CHECK (nvidia-smi) ===', flush=True)\n",
        "try:\n",
        "    out = subprocess.run(['bash','-lc','nvidia-smi || true'], capture_output=True, text=True)\n",
        "    print(out.stdout)\n",
        "except Exception as e:\n",
        "    print('nvidia-smi failed:', e)\n",
        "\n",
        "def load_df(path):\n",
        "    # Try robust JSON loading via pandas; fallback to json.load + DataFrame\n",
        "    try:\n",
        "        df = pd.read_json(path)\n",
        "        return df\n",
        "    except ValueError:\n",
        "        with open(path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "t0 = time.time()\n",
        "train_path = 'train.json'\n",
        "test_path = 'test.json'\n",
        "print('Loading train/test...', flush=True)\n",
        "train = load_df(train_path)\n",
        "test = load_df(test_path)\n",
        "print(f'train shape: {train.shape}; test shape: {test.shape}', flush=True)\n",
        "\n",
        "# Inspect columns and infer key fields\n",
        "print('\\nTrain columns:', list(train.columns))\n",
        "print('Test  columns:', list(test.columns))\n",
        "\n",
        "id_col_candidates = [c for c in train.columns if c.lower() in ('request_id','id')]\n",
        "target_candidates = [c for c in train.columns if c.lower() in ('requester_received_pizza','target','label','outcome')]\n",
        "text_candidates = [c for c in train.columns if 'text' in c.lower() or 'title' in c.lower()]\n",
        "time_candidates = [c for c in train.columns if 'time' in c.lower() or 'created' in c.lower() or 'timestamp' in c.lower()]\n",
        "\n",
        "print('\\nID candidates:', id_col_candidates)\n",
        "print('Target candidates:', target_candidates)\n",
        "print('Text candidates:', text_candidates[:10])\n",
        "print('Time candidates:', time_candidates[:10])\n",
        "\n",
        "id_col = id_col_candidates[0] if id_col_candidates else None\n",
        "target_col = target_candidates[0] if target_candidates else None\n",
        "print(f'Chosen id_col={id_col}, target_col={target_col}')\n",
        "\n",
        "if target_col is None:\n",
        "    # Fallback guess for RAOP\n",
        "    if 'requester_received_pizza' in train.columns:\n",
        "        target_col = 'requester_received_pizza'\n",
        "        print('Fallback target_col=requester_received_pizza')\n",
        "\n",
        "if id_col is None:\n",
        "    if 'request_id' in train.columns:\n",
        "        id_col = 'request_id'\n",
        "        print('Fallback id_col=request_id')\n",
        "\n",
        "# Basic target distribution\n",
        "if target_col in train.columns:\n",
        "    y = train[target_col].astype(int) if train[target_col].dtype != bool else train[target_col].astype(int)\n",
        "    pos_rate = y.mean()\n",
        "    print(f'Target positive rate: {pos_rate:.4f} ({y.sum()}/{len(y)})')\n",
        "\n",
        "# Prefer edit-aware text if present\n",
        "body_fields_order = [\n",
        "    'request_text_edit_aware',\n",
        "    'request_text',\n",
        "    'request_text_edit_aware_unnormalized',\n",
        "]\n",
        "title_fields = [\n",
        "    'request_title', 'title'\n",
        "]\n",
        "body_col = next((c for c in body_fields_order if c in train.columns), None)\n",
        "title_col = next((c for c in title_fields if c in train.columns), None)\n",
        "print(f'Selected title_col={title_col}, body_col={body_col}')\n",
        "\n",
        "# Quick sanity on missingness\n",
        "if title_col:\n",
        "    print('Title nulls:', train[title_col].isna().sum(), '/', len(train))\n",
        "if body_col:\n",
        "    print('Body nulls:', train[body_col].isna().sum(), '/', len(train))\n",
        "\n",
        "# Verify sample submission format expectations\n",
        "if os.path.exists('sampleSubmission.csv'):\n",
        "    ss = pd.read_csv('sampleSubmission.csv')\n",
        "    print('\\nSampleSubmission head:')\n",
        "    print(ss.head())\n",
        "    print('SampleSubmission columns:', list(ss.columns))\n",
        "\n",
        "print('\\nEnvironment/data audit complete in %.2fs' % (time.time()-t0), flush=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== GPU CHECK (nvidia-smi) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Sep 29 00:22:15 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.06             Driver Version: 550.144.06     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10-24Q                 On  |   00000002:00:00.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |     182MiB /  24512MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\nLoading train/test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train shape: (2878, 32); test shape: (1162, 17)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nTrain columns: ['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\nTest  columns: ['giver_username_if_known', 'request_id', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_days_since_first_post_on_raop_at_request', 'requester_number_of_comments_at_request', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_posts_at_request', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_subreddits_at_request', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_request', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\n\nID candidates: ['request_id']\nTarget candidates: ['requester_received_pizza']\nText candidates: ['request_text', 'request_text_edit_aware', 'request_title']\nTime candidates: ['unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\nChosen id_col=request_id, target_col=requester_received_pizza\nTarget positive rate: 0.2484 (715/2878)\nSelected title_col=request_title, body_col=request_text_edit_aware\nTitle nulls: 0 / 2878\nBody nulls: 0 / 2878\n\nSampleSubmission head:\n  request_id  requester_received_pizza\n0  t3_1aw5zf                         0\n1   t3_roiuw                         0\n2   t3_mjnbq                         0\n3   t3_t8wd1                         0\n4  t3_1m4zxu                         0\nSampleSubmission columns: ['request_id', 'requester_received_pizza']\n\nEnvironment/data audit complete in 0.08s\n"
          ]
        }
      ]
    },
    {
      "id": "c07200f7-51e0-4660-b0d0-046e9c1b4eec",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "from typing import Tuple\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Build full_text field (lowercase, keep punctuation)\n",
        "def build_text(df: pd.DataFrame, title_col: str, body_col: str) -> pd.Series:\n",
        "    title = df[title_col].fillna(\"\").astype(str)\n",
        "    body = df[body_col].fillna(\"\").astype(str)\n",
        "    full_text = (title + \" [SEP] \" + body).str.lower()\n",
        "    return full_text\n",
        "\n",
        "full_text_train = build_text(train, title_col, body_col)\n",
        "full_text_test = build_text(test, title_col, body_col)\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "def get_vectorizers() -> Tuple[TfidfVectorizer, TfidfVectorizer]:\n",
        "    word_vec = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.9, max_features=50000,\n",
        "                               sublinear_tf=True, dtype=np.float32, lowercase=False)\n",
        "    char_vec = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,5), min_df=2, max_features=200000,\n",
        "                               sublinear_tf=True, dtype=np.float32, lowercase=False)\n",
        "    return word_vec, char_vec\n",
        "\n",
        "def hstack_features(word_X, char_X):\n",
        "    return sparse.hstack([word_X, char_X], format='csr')\n",
        "\n",
        "def fit_lr(X, y):\n",
        "    # saga handles large sparse matrices; increase max_iter for convergence\n",
        "    return LogisticRegression(penalty='l2', C=4.0, solver='saga', max_iter=4000, n_jobs=-1, verbose=0)\n",
        "\n",
        "print('=== CV: TF-IDF(word+char) -> Logistic Regression ===', flush=True)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof = np.zeros(len(train), dtype=np.float32)\n",
        "fold_times = []\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(full_text_train, y), 1):\n",
        "    t0 = time.time()\n",
        "    print(f'Fold {fold} start: train {len(trn_idx)} | val {len(val_idx)}', flush=True)\n",
        "    X_tr_text = full_text_train.iloc[trn_idx]\n",
        "    X_va_text = full_text_train.iloc[val_idx]\n",
        "\n",
        "    word_vec, char_vec = get_vectorizers()\n",
        "    Xw_tr = word_vec.fit_transform(X_tr_text)\n",
        "    Xc_tr = char_vec.fit_transform(X_tr_text)\n",
        "    X_tr = hstack_features(Xw_tr, Xc_tr)\n",
        "\n",
        "    Xw_va = word_vec.transform(X_va_text)\n",
        "    Xc_va = char_vec.transform(X_va_text)\n",
        "    X_va = hstack_features(Xw_va, Xc_va)\n",
        "\n",
        "    clf = fit_lr(X_tr, y[trn_idx])\n",
        "    clf.fit(X_tr, y[trn_idx])\n",
        "    oof[val_idx] = clf.predict_proba(X_va)[:,1]\n",
        "    auc = roc_auc_score(y[val_idx], oof[val_idx])\n",
        "    dt = time.time()-t0\n",
        "    fold_times.append(dt)\n",
        "    print(f'Fold {fold} AUC: {auc:.5f} | time: {dt:.1f}s', flush=True)\n",
        "\n",
        "oof_auc = roc_auc_score(y, oof)\n",
        "print(f'OOF AUC: {oof_auc:.5f} | mean fold time: {np.mean(fold_times):.1f}s', flush=True)\n",
        "\n",
        "# Fit final model on full train (refit vectorizers on full train text), then predict test\n",
        "print('Fitting full model on all training data...', flush=True)\n",
        "word_vec_full, char_vec_full = get_vectorizers()\n",
        "Xw_full = word_vec_full.fit_transform(full_text_train)\n",
        "Xc_full = char_vec_full.fit_transform(full_text_train)\n",
        "X_full = hstack_features(Xw_full, Xc_full)\n",
        "clf_full = fit_lr(X_full, y)\n",
        "clf_full.fit(X_full, y)\n",
        "\n",
        "Xw_test = word_vec_full.transform(full_text_test)\n",
        "Xc_test = char_vec_full.transform(full_text_test)\n",
        "X_test = hstack_features(Xw_test, Xc_test)\n",
        "test_pred = clf_full.predict_proba(X_test)[:,1].astype(np.float32)\n",
        "\n",
        "# Write submission\n",
        "sub = pd.DataFrame({\n",
        "    'request_id': test['request_id'],\n",
        "    'requester_received_pizza': test_pred\n",
        "})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv:', sub.shape, 'min/max:', float(test_pred.min()), float(test_pred.max()))\n",
        "\n",
        "# Cache OOF/test for future blending\n",
        "np.save('oof_lr_wordchar.npy', oof)\n",
        "np.save('test_lr_wordchar.npy', test_pred)\n",
        "print('Cached OOF and test predictions to .npy files.')\n",
        "\n",
        "# Quick sanity checks\n",
        "assert sub.shape[0] == len(test), 'Submission row count mismatch'\n",
        "assert 0.0 <= test_pred.min() and test_pred.max() <= 1.0, 'Predictions out of [0,1]'\n",
        "print('Baseline complete.')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CV: TF-IDF(word+char) -> Logistic Regression ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 start: train 2302 | val 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 AUC: 0.61902 | time: 9.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 start: train 2302 | val 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 AUC: 0.64731 | time: 8.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 start: train 2302 | val 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 AUC: 0.62645 | time: 9.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 start: train 2303 | val 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 AUC: 0.59316 | time: 9.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 start: train 2303 | val 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 AUC: 0.61375 | time: 8.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF AUC: 0.62018 | mean fold time: 8.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting full model on all training data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv: (1162, 2) min/max: 0.008493145927786827 0.8981638550758362\nCached OOF and test predictions to .npy files.\nBaseline complete.\n"
          ]
        }
      ]
    },
    {
      "id": "4bead785-5179-4285-bbe5-888677e2c1f4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Improved TF-IDF + LR baseline per expert advice\n",
        "import time\n",
        "from typing import Tuple\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def build_text_upweighted(df: pd.DataFrame, title_col: str, body_col: str) -> pd.Series:\n",
        "    title = df[title_col].fillna(\"\").astype(str)\n",
        "    body = df[body_col].fillna(\"\").astype(str)\n",
        "    # Duplicate title once to upweight it; no manual lowercasing (vectorizers will lowercase)\n",
        "    return title + \" \" + title + \" [SEP] \" + body\n",
        "\n",
        "full_text_train2 = build_text_upweighted(train, title_col, body_col)\n",
        "full_text_test2 = build_text_upweighted(test, title_col, body_col)\n",
        "y2 = train[target_col].astype(int).values\n",
        "\n",
        "def get_vecs_improved() -> Tuple[TfidfVectorizer, TfidfVectorizer]:\n",
        "    word_vec = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.95, max_features=150000,\n",
        "                               sublinear_tf=True, lowercase=True, dtype=np.float32)\n",
        "    char_vec = TfidfVectorizer(analyzer='char', ngram_range=(3,6), min_df=2, max_features=300000,\n",
        "                               sublinear_tf=True, lowercase=True, dtype=np.float32)\n",
        "    return word_vec, char_vec\n",
        "\n",
        "def hstack_features(word_X, char_X):\n",
        "    return sparse.hstack([word_X, char_X], format='csr')\n",
        "\n",
        "def run_cv_lr(C: float, cls_weight=None):\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    oof = np.zeros(len(full_text_train2), dtype=np.float32)\n",
        "    times = []\n",
        "    for fold, (trn_idx, val_idx) in enumerate(skf.split(full_text_train2, y2), 1):\n",
        "        t0 = time.time()\n",
        "        Xtr_text = full_text_train2.iloc[trn_idx]\n",
        "        Xva_text = full_text_train2.iloc[val_idx]\n",
        "        wv, cv = get_vecs_improved()\n",
        "        Xw_tr = wv.fit_transform(Xtr_text)\n",
        "        Xc_tr = cv.fit_transform(Xtr_text)\n",
        "        X_tr = hstack_features(Xw_tr, Xc_tr)\n",
        "        Xw_va = wv.transform(Xva_text)\n",
        "        Xc_va = cv.transform(Xva_text)\n",
        "        X_va = hstack_features(Xw_va, Xc_va)\n",
        "        solver = 'saga'\n",
        "        clf = LogisticRegression(penalty='l2', C=C, solver=solver, max_iter=3000, n_jobs=-1, class_weight=cls_weight, verbose=0)\n",
        "        clf.fit(X_tr, y2[trn_idx])\n",
        "        oof[val_idx] = clf.predict_proba(X_va)[:,1]\n",
        "        times.append(time.time()-t0)\n",
        "        print(f'  Fold {fold} done in {times[-1]:.1f}s', flush=True)\n",
        "    auc = roc_auc_score(y2, oof)\n",
        "    return auc, oof\n",
        "\n",
        "print('=== Improved LR baseline grid ===', flush=True)\n",
        "best = (-1, None, None)  # (auc, C, class_weight)\n",
        "best_oof = None\n",
        "for C in (2.0, 4.0, 8.0):\n",
        "    for cw in (None, 'balanced'):\n",
        "        print(f'Trying C={C}, class_weight={cw}', flush=True)\n",
        "        auc, oof_preds = run_cv_lr(C, cw)\n",
        "        print(f'  OOF AUC: {auc:.5f}', flush=True)\n",
        "        if auc > best[0]:\n",
        "            best = (auc, C, cw)\n",
        "            best_oof = oof_preds\n",
        "\n",
        "print(f'Best OOF AUC: {best[0]:.5f} with C={best[1]} class_weight={best[2]}', flush=True)\n",
        "np.save('oof_lr_improved.npy', best_oof)\n",
        "\n",
        "# Fit final model on full training with best params\n",
        "wv_full, cv_full = get_vecs_improved()\n",
        "Xw_full = wv_full.fit_transform(full_text_train2)\n",
        "Xc_full = cv_full.fit_transform(full_text_train2)\n",
        "X_full = hstack_features(Xw_full, Xc_full)\n",
        "solver = 'saga'\n",
        "clf = LogisticRegression(penalty='l2', C=best[1], solver=solver, max_iter=3000, n_jobs=-1, class_weight=best[2], verbose=0)\n",
        "clf.fit(X_full, y2)\n",
        "Xw_test = wv_full.transform(full_text_test2)\n",
        "Xc_test = cv_full.transform(full_text_test2)\n",
        "X_test = hstack_features(Xw_test, Xc_test)\n",
        "test_pred2 = clf.predict_proba(X_test)[:,1].astype(np.float32)\n",
        "np.save('test_lr_improved.npy', test_pred2)\n",
        "\n",
        "sub2 = pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_pred2})\n",
        "sub2.to_csv('submission_lr_improved.csv', index=False)\n",
        "print('Saved submission_lr_improved.csv', sub2.shape, 'min/max:', float(test_pred2.min()), float(test_pred2.max()))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Improved LR baseline grid ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying C=2.0, class_weight=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1 done in 18.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2 done in 17.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3 done in 18.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4 done in 18.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 5 done in 17.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  OOF AUC: 0.63452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying C=2.0, class_weight=balanced\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1 done in 15.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2 done in 14.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3 done in 15.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4 done in 14.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 5 done in 15.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  OOF AUC: 0.63389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying C=4.0, class_weight=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1 done in 20.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2 done in 20.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3 done in 21.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4 done in 21.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 5 done in 20.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  OOF AUC: 0.62974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying C=4.0, class_weight=balanced\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1 done in 18.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2 done in 18.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3 done in 16.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4 done in 20.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 5 done in 16.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  OOF AUC: 0.62920\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying C=8.0, class_weight=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1 done in 23.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2 done in 24.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3 done in 25.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4 done in 25.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 5 done in 24.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  OOF AUC: 0.62554\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying C=8.0, class_weight=balanced\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1 done in 20.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2 done in 22.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3 done in 22.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4 done in 23.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 5 done in 21.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  OOF AUC: 0.62519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best OOF AUC: 0.63452 with C=2.0 class_weight=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_lr_improved.csv (1162, 2) min/max: 0.02404523827135563 0.7660126686096191\n"
          ]
        }
      ]
    },
    {
      "id": "07d3a6fe-c643-424a-9a78-83b0e75b89d9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# NB-SVM (word 1-2) + OOF-weighted blend with improved LR\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def nbs_log_count_ratio(X, ybin, alpha=0.25):\n",
        "    # ybin: 1 for positive, 0 for negative\n",
        "    pos = X[ybin == 1].sum(axis=0) + alpha\n",
        "    neg = X[ybin == 0].sum(axis=0) + alpha\n",
        "    # Normalize to probabilities\n",
        "    pos = np.asarray(pos).ravel()\n",
        "    neg = np.asarray(neg).ravel()\n",
        "    pos = pos / pos.sum()\n",
        "    neg = neg / neg.sum()\n",
        "    r = np.log(pos) - np.log(neg)\n",
        "    return r\n",
        "\n",
        "def nbs_transform(X, r):\n",
        "    return X.multiply(r)\n",
        "\n",
        "def run_nbsvm_cv(text_series, y, alpha=0.25, C=4.0, min_df=2, max_features=100000):\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    oof = np.zeros(len(text_series), dtype=np.float32)\n",
        "    times = []\n",
        "    for fold, (trn_idx, val_idx) in enumerate(skf.split(text_series, y), 1):\n",
        "        t0 = time.time()\n",
        "        Xtr_text = text_series.iloc[trn_idx]\n",
        "        Xva_text = text_series.iloc[val_idx]\n",
        "        ytr = y[trn_idx]\n",
        "        vec = CountVectorizer(ngram_range=(1,2), min_df=min_df, max_features=max_features, lowercase=True, dtype=np.float32)\n",
        "        X_tr = vec.fit_transform(Xtr_text)\n",
        "        X_va = vec.transform(Xva_text)\n",
        "        r = nbs_log_count_ratio(X_tr, ytr, alpha=alpha)\n",
        "        X_tr_nb = nbs_transform(X_tr, r)\n",
        "        X_va_nb = nbs_transform(X_va, r)\n",
        "        clf = LogisticRegression(C=C, solver='liblinear', max_iter=2000)\n",
        "        clf.fit(X_tr_nb, ytr)\n",
        "        oof[val_idx] = clf.predict_proba(X_va_nb)[:,1]\n",
        "        dt = time.time() - t0\n",
        "        times.append(dt)\n",
        "        print(f'  NB-SVM fold {fold} done in {dt:.1f}s', flush=True)\n",
        "    auc = roc_auc_score(y, oof)\n",
        "    return auc, oof, vec, r, clf\n",
        "\n",
        "print('=== NB-SVM (word 1-2) CV ===', flush=True)\n",
        "ybin = train[target_col].astype(int).values\n",
        "nb_auc, nb_oof, nb_vec, nb_r, nb_clf = run_nbsvm_cv(full_text_train2, ybin, alpha=0.25, C=4.0, min_df=2, max_features=100000)\n",
        "print(f'NB-SVM OOF AUC: {nb_auc:.5f}', flush=True)\n",
        "np.save('oof_nbsvm.npy', nb_oof)\n",
        "\n",
        "# Fit NB-SVM on full train and predict test\n",
        "X_full_nb = nb_vec.fit_transform(full_text_train2)\n",
        "nb_r_full = nbs_log_count_ratio(X_full_nb, ybin, alpha=0.25)\n",
        "X_full_nb_tr = nbs_transform(X_full_nb, nb_r_full)\n",
        "nb_full_clf = LogisticRegression(C=4.0, solver='liblinear', max_iter=2000)\n",
        "nb_full_clf.fit(X_full_nb_tr, ybin)\n",
        "X_test_nb = nb_vec.transform(full_text_test2)\n",
        "X_test_nb_tr = nbs_transform(X_test_nb, nb_r_full)\n",
        "test_nb = nb_full_clf.predict_proba(X_test_nb_tr)[:,1].astype(np.float32)\n",
        "np.save('test_nbsvm.npy', test_nb)\n",
        "\n",
        "# Blend NB-SVM with improved LR using OOF to choose weight\n",
        "lr_oof = np.load('oof_lr_improved.npy')\n",
        "weights = np.linspace(0.0, 1.0, 21)\n",
        "best_w, best_auc = None, -1.0\n",
        "for w in weights:\n",
        "    blend = w*lr_oof + (1.0-w)*nb_oof\n",
        "    auc = roc_auc_score(ybin, blend)\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_w = auc, w\n",
        "print(f'Best text-blend OOF AUC: {best_auc:.5f} at w(LR)={best_w:.2f}', flush=True)\n",
        "\n",
        "test_lr = np.load('test_lr_improved.npy')\n",
        "test_blend = best_w*test_lr + (1.0-best_w)*test_nb\n",
        "sub_blend = pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend})\n",
        "sub_blend.to_csv('submission_text_blend.csv', index=False)\n",
        "print('Saved submission_text_blend.csv', sub_blend.shape, 'min/max:', float(test_blend.min()), float(test_blend.max()))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== NB-SVM (word 1-2) CV ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NB-SVM fold 1 done in 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NB-SVM fold 2 done in 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NB-SVM fold 3 done in 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NB-SVM fold 4 done in 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NB-SVM fold 5 done in 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM OOF AUC: 0.56533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best text-blend OOF AUC: 0.63452 at w(LR)=1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_text_blend.csv (1162, 2) min/max: 0.02404523827135563 0.7660126686096191\n"
          ]
        }
      ]
    },
    {
      "id": "36641ffd-0b1d-44e7-8ea6-9b09172b6fd8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Meta + text features (leakage-safe) with LR + RAOP lexicons\n",
        "import re, time\n",
        "from datetime import datetime, timezone\n",
        "from scipy import sparse\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def count_urls(s: str) -> int:\n",
        "    return len(re.findall(r'https?://\\S+', s))\n",
        "\n",
        "def has_imgur(s: str) -> int:\n",
        "    return 1 if re.search(r'imgur\\.com', s, flags=re.IGNORECASE) else 0\n",
        "\n",
        "def count_digits(s: str) -> int:\n",
        "    return sum(ch.isdigit() for ch in s)\n",
        "\n",
        "def dollar_flag(s: str) -> int:\n",
        "    return 1 if ('$' in s) or re.search(r'\\b\\d+\\s*(dollars|bucks)\\b', s, flags=re.IGNORECASE) else 0\n",
        "\n",
        "def caps_ratio(s: str) -> float:\n",
        "    if not s:\n",
        "        return 0.0\n",
        "    letters = [c for c in s if c.isalpha()]\n",
        "    if not letters:\n",
        "        return 0.0\n",
        "    return sum(c.isupper() for c in letters) / max(1, len(letters))\n",
        "\n",
        "def word_count(s: str) -> int:\n",
        "    return len(s.split()) if s else 0\n",
        "\n",
        "def exclam_count(s: str) -> int:\n",
        "    return s.count('!') if s else 0\n",
        "\n",
        "def question_count(s: str) -> int:\n",
        "    return s.count('?') if s else 0\n",
        "\n",
        "def parse_subreddit_count(x) -> int:\n",
        "    # requester_subreddits_at_request is a list; fall back to 0 otherwise\n",
        "    if isinstance(x, list):\n",
        "        return len(x)\n",
        "    return 0\n",
        "\n",
        "def safe_log1p_signed(x):\n",
        "    # signed log1p for values that can be negative\n",
        "    return np.sign(x) * np.log1p(np.abs(x))\n",
        "\n",
        "LEX_PATTERNS = {\n",
        "    # Politeness / gratitude\n",
        "    'lex_please': r'\\bplease\\b',\n",
        "    'lex_thanks': r'\\b(thank you|thanks in advance|thanks|thank|tia)\\b',\n",
        "    'lex_appreciate': r'\\b(appreciate|appreciated)\\b',\n",
        "    # Reciprocity / willingness / repay\n",
        "    'lex_pay_it_forward': r'\\b(pay it forward|return the favor)\\b',\n",
        "    'lex_repay': r'\\b(repay|pay you back|pay back)\\b',\n",
        "    'lex_willing': r'\\b(willing to|i\\'ll|i will|i can)\\b',\n",
        "    'lex_karma': r'\\bkarma\\b',\n",
        "    # Evidence / credibility\n",
        "    'lex_evidence': r'\\b(proof|receipt|photo|picture|pic|verify|verification|evidence)\\b',\n",
        "    'lex_imgur_word': r'\\bimgur\\b',\n",
        "    # Hardship / need\n",
        "    'lex_student': r'\\b(student|college|university|finals|exam|midterm)\\b',\n",
        "    'lex_jobloss': r'\\b(unemployed|laid off|lost my job|between jobs|job hunt)\\b',\n",
        "    'lex_broke': r'\\b(broke)\\b',\n",
        "    'lex_rent_bills': r'\\b(rent|bill|bills|utilities|electric|gas|water|paycheck)\\b',\n",
        "    'lex_family': r'\\b(family|kids?|children|baby|pregnant|son|daughter|wife|husband)\\b',\n",
        "    'lex_hungry': r'\\b(hungry|starving|no food)\\b|\\bfood (stamps|pantry)\\b',\n",
        "    'lex_struggling': r'\\b(desperate|struggling)\\b',\n",
        "    # Urgency / help\n",
        "    'lex_urgency': r'\\b(tonight|today|tomorrow|asap|urgent)\\b',\n",
        "    'lex_help': r'\\bhelp\\b',\n",
        "}\n",
        "\n",
        "def add_lexicons(df: pd.DataFrame, title_col: str, body_col: str) -> pd.DataFrame:\n",
        "    txt = (df[title_col].fillna('').astype(str) + ' ' + df[body_col].fillna('').astype(str)).str.lower()\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    for name, pat in LEX_PATTERNS.items():\n",
        "        out[name] = txt.str.count(pat, flags=re.IGNORECASE).astype(float)\n",
        "    return out\n",
        "\n",
        "def build_meta(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    title = df[title_col].fillna('').astype(str)\n",
        "    body = df[body_col].fillna('').astype(str)\n",
        "    ts = pd.to_numeric(df['unix_timestamp_of_request'], errors='coerce')\n",
        "    dt = pd.to_datetime(ts, unit='s', utc=True)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    # user/account features (at_request only)\n",
        "    keep_cols = [\n",
        "        'requester_account_age_in_days_at_request',\n",
        "        'requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request',\n",
        "        'requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request',\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "    ]\n",
        "    for c in keep_cols:\n",
        "        if c in df.columns:\n",
        "            out[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "        else:\n",
        "            out[c] = np.nan\n",
        "    # subreddit count\n",
        "    if 'requester_subreddits_at_request' in df.columns:\n",
        "        out['requester_subreddits_count'] = df['requester_subreddits_at_request'].apply(parse_subreddit_count).astype(float)\n",
        "    else:\n",
        "        out['requester_subreddits_count'] = np.nan\n",
        "    # temporal\n",
        "    out['hour'] = dt.dt.hour.astype(float)\n",
        "    out['weekday'] = dt.dt.weekday.astype(float)\n",
        "    out['is_weekend'] = dt.dt.weekday.isin([5,6]).astype(float)\n",
        "    # text stats\n",
        "    out['title_len_chars'] = title.str.len().astype(float)\n",
        "    out['title_len_words'] = title.apply(word_count).astype(float)\n",
        "    out['body_len_chars'] = body.str.len().astype(float)\n",
        "    out['body_len_words'] = body.apply(word_count).astype(float)\n",
        "    out['url_count'] = body.apply(count_urls).astype(float)\n",
        "    out['has_imgur'] = body.apply(has_imgur).astype(float)\n",
        "    out['digits_count'] = body.apply(count_digits).astype(float)\n",
        "    out['dollar_flag'] = body.apply(dollar_flag).astype(float)\n",
        "    out['caps_ratio'] = body.apply(caps_ratio).astype(float)\n",
        "    out['exclam_count'] = body.apply(exclam_count).astype(float)\n",
        "    out['question_count'] = body.apply(question_count).astype(float)\n",
        "    # lexicon counts\n",
        "    lex = add_lexicons(df, title_col, body_col)\n",
        "    out = pd.concat([out, lex], axis=1)\n",
        "    # Fill NaNs and infs\n",
        "    out = out.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "    # Apply log transforms safely: log1p for nonnegative heavy-tailed, signed log1p for possible negatives\n",
        "    nonneg_cols = [\n",
        "        'requester_account_age_in_days_at_request',\n",
        "        'requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request',\n",
        "        'requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_subreddits_count',\n",
        "        'hour','weekday','is_weekend',\n",
        "        'title_len_chars','title_len_words','body_len_chars','body_len_words',\n",
        "        'url_count','has_imgur','digits_count','dollar_flag','exclam_count','question_count',\n",
        "        # lexicon features (all counts, nonnegative)\n",
        "        *list(LEX_PATTERNS.keys()),\n",
        "    ]\n",
        "    for c in nonneg_cols:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(np.clip(out[c], a_min=0.0, a_max=None))\n",
        "    # Signed log for karma difference (can be negative)\n",
        "    if 'requester_upvotes_minus_downvotes_at_request' in out.columns:\n",
        "        out['requester_upvotes_minus_downvotes_at_request'] = safe_log1p_signed(out['requester_upvotes_minus_downvotes_at_request'].values)\n",
        "    # caps_ratio already in [0,1]; keep as-is\n",
        "    out = out.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
        "    return out\n",
        "\n",
        "meta_train = build_meta(train)\n",
        "meta_test = build_meta(test)\n",
        "print('Meta shapes:', meta_train.shape, meta_test.shape, flush=True)\n",
        "\n",
        "# Vectorizers for text (reuse improved settings) - smaller to speed up if needed\n",
        "def get_vecs_meta():\n",
        "    wv = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.95, max_features=100000,\n",
        "                         sublinear_tf=True, lowercase=True, dtype=np.float32)\n",
        "    cv = TfidfVectorizer(analyzer='char', ngram_range=(3,6), min_df=2, max_features=200000,\n",
        "                         sublinear_tf=True, lowercase=True, dtype=np.float32)\n",
        "    return wv, cv\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_meta = np.zeros(len(train), dtype=np.float32)\n",
        "\n",
        "def hstack3(X1, X2, X3):\n",
        "    return sparse.hstack([X1, X2, X3], format='csr')\n",
        "\n",
        "print('=== CV: Text (word+char) + Scaled Meta(+lexicons) -> LR (C=2, cw=balanced) ===', flush=True)\n",
        "# Per expert advice, rerun only the best config: C=2.0, class_weight='balanced' to save time\n",
        "oof_tmp = np.zeros(len(train), dtype=np.float32)\n",
        "times = []\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(full_text_train2, y), 1):\n",
        "    t0 = time.time()\n",
        "    Xtr_text = full_text_train2.iloc[trn_idx]\n",
        "    Xva_text = full_text_train2.iloc[val_idx]\n",
        "    wv, cv = get_vecs_meta()\n",
        "    Xw_tr = wv.fit_transform(Xtr_text)\n",
        "    Xc_tr = cv.fit_transform(Xtr_text)\n",
        "    Xw_va = wv.transform(Xva_text)\n",
        "    Xc_va = cv.transform(Xva_text)\n",
        "    # Scale meta within fold\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    Xm_tr = scaler.fit_transform(meta_train.iloc[trn_idx])\n",
        "    Xm_va = scaler.transform(meta_train.iloc[val_idx])\n",
        "    X_tr = hstack3(Xw_tr, Xc_tr, Xm_tr)\n",
        "    X_va = hstack3(Xw_va, Xc_va, Xm_va)\n",
        "    clf = LogisticRegression(penalty='l2', C=2.0, solver='saga', max_iter=3000, n_jobs=-1, class_weight='balanced', verbose=0)\n",
        "    clf.fit(X_tr, y[trn_idx])\n",
        "    oof_tmp[val_idx] = clf.predict_proba(X_va)[:,1]\n",
        "    times.append(time.time()-t0)\n",
        "    print(f'  fold {fold} in {times[-1]:.1f}s', flush=True)\n",
        "auc = roc_auc_score(y, oof_tmp)\n",
        "print(f'OOF AUC (meta+text+lexicons): {auc:.5f}', flush=True)\n",
        "oof_meta = oof_tmp.copy()\n",
        "np.save('oof_lr_text_meta.npy', oof_meta)\n",
        "\n",
        "# Fit final with best params and predict test\n",
        "wv_full, cv_full = get_vecs_meta()\n",
        "Xw_full = wv_full.fit_transform(full_text_train2)\n",
        "Xc_full = cv_full.fit_transform(full_text_train2)\n",
        "scaler_full = StandardScaler(with_mean=False)\n",
        "Xm_full = scaler_full.fit_transform(meta_train)\n",
        "X_full = hstack3(Xw_full, Xc_full, Xm_full)\n",
        "clf_full = LogisticRegression(penalty='l2', C=2.0, solver='saga', max_iter=3000, n_jobs=-1, class_weight='balanced', verbose=0)\n",
        "clf_full.fit(X_full, y)\n",
        "Xw_test = wv_full.transform(full_text_test2)\n",
        "Xc_test = cv_full.transform(full_text_test2)\n",
        "Xm_test = scaler_full.transform(meta_test)\n",
        "X_test = hstack3(Xw_test, Xc_test, Xm_test)\n",
        "test_pred_meta = clf_full.predict_proba(X_test)[:,1].astype(np.float32)\n",
        "np.save('test_lr_text_meta.npy', test_pred_meta)\n",
        "sub_meta = pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_pred_meta})\n",
        "sub_meta.to_csv('submission_text_meta.csv', index=False)\n",
        "print('Saved submission_text_meta.csv', sub_meta.shape, 'min/max:', float(test_pred_meta.min()), float(test_pred_meta.max()))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta shapes: (2878, 42) (1162, 42)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CV: Text (word+char) + Scaled Meta(+lexicons) -> LR (C=2, cw=balanced) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 1 in 54.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 2 in 62.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 3 in 59.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 4 in 55.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 5 in 58.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF AUC (meta+text+lexicons): 0.68297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_text_meta.csv (1162, 2) min/max: 0.024701131507754326 0.970310389995575\n"
          ]
        }
      ]
    },
    {
      "id": "4d816b49-2580-461d-8916-931ee0fe0820",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Blend meta+text LR with improved text-only LR using OOF to pick weight, then write submission.csv\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "oof_meta = np.load('oof_lr_text_meta.npy')\n",
        "oof_lr = np.load('oof_lr_improved.npy')\n",
        "y_blend = train[target_col].astype(int).values\n",
        "\n",
        "weights = np.linspace(0.0, 1.0, 41)  # finer grid\n",
        "best_w, best_auc = None, -1.0\n",
        "for w in weights:\n",
        "    blend = w*oof_meta + (1.0-w)*oof_lr\n",
        "    auc = roc_auc_score(y_blend, blend)\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_w = auc, w\n",
        "print(f'Best OOF AUC blend(meta,text-only)={best_auc:.5f} at w(meta)={best_w:.2f}', flush=True)\n",
        "\n",
        "test_meta = np.load('test_lr_text_meta.npy')\n",
        "test_lr = np.load('test_lr_improved.npy')\n",
        "test_blend = best_w*test_meta + (1.0-best_w)*test_lr\n",
        "\n",
        "sub = pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend.astype(np.float32)})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv from blended model:', sub.shape, 'min/max:', float(test_blend.min()), float(test_blend.max()))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best OOF AUC blend(meta,text-only)=0.68381 at w(meta)=1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv from blended model: (1162, 2) min/max: 0.032168105244636536 0.9459234476089478\n"
          ]
        }
      ]
    },
    {
      "id": "09305fbf-ff9b-43a3-9730-2f9a339f7f07",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fix NB-SVM normalization, then train a small meta stacker (meta+lexicons+OOFs) and blend\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "def nbsvm_oof_fixed(text_series, y, alpha=1.0, C=2.0, min_df=2, max_features=120000):\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    oof = np.zeros(len(text_series), dtype=np.float32)\n",
        "    for fold, (trn_idx, val_idx) in enumerate(skf.split(text_series, y), 1):\n",
        "        t0 = time.time()\n",
        "        Xtr_text = text_series.iloc[trn_idx]\n",
        "        Xva_text = text_series.iloc[val_idx]\n",
        "        ytr = y[trn_idx]\n",
        "        vec = CountVectorizer(ngram_range=(1,2), min_df=min_df, max_features=max_features, lowercase=True, dtype=np.float32)\n",
        "        X_tr = vec.fit_transform(Xtr_text)\n",
        "        X_va = vec.transform(Xva_text)\n",
        "        # class-frequency-normalized r\n",
        "        num_pos = (ytr == 1).sum()\n",
        "        num_neg = (ytr == 0).sum()\n",
        "        pos_counts = X_tr[ytr == 1].sum(axis=0).A1 / max(1, num_pos)\n",
        "        neg_counts = X_tr[ytr == 0].sum(axis=0).A1 / max(1, num_neg)\n",
        "        r = np.log((pos_counts + alpha) / (neg_counts + alpha))\n",
        "        X_tr_nb = X_tr.multiply(r)\n",
        "        X_va_nb = X_va.multiply(r)\n",
        "        clf = LogisticRegression(C=C, solver='liblinear', max_iter=2000)\n",
        "        clf.fit(X_tr_nb, ytr)\n",
        "        oof[val_idx] = clf.predict_proba(X_va_nb)[:,1]\n",
        "        print(f'  NB-SVM(fixed) fold {fold} in {time.time()-t0:.1f}s', flush=True)\n",
        "    auc = roc_auc_score(y, oof)\n",
        "    return auc, oof, vec, r, clf\n",
        "\n",
        "print('=== NB-SVM fixed normalization CV ===', flush=True)\n",
        "ybin = train[target_col].astype(int).values\n",
        "nb_auc2, nb_oof2, nb_vec2, nb_r2, nb_clf2 = nbsvm_oof_fixed(full_text_train2, ybin, alpha=1.0, C=2.0, min_df=2, max_features=120000)\n",
        "print(f'NB-SVM fixed OOF AUC: {nb_auc2:.5f}', flush=True)\n",
        "np.save('oof_nbsvm_fixed.npy', nb_oof2)\n",
        "\n",
        "# Fit NB-SVM fixed on full train and predict test\n",
        "X_full_nb2 = nb_vec2.fit_transform(full_text_train2)\n",
        "num_pos_full = (ybin == 1).sum()\n",
        "num_neg_full = (ybin == 0).sum()\n",
        "pos_full = X_full_nb2[ybin == 1].sum(axis=0).A1 / max(1, num_pos_full)\n",
        "neg_full = X_full_nb2[ybin == 0].sum(axis=0).A1 / max(1, num_neg_full)\n",
        "r_full = np.log((pos_full + 1.0) / (neg_full + 1.0))\n",
        "X_full_nb2_tr = X_full_nb2.multiply(r_full)\n",
        "nb_full2 = LogisticRegression(C=2.0, solver='liblinear', max_iter=2000)\n",
        "nb_full2.fit(X_full_nb2_tr, ybin)\n",
        "X_test_nb2 = nb_vec2.transform(full_text_test2)\n",
        "X_test_nb2_tr = X_test_nb2.multiply(r_full)\n",
        "test_nb2 = nb_full2.predict_proba(X_test_nb2_tr)[:,1].astype(np.float32)\n",
        "np.save('test_nbsvm_fixed.npy', test_nb2)\n",
        "\n",
        "# Build stacker features: dense meta (with lexicons) + OOF columns from base models\n",
        "oof_meta_lr = np.load('oof_lr_text_meta.npy')\n",
        "oof_lr_only = np.load('oof_lr_improved.npy')\n",
        "oof_nb_fixed = np.load('oof_nbsvm_fixed.npy')\n",
        "\n",
        "X_stacker = np.hstack([meta_train.values,\n",
        "                        oof_meta_lr.reshape(-1,1),\n",
        "                        oof_lr_only.reshape(-1,1),\n",
        "                        oof_nb_fixed.reshape(-1,1)])\n",
        "y_stacker = ybin\n",
        "\n",
        "print('=== Stacker CV (HistGradientBoosting) on meta+lexicons+OOFs ===', flush=True)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_stack = np.zeros(len(train), dtype=np.float32)\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(X_stacker, y_stacker), 1):\n",
        "    X_tr, X_va = X_stacker[trn_idx], X_stacker[val_idx]\n",
        "    y_tr, y_va = y_stacker[trn_idx], y_stacker[val_idx]\n",
        "    clf = HistGradientBoostingClassifier(max_depth=3, learning_rate=0.075, max_iter=600,\n",
        "                                         early_stopping=True, validation_fraction=0.1,\n",
        "                                         random_state=42)\n",
        "    t0 = time.time()\n",
        "    clf.fit(X_tr, y_tr)\n",
        "    oof_stack[val_idx] = clf.predict_proba(X_va)[:,1]\n",
        "    print(f'  Stacker fold {fold} in {time.time()-t0:.1f}s', flush=True)\n",
        "auc_stack = roc_auc_score(y_stacker, oof_stack)\n",
        "print(f'Stacker OOF AUC: {auc_stack:.5f}', flush=True)\n",
        "np.save('oof_stacker.npy', oof_stack)\n",
        "\n",
        "# Fit stacker on full train and predict test\n",
        "test_meta_lr = np.load('test_lr_text_meta.npy')\n",
        "test_lr_only = np.load('test_lr_improved.npy')\n",
        "test_nb_fixed = np.load('test_nbsvm_fixed.npy')\n",
        "X_test_stacker = np.hstack([meta_test.values,\n",
        "                             test_meta_lr.reshape(-1,1),\n",
        "                             test_lr_only.reshape(-1,1),\n",
        "                             test_nb_fixed.reshape(-1,1)])\n",
        "final_stacker = HistGradientBoostingClassifier(max_depth=3, learning_rate=0.075, max_iter=600,\n",
        "                                               early_stopping=True, validation_fraction=0.1,\n",
        "                                               random_state=42)\n",
        "final_stacker.fit(X_stacker, y_stacker)\n",
        "test_stack = final_stacker.predict_proba(X_test_stacker)[:,1].astype(np.float32)\n",
        "np.save('test_stacker.npy', test_stack)\n",
        "\n",
        "# Blend LR(meta+text+lex) with stacker using OOF to choose weight\n",
        "weights = np.linspace(0.0, 1.0, 41)\n",
        "best_w, best_auc = None, -1.0\n",
        "for w in weights:\n",
        "    blend = w*oof_meta_lr + (1.0-w)*oof_stack\n",
        "    auc = roc_auc_score(ybin, blend)\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_w = auc, w\n",
        "print(f'Best OOF AUC blend(LR_meta_text, stacker)={best_auc:.5f} at w(LR)={best_w:.2f}', flush=True)\n",
        "test_blend = best_w*test_meta_lr + (1.0-best_w)*test_stack\n",
        "sub = pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (stack blend):', sub.shape, 'min/max:', float(test_blend.min()), float(test_blend.max()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== NB-SVM fixed normalization CV ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NB-SVM(fixed) fold 1 in 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NB-SVM(fixed) fold 2 in 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NB-SVM(fixed) fold 3 in 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NB-SVM(fixed) fold 4 in 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NB-SVM(fixed) fold 5 in 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM fixed OOF AUC: 0.59933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Stacker CV (HistGradientBoosting) on meta+lexicons+OOFs ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Stacker fold 1 in 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Stacker fold 2 in 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Stacker fold 3 in 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Stacker fold 4 in 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Stacker fold 5 in 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacker OOF AUC: 0.66893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best OOF AUC blend(LR_meta_text, stacker)=0.68297 at w(LR)=1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (stack blend): (1162, 2) min/max: 0.024701131507754326 0.970310389995575\n"
          ]
        }
      ]
    },
    {
      "id": "e57a5143-b33c-47bf-94bb-b65d37e061ae",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Add subreddit bag (TF-IDF of requester_subreddits_at_request) to LR meta+text pipeline\n",
        "import time, re\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def build_subreddit_text(df: pd.DataFrame) -> pd.Series:\n",
        "    if 'requester_subreddits_at_request' not in df.columns:\n",
        "        return pd.Series(['']*len(df))\n",
        "    def to_line(x):\n",
        "        if isinstance(x, list):\n",
        "            # join subreddit names with space; ensure strings and lowercase\n",
        "            return ' '.join([str(s).lower() for s in x if isinstance(s, str)])\n",
        "        return ''\n",
        "    return df['requester_subreddits_at_request'].apply(to_line)\n",
        "\n",
        "sub_train_txt = build_subreddit_text(train)\n",
        "sub_test_txt = build_subreddit_text(test)\n",
        "\n",
        "def get_text_vecs():\n",
        "    wv = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.95, max_features=100000,\n",
        "                         sublinear_tf=True, lowercase=True, dtype=np.float32)\n",
        "    cv = TfidfVectorizer(analyzer='char', ngram_range=(3,6), min_df=2, max_features=200000,\n",
        "                         sublinear_tf=True, lowercase=True, dtype=np.float32)\n",
        "    return wv, cv\n",
        "\n",
        "def get_sub_vec():\n",
        "    # Compact subreddit vocabulary\n",
        "    return TfidfVectorizer(ngram_range=(1,1), min_df=3, max_features=600,\n",
        "                           lowercase=True, token_pattern=r'[^\\s]+' , dtype=np.float32)\n",
        "\n",
        "def hstack4(X1, X2, X3, X4):\n",
        "    return sparse.hstack([X1, X2, X3, X4], format='csr')\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_sr = np.zeros(len(train), dtype=np.float32)\n",
        "\n",
        "print('=== CV: Text(word+char) + Scaled Meta(+lex) + Subreddit TF-IDF -> LR (C=2, cw=balanced) ===', flush=True)\n",
        "times = []\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(full_text_train2, y), 1):\n",
        "    t0 = time.time()\n",
        "    # Text\n",
        "    wv, cv = get_text_vecs()\n",
        "    Xw_tr = wv.fit_transform(full_text_train2.iloc[trn_idx])\n",
        "    Xc_tr = cv.fit_transform(full_text_train2.iloc[trn_idx])\n",
        "    Xw_va = wv.transform(full_text_train2.iloc[val_idx])\n",
        "    Xc_va = cv.transform(full_text_train2.iloc[val_idx])\n",
        "    # Meta (already built in previous cell as meta_train/meta_test); scale in-fold\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    Xm_tr = scaler.fit_transform(meta_train.iloc[trn_idx])\n",
        "    Xm_va = scaler.transform(meta_train.iloc[val_idx])\n",
        "    # Subreddits TF-IDF\n",
        "    sv = get_sub_vec()\n",
        "    Xs_tr = sv.fit_transform(sub_train_txt.iloc[trn_idx])\n",
        "    Xs_va = sv.transform(sub_train_txt.iloc[val_idx])\n",
        "    # Stack\n",
        "    X_tr = hstack4(Xw_tr, Xc_tr, Xm_tr, Xs_tr)\n",
        "    X_va = hstack4(Xw_va, Xc_va, Xm_va, Xs_va)\n",
        "    clf = LogisticRegression(penalty='l2', C=2.0, solver='saga', max_iter=3000, n_jobs=-1, class_weight='balanced', verbose=0)\n",
        "    clf.fit(X_tr, y[trn_idx])\n",
        "    oof_sr[val_idx] = clf.predict_proba(X_va)[:,1]\n",
        "    times.append(time.time()-t0)\n",
        "    print(f'  fold {fold} in {times[-1]:.1f}s', flush=True)\n",
        "auc_sr = roc_auc_score(y, oof_sr)\n",
        "print(f'OOF AUC (meta+text+lex+subs): {auc_sr:.5f}', flush=True)\n",
        "np.save('oof_lr_text_meta_subs.npy', oof_sr)\n",
        "\n",
        "# Fit final and predict test\n",
        "wv_full, cv_full = get_text_vecs()\n",
        "Xw_full = wv_full.fit_transform(full_text_train2)\n",
        "Xc_full = cv_full.fit_transform(full_text_train2)\n",
        "scaler_full = StandardScaler(with_mean=False)\n",
        "Xm_full = scaler_full.fit_transform(meta_train)\n",
        "sv_full = get_sub_vec()\n",
        "Xs_full = sv_full.fit_transform(sub_train_txt)\n",
        "X_full = hstack4(Xw_full, Xc_full, Xm_full, Xs_full)\n",
        "clf_full = LogisticRegression(penalty='l2', C=2.0, solver='saga', max_iter=3000, n_jobs=-1, class_weight='balanced', verbose=0)\n",
        "clf_full.fit(X_full, y)\n",
        "Xw_test = wv_full.transform(full_text_test2)\n",
        "Xc_test = cv_full.transform(full_text_test2)\n",
        "Xm_test = scaler_full.transform(meta_test)\n",
        "Xs_test = sv_full.transform(sub_test_txt)\n",
        "X_test = hstack4(Xw_test, Xc_test, Xm_test, Xs_test)\n",
        "test_pred_sr = clf_full.predict_proba(X_test)[:,1].astype(np.float32)\n",
        "np.save('test_lr_text_meta_subs.npy', test_pred_sr)\n",
        "sub_sr = pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_pred_sr})\n",
        "sub_sr.to_csv('submission_text_meta_subs.csv', index=False)\n",
        "print('Saved submission_text_meta_subs.csv', sub_sr.shape, 'min/max:', float(test_pred_sr.min()), float(test_pred_sr.max()))\n",
        "\n",
        "# Optional: OOF-weighted blend among (meta+text+lex) and (meta+text+lex+subs)\n",
        "oof_base = np.load('oof_lr_text_meta.npy') if os.path.exists('oof_lr_text_meta.npy') else oof_sr\n",
        "best_w, best_auc = 1.0, auc_sr\n",
        "for w in np.linspace(0.0,1.0,21):\n",
        "    blend = w*oof_sr + (1.0-w)*oof_base\n",
        "    a = roc_auc_score(y, blend)\n",
        "    if a > best_auc:\n",
        "        best_auc, best_w = a, w\n",
        "print(f'Best OOF blend(base vs subs) AUC: {best_auc:.5f} at w(subs)={best_w:.2f}', flush=True)\n",
        "test_base = np.load('test_lr_text_meta.npy') if os.path.exists('test_lr_text_meta.npy') else test_pred_sr\n",
        "test_blend = best_w*test_pred_sr + (1.0-best_w)*test_base\n",
        "pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend}).to_csv('submission.csv', index=False)\n",
        "print('Updated submission.csv from base/subs blend.')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CV: Text(word+char) + Scaled Meta(+lex) + Subreddit TF-IDF -> LR (C=2, cw=balanced) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 1 in 48.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 2 in 58.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 3 in 56.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 4 in 47.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 5 in 51.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF AUC (meta+text+lex+subs): 0.67327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_text_meta_subs.csv (1162, 2) min/max: 0.016643734648823738 0.9696571230888367\nBest OOF blend(base vs subs) AUC: 0.68409 at w(subs)=0.20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated submission.csv from base/subs blend.\n"
          ]
        }
      ]
    },
    {
      "id": "3c6c64a0-5996-48aa-a9bc-28216656c145",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Try char_wb instead of char for the text+meta(+lex) LR (single best config) to seek OOF lift\n",
        "import time\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def get_vecs_meta_charwb():\n",
        "    wv = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.95, max_features=100000,\n",
        "                         sublinear_tf=True, lowercase=True, dtype=np.float32)\n",
        "    # switch to char_wb and keep (3,6); sometimes improves RAOP\n",
        "    cv = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,6), min_df=2, max_features=200000,\n",
        "                         sublinear_tf=True, lowercase=True, dtype=np.float32)\n",
        "    return wv, cv\n",
        "\n",
        "def hstack3(X1, X2, X3):\n",
        "    return sparse.hstack([X1, X2, X3], format='csr')\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_cwb = np.zeros(len(train), dtype=np.float32)\n",
        "\n",
        "print('=== CV: word(1-2) TF-IDF + char_wb(3-6) + Scaled Meta(+lex) -> LR (C=2, cw=balanced) ===', flush=True)\n",
        "times = []\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(full_text_train2, y), 1):\n",
        "    t0 = time.time()\n",
        "    Xtr_text = full_text_train2.iloc[trn_idx]\n",
        "    Xva_text = full_text_train2.iloc[val_idx]\n",
        "    wv, cv = get_vecs_meta_charwb()\n",
        "    Xw_tr = wv.fit_transform(Xtr_text)\n",
        "    Xc_tr = cv.fit_transform(Xtr_text)\n",
        "    Xw_va = wv.transform(Xva_text)\n",
        "    Xc_va = cv.transform(Xva_text)\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    Xm_tr = scaler.fit_transform(meta_train.iloc[trn_idx])\n",
        "    Xm_va = scaler.transform(meta_train.iloc[val_idx])\n",
        "    X_tr = hstack3(Xw_tr, Xc_tr, Xm_tr)\n",
        "    X_va = hstack3(Xw_va, Xc_va, Xm_va)\n",
        "    clf = LogisticRegression(penalty='l2', C=2.0, solver='saga', max_iter=3000, n_jobs=-1, class_weight='balanced', verbose=0)\n",
        "    clf.fit(X_tr, y[trn_idx])\n",
        "    oof_cwb[val_idx] = clf.predict_proba(X_va)[:,1]\n",
        "    times.append(time.time()-t0)\n",
        "    print(f'  fold {fold} in {times[-1]:.1f}s', flush=True)\n",
        "auc_cwb = roc_auc_score(y, oof_cwb)\n",
        "print(f'OOF AUC (char_wb variant): {auc_cwb:.5f}', flush=True)\n",
        "np.save('oof_lr_text_meta_charwb.npy', oof_cwb)\n",
        "\n",
        "# Fit final and predict test with char_wb if it helps; else keep prior best\n",
        "wv_full, cv_full = get_vecs_meta_charwb()\n",
        "Xw_full = wv_full.fit_transform(full_text_train2)\n",
        "Xc_full = cv_full.fit_transform(full_text_train2)\n",
        "scaler_full = StandardScaler(with_mean=False)\n",
        "Xm_full = scaler_full.fit_transform(meta_train)\n",
        "X_full = hstack3(Xw_full, Xc_full, Xm_full)\n",
        "clf_full = LogisticRegression(penalty='l2', C=2.0, solver='saga', max_iter=3000, n_jobs=-1, class_weight='balanced', verbose=0)\n",
        "clf_full.fit(X_full, y)\n",
        "Xw_test = wv_full.transform(full_text_test2)\n",
        "Xc_test = cv_full.transform(full_text_test2)\n",
        "Xm_test = scaler_full.transform(meta_test)\n",
        "X_test = hstack3(Xw_test, Xc_test, Xm_test)\n",
        "test_pred_cwb = clf_full.predict_proba(X_test)[:,1].astype(np.float32)\n",
        "np.save('test_lr_text_meta_charwb.npy', test_pred_cwb)\n",
        "\n",
        "# Choose best among (meta+text+lex char) vs (char_wb) by OOF and update submission if improved\n",
        "try:\n",
        "    oof_base = np.load('oof_lr_text_meta.npy')\n",
        "    test_base = np.load('test_lr_text_meta.npy')\n",
        "    base_auc = roc_auc_score(y, oof_base)\n",
        "except Exception:\n",
        "    oof_base = oof_cwb\n",
        "    test_base = test_pred_cwb\n",
        "    base_auc = auc_cwb\n",
        "\n",
        "if auc_cwb > base_auc:\n",
        "    print(f'char_wb improved OOF from {base_auc:.5f} to {auc_cwb:.5f}; updating submission.csv')\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_pred_cwb}).to_csv('submission.csv', index=False)\n",
        "else:\n",
        "    print(f'char_wb did not improve OOF ({auc_cwb:.5f} <= {base_auc:.5f}); keeping current submission.csv')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CV: word(1-2) TF-IDF + char_wb(3-6) + Scaled Meta(+lex) -> LR (C=2, cw=balanced) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 1 in 29.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 2 in 33.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 3 in 33.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 4 in 29.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 5 in 30.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF AUC (char_wb variant): 0.68276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb did not improve OOF (0.68276 <= 0.68297); keeping current submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "7704eb69-9a9c-4ff8-8bda-d111505210d4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost TEXT+META with StratifiedGroupKFold by requester_username; seed-bag; OOF blend with LR (no custom text_processing to avoid param errors)\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import catboost as cb\n",
        "\n",
        "assert 'meta_train' in globals() and 'meta_test' in globals(), 'Build meta_train/meta_test first'\n",
        "assert 'train' in globals() and 'test' in globals() and 'title_col' in globals() and 'body_col' in globals()\n",
        "\n",
        "y = train['requester_received_pizza'].astype(int).values\n",
        "groups = train['requester_username'].fillna('').astype(str).values\n",
        "\n",
        "df_tr = pd.concat([\n",
        "    pd.DataFrame({'t_title': train[title_col].astype(str).fillna(''),\n",
        "                  't_body':  train[body_col].astype(str).fillna('')}).reset_index(drop=True),\n",
        "    meta_train.reset_index(drop=True)\n",
        "], axis=1)\n",
        "df_te = pd.concat([\n",
        "    pd.DataFrame({'t_title': test[title_col].astype(str).fillna(''),\n",
        "                  't_body':  test[body_col].astype(str).fillna('')}).reset_index(drop=True),\n",
        "    meta_test.reset_index(drop=True)\n",
        "], axis=1)\n",
        "text_idx = [0, 1]\n",
        "\n",
        "base_params = dict(\n",
        "    loss_function='Logloss',\n",
        "    eval_metric='AUC',\n",
        "    depth=5,\n",
        "    learning_rate=0.04,\n",
        "    l2_leaf_reg=4.0,\n",
        "    iterations=3000,\n",
        "    early_stopping_rounds=200,\n",
        "    random_strength=1.0,\n",
        "    auto_class_weights='Balanced',\n",
        "    border_count=128,\n",
        "    verbose=100\n",
        ")\n",
        "try:\n",
        "    from catboost.utils import get_gpu_device_count\n",
        "    if get_gpu_device_count() > 0:\n",
        "        base_params['task_type'] = 'GPU'\n",
        "        print('CatBoost task_type: GPU', flush=True)\n",
        "    else:\n",
        "        print('CatBoost task_type: CPU', flush=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def fit_cb_seed(seed):\n",
        "    params = dict(base_params, random_seed=int(seed))\n",
        "    skf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    oof = np.zeros(len(df_tr), dtype=np.float32)\n",
        "    test_fold = []\n",
        "    for fold, (trn_idx, val_idx) in enumerate(skf.split(df_tr, y, groups), 1):\n",
        "        t0 = time.time()\n",
        "        tr_pool = cb.Pool(df_tr.iloc[trn_idx], label=y[trn_idx], text_features=text_idx)\n",
        "        va_pool = cb.Pool(df_tr.iloc[val_idx], label=y[val_idx], text_features=text_idx)\n",
        "        model = cb.CatBoostClassifier(**params)\n",
        "        model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n",
        "        oof[val_idx] = model.predict_proba(va_pool)[:,1]\n",
        "        if fold == 1:\n",
        "            te_pool = cb.Pool(df_te, text_features=text_idx)\n",
        "        test_fold.append(model.predict_proba(te_pool)[:,1].astype(np.float32))\n",
        "        auc = roc_auc_score(y[val_idx], oof[val_idx])\n",
        "        print(f'Fold {fold} seed {seed}: AUC {auc:.5f} | {time.time()-t0:.1f}s', flush=True)\n",
        "    return oof, np.mean(np.vstack(test_fold), axis=0).astype(np.float32)\n",
        "\n",
        "seeds = [42, 1337, 7]\n",
        "oofs, tests = [], []\n",
        "for s in seeds:\n",
        "    oof_s, test_s = fit_cb_seed(s)\n",
        "    oofs.append(oof_s); tests.append(test_s)\n",
        "oof_cb = np.mean(np.vstack(oofs), axis=0).astype(np.float32)\n",
        "test_cb = np.mean(np.vstack(tests), axis=0).astype(np.float32)\n",
        "auc_cb = roc_auc_score(y, oof_cb)\n",
        "print('CatBoost grouped bag OOF AUC:', f'{auc_cb:.5f}')\n",
        "np.save('oof_catboost_text_meta_g.npy', oof_cb)\n",
        "np.save('test_catboost_text_meta_g.npy', test_cb)\n",
        "\n",
        "# Blend with best LR leg (prefer grouped LR if present; else fallback)\n",
        "lr_oof_path = 'oof_lr_meta_g.npy' if os.path.exists('oof_lr_meta_g.npy') else ('oof_lr_text_meta.npy' if os.path.exists('oof_lr_text_meta.npy') else None)\n",
        "lr_te_path  = 'test_lr_meta_g.npy' if os.path.exists('test_lr_meta_g.npy') else ('test_lr_text_meta.npy' if os.path.exists('test_lr_text_meta.npy') else None)\n",
        "\n",
        "if lr_oof_path and lr_te_path:\n",
        "    oof_lr = np.load(lr_oof_path); test_lr = np.load(lr_te_path)\n",
        "    ws = np.linspace(0.0, 1.0, 41)\n",
        "    best_w, best_auc = 0.0, roc_auc_score(y, oof_lr)\n",
        "    for w in ws:\n",
        "        a = roc_auc_score(y, w*oof_cb + (1.0-w)*oof_lr)\n",
        "        if a > best_auc:\n",
        "            best_auc, best_w = a, w\n",
        "    w_cb = 0.85*best_w + 0.15*0.5\n",
        "    print(f'Best OOF blend AUC: {best_auc:.5f} at w(CB)={best_w:.2f} -> using w(CB)={w_cb:.2f}')\n",
        "    test_blend = w_cb*test_cb + (1.0-w_cb)*test_lr\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend}).to_csv('submission.csv', index=False)\n",
        "else:\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_cb}).to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (CatBoost blend or pure).')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost task_type: GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.5680620\tbest: 0.5680620 (0)\ttotal: 17.3ms\tremaining: 52s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\ttest: 0.6745113\tbest: 0.6776460 (82)\ttotal: 1.7s\tremaining: 48.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\ttest: 0.6676311\tbest: 0.6776460 (82)\ttotal: 3.34s\tremaining: 46.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bestTest = 0.6776459813\nbestIteration = 82\nShrink model to first 83 iterations.\nFold 1 seed 42: AUC 0.67765 | 5.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.6073511\tbest: 0.6073511 (0)\ttotal: 17.5ms\tremaining: 52.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\ttest: 0.6956515\tbest: 0.7059892 (77)\ttotal: 1.68s\tremaining: 48.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\ttest: 0.6893278\tbest: 0.7059892 (77)\ttotal: 3.33s\tremaining: 46.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bestTest = 0.705989182\nbestIteration = 77\nShrink model to first 78 iterations.\nFold 2 seed 42: AUC 0.70599 | 5.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.5904411\tbest: 0.5904411 (0)\ttotal: 17.9ms\tremaining: 53.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\ttest: 0.6587330\tbest: 0.6603368 (95)\ttotal: 1.69s\tremaining: 48.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\ttest: 0.6662069\tbest: 0.6662069 (200)\ttotal: 3.33s\tremaining: 46.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300:\ttest: 0.6755573\tbest: 0.6756375 (291)\ttotal: 4.98s\tremaining: 44.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400:\ttest: 0.6813793\tbest: 0.6858541 (370)\ttotal: 6.63s\tremaining: 43s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500:\ttest: 0.6774659\tbest: 0.6858541 (370)\ttotal: 8.27s\tremaining: 41.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bestTest = 0.6858540773\nbestIteration = 370\nShrink model to first 371 iterations.\nFold 3 seed 42: AUC 0.68585 | 10.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.6104848\tbest: 0.6104848 (0)\ttotal: 18.6ms\tremaining: 55.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\ttest: 0.6786738\tbest: 0.6791282 (99)\ttotal: 1.68s\tremaining: 48.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\ttest: 0.6790583\tbest: 0.6859969 (152)\ttotal: 3.32s\tremaining: 46.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300:\ttest: 0.6855600\tbest: 0.6874475 (252)\ttotal: 4.96s\tremaining: 44.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400:\ttest: 0.6865562\tbest: 0.6874475 (252)\ttotal: 6.59s\tremaining: 42.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500:\ttest: 0.6895099\tbest: 0.6895099 (500)\ttotal: 8.21s\tremaining: 41s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600:\ttest: 0.6891254\tbest: 0.6907159 (518)\ttotal: 9.85s\tremaining: 39.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "700:\ttest: 0.6893701\tbest: 0.6907159 (518)\ttotal: 11.5s\tremaining: 37.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bestTest = 0.690715909\nbestIteration = 518\nShrink model to first 519 iterations.\nFold 4 seed 42: AUC 0.69072 | 13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.6003522\tbest: 0.6003522 (0)\ttotal: 17.8ms\tremaining: 53.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\ttest: 0.7230480\tbest: 0.7300459 (63)\ttotal: 1.69s\tremaining: 48.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\ttest: 0.7326217\tbest: 0.7347940 (199)\ttotal: 3.33s\tremaining: 46.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300:\ttest: 0.7326837\tbest: 0.7351508 (255)\ttotal: 4.97s\tremaining: 44.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400:\ttest: 0.7221015\tbest: 0.7351508 (255)\ttotal: 6.62s\tremaining: 42.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bestTest = 0.7351508141\nbestIteration = 255\nShrink model to first 256 iterations.\nFold 5 seed 42: AUC 0.73515 | 8.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.5502106\tbest: 0.5502106 (0)\ttotal: 18.1ms\tremaining: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\ttest: 0.6681295\tbest: 0.6717786 (80)\ttotal: 1.67s\tremaining: 47.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\ttest: 0.6753312\tbest: 0.6785462 (128)\ttotal: 3.31s\tremaining: 46.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300:\ttest: 0.6767618\tbest: 0.6785462 (128)\ttotal: 4.95s\tremaining: 44.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400:\ttest: 0.6810861\tbest: 0.6816165 (367)\ttotal: 6.58s\tremaining: 42.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500:\ttest: 0.6812789\tbest: 0.6828061 (412)\ttotal: 8.22s\tremaining: 41s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600:\ttest: 0.6786909\tbest: 0.6828061 (412)\ttotal: 9.85s\tremaining: 39.3s\nbestTest = 0.6828060746\nbestIteration = 412\nShrink model to first 413 iterations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 seed 1337: AUC 0.68281 | 10.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.6074944\tbest: 0.6074944 (0)\ttotal: 17.2ms\tremaining: 51.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\ttest: 0.6927684\tbest: 0.6976426 (56)\ttotal: 1.69s\tremaining: 48.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\ttest: 0.6868907\tbest: 0.6976426 (56)\ttotal: 3.34s\tremaining: 46.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bestTest = 0.6976425648\nbestIteration = 56\nShrink model to first 57 iterations.\nFold 2 seed 1337: AUC 0.69764 | 5.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.5734803\tbest: 0.5734803 (0)\ttotal: 17.2ms\tremaining: 51.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\ttest: 0.6666560\tbest: 0.6702967 (66)\ttotal: 1.67s\tremaining: 47.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\ttest: 0.6661428\tbest: 0.6702967 (66)\ttotal: 3.3s\tremaining: 46s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300:\ttest: 0.6731676\tbest: 0.6772414 (289)\ttotal: 4.93s\tremaining: 44.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400:\ttest: 0.6786367\tbest: 0.6812029 (370)\ttotal: 6.57s\tremaining: 42.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500:\ttest: 0.6785726\tbest: 0.6812029 (370)\ttotal: 8.21s\tremaining: 41s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bestTest = 0.6812028885\nbestIteration = 370\nShrink model to first 371 iterations.\nFold 3 seed 1337: AUC 0.68120 | 10.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.5925790\tbest: 0.5925790 (0)\ttotal: 17.4ms\tremaining: 52.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\ttest: 0.6838472\tbest: 0.6838472 (100)\ttotal: 1.68s\tremaining: 48.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\ttest: 0.6951552\tbest: 0.6953999 (196)\ttotal: 3.33s\tremaining: 46.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300:\ttest: 0.6966408\tbest: 0.6966408 (300)\ttotal: 4.96s\tremaining: 44.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400:\ttest: 0.6954873\tbest: 0.6987556 (362)\ttotal: 6.59s\tremaining: 42.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500:\ttest: 0.6878146\tbest: 0.6987556 (362)\ttotal: 8.23s\tremaining: 41s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bestTest = 0.6987556219\nbestIteration = 362\nShrink model to first 363 iterations.\nFold 4 seed 1337: AUC 0.69876 | 10.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.6270016\tbest: 0.6270016 (0)\ttotal: 17.1ms\tremaining: 51.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\ttest: 0.7286339\tbest: 0.7294874 (48)\ttotal: 1.7s\tremaining: 48.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\ttest: 0.7216981\tbest: 0.7294874 (48)\ttotal: 3.36s\tremaining: 46.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bestTest = 0.7294873595\nbestIteration = 48\nShrink model to first 49 iterations.\nFold 5 seed 1337: AUC 0.72949 | 5.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.5514162\tbest: 0.5514162 (0)\ttotal: 17.5ms\tremaining: 52.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\ttest: 0.6634838\tbest: 0.6678884 (61)\ttotal: 1.67s\tremaining: 47.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\ttest: 0.6733860\tbest: 0.6750739 (180)\ttotal: 3.3s\tremaining: 46s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300:\ttest: 0.6699299\tbest: 0.6750739 (180)\ttotal: 4.92s\tremaining: 44.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bestTest = 0.6750739217\nbestIteration = 180\nShrink model to first 181 iterations.\nFold 1 seed 7: AUC 0.67507 | 6.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.6092864\tbest: 0.6092864 (0)\ttotal: 17.7ms\tremaining: 53s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\ttest: 0.6741797\tbest: 0.6843581 (64)\ttotal: 1.68s\tremaining: 48.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\ttest: 0.6609589\tbest: 0.6843581 (64)\ttotal: 3.32s\tremaining: 46.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bestTest = 0.6843580604\nbestIteration = 64\nShrink model to first 65 iterations.\nFold 2 seed 7: AUC 0.68436 | 5.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.6095349\tbest: 0.6095349 (0)\ttotal: 19.3ms\tremaining: 57.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\ttest: 0.6627426\tbest: 0.6676503 (45)\ttotal: 1.67s\tremaining: 48s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\ttest: 0.6754290\tbest: 0.6770489 (192)\ttotal: 3.31s\tremaining: 46s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300:\ttest: 0.6859984\tbest: 0.6859984 (300)\ttotal: 4.95s\tremaining: 44.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400:\ttest: 0.6902165\tbest: 0.6904411 (393)\ttotal: 6.59s\tremaining: 42.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500:\ttest: 0.6882117\tbest: 0.6927506 (426)\ttotal: 8.25s\tremaining: 41.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600:\ttest: 0.6888533\tbest: 0.6927506 (426)\ttotal: 9.89s\tremaining: 39.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bestTest = 0.6927505732\nbestIteration = 426\nShrink model to first 427 iterations.\nFold 3 seed 7: AUC 0.69275 | 11.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.5922819\tbest: 0.5922819 (0)\ttotal: 17.7ms\tremaining: 53s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\ttest: 0.6898595\tbest: 0.6898595 (100)\ttotal: 1.69s\tremaining: 48.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\ttest: 0.6949804\tbest: 0.6956446 (186)\ttotal: 3.33s\tremaining: 46.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300:\ttest: 0.6973923\tbest: 0.7007830 (253)\ttotal: 4.96s\tremaining: 44.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400:\ttest: 0.6976895\tbest: 0.7007830 (253)\ttotal: 6.6s\tremaining: 42.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bestTest = 0.7007830143\nbestIteration = 253\nShrink model to first 254 iterations.\nFold 4 seed 7: AUC 0.70078 | 8.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.6159462\tbest: 0.6159462 (0)\ttotal: 17.2ms\tremaining: 51.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\ttest: 0.7264151\tbest: 0.7266168 (99)\ttotal: 1.67s\tremaining: 48s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\ttest: 0.7195569\tbest: 0.7309459 (122)\ttotal: 3.31s\tremaining: 46s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300:\ttest: 0.7117211\tbest: 0.7309459 (122)\ttotal: 4.94s\tremaining: 44.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bestTest = 0.7309458852\nbestIteration = 122\nShrink model to first 123 iterations.\nFold 5 seed 7: AUC 0.73095 | 6.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost grouped bag OOF AUC: 0.70023\nBest OOF blend AUC: 0.70702 at w(CB)=0.80 -> using w(CB)=0.76\nSaved submission.csv (CatBoost blend or pure).\n"
          ]
        }
      ]
    },
    {
      "id": "b68e8f67-0b8b-4c19-a044-85798993f7f0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick re-blend: use best OOF weight (no shrink) and finer grid; write submission.csv\n",
        "import os, numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "assert 'train' in globals() and 'test' in globals(), 'Run earlier cells to load train/test'\n",
        "y = train['requester_received_pizza'].astype(int).values\n",
        "\n",
        "cb_oof_path = 'oof_catboost_text_meta_g.npy'\n",
        "cb_te_path  = 'test_catboost_text_meta_g.npy'\n",
        "lr_oof_path = 'oof_lr_meta_g.npy' if os.path.exists('oof_lr_meta_g.npy') else ('oof_lr_text_meta.npy' if os.path.exists('oof_lr_text_meta.npy') else None)\n",
        "lr_te_path  = 'test_lr_meta_g.npy' if os.path.exists('test_lr_meta_g.npy') else ('test_lr_text_meta.npy' if os.path.exists('test_lr_text_meta.npy') else None)\n",
        "assert os.path.exists(cb_oof_path) and os.path.exists(cb_te_path), 'Missing CatBoost OOF/test npy artifacts'\n",
        "assert lr_oof_path and lr_te_path, 'Missing LR OOF/test npy artifacts'\n",
        "\n",
        "oof_cb = np.load(cb_oof_path).astype(np.float32)\n",
        "test_cb = np.load(cb_te_path).astype(np.float32)\n",
        "oof_lr = np.load(lr_oof_path).astype(np.float32)\n",
        "test_lr = np.load(lr_te_path).astype(np.float32)\n",
        "\n",
        "# Fine search around 0.70-0.90 and global coarse backup\n",
        "ws_fine = np.arange(0.70, 0.901, 0.01)\n",
        "ws_coarse = np.linspace(0.0, 1.0, 41)\n",
        "ws = np.unique(np.concatenate([ws_coarse, ws_fine]))\n",
        "best_w, best_auc = 0.0, roc_auc_score(y, oof_lr)\n",
        "for w in ws:\n",
        "    a = roc_auc_score(y, w*oof_cb + (1.0-w)*oof_lr)\n",
        "    if a > best_auc:\n",
        "        best_auc, best_w = a, float(w)\n",
        "print(f'REBLEND Best OOF blend(LR,CB) AUC: {best_auc:.5f} at w(CB)={best_w:.2f}', flush=True)\n",
        "\n",
        "test_blend = best_w*test_cb + (1.0-best_w)*test_lr\n",
        "sub = pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with w(CB)=', best_w)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REBLEND Best OOF blend(LR,CB) AUC: 0.70702 at w(CB)=0.79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv with w(CB)= 0.79\n"
          ]
        }
      ]
    },
    {
      "id": "cfc22a9e-95e5-476f-adb5-b49fe504cf93",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost TEXT+META: title/body as TEXT + meta(+lex) numeric; 5-fold CV; OOF-optimal blend\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'catboost', '-q'], check=True)\n",
        "    import catboost as cb\n",
        "\n",
        "assert 'meta_train' in globals() and 'meta_test' in globals(), 'Run meta/meta+lex cell first'\n",
        "assert title_col in train.columns and body_col in train.columns and target_col in train.columns, 'Missing columns'\n",
        "\n",
        "y_cb = train[target_col].astype(int).values\n",
        "df_tr = pd.concat([pd.DataFrame({'t_title': train[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  train[body_col].astype(str).fillna('')}),\n",
        "                   meta_train.reset_index(drop=True)], axis=1)\n",
        "df_te = pd.concat([pd.DataFrame({'t_title': test[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  test[body_col].astype(str).fillna('')}),\n",
        "                   meta_test.reset_index(drop=True)], axis=1)\n",
        "text_features_idx = [0, 1]\n",
        "\n",
        "params = dict(loss_function='Logloss', eval_metric='AUC', depth=6, learning_rate=0.05, l2_leaf_reg=5.0,\n",
        "              iterations=2000, early_stopping_rounds=200, random_seed=42, verbose=False)\n",
        "try:\n",
        "    from catboost.utils import get_gpu_device_count\n",
        "    params['task_type'] = 'GPU' if get_gpu_device_count() > 0 else 'CPU'\n",
        "except Exception:\n",
        "    params['task_type'] = 'CPU'\n",
        "print('CatBoost task_type:', params['task_type'], flush=True)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_cb = np.zeros(len(df_tr), dtype=np.float32)\n",
        "test_fold_preds = []\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(df_tr, y_cb), 1):\n",
        "    t0 = time.time()\n",
        "    tr_pool = cb.Pool(df_tr.iloc[trn_idx], label=y_cb[trn_idx], text_features=text_features_idx)\n",
        "    va_pool = cb.Pool(df_tr.iloc[val_idx], label=y_cb[val_idx], text_features=text_features_idx)\n",
        "    model = cb.CatBoostClassifier(**params)\n",
        "    model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n",
        "    oof_cb[val_idx] = model.predict_proba(va_pool)[:,1].astype(np.float32)\n",
        "    if fold == 1:\n",
        "        te_pool = cb.Pool(df_te, text_features=text_features_idx)\n",
        "    test_fold_preds.append(model.predict_proba(te_pool)[:,1].astype(np.float32))\n",
        "    print(f'Fold {fold}: AUC {roc_auc_score(y_cb[val_idx], oof_cb[val_idx]):.5f} | {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "oof_auc_cb = roc_auc_score(y_cb, oof_cb)\n",
        "print('CatBoost OOF AUC:', round(oof_auc_cb, 6))\n",
        "np.save('oof_catboost_text_meta.npy', oof_cb)\n",
        "test_cb = np.mean(np.vstack(test_fold_preds), axis=0).astype(np.float32)\n",
        "np.save('test_catboost_text_meta.npy', test_cb)\n",
        "\n",
        "# OOF-based blend with current best LR(meta+text/lex) if present\n",
        "if os.path.exists('oof_lr_text_meta.npy') and os.path.exists('test_lr_text_meta.npy'):\n",
        "    oof_lr = np.load('oof_lr_text_meta.npy')\n",
        "    test_lr = np.load('test_lr_text_meta.npy')\n",
        "    weights = np.linspace(0.0, 1.0, 41)\n",
        "    best_w, best_auc = 0.0, roc_auc_score(y_cb, oof_lr)\n",
        "    for w in weights:\n",
        "        a = roc_auc_score(y_cb, w*oof_cb + (1.0-w)*oof_lr)\n",
        "        if a > best_auc:\n",
        "            best_auc, best_w = a, w\n",
        "    print(f'Best OOF blend(LR,CatBoost) AUC: {best_auc:.5f} at w(CB)={best_w:.2f}', flush=True)\n",
        "    test_blend = best_w*test_cb + (1.0-best_w)*test_lr\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (LR+CatBoost blend)')\n",
        "else:\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_cb}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (CatBoost only)')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost task_type: GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: AUC 0.69379 | 8.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: AUC 0.69379 | 16.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: AUC 0.71684 | 15.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: AUC 0.65514 | 9.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5: AUC 0.71988 | 7.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost OOF AUC: 0.691649\nBest OOF blend(LR,CatBoost) AUC: 0.70600 at w(CB)=0.70\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (LR+CatBoost blend)\n"
          ]
        }
      ]
    },
    {
      "id": "5aebcc8c-807b-4512-b721-2c1cffa9b36b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost TEXT+META model: title/body as TEXT + meta(+lex) numeric; 5-fold CV; OOF-optimal blend with LR(meta+text)\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'catboost', '-q'], check=True)\n",
        "    import catboost as cb\n",
        "\n",
        "assert 'meta_train' in globals() and 'meta_test' in globals(), 'Build meta_train/meta_test first'\n",
        "assert title_col in train.columns and body_col in train.columns and target_col in train.columns, 'Missing columns'\n",
        "\n",
        "# Assemble frames: first two columns are TEXT, rest are numeric\n",
        "y_cb = train[target_col].astype(int).values\n",
        "df_tr = pd.concat([pd.DataFrame({'t_title': train[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  train[body_col].astype(str).fillna('')}),\n",
        "                   meta_train.reset_index(drop=True)], axis=1)\n",
        "df_te = pd.concat([pd.DataFrame({'t_title': test[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  test[body_col].astype(str).fillna('')}),\n",
        "                   meta_test.reset_index(drop=True)], axis=1)\n",
        "text_features_idx = [0, 1]\n",
        "\n",
        "# Params; prefer GPU if available\n",
        "params = dict(loss_function='Logloss', eval_metric='AUC',\n",
        "              depth=6, learning_rate=0.05, l2_leaf_reg=5.0,\n",
        "              iterations=2000, early_stopping_rounds=200,\n",
        "              random_seed=42, verbose=False)\n",
        "try:\n",
        "    from catboost.utils import get_gpu_device_count\n",
        "    params['task_type'] = 'GPU' if get_gpu_device_count() > 0 else 'CPU'\n",
        "except Exception:\n",
        "    params['task_type'] = 'CPU'\n",
        "print('CatBoost task_type:', params['task_type'], flush=True)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_cb = np.zeros(len(df_tr), dtype=np.float32)\n",
        "test_fold_preds = []\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(df_tr, y_cb), 1):\n",
        "    t0 = time.time()\n",
        "    tr_pool = cb.Pool(df_tr.iloc[trn_idx], label=y_cb[trn_idx], text_features=text_features_idx)\n",
        "    va_pool = cb.Pool(df_tr.iloc[val_idx], label=y_cb[val_idx], text_features=text_features_idx)\n",
        "    model = cb.CatBoostClassifier(**params)\n",
        "    model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n",
        "    oof_cb[val_idx] = model.predict_proba(va_pool)[:,1].astype(np.float32)\n",
        "    if fold == 1:\n",
        "        te_pool = cb.Pool(df_te, text_features=text_features_idx)\n",
        "    test_fold_preds.append(model.predict_proba(te_pool)[:,1].astype(np.float32))\n",
        "    print(f'Fold {fold}: AUC {roc_auc_score(y_cb[val_idx], oof_cb[val_idx]):.5f} | {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "oof_auc_cb = roc_auc_score(y_cb, oof_cb)\n",
        "print('CatBoost OOF AUC:', round(oof_auc_cb, 6))\n",
        "np.save('oof_catboost_text_meta.npy', oof_cb)\n",
        "test_cb = np.mean(np.vstack(test_fold_preds), axis=0).astype(np.float32)\n",
        "np.save('test_catboost_text_meta.npy', test_cb)\n",
        "\n",
        "# Blend with current best LR(meta+text) using OOF to pick weight\n",
        "if os.path.exists('oof_lr_text_meta.npy') and os.path.exists('test_lr_text_meta.npy'):\n",
        "    oof_lr = np.load('oof_lr_text_meta.npy')\n",
        "    test_lr = np.load('test_lr_text_meta.npy')\n",
        "    weights = np.linspace(0.0, 1.0, 41)\n",
        "    best_w, best_auc = 0.0, roc_auc_score(y_cb, oof_lr)\n",
        "    for w in weights:\n",
        "        a = roc_auc_score(y_cb, w*oof_cb + (1.0-w)*oof_lr)\n",
        "        if a > best_auc:\n",
        "            best_auc, best_w = a, w\n",
        "    print(f'Best OOF blend(LR,CatBoost) AUC: {best_auc:.5f} at w(CB)={best_w:.2f}', flush=True)\n",
        "    test_blend = best_w*test_cb + (1.0-best_w)*test_lr\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (LR+CatBoost blend)')\n",
        "else:\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_cb}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (CatBoost only)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "863f4eb9-c74e-4275-a4b9-27d3c1a0bf1a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost TEXT+META(+lex) 5-fold CV; GPU if available; OOF-optimal blend with LR(meta+text)\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'catboost', '-q'], check=True)\n",
        "    import catboost as cb\n",
        "\n",
        "assert 'meta_train' in globals() and 'meta_test' in globals(), 'Run meta feature cell first'\n",
        "assert title_col in train.columns and body_col in train.columns and target_col in train.columns, 'Missing columns'\n",
        "\n",
        "# Assemble DataFrames for CatBoost: first two columns are TEXT, rest are numeric meta(+lex) features\n",
        "df_tr = pd.concat([pd.DataFrame({'t_title': train[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  train[body_col].astype(str).fillna('')}),\n",
        "                   meta_train.reset_index(drop=True)], axis=1)\n",
        "df_te = pd.concat([pd.DataFrame({'t_title': test[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  test[body_col].astype(str).fillna('')}),\n",
        "                   meta_test.reset_index(drop=True)], axis=1)\n",
        "y_cb = train[target_col].astype(int).values\n",
        "text_features_idx = [0, 1]\n",
        "\n",
        "# CatBoost params; prefer GPU\n",
        "params = dict(\n",
        "    loss_function='Logloss',\n",
        "    eval_metric='AUC',\n",
        "    depth=6,\n",
        "    learning_rate=0.05,\n",
        "    l2_leaf_reg=5.0,\n",
        "    iterations=2000,\n",
        "    early_stopping_rounds=200,\n",
        "    random_seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "try:\n",
        "    from catboost.utils import get_gpu_device_count\n",
        "    params['task_type'] = 'GPU' if get_gpu_device_count() > 0 else 'CPU'\n",
        "except Exception:\n",
        "    params['task_type'] = 'CPU'\n",
        "print('CatBoost task_type:', params['task_type'], flush=True)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_cb = np.zeros(len(df_tr), dtype=np.float32)\n",
        "test_fold_preds = []\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(df_tr, y_cb), 1):\n",
        "    t0 = time.time()\n",
        "    tr_pool = cb.Pool(df_tr.iloc[trn_idx], label=y_cb[trn_idx], text_features=text_features_idx)\n",
        "    va_pool = cb.Pool(df_tr.iloc[val_idx], label=y_cb[val_idx], text_features=text_features_idx)\n",
        "    model = cb.CatBoostClassifier(**params)\n",
        "    model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n",
        "    oof_cb[val_idx] = model.predict_proba(va_pool)[:,1].astype(np.float32)\n",
        "    if fold == 1:\n",
        "        te_pool = cb.Pool(df_te, text_features=text_features_idx)\n",
        "    test_fold_preds.append(model.predict_proba(te_pool)[:,1].astype(np.float32))\n",
        "    print(f'Fold {fold}: AUC {roc_auc_score(y_cb[val_idx], oof_cb[val_idx]):.5f} | {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "oof_auc_cb = roc_auc_score(y_cb, oof_cb)\n",
        "print('CatBoost OOF AUC:', round(oof_auc_cb, 6))\n",
        "np.save('oof_catboost_text_meta.npy', oof_cb)\n",
        "test_cb = np.mean(np.vstack(test_fold_preds), axis=0).astype(np.float32)\n",
        "np.save('test_catboost_text_meta.npy', test_cb)\n",
        "\n",
        "# Blend with our best LR(meta+text[+lex/subs]) using OOF to pick weight\n",
        "if os.path.exists('oof_lr_text_meta.npy') and os.path.exists('test_lr_text_meta.npy'):\n",
        "    oof_lr = np.load('oof_lr_text_meta.npy')\n",
        "    test_lr = np.load('test_lr_text_meta.npy')\n",
        "    weights = np.linspace(0.0, 1.0, 41)\n",
        "    best_w, best_auc = 0.0, roc_auc_score(y_cb, oof_lr)\n",
        "    for w in weights:\n",
        "        a = roc_auc_score(y_cb, w*oof_cb + (1.0-w)*oof_lr)\n",
        "        if a > best_auc:\n",
        "            best_auc, best_w = a, w\n",
        "    print(f'Best OOF blend(LR(meta+text),CatBoost) AUC: {best_auc:.5f} at w(CB)={best_w:.2f}', flush=True)\n",
        "    test_blend = best_w*test_cb + (1.0-best_w)*test_lr\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (LR+CatBoost blend)')\n",
        "else:\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_cb}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (CatBoost only)')"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "id": "04834949-ff30-443b-8758-a8ade2dd4164",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost TEXT+META model: title/body as TEXT + meta(+lex); 5-fold CV; OOF-optimal blend with LR(meta+text)\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'catboost', '-q'], check=True)\n",
        "    import catboost as cb\n",
        "\n",
        "assert 'meta_train' in globals() and 'meta_test' in globals(), 'Run meta feature cell first'\n",
        "assert title_col in train.columns and body_col in train.columns and target_col in train.columns, 'Missing columns'\n",
        "\n",
        "# Assemble frames: first two columns are TEXT, rest are numeric\n",
        "y_cb = train[target_col].astype(int).values\n",
        "df_tr = pd.concat([pd.DataFrame({'t_title': train[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  train[body_col].astype(str).fillna('')}),\n",
        "                   meta_train.reset_index(drop=True)], axis=1)\n",
        "df_te = pd.concat([pd.DataFrame({'t_title': test[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  test[body_col].astype(str).fillna('')}),\n",
        "                   meta_test.reset_index(drop=True)], axis=1)\n",
        "text_features_idx = [0, 1]\n",
        "\n",
        "# Params; prefer GPU if available\n",
        "params = dict(\n",
        "    loss_function='Logloss',\n",
        "    eval_metric='AUC',\n",
        "    depth=6,\n",
        "    learning_rate=0.05,\n",
        "    l2_leaf_reg=5.0,\n",
        "    iterations=2000,\n",
        "    early_stopping_rounds=200,\n",
        "    random_seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "try:\n",
        "    from catboost.utils import get_gpu_device_count\n",
        "    params['task_type'] = 'GPU' if get_gpu_device_count() > 0 else 'CPU'\n",
        "except Exception:\n",
        "    params['task_type'] = 'CPU'\n",
        "print('CatBoost task_type:', params['task_type'], flush=True)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_cb = np.zeros(len(df_tr), dtype=np.float32)\n",
        "test_fold_preds = []\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(df_tr, y_cb), 1):\n",
        "    t0 = time.time()\n",
        "    tr_pool = cb.Pool(df_tr.iloc[trn_idx], label=y_cb[trn_idx], text_features=text_features_idx)\n",
        "    va_pool = cb.Pool(df_tr.iloc[val_idx], label=y_cb[val_idx], text_features=text_features_idx)\n",
        "    model = cb.CatBoostClassifier(**params)\n",
        "    model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n",
        "    oof_cb[val_idx] = model.predict_proba(va_pool)[:,1].astype(np.float32)\n",
        "    if fold == 1:\n",
        "        te_pool = cb.Pool(df_te, text_features=text_features_idx)\n",
        "    test_fold_preds.append(model.predict_proba(te_pool)[:,1].astype(np.float32))\n",
        "    print(f'Fold {fold}: AUC {roc_auc_score(y_cb[val_idx], oof_cb[val_idx]):.5f} | {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "oof_auc_cb = roc_auc_score(y_cb, oof_cb)\n",
        "print('CatBoost OOF AUC:', round(oof_auc_cb, 6))\n",
        "np.save('oof_catboost_text_meta.npy', oof_cb)\n",
        "test_cb = np.mean(np.vstack(test_fold_preds), axis=0).astype(np.float32)\n",
        "np.save('test_catboost_text_meta.npy', test_cb)\n",
        "\n",
        "# Blend with best LR(meta+text[+lex/subs]) using OOF to pick weight\n",
        "if os.path.exists('oof_lr_text_meta.npy') and os.path.exists('test_lr_text_meta.npy'):\n",
        "    oof_lr = np.load('oof_lr_text_meta.npy')\n",
        "    test_lr = np.load('test_lr_text_meta.npy')\n",
        "    weights = np.linspace(0.0, 1.0, 41)\n",
        "    best_w, best_auc = 0.0, roc_auc_score(y_cb, oof_lr)\n",
        "    for w in weights:\n",
        "        a = roc_auc_score(y_cb, w*oof_cb + (1.0-w)*oof_lr)\n",
        "        if a > best_auc:\n",
        "            best_auc, best_w = a, w\n",
        "    print(f'Best OOF blend(LR,CatBoost) AUC: {best_auc:.5f} at w(CB)={best_w:.2f}', flush=True)\n",
        "    test_blend = best_w*test_cb + (1.0-best_w)*test_lr\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (LR+CatBoost blend)')\n",
        "else:\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_cb}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (CatBoost only)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "6d32b19c-ca0c-4a59-985b-e97ec951b5d8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost TEXT+META(+lex) 5-fold CV (GPU if available) + OOF-optimal blend with LR(meta+text)\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'catboost', '-q'], check=True)\n",
        "    import catboost as cb\n",
        "\n",
        "assert 'meta_train' in globals() and 'meta_test' in globals(), 'Build meta_train/meta_test first'\n",
        "assert title_col in train.columns and body_col in train.columns and target_col in train.columns, 'Missing columns'\n",
        "\n",
        "y_cb = train[target_col].astype(int).values\n",
        "df_tr = pd.concat([pd.DataFrame({'t_title': train[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  train[body_col].astype(str).fillna('')}),\n",
        "                   meta_train.reset_index(drop=True)], axis=1)\n",
        "df_te = pd.concat([pd.DataFrame({'t_title': test[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  test[body_col].astype(str).fillna('')}),\n",
        "                   meta_test.reset_index(drop=True)], axis=1)\n",
        "text_features_idx = [0, 1]\n",
        "\n",
        "params = dict(\n",
        "    loss_function='Logloss',\n",
        "    eval_metric='AUC',\n",
        "    depth=6,\n",
        "    learning_rate=0.05,\n",
        "    l2_leaf_reg=5.0,\n",
        "    iterations=2000,\n",
        "    early_stopping_rounds=200,\n",
        "    random_seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "try:\n",
        "    from catboost.utils import get_gpu_device_count\n",
        "    params['task_type'] = 'GPU' if get_gpu_device_count() > 0 else 'CPU'\n",
        "except Exception:\n",
        "    params['task_type'] = 'CPU'\n",
        "print('CatBoost task_type:', params['task_type'], flush=True)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_cb = np.zeros(len(df_tr), dtype=np.float32)\n",
        "test_fold_preds = []\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(df_tr, y_cb), 1):\n",
        "    t0 = time.time()\n",
        "    tr_pool = cb.Pool(df_tr.iloc[trn_idx], label=y_cb[trn_idx], text_features=text_features_idx)\n",
        "    va_pool = cb.Pool(df_tr.iloc[val_idx], label=y_cb[val_idx], text_features=text_features_idx)\n",
        "    model = cb.CatBoostClassifier(**params)\n",
        "    model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n",
        "    oof_cb[val_idx] = model.predict_proba(va_pool)[:,1].astype(np.float32)\n",
        "    if fold == 1:\n",
        "        te_pool = cb.Pool(df_te, text_features=text_features_idx)\n",
        "    test_fold_preds.append(model.predict_proba(te_pool)[:,1].astype(np.float32))\n",
        "    print(f'Fold {fold}: AUC {roc_auc_score(y_cb[val_idx], oof_cb[val_idx]):.5f} | {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "oof_auc_cb = roc_auc_score(y_cb, oof_cb)\n",
        "print('CatBoost OOF AUC:', round(oof_auc_cb, 6))\n",
        "np.save('oof_catboost_text_meta.npy', oof_cb)\n",
        "test_cb = np.mean(np.vstack(test_fold_preds), axis=0).astype(np.float32)\n",
        "np.save('test_catboost_text_meta.npy', test_cb)\n",
        "\n",
        "# OOF-optimal blend with our LR(meta+text) if available\n",
        "if os.path.exists('oof_lr_text_meta.npy') and os.path.exists('test_lr_text_meta.npy'):\n",
        "    oof_lr = np.load('oof_lr_text_meta.npy')\n",
        "    test_lr = np.load('test_lr_text_meta.npy')\n",
        "    weights = np.linspace(0.0, 1.0, 41)\n",
        "    best_w, best_auc = 0.0, roc_auc_score(y_cb, oof_lr)\n",
        "    for w in weights:\n",
        "        a = roc_auc_score(y_cb, w*oof_cb + (1.0-w)*oof_lr)\n",
        "        if a > best_auc:\n",
        "            best_auc, best_w = a, w\n",
        "    print(f'Best OOF blend(LR(meta+text),CatBoost) AUC: {best_auc:.5f} at w(CB)={best_w:.2f}', flush=True)\n",
        "    test_blend = best_w*test_cb + (1.0-best_w)*test_lr\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (LR+CatBoost blend)')\n",
        "else:\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_cb}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (CatBoost only)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "25b5d912-477e-4e7f-bd50-fa45c11ba56b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost: native TEXT (title/body) + numeric meta(+lexicons); 5-fold CV; OOF-weighted blend with LR(meta+text)\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'catboost', '-q'], check=True)\n",
        "    import catboost as cb\n",
        "\n",
        "assert 'meta_train' in globals() and 'meta_test' in globals(), 'Build meta_train/meta_test first'\n",
        "assert title_col in train.columns and body_col in train.columns and target_col in train.columns, 'Missing columns'\n",
        "\n",
        "# Assemble frames: first two columns are TEXT, rest are numeric\n",
        "y_cb = train[target_col].astype(int).values\n",
        "df_tr = pd.concat([pd.DataFrame({'t_title': train[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  train[body_col].astype(str).fillna('')}),\n",
        "                   meta_train.reset_index(drop=True)], axis=1)\n",
        "df_te = pd.concat([pd.DataFrame({'t_title': test[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  test[body_col].astype(str).fillna('')}),\n",
        "                   meta_test.reset_index(drop=True)], axis=1)\n",
        "text_features_idx = [0, 1]\n",
        "\n",
        "# CatBoost params; prefer GPU if available\n",
        "params = dict(\n",
        "    loss_function='Logloss',\n",
        "    eval_metric='AUC',\n",
        "    depth=6,\n",
        "    learning_rate=0.05,\n",
        "    l2_leaf_reg=5.0,\n",
        "    iterations=2000,\n",
        "    early_stopping_rounds=200,\n",
        "    random_seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "try:\n",
        "    from catboost.utils import get_gpu_device_count\n",
        "    params['task_type'] = 'GPU' if get_gpu_device_count() > 0 else 'CPU'\n",
        "except Exception:\n",
        "    params['task_type'] = 'CPU'\n",
        "print('CatBoost task_type:', params['task_type'], flush=True)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_cb = np.zeros(len(df_tr), dtype=np.float32)\n",
        "test_fold_preds = []\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(df_tr, y_cb), 1):\n",
        "    t0 = time.time()\n",
        "    tr_pool = cb.Pool(df_tr.iloc[trn_idx], label=y_cb[trn_idx], text_features=text_features_idx)\n",
        "    va_pool = cb.Pool(df_tr.iloc[val_idx], label=y_cb[val_idx], text_features=text_features_idx)\n",
        "    model = cb.CatBoostClassifier(**params)\n",
        "    model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n",
        "    oof_cb[val_idx] = model.predict_proba(va_pool)[:,1].astype(np.float32)\n",
        "    if fold == 1:\n",
        "        te_pool = cb.Pool(df_te, text_features=text_features_idx)\n",
        "    test_fold_preds.append(model.predict_proba(te_pool)[:,1].astype(np.float32))\n",
        "    auc = roc_auc_score(y_cb[val_idx], oof_cb[val_idx])\n",
        "    print(f'Fold {fold}: AUC {auc:.5f} | {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "oof_auc_cb = roc_auc_score(y_cb, oof_cb)\n",
        "print('CatBoost OOF AUC:', round(oof_auc_cb, 6))\n",
        "np.save('oof_catboost_text_meta.npy', oof_cb)\n",
        "test_cb = np.mean(np.vstack(test_fold_preds), axis=0).astype(np.float32)\n",
        "np.save('test_catboost_text_meta.npy', test_cb)\n",
        "\n",
        "# Blend with best LR(meta+text) using OOF to pick weight\n",
        "if os.path.exists('oof_lr_text_meta.npy') and os.path.exists('test_lr_text_meta.npy'):\n",
        "    oof_lr = np.load('oof_lr_text_meta.npy')\n",
        "    test_lr = np.load('test_lr_text_meta.npy')\n",
        "    weights = np.linspace(0.0, 1.0, 41)\n",
        "    best_w, best_auc = 0.0, roc_auc_score(y_cb, oof_lr)\n",
        "    for w in weights:\n",
        "        a = roc_auc_score(y_cb, w*oof_cb + (1.0-w)*oof_lr)\n",
        "        if a > best_auc:\n",
        "            best_auc, best_w = a, w\n",
        "    print(f'Best OOF blend(LR(meta+text),CatBoost) AUC: {best_auc:.5f} at w(CB)={best_w:.2f}', flush=True)\n",
        "    test_blend = best_w*test_cb + (1.0-best_w)*test_lr\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (LR+CatBoost blend)')\n",
        "else:\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_cb}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (CatBoost only)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "19a0436b-4df2-4dcc-9671-631b375eba2e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost TEXT+META model (GPU if available): 5-fold CV, save OOF/test, OOF-optimal blend with LR(meta+text)\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.argv[0], '-m', 'pip', 'install', 'catboost', '-q'], check=True)\n",
        "    import catboost as cb\n",
        "\n",
        "assert 'meta_train' in globals() and 'meta_test' in globals(), 'Run meta feature cell first'\n",
        "assert title_col in train.columns and body_col in train.columns and target_col in train.columns, 'Missing columns'\n",
        "\n",
        "# Assemble frames: first two columns are TEXT, rest are numeric (meta+lex)\n",
        "y_cb = train[target_col].astype(int).values\n",
        "df_tr = pd.concat([pd.DataFrame({'t_title': train[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  train[body_col].astype(str).fillna('')}),\n",
        "                   meta_train.reset_index(drop=True)], axis=1)\n",
        "df_te = pd.concat([pd.DataFrame({'t_title': test[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  test[body_col].astype(str).fillna('')}),\n",
        "                   meta_test.reset_index(drop=True)], axis=1)\n",
        "text_features_idx = [0, 1]\n",
        "\n",
        "# CatBoost params (fast/robust); prefer GPU\n",
        "params = dict(\n",
        "    loss_function='Logloss',\n",
        "    eval_metric='AUC',\n",
        "    depth=6,\n",
        "    learning_rate=0.05,\n",
        "    l2_leaf_reg=5.0,\n",
        "    iterations=1200,\n",
        "    early_stopping_rounds=100,\n",
        "    random_seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "try:\n",
        "    from catboost.utils import get_gpu_device_count\n",
        "    params['task_type'] = 'GPU' if get_gpu_device_count() > 0 else 'CPU'\n",
        "except Exception:\n",
        "    params['task_type'] = 'CPU'\n",
        "print('CatBoost task_type:', params['task_type'], flush=True)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_cb = np.zeros(len(df_tr), dtype=np.float32)\n",
        "test_fold_preds = []\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(df_tr, y_cb), 1):\n",
        "    t0 = time.time()\n",
        "    tr_pool = cb.Pool(df_tr.iloc[trn_idx], label=y_cb[trn_idx], text_features=text_features_idx)\n",
        "    va_pool = cb.Pool(df_tr.iloc[val_idx], label=y_cb[val_idx], text_features=text_features_idx)\n",
        "    model = cb.CatBoostClassifier(**params)\n",
        "    model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n",
        "    oof_cb[val_idx] = model.predict_proba(va_pool)[:,1].astype(np.float32)\n",
        "    if fold == 1:\n",
        "        te_pool = cb.Pool(df_te, text_features=text_features_idx)\n",
        "    test_fold_preds.append(model.predict_proba(te_pool)[:,1].astype(np.float32))\n",
        "    print(f'Fold {fold}: AUC {roc_auc_score(y_cb[val_idx], oof_cb[val_idx]):.5f} | {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "oof_auc_cb = roc_auc_score(y_cb, oof_cb)\n",
        "print('CatBoost OOF AUC:', round(oof_auc_cb, 6))\n",
        "np.save('oof_catboost_text_meta.npy', oof_cb)\n",
        "test_cb = np.mean(np.vstack(test_fold_preds), axis=0).astype(np.float32)\n",
        "np.save('test_catboost_text_meta.npy', test_cb)\n",
        "\n",
        "# Blend with current best LR(meta+text) using OOF to pick weight\n",
        "if os.path.exists('oof_lr_text_meta.npy') and os.path.exists('test_lr_text_meta.npy'):\n",
        "    oof_lr = np.load('oof_lr_text_meta.npy')\n",
        "    test_lr = np.load('test_lr_text_meta.npy')\n",
        "    weights = np.linspace(0.0, 1.0, 41)\n",
        "    best_w, best_auc = 0.0, roc_auc_score(y_cb, oof_lr)\n",
        "    for w in weights:\n",
        "        a = roc_auc_score(y_cb, w*oof_cb + (1.0-w)*oof_lr)\n",
        "        if a > best_auc:\n",
        "            best_auc, best_w = a, w\n",
        "    print(f'Best OOF blend(LR,CatBoost) AUC: {best_auc:.5f} at w(CB)={best_w:.2f}', flush=True)\n",
        "    test_blend = best_w*test_cb + (1.0-best_w)*test_lr\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (LR+CatBoost blend)')\n",
        "else:\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_cb}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (CatBoost only)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "f7c32cb1-2f84-4049-934a-4671478e005e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost TEXT + META (+lexicons) 5-fold CV; GPU if available; OOF-optimal blend with LR(meta+text)\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'catboost', '-q'], check=True)\n",
        "    import catboost as cb\n",
        "\n",
        "assert 'meta_train' in globals() and 'meta_test' in globals(), 'Run meta feature cell first'\n",
        "assert title_col in train.columns and body_col in train.columns and target_col in train.columns, 'Missing columns'\n",
        "\n",
        "# Assemble frames: first two columns are TEXT, rest are numeric\n",
        "y_cb = train[target_col].astype(int).values\n",
        "df_tr = pd.concat([pd.DataFrame({'t_title': train[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  train[body_col].astype(str).fillna('')}),\n",
        "                   meta_train.reset_index(drop=True)], axis=1)\n",
        "df_te = pd.concat([pd.DataFrame({'t_title': test[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  test[body_col].astype(str).fillna('')}),\n",
        "                   meta_test.reset_index(drop=True)], axis=1)\n",
        "text_features_idx = [0, 1]\n",
        "\n",
        "# Params; prefer GPU\n",
        "params = dict(loss_function='Logloss', eval_metric='AUC',\n",
        "              depth=6, learning_rate=0.05, l2_leaf_reg=5.0,\n",
        "              iterations=2000, early_stopping_rounds=200,\n",
        "              random_seed=42, verbose=False)\n",
        "try:\n",
        "    from catboost.utils import get_gpu_device_count\n",
        "    params['task_type'] = 'GPU' if get_gpu_device_count() > 0 else 'CPU'\n",
        "except Exception:\n",
        "    params['task_type'] = 'CPU'\n",
        "print('CatBoost task_type:', params['task_type'], flush=True)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_cb = np.zeros(len(df_tr), dtype=np.float32)\n",
        "test_fold_preds = []\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(df_tr, y_cb), 1):\n",
        "    t0 = time.time()\n",
        "    tr_pool = cb.Pool(df_tr.iloc[trn_idx], label=y_cb[trn_idx], text_features=text_features_idx)\n",
        "    va_pool = cb.Pool(df_tr.iloc[val_idx], label=y_cb[val_idx], text_features=text_features_idx)\n",
        "    model = cb.CatBoostClassifier(**params)\n",
        "    model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n",
        "    oof_cb[val_idx] = model.predict_proba(va_pool)[:,1].astype(np.float32)\n",
        "    if fold == 1:\n",
        "        te_pool = cb.Pool(df_te, text_features=text_features_idx)\n",
        "    test_fold_preds.append(model.predict_proba(te_pool)[:,1].astype(np.float32))\n",
        "    print(f'Fold {fold}: AUC {roc_auc_score(y_cb[val_idx], oof_cb[val_idx]):.5f} | {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "oof_auc_cb = roc_auc_score(y_cb, oof_cb)\n",
        "print('CatBoost OOF AUC:', round(oof_auc_cb, 6))\n",
        "np.save('oof_catboost_text_meta.npy', oof_cb)\n",
        "test_cb = np.mean(np.vstack(test_fold_preds), axis=0).astype(np.float32)\n",
        "np.save('test_catboost_text_meta.npy', test_cb)\n",
        "\n",
        "# Blend with best LR(meta+text[+lex/subs]) using OOF to pick weight\n",
        "if os.path.exists('oof_lr_text_meta.npy') and os.path.exists('test_lr_text_meta.npy'):\n",
        "    oof_lr = np.load('oof_lr_text_meta.npy')\n",
        "    test_lr = np.load('test_lr_text_meta.npy')\n",
        "    weights = np.linspace(0.0, 1.0, 41)\n",
        "    best_w, best_auc = 0.0, roc_auc_score(y_cb, oof_lr)\n",
        "    for w in weights:\n",
        "        a = roc_auc_score(y_cb, w*oof_cb + (1.0-w)*oof_lr)\n",
        "        if a > best_auc:\n",
        "            best_auc, best_w = a, w\n",
        "    print(f'Best OOF blend(LR(meta+text),CatBoost) AUC: {best_auc:.5f} at w(CB)={best_w:.2f}', flush=True)\n",
        "    test_blend = best_w*test_cb + (1.0-best_w)*test_lr\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (LR+CatBoost blend)')\n",
        "else:\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_cb}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (CatBoost only)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "8e24a9a6-0fe0-49a3-87a3-b67c642663da",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost TEXT + META(+lex) 5-fold CV; prefer GPU; OOF-optimal blend with LR(meta+text); write submission.csv\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'catboost', '-q'], check=True)\n",
        "    import catboost as cb\n",
        "\n",
        "assert title_col in train.columns and body_col in train.columns, 'Missing title/body columns'\n",
        "assert target_col in train.columns, 'Missing target_col'\n",
        "assert 'meta_train' in globals() and 'meta_test' in globals(), 'Run meta feature cell first'\n",
        "\n",
        "# Assemble frames for CatBoost: first two columns are TEXT, rest numeric meta(+lex)\n",
        "df_tr = pd.concat([pd.DataFrame({'t_title': train[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  train[body_col].astype(str).fillna('')}),\n",
        "                   meta_train.reset_index(drop=True)], axis=1)\n",
        "df_te = pd.concat([pd.DataFrame({'t_title': test[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  test[body_col].astype(str).fillna('')}),\n",
        "                   meta_test.reset_index(drop=True)], axis=1)\n",
        "y_cb = train[target_col].astype(int).values\n",
        "text_features_idx = [0, 1]\n",
        "\n",
        "# Params; use GPU if available\n",
        "params = dict(loss_function='Logloss', eval_metric='AUC',\n",
        "              depth=6, learning_rate=0.05, l2_leaf_reg=5.0,\n",
        "              iterations=2000, early_stopping_rounds=200,\n",
        "              random_seed=42, verbose=False)\n",
        "try:\n",
        "    from catboost.utils import get_gpu_device_count\n",
        "    params['task_type'] = 'GPU' if get_gpu_device_count() > 0 else 'CPU'\n",
        "except Exception:\n",
        "    params['task_type'] = 'CPU'\n",
        "print('CatBoost task_type:', params['task_type'], flush=True)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_cb = np.zeros(len(df_tr), dtype=np.float32)\n",
        "test_fold_preds = []\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(df_tr, y_cb), 1):\n",
        "    t0 = time.time()\n",
        "    tr_pool = cb.Pool(df_tr.iloc[trn_idx], label=y_cb[trn_idx], text_features=text_features_idx)\n",
        "    va_pool = cb.Pool(df_tr.iloc[val_idx], label=y_cb[val_idx], text_features=text_features_idx)\n",
        "    model = cb.CatBoostClassifier(**params)\n",
        "    model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n",
        "    oof_cb[val_idx] = model.predict_proba(va_pool)[:,1].astype(np.float32)\n",
        "    if fold == 1:\n",
        "        te_pool = cb.Pool(df_te, text_features=text_features_idx)\n",
        "    test_fold_preds.append(model.predict_proba(te_pool)[:,1].astype(np.float32))\n",
        "    print(f'Fold {fold}: AUC {roc_auc_score(y_cb[val_idx], oof_cb[val_idx]):.5f} | {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "oof_auc_cb = roc_auc_score(y_cb, oof_cb)\n",
        "print('CatBoost OOF AUC:', round(oof_auc_cb, 6))\n",
        "np.save('oof_catboost_text_meta.npy', oof_cb)\n",
        "test_cb = np.mean(np.vstack(test_fold_preds), axis=0).astype(np.float32)\n",
        "np.save('test_catboost_text_meta.npy', test_cb)\n",
        "\n",
        "# Blend with current best LR(meta+text[+lex/subs]) using OOF to pick weight\n",
        "if os.path.exists('oof_lr_text_meta.npy') and os.path.exists('test_lr_text_meta.npy'):\n",
        "    oof_lr = np.load('oof_lr_text_meta.npy')\n",
        "    test_lr = np.load('test_lr_text_meta.npy')\n",
        "    weights = np.linspace(0.0, 1.0, 41)\n",
        "    best_w, best_auc = 0.0, roc_auc_score(y_cb, oof_lr)\n",
        "    for w in weights:\n",
        "        a = roc_auc_score(y_cb, w*oof_cb + (1.0-w)*oof_lr)\n",
        "        if a > best_auc:\n",
        "            best_auc, best_w = a, w\n",
        "    print(f'Best OOF blend(LR(meta+text),CatBoost) AUC: {best_auc:.5f} at w(CB)={best_w:.2f}', flush=True)\n",
        "    test_blend = best_w*test_cb + (1.0-best_w)*test_lr\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (LR+CatBoost blend)')\n",
        "else:\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_cb}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (CatBoost only)')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost task_type: GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: AUC 0.69379 | 8.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: AUC 0.69379 | 16.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: AUC 0.71684 | 15.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: AUC 0.65514 | 9.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5: AUC 0.71988 | 8.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost OOF AUC: 0.691649\nBest OOF blend(LR,CatBoost) AUC: 0.70600 at w(CB)=0.70\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (LR+CatBoost blend)\n"
          ]
        }
      ]
    },
    {
      "id": "2733d7aa-8bfc-4492-b79d-ba77e7d653cd",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost TEXT+META: use title/body as TEXT features + meta(+lex) numeric; 5-fold CV; OOF-optimal blend\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'catboost', '-q'], check=True)\n",
        "    import catboost as cb\n",
        "\n",
        "assert 'meta_train' in globals() and 'meta_test' in globals(), 'Build meta_train/meta_test first'\n",
        "assert title_col in train.columns and body_col in train.columns and target_col in train.columns, 'Missing columns'\n",
        "\n",
        "# Assemble frames: first two columns are TEXT, rest are numeric meta(+lexicons) already built\n",
        "y_cb = train[target_col].astype(int).values\n",
        "df_tr = pd.concat([pd.DataFrame({'t_title': train[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  train[body_col].astype(str).fillna('')}),\n",
        "                   meta_train.reset_index(drop=True)], axis=1)\n",
        "df_te = pd.concat([pd.DataFrame({'t_title': test[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  test[body_col].astype(str).fillna('')}),\n",
        "                   meta_test.reset_index(drop=True)], axis=1)\n",
        "text_features_idx = [0, 1]\n",
        "\n",
        "# CatBoost params (fast, robust); prefer GPU if available\n",
        "params = dict(\n",
        "    loss_function='Logloss',\n",
        "    eval_metric='AUC',\n",
        "    depth=6,\n",
        "    learning_rate=0.05,\n",
        "    l2_leaf_reg=5.0,\n",
        "    iterations=2000,\n",
        "    early_stopping_rounds=200,\n",
        "    random_seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "try:\n",
        "    from catboost.utils import get_gpu_device_count\n",
        "    params['task_type'] = 'GPU' if get_gpu_device_count() > 0 else 'CPU'\n",
        "except Exception:\n",
        "    params['task_type'] = 'CPU'\n",
        "print('CatBoost task_type:', params['task_type'], flush=True)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_cb = np.zeros(len(df_tr), dtype=np.float32)\n",
        "test_fold_preds = []\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(df_tr, y_cb), 1):\n",
        "    t0 = time.time()\n",
        "    tr_pool = cb.Pool(df_tr.iloc[trn_idx], label=y_cb[trn_idx], text_features=text_features_idx)\n",
        "    va_pool = cb.Pool(df_tr.iloc[val_idx], label=y_cb[val_idx], text_features=text_features_idx)\n",
        "    model = cb.CatBoostClassifier(**params)\n",
        "    model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n",
        "    oof_cb[val_idx] = model.predict_proba(va_pool)[:,1].astype(np.float32)\n",
        "    if fold == 1:\n",
        "        te_pool = cb.Pool(df_te, text_features=text_features_idx)\n",
        "    test_fold_preds.append(model.predict_proba(te_pool)[:,1].astype(np.float32))\n",
        "    auc = roc_auc_score(y_cb[val_idx], oof_cb[val_idx])\n",
        "    print(f'Fold {fold}: AUC {auc:.5f} | {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "oof_auc_cb = roc_auc_score(y_cb, oof_cb)\n",
        "print('CatBoost OOF AUC:', round(oof_auc_cb, 6))\n",
        "np.save('oof_catboost_text_meta.npy', oof_cb)\n",
        "test_cb = np.mean(np.vstack(test_fold_preds), axis=0).astype(np.float32)\n",
        "np.save('test_catboost_text_meta.npy', test_cb)\n",
        "\n",
        "# Blend with best LR(meta+text[+lex/subs]) using OOF to pick weight\n",
        "if os.path.exists('oof_lr_text_meta.npy') and os.path.exists('test_lr_text_meta.npy'):\n",
        "    oof_lr = np.load('oof_lr_text_meta.npy')\n",
        "    test_lr = np.load('test_lr_text_meta.npy')\n",
        "    weights = np.linspace(0.0, 1.0, 41)\n",
        "    best_w, best_auc = 0.0, roc_auc_score(y_cb, oof_lr)\n",
        "    for w in weights:\n",
        "        a = roc_auc_score(y_cb, w*oof_cb + (1.0-w)*oof_lr)\n",
        "        if a > best_auc:\n",
        "            best_auc, best_w = a, w\n",
        "    print(f'Best OOF blend(LR(meta+text),CatBoost) AUC: {best_auc:.5f} at w(CB)={best_w:.2f}', flush=True)\n",
        "    test_blend = best_w*test_cb + (1.0-best_w)*test_lr\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (LR+CatBoost blend)')\n",
        "else:\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_cb}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (CatBoost only)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "1c9a4589-3159-4f97-b3e4-230de331d2ae",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost TEXT+META model: title/body as TEXT + meta(+lex) numeric; 5-fold CV; OOF-optimal blend with LR(meta+text)\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'catboost', '-q'], check=True)\n",
        "    import catboost as cb\n",
        "\n",
        "assert 'meta_train' in globals() and 'meta_test' in globals(), 'Build meta_train/meta_test first'\n",
        "assert title_col in train.columns and body_col in train.columns and target_col in train.columns, 'Missing columns'\n",
        "\n",
        "# Assemble frames: first two columns are TEXT, rest are numeric\n",
        "y_cb = train[target_col].astype(int).values\n",
        "df_tr = pd.concat([pd.DataFrame({'t_title': train[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  train[body_col].astype(str).fillna('')}),\n",
        "                   meta_train.reset_index(drop=True)], axis=1)\n",
        "df_te = pd.concat([pd.DataFrame({'t_title': test[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  test[body_col].astype(str).fillna('')}),\n",
        "                   meta_test.reset_index(drop=True)], axis=1)\n",
        "text_features_idx = [0, 1]\n",
        "\n",
        "# Params; prefer GPU if available\n",
        "params = dict(loss_function='Logloss', eval_metric='AUC',\n",
        "              depth=6, learning_rate=0.05, l2_leaf_reg=5.0,\n",
        "              iterations=2000, early_stopping_rounds=200,\n",
        "              random_seed=42, verbose=False)\n",
        "try:\n",
        "    from catboost.utils import get_gpu_device_count\n",
        "    params['task_type'] = 'GPU' if get_gpu_device_count() > 0 else 'CPU'\n",
        "except Exception:\n",
        "    params['task_type'] = 'CPU'\n",
        "print('CatBoost task_type:', params['task_type'], flush=True)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_cb = np.zeros(len(df_tr), dtype=np.float32)\n",
        "test_fold_preds = []\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(df_tr, y_cb), 1):\n",
        "    t0 = time.time()\n",
        "    tr_pool = cb.Pool(df_tr.iloc[trn_idx], label=y_cb[trn_idx], text_features=text_features_idx)\n",
        "    va_pool = cb.Pool(df_tr.iloc[val_idx], label=y_cb[val_idx], text_features=text_features_idx)\n",
        "    model = cb.CatBoostClassifier(**params)\n",
        "    model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n",
        "    oof_cb[val_idx] = model.predict_proba(va_pool)[:,1].astype(np.float32)\n",
        "    if fold == 1:\n",
        "        te_pool = cb.Pool(df_te, text_features=text_features_idx)\n",
        "    test_fold_preds.append(model.predict_proba(te_pool)[:,1].astype(np.float32))\n",
        "    print(f'Fold {fold}: AUC {roc_auc_score(y_cb[val_idx], oof_cb[val_idx]):.5f} | {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "oof_auc_cb = roc_auc_score(y_cb, oof_cb)\n",
        "print('CatBoost OOF AUC:', round(oof_auc_cb, 6))\n",
        "np.save('oof_catboost_text_meta.npy', oof_cb)\n",
        "test_cb = np.mean(np.vstack(test_fold_preds), axis=0).astype(np.float32)\n",
        "np.save('test_catboost_text_meta.npy', test_cb)\n",
        "\n",
        "# Blend with best LR(meta+text[+lex/subs]) using OOF to pick weight\n",
        "if os.path.exists('oof_lr_text_meta.npy') and os.path.exists('test_lr_text_meta.npy'):\n",
        "    oof_lr = np.load('oof_lr_text_meta.npy')\n",
        "    test_lr = np.load('test_lr_text_meta.npy')\n",
        "    weights = np.linspace(0.0, 1.0, 41)\n",
        "    best_w, best_auc = 0.0, roc_auc_score(y_cb, oof_lr)\n",
        "    for w in weights:\n",
        "        a = roc_auc_score(y_cb, w*oof_cb + (1.0-w)*oof_lr)\n",
        "        if a > best_auc:\n",
        "            best_auc, best_w = a, w\n",
        "    print(f'Best OOF blend(LR,CatBoost) AUC: {best_auc:.5f} at w(CB)={best_w:.2f}', flush=True)\n",
        "    test_blend = best_w*test_cb + (1.0-best_w)*test_lr\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (LR+CatBoost blend)')\n",
        "else:\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_cb}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (CatBoost only)')"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "id": "044de88b-acb2-4a02-8869-38f2dbe8628a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost TEXT+META: title/body as TEXT + meta(+lex) numeric; 5-fold CV; OOF-optimal blend; save submission.csv\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'catboost', '-q'], check=True)\n",
        "    import catboost as cb\n",
        "\n",
        "assert 'meta_train' in globals() and 'meta_test' in globals(), 'Run meta feature cell first'\n",
        "assert title_col in train.columns and body_col in train.columns and target_col in train.columns, 'Missing columns'\n",
        "\n",
        "# Build frames: first two columns are TEXT, remainder numeric\n",
        "df_tr = pd.concat([pd.DataFrame({'t_title': train[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  train[body_col].astype(str).fillna('')}),\n",
        "                   meta_train.reset_index(drop=True)], axis=1)\n",
        "df_te = pd.concat([pd.DataFrame({'t_title': test[title_col].astype(str).fillna(''),\n",
        "                                 't_body':  test[body_col].astype(str).fillna('')}),\n",
        "                   meta_test.reset_index(drop=True)], axis=1)\n",
        "y_cb = train[target_col].astype(int).values\n",
        "text_features_idx = [0, 1]\n",
        "\n",
        "# Params; prefer GPU\n",
        "params = dict(loss_function='Logloss', eval_metric='AUC',\n",
        "              depth=6, learning_rate=0.05, l2_leaf_reg=5.0,\n",
        "              iterations=2000, early_stopping_rounds=200,\n",
        "              random_seed=42, verbose=False)\n",
        "try:\n",
        "    from catboost.utils import get_gpu_device_count\n",
        "    params['task_type'] = 'GPU' if get_gpu_device_count() > 0 else 'CPU'\n",
        "except Exception:\n",
        "    params['task_type'] = 'CPU'\n",
        "print('CatBoost task_type:', params['task_type'], flush=True)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_cb = np.zeros(len(df_tr), dtype=np.float32)\n",
        "test_fold_preds = []\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(df_tr, y_cb), 1):\n",
        "    t0 = time.time()\n",
        "    tr_pool = cb.Pool(df_tr.iloc[trn_idx], label=y_cb[trn_idx], text_features=text_features_idx)\n",
        "    va_pool = cb.Pool(df_tr.iloc[val_idx], label=y_cb[val_idx], text_features=text_features_idx)\n",
        "    model = cb.CatBoostClassifier(**params)\n",
        "    model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n",
        "    oof_cb[val_idx] = model.predict_proba(va_pool)[:,1].astype(np.float32)\n",
        "    if fold == 1:\n",
        "        te_pool = cb.Pool(df_te, text_features=text_features_idx)\n",
        "    test_fold_preds.append(model.predict_proba(te_pool)[:,1].astype(np.float32))\n",
        "    print(f'Fold {fold}: AUC {roc_auc_score(y_cb[val_idx], oof_cb[val_idx]):.5f} | {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "oof_auc_cb = roc_auc_score(y_cb, oof_cb)\n",
        "print('CatBoost OOF AUC:', round(oof_auc_cb, 6))\n",
        "np.save('oof_catboost_text_meta.npy', oof_cb)\n",
        "test_cb = np.mean(np.vstack(test_fold_preds), axis=0).astype(np.float32)\n",
        "np.save('test_catboost_text_meta.npy', test_cb)\n",
        "\n",
        "# Blend with best LR(meta+text[+lex/subs]) via OOF\n",
        "if os.path.exists('oof_lr_text_meta.npy') and os.path.exists('test_lr_text_meta.npy'):\n",
        "    oof_lr = np.load('oof_lr_text_meta.npy')\n",
        "    test_lr = np.load('test_lr_text_meta.npy')\n",
        "    weights = np.linspace(0.0, 1.0, 41)\n",
        "    best_w, best_auc = 0.0, roc_auc_score(y_cb, oof_lr)\n",
        "    for w in weights:\n",
        "        a = roc_auc_score(y_cb, w*oof_cb + (1.0-w)*oof_lr)\n",
        "        if a > best_auc:\n",
        "            best_auc, best_w = a, w\n",
        "    print(f'Best OOF blend(LR(meta+text),CatBoost) AUC: {best_auc:.5f} at w(CB)={best_w:.2f}', flush=True)\n",
        "    test_blend = best_w*test_cb + (1.0-best_w)*test_lr\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (LR+CatBoost blend)')\n",
        "else:\n",
        "    pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_cb}).to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (CatBoost only)')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost task_type: GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: AUC 0.69379 | 8.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: AUC 0.69379 | 16.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: AUC 0.71684 | 15.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: AUC 0.65514 | 9.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5: AUC 0.71988 | 8.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost OOF AUC: 0.691649\nBest OOF blend(LR(meta+text),CatBoost) AUC: 0.70600 at w(CB)=0.70\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (LR+CatBoost blend)\n"
          ]
        }
      ]
    },
    {
      "id": "f3ead647-69bb-49ca-94c5-8e0ced263264",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost TEXT model (CPU) with StratifiedGroupKFold and OOF-weighted blend with LR(meta+text+subs)\n",
        "import sys, subprocess, time, numpy as np, pandas as pd, os, re\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Force a stable CatBoost version (avoid potential segfaults), then import\n",
        "try:\n",
        "    import catboost as cb\n",
        "    _ver = getattr(cb, '__version__', 'unknown')\n",
        "    print('Existing catboost version:', _ver, flush=True)\n",
        "except Exception:\n",
        "    _ver = None\n",
        "\n",
        "if True:\n",
        "    print('Installing stable CatBoost 1.2.5 (CPU)...', flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', '--no-cache-dir', '--upgrade', 'catboost==1.2.5'], check=True)\n",
        "    import importlib, catboost as cb\n",
        "    importlib.reload(cb)\n",
        "    print('CatBoost version now:', cb.__version__, flush=True)\n",
        "\n",
        "# --- Ensure data and key columns are available (make cell self-contained) ---\n",
        "def load_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path)\n",
        "    except ValueError:\n",
        "        import json as _json\n",
        "        with open(path, 'r') as f:\n",
        "            return pd.DataFrame(_json.load(f))\n",
        "\n",
        "if 'train' not in globals() or 'test' not in globals():\n",
        "    train = load_df('train.json')\n",
        "    test = load_df('test.json')\n",
        "\n",
        "id_col = 'request_id' if 'request_id' in train.columns else train.columns[0]\n",
        "target_col = 'requester_received_pizza' if 'requester_received_pizza' in train.columns else [c for c in train.columns if c.lower() in ('target','label')][0]\n",
        "body_fields_order = ['request_text_edit_aware','request_text','request_text_edit_aware_unnormalized']\n",
        "title_fields = ['request_title','title']\n",
        "body_col = next((c for c in body_fields_order if c in train.columns), None)\n",
        "title_col = next((c for c in title_fields if c in train.columns), None)\n",
        "\n",
        "# --- Build CatBoost text-only input (avoid mixed-type pool to improve stability) ---\n",
        "def join_subs_for_cat(df: pd.DataFrame) -> pd.Series:\n",
        "    if 'requester_subreddits_at_request' not in df.columns:\n",
        "        return pd.Series(['']*len(df))\n",
        "    def to_line(x):\n",
        "        if isinstance(x, list):\n",
        "            return ' '.join([str(s) for s in x if isinstance(s, str)])\n",
        "        return ''\n",
        "    return df['requester_subreddits_at_request'].apply(to_line)\n",
        "\n",
        "cat_text_train = pd.DataFrame({\n",
        "    'cb_title': (train[title_col].fillna('').astype(str) + ' ' + train[title_col].fillna('').astype(str)),  # upweight title\n",
        "    'cb_body': train[body_col].fillna('').astype(str),\n",
        "    'cb_subs': join_subs_for_cat(train)\n",
        "})\n",
        "cat_text_test = pd.DataFrame({\n",
        "    'cb_title': (test[title_col].fillna('').astype(str) + ' ' + test[title_col].fillna('').astype(str)),\n",
        "    'cb_body': test[body_col].fillna('').astype(str),\n",
        "    'cb_subs': join_subs_for_cat(test)\n",
        "})\n",
        "\n",
        "X_cat = cat_text_train.reset_index(drop=True)\n",
        "X_cat_test = cat_text_test.reset_index(drop=True)\n",
        "text_feature_indices = [0, 1, 2]\n",
        "y_cb = train[target_col].astype(int).values\n",
        "groups = train['requester_username'].fillna('').astype(str).values if 'requester_username' in train.columns else np.arange(len(train))\n",
        "\n",
        "print('CatBoost TEXT-only features:', X_cat.shape, 'Test:', X_cat_test.shape, flush=True)\n",
        "\n",
        "# Two CatBoost configs (A and B) per expert advice, simplified text_processing (use defaults for stability)\n",
        "common_params = dict(\n",
        "    task_type='CPU',\n",
        "    eval_metric='AUC',\n",
        "    loss_function='Logloss',\n",
        "    auto_class_weights='Balanced',\n",
        "    early_stopping_rounds=100,\n",
        "    bootstrap_type='Bayesian',\n",
        "    random_seed=42,\n",
        "    verbose=100,\n",
        "    allow_writing_files=False,\n",
        "    thread_count=-1\n",
        ")\n",
        "\n",
        "# Config A\n",
        "params_A = dict(common_params, **{\n",
        "    'depth': 6,\n",
        "    'learning_rate': 0.04,\n",
        "    'l2_leaf_reg': 4,\n",
        "    'iterations': 1000,  # slightly reduced to improve stability/speed\n",
        "    'bagging_temperature': 0.2,\n",
        "    'rsm': 0.8,\n",
        "})\n",
        "\n",
        "# Config B (regularized)\n",
        "params_B = dict(common_params, **{\n",
        "    'depth': 5,\n",
        "    'learning_rate': 0.05,\n",
        "    'l2_leaf_reg': 6,\n",
        "    'iterations': 1200,\n",
        "    'bagging_temperature': 0.5,\n",
        "    'rsm': 0.9,\n",
        "})\n",
        "\n",
        "def run_cb_cv_textonly(params: dict, name: str):\n",
        "    skf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    oof = np.zeros(len(X_cat), dtype=np.float32)\n",
        "    test_preds = []\n",
        "    for fold, (trn_idx, val_idx) in enumerate(skf.split(X_cat, y_cb, groups), 1):\n",
        "        t0 = time.time()\n",
        "        train_pool = cb.Pool(data=X_cat.iloc[trn_idx], label=y_cb[trn_idx], text_features=text_feature_indices)\n",
        "        valid_pool = cb.Pool(data=X_cat.iloc[val_idx], label=y_cb[val_idx], text_features=text_feature_indices)\n",
        "        model = cb.CatBoostClassifier(**params)\n",
        "        model.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n",
        "        oof[val_idx] = model.predict_proba(valid_pool)[:,1]\n",
        "        test_pool = cb.Pool(data=X_cat_test, text_features=text_feature_indices)\n",
        "        test_preds.append(model.predict_proba(test_pool)[:,1].astype(np.float32))\n",
        "        print(f'[CB {name}] Fold {fold} done in {time.time()-t0:.1f}s | val AUC={roc_auc_score(y_cb[val_idx], oof[val_idx]):.5f}', flush=True)\n",
        "    auc = roc_auc_score(y_cb, oof)\n",
        "    test_mean = np.mean(np.vstack(test_preds), axis=0).astype(np.float32)\n",
        "    print(f'[CB {name}] OOF AUC: {auc:.5f}', flush=True)\n",
        "    return oof, test_mean, auc\n",
        "\n",
        "oof_A, test_A, auc_A = run_cb_cv_textonly(params_A, 'A')\n",
        "oof_B, test_B, auc_B = run_cb_cv_textonly(params_B, 'B')\n",
        "\n",
        "# Choose best CB by OOF; if close (<=0.001), average them for stability\n",
        "if abs(auc_A - auc_B) <= 0.001:\n",
        "    oof_cat = 0.5*(oof_A + oof_B)\n",
        "    test_cat = 0.5*(test_A + test_B)\n",
        "    auc_cat = roc_auc_score(y_cb, oof_cat)\n",
        "    chosen = 'Averaged A+B'\n",
        "else:\n",
        "    if auc_A >= auc_B:\n",
        "        oof_cat, test_cat, auc_cat, chosen = oof_A, test_A, auc_A, 'A'\n",
        "    else:\n",
        "        oof_cat, test_cat, auc_cat, chosen = oof_B, test_B, auc_B, 'B'\n",
        "print(f'Chosen CB variant: {chosen} | OOF AUC: {auc_cat:.5f}', flush=True)\n",
        "np.save('oof_catboost.npy', oof_cat)\n",
        "np.save('test_catboost.npy', test_cat)\n",
        "\n",
        "# Build best LR base OOF via blending base and subs (recompute best weight) for fair comparison\n",
        "oof_lr_base = np.load('oof_lr_text_meta.npy') if os.path.exists('oof_lr_text_meta.npy') else None\n",
        "oof_lr_subs = np.load('oof_lr_text_meta_subs.npy') if os.path.exists('oof_lr_text_meta_subs.npy') else None\n",
        "if oof_lr_base is not None and oof_lr_subs is not None:\n",
        "    best_w_bs, best_auc_bs = 0.0, -1.0\n",
        "    for w in np.linspace(0.0, 1.0, 21):\n",
        "        blend_bs = (1.0-w)*oof_lr_base + w*oof_lr_subs\n",
        "        a = roc_auc_score(y_cb, blend_bs)\n",
        "        if a > best_auc_bs:\n",
        "            best_auc_bs, best_w_bs = a, w\n",
        "    print(f'Best OOF AUC (LR base vs subs): {best_auc_bs:.5f} at w(subs)={best_w_bs:.2f}', flush=True)\n",
        "    oof_lr_best = (1.0-best_w_bs)*oof_lr_base + best_w_bs*oof_lr_subs\n",
        "    test_lr_base = np.load('test_lr_text_meta.npy') if os.path.exists('test_lr_text_meta.npy') else None\n",
        "    test_lr_subs = np.load('test_lr_text_meta_subs.npy') if os.path.exists('test_lr_text_meta_subs.npy') else None\n",
        "    if test_lr_base is not None and test_lr_subs is not None:\n",
        "        test_lr_best = (1.0-best_w_bs)*test_lr_base + best_w_bs*test_lr_subs\n",
        "    else:\n",
        "        test_lr_best = test_lr_base if test_lr_base is not None else test_cat\n",
        "else:\n",
        "    if os.path.exists('oof_lr_text_meta.npy'):\n",
        "        oof_lr_best = np.load('oof_lr_text_meta.npy')\n",
        "        test_lr_best = np.load('test_lr_text_meta.npy')\n",
        "    else:\n",
        "        oof_lr_best = oof_cat\n",
        "        test_lr_best = test_cat\n",
        "\n",
        "# Blend CatBoost with LR-best using OOF to choose CatBoost weight in [0.10, 0.70]\n",
        "best_w, best_auc = None, -1.0\n",
        "for w in np.arange(0.10, 0.75, 0.05):\n",
        "    blend = w*oof_cat + (1.0-w)*oof_lr_best\n",
        "    a = roc_auc_score(y_cb, blend)\n",
        "    if a > best_auc:\n",
        "        best_auc, best_w = a, w\n",
        "print(f'Best OOF AUC blend(CB, LR-best)={best_auc:.5f} at w(CB)={best_w:.2f}', flush=True)\n",
        "test_blend = best_w*test_cat + (1.0-best_w)*test_lr_best\n",
        "pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend.astype(np.float32)}).to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (CatBoost blend). Rows:', len(test_blend))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost features: (2878, 45) Test: (1162, 45)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KernelDied",
          "evalue": "Kernel died unexpectedly.",
          "traceback": []
        }
      ]
    },
    {
      "id": "19e39a5a-da13-463b-91ad-09018bfb6400",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# XGBoost on TF-IDF SVD (word+char) + meta(+lexicons), StratifiedGroupKFold; blend with LR-best\n",
        "import sys, subprocess, time, numpy as np, pandas as pd, os, re\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Ensure xgboost installed\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    print('Installing xgboost...', flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "\n",
        "# --- Make cell self-contained: load data, define key columns, texts, and meta/lexicons ---\n",
        "def load_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path)\n",
        "    except ValueError:\n",
        "        import json as _json\n",
        "        with open(path, 'r') as f:\n",
        "            return pd.DataFrame(_json.load(f))\n",
        "\n",
        "if 'train' not in globals() or 'test' not in globals():\n",
        "    train = load_df('train.json')\n",
        "    test = load_df('test.json')\n",
        "\n",
        "target_col = 'requester_received_pizza' if 'requester_received_pizza' in train.columns else [c for c in train.columns if c.lower() in ('target','label')][0]\n",
        "body_fields_order = ['request_text_edit_aware','request_text','request_text_edit_aware_unnormalized']\n",
        "title_fields = ['request_title','title']\n",
        "body_col = next((c for c in body_fields_order if c in train.columns), None)\n",
        "title_col = next((c for c in title_fields if c in train.columns), None)\n",
        "\n",
        "def build_text_upweighted(df: pd.DataFrame, title_col: str, body_col: str) -> pd.Series:\n",
        "    title = df[title_col].fillna('').astype(str)\n",
        "    body = df[body_col].fillna('').astype(str)\n",
        "    return title + ' ' + title + ' [SEP] ' + body\n",
        "\n",
        "full_text_train2 = build_text_upweighted(train, title_col, body_col)\n",
        "full_text_test2 = build_text_upweighted(test, title_col, body_col)\n",
        "\n",
        "# Meta/lexicon builders (leakage-safe; mirror prior cell) \n",
        "def count_urls(s: str) -> int: return len(re.findall(r'https?://\\S+', s or ''))\n",
        "def has_imgur(s: str) -> int: return 1 if re.search(r'imgur\\.com', s or '', flags=re.IGNORECASE) else 0\n",
        "def count_digits(s: str) -> int: return sum(ch.isdigit() for ch in (s or ''))\n",
        "def dollar_flag(s: str) -> int: return 1 if ('$' in (s or '')) or re.search(r'\\b\\d+\\s*(dollars|bucks)\\b', s or '', flags=re.IGNORECASE) else 0\n",
        "def caps_ratio(s: str) -> float:\n",
        "    if not s: return 0.0\n",
        "    letters = [c for c in s if c.isalpha()]\n",
        "    return (sum(c.isupper() for c in letters) / max(1, len(letters))) if letters else 0.0\n",
        "def word_count(s: str) -> int: return len((s or '').split())\n",
        "def exclam_count(s: str) -> int: return (s or '').count('!')\n",
        "def question_count(s: str) -> int: return (s or '').count('?')\n",
        "def parse_subreddit_count(x) -> int: return len(x) if isinstance(x, list) else 0\n",
        "def safe_log1p_signed(x): return np.sign(x) * np.log1p(np.abs(x))\n",
        "\n",
        "LEX_PATTERNS = {\n",
        "    'lex_please': r'\\bplease\\b',\n",
        "    'lex_thanks': r'\\b(thank you|thanks in advance|thanks|thank|tia)\\b',\n",
        "    'lex_appreciate': r'\\b(appreciate|appreciated)\\b',\n",
        "    'lex_pay_it_forward': r'\\b(pay it forward|return the favor)\\b',\n",
        "    'lex_repay': r'\\b(repay|pay you back|pay back)\\b',\n",
        "    'lex_willing': r\"\\b(willing to|i\\'ll|i will|i can)\\b\",\n",
        "    'lex_karma': r'\\bkarma\\b',\n",
        "    'lex_evidence': r'\\b(proof|receipt|photo|picture|pic|verify|verification|evidence)\\b',\n",
        "    'lex_imgur_word': r'\\bimgur\\b',\n",
        "    'lex_student': r'\\b(student|college|university|finals|exam|midterm)\\b',\n",
        "    'lex_jobloss': r'\\b(unemployed|laid off|lost my job|between jobs|job hunt)\\b',\n",
        "    'lex_broke': r'\\b(broke)\\b',\n",
        "    'lex_rent_bills': r'\\b(rent|bill|bills|utilities|electric|gas|water|paycheck)\\b',\n",
        "    'lex_family': r'\\b(family|kids?|children|baby|pregnant|son|daughter|wife|husband)\\b',\n",
        "    'lex_hungry': r'\\b(hungry|starving|no food)\\b|\\bfood (stamps|pantry)\\b',\n",
        "    'lex_struggling': r'\\b(desperate|struggling)\\b',\n",
        "    'lex_urgency': r'\\b(tonight|today|tomorrow|asap|urgent)\\b',\n",
        "    'lex_help': r'\\bhelp\\b',\n",
        "}\n",
        "\n",
        "def add_lexicons(df: pd.DataFrame, title_col: str, body_col: str) -> pd.DataFrame:\n",
        "    txt = (df[title_col].fillna('').astype(str) + ' ' + df[body_col].fillna('').astype(str)).str.lower()\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    for name, pat in LEX_PATTERNS.items():\n",
        "        out[name] = txt.str.count(pat, flags=re.IGNORECASE).astype(float)\n",
        "    return out\n",
        "\n",
        "def build_meta(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    title = df[title_col].fillna('').astype(str)\n",
        "    body = df[body_col].fillna('').astype(str)\n",
        "    ts = pd.to_numeric(df['unix_timestamp_of_request'], errors='coerce')\n",
        "    dt = pd.to_datetime(ts, unit='s', utc=True)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    keep_cols = [\n",
        "        'requester_account_age_in_days_at_request',\n",
        "        'requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request',\n",
        "        'requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request',\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "    ]\n",
        "    for c in keep_cols:\n",
        "        out[c] = pd.to_numeric(df[c], errors='coerce') if c in df.columns else 0.0\n",
        "    out['requester_subreddits_count'] = df['requester_subreddits_at_request'].apply(parse_subreddit_count).astype(float) if 'requester_subreddits_at_request' in df.columns else 0.0\n",
        "    out['hour'] = dt.dt.hour.astype(float)\n",
        "    out['weekday'] = dt.dt.weekday.astype(float)\n",
        "    out['is_weekend'] = dt.dt.weekday.isin([5,6]).astype(float)\n",
        "    out['title_len_chars'] = title.str.len().astype(float)\n",
        "    out['title_len_words'] = title.apply(word_count).astype(float)\n",
        "    out['body_len_chars'] = body.str.len().astype(float)\n",
        "    out['body_len_words'] = body.apply(word_count).astype(float)\n",
        "    out['url_count'] = body.apply(count_urls).astype(float)\n",
        "    out['has_imgur'] = body.apply(has_imgur).astype(float)\n",
        "    out['digits_count'] = body.apply(count_digits).astype(float)\n",
        "    out['dollar_flag'] = body.apply(dollar_flag).astype(float)\n",
        "    out['caps_ratio'] = body.apply(caps_ratio).astype(float)\n",
        "    out['exclam_count'] = body.apply(exclam_count).astype(float)\n",
        "    out['question_count'] = body.apply(question_count).astype(float)\n",
        "    lex = add_lexicons(df, title_col, body_col)\n",
        "    out = pd.concat([out, lex], axis=1)\n",
        "    out = out.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "    nonneg_cols = [\n",
        "        'requester_account_age_in_days_at_request','requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request','requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request','requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request','requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_subreddits_count','hour','weekday','is_weekend',\n",
        "        'title_len_chars','title_len_words','body_len_chars','body_len_words',\n",
        "        'url_count','has_imgur','digits_count','dollar_flag','exclam_count','question_count',\n",
        "        *list(LEX_PATTERNS.keys()),\n",
        "    ]\n",
        "    for c in nonneg_cols:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(np.clip(out[c], a_min=0.0, a_max=None))\n",
        "    if 'requester_upvotes_minus_downvotes_at_request' in out.columns:\n",
        "        out['requester_upvotes_minus_downvotes_at_request'] = safe_log1p_signed(out['requester_upvotes_minus_downvotes_at_request'].values)\n",
        "    out = out.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
        "    return out\n",
        "\n",
        "meta_train = build_meta(train)\n",
        "meta_test = build_meta(test)\n",
        "\n",
        "# === XGBoost SVD pipeline ===\n",
        "def get_vecs_for_svd():\n",
        "    wv = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.95, max_features=200000,\n",
        "                         sublinear_tf=True, lowercase=True, dtype=np.float32)\n",
        "    cv = TfidfVectorizer(analyzer='char', ngram_range=(3,6), min_df=2, max_features=300000,\n",
        "                         sublinear_tf=True, lowercase=True, dtype=np.float32)\n",
        "    return wv, cv\n",
        "\n",
        "def build_fold_features(tr_text, va_text, tr_meta, va_meta, n_comp_word=200, n_comp_char=200):\n",
        "    wv, cv = get_vecs_for_svd()\n",
        "    Xw_tr = wv.fit_transform(tr_text)\n",
        "    Xc_tr = cv.fit_transform(tr_text)\n",
        "    Xw_va = wv.transform(va_text)\n",
        "    Xc_va = cv.transform(va_text)\n",
        "    svd_w = TruncatedSVD(n_components=n_comp_word, random_state=42)\n",
        "    svd_c = TruncatedSVD(n_components=n_comp_char, random_state=42)\n",
        "    Zw_tr = svd_w.fit_transform(Xw_tr).astype(np.float32)\n",
        "    Zc_tr = svd_c.fit_transform(Xc_tr).astype(np.float32)\n",
        "    Zw_va = svd_w.transform(Xw_va).astype(np.float32)\n",
        "    Zc_va = svd_c.transform(Xc_va).astype(np.float32)\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    M_tr = scaler.fit_transform(tr_meta).astype(np.float32)\n",
        "    M_va = scaler.transform(va_meta).astype(np.float32)\n",
        "    X_tr = np.hstack([Zw_tr, Zc_tr, M_tr]).astype(np.float32)\n",
        "    X_va = np.hstack([Zw_va, Zc_va, M_va]).astype(np.float32)\n",
        "    return X_tr, X_va, (wv, cv, svd_w, svd_c, scaler)\n",
        "\n",
        "def build_test_features(text_train, text_test, transformers, meta_train, meta_test):\n",
        "    wv, cv, svd_w, svd_c, scaler = transformers\n",
        "    Xw_tr = wv.fit_transform(text_train)\n",
        "    Xc_tr = cv.fit_transform(text_train)\n",
        "    Xw_te = wv.transform(text_test)\n",
        "    Xc_te = cv.transform(text_test)\n",
        "    Zw_te = svd_w.fit(Xw_tr).transform(Xw_te).astype(np.float32)\n",
        "    Zc_te = svd_c.fit(Xc_tr).transform(Xc_te).astype(np.float32)\n",
        "    M_te = scaler.fit(meta_train).transform(meta_test).astype(np.float32)\n",
        "    X_te = np.hstack([Zw_te, Zc_te, M_te]).astype(np.float32)\n",
        "    return X_te\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "groups = train['requester_username'].fillna('').astype(str).values if 'requester_username' in train.columns else np.arange(len(train))\n",
        "skf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "print('=== CV: XGBoost on (SVD-word,char) + meta ===', flush=True)\n",
        "oof_xgb = np.zeros(len(train), dtype=np.float32)\n",
        "test_preds = []\n",
        "fold_times = []\n",
        "\n",
        "params = dict(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='auc',\n",
        "    tree_method='hist',\n",
        "    max_depth=5,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.0,\n",
        "    reg_lambda=1.0,\n",
        "    max_bin=256,\n",
        "    min_child_weight=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(full_text_train2, y, groups), 1):\n",
        "    t0 = time.time()\n",
        "    X_tr, X_va, pipes = build_fold_features(\n",
        "        full_text_train2.iloc[trn_idx],\n",
        "        full_text_train2.iloc[val_idx],\n",
        "        meta_train.iloc[trn_idx].values,\n",
        "        meta_train.iloc[val_idx].values,\n",
        "        n_comp_word=200, n_comp_char=200\n",
        "    )\n",
        "    dtr = xgb.DMatrix(X_tr, label=y[trn_idx])\n",
        "    dva = xgb.DMatrix(X_va, label=y[val_idx])\n",
        "    booster = xgb.train(params, dtr, num_boost_round=2000, evals=[(dva, 'valid')],\n",
        "                        verbose_eval=False,\n",
        "                        early_stopping_rounds=100)\n",
        "    oof_xgb[val_idx] = booster.predict(dva, iteration_range=(0, booster.best_iteration+1))\n",
        "    # Build test features for this fold using the same pipelines fit pattern\n",
        "    Xt = build_test_features(full_text_train2, full_text_test2, pipes, meta_train.values, meta_test.values)\n",
        "    dte = xgb.DMatrix(Xt)\n",
        "    test_preds.append(booster.predict(dte, iteration_range=(0, booster.best_iteration+1)).astype(np.float32))\n",
        "    dt = time.time()-t0\n",
        "    fold_times.append(dt)\n",
        "    print(f'  Fold {fold} AUC={roc_auc_score(y[val_idx], oof_xgb[val_idx]):.5f} | iters={booster.best_iteration+1} | {dt:.1f}s', flush=True)\n",
        "\n",
        "auc_xgb = roc_auc_score(y, oof_xgb)\n",
        "print(f'OOF AUC (XGB SVD+meta): {auc_xgb:.5f} | mean fold time: {np.mean(fold_times):.1f}s', flush=True)\n",
        "test_xgb = np.mean(np.vstack(test_preds), axis=0).astype(np.float32)\n",
        "np.save('oof_xgb_svd_meta.npy', oof_xgb)\n",
        "np.save('test_xgb_svd_meta.npy', test_xgb)\n",
        "\n",
        "# Blend with LR-best (base vs subs optimized) using OOF to choose weight\n",
        "oof_lr_base = np.load('oof_lr_text_meta.npy') if os.path.exists('oof_lr_text_meta.npy') else None\n",
        "oof_lr_subs = np.load('oof_lr_text_meta_subs.npy') if os.path.exists('oof_lr_text_meta_subs.npy') else None\n",
        "if oof_lr_base is not None and oof_lr_subs is not None:\n",
        "    best_w_bs, best_auc_bs = 0.0, -1.0\n",
        "    for w in np.linspace(0.0, 1.0, 21):\n",
        "        blend_bs = (1.0-w)*oof_lr_base + w*oof_lr_subs\n",
        "        a = roc_auc_score(y, blend_bs)\n",
        "        if a > best_auc_bs:\n",
        "            best_auc_bs, best_w_bs = a, w\n",
        "    oof_lr_best = (1.0-best_w_bs)*oof_lr_base + best_w_bs*oof_lr_subs\n",
        "    test_lr_base = np.load('test_lr_text_meta.npy') if os.path.exists('test_lr_text_meta.npy') else None\n",
        "    test_lr_subs = np.load('test_lr_text_meta_subs.npy') if os.path.exists('test_lr_text_meta_subs.npy') else None\n",
        "    test_lr_best = (1.0-best_w_bs)*(test_lr_base if test_lr_base is not None else test_xgb) + best_w_bs*(test_lr_subs if test_lr_subs is not None else test_xgb)\n",
        "else:\n",
        "    oof_lr_best = oof_xgb\n",
        "    test_lr_best = test_xgb\n",
        "\n",
        "best_w, best_auc = None, -1.0\n",
        "for w in np.arange(0.10, 0.75, 0.05):\n",
        "    blend = w*oof_xgb + (1.0-w)*oof_lr_best\n",
        "    a = roc_auc_score(y, blend)\n",
        "    if a > best_auc:\n",
        "        best_auc, best_w = a, w\n",
        "print(f'Best OOF AUC blend(XGB, LR-best)={best_auc:.5f} at w(XGB)={best_w:.2f}', flush=True)\n",
        "test_blend = best_w*test_xgb + (1.0-best_w)*test_lr_best\n",
        "pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend}).to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (XGB blend). Rows:', len(test_blend))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CV: XGBoost on (SVD-word,char) + meta ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1 AUC=0.64836 | iters=241 | 24.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2 AUC=0.66104 | iters=156 | 23.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3 AUC=0.67990 | iters=31 | 22.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4 AUC=0.66364 | iters=197 | 23.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 5 AUC=0.68882 | iters=218 | 23.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF AUC (XGB SVD+meta): 0.66059 | mean fold time: 23.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best OOF AUC blend(XGB, LR-best)=0.69127 at w(XGB)=0.45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (XGB blend). Rows: 1162\n"
          ]
        }
      ]
    },
    {
      "id": "e2f9f1ae-1435-4692-9137-03126ea57c1e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Grouped CV unification for LR models + NNLS 3-way blend with XGB (grouped)\n",
        "import os, sys, time, json, re, numpy as np, pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "def load_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path)\n",
        "    except ValueError:\n",
        "        with open(path, 'r') as f:\n",
        "            return pd.DataFrame(json.load(f))\n",
        "\n",
        "# Load data\n",
        "train = load_df('train.json')\n",
        "test = load_df('test.json')\n",
        "target_col = 'requester_received_pizza'\n",
        "body_fields_order = ['request_text_edit_aware','request_text','request_text_edit_aware_unnormalized']\n",
        "title_fields = ['request_title','title']\n",
        "body_col = next((c for c in body_fields_order if c in train.columns), None)\n",
        "title_col = next((c for c in title_fields if c in train.columns), None)\n",
        "\n",
        "def build_text_upweighted(df, title_col, body_col):\n",
        "    t = df[title_col].fillna('').astype(str)\n",
        "    b = df[body_col].fillna('').astype(str)\n",
        "    return (t + ' ' + t + ' [SEP] ' + b)\n",
        "\n",
        "full_text_train2 = build_text_upweighted(train, title_col, body_col)\n",
        "full_text_test2 = build_text_upweighted(test, title_col, body_col)\n",
        "y = train[target_col].astype(int).values\n",
        "groups = train['requester_username'].fillna('').astype(str).values if 'requester_username' in train.columns else np.arange(len(train))\n",
        "\n",
        "# Meta/lexicons (same as earlier, leakage-safe)\n",
        "def count_urls(s: str) -> int: return len(re.findall(r'https?://\\S+', s or ''))\n",
        "def has_imgur(s: str) -> int: return 1 if re.search(r'imgur\\.com', s or '', flags=re.IGNORECASE) else 0\n",
        "def count_digits(s: str) -> int: return sum(ch.isdigit() for ch in (s or ''))\n",
        "def dollar_flag(s: str) -> int: return 1 if ('$' in (s or '')) or re.search(r'\\b\\d+\\s*(dollars|bucks)\\b', s or '', flags=re.IGNORECASE) else 0\n",
        "def caps_ratio(s: str) -> float:\n",
        "    if not s: return 0.0\n",
        "    letters = [c for c in s if c.isalpha()]\n",
        "    return (sum(c.isupper() for c in letters) / max(1, len(letters))) if letters else 0.0\n",
        "def word_count(s: str) -> int: return len((s or '').split())\n",
        "def exclam_count(s: str) -> int: return (s or '').count('!')\n",
        "def question_count(s: str) -> int: return (s or '').count('?')\n",
        "def parse_subreddit_count(x) -> int: return len(x) if isinstance(x, list) else 0\n",
        "def safe_log1p_signed(x): return np.sign(x) * np.log1p(np.abs(x))\n",
        "LEX_PATTERNS = {\n",
        "    'lex_please': r'\\bplease\\b',\n",
        "    'lex_thanks': r'\\b(thank you|thanks in advance|thanks|thank|tia)\\b',\n",
        "    'lex_appreciate': r'\\b(appreciate|appreciated)\\b',\n",
        "    'lex_pay_it_forward': r'\\b(pay it forward|return the favor)\\b',\n",
        "    'lex_repay': r'\\b(repay|pay you back|pay back)\\b',\n",
        "    'lex_willing': r\"\\b(willing to|i\\'ll|i will|i can)\\b\",\n",
        "    'lex_karma': r'\\bkarma\\b',\n",
        "    'lex_evidence': r'\\b(proof|receipt|photo|picture|pic|verify|verification|evidence)\\b',\n",
        "    'lex_imgur_word': r'\\bimgur\\b',\n",
        "    'lex_student': r'\\b(student|college|university|finals|exam|midterm)\\b',\n",
        "    'lex_jobloss': r'\\b(unemployed|laid off|lost my job|between jobs|job hunt)\\b',\n",
        "    'lex_broke': r'\\b(broke)\\b',\n",
        "    'lex_rent_bills': r'\\b(rent|bill|bills|utilities|electric|gas|water|paycheck)\\b',\n",
        "    'lex_family': r'\\b(family|kids?|children|baby|pregnant|son|daughter|wife|husband)\\b',\n",
        "    'lex_hungry': r'\\b(hungry|starving|no food)\\b|\\bfood (stamps|pantry)\\b',\n",
        "    'lex_struggling': r'\\b(desperate|struggling)\\b',\n",
        "    'lex_urgency': r'\\b(tonight|today|tomorrow|asap|urgent)\\b',\n",
        "    'lex_help': r'\\bhelp\\b',\n",
        "}\n",
        "def add_lexicons(df, title_col, body_col):\n",
        "    txt = (df[title_col].fillna('').astype(str) + ' ' + df[body_col].fillna('').astype(str)).str.lower()\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    for name, pat in LEX_PATTERNS.items():\n",
        "        out[name] = txt.str.count(pat, flags=re.IGNORECASE).astype(float)\n",
        "    return out\n",
        "def build_meta(df):\n",
        "    title = df[title_col].fillna('').astype(str)\n",
        "    body = df[body_col].fillna('').astype(str)\n",
        "    ts = pd.to_numeric(df['unix_timestamp_of_request'], errors='coerce')\n",
        "    dt = pd.to_datetime(ts, unit='s', utc=True)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    keep_cols = [\n",
        "        'requester_account_age_in_days_at_request',\n",
        "        'requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request',\n",
        "        'requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request',\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "    ]\n",
        "    for c in keep_cols:\n",
        "        out[c] = pd.to_numeric(df[c], errors='coerce') if c in df.columns else 0.0\n",
        "    out['requester_subreddits_count'] = df['requester_subreddits_at_request'].apply(parse_subreddit_count).astype(float) if 'requester_subreddits_at_request' in df.columns else 0.0\n",
        "    out['hour'] = dt.dt.hour.astype(float)\n",
        "    out['weekday'] = dt.dt.weekday.astype(float)\n",
        "    out['is_weekend'] = dt.dt.weekday.isin([5,6]).astype(float)\n",
        "    out['title_len_chars'] = title.str.len().astype(float)\n",
        "    out['title_len_words'] = title.apply(word_count).astype(float)\n",
        "    out['body_len_chars'] = body.str.len().astype(float)\n",
        "    out['body_len_words'] = body.apply(word_count).astype(float)\n",
        "    out['url_count'] = body.apply(count_urls).astype(float)\n",
        "    out['has_imgur'] = body.apply(has_imgur).astype(float)\n",
        "    out['digits_count'] = body.apply(count_digits).astype(float)\n",
        "    out['dollar_flag'] = body.apply(dollar_flag).astype(float)\n",
        "    out['caps_ratio'] = body.apply(caps_ratio).astype(float)\n",
        "    out['exclam_count'] = body.apply(exclam_count).astype(float)\n",
        "    out['question_count'] = body.apply(question_count).astype(float)\n",
        "    lex = add_lexicons(df, title_col, body_col)\n",
        "    out = pd.concat([out, lex], axis=1).replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
        "    nonneg_cols = [\n",
        "        'requester_account_age_in_days_at_request','requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request','requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request','requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request','requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_subreddits_count','hour','weekday','is_weekend',\n",
        "        'title_len_chars','title_len_words','body_len_chars','body_len_words',\n",
        "        'url_count','has_imgur','digits_count','dollar_flag','exclam_count','question_count',\n",
        "        *list(LEX_PATTERNS.keys()),\n",
        "    ]\n",
        "    for c in nonneg_cols:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(np.clip(out[c], a_min=0.0, a_max=None))\n",
        "    if 'requester_upvotes_minus_downvotes_at_request' in out.columns:\n",
        "        out['requester_upvotes_minus_downvotes_at_request'] = safe_log1p_signed(out['requester_upvotes_minus_downvotes_at_request'].values)\n",
        "    return out.replace([np.inf,-np.inf],0.0).fillna(0.0)\n",
        "\n",
        "meta_train = build_meta(train)\n",
        "meta_test = build_meta(test)\n",
        "print('Meta shapes:', meta_train.shape, meta_test.shape, flush=True)\n",
        "\n",
        "def build_subreddit_text(df: pd.DataFrame) -> pd.Series:\n",
        "    if 'requester_subreddits_at_request' not in df.columns:\n",
        "        return pd.Series(['']*len(df))\n",
        "    def to_line(x):\n",
        "        if isinstance(x, list):\n",
        "            return ' '.join([str(s).lower() for s in x if isinstance(s, str)])\n",
        "        return ''\n",
        "    return df['requester_subreddits_at_request'].apply(to_line)\n",
        "\n",
        "sub_train_txt = build_subreddit_text(train)\n",
        "sub_test_txt = build_subreddit_text(test)\n",
        "\n",
        "# Freeze grouped folds and reuse\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = list(sgkf.split(np.zeros(len(y)), y, groups))\n",
        "print('Prepared grouped folds:', [(len(tr), len(va)) for tr,va in folds], flush=True)\n",
        "\n",
        "def get_vecs_meta():\n",
        "    wv = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.95, max_features=100000,\n",
        "                         sublinear_tf=True, lowercase=True, dtype=np.float32)\n",
        "    cv = TfidfVectorizer(analyzer='char', ngram_range=(3,6), min_df=2, max_features=200000,\n",
        "                         sublinear_tf=True, lowercase=True, dtype=np.float32)\n",
        "    return wv, cv\n",
        "def get_sub_vec():\n",
        "    return TfidfVectorizer(ngram_range=(1,1), min_df=3, max_features=600, lowercase=True, token_pattern=r'[^\\s]+', dtype=np.float32)\n",
        "\n",
        "def run_lr_grouped(include_subs: bool):\n",
        "    oof = np.zeros(len(train), dtype=np.float32)\n",
        "    test_preds = []\n",
        "    for i, (trn_idx, val_idx) in enumerate(folds, 1):\n",
        "        t0 = time.time()\n",
        "        # Text vecs\n",
        "        wv, cv = get_vecs_meta()\n",
        "        Xw_tr = wv.fit_transform(full_text_train2.iloc[trn_idx])\n",
        "        Xc_tr = cv.fit_transform(full_text_train2.iloc[trn_idx])\n",
        "        Xw_va = wv.transform(full_text_train2.iloc[val_idx])\n",
        "        Xc_va = cv.transform(full_text_train2.iloc[val_idx])\n",
        "        # Meta scaler\n",
        "        scaler = StandardScaler(with_mean=False)\n",
        "        Xm_tr = scaler.fit_transform(meta_train.iloc[trn_idx])\n",
        "        Xm_va = scaler.transform(meta_train.iloc[val_idx])\n",
        "        if include_subs:\n",
        "            sv = get_sub_vec()\n",
        "            Xs_tr = sv.fit_transform(sub_train_txt.iloc[trn_idx])\n",
        "            Xs_va = sv.transform(sub_train_txt.iloc[val_idx])\n",
        "            X_tr = sparse.hstack([Xw_tr, Xc_tr, Xm_tr, Xs_tr], format='csr')\n",
        "            X_va = sparse.hstack([Xw_va, Xc_va, Xm_va, Xs_va], format='csr')\n",
        "        else:\n",
        "            X_tr = sparse.hstack([Xw_tr, Xc_tr, Xm_tr], format='csr')\n",
        "            X_va = sparse.hstack([Xw_va, Xc_va, Xm_va], format='csr')\n",
        "        clf = LogisticRegression(penalty='l2', C=2.0, solver='saga', max_iter=3000, n_jobs=-1, class_weight='balanced', verbose=0)\n",
        "        clf.fit(X_tr, y[trn_idx])\n",
        "        oof[val_idx] = clf.predict_proba(X_va)[:,1]\n",
        "        # Test via fold pipelines\n",
        "        Xw_te = wv.transform(full_text_test2)\n",
        "        Xc_te = cv.transform(full_text_test2)\n",
        "        Xm_te = scaler.transform(meta_test)\n",
        "        if include_subs:\n",
        "            Xs_te = sv.transform(sub_test_txt)\n",
        "            X_te = sparse.hstack([Xw_te, Xc_te, Xm_te, Xs_te], format='csr')\n",
        "        else:\n",
        "            X_te = sparse.hstack([Xw_te, Xc_te, Xm_te], format='csr')\n",
        "        test_preds.append(clf.predict_proba(X_te)[:,1].astype(np.float32))\n",
        "        print(f'  LR {\"+subs\" if include_subs else \"+meta\"} fold {i} AUC={roc_auc_score(y[val_idx], oof[val_idx]):.5f} in {time.time()-t0:.1f}s', flush=True)\n",
        "    test_mean = np.mean(np.vstack(test_preds), axis=0).astype(np.float32)\n",
        "    auc = roc_auc_score(y, oof)\n",
        "    return oof, test_mean, auc\n",
        "\n",
        "print('=== Refit LR meta+text+lex with grouped folds ===', flush=True)\n",
        "oof_lr_meta_g, test_lr_meta_g, auc_lr_meta_g = run_lr_grouped(include_subs=False)\n",
        "print(f'OOF AUC (LR meta+text+lex, grouped): {auc_lr_meta_g:.5f}', flush=True)\n",
        "np.save('oof_lr_meta_g.npy', oof_lr_meta_g); np.save('test_lr_meta_g.npy', test_lr_meta_g)\n",
        "\n",
        "print('=== Refit LR meta+text+lex+subs with grouped folds ===', flush=True)\n",
        "oof_lr_subs_g, test_lr_subs_g, auc_lr_subs_g = run_lr_grouped(include_subs=True)\n",
        "print(f'OOF AUC (LR meta+text+lex+subs, grouped): {auc_lr_subs_g:.5f}', flush=True)\n",
        "np.save('oof_lr_subs_g.npy', oof_lr_subs_g); np.save('test_lr_subs_g.npy', test_lr_subs_g)\n",
        "\n",
        "# Load XGB grouped (from prior cell 11) and assemble NNLS 3-way blend\n",
        "oof_xgb_g = np.load('oof_xgb_svd_meta.npy') if os.path.exists('oof_xgb_svd_meta.npy') else None\n",
        "test_xgb_g = np.load('test_xgb_svd_meta.npy') if os.path.exists('test_xgb_svd_meta.npy') else None\n",
        "if oof_xgb_g is None or test_xgb_g is None:\n",
        "    print('XGB group OOF/Test not found; skipping XGB in blend.')\n",
        "    P = np.vstack([oof_lr_meta_g, oof_lr_subs_g]).T\n",
        "    T = np.vstack([test_lr_meta_g, test_lr_subs_g]).T\n",
        "else:\n",
        "    P = np.vstack([oof_lr_meta_g, oof_lr_subs_g, oof_xgb_g]).T\n",
        "    T = np.vstack([test_lr_meta_g, test_lr_subs_g, test_xgb_g]).T\n",
        "\n",
        "w, _ = nnls(P, y.astype(float))\n",
        "w = w / w.sum() if w.sum() > 0 else w\n",
        "blend_oof = P @ w\n",
        "blend_auc = roc_auc_score(y, blend_oof)\n",
        "print('NNLS weights:', w, flush=True)\n",
        "print(f'Blended OOF AUC (grouped, NNLS): {blend_auc:.5f}', flush=True)\n",
        "\n",
        "# T is (n_samples, n_models). Multiply directly by weights to get test preds (n_samples,).\n",
        "blend_test = T @ w\n",
        "sub = pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': blend_test.astype(np.float32)})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (NNLS 3-way grouped blend). Rows:', len(sub))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta shapes: (2878, 42) (1162, 42)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared grouped folds: [(2302, 576), (2302, 576), (2303, 575), (2303, 575), (2302, 576)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Refit LR meta+text+lex with grouped folds ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  LR +meta fold 1 AUC=0.64905 in 65.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  LR +meta fold 2 AUC=0.67542 in 63.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  LR +meta fold 3 AUC=0.65399 in 60.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  LR +meta fold 4 AUC=0.67675 in 58.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  LR +meta fold 5 AUC=0.69296 in 54.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF AUC (LR meta+text+lex, grouped): 0.67012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Refit LR meta+text+lex+subs with grouped folds ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  LR +subs fold 1 AUC=0.62822 in 63.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  LR +subs fold 2 AUC=0.66985 in 55.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  LR +subs fold 3 AUC=0.64885 in 51.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  LR +subs fold 4 AUC=0.67411 in 56.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  LR +subs fold 5 AUC=0.69863 in 55.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF AUC (LR meta+text+lex+subs, grouped): 0.66413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NNLS weights: [0.31847639 0.21452477 0.46699884]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blended OOF AUC (grouped, NNLS): 0.68219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (NNLS 3-way grouped blend). Rows: 1162\n"
          ]
        }
      ]
    },
    {
      "id": "4b3e39a8-429b-448b-bbfd-f9944df0c0dc",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 3A: CatBoost TEXT-only smoke test (1-fold) on grouped folds\n",
        "import sys, subprocess, time, numpy as np, pandas as pd, os\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Ensure stable CatBoost version\n",
        "try:\n",
        "    import catboost as cb\n",
        "    print('CatBoost existing:', getattr(cb, '__version__', 'unknown'), flush=True)\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', '--no-cache-dir', 'catboost==1.2.5'], check=True)\n",
        "    import catboost as cb\n",
        "    print('CatBoost installed:', cb.__version__, flush=True)\n",
        "\n",
        "def load_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path)\n",
        "    except ValueError:\n",
        "        import json as _json\n",
        "        with open(path, 'r') as f:\n",
        "            return pd.DataFrame(_json.load(f))\n",
        "\n",
        "train = load_df('train.json')\n",
        "test = load_df('test.json')\n",
        "target_col = 'requester_received_pizza'\n",
        "title_col = 'request_title' if 'request_title' in train.columns else 'title'\n",
        "body_col = 'request_text_edit_aware' if 'request_text_edit_aware' in train.columns else 'request_text'\n",
        "\n",
        "def join_subs_for_cat(df: pd.DataFrame) -> pd.Series:\n",
        "    if 'requester_subreddits_at_request' not in df.columns:\n",
        "        return pd.Series(['']*len(df))\n",
        "    def to_line(x):\n",
        "        if isinstance(x, list):\n",
        "            return ' '.join([str(s) for s in x if isinstance(s, str)])\n",
        "        return ''\n",
        "    return df['requester_subreddits_at_request'].apply(to_line)\n",
        "\n",
        "# Text-only features (3 columns) with title duplicated to upweight\n",
        "X_text = pd.DataFrame({\n",
        "    'cb_title': (train[title_col].fillna('').astype(str) + ' ' + train[title_col].fillna('').astype(str)),\n",
        "    'cb_body': train[body_col].fillna('').astype(str),\n",
        "    'cb_subs': join_subs_for_cat(train)\n",
        "})\n",
        "X_text_test = pd.DataFrame({\n",
        "    'cb_title': (test[title_col].fillna('').astype(str) + ' ' + test[title_col].fillna('').astype(str)),\n",
        "    'cb_body': test[body_col].fillna('').astype(str),\n",
        "    'cb_subs': join_subs_for_cat(test)\n",
        "})\n",
        "y = train[target_col].astype(int).values\n",
        "groups = train['requester_username'].fillna('').astype(str).values if 'requester_username' in train.columns else np.arange(len(train))\n",
        "text_feature_indices = [0,1,2]\n",
        "print('CB text-only shapes:', X_text.shape, X_text_test.shape, flush=True)\n",
        "\n",
        "# Freeze grouped folds and take the first fold for a smoke test\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = list(sgkf.split(X_text, y, groups))\n",
        "trn_idx, val_idx = folds[0]\n",
        "print('Smoke test using fold 1:', len(trn_idx), len(val_idx), flush=True)\n",
        "\n",
        "train_pool = cb.Pool(data=X_text.iloc[trn_idx], label=y[trn_idx], text_features=text_feature_indices)\n",
        "valid_pool = cb.Pool(data=X_text.iloc[val_idx], label=y[val_idx], text_features=text_feature_indices)\n",
        "test_pool = cb.Pool(data=X_text_test, text_features=text_feature_indices)\n",
        "\n",
        "# Safe CPU params (Config B from expert advice)\n",
        "params = dict(\n",
        "    task_type='CPU',\n",
        "    eval_metric='AUC',\n",
        "    loss_function='Logloss',\n",
        "    auto_class_weights='Balanced',\n",
        "    early_stopping_rounds=100,\n",
        "    bootstrap_type='Bayesian',\n",
        "    bagging_temperature=0.5,\n",
        "    rsm=0.9,\n",
        "    depth=5,\n",
        "    learning_rate=0.05,\n",
        "    l2_leaf_reg=6,\n",
        "    iterations=600,  # shorter for smoke test\n",
        "    random_seed=42,\n",
        "    verbose=100,\n",
        "    allow_writing_files=False,\n",
        "    thread_count=-1\n",
        ")\n",
        "\n",
        "t0 = time.time()\n",
        "model = cb.CatBoostClassifier(**params)\n",
        "model.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n",
        "oof_fold = model.predict_proba(valid_pool)[:,1]\n",
        "auc_fold = roc_auc_score(y[val_idx], oof_fold)\n",
        "print(f'[CB smoke] Fold1 val AUC={auc_fold:.5f} | elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "# If stable, produce provisional test preds for this fold (will average across folds in full run)\n",
        "test_pred_fold1 = model.predict_proba(test_pool)[:,1].astype(np.float32)\n",
        "np.save('cb_smoke_oof_fold1.npy', oof_fold)\n",
        "np.save('cb_smoke_test_fold1.npy', test_pred_fold1)\n",
        "print('Saved cb_smoke_oof_fold1.npy and cb_smoke_test_fold1.npy', flush=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost existing: 1.2.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CB text-only shapes: (2878, 3) (1162, 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Smoke test using fold 1: 2302 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.5682790\tbest: 0.5682790 (0)\ttotal: 86.3ms\tremaining: 51.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\ttest: 0.5884131\tbest: 0.5884131 (100)\ttotal: 2.39s\tremaining: 11.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\ttest: 0.6149531\tbest: 0.6149531 (200)\ttotal: 4.64s\tremaining: 9.22s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300:\ttest: 0.6099216\tbest: 0.6158050 (201)\ttotal: 6.92s\tremaining: 6.87s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.6158050412\nbestIteration = 201\n\nShrink model to first 202 iterations.\n[CB smoke] Fold1 val AUC=0.61581 | elapsed 7.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved cb_smoke_oof_fold1.npy and cb_smoke_test_fold1.npy\n"
          ]
        }
      ]
    },
    {
      "id": "64aa062f-87f1-4bf9-9db7-98dce4edeb94",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 3A (full): CatBoost TEXT-only 5-fold grouped CV + 4-way NNLS blend\n",
        "import sys, subprocess, time, numpy as np, pandas as pd, os\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "    print('CatBoost version:', getattr(cb, '__version__', 'unknown'), flush=True)\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', '--no-cache-dir', 'catboost==1.2.5'], check=True)\n",
        "    import catboost as cb\n",
        "    print('CatBoost installed:', cb.__version__, flush=True)\n",
        "\n",
        "def load_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path)\n",
        "    except ValueError:\n",
        "        import json as _json\n",
        "        with open(path, 'r') as f:\n",
        "            return pd.DataFrame(_json.load(f))\n",
        "\n",
        "train = load_df('train.json')\n",
        "test = load_df('test.json')\n",
        "target_col = 'requester_received_pizza'\n",
        "title_col = 'request_title' if 'request_title' in train.columns else 'title'\n",
        "body_col = 'request_text_edit_aware' if 'request_text_edit_aware' in train.columns else 'request_text'\n",
        "\n",
        "def join_subs_for_cat(df: pd.DataFrame) -> pd.Series:\n",
        "    if 'requester_subreddits_at_request' not in df.columns:\n",
        "        return pd.Series(['']*len(df))\n",
        "    def to_line(x):\n",
        "        if isinstance(x, list):\n",
        "            return ' '.join([str(s) for s in x if isinstance(s, str)])\n",
        "        return ''\n",
        "    return df['requester_subreddits_at_request'].apply(to_line)\n",
        "\n",
        "# 3 text columns (title duplicated to upweight)\n",
        "X_text = pd.DataFrame({\n",
        "    'cb_title': (train[title_col].fillna('').astype(str) + ' ' + train[title_col].fillna('').astype(str)),\n",
        "    'cb_body': train[body_col].fillna('').astype(str),\n",
        "    'cb_subs': join_subs_for_cat(train)\n",
        "})\n",
        "X_text_test = pd.DataFrame({\n",
        "    'cb_title': (test[title_col].fillna('').astype(str) + ' ' + test[title_col].fillna('').astype(str)),\n",
        "    'cb_body': test[body_col].fillna('').astype(str),\n",
        "    'cb_subs': join_subs_for_cat(test)\n",
        "})\n",
        "y = train[target_col].astype(int).values\n",
        "groups = train['requester_username'].fillna('').astype(str).values if 'requester_username' in train.columns else np.arange(len(train))\n",
        "text_feature_indices = [0,1,2]\n",
        "print('CB text-only shapes:', X_text.shape, X_text_test.shape, flush=True)\n",
        "\n",
        "# Freeze folds (must match other models):\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = list(sgkf.split(X_text, y, groups))\n",
        "\n",
        "params = dict(\n",
        "    task_type='CPU',\n",
        "    eval_metric='AUC',\n",
        "    loss_function='Logloss',\n",
        "    auto_class_weights='Balanced',\n",
        "    early_stopping_rounds=100,\n",
        "    bootstrap_type='Bayesian',\n",
        "    bagging_temperature=0.5,\n",
        "    rsm=0.9,\n",
        "    depth=5,\n",
        "    learning_rate=0.05,\n",
        "    l2_leaf_reg=6,\n",
        "    iterations=1200,\n",
        "    random_seed=42,\n",
        "    verbose=100,\n",
        "    allow_writing_files=False,\n",
        "    thread_count=-1\n",
        ")\n",
        "\n",
        "oof_cb = np.zeros(len(train), dtype=np.float32)\n",
        "test_preds = []\n",
        "for i, (trn_idx, val_idx) in enumerate(folds, 1):\n",
        "    t0 = time.time()\n",
        "    train_pool = cb.Pool(data=X_text.iloc[trn_idx], label=y[trn_idx], text_features=text_feature_indices)\n",
        "    valid_pool = cb.Pool(data=X_text.iloc[val_idx], label=y[val_idx], text_features=text_feature_indices)\n",
        "    test_pool = cb.Pool(data=X_text_test, text_features=text_feature_indices)\n",
        "    model = cb.CatBoostClassifier(**params)\n",
        "    model.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n",
        "    oof_cb[val_idx] = model.predict_proba(valid_pool)[:,1]\n",
        "    test_preds.append(model.predict_proba(test_pool)[:,1].astype(np.float32))\n",
        "    print(f'  CB fold {i} AUC={roc_auc_score(y[val_idx], oof_cb[val_idx]):.5f} in {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "auc_cb = roc_auc_score(y, oof_cb)\n",
        "test_cb = np.mean(np.vstack(test_preds), axis=0).astype(np.float32)\n",
        "print(f'OOF AUC (CB text-only, grouped): {auc_cb:.5f}', flush=True)\n",
        "np.save('oof_cat_text_g.npy', oof_cb)\n",
        "np.save('test_cat_text_g.npy', test_cb)\n",
        "\n",
        "# 4-way NNLS blend with grouped models if available\n",
        "oof_lr_meta_g = np.load('oof_lr_meta_g.npy') if os.path.exists('oof_lr_meta_g.npy') else None\n",
        "oof_lr_subs_g = np.load('oof_lr_subs_g.npy') if os.path.exists('oof_lr_subs_g.npy') else None\n",
        "oof_xgb_g = np.load('oof_xgb_svd_meta.npy') if os.path.exists('oof_xgb_svd_meta.npy') else None\n",
        "test_lr_meta_g = np.load('test_lr_meta_g.npy') if os.path.exists('test_lr_meta_g.npy') else None\n",
        "test_lr_subs_g = np.load('test_lr_subs_g.npy') if os.path.exists('test_lr_subs_g.npy') else None\n",
        "test_xgb_g = np.load('test_xgb_svd_meta.npy') if os.path.exists('test_xgb_svd_meta.npy') else None\n",
        "\n",
        "parts_oof = [arr for arr in [oof_lr_meta_g, oof_lr_subs_g, oof_xgb_g, oof_cb] if arr is not None]\n",
        "parts_test = [arr for arr in [test_lr_meta_g, test_lr_subs_g, test_xgb_g, test_cb] if arr is not None]\n",
        "P = np.vstack(parts_oof).T\n",
        "T = np.vstack(parts_test).T\n",
        "w, _ = nnls(P, y.astype(float))\n",
        "w = w / w.sum() if w.sum() > 0 else w\n",
        "blend_oof = P @ w\n",
        "blend_auc = roc_auc_score(y, blend_oof)\n",
        "print('NNLS weights (4-way):', w, flush=True)\n",
        "print(f'Blended OOF AUC (grouped, NNLS 4-way): {blend_auc:.5f}', flush=True)\n",
        "\n",
        "blend_test = T @ w\n",
        "pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': blend_test.astype(np.float32)}).to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (NNLS 4-way grouped blend). Rows:', len(blend_test))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost version: 1.2.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CB text-only shapes: (2878, 3) (1162, 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.5682790\tbest: 0.5682790 (0)\ttotal: 24.9ms\tremaining: 29.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\ttest: 0.5884131\tbest: 0.5884131 (100)\ttotal: 2.35s\tremaining: 25.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\ttest: 0.6149531\tbest: 0.6149531 (200)\ttotal: 4.62s\tremaining: 23s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300:\ttest: 0.6099216\tbest: 0.6158050 (201)\ttotal: 6.92s\tremaining: 20.7s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.6158050412\nbestIteration = 201\n\nShrink model to first 202 iterations.\n  CB fold 1 AUC=0.61581 in 7.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.5548821\tbest: 0.5548821 (0)\ttotal: 21.3ms\tremaining: 25.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\ttest: 0.6285441\tbest: 0.6442657 (38)\ttotal: 2.35s\tremaining: 25.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.6442656897\nbestIteration = 38\n\nShrink model to first 39 iterations.\n  CB fold 2 AUC=0.64427 in 3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.5471933\tbest: 0.5471933 (0)\ttotal: 20.4ms\tremaining: 24.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\ttest: 0.6064956\tbest: 0.6074098 (94)\ttotal: 2.28s\tremaining: 24.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\ttest: 0.6246672\tbest: 0.6252606 (168)\ttotal: 4.61s\tremaining: 22.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300:\ttest: 0.6210746\tbest: 0.6316921 (215)\ttotal: 6.96s\tremaining: 20.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.6316920609\nbestIteration = 215\n\nShrink model to first 216 iterations.\n  CB fold 3 AUC=0.63169 in 7.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.5549409\tbest: 0.5549409 (0)\ttotal: 21.5ms\tremaining: 25.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\ttest: 0.6149678\tbest: 0.6214870 (86)\ttotal: 2.32s\tremaining: 25.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.6214869966\nbestIteration = 86\n\nShrink model to first 87 iterations.\n  CB fold 4 AUC=0.62149 in 4.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.5545711\tbest: 0.5545711 (0)\ttotal: 22.5ms\tremaining: 27s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\ttest: 0.6426964\tbest: 0.6568086 (22)\ttotal: 2.34s\tremaining: 25.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.6568085899\nbestIteration = 22\n\nShrink model to first 23 iterations.\n  CB fold 5 AUC=0.65681 in 3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF AUC (CB text-only, grouped): 0.62283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NNLS weights (4-way): [0.27539628 0.21164104 0.4244068  0.08855588]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blended OOF AUC (grouped, NNLS 4-way): 0.68318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (NNLS 4-way grouped blend). Rows: 1162\n"
          ]
        }
      ]
    },
    {
      "id": "1048329f-57f5-488a-aaf7-8e8d218b32f0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 3B: XGBoost SVD bump (word=300, char=400, +subreddit SVD=50) with grouped CV + NNLS blend\n",
        "import sys, subprocess, time, numpy as np, pandas as pd, os, re\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "# Ensure xgboost installed\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    print('Installing xgboost...', flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "\n",
        "def load_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path)\n",
        "    except ValueError:\n",
        "        import json as _json\n",
        "        with open(path, 'r') as f:\n",
        "            return pd.DataFrame(_json.load(f))\n",
        "\n",
        "train = load_df('train.json')\n",
        "test = load_df('test.json')\n",
        "target_col = 'requester_received_pizza'\n",
        "title_col = 'request_title' if 'request_title' in train.columns else 'title'\n",
        "body_col = 'request_text_edit_aware' if 'request_text_edit_aware' in train.columns else 'request_text'\n",
        "\n",
        "def build_text_upweighted(df: pd.DataFrame, title_col: str, body_col: str) -> pd.Series:\n",
        "    t = df[title_col].fillna('').astype(str)\n",
        "    b = df[body_col].fillna('').astype(str)\n",
        "    return t + ' ' + t + ' [SEP] ' + b\n",
        "\n",
        "full_text_train2 = build_text_upweighted(train, title_col, body_col)\n",
        "full_text_test2 = build_text_upweighted(test, title_col, body_col)\n",
        "y = train[target_col].astype(int).values\n",
        "groups = train['requester_username'].fillna('').astype(str).values if 'requester_username' in train.columns else np.arange(len(train))\n",
        "\n",
        "# Meta/lexicon builders (same leakage-safe set used earlier)\n",
        "def count_urls(s: str) -> int: return len(re.findall(r'https?://\\S+', s or ''))\n",
        "def has_imgur(s: str) -> int: return 1 if re.search(r'imgur\\.com', s or '', flags=re.IGNORECASE) else 0\n",
        "def count_digits(s: str) -> int: return sum(ch.isdigit() for ch in (s or ''))\n",
        "def dollar_flag(s: str) -> int: return 1 if ('$' in (s or '')) or re.search(r'\\b\\d+\\s*(dollars|bucks)\\b', s or '', flags=re.IGNORECASE) else 0\n",
        "def caps_ratio(s: str) -> float:\n",
        "    if not s: return 0.0\n",
        "    letters = [c for c in s if c.isalpha()]\n",
        "    return (sum(c.isupper() for c in letters) / max(1, len(letters))) if letters else 0.0\n",
        "def word_count(s: str) -> int: return len((s or '').split())\n",
        "def exclam_count(s: str) -> int: return (s or '').count('!')\n",
        "def question_count(s: str) -> int: return (s or '').count('?')\n",
        "def parse_subreddit_count(x) -> int: return len(x) if isinstance(x, list) else 0\n",
        "def safe_log1p_signed(x): return np.sign(x) * np.log1p(np.abs(x))\n",
        "\n",
        "LEX_PATTERNS = {\n",
        "    'lex_please': r'\\bplease\\b',\n",
        "    'lex_thanks': r'\\b(thank you|thanks in advance|thanks|thank|tia)\\b',\n",
        "    'lex_appreciate': r'\\b(appreciate|appreciated)\\b',\n",
        "    'lex_pay_it_forward': r'\\b(pay it forward|return the favor)\\b',\n",
        "    'lex_repay': r'\\b(repay|pay you back|pay back)\\b',\n",
        "    'lex_willing': r\"\\b(willing to|i\\'ll|i will|i can)\\b\",\n",
        "    'lex_karma': r'\\bkarma\\b',\n",
        "    'lex_evidence': r'\\b(proof|receipt|photo|picture|pic|verify|verification|evidence)\\b',\n",
        "    'lex_imgur_word': r'\\bimgur\\b',\n",
        "    'lex_student': r'\\b(student|college|university|finals|exam|midterm)\\b',\n",
        "    'lex_jobloss': r'\\b(unemployed|laid off|lost my job|between jobs|job hunt)\\b',\n",
        "    'lex_broke': r'\\b(broke)\\b',\n",
        "    'lex_rent_bills': r'\\b(rent|bill|bills|utilities|electric|gas|water|paycheck)\\b',\n",
        "    'lex_family': r'\\b(family|kids?|children|baby|pregnant|son|daughter|wife|husband)\\b',\n",
        "    'lex_hungry': r'\\b(hungry|starving|no food)\\b|\\bfood (stamps|pantry)\\b',\n",
        "    'lex_struggling': r'\\b(desperate|struggling)\\b',\n",
        "    'lex_urgency': r'\\b(tonight|today|tomorrow|asap|urgent)\\b',\n",
        "    'lex_help': r'\\bhelp\\b',\n",
        "}\n",
        "\n",
        "def add_lexicons(df: pd.DataFrame, title_col: str, body_col: str) -> pd.DataFrame:\n",
        "    txt = (df[title_col].fillna('').astype(str) + ' ' + df[body_col].fillna('').astype(str)).str.lower()\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    for name, pat in LEX_PATTERNS.items():\n",
        "        out[name] = txt.str.count(pat, flags=re.IGNORECASE).astype(float)\n",
        "    return out\n",
        "\n",
        "def build_meta(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    title = df[title_col].fillna('').astype(str)\n",
        "    body = df[body_col].fillna('').astype(str)\n",
        "    ts = pd.to_numeric(df['unix_timestamp_of_request'], errors='coerce')\n",
        "    dt = pd.to_datetime(ts, unit='s', utc=True)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    keep_cols = [\n",
        "        'requester_account_age_in_days_at_request',\n",
        "        'requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request',\n",
        "        'requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request',\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "    ]\n",
        "    for c in keep_cols:\n",
        "        out[c] = pd.to_numeric(df[c], errors='coerce') if c in df.columns else 0.0\n",
        "    out['requester_subreddits_count'] = df['requester_subreddits_at_request'].apply(parse_subreddit_count).astype(float) if 'requester_subreddits_at_request' in df.columns else 0.0\n",
        "    out['hour'] = dt.dt.hour.astype(float)\n",
        "    out['weekday'] = dt.dt.weekday.astype(float)\n",
        "    out['is_weekend'] = dt.dt.weekday.isin([5,6]).astype(float)\n",
        "    out['title_len_chars'] = title.str.len().astype(float)\n",
        "    out['title_len_words'] = title.apply(word_count).astype(float)\n",
        "    out['body_len_chars'] = body.str.len().astype(float)\n",
        "    out['body_len_words'] = body.apply(word_count).astype(float)\n",
        "    out['url_count'] = body.apply(count_urls).astype(float)\n",
        "    out['has_imgur'] = body.apply(has_imgur).astype(float)\n",
        "    out['digits_count'] = body.apply(count_digits).astype(float)\n",
        "    out['dollar_flag'] = body.apply(dollar_flag).astype(float)\n",
        "    out['caps_ratio'] = body.apply(caps_ratio).astype(float)\n",
        "    out['exclam_count'] = body.apply(exclam_count).astype(float)\n",
        "    out['question_count'] = body.apply(question_count).astype(float)\n",
        "    lex = add_lexicons(df, title_col, body_col)\n",
        "    out = pd.concat([out, lex], axis=1)\n",
        "    out = out.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "    nonneg_cols = [\n",
        "        'requester_account_age_in_days_at_request','requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request','requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request','requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request','requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_subreddits_count','hour','weekday','is_weekend',\n",
        "        'title_len_chars','title_len_words','body_len_chars','body_len_words',\n",
        "        'url_count','has_imgur','digits_count','dollar_flag','exclam_count','question_count',\n",
        "        *list(LEX_PATTERNS.keys()),\n",
        "    ]\n",
        "    for c in nonneg_cols:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(np.clip(out[c], a_min=0.0, a_max=None))\n",
        "    if 'requester_upvotes_minus_downvotes_at_request' in out.columns:\n",
        "        out['requester_upvotes_minus_downvotes_at_request'] = safe_log1p_signed(out['requester_upvotes_minus_downvotes_at_request'].values)\n",
        "    out = out.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
        "    return out\n",
        "\n",
        "meta_train = build_meta(train)\n",
        "meta_test = build_meta(test)\n",
        "\n",
        "def build_subreddit_text(df: pd.DataFrame) -> pd.Series:\n",
        "    if 'requester_subreddits_at_request' not in df.columns:\n",
        "        return pd.Series(['']*len(df))\n",
        "    def to_line(x):\n",
        "        if isinstance(x, list):\n",
        "            return ' '.join([str(s).lower() for s in x if isinstance(s, str)])\n",
        "        return ''\n",
        "    return df['requester_subreddits_at_request'].apply(to_line)\n",
        "\n",
        "sub_train_txt = build_subreddit_text(train)\n",
        "sub_test_txt = build_subreddit_text(test)\n",
        "\n",
        "def get_vecs_for_svd():\n",
        "    wv = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.95, max_features=250000,\n",
        "                         sublinear_tf=True, lowercase=True, dtype=np.float32)\n",
        "    cv = TfidfVectorizer(analyzer='char', ngram_range=(3,6), min_df=2, max_features=400000,\n",
        "                         sublinear_tf=True, lowercase=True, dtype=np.float32)\n",
        "    return wv, cv\n",
        "\n",
        "def get_sub_vec():\n",
        "    return TfidfVectorizer(ngram_range=(1,1), min_df=3, max_features=600, lowercase=True, token_pattern=r'[^\\s]+', dtype=np.float32)\n",
        "\n",
        "def build_fold_features(tr_text, va_text, tr_meta, va_meta, tr_sub, va_sub,\n",
        "                        n_comp_word=300, n_comp_char=400, n_comp_sub=50):\n",
        "    # Fit vectorizers on train fold only\n",
        "    wv, cv = get_vecs_for_svd()\n",
        "    sv = get_sub_vec()\n",
        "    Xw_tr = wv.fit_transform(tr_text)\n",
        "    Xc_tr = cv.fit_transform(tr_text)\n",
        "    Xs_tr = sv.fit_transform(tr_sub)\n",
        "    Xw_va = wv.transform(va_text)\n",
        "    Xc_va = cv.transform(va_text)\n",
        "    Xs_va = sv.transform(va_sub)\n",
        "    # SVD reductions\n",
        "    svd_w = TruncatedSVD(n_components=n_comp_word, random_state=42)\n",
        "    svd_c = TruncatedSVD(n_components=n_comp_char, random_state=42)\n",
        "    svd_s = TruncatedSVD(n_components=n_comp_sub, random_state=42)\n",
        "    Zw_tr = svd_w.fit_transform(Xw_tr).astype(np.float32)\n",
        "    Zc_tr = svd_c.fit_transform(Xc_tr).astype(np.float32)\n",
        "    Zs_tr = svd_s.fit_transform(Xs_tr).astype(np.float32)\n",
        "    Zw_va = svd_w.transform(Xw_va).astype(np.float32)\n",
        "    Zc_va = svd_c.transform(Xc_va).astype(np.float32)\n",
        "    Zs_va = svd_s.transform(Xs_va).astype(np.float32)\n",
        "    # Scale meta\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    M_tr = scaler.fit_transform(tr_meta).astype(np.float32)\n",
        "    M_va = scaler.transform(va_meta).astype(np.float32)\n",
        "    X_tr = np.hstack([Zw_tr, Zc_tr, Zs_tr, M_tr]).astype(np.float32)\n",
        "    X_va = np.hstack([Zw_va, Zc_va, Zs_va, M_va]).astype(np.float32)\n",
        "    return X_tr, X_va, (wv, cv, sv, svd_w, svd_c, svd_s, scaler)\n",
        "\n",
        "def build_test_features(text_train, text_test, sub_train, sub_test, transformers, meta_train, meta_test):\n",
        "    wv, cv, sv, svd_w, svd_c, svd_s, scaler = transformers\n",
        "    Xw_tr = wv.fit_transform(text_train)\n",
        "    Xc_tr = cv.fit_transform(text_train)\n",
        "    Xs_tr = sv.fit_transform(sub_train)\n",
        "    Xw_te = wv.transform(text_test)\n",
        "    Xc_te = cv.transform(text_test)\n",
        "    Xs_te = sv.transform(sub_test)\n",
        "    Zw_te = svd_w.fit(Xw_tr).transform(Xw_te).astype(np.float32)\n",
        "    Zc_te = svd_c.fit(Xc_tr).transform(Xc_te).astype(np.float32)\n",
        "    Zs_te = svd_s.fit(Xs_tr).transform(Xs_te).astype(np.float32)\n",
        "    M_te = scaler.fit(meta_train).transform(meta_test).astype(np.float32)\n",
        "    X_te = np.hstack([Zw_te, Zc_te, Zs_te, M_te]).astype(np.float32)\n",
        "    return X_te\n",
        "\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = list(sgkf.split(full_text_train2, y, groups))\n",
        "\n",
        "print('=== CV: XGB on SVD(word=300,char=400,subs=50) + meta ===', flush=True)\n",
        "oof_xgb_bump = np.zeros(len(train), dtype=np.float32)\n",
        "test_preds = []\n",
        "fold_times = []\n",
        "\n",
        "params = dict(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='auc',\n",
        "    tree_method='hist',\n",
        "    max_depth=4,\n",
        "    learning_rate=0.045,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.0,\n",
        "    reg_lambda=1.5,\n",
        "    max_bin=256,\n",
        "    min_child_weight=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(folds, 1):\n",
        "    t0 = time.time()\n",
        "    X_tr, X_va, pipes = build_fold_features(\n",
        "        full_text_train2.iloc[trn_idx],\n",
        "        full_text_train2.iloc[val_idx],\n",
        "        meta_train.iloc[trn_idx].values,\n",
        "        meta_train.iloc[val_idx].values,\n",
        "        sub_train_txt.iloc[trn_idx],\n",
        "        sub_train_txt.iloc[val_idx],\n",
        "        n_comp_word=300, n_comp_char=400, n_comp_sub=50\n",
        "    )\n",
        "    dtr = xgb.DMatrix(X_tr, label=y[trn_idx])\n",
        "    dva = xgb.DMatrix(X_va, label=y[val_idx])\n",
        "    booster = xgb.train(params, dtr, num_boost_round=2000, evals=[(dva, 'valid')],\n",
        "                        verbose_eval=False,\n",
        "                        early_stopping_rounds=100)\n",
        "    oof_xgb_bump[val_idx] = booster.predict(dva, iteration_range=(0, booster.best_iteration+1))\n",
        "    Xt = build_test_features(full_text_train2, full_text_test2, sub_train_txt, sub_test_txt, pipes, meta_train.values, meta_test.values)\n",
        "    dte = xgb.DMatrix(Xt)\n",
        "    test_preds.append(booster.predict(dte, iteration_range=(0, booster.best_iteration+1)).astype(np.float32))\n",
        "    dt = time.time()-t0\n",
        "    fold_times.append(dt)\n",
        "    print(f'  Fold {fold} AUC={roc_auc_score(y[val_idx], oof_xgb_bump[val_idx]):.5f} | iters={booster.best_iteration+1} | {dt:.1f}s', flush=True)\n",
        "\n",
        "auc_xgb_bump = roc_auc_score(y, oof_xgb_bump)\n",
        "print(f'OOF AUC (XGB SVD-bump+subs+meta): {auc_xgb_bump:.5f} | mean fold time: {np.mean(fold_times):.1f}s', flush=True)\n",
        "test_xgb_bump = np.mean(np.vstack(test_preds), axis=0).astype(np.float32)\n",
        "np.save('oof_xgb_svd_meta_subs_g.npy', oof_xgb_bump)\n",
        "np.save('test_xgb_svd_meta_subs_g.npy', test_xgb_bump)\n",
        "\n",
        "# NNLS blend with grouped models (LR_meta_g, LR_subs_g, CB_text_g, XGB_bump)\n",
        "oof_lr_meta_g = np.load('oof_lr_meta_g.npy') if os.path.exists('oof_lr_meta_g.npy') else None\n",
        "oof_lr_subs_g = np.load('oof_lr_subs_g.npy') if os.path.exists('oof_lr_subs_g.npy') else None\n",
        "oof_cb_g = np.load('oof_cat_text_g.npy') if os.path.exists('oof_cat_text_g.npy') else None\n",
        "test_lr_meta_g = np.load('test_lr_meta_g.npy') if os.path.exists('test_lr_meta_g.npy') else None\n",
        "test_lr_subs_g = np.load('test_lr_subs_g.npy') if os.path.exists('test_lr_subs_g.npy') else None\n",
        "test_cb_g = np.load('test_cat_text_g.npy') if os.path.exists('test_cat_text_g.npy') else None\n",
        "\n",
        "parts_oof = [arr for arr in [oof_lr_meta_g, oof_lr_subs_g, oof_cb_g, oof_xgb_bump] if arr is not None]\n",
        "parts_test = [arr for arr in [test_lr_meta_g, test_lr_subs_g, test_cb_g, test_xgb_bump] if arr is not None]\n",
        "P = np.vstack(parts_oof).T\n",
        "T = np.vstack(parts_test).T\n",
        "w, _ = nnls(P, y.astype(float))\n",
        "w = w / w.sum() if w.sum() > 0 else w\n",
        "blend_oof = P @ w\n",
        "blend_auc = roc_auc_score(y, blend_oof)\n",
        "print('NNLS weights (with XGB bump):', w, flush=True)\n",
        "print(f'Blended OOF AUC (grouped, NNLS): {blend_auc:.5f}', flush=True)\n",
        "\n",
        "blend_test = T @ w\n",
        "pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': blend_test.astype(np.float32)}).to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (NNLS with XGB bump). Rows:', len(blend_test))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CV: XGB on SVD(word=300,char=400,subs=50) + meta ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1 AUC=0.63497 | iters=135 | 35.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2 AUC=0.65671 | iters=146 | 35.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3 AUC=0.66933 | iters=176 | 36.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4 AUC=0.65017 | iters=58 | 34.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 5 AUC=0.70044 | iters=12 | 35.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF AUC (XGB SVD-bump+subs+meta): 0.64826 | mean fold time: 35.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NNLS weights (with XGB bump): [0.36479612 0.17490624 0.01748015 0.4428175 ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blended OOF AUC (grouped, NNLS): 0.67828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (NNLS with XGB bump). Rows: 1162\n"
          ]
        }
      ]
    },
    {
      "id": "455ab022-14eb-4b75-809e-f81e3c584eb7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 3C: Sentence-Transformer embeddings (all-MiniLM-L6-v2) + meta -> XGBoost (grouped CV) + NNLS blend\n",
        "import sys, subprocess, time, os, re, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "# Install sentence-transformers (CPU) and xgboost if missing\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'sentence-transformers==2.7.0'], check=True)\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "\n",
        "def load_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path)\n",
        "    except ValueError:\n",
        "        import json as _json\n",
        "        with open(path, 'r') as f:\n",
        "            return pd.DataFrame(_json.load(f))\n",
        "\n",
        "train = load_df('train.json')\n",
        "test = load_df('test.json')\n",
        "target_col = 'requester_received_pizza'\n",
        "title_col = 'request_title' if 'request_title' in train.columns else 'title'\n",
        "body_col = 'request_text_edit_aware' if 'request_text_edit_aware' in train.columns else 'request_text'\n",
        "\n",
        "def build_text_upweighted(df: pd.DataFrame, title_col: str, body_col: str) -> pd.Series:\n",
        "    t = df[title_col].fillna('').astype(str)\n",
        "    b = df[body_col].fillna('').astype(str)\n",
        "    return (t + ' ' + t + ' [SEP] ' + b)\n",
        "\n",
        "text_tr = build_text_upweighted(train, title_col, body_col).tolist()\n",
        "text_te = build_text_upweighted(test, title_col, body_col).tolist()\n",
        "y = train[target_col].astype(int).values\n",
        "groups = train['requester_username'].fillna('').astype(str).values if 'requester_username' in train.columns else np.arange(len(train))\n",
        "\n",
        "# Build leakage-safe meta (reuse prior definitions)\n",
        "def count_urls(s: str) -> int: return len(re.findall(r'https?://\\S+', s or ''))\n",
        "def has_imgur(s: str) -> int: return 1 if re.search(r'imgur\\.com', s or '', flags=re.IGNORECASE) else 0\n",
        "def count_digits(s: str) -> int: return sum(ch.isdigit() for ch in (s or ''))\n",
        "def dollar_flag(s: str) -> int: return 1 if ('$' in (s or '')) or re.search(r'\\b\\d+\\s*(dollars|bucks)\\b', s or '', flags=re.IGNORECASE) else 0\n",
        "def caps_ratio(s: str) -> float:\n",
        "    if not s: return 0.0\n",
        "    letters = [c for c in s if c.isalpha()]\n",
        "    return (sum(c.isupper() for c in letters) / max(1, len(letters))) if letters else 0.0\n",
        "def word_count(s: str) -> int: return len((s or '').split())\n",
        "def exclam_count(s: str) -> int: return (s or '').count('!')\n",
        "def question_count(s: str) -> int: return (s or '').count('?')\n",
        "def parse_subreddit_count(x) -> int: return len(x) if isinstance(x, list) else 0\n",
        "def safe_log1p_signed(x): return np.sign(x) * np.log1p(np.abs(x))\n",
        "LEX_PATTERNS = {\n",
        "    'lex_please': r'\\bplease\\b',\n",
        "    'lex_thanks': r'\\b(thank you|thanks in advance|thanks|thank|tia)\\b',\n",
        "    'lex_appreciate': r'\\b(appreciate|appreciated)\\b',\n",
        "    'lex_pay_it_forward': r'\\b(pay it forward|return the favor)\\b',\n",
        "    'lex_repay': r'\\b(repay|pay you back|pay back)\\b',\n",
        "    'lex_willing': r\"\\b(willing to|i\\'ll|i will|i can)\\b\",\n",
        "    'lex_karma': r'\\bkarma\\b',\n",
        "    'lex_evidence': r'\\b(proof|receipt|photo|picture|pic|verify|verification|evidence)\\b',\n",
        "    'lex_imgur_word': r'\\bimgur\\b',\n",
        "    'lex_student': r'\\b(student|college|university|finals|exam|midterm)\\b',\n",
        "    'lex_jobloss': r'\\b(unemployed|laid off|lost my job|between jobs|job hunt)\\b',\n",
        "    'lex_broke': r'\\b(broke)\\b',\n",
        "    'lex_rent_bills': r'\\b(rent|bill|bills|utilities|electric|gas|water|paycheck)\\b',\n",
        "    'lex_family': r'\\b(family|kids?|children|baby|pregnant|son|daughter|wife|husband)\\b',\n",
        "    'lex_hungry': r'\\b(hungry|starving|no food)\\b|\\bfood (stamps|pantry)\\b',\n",
        "    'lex_struggling': r'\\b(desperate|struggling)\\b',\n",
        "    'lex_urgency': r'\\b(tonight|today|tomorrow|asap|urgent)\\b',\n",
        "    'lex_help': r'\\bhelp\\b',\n",
        "}\n",
        "def add_lexicons(df: pd.DataFrame, title_col: str, body_col: str) -> pd.DataFrame:\n",
        "    txt = (df[title_col].fillna('').astype(str) + ' ' + df[body_col].fillna('').astype(str)).str.lower()\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    for name, pat in LEX_PATTERNS.items():\n",
        "        out[name] = txt.str.count(pat, flags=re.IGNORECASE).astype(float)\n",
        "    return out\n",
        "def build_meta(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    title = df[title_col].fillna('').astype(str)\n",
        "    body = df[body_col].fillna('').astype(str)\n",
        "    ts = pd.to_numeric(df['unix_timestamp_of_request'], errors='coerce')\n",
        "    dt = pd.to_datetime(ts, unit='s', utc=True)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    keep_cols = [\n",
        "        'requester_account_age_in_days_at_request',\n",
        "        'requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request',\n",
        "        'requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request',\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "    ]\n",
        "    for c in keep_cols:\n",
        "        out[c] = pd.to_numeric(df[c], errors='coerce') if c in df.columns else 0.0\n",
        "    out['requester_subreddits_count'] = df['requester_subreddits_at_request'].apply(parse_subreddit_count).astype(float) if 'requester_subreddits_at_request' in df.columns else 0.0\n",
        "    out['hour'] = dt.dt.hour.astype(float)\n",
        "    out['weekday'] = dt.dt.weekday.astype(float)\n",
        "    out['is_weekend'] = dt.dt.weekday.isin([5,6]).astype(float)\n",
        "    out['title_len_chars'] = title.str.len().astype(float)\n",
        "    out['title_len_words'] = title.apply(word_count).astype(float)\n",
        "    out['body_len_chars'] = body.str.len().astype(float)\n",
        "    out['body_len_words'] = body.apply(word_count).astype(float)\n",
        "    out['url_count'] = body.apply(count_urls).astype(float)\n",
        "    out['has_imgur'] = body.apply(has_imgur).astype(float)\n",
        "    out['digits_count'] = body.apply(count_digits).astype(float)\n",
        "    out['dollar_flag'] = body.apply(dollar_flag).astype(float)\n",
        "    out['caps_ratio'] = body.apply(caps_ratio).astype(float)\n",
        "    out['exclam_count'] = body.apply(exclam_count).astype(float)\n",
        "    out['question_count'] = body.apply(question_count).astype(float)\n",
        "    lex = add_lexicons(df, title_col, body_col)\n",
        "    out = pd.concat([out, lex], axis=1)\n",
        "    out = out.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "    nonneg_cols = [\n",
        "        'requester_account_age_in_days_at_request','requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request','requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request','requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request','requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_subreddits_count','hour','weekday','is_weekend',\n",
        "        'title_len_chars','title_len_words','body_len_chars','body_len_words',\n",
        "        'url_count','has_imgur','digits_count','dollar_flag','exclam_count','question_count',\n",
        "        *list(LEX_PATTERNS.keys()),\n",
        "    ]\n",
        "    for c in nonneg_cols:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(np.clip(out[c], a_min=0.0, a_max=None))\n",
        "    if 'requester_upvotes_minus_downvotes_at_request' in out.columns:\n",
        "        out['requester_upvotes_minus_downvotes_at_request'] = safe_log1p_signed(out['requester_upvotes_minus_downvotes_at_request'].values)\n",
        "    out = out.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
        "    return out\n",
        "\n",
        "meta_train = build_meta(train)\n",
        "meta_test = build_meta(test)\n",
        "print('Meta shapes:', meta_train.shape, meta_test.shape, flush=True)\n",
        "\n",
        "# Encode sentence embeddings (CPU)\n",
        "print('Encoding sentence embeddings (all-MiniLM-L6-v2)...', flush=True)\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')\n",
        "emb_tr = model.encode(text_tr, batch_size=64, show_progress_bar=False, normalize_embeddings=True)\n",
        "emb_te = model.encode(text_te, batch_size=64, show_progress_bar=False, normalize_embeddings=True)\n",
        "emb_tr = emb_tr.astype(np.float32)\n",
        "emb_te = emb_te.astype(np.float32)\n",
        "print('Embeddings shape:', emb_tr.shape, emb_te.shape, flush=True)\n",
        "\n",
        "# Grouped CV with XGBoost on [embeddings + scaled meta]\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = list(sgkf.split(np.zeros(len(y)), y, groups))\n",
        "oof_st = np.zeros(len(train), dtype=np.float32)\n",
        "test_preds = []\n",
        "times = []\n",
        "params = dict(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='auc',\n",
        "    tree_method='hist',\n",
        "    max_depth=4,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=1.0,\n",
        "    min_child_weight=1.0,\n",
        "    max_bin=256,\n",
        "    random_state=42\n",
        ")\n",
        "for i, (trn_idx, val_idx) in enumerate(folds, 1):\n",
        "    t0 = time.time()\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    M_tr = scaler.fit_transform(meta_train.iloc[trn_idx].values).astype(np.float32)\n",
        "    M_va = scaler.transform(meta_train.iloc[val_idx].values).astype(np.float32)\n",
        "    X_tr = np.hstack([emb_tr[trn_idx], M_tr]).astype(np.float32)\n",
        "    X_va = np.hstack([emb_tr[val_idx], M_va]).astype(np.float32)\n",
        "    dtr = xgb.DMatrix(X_tr, label=y[trn_idx])\n",
        "    dva = xgb.DMatrix(X_va, label=y[val_idx])\n",
        "    booster = xgb.train(params, dtr, num_boost_round=2000, evals=[(dva,'valid')],\n",
        "                        verbose_eval=False, early_stopping_rounds=100)\n",
        "    oof_st[val_idx] = booster.predict(dva, iteration_range=(0, booster.best_iteration+1))\n",
        "    # Test build\n",
        "    M_te = scaler.transform(meta_test.values).astype(np.float32)\n",
        "    Xt = np.hstack([emb_te, M_te]).astype(np.float32)\n",
        "    dte = xgb.DMatrix(Xt)\n",
        "    test_preds.append(booster.predict(dte, iteration_range=(0, booster.best_iteration+1)).astype(np.float32))\n",
        "    times.append(time.time()-t0)\n",
        "    print(f'  Fold {i} AUC={roc_auc_score(y[val_idx], oof_st[val_idx]):.5f} | iters={booster.best_iteration+1} | {times[-1]:.1f}s', flush=True)\n",
        "\n",
        "auc_st = roc_auc_score(y, oof_st)\n",
        "print(f'OOF AUC (ST-emb + meta XGB): {auc_st:.5f} | mean fold time: {np.mean(times):.1f}s', flush=True)\n",
        "test_st = np.mean(np.vstack(test_preds), axis=0).astype(np.float32)\n",
        "np.save('oof_st_embed_g.npy', oof_st)\n",
        "np.save('test_st_embed_g.npy', test_st)\n",
        "\n",
        "# NNLS blend including ST model\n",
        "oof_lr_meta_g = np.load('oof_lr_meta_g.npy') if os.path.exists('oof_lr_meta_g.npy') else None\n",
        "oof_lr_subs_g = np.load('oof_lr_subs_g.npy') if os.path.exists('oof_lr_subs_g.npy') else None\n",
        "oof_xgb_g = np.load('oof_xgb_svd_meta.npy') if os.path.exists('oof_xgb_svd_meta.npy') else None\n",
        "oof_cb_g = np.load('oof_cat_text_g.npy') if os.path.exists('oof_cat_text_g.npy') else None\n",
        "\n",
        "test_lr_meta_g = np.load('test_lr_meta_g.npy') if os.path.exists('test_lr_meta_g.npy') else None\n",
        "test_lr_subs_g = np.load('test_lr_subs_g.npy') if os.path.exists('test_lr_subs_g.npy') else None\n",
        "test_xgb_g = np.load('test_xgb_svd_meta.npy') if os.path.exists('test_xgb_svd_meta.npy') else None\n",
        "test_cb_g = np.load('test_cat_text_g.npy') if os.path.exists('test_cat_text_g.npy') else None\n",
        "\n",
        "parts_oof = [arr for arr in [oof_lr_meta_g, oof_lr_subs_g, oof_xgb_g, oof_cb_g, oof_st] if arr is not None]\n",
        "parts_test = [arr for arr in [test_lr_meta_g, test_lr_subs_g, test_xgb_g, test_cb_g, test_st] if arr is not None]\n",
        "P = np.vstack(parts_oof).T\n",
        "T = np.vstack(parts_test).T\n",
        "w, _ = nnls(P, y.astype(float))\n",
        "w = w / w.sum() if w.sum() > 0 else w\n",
        "blend_oof = P @ w\n",
        "blend_auc = roc_auc_score(y, blend_oof)\n",
        "print('NNLS weights (with ST):', w, flush=True)\n",
        "print(f'Blended OOF AUC (grouped, NNLS + ST): {blend_auc:.5f}', flush=True)\n",
        "\n",
        "blend_test = T @ w\n",
        "pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': blend_test.astype(np.float32)}).to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (NNLS with ST). Rows:', len(blend_test))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta shapes: (2878, 42) (1162, 42)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding sentence embeddings (all-MiniLM-L6-v2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape: (2878, 384) (1162, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1 AUC=0.65715 | iters=165 | 1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2 AUC=0.66457 | iters=68 | 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3 AUC=0.66515 | iters=71 | 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4 AUC=0.64501 | iters=241 | 1.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 5 AUC=0.70300 | iters=25 | 0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF AUC (ST-emb + meta XGB): 0.65947 | mean fold time: 1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NNLS weights (with ST): [0.19340665 0.18262475 0.25767387 0.         0.36629473]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blended OOF AUC (grouped, NNLS + ST): 0.68873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (NNLS with ST). Rows: 1162\n"
          ]
        }
      ]
    },
    {
      "id": "e3f9ac0a-0e6b-4ba4-8a54-d98d68dd80a9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 3D: Sentence-Transformer embeddings (multi-qa-mpnet-base-dot-v1) + meta -> XGBoost (grouped CV) + NNLS blend\n",
        "import sys, subprocess, time, os, re, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "# Install sentence-transformers and xgboost if missing\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'sentence-transformers==2.7.0'], check=True)\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "\n",
        "def load_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path)\n",
        "    except ValueError:\n",
        "        import json as _json\n",
        "        with open(path, 'r') as f:\n",
        "            return pd.DataFrame(_json.load(f))\n",
        "\n",
        "train = load_df('train.json')\n",
        "test = load_df('test.json')\n",
        "target_col = 'requester_received_pizza'\n",
        "title_col = 'request_title' if 'request_title' in train.columns else 'title'\n",
        "body_col = 'request_text_edit_aware' if 'request_text_edit_aware' in train.columns else 'request_text'\n",
        "\n",
        "def build_text_upweighted(df: pd.DataFrame, title_col: str, body_col: str) -> pd.Series:\n",
        "    t = df[title_col].fillna('').astype(str)\n",
        "    b = df[body_col].fillna('').astype(str)\n",
        "    return (t + ' ' + t + ' [SEP] ' + b)\n",
        "\n",
        "text_tr = build_text_upweighted(train, title_col, body_col).tolist()\n",
        "text_te = build_text_upweighted(test, title_col, body_col).tolist()\n",
        "y = train[target_col].astype(int).values\n",
        "groups = train['requester_username'].fillna('').astype(str).values if 'requester_username' in train.columns else np.arange(len(train))\n",
        "\n",
        "# Build leakage-safe meta (reuse prior definitions)\n",
        "def count_urls(s: str) -> int: return len(re.findall(r'https?://\\S+', s or ''))\n",
        "def has_imgur(s: str) -> int: return 1 if re.search(r'imgur\\.com', s or '', flags=re.IGNORECASE) else 0\n",
        "def count_digits(s: str) -> int: return sum(ch.isdigit() for ch in (s or ''))\n",
        "def dollar_flag(s: str) -> int: return 1 if ('$' in (s or '')) or re.search(r'\\b\\d+\\s*(dollars|bucks)\\b', s or '', flags=re.IGNORECASE) else 0\n",
        "def caps_ratio(s: str) -> float:\n",
        "    if not s: return 0.0\n",
        "    letters = [c for c in s if c.isalpha()]\n",
        "    return (sum(c.isupper() for c in letters) / max(1, len(letters))) if letters else 0.0\n",
        "def word_count(s: str) -> int: return len((s or '').split())\n",
        "def exclam_count(s: str) -> int: return (s or '').count('!')\n",
        "def question_count(s: str) -> int: return (s or '').count('?')\n",
        "def parse_subreddit_count(x) -> int: return len(x) if isinstance(x, list) else 0\n",
        "def safe_log1p_signed(x): return np.sign(x) * np.log1p(np.abs(x))\n",
        "LEX_PATTERNS = {\n",
        "    'lex_please': r'\\bplease\\b',\n",
        "    'lex_thanks': r'\\b(thank you|thanks in advance|thanks|thank|tia)\\b',\n",
        "    'lex_appreciate': r'\\b(appreciate|appreciated)\\b',\n",
        "    'lex_pay_it_forward': r'\\b(pay it forward|return the favor)\\b',\n",
        "    'lex_repay': r'\\b(repay|pay you back|pay back)\\b',\n",
        "    'lex_willing': r\"\\b(willing to|i\\'ll|i will|i can)\\b\",\n",
        "    'lex_karma': r'\\bkarma\\b',\n",
        "    'lex_evidence': r'\\b(proof|receipt|photo|picture|pic|verify|verification|evidence)\\b',\n",
        "    'lex_imgur_word': r'\\bimgur\\b',\n",
        "    'lex_student': r'\\b(student|college|university|finals|exam|midterm)\\b',\n",
        "    'lex_jobloss': r'\\b(unemployed|laid off|lost my job|between jobs|job hunt)\\b',\n",
        "    'lex_broke': r'\\b(broke)\\b',\n",
        "    'lex_rent_bills': r'\\b(rent|bill|bills|utilities|electric|gas|water|paycheck)\\b',\n",
        "    'lex_family': r'\\b(family|kids?|children|baby|pregnant|son|daughter|wife|husband)\\b',\n",
        "    'lex_hungry': r'\\b(hungry|starving|no food)\\b|\\bfood (stamps|pantry)\\b',\n",
        "    'lex_struggling': r'\\b(desperate|struggling)\\b',\n",
        "    'lex_urgency': r'\\b(tonight|today|tomorrow|asap|urgent)\\b',\n",
        "    'lex_help': r'\\bhelp\\b',\n",
        "}\n",
        "def add_lexicons(df: pd.DataFrame, title_col: str, body_col: str) -> pd.DataFrame:\n",
        "    txt = (df[title_col].fillna('').astype(str) + ' ' + df[body_col].fillna('').astype(str)).str.lower()\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    for name, pat in LEX_PATTERNS.items():\n",
        "        out[name] = txt.str.count(pat, flags=re.IGNORECASE).astype(float)\n",
        "    return out\n",
        "def build_meta(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    title = df[title_col].fillna('').astype(str)\n",
        "    body = df[body_col].fillna('').astype(str)\n",
        "    ts = pd.to_numeric(df['unix_timestamp_of_request'], errors='coerce')\n",
        "    dt = pd.to_datetime(ts, unit='s', utc=True)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    keep_cols = [\n",
        "        'requester_account_age_in_days_at_request',\n",
        "        'requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request',\n",
        "        'requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request',\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "    ]\n",
        "    for c in keep_cols:\n",
        "        out[c] = pd.to_numeric(df[c], errors='coerce') if c in df.columns else 0.0\n",
        "    out['requester_subreddits_count'] = df['requester_subreddits_at_request'].apply(parse_subreddit_count).astype(float) if 'requester_subreddits_at_request' in df.columns else 0.0\n",
        "    out['hour'] = dt.dt.hour.astype(float)\n",
        "    out['weekday'] = dt.dt.weekday.astype(float)\n",
        "    out['is_weekend'] = dt.dt.weekday.isin([5,6]).astype(float)\n",
        "    out['title_len_chars'] = title.str.len().astype(float)\n",
        "    out['title_len_words'] = title.apply(word_count).astype(float)\n",
        "    out['body_len_chars'] = body.str.len().astype(float)\n",
        "    out['body_len_words'] = body.apply(word_count).astype(float)\n",
        "    out['url_count'] = body.apply(count_urls).astype(float)\n",
        "    out['has_imgur'] = body.apply(has_imgur).astype(float)\n",
        "    out['digits_count'] = body.apply(count_digits).astype(float)\n",
        "    out['dollar_flag'] = body.apply(dollar_flag).astype(float)\n",
        "    out['caps_ratio'] = body.apply(caps_ratio).astype(float)\n",
        "    out['exclam_count'] = body.apply(exclam_count).astype(float)\n",
        "    out['question_count'] = body.apply(question_count).astype(float)\n",
        "    lex = add_lexicons(df, title_col, body_col)\n",
        "    out = pd.concat([out, lex], axis=1)\n",
        "    out = out.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "    nonneg_cols = [\n",
        "        'requester_account_age_in_days_at_request','requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request','requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request','requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request','requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_subreddits_count','hour','weekday','is_weekend',\n",
        "        'title_len_chars','title_len_words','body_len_chars','body_len_words',\n",
        "        'url_count','has_imgur','digits_count','dollar_flag','exclam_count','question_count',\n",
        "        *list(LEX_PATTERNS.keys()),\n",
        "    ]\n",
        "    for c in nonneg_cols:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(np.clip(out[c], a_min=0.0, a_max=None))\n",
        "    if 'requester_upvotes_minus_downvotes_at_request' in out.columns:\n",
        "        out['requester_upvotes_minus_downvotes_at_request'] = safe_log1p_signed(out['requester_upvotes_minus_downvotes_at_request'].values)\n",
        "    out = out.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
        "    return out\n",
        "\n",
        "meta_train = build_meta(train)\n",
        "meta_test = build_meta(test)\n",
        "print('Meta shapes:', meta_train.shape, meta_test.shape, flush=True)\n",
        "\n",
        "# Encode sentence embeddings (MPNet base, CPU encode for stability; normalize embeddings)\n",
        "print('Encoding sentence embeddings (multi-qa-mpnet-base-dot-v1)...', flush=True)\n",
        "model = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1', device='cpu')\n",
        "emb_tr = model.encode(text_tr, batch_size=64, show_progress_bar=False, normalize_embeddings=True).astype(np.float32)\n",
        "emb_te = model.encode(text_te, batch_size=64, show_progress_bar=False, normalize_embeddings=True).astype(np.float32)\n",
        "print('Embeddings shape:', emb_tr.shape, emb_te.shape, flush=True)\n",
        "\n",
        "# Grouped CV with XGBoost on [embeddings + scaled meta]\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = list(sgkf.split(np.zeros(len(y)), y, groups))\n",
        "oof_mp = np.zeros(len(train), dtype=np.float32)\n",
        "test_preds = []\n",
        "fold_times = []\n",
        "params = dict(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='auc',\n",
        "    tree_method='hist',\n",
        "    max_depth=4,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=1.0,\n",
        "    min_child_weight=1.0,\n",
        "    max_bin=256,\n",
        "    random_state=42\n",
        ")\n",
        "for i, (trn_idx, val_idx) in enumerate(folds, 1):\n",
        "    t0 = time.time()\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    M_tr = scaler.fit_transform(meta_train.iloc[trn_idx].values).astype(np.float32)\n",
        "    M_va = scaler.transform(meta_train.iloc[val_idx].values).astype(np.float32)\n",
        "    X_tr = np.hstack([emb_tr[trn_idx], M_tr]).astype(np.float32)\n",
        "    X_va = np.hstack([emb_tr[val_idx], M_va]).astype(np.float32)\n",
        "    dtr = xgb.DMatrix(X_tr, label=y[trn_idx])\n",
        "    dva = xgb.DMatrix(X_va, label=y[val_idx])\n",
        "    booster = xgb.train(params, dtr, num_boost_round=2000, evals=[(dva,'valid')],\n",
        "                        verbose_eval=False, early_stopping_rounds=100)\n",
        "    oof_mp[val_idx] = booster.predict(dva, iteration_range=(0, booster.best_iteration+1))\n",
        "    M_te = scaler.transform(meta_test.values).astype(np.float32)\n",
        "    Xt = np.hstack([emb_te, M_te]).astype(np.float32)\n",
        "    dte = xgb.DMatrix(Xt)\n",
        "    test_preds.append(booster.predict(dte, iteration_range=(0, booster.best_iteration+1)).astype(np.float32))\n",
        "    dt = time.time()-t0\n",
        "    fold_times.append(dt)\n",
        "    print(f'  Fold {i} AUC={roc_auc_score(y[val_idx], oof_mp[val_idx]):.5f} | iters={booster.best_iteration+1} | {dt:.1f}s', flush=True)\n",
        "\n",
        "auc_mp = roc_auc_score(y, oof_mp)\n",
        "print(f'OOF AUC (MPNet-emb + meta XGB): {auc_mp:.5f} | mean fold time: {np.mean(fold_times):.1f}s', flush=True)\n",
        "test_mp = np.mean(np.vstack(test_preds), axis=0).astype(np.float32)\n",
        "np.save('oof_mpnet_embed_g.npy', oof_mp)\n",
        "np.save('test_mpnet_embed_g.npy', test_mp)\n",
        "\n",
        "# NNLS blend including MPNet model\n",
        "oof_lr_meta_g = np.load('oof_lr_meta_g.npy') if os.path.exists('oof_lr_meta_g.npy') else None\n",
        "oof_lr_subs_g = np.load('oof_lr_subs_g.npy') if os.path.exists('oof_lr_subs_g.npy') else None\n",
        "oof_xgb_g = np.load('oof_xgb_svd_meta.npy') if os.path.exists('oof_xgb_svd_meta.npy') else None\n",
        "oof_cb_g = np.load('oof_cat_text_g.npy') if os.path.exists('oof_cat_text_g.npy') else None\n",
        "oof_st_g = np.load('oof_st_embed_g.npy') if os.path.exists('oof_st_embed_g.npy') else None\n",
        "\n",
        "test_lr_meta_g = np.load('test_lr_meta_g.npy') if os.path.exists('test_lr_meta_g.npy') else None\n",
        "test_lr_subs_g = np.load('test_lr_subs_g.npy') if os.path.exists('test_lr_subs_g.npy') else None\n",
        "test_xgb_g = np.load('test_xgb_svd_meta.npy') if os.path.exists('test_xgb_svd_meta.npy') else None\n",
        "test_cb_g = np.load('test_cat_text_g.npy') if os.path.exists('test_cat_text_g.npy') else None\n",
        "test_st_g = np.load('test_st_embed_g.npy') if os.path.exists('test_st_embed_g.npy') else None\n",
        "\n",
        "parts_oof = [arr for arr in [oof_lr_meta_g, oof_lr_subs_g, oof_xgb_g, oof_cb_g, oof_st_g, oof_mp] if arr is not None]\n",
        "parts_test = [arr for arr in [test_lr_meta_g, test_lr_subs_g, test_xgb_g, test_cb_g, test_st_g, test_mp] if arr is not None]\n",
        "P = np.vstack(parts_oof).T\n",
        "T = np.vstack(parts_test).T\n",
        "w, _ = nnls(P, y.astype(float))\n",
        "w = w / w.sum() if w.sum() > 0 else w\n",
        "blend_oof = P @ w\n",
        "blend_auc = roc_auc_score(y, blend_oof)\n",
        "print('NNLS weights (with MPNet):', w, flush=True)\n",
        "print(f'Blended OOF AUC (grouped, NNLS + MPNet): {blend_auc:.5f}', flush=True)\n",
        "\n",
        "blend_test = T @ w\n",
        "pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': blend_test.astype(np.float32)}).to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (NNLS with MPNet). Rows:', len(blend_test))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta shapes: (2878, 42) (1162, 42)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding sentence embeddings (multi-qa-mpnet-base-dot-v1)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You try to use a model that was created with version 3.0.0.dev0, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n\n\n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape: (2878, 768) (1162, 768)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1 AUC=0.66107 | iters=49 | 1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2 AUC=0.66956 | iters=41 | 1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3 AUC=0.63849 | iters=31 | 1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4 AUC=0.66784 | iters=76 | 1.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 5 AUC=0.68682 | iters=127 | 1.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF AUC (MPNet-emb + meta XGB): 0.66457 | mean fold time: 1.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NNLS weights (with MPNet): [0.1455568  0.17538714 0.17781834 0.         0.2169544  0.28428332]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blended OOF AUC (grouped, NNLS + MPNet): 0.69224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (NNLS with MPNet). Rows: 1162\n"
          ]
        }
      ]
    },
    {
      "id": "2a4288fd-bef0-461d-a597-3a26a23606a3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 3E: Rank-then-NNLS blend (robust AUC ensembling) over grouped OOFs\n",
        "import numpy as np, pandas as pd, os\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train['requester_received_pizza'].astype(int).values\n",
        "\n",
        "# Load available grouped OOF/test predictions (same set as 3D)\n",
        "paths = {\n",
        "    'lr_meta_g': ('oof_lr_meta_g.npy','test_lr_meta_g.npy'),\n",
        "    'lr_subs_g': ('oof_lr_subs_g.npy','test_lr_subs_g.npy'),\n",
        "    'xgb_svd_meta': ('oof_xgb_svd_meta.npy','test_xgb_svd_meta.npy'),\n",
        "    'cb_text_g': ('oof_cat_text_g.npy','test_cat_text_g.npy'),\n",
        "    'st_embed_g': ('oof_st_embed_g.npy','test_st_embed_g.npy'),\n",
        "    'mpnet_embed_g': ('oof_mpnet_embed_g.npy','test_mpnet_embed_g.npy'),\n",
        "}\n",
        "oof_list, test_list, names = [], [], []\n",
        "for name, (poof, ptest) in paths.items():\n",
        "    if os.path.exists(poof) and os.path.exists(ptest):\n",
        "        oof_list.append(np.load(poof))\n",
        "        test_list.append(np.load(ptest))\n",
        "        names.append(name)\n",
        "\n",
        "assert len(oof_list) >= 3, f'Not enough models found for rank-NNLS, got {len(oof_list)}'\n",
        "P = np.vstack(oof_list).T  # (n_samples, n_models)\n",
        "T = np.vstack(test_list).T\n",
        "\n",
        "def rank01(a):\n",
        "    # rank to [0,1]; stable ties average rank behavior via argsort twice\n",
        "    order = a.argsort(kind='mergesort')\n",
        "    ranks = np.empty_like(order, dtype=np.float64)\n",
        "    ranks[order] = np.arange(len(a), dtype=np.float64)\n",
        "    return ranks / max(1, len(a)-1)\n",
        "\n",
        "# Rank-transform OOF and test per model independently\n",
        "P_rank = np.apply_along_axis(rank01, 0, P)\n",
        "T_rank = np.apply_along_axis(rank01, 0, T)\n",
        "\n",
        "# NNLS on ranked OOF\n",
        "w, _ = nnls(P_rank, y.astype(float))\n",
        "w = w / w.sum() if w.sum() > 0 else w\n",
        "blend_oof_rank = P_rank @ w\n",
        "auc_rank = roc_auc_score(y, blend_oof_rank)\n",
        "print('Models in rank-NNLS:', names)\n",
        "print('Rank-NNLS weights:', w)\n",
        "print(f'Rank-blended OOF AUC: {auc_rank:.5f}')\n",
        "\n",
        "# Build test submission with rank blend\n",
        "blend_test_rank = T_rank @ w\n",
        "pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': blend_test_rank.astype(np.float32)}).to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (Rank-NNLS). Rows:', len(blend_test_rank))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models in rank-NNLS: ['lr_meta_g', 'lr_subs_g', 'xgb_svd_meta', 'cb_text_g', 'st_embed_g', 'mpnet_embed_g']\nRank-NNLS weights: [0.16470432 0.16774478 0.14264587 0.06567989 0.20591968 0.25330546]\nRank-blended OOF AUC: 0.69506\nSaved submission.csv (Rank-NNLS). Rows: 1162\n"
          ]
        }
      ]
    },
    {
      "id": "0142e8b7-cb79-4be1-b09c-a91a15823b53",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 3F: Seed-bag XGB on ST embeddings (MiniLM + MPNet) with grouped CV, then rank-NNLS blend\n",
        "import sys, subprocess, time, os, re, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'sentence-transformers==2.7.0'], check=True)\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "\n",
        "def load_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path)\n",
        "    except ValueError:\n",
        "        import json as _json\n",
        "        with open(path, 'r') as f:\n",
        "            return pd.DataFrame(_json.load(f))\n",
        "\n",
        "train = load_df('train.json')\n",
        "test = load_df('test.json')\n",
        "target_col = 'requester_received_pizza'\n",
        "title_col = 'request_title' if 'request_title' in train.columns else 'title'\n",
        "body_col = 'request_text_edit_aware' if 'request_text_edit_aware' in train.columns else 'request_text'\n",
        "\n",
        "def build_text_upweighted(df: pd.DataFrame, title_col: str, body_col: str) -> pd.Series:\n",
        "    t = df[title_col].fillna('').astype(str)\n",
        "    b = df[body_col].fillna('').astype(str)\n",
        "    return (t + ' ' + t + ' [SEP] ' + b)\n",
        "\n",
        "text_tr = build_text_upweighted(train, title_col, body_col).tolist()\n",
        "text_te = build_text_upweighted(test, title_col, body_col).tolist()\n",
        "y = train[target_col].astype(int).values\n",
        "groups = train['requester_username'].fillna('').astype(str).values if 'requester_username' in train.columns else np.arange(len(train))\n",
        "\n",
        "# Meta/lexicons (leakage-safe, same as prior)\n",
        "def count_urls(s: str) -> int: return len(re.findall(r'https?://\\S+', s or ''))\n",
        "def has_imgur(s: str) -> int: return 1 if re.search(r'imgur\\.com', s or '', flags=re.IGNORECASE) else 0\n",
        "def count_digits(s: str) -> int: return sum(ch.isdigit() for ch in (s or ''))\n",
        "def dollar_flag(s: str) -> int: return 1 if ('$' in (s or '')) or re.search(r'\\b\\d+\\s*(dollars|bucks)\\b', s or '', flags=re.IGNORECASE) else 0\n",
        "def caps_ratio(s: str) -> float:\n",
        "    if not s: return 0.0\n",
        "    letters = [c for c in s if c.isalpha()]\n",
        "    return (sum(c.isupper() for c in letters) / max(1, len(letters))) if letters else 0.0\n",
        "def word_count(s: str) -> int: return len((s or '').split())\n",
        "def exclam_count(s: str) -> int: return (s or '').count('!')\n",
        "def question_count(s: str) -> int: return (s or '').count('?')\n",
        "def parse_subreddit_count(x) -> int: return len(x) if isinstance(x, list) else 0\n",
        "def safe_log1p_signed(x): return np.sign(x) * np.log1p(np.abs(x))\n",
        "LEX_PATTERNS = {\n",
        "    'lex_please': r'\\bplease\\b',\n",
        "    'lex_thanks': r'\\b(thank you|thanks in advance|thanks|thank|tia)\\b',\n",
        "    'lex_appreciate': r'\\b(appreciate|appreciated)\\b',\n",
        "    'lex_pay_it_forward': r'\\b(pay it forward|return the favor)\\b',\n",
        "    'lex_repay': r'\\b(repay|pay you back|pay back)\\b',\n",
        "    'lex_willing': r\"\\b(willing to|i\\'ll|i will|i can)\\b\",\n",
        "    'lex_karma': r'\\bkarma\\b',\n",
        "    'lex_evidence': r'\\b(proof|receipt|photo|picture|pic|verify|verification|evidence)\\b',\n",
        "    'lex_imgur_word': r'\\bimgur\\b',\n",
        "    'lex_student': r'\\b(student|college|university|finals|exam|midterm)\\b',\n",
        "    'lex_jobloss': r'\\b(unemployed|laid off|lost my job|between jobs|job hunt)\\b',\n",
        "    'lex_broke': r'\\b(broke)\\b',\n",
        "    'lex_rent_bills': r'\\b(rent|bill|bills|utilities|electric|gas|water|paycheck)\\b',\n",
        "    'lex_family': r'\\b(family|kids?|children|baby|pregnant|son|daughter|wife|husband)\\b',\n",
        "    'lex_hungry': r'\\b(hungry|starving|no food)\\b|\\bfood (stamps|pantry)\\b',\n",
        "    'lex_struggling': r'\\b(desperate|struggling)\\b',\n",
        "    'lex_urgency': r'\\b(tonight|today|tomorrow|asap|urgent)\\b',\n",
        "    'lex_help': r'\\bhelp\\b',\n",
        "}\n",
        "def add_lexicons(df: pd.DataFrame, title_col: str, body_col: str) -> pd.DataFrame:\n",
        "    txt = (df[title_col].fillna('').astype(str) + ' ' + df[body_col].fillna('').astype(str)).str.lower()\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    for name, pat in LEX_PATTERNS.items():\n",
        "        out[name] = txt.str.count(pat, flags=re.IGNORECASE).astype(float)\n",
        "    return out\n",
        "def build_meta(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    title = df[title_col].fillna('').astype(str)\n",
        "    body = df[body_col].fillna('').astype(str)\n",
        "    ts = pd.to_numeric(df['unix_timestamp_of_request'], errors='coerce')\n",
        "    dt = pd.to_datetime(ts, unit='s', utc=True)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    keep_cols = [\n",
        "        'requester_account_age_in_days_at_request',\n",
        "        'requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request',\n",
        "        'requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request',\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "    ]\n",
        "    for c in keep_cols:\n",
        "        out[c] = pd.to_numeric(df[c], errors='coerce') if c in df.columns else 0.0\n",
        "    out['requester_subreddits_count'] = df['requester_subreddits_at_request'].apply(parse_subreddit_count).astype(float) if 'requester_subreddits_at_request' in df.columns else 0.0\n",
        "    out['hour'] = dt.dt.hour.astype(float)\n",
        "    out['weekday'] = dt.dt.weekday.astype(float)\n",
        "    out['is_weekend'] = dt.dt.weekday.isin([5,6]).astype(float)\n",
        "    out['title_len_chars'] = title.str.len().astype(float)\n",
        "    out['title_len_words'] = title.apply(word_count).astype(float)\n",
        "    out['body_len_chars'] = body.str.len().astype(float)\n",
        "    out['body_len_words'] = body.apply(word_count).astype(float)\n",
        "    out['url_count'] = body.apply(count_urls).astype(float)\n",
        "    out['has_imgur'] = body.apply(has_imgur).astype(float)\n",
        "    out['digits_count'] = body.apply(count_digits).astype(float)\n",
        "    out['dollar_flag'] = body.apply(dollar_flag).astype(float)\n",
        "    out['caps_ratio'] = body.apply(caps_ratio).astype(float)\n",
        "    out['exclam_count'] = body.apply(exclam_count).astype(float)\n",
        "    out['question_count'] = body.apply(question_count).astype(float)\n",
        "    lex = add_lexicons(df, title_col, body_col)\n",
        "    out = pd.concat([out, lex], axis=1).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "    nonneg_cols = [\n",
        "        'requester_account_age_in_days_at_request','requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request','requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request','requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request','requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_subreddits_count','hour','weekday','is_weekend',\n",
        "        'title_len_chars','title_len_words','body_len_chars','body_len_words',\n",
        "        'url_count','has_imgur','digits_count','dollar_flag','exclam_count','question_count',\n",
        "        *list(LEX_PATTERNS.keys()),\n",
        "    ]\n",
        "    for c in nonneg_cols:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(np.clip(out[c], a_min=0.0, a_max=None))\n",
        "    if 'requester_upvotes_minus_downvotes_at_request' in out.columns:\n",
        "        out['requester_upvotes_minus_downvotes_at_request'] = safe_log1p_signed(out['requester_upvotes_minus_downvotes_at_request'].values)\n",
        "    return out.replace([np.inf,-np.inf],0.0).fillna(0.0)\n",
        "\n",
        "meta_train = build_meta(train)\n",
        "meta_test = build_meta(test)\n",
        "print('Meta shapes:', meta_train.shape, meta_test.shape, flush=True)\n",
        "\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = list(sgkf.split(np.zeros(len(y)), y, groups))\n",
        "\n",
        "def train_xgb_bag(emb_tr, emb_te, seeds=(7,13,29), jitter=True, name='model'):\n",
        "    oof_bag = np.zeros(len(train), dtype=np.float32)\n",
        "    test_bag_per_seed = []\n",
        "    for si, seed in enumerate(seeds, 1):\n",
        "        # jitter params slightly per seed\n",
        "        rs = np.random.RandomState(seed)\n",
        "        md = 4 + (rs.rand() < 0.5) if jitter else 4\n",
        "        subs = 0.75 + rs.rand()*0.10 if jitter else 0.80\n",
        "        cols = 0.75 + rs.rand()*0.15 if jitter else 0.80\n",
        "        lr = 0.045 + rs.rand()*0.010 if jitter else 0.05\n",
        "        params = dict(\n",
        "            objective='binary:logistic',\n",
        "            eval_metric='auc',\n",
        "            tree_method='hist',\n",
        "            max_depth=int(md),\n",
        "            learning_rate=float(lr),\n",
        "            subsample=float(subs),\n",
        "            colsample_bytree=float(cols),\n",
        "            reg_lambda=1.0,\n",
        "            min_child_weight=1.0,\n",
        "            max_bin=256,\n",
        "            random_state=int(seed)\n",
        "        )\n",
        "        oof_seed = np.zeros(len(train), dtype=np.float32)\n",
        "        test_preds = []\n",
        "        t_se = time.time()\n",
        "        for i, (trn_idx, val_idx) in enumerate(folds, 1):\n",
        "            t0 = time.time()\n",
        "            scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "            M_tr = scaler.fit_transform(meta_train.iloc[trn_idx].values).astype(np.float32)\n",
        "            M_va = scaler.transform(meta_train.iloc[val_idx].values).astype(np.float32)\n",
        "            X_tr = np.hstack([emb_tr[trn_idx], M_tr]).astype(np.float32)\n",
        "            X_va = np.hstack([emb_tr[val_idx], M_va]).astype(np.float32)\n",
        "            dtr = xgb.DMatrix(X_tr, label=y[trn_idx])\n",
        "            dva = xgb.DMatrix(X_va, label=y[val_idx])\n",
        "            booster = xgb.train(params, dtr, num_boost_round=2000, evals=[(dva,'valid')],\n",
        "                                verbose_eval=False, early_stopping_rounds=100)\n",
        "            oof_seed[val_idx] = booster.predict(dva, iteration_range=(0, booster.best_iteration+1))\n",
        "            M_te = scaler.transform(meta_test.values).astype(np.float32)\n",
        "            Xt = np.hstack([emb_te, M_te]).astype(np.float32)\n",
        "            dte = xgb.DMatrix(Xt)\n",
        "            test_preds.append(booster.predict(dte, iteration_range=(0, booster.best_iteration+1)).astype(np.float32))\n",
        "            print(f'    [{name} seed {seed}] Fold {i} AUC={roc_auc_score(y[val_idx], oof_seed[val_idx]):.5f} | iters={booster.best_iteration+1} | {time.time()-t0:.1f}s', flush=True)\n",
        "        oof_bag += oof_seed / len(seeds)\n",
        "        test_bag_per_seed.append(np.mean(np.vstack(test_preds), axis=0).astype(np.float32))\n",
        "        print(f'  [{name}] Seed {seed} OOF AUC={roc_auc_score(y, oof_seed):.5f} | elapsed {time.time()-t_se:.1f}s', flush=True)\n",
        "    test_bag = np.mean(np.vstack(test_bag_per_seed), axis=0).astype(np.float32)\n",
        "    auc_bag = roc_auc_score(y, oof_bag)\n",
        "    print(f'[{name}] Bagged OOF AUC: {auc_bag:.5f}', flush=True)\n",
        "    return oof_bag, test_bag\n",
        "\n",
        "# Encode MiniLM and MPNet (normalized embeddings) - CPU for stability; dataset is small\n",
        "print('Encoding MiniLM (all-MiniLM-L6-v2)...', flush=True)\n",
        "minilm = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')\n",
        "emb_minilm_tr = minilm.encode(text_tr, batch_size=64, show_progress_bar=False, normalize_embeddings=True).astype(np.float32)\n",
        "emb_minilm_te = minilm.encode(text_te, batch_size=64, show_progress_bar=False, normalize_embeddings=True).astype(np.float32)\n",
        "print('MiniLM shapes:', emb_minilm_tr.shape, emb_minilm_te.shape, flush=True)\n",
        "\n",
        "print('Encoding MPNet (multi-qa-mpnet-base-dot-v1)...', flush=True)\n",
        "mpnet = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1', device='cpu')\n",
        "emb_mpnet_tr = mpnet.encode(text_tr, batch_size=64, show_progress_bar=False, normalize_embeddings=True).astype(np.float32)\n",
        "emb_mpnet_te = mpnet.encode(text_te, batch_size=64, show_progress_bar=False, normalize_embeddings=True).astype(np.float32)\n",
        "print('MPNet shapes:', emb_mpnet_tr.shape, emb_mpnet_te.shape, flush=True)\n",
        "\n",
        "seeds = [7, 13, 29]\n",
        "print('=== Seed bagging XGB on MiniLM embeddings ===', flush=True)\n",
        "oof_st_bag, test_st_bag = train_xgb_bag(emb_minilm_tr, emb_minilm_te, seeds=seeds, jitter=True, name='MiniLM')\n",
        "np.save('oof_st_embed_bag.npy', oof_st_bag); np.save('test_st_embed_bag.npy', test_st_bag)\n",
        "\n",
        "print('=== Seed bagging XGB on MPNet embeddings ===', flush=True)\n",
        "oof_mp_bag, test_mp_bag = train_xgb_bag(emb_mpnet_tr, emb_mpnet_te, seeds=seeds, jitter=True, name='MPNet')\n",
        "np.save('oof_mpnet_embed_bag.npy', oof_mp_bag); np.save('test_mpnet_embed_bag.npy', test_mp_bag)\n",
        "\n",
        "# Rank-then-NNLS including bagged ST models\n",
        "paths = {\n",
        "    'lr_meta_g': ('oof_lr_meta_g.npy','test_lr_meta_g.npy'),\n",
        "    'lr_subs_g': ('oof_lr_subs_g.npy','test_lr_subs_g.npy'),\n",
        "    'xgb_svd_meta': ('oof_xgb_svd_meta.npy','test_xgb_svd_meta.npy'),\n",
        "    'cb_text_g': ('oof_cat_text_g.npy','test_cat_text_g.npy'),\n",
        "    'st_embed_bag': ('oof_st_embed_bag.npy','test_st_embed_bag.npy'),\n",
        "    'mpnet_embed_bag': ('oof_mpnet_embed_bag.npy','test_mpnet_embed_bag.npy'),\n",
        "}\n",
        "oof_list, test_list, names = [], [], []\n",
        "for name, (poof, ptest) in paths.items():\n",
        "    if os.path.exists(poof) and os.path.exists(ptest):\n",
        "        oof_list.append(np.load(poof))\n",
        "        test_list.append(np.load(ptest))\n",
        "        names.append(name)\n",
        "assert len(oof_list) >= 3, f'Not enough models found for rank-NNLS, got {len(oof_list)}'\n",
        "P = np.vstack(oof_list).T\n",
        "T = np.vstack(test_list).T\n",
        "def rank01(a):\n",
        "    order = a.argsort(kind='mergesort')\n",
        "    ranks = np.empty_like(order, dtype=np.float64)\n",
        "    ranks[order] = np.arange(len(a), dtype=np.float64)\n",
        "    return ranks / max(1, len(a)-1)\n",
        "P_rank = np.apply_along_axis(rank01, 0, P)\n",
        "T_rank = np.apply_along_axis(rank01, 0, T)\n",
        "w, _ = nnls(P_rank, y.astype(float))\n",
        "w = w / w.sum() if w.sum() > 0 else w\n",
        "blend_oof_rank = P_rank @ w\n",
        "auc_rank = roc_auc_score(y, blend_oof_rank)\n",
        "print('Models in rank-NNLS (bagged):', names)\n",
        "print('Rank-NNLS weights (bagged):', w)\n",
        "print(f'Rank-blended OOF AUC (bagged): {auc_rank:.5f}')\n",
        "blend_test_rank = T_rank @ w\n",
        "pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': blend_test_rank.astype(np.float32)}).to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (Rank-NNLS with bagged ST). Rows:', len(blend_test_rank))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta shapes: (2878, 42) (1162, 42)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding MiniLM (all-MiniLM-L6-v2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MiniLM shapes: (2878, 384) (1162, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding MPNet (multi-qa-mpnet-base-dot-v1)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You try to use a model that was created with version 3.0.0.dev0, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n\n\n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MPNet shapes: (2878, 768) (1162, 768)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Seed bagging XGB on MiniLM embeddings ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM seed 7] Fold 1 AUC=0.65329 | iters=31 | 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM seed 7] Fold 2 AUC=0.65777 | iters=53 | 1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM seed 7] Fold 3 AUC=0.63962 | iters=69 | 1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM seed 7] Fold 4 AUC=0.66336 | iters=91 | 1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM seed 7] Fold 5 AUC=0.69458 | iters=33 | 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MiniLM] Seed 7 OOF AUC=0.66008 | elapsed 4.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM seed 13] Fold 1 AUC=0.64450 | iters=193 | 1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM seed 13] Fold 2 AUC=0.67770 | iters=44 | 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM seed 13] Fold 3 AUC=0.65054 | iters=87 | 1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM seed 13] Fold 4 AUC=0.63736 | iters=40 | 1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM seed 13] Fold 5 AUC=0.68210 | iters=36 | 0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MiniLM] Seed 13 OOF AUC=0.65290 | elapsed 4.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM seed 29] Fold 1 AUC=0.64283 | iters=22 | 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM seed 29] Fold 2 AUC=0.66292 | iters=19 | 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM seed 29] Fold 3 AUC=0.65113 | iters=321 | 1.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM seed 29] Fold 4 AUC=0.64253 | iters=357 | 2.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM seed 29] Fold 5 AUC=0.67641 | iters=25 | 0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MiniLM] Seed 29 OOF AUC=0.64027 | elapsed 5.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MiniLM] Bagged OOF AUC: 0.66566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Seed bagging XGB on MPNet embeddings ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet seed 7] Fold 1 AUC=0.65987 | iters=87 | 2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet seed 7] Fold 2 AUC=0.69404 | iters=42 | 2.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet seed 7] Fold 3 AUC=0.63298 | iters=73 | 2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet seed 7] Fold 4 AUC=0.66399 | iters=54 | 2.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet seed 7] Fold 5 AUC=0.67982 | iters=67 | 2.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MPNet] Seed 7 OOF AUC=0.66370 | elapsed 11.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet seed 13] Fold 1 AUC=0.67413 | iters=120 | 2.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet seed 13] Fold 2 AUC=0.67233 | iters=45 | 1.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet seed 13] Fold 3 AUC=0.63740 | iters=109 | 2.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet seed 13] Fold 4 AUC=0.66546 | iters=53 | 1.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet seed 13] Fold 5 AUC=0.69143 | iters=92 | 1.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MPNet] Seed 13 OOF AUC=0.66589 | elapsed 8.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet seed 29] Fold 1 AUC=0.67146 | iters=53 | 1.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet seed 29] Fold 2 AUC=0.67579 | iters=55 | 1.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet seed 29] Fold 3 AUC=0.64472 | iters=42 | 1.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet seed 29] Fold 4 AUC=0.66022 | iters=60 | 1.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet seed 29] Fold 5 AUC=0.69020 | iters=69 | 1.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MPNet] Seed 29 OOF AUC=0.66906 | elapsed 7.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MPNet] Bagged OOF AUC: 0.67582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models in rank-NNLS (bagged): ['lr_meta_g', 'lr_subs_g', 'xgb_svd_meta', 'cb_text_g', 'st_embed_bag', 'mpnet_embed_bag']\nRank-NNLS weights (bagged): [0.1359503  0.17626838 0.15552755 0.04538354 0.16800549 0.31886475]\nRank-blended OOF AUC (bagged): 0.69644\nSaved submission.csv (Rank-NNLS with bagged ST). Rows: 1162\n"
          ]
        }
      ]
    },
    {
      "id": "18b934c9-050a-46ef-89fc-aec569babc2b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 3G: Grouped rank-stacking (Logistic Regression on OOF ranks) and submission\n",
        "import numpy as np, pandas as pd, os, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train['requester_received_pizza'].astype(int).values\n",
        "groups = train['requester_username'].fillna('').astype(str).values if 'requester_username' in train.columns else np.arange(len(train))\n",
        "\n",
        "# Use the strongest, diverse grouped models (bagged ST + MPNet, LR_meta_g, LR_subs_g, XGB_svd_meta, CB_text_g)\n",
        "paths = {\n",
        "    'lr_meta_g': ('oof_lr_meta_g.npy','test_lr_meta_g.npy'),\n",
        "    'lr_subs_g': ('oof_lr_subs_g.npy','test_lr_subs_g.npy'),\n",
        "    'xgb_svd_meta': ('oof_xgb_svd_meta.npy','test_xgb_svd_meta.npy'),\n",
        "    'cb_text_g': ('oof_cat_text_g.npy','test_cat_text_g.npy'),\n",
        "    'st_embed_bag': ('oof_st_embed_bag.npy','test_st_embed_bag.npy'),\n",
        "    'mpnet_embed_bag': ('oof_mpnet_embed_bag.npy','test_mpnet_embed_bag.npy'),\n",
        "}\n",
        "oof_list, test_list, names = [], [], []\n",
        "for name, (poof, ptest) in paths.items():\n",
        "    if os.path.exists(poof) and os.path.exists(ptest):\n",
        "        oof_list.append(np.load(poof))\n",
        "        test_list.append(np.load(ptest))\n",
        "        names.append(name)\n",
        "\n",
        "assert len(oof_list) >= 3, f'Need >=3 models for stacking; found {len(oof_list)}'\n",
        "P = np.vstack(oof_list).T  # (n_samples, n_models)\n",
        "T = np.vstack(test_list).T  # (n_test, n_models)\n",
        "\n",
        "def rank01(a):\n",
        "    order = a.argsort(kind='mergesort')\n",
        "    ranks = np.empty_like(order, dtype=np.float64)\n",
        "    ranks[order] = np.arange(len(a), dtype=np.float64)\n",
        "    return ranks / max(1, len(a)-1)\n",
        "\n",
        "P_rank = np.apply_along_axis(rank01, 0, P)\n",
        "T_rank = np.apply_along_axis(rank01, 0, T)\n",
        "\n",
        "# Grouped CV logistic stacker on ranks\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_stack = np.zeros(len(train), dtype=np.float32)\n",
        "test_stack_preds = []\n",
        "fold_times = []\n",
        "for i, (trn_idx, val_idx) in enumerate(sgkf.split(P_rank, y, groups), 1):\n",
        "    t0 = time.time()\n",
        "    X_tr, X_va = P_rank[trn_idx], P_rank[val_idx]\n",
        "    y_tr, y_va = y[trn_idx], y[val_idx]\n",
        "    clf = LogisticRegression(C=1.0, solver='liblinear', max_iter=2000, class_weight=None)\n",
        "    clf.fit(X_tr, y_tr)\n",
        "    oof_stack[val_idx] = clf.predict_proba(X_va)[:,1]\n",
        "    test_stack_preds.append(clf.predict_proba(T_rank)[:,1].astype(np.float32))\n",
        "    fold_times.append(time.time()-t0)\n",
        "    print(f'  Stacker fold {i} AUC={roc_auc_score(y_va, oof_stack[val_idx]):.5f} | {fold_times[-1]:.2f}s', flush=True)\n",
        "\n",
        "auc_stack = roc_auc_score(y, oof_stack)\n",
        "print('Models in stack:', names)\n",
        "print(f'Rank-Logistic stacker OOF AUC: {auc_stack:.5f} | mean fold time: {np.mean(fold_times):.2f}s')\n",
        "\n",
        "# Average test predictions across folds\n",
        "test_stack = np.mean(np.vstack(test_stack_preds), axis=0).astype(np.float32)\n",
        "\n",
        "# Save and submit\n",
        "np.save('oof_rank_stacker.npy', oof_stack)\n",
        "np.save('test_rank_stacker.npy', test_stack)\n",
        "pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_stack}).to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (rank-logistic stacker). Rows:', len(test_stack))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Stacker fold 1 AUC=0.67975 | 0.00s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Stacker fold 2 AUC=0.69998 | 0.00s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Stacker fold 3 AUC=0.67407 | 0.00s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Stacker fold 4 AUC=0.69447 | 0.00s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Stacker fold 5 AUC=0.72669 | 0.00s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models in stack: ['lr_meta_g', 'lr_subs_g', 'xgb_svd_meta', 'cb_text_g', 'st_embed_bag', 'mpnet_embed_bag']\nRank-Logistic stacker OOF AUC: 0.69346 | mean fold time: 0.00s\nSaved submission.csv (rank-logistic stacker). Rows: 1162\n"
          ]
        }
      ]
    },
    {
      "id": "1009c439-ff4f-4348-8e1e-4c357566666c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 3H: Enrich meta with time features (month/quarter/days_since_start/relative_position) and retrain bagged ST-XGB; rank-NNLS blend\n",
        "import sys, subprocess, time, os, re, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'sentence-transformers==2.7.0'], check=True)\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "\n",
        "def load_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path)\n",
        "    except ValueError:\n",
        "        import json as _json\n",
        "        with open(path, 'r') as f:\n",
        "            return pd.DataFrame(_json.load(f))\n",
        "\n",
        "train = load_df('train.json')\n",
        "test = load_df('test.json')\n",
        "target_col = 'requester_received_pizza'\n",
        "title_col = 'request_title' if 'request_title' in train.columns else 'title'\n",
        "body_col = 'request_text_edit_aware' if 'request_text_edit_aware' in train.columns else 'request_text'\n",
        "\n",
        "def build_text_upweighted(df: pd.DataFrame, title_col: str, body_col: str) -> pd.Series:\n",
        "    t = df[title_col].fillna('').astype(str)\n",
        "    b = df[body_col].fillna('').astype(str)\n",
        "    return (t + ' ' + t + ' [SEP] ' + b)\n",
        "\n",
        "text_tr = build_text_upweighted(train, title_col, body_col).tolist()\n",
        "text_te = build_text_upweighted(test, title_col, body_col).tolist()\n",
        "y = train[target_col].astype(int).values\n",
        "groups = train['requester_username'].fillna('').astype(str).values if 'requester_username' in train.columns else np.arange(len(train))\n",
        "\n",
        "# Meta/lexicons + extra time features\n",
        "def count_urls(s: str) -> int: return len(re.findall(r'https?://\\S+', s or ''))\n",
        "def has_imgur(s: str) -> int: return 1 if re.search(r'imgur\\.com', s or '', flags=re.IGNORECASE) else 0\n",
        "def count_digits(s: str) -> int: return sum(ch.isdigit() for ch in (s or ''))\n",
        "def dollar_flag(s: str) -> int: return 1 if ('$' in (s or '')) or re.search(r'\\b\\d+\\s*(dollars|bucks)\\b', s or '', flags=re.IGNORECASE) else 0\n",
        "def caps_ratio(s: str) -> float:\n",
        "    if not s: return 0.0\n",
        "    letters = [c for c in s if c.isalpha()]\n",
        "    return (sum(c.isupper() for c in letters) / max(1, len(letters))) if letters else 0.0\n",
        "def word_count(s: str) -> int: return len((s or '').split())\n",
        "def exclam_count(s: str) -> int: return (s or '').count('!')\n",
        "def question_count(s: str) -> int: return (s or '').count('?')\n",
        "def parse_subreddit_count(x) -> int: return len(x) if isinstance(x, list) else 0\n",
        "def safe_log1p_signed(x): return np.sign(x) * np.log1p(np.abs(x))\n",
        "LEX_PATTERNS = {\n",
        "    'lex_please': r'\\bplease\\b',\n",
        "    'lex_thanks': r'\\b(thank you|thanks in advance|thanks|thank|tia)\\b',\n",
        "    'lex_appreciate': r'\\b(appreciate|appreciated)\\b',\n",
        "    'lex_pay_it_forward': r'\\b(pay it forward|return the favor)\\b',\n",
        "    'lex_repay': r'\\b(repay|pay you back|pay back)\\b',\n",
        "    'lex_willing': r\"\\b(willing to|i\\'ll|i will|i can)\\b\",\n",
        "    'lex_karma': r'\\bkarma\\b',\n",
        "    'lex_evidence': r'\\b(proof|receipt|photo|picture|pic|verify|verification|evidence)\\b',\n",
        "    'lex_imgur_word': r'\\bimgur\\b',\n",
        "    'lex_student': r'\\b(student|college|university|finals|exam|midterm)\\b',\n",
        "    'lex_jobloss': r'\\b(unemployed|laid off|lost my job|between jobs|job hunt)\\b',\n",
        "    'lex_broke': r'\\b(broke)\\b',\n",
        "    'lex_rent_bills': r'\\b(rent|bill|bills|utilities|electric|gas|water|paycheck)\\b',\n",
        "    'lex_family': r'\\b(family|kids?|children|baby|pregnant|son|daughter|wife|husband)\\b',\n",
        "    'lex_hungry': r'\\b(hungry|starving|no food)\\b|\\bfood (stamps|pantry)\\b',\n",
        "    'lex_struggling': r'\\b(desperate|struggling)\\b',\n",
        "    'lex_urgency': r'\\b(tonight|today|tomorrow|asap|urgent)\\b',\n",
        "    'lex_help': r'\\bhelp\\b',\n",
        "}\n",
        "def add_lexicons(df: pd.DataFrame, title_col: str, body_col: str) -> pd.DataFrame:\n",
        "    txt = (df[title_col].fillna('').astype(str) + ' ' + df[body_col].fillna('').astype(str)).str.lower()\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    for name, pat in LEX_PATTERNS.items():\n",
        "        out[name] = txt.str.count(pat, flags=re.IGNORECASE).astype(float)\n",
        "    return out\n",
        "\n",
        "def build_meta_enriched(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    title = df[title_col].fillna('').astype(str)\n",
        "    body = df[body_col].fillna('').astype(str)\n",
        "    ts = pd.to_numeric(df['unix_timestamp_of_request'], errors='coerce')\n",
        "    dt = pd.to_datetime(ts, unit='s', utc=True)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    keep_cols = [\n",
        "        'requester_account_age_in_days_at_request',\n",
        "        'requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request',\n",
        "        'requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request',\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "    ]\n",
        "    for c in keep_cols:\n",
        "        out[c] = pd.to_numeric(df[c], errors='coerce') if c in df.columns else 0.0\n",
        "    out['requester_subreddits_count'] = df['requester_subreddits_at_request'].apply(parse_subreddit_count).astype(float) if 'requester_subreddits_at_request' in df.columns else 0.0\n",
        "    # existing temporal\n",
        "    out['hour'] = dt.dt.hour.astype(float)\n",
        "    out['weekday'] = dt.dt.weekday.astype(float)\n",
        "    out['is_weekend'] = dt.dt.weekday.isin([5,6]).astype(float)\n",
        "    # new temporal features\n",
        "    out['month'] = dt.dt.month.astype(float)\n",
        "    out['quarter'] = dt.dt.quarter.astype(float)\n",
        "    # days since start and relative position\n",
        "    base_ts = np.nanmin(ts.values)\n",
        "    out['days_since_start'] = ((ts - base_ts) / 86400.0).astype(float)\n",
        "    # relative position rank 0..1\n",
        "    order = np.argsort(ts.values)\n",
        "    rel = np.empty_like(order, dtype=np.float64)\n",
        "    rel[order] = np.arange(len(order), dtype=np.float64)\n",
        "    out['relative_position'] = (rel / max(1, len(order)-1)).astype(float)\n",
        "    # basic text stats\n",
        "    out['title_len_chars'] = title.str.len().astype(float)\n",
        "    out['title_len_words'] = title.apply(word_count).astype(float)\n",
        "    out['body_len_chars'] = body.str.len().astype(float)\n",
        "    out['body_len_words'] = body.apply(word_count).astype(float)\n",
        "    out['url_count'] = body.apply(count_urls).astype(float)\n",
        "    out['has_imgur'] = body.apply(has_imgur).astype(float)\n",
        "    out['digits_count'] = body.apply(count_digits).astype(float)\n",
        "    out['dollar_flag'] = body.apply(dollar_flag).astype(float)\n",
        "    out['caps_ratio'] = body.apply(caps_ratio).astype(float)\n",
        "    out['exclam_count'] = body.apply(exclam_count).astype(float)\n",
        "    out['question_count'] = body.apply(question_count).astype(float)\n",
        "    # lexicons\n",
        "    lex = add_lexicons(df, title_col, body_col)\n",
        "    out = pd.concat([out, lex], axis=1)\n",
        "    # transforms\n",
        "    out = out.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "    nonneg_cols = [\n",
        "        'requester_account_age_in_days_at_request','requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request','requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request','requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request','requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_subreddits_count','hour','weekday','is_weekend','month','quarter','days_since_start','relative_position',\n",
        "        'title_len_chars','title_len_words','body_len_chars','body_len_words',\n",
        "        'url_count','has_imgur','digits_count','dollar_flag','exclam_count','question_count',\n",
        "        *list(LEX_PATTERNS.keys()),\n",
        "    ]\n",
        "    for c in nonneg_cols:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(np.clip(out[c], a_min=0.0, a_max=None))\n",
        "    if 'requester_upvotes_minus_downvotes_at_request' in out.columns:\n",
        "        out['requester_upvotes_minus_downvotes_at_request'] = safe_log1p_signed(out['requester_upvotes_minus_downvotes_at_request'].values)\n",
        "    return out.replace([np.inf,-np.inf],0.0).fillna(0.0)\n",
        "\n",
        "meta_train = build_meta_enriched(train)\n",
        "meta_test = build_meta_enriched(test)\n",
        "print('Meta(enriched) shapes:', meta_train.shape, meta_test.shape, flush=True)\n",
        "\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = list(sgkf.split(np.zeros(len(y)), y, groups))\n",
        "\n",
        "def train_xgb_bag(emb_tr, emb_te, seeds=(7,13,29), jitter=True, name='model'):\n",
        "    oof_bag = np.zeros(len(train), dtype=np.float32)\n",
        "    test_bag_per_seed = []\n",
        "    for seed in seeds:\n",
        "        rs = np.random.RandomState(seed)\n",
        "        md = 4 + (rs.rand() < 0.5) if jitter else 4\n",
        "        subs = 0.75 + rs.rand()*0.10 if jitter else 0.80\n",
        "        cols = 0.75 + rs.rand()*0.15 if jitter else 0.80\n",
        "        lr = 0.045 + rs.rand()*0.010 if jitter else 0.05\n",
        "        params = dict(\n",
        "            objective='binary:logistic',\n",
        "            eval_metric='auc',\n",
        "            tree_method='hist',\n",
        "            max_depth=int(md),\n",
        "            learning_rate=float(lr),\n",
        "            subsample=float(subs),\n",
        "            colsample_bytree=float(cols),\n",
        "            reg_lambda=1.0,\n",
        "            min_child_weight=1.0,\n",
        "            max_bin=256,\n",
        "            random_state=int(seed)\n",
        "        )\n",
        "        oof_seed = np.zeros(len(train), dtype=np.float32)\n",
        "        test_preds = []\n",
        "        for i, (trn_idx, val_idx) in enumerate(folds, 1):\n",
        "            t0 = time.time()\n",
        "            scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "            M_tr = scaler.fit_transform(meta_train.iloc[trn_idx].values).astype(np.float32)\n",
        "            M_va = scaler.transform(meta_train.iloc[val_idx].values).astype(np.float32)\n",
        "            X_tr = np.hstack([emb_tr[trn_idx], M_tr]).astype(np.float32)\n",
        "            X_va = np.hstack([emb_tr[val_idx], M_va]).astype(np.float32)\n",
        "            dtr = xgb.DMatrix(X_tr, label=y[trn_idx])\n",
        "            dva = xgb.DMatrix(X_va, label=y[val_idx])\n",
        "            booster = xgb.train(params, dtr, num_boost_round=2000, evals=[(dva,'valid')], verbose_eval=False, early_stopping_rounds=100)\n",
        "            oof_seed[val_idx] = booster.predict(dva, iteration_range=(0, booster.best_iteration+1))\n",
        "            M_te = scaler.transform(meta_test.values).astype(np.float32)\n",
        "            Xt = np.hstack([emb_te, M_te]).astype(np.float32)\n",
        "            dte = xgb.DMatrix(Xt)\n",
        "            test_preds.append(booster.predict(dte, iteration_range=(0, booster.best_iteration+1)).astype(np.float32))\n",
        "            print(f'    [{name} seed {seed}] Fold {i} AUC={roc_auc_score(y[val_idx], oof_seed[val_idx]):.5f} | {time.time()-t0:.1f}s', flush=True)\n",
        "        oof_bag += oof_seed / len(seeds)\n",
        "        test_bag_per_seed.append(np.mean(np.vstack(test_preds), axis=0).astype(np.float32))\n",
        "        print(f'  [{name}] Seed {seed} OOF AUC={roc_auc_score(y, oof_seed):.5f}', flush=True)\n",
        "    test_bag = np.mean(np.vstack(test_bag_per_seed), axis=0).astype(np.float32)\n",
        "    auc_bag = roc_auc_score(y, oof_bag)\n",
        "    print(f'[{name}] Bagged OOF AUC: {auc_bag:.5f}', flush=True)\n",
        "    return oof_bag, test_bag\n",
        "\n",
        "print('Encoding MiniLM (all-MiniLM-L6-v2)...', flush=True)\n",
        "minilm = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')\n",
        "emb_minilm_tr = minilm.encode(text_tr, batch_size=64, show_progress_bar=False, normalize_embeddings=True).astype(np.float32)\n",
        "emb_minilm_te = minilm.encode(text_te, batch_size=64, show_progress_bar=False, normalize_embeddings=True).astype(np.float32)\n",
        "print('MiniLM shapes:', emb_minilm_tr.shape, emb_minilm_te.shape, flush=True)\n",
        "\n",
        "print('Encoding MPNet (multi-qa-mpnet-base-dot-v1)...', flush=True)\n",
        "mpnet = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1', device='cpu')\n",
        "emb_mpnet_tr = mpnet.encode(text_tr, batch_size=64, show_progress_bar=False, normalize_embeddings=True).astype(np.float32)\n",
        "emb_mpnet_te = mpnet.encode(text_te, batch_size=64, show_progress_bar=False, normalize_embeddings=True).astype(np.float32)\n",
        "print('MPNet shapes:', emb_mpnet_tr.shape, emb_mpnet_te.shape, flush=True)\n",
        "\n",
        "seeds = [7, 13, 29]\n",
        "print('=== Bagging XGB on MiniLM with enriched meta ===', flush=True)\n",
        "oof_st_bag2, test_st_bag2 = train_xgb_bag(emb_minilm_tr, emb_minilm_te, seeds=seeds, jitter=True, name='MiniLM+time')\n",
        "np.save('oof_st_embed_bag_v2.npy', oof_st_bag2); np.save('test_st_embed_bag_v2.npy', test_st_bag2)\n",
        "\n",
        "print('=== Bagging XGB on MPNet with enriched meta ===', flush=True)\n",
        "oof_mp_bag2, test_mp_bag2 = train_xgb_bag(emb_mpnet_tr, emb_mpnet_te, seeds=seeds, jitter=True, name='MPNet+time')\n",
        "np.save('oof_mpnet_embed_bag_v2.npy', oof_mp_bag2); np.save('test_mpnet_embed_bag_v2.npy', test_mp_bag2)\n",
        "\n",
        "# Rank-then-NNLS including enriched bagged models\n",
        "paths = {\n",
        "    'lr_meta_g': ('oof_lr_meta_g.npy','test_lr_meta_g.npy'),\n",
        "    'lr_subs_g': ('oof_lr_subs_g.npy','test_lr_subs_g.npy'),\n",
        "    'xgb_svd_meta': ('oof_xgb_svd_meta.npy','test_xgb_svd_meta.npy'),\n",
        "    'cb_text_g': ('oof_cat_text_g.npy','test_cat_text_g.npy'),\n",
        "    'st_embed_bag_v2': ('oof_st_embed_bag_v2.npy','test_st_embed_bag_v2.npy'),\n",
        "    'mpnet_embed_bag_v2': ('oof_mpnet_embed_bag_v2.npy','test_mpnet_embed_bag_v2.npy'),\n",
        "}\n",
        "oof_list, test_list, names = [], [], []\n",
        "for name, (poof, ptest) in paths.items():\n",
        "    if os.path.exists(poof) and os.path.exists(ptest):\n",
        "        oof_list.append(np.load(poof))\n",
        "        test_list.append(np.load(ptest))\n",
        "        names.append(name)\n",
        "assert len(oof_list) >= 3, 'Not enough models for rank-NNLS'\n",
        "P = np.vstack(oof_list).T\n",
        "T = np.vstack(test_list).T\n",
        "def rank01(a):\n",
        "    order = a.argsort(kind='mergesort')\n",
        "    ranks = np.empty_like(order, dtype=np.float64)\n",
        "    ranks[order] = np.arange(len(a), dtype=np.float64)\n",
        "    return ranks / max(1, len(a)-1)\n",
        "P_rank = np.apply_along_axis(rank01, 0, P)\n",
        "T_rank = np.apply_along_axis(rank01, 0, T)\n",
        "w, _ = nnls(P_rank, y.astype(float))\n",
        "w = w / w.sum() if w.sum() > 0 else w\n",
        "blend_oof_rank = P_rank @ w\n",
        "auc_rank = roc_auc_score(y, blend_oof_rank)\n",
        "print('Models in rank-NNLS (enriched):', names)\n",
        "print('Rank-NNLS weights (enriched):', w)\n",
        "print(f'Rank-blended OOF AUC (enriched): {auc_rank:.5f}')\n",
        "blend_test_rank = T_rank @ w\n",
        "pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': blend_test_rank.astype(np.float32)}).to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (Rank-NNLS with enriched bagged ST). Rows:', len(blend_test_rank))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta(enriched) shapes: (2878, 46) (1162, 46)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding MiniLM (all-MiniLM-L6-v2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MiniLM shapes: (2878, 384) (1162, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding MPNet (multi-qa-mpnet-base-dot-v1)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You try to use a model that was created with version 3.0.0.dev0, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n\n\n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MPNet shapes: (2878, 768) (1162, 768)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Bagging XGB on MiniLM with enriched meta ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM+time seed 7] Fold 1 AUC=0.66065 | 1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM+time seed 7] Fold 2 AUC=0.67639 | 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM+time seed 7] Fold 3 AUC=0.67094 | 1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM+time seed 7] Fold 4 AUC=0.68056 | 1.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM+time seed 7] Fold 5 AUC=0.70922 | 0.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MiniLM+time] Seed 7 OOF AUC=0.67355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM+time seed 13] Fold 1 AUC=0.67356 | 1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM+time seed 13] Fold 2 AUC=0.68896 | 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM+time seed 13] Fold 3 AUC=0.68510 | 1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM+time seed 13] Fold 4 AUC=0.67203 | 1.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM+time seed 13] Fold 5 AUC=0.70925 | 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MiniLM+time] Seed 13 OOF AUC=0.67955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM+time seed 29] Fold 1 AUC=0.65940 | 1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM+time seed 29] Fold 2 AUC=0.70464 | 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM+time seed 29] Fold 3 AUC=0.67585 | 1.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM+time seed 29] Fold 4 AUC=0.65632 | 0.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MiniLM+time seed 29] Fold 5 AUC=0.72977 | 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MiniLM+time] Seed 29 OOF AUC=0.67563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MiniLM+time] Bagged OOF AUC: 0.68704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Bagging XGB on MPNet with enriched meta ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet+time seed 7] Fold 1 AUC=0.67895 | 2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet+time seed 7] Fold 2 AUC=0.69270 | 2.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet+time seed 7] Fold 3 AUC=0.65411 | 2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet+time seed 7] Fold 4 AUC=0.66384 | 2.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet+time seed 7] Fold 5 AUC=0.70348 | 2.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MPNet+time] Seed 7 OOF AUC=0.67563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet+time seed 13] Fold 1 AUC=0.68036 | 1.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet+time seed 13] Fold 2 AUC=0.69828 | 1.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet+time seed 13] Fold 3 AUC=0.64112 | 1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet+time seed 13] Fold 4 AUC=0.66988 | 1.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet+time seed 13] Fold 5 AUC=0.72449 | 1.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MPNet+time] Seed 13 OOF AUC=0.68266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet+time seed 29] Fold 1 AUC=0.67726 | 1.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet+time seed 29] Fold 2 AUC=0.70656 | 3.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet+time seed 29] Fold 3 AUC=0.64816 | 1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet+time seed 29] Fold 4 AUC=0.65756 | 1.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [MPNet+time seed 29] Fold 5 AUC=0.71933 | 1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MPNet+time] Seed 29 OOF AUC=0.67267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MPNet+time] Bagged OOF AUC: 0.69010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models in rank-NNLS (enriched): ['lr_meta_g', 'lr_subs_g', 'xgb_svd_meta', 'cb_text_g', 'st_embed_bag_v2', 'mpnet_embed_bag_v2']\nRank-NNLS weights (enriched): [0.15353256 0.10420435 0.09925398 0.         0.29518131 0.3478278 ]\nRank-blended OOF AUC (enriched): 0.70824\nSaved submission.csv (Rank-NNLS with enriched bagged ST). Rows: 1162\n"
          ]
        }
      ]
    },
    {
      "id": "fdc1632f-50d6-4eae-b240-9a192c7836b1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 4: DeBERTa-v3-small fine-tune (mean-pool head) with StratifiedGroupKFold, add to rank-NNLS\n",
        "import os, sys, time, json, math, numpy as np, pandas as pd, subprocess, shutil\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# ================= Install GPU torch + transformers (CUDA 12.1) =================\n",
        "try:\n",
        "    import torch\n",
        "    cuda_ok = torch.cuda.is_available() and str(getattr(torch.version, 'cuda', '0')).startswith('12.1')\n",
        "except Exception:\n",
        "    cuda_ok = False\n",
        "if not cuda_ok:\n",
        "    # Clean any prior torch installs\n",
        "    for pkg in (\"torch\",\"torchvision\",\"torchaudio\"):\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", pkg], check=False)\n",
        "    # Install exact cu121 stack\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                    \"--index-url\", \"https://download.pytorch.org/whl/cu121\",\n",
        "                    \"--extra-index-url\", \"https://pypi.org/simple\",\n",
        "                    \"torch==2.4.1\", \"torchvision==0.19.1\", \"torchaudio==2.4.1\"], check=True)\n",
        "    # Transformers + accelerate\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                    \"transformers==4.44.2\", \"accelerate==0.34.2\", \"scikit-learn==1.4.2\"], check=True)\n",
        "# Ensure sentencepiece is available for DeBERTa-v3 tokenizer\n",
        "try:\n",
        "    import sentencepiece  # noqa: F401\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"sentencepiece\"], check=True)\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoModel, DataCollatorWithPadding, Trainer, TrainingArguments\n",
        "from transformers.models.deberta_v2 import DebertaV2Tokenizer\n",
        "\n",
        "print('torch:', torch.__version__, 'cuda:', getattr(torch.version, 'cuda', None), 'cuda_available:', torch.cuda.is_available(), flush=True)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ================= Data =================\n",
        "def load_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path)\n",
        "    except ValueError:\n",
        "        with open(path, 'r') as f:\n",
        "            return pd.DataFrame(json.load(f))\n",
        "\n",
        "train = load_df('train.json')\n",
        "test = load_df('test.json')\n",
        "target_col = 'requester_received_pizza'\n",
        "title_col = 'request_title' if 'request_title' in train.columns else 'title'\n",
        "body_col = 'request_text_edit_aware' if 'request_text_edit_aware' in train.columns else 'request_text'\n",
        "y = train[target_col].astype(int).values\n",
        "groups = train['requester_username'].fillna('').astype(str).values if 'requester_username' in train.columns else np.arange(len(train))\n",
        "\n",
        "def build_text(df):\n",
        "    t = df[title_col].fillna('').astype(str)\n",
        "    b = df[body_col].fillna('').astype(str)\n",
        "    return (t + ' [SEP] ' + t + ' [SEP] ' + b).tolist()\n",
        "\n",
        "texts_tr = build_text(train)\n",
        "texts_te = build_text(test)\n",
        "print('Data sizes:', len(texts_tr), len(texts_te), flush=True)\n",
        "\n",
        "# ================= Tokenizer (force slow) =================\n",
        "model_name = 'microsoft/deberta-v3-small'\n",
        "tok = DebertaV2Tokenizer.from_pretrained(model_name, do_lower_case=False)\n",
        "max_len = 384\n",
        "\n",
        "class TxtDataset(Dataset):\n",
        "    def __init__(self, texts, labels=None):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        enc = tok(self.texts[idx], truncation=True, max_length=max_len, return_tensors=None)\n",
        "        if self.labels is not None:\n",
        "            enc['labels'] = float(self.labels[idx])\n",
        "        return enc\n",
        "\n",
        "ds_all = TxtDataset(texts_tr, y)\n",
        "ds_test = TxtDataset(texts_te, None)\n",
        "collate = DataCollatorWithPadding(tokenizer=tok, pad_to_multiple_of=8)  # helps fp16\n",
        "\n",
        "# ================= Mean-pool head =================\n",
        "class MeanPoolHead(nn.Module):\n",
        "    def __init__(self, in_dim, p=0.2):\n",
        "        super().__init__()\n",
        "        self.ln = nn.LayerNorm(in_dim)\n",
        "        self.drop = nn.Dropout(p)\n",
        "        self.fc = nn.Linear(in_dim, 1)\n",
        "    def forward(self, last_hidden_state, attention_mask):\n",
        "        mask = attention_mask.unsqueeze(-1).to(last_hidden_state.dtype)\n",
        "        summed = (last_hidden_state * mask).sum(dim=1)\n",
        "        counts = mask.sum(dim=1).clamp(min=1e-6)\n",
        "        mean = summed / counts\n",
        "        x = self.ln(mean)\n",
        "        x = self.drop(x)\n",
        "        return self.fc(x).squeeze(-1)\n",
        "\n",
        "class MeanPoolDeberta(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "        self.backbone = AutoModel.from_pretrained(name)\n",
        "        hidden = self.backbone.config.hidden_size\n",
        "        self.head = MeanPoolHead(hidden, p=0.2)\n",
        "    def forward(self, input_ids=None, attention_mask=None):\n",
        "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = self.head(out.last_hidden_state, attention_mask)\n",
        "        return logits\n",
        "\n",
        "# ================= Trainer utils =================\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]\n",
        "    probs = 1/(1+np.exp(-logits))\n",
        "    try:\n",
        "        auc = roc_auc_score(labels, probs)\n",
        "    except Exception:\n",
        "        auc = 0.5\n",
        "    return {'roc_auc': float(auc)}\n",
        "\n",
        "class WrapModel(nn.Module):\n",
        "    def __init__(self, base):\n",
        "        super().__init__()\n",
        "        self.base = base\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
        "        logits = self.base(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        if labels is not None:\n",
        "            loss = self.loss_fn(logits.view(-1), labels.float().view(-1))\n",
        "            return {'loss': loss, 'logits': logits.detach()}\n",
        "        return {'logits': logits}\n",
        "\n",
        "# ================= 5-fold StratifiedGroupKFold =================\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = list(sgkf.split(np.zeros(len(y)), y, groups))\n",
        "oof = np.zeros(len(y), dtype=np.float32)\n",
        "test_preds_folds = []\n",
        "fold_times = []\n",
        "\n",
        "for fi, (trn_idx, val_idx) in enumerate(folds, 1):\n",
        "    t0 = time.time()\n",
        "    train_texts = [texts_tr[i] for i in trn_idx]\n",
        "    val_texts = [texts_tr[i] for i in val_idx]\n",
        "    train_labels = y[trn_idx]\n",
        "    val_labels = y[val_idx]\n",
        "    ds_tr = TxtDataset(train_texts, train_labels)\n",
        "    ds_va = TxtDataset(val_texts, val_labels)\n",
        "    model = MeanPoolDeberta(model_name)\n",
        "    model.to(device)\n",
        "    wrap = WrapModel(model)\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"debv3_fold{fi}\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=64,\n",
        "        gradient_accumulation_steps=2,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        warmup_ratio=0.06,\n",
        "        fp16=True,\n",
        "        logging_steps=50,\n",
        "        evaluation_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='roc_auc',\n",
        "        greater_is_better=True,\n",
        "        report_to=[],\n",
        "        dataloader_num_workers=2\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=wrap,\n",
        "        args=args,\n",
        "        train_dataset=ds_tr,\n",
        "        eval_dataset=ds_va,\n",
        "        tokenizer=tok,\n",
        "        data_collator=collate,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "    trainer.train()\n",
        "    # Val preds\n",
        "    val_outputs = trainer.predict(ds_va)\n",
        "    val_logits = val_outputs.predictions if not isinstance(val_outputs.predictions, tuple) else val_outputs.predictions[0]\n",
        "    val_probs = 1/(1+np.exp(-val_logits))\n",
        "    oof[val_idx] = val_probs.astype(np.float32)\n",
        "    fold_auc = roc_auc_score(val_labels, oof[val_idx])\n",
        "    # Test preds\n",
        "    te_outputs = trainer.predict(ds_test)\n",
        "    te_logits = te_outputs.predictions if not isinstance(te_outputs.predictions, tuple) else te_outputs.predictions[0]\n",
        "    te_probs = 1/(1+np.exp(-te_logits))\n",
        "    test_preds_folds.append(te_probs.astype(np.float32))\n",
        "    dt = time.time()-t0\n",
        "    fold_times.append(dt)\n",
        "    print(f\"  DeBERTa fold {fi} AUC={fold_auc:.5f} | {dt/60:.1f} min\", flush=True)\n",
        "\n",
        "auc_oof = roc_auc_score(y, oof)\n",
        "print(f'DeBERTa-v3-small OOF AUC: {auc_oof:.5f} | mean fold time: {np.mean(fold_times)/60:.1f} min', flush=True)\n",
        "test_mean = np.mean(np.vstack(test_preds_folds), axis=0).astype(np.float32)\n",
        "np.save('oof_deb_v3_small.npy', oof)\n",
        "np.save('test_deb_v3_small.npy', test_mean)\n",
        "\n",
        "# ================= Rank-then-NNLS including DeBERTa =================\n",
        "from scipy.optimize import nnls\n",
        "def rank01(a):\n",
        "    order = a.argsort(kind='mergesort')\n",
        "    ranks = np.empty_like(order, dtype=np.float64)\n",
        "    ranks[order] = np.arange(len(a), dtype=np.float64)\n",
        "    return ranks / max(1, len(a)-1)\n",
        "\n",
        "paths = {\n",
        "    'lr_meta_g': ('oof_lr_meta_g.npy','test_lr_meta_g.npy'),\n",
        "    'lr_subs_g': ('oof_lr_subs_g.npy','test_lr_subs_g.npy'),\n",
        "    'xgb_svd_meta': ('oof_xgb_svd_meta.npy','test_xgb_svd_meta.npy'),\n",
        "    'cb_text_g': ('oof_cat_text_g.npy','test_cat_text_g.npy'),\n",
        "    'st_embed_bag_v2': ('oof_st_embed_bag_v2.npy','test_st_embed_bag_v2.npy') if os.path.exists('oof_st_embed_bag_v2.npy') else ('oof_st_embed_bag.npy','test_st_embed_bag.npy'),\n",
        "    'mpnet_embed_bag_v2': ('oof_mpnet_embed_bag_v2.npy','test_mpnet_embed_bag_v2.npy') if os.path.exists('oof_mpnet_embed_bag_v2.npy') else ('oof_mpnet_embed_bag.npy','test_mpnet_embed_bag.npy'),\n",
        "    'deb_v3_small': ('oof_deb_v3_small.npy','test_deb_v3_small.npy'),\n",
        "}\n",
        "oof_list, test_list, names = [], [], []\n",
        "for name, (poof, ptest) in paths.items():\n",
        "    if os.path.exists(poof) and os.path.exists(ptest):\n",
        "        oof_list.append(np.load(poof))\n",
        "        test_list.append(np.load(ptest))\n",
        "        names.append(name)\n",
        "assert len(oof_list) >= 3, f'Not enough models for rank-NNLS, got {len(oof_list)}'\n",
        "P = np.vstack(oof_list).T\n",
        "T = np.vstack(test_list).T\n",
        "P_rank = np.apply_along_axis(rank01, 0, P)\n",
        "T_rank = np.apply_along_axis(rank01, 0, T)\n",
        "w, _ = nnls(P_rank, y.astype(float))\n",
        "w = w / w.sum() if w.sum() > 0 else w\n",
        "blend_oof_rank = P_rank @ w\n",
        "auc_rank = roc_auc_score(y, blend_oof_rank)\n",
        "print('Rank-NNLS models:', names)\n",
        "print('Rank-NNLS weights:', w)\n",
        "print(f'Rank-blended OOF AUC (with DeBERTa): {auc_rank:.5f}')\n",
        "blend_test_rank = T_rank @ w\n",
        "pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': blend_test_rank.astype(np.float32)}).to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (Rank-NNLS + DeBERTa). Rows:', len(blend_test_rank))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.8.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uninstalling torch-2.8.0:\n  Successfully uninstalled torch-2.8.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchvision 0.19.1+cu121\nUninstalling torchvision-0.19.1+cu121:\n  Successfully uninstalled torchvision-0.19.1+cu121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchaudio 2.4.1+cu121\nUninstalling torchaudio-2.4.1+cu121:\n  Successfully uninstalled torchaudio-2.4.1+cu121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121, https://pypi.org/simple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (799.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 MB 534.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.19.1\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.1/7.1 MB 517.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.4/3.4 MB 81.0 MB/s eta 0:00:00\nCollecting jinja2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 6.6 MB/s eta 0:00:00\nCollecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 96.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 61.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 153.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 186.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 120.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 194.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 531.4 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 108.4 MB/s eta 0:00:00\nCollecting fsspec\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 495.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing-extensions>=4.8.0\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 340.2 MB/s eta 0:00:00\nCollecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 416.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 159.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 435.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 145.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 139.3 MB/s eta 0:00:00\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 235.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 91.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow!=8.3.*,>=5.3.0\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 160.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 258.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 512.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.3 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pillow-11.3.0 sympy-1.14.0 torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121 triton-3.0.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/torchgen already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torch-2.4.1+cu121.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusolver_cu12-11.4.5.107.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2-3.1.6.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cudnn_cu12-9.1.0.70.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusparse_cu12-12.1.0.106.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton-3.0.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock-3.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec-2025.9.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe-3.0.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx-3.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cublas_cu12-12.1.3.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_cupti_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_nvrtc_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_runtime_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cufft_cu12-11.0.2.54.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_curand_cu12-10.3.2.106.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nccl_cu12-2.20.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvjitlink_cu12-12.9.86.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvtx_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow-11.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/PIL already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy-1.14.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/isympy.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath-1.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/share already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.44.2\n  Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.5/9.5 MB 94.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate==0.34.2\n  Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 324.4/324.4 KB 494.9 MB/s eta 0:00:00\nCollecting scikit-learn==1.4.2\n  Downloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.1/12.1 MB 132.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy>=1.17\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 457.2 MB/s eta 0:00:00\nCollecting requests\n  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.7/64.7 KB 423.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers<0.20,>=0.19\n  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.6/3.6 MB 152.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting regex!=2019.12.17\n  Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 KB 237.1 MB/s eta 0:00:00\nCollecting packaging>=20.0\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 399.7 MB/s eta 0:00:00\nCollecting huggingface-hub<1.0,>=0.23.2\n  Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 563.3/563.3 KB 442.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting safetensors>=0.4.1\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 485.8/485.8 KB 319.9 MB/s eta 0:00:00\nCollecting pyyaml>=5.1\n  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 806.6/806.6 KB 454.8 MB/s eta 0:00:00\nCollecting tqdm>=4.27\n  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.5/78.5 KB 421.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch>=1.10.0\n  Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 888.1/888.1 MB 232.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psutil\n  Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 291.2/291.2 KB 476.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy>=1.6.0\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.9/35.9 MB 235.0 MB/s eta 0:00:00\nCollecting threadpoolctl>=2.0.0\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nCollecting joblib>=1.2.0\n  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 308.4/308.4 KB 446.8 MB/s eta 0:00:00\nCollecting typing-extensions>=3.7.4.3\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 286.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fsspec>=2023.5.0\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 432.0 MB/s eta 0:00:00\nCollecting hf-xet<2.0.0,>=1.1.3\n  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.2/3.2 MB 257.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusolver-cu12==11.7.3.90\n  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 267.5/267.5 MB 158.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nccl-cu12==2.27.3\n  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 322.4/322.4 MB 179.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy>=1.13.3\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 373.6 MB/s eta 0:00:00\nCollecting nvidia-curand-cu12==10.3.9.90\n  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 63.6/63.6 MB 117.7 MB/s eta 0:00:00\nCollecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 451.7 MB/s eta 0:00:00\nCollecting nvidia-nvjitlink-cu12==12.8.93\n  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.3/39.3 MB 236.0 MB/s eta 0:00:00\nCollecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 497.9 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.8.90\n  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10.2/10.2 MB 182.1 MB/s eta 0:00:00\nCollecting nvidia-cusparselt-cu12==0.7.1\n  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 287.2/287.2 MB 56.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusparse-cu12==12.5.8.93\n  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 288.2/288.2 MB 162.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93\n  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 88.0/88.0 MB 163.4 MB/s eta 0:00:00\nCollecting nvidia-cudnn-cu12==9.10.2.21\n  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 706.8/706.8 MB 194.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cublas-cu12==12.8.4.1\n  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 594.3/594.3 MB 176.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton==3.4.0\n  Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 155.5/155.5 MB 176.0 MB/s eta 0:00:00\nCollecting nvidia-cufile-cu12==1.13.1.3\n  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.2/1.2 MB 342.2 MB/s eta 0:00:00\nCollecting nvidia-nvtx-cu12==12.8.90\n  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 90.0/90.0 KB 356.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.8.90\n  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 954.8/954.8 KB 499.0 MB/s eta 0:00:00\nCollecting nvidia-cufft-cu12==11.3.3.83\n  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 193.1/193.1 MB 196.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting setuptools>=40.8.0\n  Downloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.2/1.2 MB 472.2 MB/s eta 0:00:00\nCollecting charset_normalizer<4,>=2\n  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 150.3/150.3 KB 464.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting urllib3<3,>=1.21.1\n  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 129.8/129.8 KB 472.5 MB/s eta 0:00:00\nCollecting certifi>=2017.4.17\n  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 161.2/161.2 KB 472.8 MB/s eta 0:00:00\nCollecting idna<4,>=2.5\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 70.4/70.4 KB 375.6 MB/s eta 0:00:00\nCollecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 494.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: nvidia-cusparselt-cu12, mpmath, urllib3, typing-extensions, tqdm, threadpoolctl, sympy, setuptools, safetensors, regex, pyyaml, psutil, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, joblib, idna, hf-xet, fsspec, filelock, charset_normalizer, certifi, triton, scipy, requests, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, scikit-learn, nvidia-cusolver-cu12, huggingface-hub, torch, tokenizers, transformers, accelerate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchvision 0.19.1+cu121 requires torch==2.4.1, but you have torch 2.8.0 which is incompatible.\ntorchaudio 2.4.1+cu121 requires torch==2.4.1, but you have torch 2.8.0 which is incompatible.\nWARNING: Target directory /app/.pip-target/accelerate-0.34.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/accelerate already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/transformers-4.44.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/transformers already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tokenizers already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tokenizers-0.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchgen already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/functorch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/huggingface_hub-0.35.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/huggingface_hub already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusolver_cu12-11.7.3.90.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scikit_learn-1.4.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scikit_learn.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sklearn already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2-3.1.6.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cudnn_cu12-9.10.2.21.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cufft_cu12-11.3.3.83.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusparse_cu12-12.5.8.93.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/requests-2.32.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/requests already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy-1.16.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton-3.4.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/certifi-2025.8.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/certifi already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/charset_normalizer-3.4.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/charset_normalizer already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock-3.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec-2025.9.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/hf_xet-1.1.10.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/hf_xet already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/idna-3.10.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/idna already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/joblib-1.5.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/joblib already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe-3.0.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx-3.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cublas_cu12-12.8.4.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_cupti_cu12-12.8.90.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_nvrtc_cu12-12.8.93.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_runtime_cu12-12.8.90.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cufile_cu12-1.13.1.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_curand_cu12-10.3.9.90.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nccl_cu12-2.27.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvjitlink_cu12-12.8.93.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvtx_cu12-12.8.90.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging-25.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/psutil-7.1.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/psutil already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pyyaml-6.0.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/yaml already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/_yaml already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/regex-2025.9.18.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/regex already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/safetensors already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/safetensors-0.6.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/setuptools-80.9.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/setuptools already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pkg_resources already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/_distutils_hack already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/distutils-precedence.pth already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy-1.14.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/isympy.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/threadpoolctl-3.6.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/threadpoolctl.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tqdm-4.67.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tqdm already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/urllib3-2.5.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/urllib3 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath-1.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusparselt_cu12-0.7.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/share already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.3 accelerate-0.34.2 certifi-2025.8.3 charset_normalizer-3.4.3 filelock-3.19.1 fsspec-2025.9.0 hf-xet-1.1.10 huggingface-hub-0.35.1 idna-3.10 jinja2-3.1.6 joblib-1.5.2 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 packaging-25.0 psutil-7.1.0 pyyaml-6.0.3 regex-2025.9.18 requests-2.32.5 safetensors-0.6.2 scikit-learn-1.4.2 scipy-1.16.2 setuptools-80.9.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.19.1 torch-2.8.0 tqdm-4.67.1 transformers-4.44.2 triton-3.4.0 typing-extensions-4.15.0 urllib3-2.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.8.0+cu128 cuda: 12.8 cuda_available: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data sizes: 2878 1162\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "\nDebertaV2Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# ================= Tokenizer (force slow) =================\u001b[39;00m\n\u001b[32m     66\u001b[39m model_name = \u001b[33m'\u001b[39m\u001b[33mmicrosoft/deberta-v3-small\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m tok = \u001b[43mDebertaV2Tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m(model_name, do_lower_case=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     68\u001b[39m max_len = \u001b[32m384\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mTxtDataset\u001b[39;00m(Dataset):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/transformers/utils/import_utils.py:2142\u001b[39m, in \u001b[36mDummyObject.__getattribute__\u001b[39m\u001b[34m(cls, key)\u001b[39m\n\u001b[32m   2140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (key.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33m_from_config\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mis_dummy\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mmro\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m2142\u001b[39m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/transformers/utils/import_utils.py:2128\u001b[39m, in \u001b[36mrequires_backends\u001b[39m\u001b[34m(obj, backends)\u001b[39m\n\u001b[32m   2125\u001b[39m         failed.append(msg.format(name))\n\u001b[32m   2127\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[32m-> \u001b[39m\u001b[32m2128\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(failed))\n",
            "\u001b[31mImportError\u001b[39m: \nDebertaV2Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
          ]
        }
      ]
    },
    {
      "id": "a38c82d8-5df7-491e-bebe-4bbac65627e1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 4A: Add intfloat/e5-base-v2 embeddings + meta (XGB, grouped CV) and rank-NNLS blend\n",
        "import sys, subprocess, time, os, re, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "# sentence-transformers + xgboost\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'sentence-transformers==2.7.0'], check=True)\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "\n",
        "def load_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path)\n",
        "    except ValueError:\n",
        "        import json as _json\n",
        "        with open(path, 'r') as f:\n",
        "            return pd.DataFrame(_json.load(f))\n",
        "\n",
        "train = load_df('train.json')\n",
        "test = load_df('test.json')\n",
        "target_col = 'requester_received_pizza'\n",
        "title_col = 'request_title' if 'request_title' in train.columns else 'title'\n",
        "body_col = 'request_text_edit_aware' if 'request_text_edit_aware' in train.columns else 'request_text'\n",
        "\n",
        "def build_text_upweighted(df: pd.DataFrame, title_col: str, body_col: str) -> pd.Series:\n",
        "    t = df[title_col].fillna('').astype(str)\n",
        "    b = df[body_col].fillna('').astype(str)\n",
        "    return (t + ' [SEP] ' + t + ' [SEP] ' + b)\n",
        "\n",
        "text_tr = build_text_upweighted(train, title_col, body_col).tolist()\n",
        "text_te = build_text_upweighted(test, title_col, body_col).tolist()\n",
        "y = train[target_col].astype(int).values\n",
        "groups = train['requester_username'].fillna('').astype(str).values if 'requester_username' in train.columns else np.arange(len(train))\n",
        "\n",
        "# Meta (reuse leakage-safe features, with time enrichments already validated)\n",
        "def count_urls(s: str) -> int: return len(re.findall(r'https?://\\S+', s or ''))\n",
        "def has_imgur(s: str) -> int: return 1 if re.search(r'imgur\\.com', s or '', flags=re.IGNORECASE) else 0\n",
        "def count_digits(s: str) -> int: return sum(ch.isdigit() for ch in (s or ''))\n",
        "def dollar_flag(s: str) -> int: return 1 if ('$' in (s or '')) or re.search(r'\\b\\d+\\s*(dollars|bucks)\\b', s or '', flags=re.IGNORECASE) else 0\n",
        "def caps_ratio(s: str) -> float:\n",
        "    if not s: return 0.0\n",
        "    letters = [c for c in s if c.isalpha()]\n",
        "    return (sum(c.isupper() for c in letters) / max(1, len(letters))) if letters else 0.0\n",
        "def word_count(s: str) -> int: return len((s or '').split())\n",
        "def exclam_count(s: str) -> int: return (s or '').count('!')\n",
        "def question_count(s: str) -> int: return (s or '').count('?')\n",
        "def parse_subreddit_count(x) -> int: return len(x) if isinstance(x, list) else 0\n",
        "def safe_log1p_signed(x): return np.sign(x) * np.log1p(np.abs(x))\n",
        "LEX_PATTERNS = {\n",
        "    'lex_please': r'\\bplease\\b',\n",
        "    'lex_thanks': r'\\b(thank you|thanks in advance|thanks|thank|tia)\\b',\n",
        "    'lex_appreciate': r'\\b(appreciate|appreciated)\\b',\n",
        "    'lex_pay_it_forward': r'\\b(pay it forward|return the favor)\\b',\n",
        "    'lex_repay': r'\\b(repay|pay you back|pay back)\\b',\n",
        "    'lex_willing': r\"\\b(willing to|i\\'ll|i will|i can)\\b\",\n",
        "    'lex_karma': r'\\bkarma\\b',\n",
        "    'lex_evidence': r'\\b(proof|receipt|photo|picture|pic|verify|verification|evidence)\\b',\n",
        "    'lex_imgur_word': r'\\bimgur\\b',\n",
        "    'lex_student': r'\\b(student|college|university|finals|exam|midterm)\\b',\n",
        "    'lex_jobloss': r'\\b(unemployed|laid off|lost my job|between jobs|job hunt)\\b',\n",
        "    'lex_broke': r'\\b(broke)\\b',\n",
        "    'lex_rent_bills': r'\\b(rent|bill|bills|utilities|electric|gas|water|paycheck)\\b',\n",
        "    'lex_family': r'\\b(family|kids?|children|baby|pregnant|son|daughter|wife|husband)\\b',\n",
        "    'lex_hungry': r'\\b(hungry|starving|no food)\\b|\\bfood (stamps|pantry)\\b',\n",
        "    'lex_struggling': r'\\b(desperate|struggling)\\b',\n",
        "    'lex_urgency': r'\\b(tonight|today|tomorrow|asap|urgent)\\b',\n",
        "    'lex_help': r'\\bhelp\\b',\n",
        "}\n",
        "def add_lexicons(df: pd.DataFrame, title_col: str, body_col: str) -> pd.DataFrame:\n",
        "    txt = (df[title_col].fillna('').astype(str) + ' ' + df[body_col].fillna('').astype(str)).str.lower()\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    for name, pat in LEX_PATTERNS.items():\n",
        "        out[name] = txt.str.count(pat, flags=re.IGNORECASE).astype(float)\n",
        "    return out\n",
        "def build_meta_enriched(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    title = df[title_col].fillna('').astype(str)\n",
        "    body = df[body_col].fillna('').astype(str)\n",
        "    ts = pd.to_numeric(df['unix_timestamp_of_request'], errors='coerce')\n",
        "    dt = pd.to_datetime(ts, unit='s', utc=True)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    keep_cols = [\n",
        "        'requester_account_age_in_days_at_request',\n",
        "        'requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request',\n",
        "        'requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request',\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "    ]\n",
        "    for c in keep_cols:\n",
        "        out[c] = pd.to_numeric(df[c], errors='coerce') if c in df.columns else 0.0\n",
        "    out['requester_subreddits_count'] = df['requester_subreddits_at_request'].apply(parse_subreddit_count).astype(float) if 'requester_subreddits_at_request' in df.columns else 0.0\n",
        "    out['hour'] = dt.dt.hour.astype(float)\n",
        "    out['weekday'] = dt.dt.weekday.astype(float)\n",
        "    out['is_weekend'] = dt.dt.weekday.isin([5,6]).astype(float)\n",
        "    out['month'] = dt.dt.month.astype(float)\n",
        "    out['quarter'] = dt.dt.quarter.astype(float)\n",
        "    base_ts = np.nanmin(ts.values)\n",
        "    out['days_since_start'] = ((ts - base_ts) / 86400.0).astype(float)\n",
        "    order = np.argsort(ts.values)\n",
        "    rel = np.empty_like(order, dtype=np.float64)\n",
        "    rel[order] = np.arange(len(order), dtype=np.float64)\n",
        "    out['relative_position'] = (rel / max(1, len(order)-1)).astype(float)\n",
        "    out['title_len_chars'] = title.str.len().astype(float)\n",
        "    out['title_len_words'] = title.apply(word_count).astype(float) if 'word_count' in globals() else title.apply(lambda s: len((s or '').split())).astype(float)\n",
        "    out['body_len_chars'] = body.str.len().astype(float)\n",
        "    out['body_len_words'] = body.apply(word_count).astype(float) if 'word_count' in globals() else body.apply(lambda s: len((s or '').split())).astype(float)\n",
        "    out['url_count'] = body.apply(count_urls).astype(float)\n",
        "    out['has_imgur'] = body.apply(has_imgur).astype(float)\n",
        "    out['digits_count'] = body.apply(count_digits).astype(float)\n",
        "    out['dollar_flag'] = body.apply(dollar_flag).astype(float)\n",
        "    out['caps_ratio'] = body.apply(caps_ratio).astype(float)\n",
        "    out['exclam_count'] = body.apply(exclam_count).astype(float)\n",
        "    out['question_count'] = body.apply(question_count).astype(float)\n",
        "    lex = add_lexicons(df, title_col, body_col)\n",
        "    out = pd.concat([out, lex], axis=1).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "    nonneg_cols = [\n",
        "        'requester_account_age_in_days_at_request','requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request','requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request','requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request','requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_subreddits_count','hour','weekday','is_weekend','month','quarter','days_since_start','relative_position',\n",
        "        'title_len_chars','title_len_words','body_len_chars','body_len_words',\n",
        "        'url_count','has_imgur','digits_count','dollar_flag','exclam_count','question_count',\n",
        "        *list(LEX_PATTERNS.keys()),\n",
        "    ]\n",
        "    for c in nonneg_cols:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(np.clip(out[c], a_min=0.0, a_max=None))\n",
        "    if 'requester_upvotes_minus_downvotes_at_request' in out.columns:\n",
        "        out['requester_upvotes_minus_downvotes_at_request'] = safe_log1p_signed(out['requester_upvotes_minus_downvotes_at_request'].values)\n",
        "    return out.replace([np.inf,-np.inf],0.0).fillna(0.0)\n",
        "\n",
        "meta_train = build_meta_enriched(train)\n",
        "meta_test = build_meta_enriched(test)\n",
        "print('Meta(e5) shapes:', meta_train.shape, meta_test.shape, flush=True)\n",
        "\n",
        "# Encode E5-base-v2 embeddings (normalize, CPU). Prepend 'query: ' as advised.\n",
        "print('Encoding e5-base-v2 embeddings...', flush=True)\n",
        "e5 = SentenceTransformer('intfloat/e5-base-v2', device='cpu')\n",
        "e5_tr = e5.encode([f'query: {s}' for s in text_tr], batch_size=128, show_progress_bar=False, normalize_embeddings=True).astype(np.float32)\n",
        "e5_te = e5.encode([f'query: {s}' for s in text_te], batch_size=128, show_progress_bar=False, normalize_embeddings=True).astype(np.float32)\n",
        "print('E5 shapes:', e5_tr.shape, e5_te.shape, flush=True)\n",
        "\n",
        "# Grouped CV XGB on [e5 emb + scaled meta]\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = list(sgkf.split(np.zeros(len(y)), y, groups))\n",
        "oof_e5 = np.zeros(len(train), dtype=np.float32)\n",
        "test_preds = []\n",
        "times = []\n",
        "params = dict(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='auc',\n",
        "    tree_method='hist',\n",
        "    max_depth=4,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=1.0,\n",
        "    min_child_weight=1.0,\n",
        "    max_bin=256,\n",
        "    random_state=42\n",
        ")\n",
        "for i, (trn_idx, val_idx) in enumerate(folds, 1):\n",
        "    t0 = time.time()\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    M_tr = scaler.fit_transform(meta_train.iloc[trn_idx].values).astype(np.float32)\n",
        "    M_va = scaler.transform(meta_train.iloc[val_idx].values).astype(np.float32)\n",
        "    X_tr = np.hstack([e5_tr[trn_idx], M_tr]).astype(np.float32)\n",
        "    X_va = np.hstack([e5_tr[val_idx], M_va]).astype(np.float32)\n",
        "    dtr = xgb.DMatrix(X_tr, label=y[trn_idx])\n",
        "    dva = xgb.DMatrix(X_va, label=y[val_idx])\n",
        "    booster = xgb.train(params, dtr, num_boost_round=2000, evals=[(dva,'valid')], verbose_eval=False, early_stopping_rounds=100)\n",
        "    oof_e5[val_idx] = booster.predict(dva, iteration_range=(0, booster.best_iteration+1))\n",
        "    M_te = scaler.transform(meta_test.values).astype(np.float32)\n",
        "    Xt = np.hstack([e5_te, M_te]).astype(np.float32)\n",
        "    dte = xgb.DMatrix(Xt)\n",
        "    test_preds.append(booster.predict(dte, iteration_range=(0, booster.best_iteration+1)).astype(np.float32))\n",
        "    times.append(time.time()-t0)\n",
        "    print(f'  E5 fold {i} AUC={roc_auc_score(y[val_idx], oof_e5[val_idx]):.5f} | iters={booster.best_iteration+1} | {times[-1]:.1f}s', flush=True)\n",
        "\n",
        "auc_e5 = roc_auc_score(y, oof_e5)\n",
        "print(f'OOF AUC (E5 + meta XGB): {auc_e5:.5f} | mean fold time: {np.mean(times):.1f}s', flush=True)\n",
        "test_e5 = np.mean(np.vstack(test_preds), axis=0).astype(np.float32)\n",
        "np.save('oof_e5_embed_g.npy', oof_e5)\n",
        "np.save('test_e5_embed_g.npy', test_e5)\n",
        "\n",
        "# Rank-then-NNLS including E5 and strongest legs (prefer enriched bagged ST if present)\n",
        "paths = {\n",
        "    'lr_meta_g': ('oof_lr_meta_g.npy','test_lr_meta_g.npy'),\n",
        "    'lr_subs_g': ('oof_lr_subs_g.npy','test_lr_subs_g.npy'),\n",
        "    'xgb_svd_meta': ('oof_xgb_svd_meta.npy','test_xgb_svd_meta.npy'),\n",
        "    'cb_text_g': ('oof_cat_text_g.npy','test_cat_text_g.npy'),\n",
        "    'st_embed_bag_v2': ('oof_st_embed_bag_v2.npy','test_st_embed_bag_v2.npy') if os.path.exists('oof_st_embed_bag_v2.npy') else ('oof_st_embed_bag.npy','test_st_embed_bag.npy'),\n",
        "    'mpnet_embed_bag_v2': ('oof_mpnet_embed_bag_v2.npy','test_mpnet_embed_bag_v2.npy') if os.path.exists('oof_mpnet_embed_bag_v2.npy') else ('oof_mpnet_embed_bag.npy','test_mpnet_embed_bag.npy'),\n",
        "    'e5_embed_g': ('oof_e5_embed_g.npy','test_e5_embed_g.npy'),\n",
        "}\n",
        "oof_list, test_list, names = [], [], []\n",
        "for name, (poof, ptest) in paths.items():\n",
        "    if os.path.exists(poof) and os.path.exists(ptest):\n",
        "        oof_list.append(np.load(poof))\n",
        "        test_list.append(np.load(ptest))\n",
        "        names.append(name)\n",
        "assert len(oof_list) >= 3, 'Not enough models for rank-NNLS'\n",
        "P = np.vstack(oof_list).T\n",
        "T = np.vstack(test_list).T\n",
        "def rank01(a):\n",
        "    order = a.argsort(kind='mergesort')\n",
        "    ranks = np.empty_like(order, dtype=np.float64)\n",
        "    ranks[order] = np.arange(len(a), dtype=np.float64)\n",
        "    return ranks / max(1, len(a)-1)\n",
        "P_rank = np.apply_along_axis(rank01, 0, P)\n",
        "T_rank = np.apply_along_axis(rank01, 0, T)\n",
        "w, _ = nnls(P_rank, y.astype(float))\n",
        "w = w / w.sum() if w.sum() > 0 else w\n",
        "blend_oof_rank = P_rank @ w\n",
        "auc_rank = roc_auc_score(y, blend_oof_rank)\n",
        "print('Rank-NNLS models (+E5):', names)\n",
        "print('Rank-NNLS weights (+E5):', w)\n",
        "print(f'Rank-blended OOF AUC (+E5): {auc_rank:.5f}')\n",
        "blend_test_rank = T_rank @ w\n",
        "pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': blend_test_rank.astype(np.float32)}).to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (Rank-NNLS + E5). Rows:', len(blend_test_rank))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta(e5) shapes: (2878, 46) (1162, 46)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding e5-base-v2 embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E5 shapes: (2878, 768) (1162, 768)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  E5 fold 1 AUC=0.68649 | iters=56 | 1.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  E5 fold 2 AUC=0.67281 | iters=23 | 1.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  E5 fold 3 AUC=0.64393 | iters=29 | 1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  E5 fold 4 AUC=0.68757 | iters=123 | 2.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  E5 fold 5 AUC=0.71050 | iters=18 | 1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF AUC (E5 + meta XGB): 0.67627 | mean fold time: 1.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank-NNLS models (+E5): ['lr_meta_g', 'lr_subs_g', 'xgb_svd_meta', 'cb_text_g', 'st_embed_bag_v2', 'mpnet_embed_bag_v2', 'e5_embed_g']\nRank-NNLS weights (+E5): [0.14791451 0.09863673 0.08117717 0.         0.24403993 0.31331505\n 0.11491661]\nRank-blended OOF AUC (+E5): 0.70912\nSaved submission.csv (Rank-NNLS + E5). Rows: 1162\n"
          ]
        }
      ]
    },
    {
      "id": "9e34bca6-7ef7-4fc8-899c-708778d31fcd",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Time-aware, group-purged holdout rank-NNLS blender with shrink; prune weak legs; write submission\n",
        "import numpy as np, pandas as pd, os, time\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train['requester_received_pizza'].astype(int).values\n",
        "users = train['requester_username'].fillna('').astype(str).values\n",
        "ts = pd.to_numeric(train['unix_timestamp_of_request'], errors='coerce').values\n",
        "\n",
        "# Define last-20% time holdout with 5-day purge gap and group purge\n",
        "q = 0.80\n",
        "cutoff = np.nanquantile(ts, q)\n",
        "gap_days = 5\n",
        "gap_sec = gap_days * 86400.0\n",
        "train_mask = ts < (cutoff - gap_sec)\n",
        "val_mask = ts >= cutoff\n",
        "\n",
        "# Group purge: drop any user appearing on both sides\n",
        "users_train = set(users[train_mask])\n",
        "users_val = set(users[val_mask])\n",
        "overlap = users_train.intersection(users_val)\n",
        "if overlap:\n",
        "    drop_overlap = np.isin(users, list(overlap))\n",
        "    train_mask = train_mask & (~drop_overlap)\n",
        "    val_mask = val_mask & (~drop_overlap)\n",
        "\n",
        "idx_tr = np.where(train_mask)[0]\n",
        "idx_va = np.where(val_mask)[0]\n",
        "print(f'Holdout split: train={idx_tr.size}, valid={idx_va.size}, overlap_users={len(overlap)}', flush=True)\n",
        "\n",
        "# Load candidate models' OOF and test predictions\n",
        "cands = {\n",
        "    'lr_meta_g': ('oof_lr_meta_g.npy','test_lr_meta_g.npy'),\n",
        "    'xgb_svd_meta': ('oof_xgb_svd_meta.npy','test_xgb_svd_meta.npy'),\n",
        "    'st_embed_bag_v2': ('oof_st_embed_bag_v2.npy','test_st_embed_bag_v2.npy') if os.path.exists('oof_st_embed_bag_v2.npy') else ('oof_st_embed_bag.npy','test_st_embed_bag.npy'),\n",
        "    'mpnet_embed_bag_v2': ('oof_mpnet_embed_bag_v2.npy','test_mpnet_embed_bag_v2.npy') if os.path.exists('oof_mpnet_embed_bag_v2.npy') else ('oof_mpnet_embed_bag.npy','test_mpnet_embed_bag.npy'),\n",
        "    'e5_embed_g': ('oof_e5_embed_g.npy','test_e5_embed_g.npy'),\n",
        "    # Optional leg; will keep only if it earns weight\n",
        "    'lr_subs_g': ('oof_lr_subs_g.npy','test_lr_subs_g.npy'),\n",
        "    # Explicitly prune weak CatBoost text-only per advice\n",
        "    # 'cb_text_g': ('oof_cat_text_g.npy','test_cat_text_g.npy'),\n",
        "}\n",
        "\n",
        "names, OOFs, TESTs = [], [], []\n",
        "for name, (poof, ptest) in list(cands.items()):\n",
        "    if os.path.exists(poof) and os.path.exists(ptest):\n",
        "        OOFs.append(np.load(poof))\n",
        "        TESTs.append(np.load(ptest))\n",
        "        names.append(name)\n",
        "    else:\n",
        "        print(f'Missing predictions for {name}; skipping')\n",
        "\n",
        "assert len(OOFs) >= 3, f'Need >=3 legs, found {len(OOFs)}'\n",
        "P = np.vstack(OOFs).T  # (n_train, n_models)\n",
        "T = np.vstack(TESTs).T # (n_test, n_models)\n",
        "\n",
        "def rank01(a):\n",
        "    order = a.argsort(kind='mergesort')\n",
        "    r = np.empty_like(order, dtype=np.float64)\n",
        "    r[order] = np.arange(len(a), dtype=np.float64)\n",
        "    return r / max(1, len(a)-1)\n",
        "\n",
        "def fit_rank_nnls_with_shrink(P_mat, y_vec, idx_valid, shrink=0.20, prune_thr=0.05):\n",
        "    P_hold = P_mat[idx_valid]\n",
        "    # Rank-transform columns on the holdout subset\n",
        "    P_rank = np.apply_along_axis(rank01, 0, P_hold)\n",
        "    w, _ = nnls(P_rank, y_vec[idx_valid].astype(float))\n",
        "    if w.sum() > 0: w = w / w.sum()\n",
        "    # Shrink toward uniform: w := (1-shrink)*w + shrink*(1/M)\n",
        "    M = w.size\n",
        "    w = (1.0 - shrink)*w + shrink*(1.0/M)\n",
        "    # Prune tiny weights and renormalize\n",
        "    mask_keep = w >= prune_thr\n",
        "    if not mask_keep.all():\n",
        "        w = w * mask_keep.astype(float)\n",
        "    if w.sum() > 0: w = w / w.sum()\n",
        "    auc = roc_auc_score(y_vec[idx_valid], P_rank @ w)\n",
        "    return w, auc, mask_keep\n",
        "\n",
        "# Try with and without lr_subs_g; choose higher holdout AUC\n",
        "all_idx = np.arange(len(names))\n",
        "try:\n",
        "    subs_idx = names.index('lr_subs_g')\n",
        "    keep_mask_with = np.ones(len(names), dtype=bool)\n",
        "    keep_mask_wo = np.ones(len(names), dtype=bool); keep_mask_wo[subs_idx] = False\n",
        "    options = [('with_subs', keep_mask_with), ('no_subs', keep_mask_wo)]\n",
        "except ValueError:\n",
        "    options = [('no_subs', np.ones(len(names), dtype=bool))]\n",
        "\n",
        "best = None\n",
        "for tag, kmask in options:\n",
        "    P_sel = P[:, kmask]\n",
        "    w, auc, kept = fit_rank_nnls_with_shrink(P_sel, y, idx_va, shrink=0.20, prune_thr=0.05)\n",
        "    kept_names = np.array(names)[kmask][kept].tolist()\n",
        "    print(f'Holdout AUC [{tag}] on ranks: {auc:.5f} | legs={np.array(names)[kmask].tolist()}')\n",
        "    print(f'  -> After shrink/prune (thr=0.05): kept={kept_names} | weights={w}', flush=True)\n",
        "    if (best is None) or (auc > best[0]):\n",
        "        # Lift weights back to option space and only place kept weights to avoid shape mismatch\n",
        "        w_full = np.zeros(kmask.sum(), dtype=float)\n",
        "        w_kept = w[kept]  # only the kept weights\n",
        "        w_full[kept] = w_kept\n",
        "        best = (auc, w_full, kmask, kept)\n",
        "\n",
        "auc_hold, w_best_optspace, keep_mask_opt, kept_opt = best\n",
        "# Project selected weights to global model list\n",
        "w_global = np.zeros(len(names), dtype=float)\n",
        "w_global[keep_mask_opt] = w_best_optspace\n",
        "sel_names = np.array(names)[keep_mask_opt][kept_opt].tolist()\n",
        "w_final = w_global[w_global > 0]\n",
        "print('Chosen legs:', sel_names)\n",
        "print('Chosen weights (post-shrink+prune):', w_final)\n",
        "\n",
        "# Apply weights to test (rank-transform per model on test independently) to build submission\n",
        "T_sel = T[:, w_global > 0]\n",
        "T_rank = np.apply_along_axis(rank01, 0, T_sel)\n",
        "test_blend = (T_rank @ (w_final / w_final.sum())).astype(np.float32) if w_final.sum() > 0 else T_rank.mean(axis=1).astype(np.float32)\n",
        "\n",
        "# Also compute holdout-blended OOF AUC for reference (using OOF ranks on holdout columns)\n",
        "P_sel = P[:, w_global > 0]\n",
        "P_hold_rank = np.apply_along_axis(rank01, 0, P_sel[idx_va])\n",
        "oof_auc_ref = roc_auc_score(y[idx_va], P_hold_rank @ (w_final / w_final.sum())) if w_final.sum() > 0 else roc_auc_score(y[idx_va], P_hold_rank.mean(axis=1))\n",
        "print(f'Final holdout AUC (ref): {oof_auc_ref:.5f}')\n",
        "\n",
        "# Save two files: time-holdout-weighted and set as primary submission\n",
        "sub = pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend})\n",
        "sub.to_csv('submission_time_holdout.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_time_holdout.csv and updated submission.csv. Rows:', len(sub))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Holdout split: train=2289, valid=576, overlap_users=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Holdout AUC [with_subs] on ranks: 0.70366 | legs=['lr_meta_g', 'xgb_svd_meta', 'st_embed_bag_v2', 'mpnet_embed_bag_v2', 'e5_embed_g', 'lr_subs_g']\n  -> After shrink/prune (thr=0.05): kept=['lr_meta_g', 'st_embed_bag_v2', 'mpnet_embed_bag_v2', 'e5_embed_g', 'lr_subs_g'] | weights=[0.27434644 0.         0.0840204  0.22384182 0.27597431 0.14181703]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Holdout AUC [no_subs] on ranks: 0.70299 | legs=['lr_meta_g', 'xgb_svd_meta', 'st_embed_bag_v2', 'mpnet_embed_bag_v2', 'e5_embed_g']\n  -> After shrink/prune (thr=0.05): kept=['lr_meta_g', 'st_embed_bag_v2', 'mpnet_embed_bag_v2', 'e5_embed_g'] | weights=[0.38039541 0.         0.09559816 0.24101888 0.28298755]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen legs: ['lr_meta_g', 'st_embed_bag_v2', 'mpnet_embed_bag_v2', 'e5_embed_g', 'lr_subs_g']\nChosen weights (post-shrink+prune): [0.27434644 0.0840204  0.22384182 0.27597431 0.14181703]\nFinal holdout AUC (ref): 0.70366\nSaved submission_time_holdout.csv and updated submission.csv. Rows: 1162\n"
          ]
        }
      ]
    },
    {
      "id": "565efe33-81da-4cc3-bd57-c6f9f8089d47",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Seed-bag E5-base-v2 embeddings + enriched meta -> XGB (grouped CV); overwrite e5 OOF/test and re-use time-holdout blender\n",
        "import sys, subprocess, time, os, re, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'sentence-transformers==2.7.0'], check=True)\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "\n",
        "def load_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path)\n",
        "    except ValueError:\n",
        "        import json as _json\n",
        "        with open(path, 'r') as f:\n",
        "            return pd.DataFrame(_json.load(f))\n",
        "\n",
        "train = load_df('train.json')\n",
        "test = load_df('test.json')\n",
        "target_col = 'requester_received_pizza'\n",
        "title_col = 'request_title' if 'request_title' in train.columns else 'title'\n",
        "body_col = 'request_text_edit_aware' if 'request_text_edit_aware' in train.columns else 'request_text'\n",
        "\n",
        "def build_text_upweighted(df: pd.DataFrame, title_col: str, body_col: str) -> pd.Series:\n",
        "    t = df[title_col].fillna('').astype(str)\n",
        "    b = df[body_col].fillna('').astype(str)\n",
        "    return (t + ' [SEP] ' + t + ' [SEP] ' + b)\n",
        "\n",
        "text_tr = build_text_upweighted(train, title_col, body_col).tolist()\n",
        "text_te = build_text_upweighted(test, title_col, body_col).tolist()\n",
        "y = train[target_col].astype(int).values\n",
        "groups = train['requester_username'].fillna('').astype(str).values if 'requester_username' in train.columns else np.arange(len(train))\n",
        "\n",
        "# Enriched meta builder (same as in cells 21/23)\n",
        "def count_urls(s: str) -> int: return len(re.findall(r'https?://\\S+', s or ''))\n",
        "def has_imgur(s: str) -> int: return 1 if re.search(r'imgur\\.com', s or '', flags=re.IGNORECASE) else 0\n",
        "def count_digits(s: str) -> int: return sum(ch.isdigit() for ch in (s or ''))\n",
        "def dollar_flag(s: str) -> int: return 1 if ('$' in (s or '')) or re.search(r'\\b\\d+\\s*(dollars|bucks)\\b', s or '', flags=re.IGNORECASE) else 0\n",
        "def caps_ratio(s: str) -> float:\n",
        "    if not s: return 0.0\n",
        "    letters = [c for c in s if c.isalpha()]\n",
        "    return (sum(c.isupper() for c in letters) / max(1, len(letters))) if letters else 0.0\n",
        "def word_count(s: str) -> int: return len((s or '').split())\n",
        "def exclam_count(s: str) -> int: return (s or '').count('!')\n",
        "def question_count(s: str) -> int: return (s or '').count('?')\n",
        "def parse_subreddit_count(x) -> int: return len(x) if isinstance(x, list) else 0\n",
        "def safe_log1p_signed(x): return np.sign(x) * np.log1p(np.abs(x))\n",
        "LEX_PATTERNS = {\n",
        "    'lex_please': r'\\bplease\\b',\n",
        "    'lex_thanks': r'\\b(thank you|thanks in advance|thanks|thank|tia)\\b',\n",
        "    'lex_appreciate': r'\\b(appreciate|appreciated)\\b',\n",
        "    'lex_pay_it_forward': r'\\b(pay it forward|return the favor)\\b',\n",
        "    'lex_repay': r'\\b(repay|pay you back|pay back)\\b',\n",
        "    'lex_willing': r\"\\b(willing to|i\\'ll|i will|i can)\\b\",\n",
        "    'lex_karma': r'\\bkarma\\b',\n",
        "    'lex_evidence': r'\\b(proof|receipt|photo|picture|pic|verify|verification|evidence)\\b',\n",
        "    'lex_imgur_word': r'\\bimgur\\b',\n",
        "    'lex_student': r'\\b(student|college|university|finals|exam|midterm)\\b',\n",
        "    'lex_jobloss': r'\\b(unemployed|laid off|lost my job|between jobs|job hunt)\\b',\n",
        "    'lex_broke': r'\\b(broke)\\b',\n",
        "    'lex_rent_bills': r'\\b(rent|bill|bills|utilities|electric|gas|water|paycheck)\\b',\n",
        "    'lex_family': r'\\b(family|kids?|children|baby|pregnant|son|daughter|wife|husband)\\b',\n",
        "    'lex_hungry': r'\\b(hungry|starving|no food)\\b|\\bfood (stamps|pantry)\\b',\n",
        "    'lex_struggling': r'\\b(desperate|struggling)\\b',\n",
        "    'lex_urgency': r'\\b(tonight|today|tomorrow|asap|urgent)\\b',\n",
        "    'lex_help': r'\\bhelp\\b',\n",
        "}\n",
        "def add_lexicons(df: pd.DataFrame, title_col: str, body_col: str) -> pd.DataFrame:\n",
        "    txt = (df[title_col].fillna('').astype(str) + ' ' + df[body_col].fillna('').astype(str)).str.lower()\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    for name, pat in LEX_PATTERNS.items():\n",
        "        out[name] = txt.str.count(pat, flags=re.IGNORECASE).astype(float)\n",
        "    return out\n",
        "def build_meta_enriched(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    title = df[title_col].fillna('').astype(str)\n",
        "    body = df[body_col].fillna('').astype(str)\n",
        "    ts = pd.to_numeric(df['unix_timestamp_of_request'], errors='coerce')\n",
        "    dt = pd.to_datetime(ts, unit='s', utc=True)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    keep_cols = [\n",
        "        'requester_account_age_in_days_at_request',\n",
        "        'requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request',\n",
        "        'requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request',\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "    ]\n",
        "    for c in keep_cols:\n",
        "        out[c] = pd.to_numeric(df[c], errors='coerce') if c in df.columns else 0.0\n",
        "    out['requester_subreddits_count'] = df['requester_subreddits_at_request'].apply(parse_subreddit_count).astype(float) if 'requester_subreddits_at_request' in df.columns else 0.0\n",
        "    out['hour'] = dt.dt.hour.astype(float)\n",
        "    out['weekday'] = dt.dt.weekday.astype(float)\n",
        "    out['is_weekend'] = dt.dt.weekday.isin([5,6]).astype(float)\n",
        "    out['month'] = dt.dt.month.astype(float)\n",
        "    out['quarter'] = dt.dt.quarter.astype(float)\n",
        "    base_ts = np.nanmin(ts.values)\n",
        "    out['days_since_start'] = ((ts - base_ts) / 86400.0).astype(float)\n",
        "    order = np.argsort(ts.values)\n",
        "    rel = np.empty_like(order, dtype=np.float64)\n",
        "    rel[order] = np.arange(len(order), dtype=np.float64)\n",
        "    out['relative_position'] = (rel / max(1, len(order)-1)).astype(float)\n",
        "    out['title_len_chars'] = title.str.len().astype(float)\n",
        "    out['title_len_words'] = title.apply(word_count).astype(float) if 'word_count' in globals() else title.apply(lambda s: len((s or '').split())).astype(float)\n",
        "    out['body_len_chars'] = body.str.len().astype(float)\n",
        "    out['body_len_words'] = body.apply(word_count).astype(float) if 'word_count' in globals() else body.apply(lambda s: len((s or '').split())).astype(float)\n",
        "    out['url_count'] = body.apply(count_urls).astype(float)\n",
        "    out['has_imgur'] = body.apply(has_imgur).astype(float)\n",
        "    out['digits_count'] = body.apply(count_digits).astype(float)\n",
        "    out['dollar_flag'] = body.apply(dollar_flag).astype(float)\n",
        "    out['caps_ratio'] = body.apply(caps_ratio).astype(float)\n",
        "    out['exclam_count'] = body.apply(exclam_count).astype(float)\n",
        "    out['question_count'] = body.apply(question_count).astype(float)\n",
        "    lex = add_lexicons(df, title_col, body_col)\n",
        "    out = pd.concat([out, lex], axis=1).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "    nonneg_cols = [\n",
        "        'requester_account_age_in_days_at_request','requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request','requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request','requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request','requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_subreddits_count','hour','weekday','is_weekend','month','quarter','days_since_start','relative_position',\n",
        "        'title_len_chars','title_len_words','body_len_chars','body_len_words',\n",
        "        'url_count','has_imgur','digits_count','dollar_flag','exclam_count','question_count',\n",
        "        *list(LEX_PATTERNS.keys()),\n",
        "    ]\n",
        "    for c in nonneg_cols:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(np.clip(out[c], a_min=0.0, a_max=None))\n",
        "    if 'requester_upvotes_minus_downvotes_at_request' in out.columns:\n",
        "        out['requester_upvotes_minus_downvotes_at_request'] = safe_log1p_signed(out['requester_upvotes_minus_downvotes_at_request'].values)\n",
        "    return out.replace([np.inf,-np.inf],0.0).fillna(0.0)\n",
        "\n",
        "meta_train = build_meta_enriched(train)\n",
        "meta_test = build_meta_enriched(test)\n",
        "print('Meta(enriched for E5 bag) shapes:', meta_train.shape, meta_test.shape, flush=True)\n",
        "\n",
        "# Encode E5 embeddings (normalized, CPU), with 'query:' prefix\n",
        "print('Encoding E5-base-v2 (for bagging)...', flush=True)\n",
        "e5 = SentenceTransformer('intfloat/e5-base-v2', device='cpu')\n",
        "emb_tr = e5.encode([f'query: {s}' for s in text_tr], batch_size=128, show_progress_bar=False, normalize_embeddings=True).astype(np.float32)\n",
        "emb_te = e5.encode([f'query: {s}' for s in text_te], batch_size=128, show_progress_bar=False, normalize_embeddings=True).astype(np.float32)\n",
        "print('E5 emb shapes:', emb_tr.shape, emb_te.shape, flush=True)\n",
        "\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = list(sgkf.split(np.zeros(len(y)), y, groups))\n",
        "\n",
        "def bag_xgb_on_emb(emb_tr, emb_te, seeds=(7,13,29), jitter=True):\n",
        "    oof_bag = np.zeros(len(train), dtype=np.float32)\n",
        "    test_bag = []\n",
        "    for seed in seeds:\n",
        "        rs = np.random.RandomState(seed)\n",
        "        md = 4 + (rs.rand() < 0.5) if jitter else 4\n",
        "        subs = 0.75 + rs.rand()*0.10 if jitter else 0.80\n",
        "        cols = 0.75 + rs.rand()*0.15 if jitter else 0.80\n",
        "        lr = 0.045 + rs.rand()*0.010 if jitter else 0.05\n",
        "        params = dict(\n",
        "            objective='binary:logistic', eval_metric='auc', tree_method='hist',\n",
        "            max_depth=int(md), learning_rate=float(lr), subsample=float(subs), colsample_bytree=float(cols),\n",
        "            reg_lambda=1.0, min_child_weight=1.0, max_bin=256, random_state=int(seed)\n",
        "        )\n",
        "        oof_seed = np.zeros(len(train), dtype=np.float32)\n",
        "        test_preds = []\n",
        "        t_se = time.time()\n",
        "        for i, (trn_idx, val_idx) in enumerate(folds, 1):\n",
        "            t0 = time.time()\n",
        "            scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "            M_tr = scaler.fit_transform(meta_train.iloc[trn_idx].values).astype(np.float32)\n",
        "            M_va = scaler.transform(meta_train.iloc[val_idx].values).astype(np.float32)\n",
        "            X_tr = np.hstack([emb_tr[trn_idx], M_tr]).astype(np.float32)\n",
        "            X_va = np.hstack([emb_tr[val_idx], M_va]).astype(np.float32)\n",
        "            dtr = xgb.DMatrix(X_tr, label=y[trn_idx])\n",
        "            dva = xgb.DMatrix(X_va, label=y[val_idx])\n",
        "            booster = xgb.train(params, dtr, num_boost_round=2000, evals=[(dva,'valid')], verbose_eval=False, early_stopping_rounds=100)\n",
        "            oof_seed[val_idx] = booster.predict(dva, iteration_range=(0, booster.best_iteration+1))\n",
        "            M_te = scaler.transform(meta_test.values).astype(np.float32)\n",
        "            Xt = np.hstack([emb_te, M_te]).astype(np.float32)\n",
        "            dte = xgb.DMatrix(Xt)\n",
        "            test_preds.append(booster.predict(dte, iteration_range=(0, booster.best_iteration+1)).astype(np.float32))\n",
        "            print(f'  [E5 seed {seed}] Fold {i} AUC={roc_auc_score(y[val_idx], oof_seed[val_idx]):.5f} | iters={booster.best_iteration+1} | {time.time()-t0:.1f}s', flush=True)\n",
        "        print(f'[E5] Seed {seed} OOF AUC={roc_auc_score(y, oof_seed):.5f} | elapsed {time.time()-t_se:.1f}s', flush=True)\n",
        "        oof_bag += oof_seed / len(seeds)\n",
        "        test_bag.append(np.mean(np.vstack(test_preds), axis=0).astype(np.float32))\n",
        "    test_mean = np.mean(np.vstack(test_bag), axis=0).astype(np.float32)\n",
        "    auc_bag = roc_auc_score(y, oof_bag)\n",
        "    print(f'[E5] Bagged OOF AUC: {auc_bag:.5f}', flush=True)\n",
        "    return oof_bag, test_mean\n",
        "\n",
        "oof_e5_bag, test_e5_bag = bag_xgb_on_emb(emb_tr, emb_te, seeds=(7,13,29), jitter=True)\n",
        "\n",
        "# Overwrite the single-seed files used by blender to pick up bagged predictions\n",
        "np.save('oof_e5_embed_g.npy', oof_e5_bag)\n",
        "np.save('test_e5_embed_g.npy', test_e5_bag)\n",
        "print('Saved bagged E5 OOF/test to oof_e5_embed_g.npy / test_e5_embed_g.npy')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta(enriched for E5 bag) shapes: (2878, 46) (1162, 46)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding E5-base-v2 (for bagging)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E5 emb shapes: (2878, 768) (1162, 768)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 seed 7] Fold 1 AUC=0.68401 | iters=73 | 2.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 seed 7] Fold 2 AUC=0.66958 | iters=84 | 2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 seed 7] Fold 3 AUC=0.66898 | iters=201 | 4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 seed 7] Fold 4 AUC=0.66938 | iters=75 | 2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 seed 7] Fold 5 AUC=0.70980 | iters=74 | 2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E5] Seed 7 OOF AUC=0.67663 | elapsed 14.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 seed 13] Fold 1 AUC=0.68546 | iters=138 | 2.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 seed 13] Fold 2 AUC=0.67963 | iters=44 | 1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 seed 13] Fold 3 AUC=0.67323 | iters=77 | 1.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 seed 13] Fold 4 AUC=0.67222 | iters=35 | 1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 seed 13] Fold 5 AUC=0.71243 | iters=72 | 1.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E5] Seed 13 OOF AUC=0.67970 | elapsed 7.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 seed 29] Fold 1 AUC=0.69703 | iters=94 | 1.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 seed 29] Fold 2 AUC=0.69184 | iters=76 | 1.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 seed 29] Fold 3 AUC=0.67099 | iters=5 | 1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 seed 29] Fold 4 AUC=0.68467 | iters=105 | 1.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 seed 29] Fold 5 AUC=0.71146 | iters=48 | 1.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E5] Seed 29 OOF AUC=0.68382 | elapsed 7.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E5] Bagged OOF AUC: 0.69692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved bagged E5 OOF/test to oof_e5_embed_g.npy / test_e5_embed_g.npy\n"
          ]
        }
      ]
    },
    {
      "id": "5af74e70-e763-4cf7-a1d7-abd2bdf2d15c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Forward-chaining, group-purged rank-NNLS blender (3 chains, no shrink); save alternate submission\n",
        "import numpy as np, pandas as pd, os\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train['requester_received_pizza'].astype(int).values\n",
        "users = train['requester_username'].fillna('').astype(str).values\n",
        "ts = pd.to_numeric(train['unix_timestamp_of_request'], errors='coerce').values\n",
        "\n",
        "# Candidate legs (drop CatBoost), keep lr_subs_g optional\n",
        "cands = {\n",
        "    'lr_meta_g': ('oof_lr_meta_g.npy','test_lr_meta_g.npy'),\n",
        "    'xgb_svd_meta': ('oof_xgb_svd_meta.npy','test_xgb_svd_meta.npy'),\n",
        "    'st_embed_bag_v2': ('oof_st_embed_bag_v2.npy','test_st_embed_bag_v2.npy') if os.path.exists('oof_st_embed_bag_v2.npy') else ('oof_st_embed_bag.npy','test_st_embed_bag.npy'),\n",
        "    'mpnet_embed_bag_v2': ('oof_mpnet_embed_bag_v2.npy','test_mpnet_embed_bag_v2.npy') if os.path.exists('oof_mpnet_embed_bag_v2.npy') else ('oof_mpnet_embed_bag.npy','test_mpnet_embed_bag.npy'),\n",
        "    'e5_embed_g': ('oof_e5_embed_g.npy','test_e5_embed_g.npy'),\n",
        "    'lr_subs_g': ('oof_lr_subs_g.npy','test_lr_subs_g.npy'),\n",
        "}\n",
        "\n",
        "names, OOFs, TESTs = [], [], []\n",
        "for name, (poof, ptest) in list(cands.items()):\n",
        "    if os.path.exists(poof) and os.path.exists(ptest):\n",
        "        OOFs.append(np.load(poof))\n",
        "        TESTs.append(np.load(ptest))\n",
        "        names.append(name)\n",
        "    else:\n",
        "        print(f'Missing predictions for {name}; skipping')\n",
        "assert len(OOFs) >= 3, f'Need >=3 legs, found {len(OOFs)}'\n",
        "P = np.vstack(OOFs).T\n",
        "T = np.vstack(TESTs).T\n",
        "\n",
        "def rank01(a):\n",
        "    order = a.argsort(kind='mergesort')\n",
        "    r = np.empty_like(order, dtype=np.float64)\n",
        "    r[order] = np.arange(len(a), dtype=np.float64)\n",
        "    return r / max(1, len(a)-1)\n",
        "\n",
        "def fit_rank_nnls(P_mat, y_vec, idx_valid):\n",
        "    P_hold = P_mat[idx_valid]\n",
        "    P_rank = np.apply_along_axis(rank01, 0, P_hold)\n",
        "    w, _ = nnls(P_rank, y_vec[idx_valid].astype(float))\n",
        "    if w.sum() > 0: w = w / w.sum()\n",
        "    auc = roc_auc_score(y_vec[idx_valid], P_rank @ w) if idx_valid.size else np.nan\n",
        "    return w, auc\n",
        "\n",
        "def group_purged_mask(idx_train_cond, idx_valid_cond):\n",
        "    tr_mask = idx_train_cond.copy()\n",
        "    va_mask = idx_valid_cond.copy()\n",
        "    users_tr = set(users[tr_mask])\n",
        "    users_va = set(users[va_mask])\n",
        "    overlap = users_tr.intersection(users_va)\n",
        "    if overlap:\n",
        "        drop = np.isin(users, list(overlap))\n",
        "        tr_mask = tr_mask & (~drop)\n",
        "        va_mask = va_mask & (~drop)\n",
        "    return tr_mask, va_mask\n",
        "\n",
        "# Define forward chains with 3-7 day purge: [0-60 -> 60-80], [0-80 -> 80-90], [0-90 -> 90-100]\n",
        "qs = np.quantile(ts[~np.isnan(ts)], [0.6, 0.8, 0.9])\n",
        "q60, q80, q90 = qs[0], qs[1], qs[2]\n",
        "gap_sec = 5*86400.0\n",
        "chains = [\n",
        "    ((ts < (q60 - 0)), (ts >= (q60 + gap_sec)) & (ts < (q80 + 0))),\n",
        "    ((ts < (q80 - 0)), (ts >= (q80 + gap_sec)) & (ts < (q90 + 0))),\n",
        "    ((ts < (q90 - 0)), (ts >= (q90 + gap_sec))),\n",
        "]\n",
        "\n",
        "weights = []\n",
        "aucs = []\n",
        "leg_masks = []\n",
        "\n",
        "for ci, (tr_cond, va_cond) in enumerate(chains, 1):\n",
        "    tr_mask, va_mask = group_purged_mask(tr_cond, va_cond)\n",
        "    idx_va = np.where(va_mask)[0]\n",
        "    print(f'Chain {ci}: valid size={idx_va.size}', flush=True)\n",
        "    # With and without lr_subs_g\n",
        "    try:\n",
        "        subs_idx = names.index('lr_subs_g')\n",
        "        keep_with = np.ones(len(names), dtype=bool)\n",
        "        keep_wo = np.ones(len(names), dtype=bool); keep_wo[subs_idx] = False\n",
        "        opts = [('with_subs', keep_with), ('no_subs', keep_wo)]\n",
        "    except ValueError:\n",
        "        opts = [('no_subs', np.ones(len(names), dtype=bool))]\n",
        "    best = None\n",
        "    for tag, kmask in opts:\n",
        "        w, auc = fit_rank_nnls(P[:, kmask], y, idx_va)\n",
        "        print(f'  Chain {ci} {tag}: AUC={auc:.5f} legs={np.array(names)[kmask].tolist()} w={w}', flush=True)\n",
        "        if (best is None) or (auc > best[0]):\n",
        "            best = (auc, w, kmask, tag)\n",
        "    auc_hold, w_best, kmask_best, tag = best\n",
        "    weights.append(w_best)\n",
        "    aucs.append(auc_hold)\n",
        "    leg_masks.append(kmask_best)\n",
        "\n",
        "# Align masks across chains (use intersection of kept legs); average weights over chains on common legs\n",
        "keep_all = np.ones(len(names), dtype=bool)\n",
        "for km in leg_masks:\n",
        "    keep_all = keep_all & km\n",
        "sel_names = np.array(names)[keep_all].tolist()\n",
        "if not any(keep_all):\n",
        "    # fallback: use mask from best AUC chain\n",
        "    best_idx = int(np.nanargmax(aucs))\n",
        "    keep_all = leg_masks[best_idx]\n",
        "    sel_names = np.array(names)[keep_all].tolist()\n",
        "\n",
        "W_mat = []\n",
        "for w, km in zip(weights, leg_masks):\n",
        "    # project weights to common leg set by zeroing dropped legs and renormalizing\n",
        "    w_full = np.zeros(len(names), dtype=np.float64)\n",
        "    w_full[km] = w\n",
        "    w_common = w_full[keep_all]\n",
        "    if w_common.sum() > 0: w_common = w_common / w_common.sum()\n",
        "    W_mat.append(w_common)\n",
        "W_mat = np.vstack(W_mat) if len(W_mat) else np.zeros((0, keep_all.sum()), dtype=np.float64)\n",
        "w_avg = W_mat.mean(axis=0) if W_mat.size else np.ones(keep_all.sum())/keep_all.sum()\n",
        "if w_avg.sum() > 0: w_avg = w_avg / w_avg.sum()\n",
        "\n",
        "print('Chosen legs (forward-chain):', sel_names)\n",
        "print('Per-chain AUCs:', [float(a) for a in aucs])\n",
        "print('Averaged weights (no shrink):', w_avg)\n",
        "\n",
        "# Build test submission using averaged weights on rank-transformed test for selected legs\n",
        "T_sel = T[:, keep_all]\n",
        "def apply_rank(mat): return np.apply_along_axis(rank01, 0, mat)\n",
        "T_rank = apply_rank(T_sel)\n",
        "test_blend_fc = (T_rank @ w_avg).astype(np.float32)\n",
        "\n",
        "# Save\n",
        "sub_fc = pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend_fc})\n",
        "sub_fc.to_csv('submission_time_forward_chain.csv', index=False)\n",
        "sub_fc.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_time_forward_chain.csv and updated submission.csv. Rows:', len(sub_fc))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 1: valid size=562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chain 1 with_subs: AUC=0.66664 legs=['lr_meta_g', 'xgb_svd_meta', 'st_embed_bag_v2', 'mpnet_embed_bag_v2', 'e5_embed_g', 'lr_subs_g'] w=[0.21159857 0.19811641 0.17801016 0.17003187 0.24224299 0.        ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chain 1 no_subs: AUC=0.66664 legs=['lr_meta_g', 'xgb_svd_meta', 'st_embed_bag_v2', 'mpnet_embed_bag_v2', 'e5_embed_g'] w=[0.21159857 0.19811641 0.17801016 0.17003187 0.24224299]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 2: valid size=278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chain 2 with_subs: AUC=0.74896 legs=['lr_meta_g', 'xgb_svd_meta', 'st_embed_bag_v2', 'mpnet_embed_bag_v2', 'e5_embed_g', 'lr_subs_g'] w=[0.2649735  0.03920233 0.08336516 0.39553677 0.         0.21692224]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chain 2 no_subs: AUC=0.74586 legs=['lr_meta_g', 'xgb_svd_meta', 'st_embed_bag_v2', 'mpnet_embed_bag_v2', 'e5_embed_g'] w=[0.46580581 0.03985465 0.07446941 0.41987014 0.        ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3: valid size=268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chain 3 with_subs: AUC=0.63565 legs=['lr_meta_g', 'xgb_svd_meta', 'st_embed_bag_v2', 'mpnet_embed_bag_v2', 'e5_embed_g', 'lr_subs_g'] w=[0.1789592  0.19912775 0.07158773 0.         0.32647242 0.2238529 ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chain 3 no_subs: AUC=0.63626 legs=['lr_meta_g', 'xgb_svd_meta', 'st_embed_bag_v2', 'mpnet_embed_bag_v2', 'e5_embed_g'] w=[0.37452554 0.19850363 0.09950774 0.         0.32746309]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen legs (forward-chain): ['lr_meta_g', 'xgb_svd_meta', 'st_embed_bag_v2', 'mpnet_embed_bag_v2', 'e5_embed_g']\nPer-chain AUCs: [0.6666371477657145, 0.7489612449950895, 0.6362621753246754]\nAveraged weights (no shrink): [0.30816619 0.14889396 0.12799208 0.22504574 0.18990203]\nSaved submission_time_forward_chain.csv and updated submission.csv. Rows: 1162\n"
          ]
        }
      ]
    },
    {
      "id": "249baba2-f662-4a12-b8fc-2f41f8b2265b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Adversarial Validation (AV): train-vs-test classifier to detect shift; save sample weights for retraining fast legs\n",
        "import os, re, time, numpy as np, pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import xgboost as xgb\n",
        "\n",
        "def load_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path)\n",
        "    except ValueError:\n",
        "        import json as _json\n",
        "        with open(path, 'r') as f:\n",
        "            return pd.DataFrame(_json.load(f))\n",
        "\n",
        "train = load_df('train.json')\n",
        "test = load_df('test.json')\n",
        "title_col = 'request_title' if 'request_title' in train.columns else 'title'\n",
        "body_col = 'request_text_edit_aware' if 'request_text_edit_aware' in train.columns else 'request_text'\n",
        "\n",
        "def build_text_upweighted(df: pd.DataFrame) -> pd.Series:\n",
        "    t = df[title_col].fillna('').astype(str)\n",
        "    b = df[body_col].fillna('').astype(str)\n",
        "    return (t + ' ' + t + ' [SEP] ' + b)\n",
        "\n",
        "txt_tr = build_text_upweighted(train)\n",
        "txt_te = build_text_upweighted(test)\n",
        "\n",
        "# Enriched meta (reuse from earlier: time features + lexicons) but defined locally\n",
        "def count_urls(s: str) -> int: return len(re.findall(r'https?://\\S+', s or ''))\n",
        "def has_imgur(s: str) -> int: return 1 if re.search(r'imgur\\.com', s or '', flags=re.IGNORECASE) else 0\n",
        "def count_digits(s: str) -> int: return sum(ch.isdigit() for ch in (s or ''))\n",
        "def dollar_flag(s: str) -> int: return 1 if ('$' in (s or '')) or re.search(r'\\b\\d+\\s*(dollars|bucks)\\b', s or '', flags=re.IGNORECASE) else 0\n",
        "def caps_ratio(s: str) -> float:\n",
        "    if not s: return 0.0\n",
        "    letters = [c for c in s if c.isalpha()]\n",
        "    return (sum(c.isupper() for c in letters) / max(1, len(letters))) if letters else 0.0\n",
        "def word_count(s: str) -> int: return len((s or '').split())\n",
        "def exclam_count(s: str) -> int: return (s or '').count('!')\n",
        "def question_count(s: str) -> int: return (s or '').count('?')\n",
        "def parse_subreddit_count(x) -> int: return len(x) if isinstance(x, list) else 0\n",
        "def safe_log1p_signed(x): return np.sign(x) * np.log1p(np.abs(x))\n",
        "LEX_PATTERNS = {\n",
        "    'lex_please': r'\\bplease\\b',\n",
        "    'lex_thanks': r'\\b(thank you|thanks in advance|thanks|thank|tia)\\b',\n",
        "    'lex_appreciate': r'\\b(appreciate|appreciated)\\b',\n",
        "    'lex_pay_it_forward': r'\\b(pay it forward|return the favor)\\b',\n",
        "    'lex_repay': r'\\b(repay|pay you back|pay back)\\b',\n",
        "    'lex_willing': r\"\\b(willing to|i\\'ll|i will|i can)\\b\",\n",
        "    'lex_karma': r'\\bkarma\\b',\n",
        "    'lex_evidence': r'\\b(proof|receipt|photo|picture|pic|verify|verification|evidence)\\b',\n",
        "    'lex_imgur_word': r'\\bimgur\\b',\n",
        "    'lex_student': r'\\b(student|college|university|finals|exam|midterm)\\b',\n",
        "    'lex_jobloss': r'\\b(unemployed|laid off|lost my job|between jobs|job hunt)\\b',\n",
        "    'lex_broke': r'\\b(broke)\\b',\n",
        "    'lex_rent_bills': r'\\b(rent|bill|bills|utilities|electric|gas|water|paycheck)\\b',\n",
        "    'lex_family': r'\\b(family|kids?|children|baby|pregnant|son|daughter|wife|husband)\\b',\n",
        "    'lex_hungry': r'\\b(hungry|starving|no food)\\b|\\bfood (stamps|pantry)\\b',\n",
        "    'lex_struggling': r'\\b(desperate|struggling)\\b',\n",
        "    'lex_urgency': r'\\b(tonight|today|tomorrow|asap|urgent)\\b',\n",
        "    'lex_help': r'\\bhelp\\b',\n",
        "}\n",
        "def add_lexicons(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    txt = (df[title_col].fillna('').astype(str) + ' ' + df[body_col].fillna('').astype(str)).str.lower()\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    for name, pat in LEX_PATTERNS.items():\n",
        "        out[name] = txt.str.count(pat, flags=re.IGNORECASE).astype(float)\n",
        "    return out\n",
        "def build_meta_enriched(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    title = df[title_col].fillna('').astype(str)\n",
        "    body = df[body_col].fillna('').astype(str)\n",
        "    ts = pd.to_numeric(df['unix_timestamp_of_request'], errors='coerce')\n",
        "    dt = pd.to_datetime(ts, unit='s', utc=True)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    keep_cols = [\n",
        "        'requester_account_age_in_days_at_request',\n",
        "        'requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request',\n",
        "        'requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request',\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "    ]\n",
        "    for c in keep_cols:\n",
        "        out[c] = pd.to_numeric(df[c], errors='coerce') if c in df.columns else 0.0\n",
        "    out['requester_subreddits_count'] = df['requester_subreddits_at_request'].apply(parse_subreddit_count).astype(float) if 'requester_subreddits_at_request' in df.columns else 0.0\n",
        "    # time features\n",
        "    out['hour'] = dt.dt.hour.astype(float)\n",
        "    out['weekday'] = dt.dt.weekday.astype(float)\n",
        "    out['is_weekend'] = dt.dt.weekday.isin([5,6]).astype(float)\n",
        "    out['month'] = dt.dt.month.astype(float)\n",
        "    out['quarter'] = dt.dt.quarter.astype(float)\n",
        "    base_ts = np.nanmin(ts.values)\n",
        "    out['days_since_start'] = ((ts - base_ts) / 86400.0).astype(float)\n",
        "    order = np.argsort(ts.values)\n",
        "    rel = np.empty_like(order, dtype=np.float64)\n",
        "    rel[order] = np.arange(len(order), dtype=np.float64)\n",
        "    out['relative_position'] = (rel / max(1, len(order)-1)).astype(float)\n",
        "    # text stats\n",
        "    out['title_len_chars'] = title.str.len().astype(float)\n",
        "    out['title_len_words'] = title.apply(word_count).astype(float)\n",
        "    out['body_len_chars'] = body.str.len().astype(float)\n",
        "    out['body_len_words'] = body.apply(word_count).astype(float)\n",
        "    out['url_count'] = body.apply(count_urls).astype(float)\n",
        "    out['has_imgur'] = body.apply(has_imgur).astype(float)\n",
        "    out['digits_count'] = body.apply(count_digits).astype(float)\n",
        "    out['dollar_flag'] = body.apply(dollar_flag).astype(float)\n",
        "    out['caps_ratio'] = body.apply(caps_ratio).astype(float)\n",
        "    out['exclam_count'] = body.apply(exclam_count).astype(float)\n",
        "    out['question_count'] = body.apply(question_count).astype(float)\n",
        "    lex = add_lexicons(df)\n",
        "    out = pd.concat([out, lex], axis=1).replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
        "    # transforms\n",
        "    nonneg_cols = [\n",
        "        'requester_account_age_in_days_at_request','requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request','requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request','requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request','requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_subreddits_count','hour','weekday','is_weekend','month','quarter','days_since_start','relative_position',\n",
        "        'title_len_chars','title_len_words','body_len_chars','body_len_words',\n",
        "        'url_count','has_imgur','digits_count','dollar_flag','exclam_count','question_count',\n",
        "        *list(LEX_PATTERNS.keys()),\n",
        "    ]\n",
        "    for c in nonneg_cols:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(np.clip(out[c], a_min=0.0, a_max=None))\n",
        "    if 'requester_upvotes_minus_downvotes_at_request' in out.columns:\n",
        "        out['requester_upvotes_minus_downvotes_at_request'] = safe_log1p_signed(out['requester_upvotes_minus_downvotes_at_request'].values)\n",
        "    return out.replace([np.inf,-np.inf],0.0).fillna(0.0)\n",
        "\n",
        "meta_tr = build_meta_enriched(train)\n",
        "meta_te = build_meta_enriched(test)\n",
        "print('AV meta shapes:', meta_tr.shape, meta_te.shape, flush=True)\n",
        "\n",
        "# TF-IDF -> SVD features (100 word + 100 char)\n",
        "wv = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.95, max_features=150000, sublinear_tf=True, lowercase=True, dtype=np.float32)\n",
        "cv = TfidfVectorizer(analyzer='char', ngram_range=(3,6), min_df=2, max_features=300000, sublinear_tf=True, lowercase=True, dtype=np.float32)\n",
        "Xw_tr = wv.fit_transform(txt_tr)\n",
        "Xc_tr = cv.fit_transform(txt_tr)\n",
        "Xw_te = wv.transform(txt_te)\n",
        "Xc_te = cv.transform(txt_te)\n",
        "svd_w = TruncatedSVD(n_components=100, random_state=42)\n",
        "svd_c = TruncatedSVD(n_components=100, random_state=42)\n",
        "Zw_tr = svd_w.fit_transform(Xw_tr).astype(np.float32)\n",
        "Zc_tr = svd_c.fit_transform(Xc_tr).astype(np.float32)\n",
        "Zw_te = svd_w.transform(Xw_te).astype(np.float32)\n",
        "Zc_te = svd_c.transform(Xc_te).astype(np.float32)\n",
        "\n",
        "# Build AV datasets\n",
        "X_meta_tr = meta_tr.values.astype(np.float32)\n",
        "X_meta_te = meta_te.values.astype(np.float32)\n",
        "sc_meta = StandardScaler(with_mean=True, with_std=True)\n",
        "Xm_tr = sc_meta.fit_transform(X_meta_tr).astype(np.float32)\n",
        "Xm_te = sc_meta.transform(X_meta_te).astype(np.float32)\n",
        "\n",
        "X_tr_all = np.hstack([Zw_tr, Zc_tr, Xm_tr]).astype(np.float32)\n",
        "X_te_all = np.hstack([Zw_te, Zc_te, Xm_te]).astype(np.float32)\n",
        "X = np.vstack([X_tr_all, X_te_all])\n",
        "y_is_test = np.array([0]*len(train) + [1]*len(test), dtype=np.int32)\n",
        "\n",
        "# Variant 1: WITHOUT time columns (semantic/user shift)\n",
        "time_cols = ['hour','weekday','is_weekend','month','quarter','days_since_start','relative_position']\n",
        "meta_no_time_tr = meta_tr.drop(columns=[c for c in time_cols if c in meta_tr.columns])\n",
        "meta_no_time_te = meta_te.drop(columns=[c for c in time_cols if c in meta_te.columns])\n",
        "Xm_nt_tr = StandardScaler(with_mean=True, with_std=True).fit_transform(meta_no_time_tr.values.astype(np.float32))\n",
        "Xm_nt_te = StandardScaler(with_mean=True, with_std=True).fit(meta_no_time_tr.values.astype(np.float32)).transform(meta_no_time_te.values.astype(np.float32))\n",
        "X_nt = np.vstack([np.hstack([Zw_tr, Zc_tr, Xm_nt_tr]).astype(np.float32),\n",
        "                  np.hstack([Zw_te, Zc_te, Xm_nt_te]).astype(np.float32)])\n",
        "\n",
        "def run_av_auc(Xmat, ybin, name):\n",
        "    # Simple XGB classifier with 5-fold CV on concatenated data\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    oof = np.zeros(len(ybin), dtype=np.float32)\n",
        "    params = dict(objective='binary:logistic', eval_metric='auc', tree_method='hist', max_depth=5, learning_rate=0.08,\n",
        "                  subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0, max_bin=256, random_state=42)\n",
        "    for i, (tr_idx, va_idx) in enumerate(skf.split(Xmat, ybin), 1):\n",
        "        dtr = xgb.DMatrix(Xmat[tr_idx], label=ybin[tr_idx])\n",
        "        dva = xgb.DMatrix(Xmat[va_idx], label=ybin[va_idx])\n",
        "        booster = xgb.train(params, dtr, num_boost_round=2000, evals=[(dva,'valid')], verbose_eval=False, early_stopping_rounds=100)\n",
        "        oof[va_idx] = booster.predict(dva, iteration_range=(0, booster.best_iteration+1))\n",
        "    auc = roc_auc_score(ybin, oof)\n",
        "    print(f'AV OOF AUC [{name}]: {auc:.5f}', flush=True)\n",
        "    return auc, oof\n",
        "\n",
        "auc_all, oof_all = run_av_auc(X, y_is_test, 'with_time')\n",
        "auc_nt, oof_nt = run_av_auc(X_nt, y_is_test, 'no_time')\n",
        "\n",
        "# Build sample weights for TRAIN rows using AV 'with_time' probabilities p/(1-p), clipped to [0.5, 2.0]\n",
        "p_train = oof_all[:len(train)]\n",
        "eps = 1e-6\n",
        "w_train = np.clip(p_train / np.clip(1.0 - p_train, eps, 1.0), 0.5, 2.0).astype(np.float32)\n",
        "np.save('av_weights.npy', w_train)\n",
        "print('Saved av_weights.npy. Train weight stats: min={:.3f} med={:.3f} max={:.3f}'.format(float(w_train.min()), float(np.median(w_train)), float(w_train.max())))\n",
        "print('AV summary -> with_time AUC={:.5f}, no_time AUC={:.5f}'.format(auc_all, auc_nt))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AV meta shapes: (2878, 46) (1162, 46)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AV OOF AUC [with_time]: 0.99989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AV OOF AUC [no_time]: 0.96817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved av_weights.npy. Train weight stats: min=0.500 med=0.500 max=0.752\nAV summary -> with_time AUC=0.99989, no_time AUC=0.96817\n"
          ]
        }
      ]
    },
    {
      "id": "b258586b-ae6f-4c64-8b3c-ce04ac22322e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# AV-reweighted training for fast XGB-on-emb legs (MiniLM, MPNet, E5) + update .npy files\n",
        "import sys, subprocess, time, os, re, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'sentence-transformers==2.7.0'], check=True)\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "\n",
        "def load_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path)\n",
        "    except ValueError:\n",
        "        import json as _json\n",
        "        with open(path, 'r') as f:\n",
        "            return pd.DataFrame(_json.load(f))\n",
        "\n",
        "train = load_df('train.json')\n",
        "test = load_df('test.json')\n",
        "y = train['requester_received_pizza'].astype(int).values\n",
        "groups = train['requester_username'].fillna('').astype(str).values if 'requester_username' in train.columns else np.arange(len(train))\n",
        "title_col = 'request_title' if 'request_title' in train.columns else 'title'\n",
        "body_col = 'request_text_edit_aware' if 'request_text_edit_aware' in train.columns else 'request_text'\n",
        "\n",
        "def build_text_upweighted(df: pd.DataFrame) -> pd.Series:\n",
        "    t = df[title_col].fillna('').astype(str)\n",
        "    b = df[body_col].fillna('').astype(str)\n",
        "    return (t + ' ' + t + ' [SEP] ' + b)\n",
        "\n",
        "text_tr = build_text_upweighted(train).tolist()\n",
        "text_te = build_text_upweighted(test).tolist()\n",
        "\n",
        "# Enriched meta (same as in cells 21/23/25)\n",
        "def count_urls(s: str) -> int: return len(re.findall(r'https?://\\S+', s or ''))\n",
        "def has_imgur(s: str) -> int: return 1 if re.search(r'imgur\\.com', s or '', flags=re.IGNORECASE) else 0\n",
        "def count_digits(s: str) -> int: return sum(ch.isdigit() for ch in (s or ''))\n",
        "def dollar_flag(s: str) -> int: return 1 if ('$' in (s or '')) or re.search(r'\\b\\d+\\s*(dollars|bucks)\\b', s or '', flags=re.IGNORECASE) else 0\n",
        "def caps_ratio(s: str) -> float:\n",
        "    if not s: return 0.0\n",
        "    letters = [c for c in s if c.isalpha()]\n",
        "    return (sum(c.isupper() for c in letters) / max(1, len(letters))) if letters else 0.0\n",
        "def word_count(s: str) -> int: return len((s or '').split())\n",
        "def exclam_count(s: str) -> int: return (s or '').count('!')\n",
        "def question_count(s: str) -> int: return (s or '').count('?')\n",
        "def parse_subreddit_count(x) -> int: return len(x) if isinstance(x, list) else 0\n",
        "def safe_log1p_signed(x): return np.sign(x) * np.log1p(np.abs(x))\n",
        "LEX_PATTERNS = {\n",
        "    'lex_please': r'\\bplease\\b',\n",
        "    'lex_thanks': r'\\b(thank you|thanks in advance|thanks|thank|tia)\\b',\n",
        "    'lex_appreciate': r'\\b(appreciate|appreciated)\\b',\n",
        "    'lex_pay_it_forward': r'\\b(pay it forward|return the favor)\\b',\n",
        "    'lex_repay': r'\\b(repay|pay you back|pay back)\\b',\n",
        "    'lex_willing': r\"\\b(willing to|i\\'ll|i will|i can)\\b\",\n",
        "    'lex_karma': r'\\bkarma\\b',\n",
        "    'lex_evidence': r'\\b(proof|receipt|photo|picture|pic|verify|verification|evidence)\\b',\n",
        "    'lex_imgur_word': r'\\bimgur\\b',\n",
        "    'lex_student': r'\\b(student|college|university|finals|exam|midterm)\\b',\n",
        "    'lex_jobloss': r'\\b(unemployed|laid off|lost my job|between jobs|job hunt)\\b',\n",
        "    'lex_broke': r'\\b(broke)\\b',\n",
        "    'lex_rent_bills': r'\\b(rent|bill|bills|utilities|electric|gas|water|paycheck)\\b',\n",
        "    'lex_family': r'\\b(family|kids?|children|baby|pregnant|son|daughter|wife|husband)\\b',\n",
        "    'lex_hungry': r'\\b(hungry|starving|no food)\\b|\\bfood (stamps|pantry)\\b',\n",
        "    'lex_struggling': r'\\b(desperate|struggling)\\b',\n",
        "    'lex_urgency': r'\\b(tonight|today|tomorrow|asap|urgent)\\b',\n",
        "    'lex_help': r'\\bhelp\\b',\n",
        "}\n",
        "def add_lexicons(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    txt = (df[title_col].fillna('').astype(str) + ' ' + df[body_col].fillna('').astype(str)).str.lower()\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    for name, pat in LEX_PATTERNS.items():\n",
        "        out[name] = txt.str.count(pat, flags=re.IGNORECASE).astype(float)\n",
        "    return out\n",
        "def build_meta_enriched(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    title = df[title_col].fillna('').astype(str)\n",
        "    body = df[body_col].fillna('').astype(str)\n",
        "    ts = pd.to_numeric(df['unix_timestamp_of_request'], errors='coerce')\n",
        "    dt = pd.to_datetime(ts, unit='s', utc=True)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    keep_cols = [\n",
        "        'requester_account_age_in_days_at_request',\n",
        "        'requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request',\n",
        "        'requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request',\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "    ]\n",
        "    for c in keep_cols:\n",
        "        out[c] = pd.to_numeric(df[c], errors='coerce') if c in df.columns else 0.0\n",
        "    out['requester_subreddits_count'] = df['requester_subreddits_at_request'].apply(parse_subreddit_count).astype(float) if 'requester_subreddits_at_request' in df.columns else 0.0\n",
        "    out['hour'] = dt.dt.hour.astype(float)\n",
        "    out['weekday'] = dt.dt.weekday.astype(float)\n",
        "    out['is_weekend'] = dt.dt.weekday.isin([5,6]).astype(float)\n",
        "    out['month'] = dt.dt.month.astype(float)\n",
        "    out['quarter'] = dt.dt.quarter.astype(float)\n",
        "    base_ts = np.nanmin(ts.values)\n",
        "    out['days_since_start'] = ((ts - base_ts) / 86400.0).astype(float)\n",
        "    order = np.argsort(ts.values)\n",
        "    rel = np.empty_like(order, dtype=np.float64)\n",
        "    rel[order] = np.arange(len(order), dtype=np.float64)\n",
        "    out['relative_position'] = (rel / max(1, len(order)-1)).astype(float)\n",
        "    out['title_len_chars'] = title.str.len().astype(float)\n",
        "    out['title_len_words'] = title.apply(word_count).astype(float)\n",
        "    out['body_len_chars'] = body.str.len().astype(float)\n",
        "    out['body_len_words'] = body.apply(word_count).astype(float)\n",
        "    out['url_count'] = body.apply(count_urls).astype(float)\n",
        "    out['has_imgur'] = body.apply(has_imgur).astype(float)\n",
        "    out['digits_count'] = body.apply(count_digits).astype(float)\n",
        "    out['dollar_flag'] = body.apply(dollar_flag).astype(float)\n",
        "    out['caps_ratio'] = body.apply(caps_ratio).astype(float)\n",
        "    out['exclam_count'] = body.apply(exclam_count).astype(float)\n",
        "    out['question_count'] = body.apply(question_count).astype(float)\n",
        "    lex = add_lexicons(df)\n",
        "    out = pd.concat([out, lex], axis=1).replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
        "    nonneg_cols = [\n",
        "        'requester_account_age_in_days_at_request','requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request','requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request','requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request','requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_subreddits_count','hour','weekday','is_weekend','month','quarter','days_since_start','relative_position',\n",
        "        'title_len_chars','title_len_words','body_len_chars','body_len_words',\n",
        "        'url_count','has_imgur','digits_count','dollar_flag','exclam_count','question_count',\n",
        "        *list(LEX_PATTERNS.keys()),\n",
        "    ]\n",
        "    for c in nonneg_cols:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(np.clip(out[c], a_min=0.0, a_max=None))\n",
        "    if 'requester_upvotes_minus_downvotes_at_request' in out.columns:\n",
        "        out['requester_upvotes_minus_downvotes_at_request'] = safe_log1p_signed(out['requester_upvotes_minus_downvotes_at_request'].values)\n",
        "    return out.replace([np.inf,-np.inf],0.0).fillna(0.0)\n",
        "\n",
        "meta_train = build_meta_enriched(train)\n",
        "meta_test = build_meta_enriched(test)\n",
        "print('Meta (for AV-weighted retrain) shapes:', meta_train.shape, meta_test.shape, flush=True)\n",
        "\n",
        "# Load AV sample weights\n",
        "w_train = np.load('av_weights.npy') if os.path.exists('av_weights.npy') else np.ones(len(train), dtype=np.float32)\n",
        "print('AV weights loaded. Stats: min={:.3f}, med={:.3f}, max={:.3f}'.format(float(w_train.min()), float(np.median(w_train)), float(w_train.max())), flush=True)\n",
        "\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = list(sgkf.split(np.zeros(len(y)), y, groups))\n",
        "\n",
        "def train_xgb_bag_weighted(emb_tr, emb_te, seeds=(7,13,29), name='leg'):\n",
        "    oof_bag = np.zeros(len(train), dtype=np.float32)\n",
        "    test_bag_per_seed = []\n",
        "    for seed in seeds:\n",
        "        rs = np.random.RandomState(seed)\n",
        "        md = 4 + (rs.rand() < 0.5)\n",
        "        subs = 0.75 + rs.rand()*0.10\n",
        "        cols = 0.75 + rs.rand()*0.15\n",
        "        lr = 0.045 + rs.rand()*0.010\n",
        "        params = dict(\n",
        "            objective='binary:logistic', eval_metric='auc', tree_method='hist',\n",
        "            max_depth=int(md), learning_rate=float(lr), subsample=float(subs), colsample_bytree=float(cols),\n",
        "            reg_lambda=1.0, min_child_weight=1.0, max_bin=256, random_state=int(seed)\n",
        "        )\n",
        "        oof_seed = np.zeros(len(train), dtype=np.float32)\n",
        "        test_preds = []\n",
        "        for i, (trn_idx, val_idx) in enumerate(folds, 1):\n",
        "            t0 = time.time()\n",
        "            scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "            M_tr = scaler.fit_transform(meta_train.iloc[trn_idx].values).astype(np.float32)\n",
        "            M_va = scaler.transform(meta_train.iloc[val_idx].values).astype(np.float32)\n",
        "            X_tr = np.hstack([emb_tr[trn_idx], M_tr]).astype(np.float32)\n",
        "            X_va = np.hstack([emb_tr[val_idx], M_va]).astype(np.float32)\n",
        "            dtr = xgb.DMatrix(X_tr, label=y[trn_idx], weight=w_train[trn_idx])\n",
        "            dva = xgb.DMatrix(X_va, label=y[val_idx])\n",
        "            booster = xgb.train(params, dtr, num_boost_round=2000, evals=[(dva,'valid')], verbose_eval=False, early_stopping_rounds=100)\n",
        "            oof_seed[val_idx] = booster.predict(dva, iteration_range=(0, booster.best_iteration+1))\n",
        "            M_te = scaler.transform(meta_test.values).astype(np.float32)\n",
        "            Xt = np.hstack([emb_te, M_te]).astype(np.float32)\n",
        "            dte = xgb.DMatrix(Xt)\n",
        "            test_preds.append(booster.predict(dte, iteration_range=(0, booster.best_iteration+1)).astype(np.float32))\n",
        "            print(f'  [{name} seed {seed}] Fold {i} AUC={roc_auc_score(y[val_idx], oof_seed[val_idx]):.5f} | iters={booster.best_iteration+1} | {time.time()-t0:.1f}s', flush=True)\n",
        "        print(f'[{name}] Seed {seed} OOF AUC={roc_auc_score(y, oof_seed):.5f}', flush=True)\n",
        "        oof_bag += oof_seed / len(seeds)\n",
        "        test_bag_per_seed.append(np.mean(np.vstack(test_preds), axis=0).astype(np.float32))\n",
        "    test_bag = np.mean(np.vstack(test_bag_per_seed), axis=0).astype(np.float32)\n",
        "    auc_bag = roc_auc_score(y, oof_bag)\n",
        "    print(f'[{name}] AV-weighted bagged OOF AUC: {auc_bag:.5f}', flush=True)\n",
        "    return oof_bag, test_bag\n",
        "\n",
        "# Encode embeddings\n",
        "print('Encoding MiniLM...', flush=True)\n",
        "minilm = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')\n",
        "emb_minilm_tr = minilm.encode(text_tr, batch_size=64, show_progress_bar=False, normalize_embeddings=True).astype(np.float32)\n",
        "emb_minilm_te = minilm.encode(text_te, batch_size=64, show_progress_bar=False, normalize_embeddings=True).astype(np.float32)\n",
        "print('Encoding MPNet...', flush=True)\n",
        "mpnet = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1', device='cpu')\n",
        "emb_mpnet_tr = mpnet.encode(text_tr, batch_size=64, show_progress_bar=False, normalize_embeddings=True).astype(np.float32)\n",
        "emb_mpnet_te = mpnet.encode(text_te, batch_size=64, show_progress_bar=False, normalize_embeddings=True).astype(np.float32)\n",
        "print('Encoding E5-base-v2...', flush=True)\n",
        "e5 = SentenceTransformer('intfloat/e5-base-v2', device='cpu')\n",
        "emb_e5_tr = e5.encode([f'query: {s}' for s in text_tr], batch_size=128, show_progress_bar=False, normalize_embeddings=True).astype(np.float32)\n",
        "emb_e5_te = e5.encode([f'query: {s}' for s in text_te], batch_size=128, show_progress_bar=False, normalize_embeddings=True).astype(np.float32)\n",
        "\n",
        "seeds = (7,13,29)\n",
        "print('=== AV-weighted bagging: MiniLM ===', flush=True)\n",
        "oof_st_bag_w, test_st_bag_w = train_xgb_bag_weighted(emb_minilm_tr, emb_minilm_te, seeds=seeds, name='MiniLM(av)')\n",
        "np.save('oof_st_embed_bag_v2.npy', oof_st_bag_w); np.save('test_st_embed_bag_v2.npy', test_st_bag_w)\n",
        "\n",
        "print('=== AV-weighted bagging: MPNet ===', flush=True)\n",
        "oof_mp_bag_w, test_mp_bag_w = train_xgb_bag_weighted(emb_mpnet_tr, emb_mpnet_te, seeds=seeds, name='MPNet(av)')\n",
        "np.save('oof_mpnet_embed_bag_v2.npy', oof_mp_bag_w); np.save('test_mpnet_embed_bag_v2.npy', test_mp_bag_w)\n",
        "\n",
        "print('=== AV-weighted bagging: E5 ===', flush=True)\n",
        "oof_e5_bag_w, test_e5_bag_w = train_xgb_bag_weighted(emb_e5_tr, emb_e5_te, seeds=seeds, name='E5(av)')\n",
        "np.save('oof_e5_embed_g.npy', oof_e5_bag_w); np.save('test_e5_embed_g.npy', test_e5_bag_w)\n",
        "print('Saved AV-weighted OOF/test for MiniLM, MPNet, E5.')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta (for AV-weighted retrain) shapes: (2878, 46) (1162, 46)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AV weights loaded. Stats: min=0.500, med=0.500, max=0.752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding MiniLM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding MPNet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You try to use a model that was created with version 3.0.0.dev0, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n\n\n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding E5-base-v2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== AV-weighted bagging: MiniLM ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MiniLM(av) seed 7] Fold 1 AUC=0.67837 | iters=336 | 3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MiniLM(av) seed 7] Fold 2 AUC=0.69849 | iters=7 | 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MiniLM(av) seed 7] Fold 3 AUC=0.67774 | iters=75 | 1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MiniLM(av) seed 7] Fold 4 AUC=0.66487 | iters=256 | 2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MiniLM(av) seed 7] Fold 5 AUC=0.70801 | iters=49 | 1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MiniLM(av)] Seed 7 OOF AUC=0.66498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MiniLM(av) seed 13] Fold 1 AUC=0.67450 | iters=24 | 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MiniLM(av) seed 13] Fold 2 AUC=0.69810 | iters=61 | 0.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MiniLM(av) seed 13] Fold 3 AUC=0.66844 | iters=37 | 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MiniLM(av) seed 13] Fold 4 AUC=0.66286 | iters=46 | 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MiniLM(av) seed 13] Fold 5 AUC=0.72800 | iters=21 | 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MiniLM(av)] Seed 13 OOF AUC=0.68104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MiniLM(av) seed 29] Fold 1 AUC=0.67422 | iters=140 | 1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MiniLM(av) seed 29] Fold 2 AUC=0.70413 | iters=30 | 0.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MiniLM(av) seed 29] Fold 3 AUC=0.65992 | iters=197 | 1.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MiniLM(av) seed 29] Fold 4 AUC=0.66373 | iters=120 | 1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MiniLM(av) seed 29] Fold 5 AUC=0.74106 | iters=16 | 0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MiniLM(av)] Seed 29 OOF AUC=0.67325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MiniLM(av)] AV-weighted bagged OOF AUC: 0.68668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== AV-weighted bagging: MPNet ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MPNet(av) seed 7] Fold 1 AUC=0.68504 | iters=112 | 3.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MPNet(av) seed 7] Fold 2 AUC=0.68625 | iters=122 | 3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MPNet(av) seed 7] Fold 3 AUC=0.65774 | iters=66 | 2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MPNet(av) seed 7] Fold 4 AUC=0.67285 | iters=109 | 3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MPNet(av) seed 7] Fold 5 AUC=0.69575 | iters=42 | 2.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MPNet(av)] Seed 7 OOF AUC=0.67854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MPNet(av) seed 13] Fold 1 AUC=0.68295 | iters=75 | 1.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MPNet(av) seed 13] Fold 2 AUC=0.70046 | iters=33 | 1.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MPNet(av) seed 13] Fold 3 AUC=0.65052 | iters=92 | 2.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MPNet(av) seed 13] Fold 4 AUC=0.67186 | iters=92 | 2.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MPNet(av) seed 13] Fold 5 AUC=0.72161 | iters=60 | 1.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MPNet(av)] Seed 13 OOF AUC=0.68283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MPNet(av) seed 29] Fold 1 AUC=0.68682 | iters=49 | 1.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MPNet(av) seed 29] Fold 2 AUC=0.69046 | iters=99 | 2.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MPNet(av) seed 29] Fold 3 AUC=0.64576 | iters=40 | 1.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MPNet(av) seed 29] Fold 4 AUC=0.66695 | iters=77 | 1.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [MPNet(av) seed 29] Fold 5 AUC=0.72553 | iters=12 | 1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MPNet(av)] Seed 29 OOF AUC=0.67312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MPNet(av)] AV-weighted bagged OOF AUC: 0.68955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== AV-weighted bagging: E5 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5(av) seed 7] Fold 1 AUC=0.66958 | iters=95 | 3.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5(av) seed 7] Fold 2 AUC=0.65991 | iters=84 | 2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5(av) seed 7] Fold 3 AUC=0.65957 | iters=41 | 2.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5(av) seed 7] Fold 4 AUC=0.65043 | iters=88 | 2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5(av) seed 7] Fold 5 AUC=0.70043 | iters=8 | 1.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E5(av)] Seed 7 OOF AUC=0.65931\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5(av) seed 13] Fold 1 AUC=0.68896 | iters=74 | 1.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5(av) seed 13] Fold 2 AUC=0.67512 | iters=120 | 2.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5(av) seed 13] Fold 3 AUC=0.65584 | iters=13 | 1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5(av) seed 13] Fold 4 AUC=0.67238 | iters=34 | 1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5(av) seed 13] Fold 5 AUC=0.72278 | iters=24 | 1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E5(av)] Seed 13 OOF AUC=0.67559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5(av) seed 29] Fold 1 AUC=0.68271 | iters=94 | 1.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5(av) seed 29] Fold 2 AUC=0.69455 | iters=65 | 1.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5(av) seed 29] Fold 3 AUC=0.65535 | iters=263 | 3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5(av) seed 29] Fold 4 AUC=0.67240 | iters=215 | 2.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5(av) seed 29] Fold 5 AUC=0.70351 | iters=190 | 2.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E5(av)] Seed 29 OOF AUC=0.67627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E5(av)] AV-weighted bagged OOF AUC: 0.68767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved AV-weighted OOF/test for MiniLM, MPNet, E5.\n"
          ]
        }
      ]
    },
    {
      "id": "6c482da8-98aa-4e9e-a9d6-c208fe814c4b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Add RAOP-specific ratios + flair + time interactions to meta; retrain/bag E5 XGB and overwrite OOF/test\n",
        "import sys, subprocess, time, os, re, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'sentence-transformers==2.7.0'], check=True)\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "\n",
        "def load_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path)\n",
        "    except ValueError:\n",
        "        import json as _json\n",
        "        with open(path, 'r') as f:\n",
        "            return pd.DataFrame(_json.load(f))\n",
        "\n",
        "train = load_df('train.json')\n",
        "test = load_df('test.json')\n",
        "y = train['requester_received_pizza'].astype(int).values\n",
        "groups = train['requester_username'].fillna('').astype(str).values if 'requester_username' in train.columns else np.arange(len(train))\n",
        "title_col = 'request_title' if 'request_title' in train.columns else 'title'\n",
        "body_col = 'request_text_edit_aware' if 'request_text_edit_aware' in train.columns else 'request_text'\n",
        "\n",
        "def build_text_upweighted(df: pd.DataFrame) -> pd.Series:\n",
        "    t = df[title_col].fillna('').astype(str)\n",
        "    b = df[body_col].fillna('').astype(str)\n",
        "    return (t + ' [SEP] ' + t + ' [SEP] ' + b)\n",
        "\n",
        "text_tr = build_text_upweighted(train).tolist()\n",
        "text_te = build_text_upweighted(test).tolist()\n",
        "\n",
        "# Utilities\n",
        "def count_urls(s: str) -> int: return len(re.findall(r'https?://\\S+', s or ''))\n",
        "def has_imgur(s: str) -> int: return 1 if re.search(r'imgur\\.com', s or '', flags=re.IGNORECASE) else 0\n",
        "def count_digits(s: str) -> int: return sum(ch.isdigit() for ch in (s or ''))\n",
        "def dollar_flag(s: str) -> int: return 1 if ('$' in (s or '')) or re.search(r'\\b\\d+\\s*(dollars|bucks)\\b', s or '', flags=re.IGNORECASE) else 0\n",
        "def caps_ratio(s: str) -> float:\n",
        "    if not s: return 0.0\n",
        "    letters = [c for c in s if c.isalpha()]\n",
        "    return (sum(c.isupper() for c in letters) / max(1, len(letters))) if letters else 0.0\n",
        "def word_count(s: str) -> int: return len((s or '').split())\n",
        "def exclam_count(s: str) -> int: return (s or '').count('!')\n",
        "def question_count(s: str) -> int: return (s or '').count('?')\n",
        "def parse_subreddit_count(x) -> int: return len(x) if isinstance(x, list) else 0\n",
        "def safe_log1p_signed(x): return np.sign(x) * np.log1p(np.abs(x))\n",
        "\n",
        "LEX_PATTERNS = {\n",
        "    'lex_please': r'\\bplease\\b',\n",
        "    'lex_thanks': r'\\b(thank you|thanks in advance|thanks|thank|tia)\\b',\n",
        "    'lex_appreciate': r'\\b(appreciate|appreciated)\\b',\n",
        "    'lex_pay_it_forward': r'\\b(pay it forward|return the favor)\\b',\n",
        "    'lex_repay': r'\\b(repay|pay you back|pay back)\\b',\n",
        "    'lex_willing': r\"\\b(willing to|i\\'ll|i will|i can)\\b\",\n",
        "    'lex_karma': r'\\bkarma\\b',\n",
        "    'lex_evidence': r'\\b(proof|receipt|photo|picture|pic|verify|verification|evidence)\\b',\n",
        "    'lex_imgur_word': r'\\bimgur\\b',\n",
        "    'lex_student': r'\\b(student|college|university|finals|exam|midterm)\\b',\n",
        "    'lex_jobloss': r'\\b(unemployed|laid off|lost my job|between jobs|job hunt)\\b',\n",
        "    'lex_broke': r'\\b(broke)\\b',\n",
        "    'lex_rent_bills': r'\\b(rent|bill|bills|utilities|electric|gas|water|paycheck)\\b',\n",
        "    'lex_family': r'\\b(family|kids?|children|baby|pregnant|son|daughter|wife|husband)\\b',\n",
        "    'lex_hungry': r'\\b(hungry|starving|no food)\\b|\\bfood (stamps|pantry)\\b',\n",
        "    'lex_struggling': r'\\b(desperate|struggling)\\b',\n",
        "    'lex_urgency': r'\\b(tonight|today|tomorrow|asap|urgent)\\b',\n",
        "    'lex_help': r'\\bhelp\\b',\n",
        "}\n",
        "def add_lexicons(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    txt = (df[title_col].fillna('').astype(str) + ' ' + df[body_col].fillna('').astype(str)).str.lower()\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    for name, pat in LEX_PATTERNS.items():\n",
        "        out[name] = txt.str.count(pat, flags=re.IGNORECASE).astype(float)\n",
        "    return out\n",
        "\n",
        "def build_meta_raop(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    title = df[title_col].fillna('').astype(str)\n",
        "    body = df[body_col].fillna('').astype(str)\n",
        "    ts = pd.to_numeric(df['unix_timestamp_of_request'], errors='coerce')\n",
        "    dt = pd.to_datetime(ts, unit='s', utc=True)\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    # base at_request numeric\n",
        "    keep = [\n",
        "        'requester_account_age_in_days_at_request',\n",
        "        'requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request',\n",
        "        'requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request',\n",
        "        'requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request',\n",
        "        'requester_upvotes_minus_downvotes_at_request',\n",
        "        'requester_upvotes_plus_downvotes_at_request',\n",
        "    ]\n",
        "    for c in keep:\n",
        "        out[c] = pd.to_numeric(df[c], errors='coerce') if c in df.columns else 0.0\n",
        "    out['requester_subreddits_count'] = df['requester_subreddits_at_request'].apply(parse_subreddit_count).astype(float) if 'requester_subreddits_at_request' in df.columns else 0.0\n",
        "    # time features\n",
        "    out['hour'] = dt.dt.hour.astype(float)\n",
        "    out['weekday'] = dt.dt.weekday.astype(float)\n",
        "    out['is_weekend'] = dt.dt.weekday.isin([5,6]).astype(float)\n",
        "    out['month'] = dt.dt.month.astype(float)\n",
        "    out['quarter'] = dt.dt.quarter.astype(float)\n",
        "    base_ts = np.nanmin(ts.values)\n",
        "    out['days_since_start'] = ((ts - base_ts) / 86400.0).astype(float)\n",
        "    order = np.argsort(ts.values)\n",
        "    rel = np.empty_like(order, dtype=np.float64); rel[order] = np.arange(len(order), dtype=np.float64)\n",
        "    out['relative_position'] = (rel / max(1, len(order)-1)).astype(float)\n",
        "    # text stats\n",
        "    out['title_len_chars'] = title.str.len().astype(float)\n",
        "    out['title_len_words'] = title.apply(word_count).astype(float)\n",
        "    out['body_len_chars'] = body.str.len().astype(float)\n",
        "    out['body_len_words'] = body.apply(word_count).astype(float)\n",
        "    out['url_count'] = body.apply(count_urls).astype(float)\n",
        "    out['has_imgur'] = body.apply(has_imgur).astype(float)\n",
        "    out['digits_count'] = body.apply(count_digits).astype(float)\n",
        "    out['dollar_flag'] = body.apply(dollar_flag).astype(float)\n",
        "    out['caps_ratio'] = body.apply(caps_ratio).astype(float)\n",
        "    out['exclam_count'] = body.apply(exclam_count).astype(float)\n",
        "    out['question_count'] = body.apply(question_count).astype(float)\n",
        "    # ratios\n",
        "    c_all = out['requester_number_of_comments_at_request'].values\n",
        "    c_raop = out['requester_number_of_comments_in_raop_at_request'].values\n",
        "    p_all = out['requester_number_of_posts_at_request'].values\n",
        "    p_raop = out['requester_number_of_posts_on_raop_at_request'].values\n",
        "    upm = out['requester_upvotes_minus_downvotes_at_request'].values\n",
        "    upp = out['requester_upvotes_plus_downvotes_at_request'].values\n",
        "    out['raop_comment_ratio'] = (c_raop / (c_all + 1.0))\n",
        "    out['raop_post_ratio'] = (p_raop / (p_all + 1.0))\n",
        "    out['karma_balance_ratio'] = (upm / (upp + 1.0))\n",
        "    out['title_to_body_len'] = (out['title_len_words'] / (out['body_len_words'] + 1.0))\n",
        "    # user flair\n",
        "    flair = df['requester_user_flair'].fillna('').astype(str) if 'requester_user_flair' in df.columns else pd.Series(['']*len(df))\n",
        "    out['has_flair'] = (flair.str.len() > 0).astype(float)\n",
        "    out['flair_len_chars'] = flair.str.len().astype(float)\n",
        "    out['flair_word_count'] = flair.apply(word_count).astype(float)\n",
        "    # lexicons\n",
        "    lex = add_lexicons(df)\n",
        "    out = pd.concat([out, lex], axis=1)\n",
        "    # interactions\n",
        "    out['int_caps_relpos'] = out['caps_ratio'] * out['relative_position']\n",
        "    out['int_urgency_month'] = out['lex_urgency'] * out['month']\n",
        "    # transforms\n",
        "    out = out.replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
        "    nonneg = [\n",
        "        'requester_account_age_in_days_at_request','requester_days_since_first_post_on_raop_at_request',\n",
        "        'requester_number_of_comments_at_request','requester_number_of_comments_in_raop_at_request',\n",
        "        'requester_number_of_posts_at_request','requester_number_of_posts_on_raop_at_request',\n",
        "        'requester_number_of_subreddits_at_request','requester_upvotes_plus_downvotes_at_request',\n",
        "        'requester_subreddits_count','hour','weekday','is_weekend','month','quarter','days_since_start','relative_position',\n",
        "        'title_len_chars','title_len_words','body_len_chars','body_len_words','url_count','has_imgur','digits_count','dollar_flag',\n",
        "        'exclam_count','question_count','raop_comment_ratio','raop_post_ratio','title_to_body_len','has_flair','flair_len_chars','flair_word_count',\n",
        "        *list(LEX_PATTERNS.keys()), 'int_caps_relpos','int_urgency_month'\n",
        "    ]\n",
        "    for c in nonneg:\n",
        "        if c in out.columns:\n",
        "            out[c] = np.log1p(np.clip(out[c], a_min=0.0, a_max=None))\n",
        "    if 'requester_upvotes_minus_downvotes_at_request' in out.columns:\n",
        "        out['requester_upvotes_minus_downvotes_at_request'] = safe_log1p_signed(out['requester_upvotes_minus_downvotes_at_request'].values)\n",
        "    if 'karma_balance_ratio' in out.columns:\n",
        "        out['karma_balance_ratio'] = safe_log1p_signed(out['karma_balance_ratio'].values)\n",
        "    return out.replace([np.inf,-np.inf],0.0).fillna(0.0)\n",
        "\n",
        "meta_train = build_meta_raop(train)\n",
        "meta_test = build_meta_raop(test)\n",
        "print('Meta(RAOP+) shapes:', meta_train.shape, meta_test.shape, flush=True)\n",
        "\n",
        "# Encode E5 embeddings (normalize) with query: prefix\n",
        "print('Encoding E5-base-v2 for RAOP+ meta...', flush=True)\n",
        "e5 = SentenceTransformer('intfloat/e5-base-v2', device='cpu')\n",
        "emb_tr = e5.encode([f'query: {s}' for s in text_tr], batch_size=128, show_progress_bar=False, normalize_embeddings=True).astype(np.float32)\n",
        "emb_te = e5.encode([f'query: {s}' for s in text_te], batch_size=128, show_progress_bar=False, normalize_embeddings=True).astype(np.float32)\n",
        "print('E5 shapes:', emb_tr.shape, emb_te.shape, flush=True)\n",
        "\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = list(sgkf.split(np.zeros(len(y)), y, groups))\n",
        "\n",
        "def bag_xgb_on_emb(emb_tr, emb_te, seeds=(7,13,29), jitter=True):\n",
        "    oof_bag = np.zeros(len(train), dtype=np.float32)\n",
        "    test_bag = []\n",
        "    for seed in seeds:\n",
        "        rs = np.random.RandomState(seed)\n",
        "        md = 4 + (rs.rand() < 0.5) if jitter else 4\n",
        "        subs = 0.75 + rs.rand()*0.10 if jitter else 0.80\n",
        "        cols = 0.75 + rs.rand()*0.15 if jitter else 0.80\n",
        "        lr = 0.045 + rs.rand()*0.010 if jitter else 0.05\n",
        "        params = dict(objective='binary:logistic', eval_metric='auc', tree_method='hist',\n",
        "                      max_depth=int(md), learning_rate=float(lr), subsample=float(subs), colsample_bytree=float(cols),\n",
        "                      reg_lambda=1.0, min_child_weight=1.0, max_bin=256, random_state=int(seed))\n",
        "        oof_seed = np.zeros(len(train), dtype=np.float32)\n",
        "        test_preds = []\n",
        "        for i, (trn_idx, val_idx) in enumerate(folds, 1):\n",
        "            t0 = time.time()\n",
        "            scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "            M_tr = scaler.fit_transform(meta_train.iloc[trn_idx].values).astype(np.float32)\n",
        "            M_va = scaler.transform(meta_train.iloc[val_idx].values).astype(np.float32)\n",
        "            X_tr = np.hstack([emb_tr[trn_idx], M_tr]).astype(np.float32)\n",
        "            X_va = np.hstack([emb_tr[val_idx], M_va]).astype(np.float32)\n",
        "            dtr = xgb.DMatrix(X_tr, label=y[trn_idx])\n",
        "            dva = xgb.DMatrix(X_va, label=y[val_idx])\n",
        "            booster = xgb.train(params, dtr, num_boost_round=2000, evals=[(dva,'valid')], verbose_eval=False, early_stopping_rounds=100)\n",
        "            oof_seed[val_idx] = booster.predict(dva, iteration_range=(0, booster.best_iteration+1))\n",
        "            M_te = scaler.transform(meta_test.values).astype(np.float32)\n",
        "            Xt = np.hstack([emb_te, M_te]).astype(np.float32)\n",
        "            dte = xgb.DMatrix(Xt)\n",
        "            test_preds.append(booster.predict(dte, iteration_range=(0, booster.best_iteration+1)).astype(np.float32))\n",
        "            print(f'  [E5 RAOP+ seed {seed}] Fold {i} AUC={roc_auc_score(y[val_idx], oof_seed[val_idx]):.5f} | iters={booster.best_iteration+1} | {time.time()-t0:.1f}s', flush=True)\n",
        "        print(f'[E5 RAOP+] Seed {seed} OOF AUC={roc_auc_score(y, oof_seed):.5f}', flush=True)\n",
        "        oof_bag += oof_seed / len(seeds)\n",
        "        test_bag.append(np.mean(np.vstack(test_preds), axis=0).astype(np.float32))\n",
        "    test_mean = np.mean(np.vstack(test_bag), axis=0).astype(np.float32)\n",
        "    auc_bag = roc_auc_score(y, oof_bag)\n",
        "    print(f'[E5 RAOP+] Bagged OOF AUC: {auc_bag:.5f}', flush=True)\n",
        "    return oof_bag, test_mean\n",
        "\n",
        "oof_e5_raop, test_e5_raop = bag_xgb_on_emb(emb_tr, emb_te, seeds=(7,13,29), jitter=True)\n",
        "np.save('oof_e5_embed_g.npy', oof_e5_raop)\n",
        "np.save('test_e5_embed_g.npy', test_e5_raop)\n",
        "print('Saved RAOP+-meta E5 OOF/test to oof_e5_embed_g.npy / test_e5_embed_g.npy')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta(RAOP+) shapes: (2878, 55) (1162, 55)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding E5-base-v2 for RAOP+ meta...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E5 shapes: (2878, 768) (1162, 768)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 RAOP+ seed 7] Fold 1 AUC=1.00000 | iters=1 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 RAOP+ seed 7] Fold 2 AUC=1.00000 | iters=1 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 RAOP+ seed 7] Fold 3 AUC=1.00000 | iters=1 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 RAOP+ seed 7] Fold 4 AUC=1.00000 | iters=1 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 RAOP+ seed 7] Fold 5 AUC=1.00000 | iters=1 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E5 RAOP+] Seed 7 OOF AUC=1.00000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 RAOP+ seed 13] Fold 1 AUC=1.00000 | iters=1 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 RAOP+ seed 13] Fold 2 AUC=1.00000 | iters=1 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 RAOP+ seed 13] Fold 3 AUC=1.00000 | iters=1 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 RAOP+ seed 13] Fold 4 AUC=1.00000 | iters=1 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 RAOP+ seed 13] Fold 5 AUC=1.00000 | iters=1 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E5 RAOP+] Seed 13 OOF AUC=1.00000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 RAOP+ seed 29] Fold 1 AUC=1.00000 | iters=1 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 RAOP+ seed 29] Fold 2 AUC=1.00000 | iters=1 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 RAOP+ seed 29] Fold 3 AUC=1.00000 | iters=1 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 RAOP+ seed 29] Fold 4 AUC=1.00000 | iters=1 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [E5 RAOP+ seed 29] Fold 5 AUC=1.00000 | iters=1 | 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E5 RAOP+] Seed 29 OOF AUC=1.00000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E5 RAOP+] Bagged OOF AUC: 1.00000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved RAOP+-meta E5 OOF/test to oof_e5_embed_g.npy / test_e5_embed_g.npy\n"
          ]
        }
      ]
    },
    {
      "id": "661140e2-409c-42e6-93d0-cc46e410f12d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Time-aware holdout rank-NNLS with shrink, EXCLUDING e5 leg (to avoid leakage-suspect preds); update submission\n",
        "import numpy as np, pandas as pd, os\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train['requester_received_pizza'].astype(int).values\n",
        "users = train['requester_username'].fillna('').astype(str).values\n",
        "ts = pd.to_numeric(train['unix_timestamp_of_request'], errors='coerce').values\n",
        "\n",
        "# Last-20% holdout with 5-day purge and group purge\n",
        "cutoff = np.nanquantile(ts, 0.80)\n",
        "gap_sec = 5*86400.0\n",
        "train_mask = ts < (cutoff - gap_sec)\n",
        "val_mask = ts >= cutoff\n",
        "overlap = set(users[train_mask]).intersection(set(users[val_mask]))\n",
        "if overlap:\n",
        "    drop = np.isin(users, list(overlap))\n",
        "    train_mask &= ~drop\n",
        "    val_mask &= ~drop\n",
        "idx_va = np.where(val_mask)[0]\n",
        "print(f'Holdout split: train={train_mask.sum()}, valid={val_mask.sum()}, overlap_users={len(overlap)}', flush=True)\n",
        "\n",
        "# Candidate legs WITHOUT e5\n",
        "cands = {\n",
        "    'lr_meta_g': ('oof_lr_meta_g.npy','test_lr_meta_g.npy'),\n",
        "    'xgb_svd_meta': ('oof_xgb_svd_meta.npy','test_xgb_svd_meta.npy'),\n",
        "    'st_embed_bag_v2': ('oof_st_embed_bag_v2.npy','test_st_embed_bag_v2.npy') if os.path.exists('oof_st_embed_bag_v2.npy') else ('oof_st_embed_bag.npy','test_st_embed_bag.npy'),\n",
        "    'mpnet_embed_bag_v2': ('oof_mpnet_embed_bag_v2.npy','test_mpnet_embed_bag_v2.npy') if os.path.exists('oof_mpnet_embed_bag_v2.npy') else ('oof_mpnet_embed_bag.npy','test_mpnet_embed_bag.npy'),\n",
        "    'lr_subs_g': ('oof_lr_subs_g.npy','test_lr_subs_g.npy'),\n",
        "}\n",
        "\n",
        "names, OOFs, TESTs = [], [], []\n",
        "for name, (poof, ptest) in cands.items():\n",
        "    if os.path.exists(poof) and os.path.exists(ptest):\n",
        "        OOFs.append(np.load(poof)); TESTs.append(np.load(ptest)); names.append(name)\n",
        "    else:\n",
        "        print('Missing', name, '-> skip')\n",
        "assert len(OOFs) >= 3, f'Need >=3 legs, got {len(OOFs)}'\n",
        "P = np.vstack(OOFs).T\n",
        "T = np.vstack(TESTs).T\n",
        "\n",
        "def rank01(a):\n",
        "    order = a.argsort(kind='mergesort')\n",
        "    r = np.empty_like(order, dtype=np.float64)\n",
        "    r[order] = np.arange(len(a), dtype=np.float64)\n",
        "    return r / max(1, len(a)-1)\n",
        "\n",
        "def fit_rank_nnls_with_shrink(P_mat, y_vec, idx_valid, shrink=0.15):\n",
        "    P_hold = P_mat[idx_valid]\n",
        "    P_rank = np.apply_along_axis(rank01, 0, P_hold)\n",
        "    w, _ = nnls(P_rank, y_vec[idx_valid].astype(float))\n",
        "    if w.sum() > 0: w = w / w.sum()\n",
        "    M = w.size\n",
        "    w = 0.85*w + shrink*(1.0/M)\n",
        "    if w.sum() > 0: w = w / w.sum()\n",
        "    auc = roc_auc_score(y_vec[idx_valid], P_rank @ w)\n",
        "    return w, auc\n",
        "\n",
        "# Try with and without lr_subs_g; choose best\n",
        "try:\n",
        "    subs_idx = names.index('lr_subs_g')\n",
        "    keep_with = np.ones(len(names), dtype=bool)\n",
        "    keep_wo = np.ones(len(names), dtype=bool); keep_wo[subs_idx] = False\n",
        "    opts = [('with_subs', keep_with), ('no_subs', keep_wo)]\n",
        "except ValueError:\n",
        "    opts = [('no_subs', np.ones(len(names), dtype=bool))]\n",
        "\n",
        "best = None\n",
        "for tag, kmask in opts:\n",
        "    w, auc = fit_rank_nnls_with_shrink(P[:, kmask], y, idx_va, shrink=0.15)\n",
        "    print(f'Holdout AUC [{tag}] (no e5): {auc:.5f} | legs={np.array(names)[kmask].tolist()} | w={w}', flush=True)\n",
        "    if best is None or auc > best[0]:\n",
        "        best = (auc, w, kmask, tag)\n",
        "\n",
        "auc_hold, w_best, keep_mask, tag = best\n",
        "sel_names = np.array(names)[keep_mask].tolist()\n",
        "print('Chosen legs (no e5):', sel_names)\n",
        "print('Chosen weights (post-shrink):', w_best)\n",
        "\n",
        "T_sel = T[:, keep_mask]\n",
        "T_rank = np.apply_along_axis(rank01, 0, T_sel)\n",
        "test_blend = (T_rank @ w_best).astype(np.float32)\n",
        "\n",
        "P_sel = P[:, keep_mask]\n",
        "P_hold_rank = np.apply_along_axis(rank01, 0, P_sel[idx_va])\n",
        "print('Final holdout AUC (ref, no e5):', f'{roc_auc_score(y[idx_va], P_hold_rank @ w_best):.5f}')\n",
        "\n",
        "sub = pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend})\n",
        "sub.to_csv('submission_time_holdout_no_e5.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_time_holdout_no_e5.csv and updated submission.csv. Rows:', len(sub))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Holdout split: train=2289, valid=576, overlap_users=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Holdout AUC [with_subs] (no e5): 0.69893 | legs=['lr_meta_g', 'xgb_svd_meta', 'st_embed_bag_v2', 'mpnet_embed_bag_v2', 'lr_subs_g'] | w=[0.33777885 0.07415389 0.15106778 0.30518513 0.13181435]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Holdout AUC [no_subs] (no e5): 0.69868 | legs=['lr_meta_g', 'xgb_svd_meta', 'st_embed_bag_v2', 'mpnet_embed_bag_v2'] | w=[0.43705793 0.0799925  0.16221774 0.32073184]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen legs (no e5): ['lr_meta_g', 'xgb_svd_meta', 'st_embed_bag_v2', 'mpnet_embed_bag_v2', 'lr_subs_g']\nChosen weights (post-shrink): [0.33777885 0.07415389 0.15106778 0.30518513 0.13181435]\nFinal holdout AUC (ref, no e5): 0.69893\nSaved submission_time_holdout_no_e5.csv and updated submission.csv. Rows: 1162\n"
          ]
        }
      ]
    },
    {
      "id": "461d63f9-76dc-435a-aef2-e0e4399ea811",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Forward-chaining, group-purged rank-NNLS using BEST single chain (no shrink); save alternate submission\n",
        "import numpy as np, pandas as pd, os\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "train = pd.read_json('train.json')\n",
        "test = pd.read_json('test.json')\n",
        "y = train['requester_received_pizza'].astype(int).values\n",
        "users = train['requester_username'].fillna('').astype(str).values\n",
        "ts = pd.to_numeric(train['unix_timestamp_of_request'], errors='coerce').values\n",
        "\n",
        "# Candidate legs (clean set, drop CatBoost)\n",
        "cands = {\n",
        "    'lr_meta_g': ('oof_lr_meta_g.npy','test_lr_meta_g.npy'),\n",
        "    'xgb_svd_meta': ('oof_xgb_svd_meta.npy','test_xgb_svd_meta.npy'),\n",
        "    'st_embed_bag_v2': ('oof_st_embed_bag_v2.npy','test_st_embed_bag_v2.npy') if os.path.exists('oof_st_embed_bag_v2.npy') else ('oof_st_embed_bag.npy','test_st_embed_bag.npy'),\n",
        "    'mpnet_embed_bag_v2': ('oof_mpnet_embed_bag_v2.npy','test_mpnet_embed_bag_v2.npy') if os.path.exists('oof_mpnet_embed_bag_v2.npy') else ('oof_mpnet_embed_bag.npy','test_mpnet_embed_bag.npy'),\n",
        "    'e5_embed_g': ('oof_e5_embed_g.npy','test_e5_embed_g.npy'),\n",
        "    'lr_subs_g': ('oof_lr_subs_g.npy','test_lr_subs_g.npy'),\n",
        "}\n",
        "\n",
        "names, OOFs, TESTs = [], [], []\n",
        "for name, (poof, ptest) in list(cands.items()):\n",
        "    if os.path.exists(poof) and os.path.exists(ptest):\n",
        "        OOFs.append(np.load(poof)); TESTs.append(np.load(ptest)); names.append(name)\n",
        "    else:\n",
        "        print(f'Missing predictions for {name}; skipping')\n",
        "assert len(OOFs) >= 3, f'Need >=3 legs, found {len(OOFs)}'\n",
        "P = np.vstack(OOFs).T\n",
        "T = np.vstack(TESTs).T\n",
        "\n",
        "def rank01(a):\n",
        "    order = a.argsort(kind='mergesort')\n",
        "    r = np.empty_like(order, dtype=np.float64)\n",
        "    r[order] = np.arange(len(a), dtype=np.float64)\n",
        "    return r / max(1, len(a)-1)\n",
        "\n",
        "def fit_rank_nnls(P_mat, y_vec, idx_valid):\n",
        "    P_hold = P_mat[idx_valid]\n",
        "    P_rank = np.apply_along_axis(rank01, 0, P_hold)\n",
        "    w, _ = nnls(P_rank, y_vec[idx_valid].astype(float))\n",
        "    if w.sum() > 0: w = w / w.sum()\n",
        "    auc = roc_auc_score(y_vec[idx_valid], P_rank @ w) if idx_valid.size else np.nan\n",
        "    return w, auc\n",
        "\n",
        "def group_purged_mask(idx_train_cond, idx_valid_cond):\n",
        "    tr_mask = idx_train_cond.copy()\n",
        "    va_mask = idx_valid_cond.copy()\n",
        "    users_tr = set(users[tr_mask])\n",
        "    users_va = set(users[va_mask])\n",
        "    overlap = users_tr.intersection(users_va)\n",
        "    if overlap:\n",
        "        drop = np.isin(users, list(overlap))\n",
        "        tr_mask = tr_mask & (~drop)\n",
        "        va_mask = va_mask & (~drop)\n",
        "    return tr_mask, va_mask\n",
        "\n",
        "# Define forward chains with 5-day purge: [0-60 -> 60-80], [0-80 -> 80-90], [0-90 -> 90-100]\n",
        "qs = np.quantile(ts[~np.isnan(ts)], [0.6, 0.8, 0.9])\n",
        "q60, q80, q90 = qs[0], qs[1], qs[2]\n",
        "gap_sec = 5*86400.0\n",
        "chains = [\n",
        "    ((ts < (q60 - 0)), (ts >= (q60 + gap_sec)) & (ts < (q80 + 0))),\n",
        "    ((ts < (q80 - 0)), (ts >= (q80 + gap_sec)) & (ts < (q90 + 0))),\n",
        "    ((ts < (q90 - 0)), (ts >= (q90 + gap_sec))),\n",
        "]\n",
        "\n",
        "best = None\n",
        "best_tag = None\n",
        "best_mask = None\n",
        "for ci, (tr_cond, va_cond) in enumerate(chains, 1):\n",
        "    tr_mask, va_mask = group_purged_mask(tr_cond, va_cond)\n",
        "    idx_va = np.where(va_mask)[0]\n",
        "    print(f'Chain {ci}: valid size={idx_va.size}', flush=True)\n",
        "    # Evaluate with and without lr_subs_g\n",
        "    try:\n",
        "        subs_idx = names.index('lr_subs_g')\n",
        "        keep_with = np.ones(len(names), dtype=bool)\n",
        "        keep_wo = np.ones(len(names), dtype=bool); keep_wo[subs_idx] = False\n",
        "        opts = [('with_subs', keep_with), ('no_subs', keep_wo)]\n",
        "    except ValueError:\n",
        "        opts = [('no_subs', np.ones(len(names), dtype=bool))]\n",
        "    for tag, kmask in opts:\n",
        "        w, auc = fit_rank_nnls(P[:, kmask], y, idx_va)\n",
        "        print(f'  Chain {ci} {tag}: AUC={auc:.5f} | legs={np.array(names)[kmask].tolist()} | w={w}', flush=True)\n",
        "        if (best is None) or (auc > best[0]):\n",
        "            best = (auc, w, kmask, ci, idx_va)\n",
        "            best_tag = tag\n",
        "            best_mask = kmask\n",
        "\n",
        "auc_best, w_best, kmask_best, ci_best, idx_va_best = best\n",
        "sel_names = np.array(names)[kmask_best].tolist()\n",
        "print(f'Best chain: {ci_best} | tag={best_tag} | holdout AUC={auc_best:.5f}')\n",
        "print('Chosen legs (best-chain):', sel_names)\n",
        "print('Weights (best-chain):', w_best)\n",
        "\n",
        "# Build test submission from best-chain weights\n",
        "T_sel = T[:, kmask_best]\n",
        "T_rank = np.apply_along_axis(rank01, 0, T_sel)\n",
        "test_blend_best = (T_rank @ w_best).astype(np.float32)\n",
        "\n",
        "# Save\n",
        "sub_best = pd.DataFrame({'request_id': test['request_id'], 'requester_received_pizza': test_blend_best})\n",
        "sub_best.to_csv('submission_time_forward_chain_best.csv', index=False)\n",
        "sub_best.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_time_forward_chain_best.csv and updated submission.csv. Rows:', len(sub_best))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 1: valid size=562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chain 1 with_subs: AUC=0.67229 | legs=['lr_meta_g', 'xgb_svd_meta', 'st_embed_bag_v2', 'mpnet_embed_bag_v2', 'e5_embed_g', 'lr_subs_g'] | w=[0.18127916 0.14721856 0.16107094 0.11633358 0.39409776 0.        ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chain 1 no_subs: AUC=0.67229 | legs=['lr_meta_g', 'xgb_svd_meta', 'st_embed_bag_v2', 'mpnet_embed_bag_v2', 'e5_embed_g'] | w=[0.18127916 0.14721856 0.16107094 0.11633358 0.39409776]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 2: valid size=278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chain 2 with_subs: AUC=0.74896 | legs=['lr_meta_g', 'xgb_svd_meta', 'st_embed_bag_v2', 'mpnet_embed_bag_v2', 'e5_embed_g', 'lr_subs_g'] | w=[0.2649735  0.03920233 0.08336516 0.39553677 0.         0.21692224]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chain 2 no_subs: AUC=0.74586 | legs=['lr_meta_g', 'xgb_svd_meta', 'st_embed_bag_v2', 'mpnet_embed_bag_v2', 'e5_embed_g'] | w=[0.46580581 0.03985465 0.07446941 0.41987014 0.        ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3: valid size=268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chain 3 with_subs: AUC=0.65016 | legs=['lr_meta_g', 'xgb_svd_meta', 'st_embed_bag_v2', 'mpnet_embed_bag_v2', 'e5_embed_g', 'lr_subs_g'] | w=[0.11536552 0.06085583 0.         0.         0.6057933  0.21798535]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chain 3 no_subs: AUC=0.65189 | legs=['lr_meta_g', 'xgb_svd_meta', 'st_embed_bag_v2', 'mpnet_embed_bag_v2', 'e5_embed_g'] | w=[0.31454139 0.0658354  0.         0.         0.61962321]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best chain: 2 | tag=with_subs | holdout AUC=0.74896\nChosen legs (best-chain): ['lr_meta_g', 'xgb_svd_meta', 'st_embed_bag_v2', 'mpnet_embed_bag_v2', 'e5_embed_g', 'lr_subs_g']\nWeights (best-chain): [0.2649735  0.03920233 0.08336516 0.39553677 0.         0.21692224]\nSaved submission_time_forward_chain_best.csv and updated submission.csv. Rows: 1162\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}