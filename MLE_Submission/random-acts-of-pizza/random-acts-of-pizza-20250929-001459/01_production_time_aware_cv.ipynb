{
  "cells": [
    {
      "id": "5658f95d-3938-4b32-b209-8e57c82b7390",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Production plan: Time-aware, group-purged CV refit\n",
        "\n",
        "Objective: Medal in RAOP by closing OOF-to-LB gap via robust time-aware training and simple, diverse ensembling.\n",
        "\n",
        "Validation protocol:\n",
        "- Forward-chaining, time-ordered, group-purged CV (group=requester_username).\n",
        "- 4 chains by time after sorting; add a 3\u20135 day time gap before each validation window; no label stratification.\n",
        "  - C1: train 0\u201360% \u2192 val 60\u201375%\n",
        "  - C2: train 0\u201375% \u2192 val 75\u201387%\n",
        "  - C3: train 0\u201387% \u2192 val 87\u201394%\n",
        "  - C4: train 0\u201394% \u2192 val 94\u2013100%\n",
        "- Enforce no requester overlap between train and val per chain; ensure \u226550\u201370 positives per val window.\n",
        "- Save fold indices and reuse across legs; deterministic seeds.\n",
        "\n",
        "Data and features (strict leakage discipline):\n",
        "- Text legs:\n",
        "  1) Sentence-transformer embeddings (all-MiniLM-L6-v2, multi-qa-mpnet-base-dot-v1, intfloat/e5-base-v2) \u2192 XGBoost binary:logistic.\n",
        "  2) TF-IDF (word+char) on title+body (+ optional subreddit TF-IDF) \u2192 LogisticRegression with C tuned on CV. Fit vectorizer within each train fold only; up-weight title if beneficial.\n",
        "- Meta features (fold-safe only; no global ranks/relative_position; no future info):\n",
        "  \u2022 lengths: title/body chars/words, unique word ratio; punctuation/!?/ALLCAPS rates; digit/currency/url flags; has_url, has_edit; title_to_body_len_ratio\n",
        "  \u2022 calendar: month, weekday, quarter per-row; optional hour\n",
        "  \u2022 user-safe at request time (computed per fold using train-only history):\n",
        "    - days_since_account_creation = req_ts \u2212 account_creation_ts\n",
        "    - raop_comment_ratio = requester_comments_in_raop_at_request / (requester_comments_at_request + 1)\n",
        "    - raop_post_ratio = requester_posts_in_raop_at_request / (requester_posts_at_request + 1)\n",
        "    - user_has_flair (binary), flair_len_chars\n",
        "  \u2022 If using \u201cdays since start,\u201d compute relative to the fold\u2019s train min timestamp.\n",
        "\n",
        "Modeling details:\n",
        "- XGBoost (embeddings legs): tree_method=gpu_hist, objective=binary:logistic, eval_metric=auc,\n",
        "  max_depth=5 (4\u20136 ok), eta=0.05 (0.05\u20130.08), subsample=0.8, colsample_bytree=0.8\u20130.9,\n",
        "  min_child_weight=3\u20135, reg_lambda=2\u20134 (reg_alpha 0\u20130.5 optional), n_estimators=2000 with early_stopping_rounds=50\u2013100;\n",
        "  optionally set scale_pos_weight=neg/pos per chain.\n",
        "- Logistic Regression (TF-IDF): solver=saga, penalty=L2, C\u2208[0.5,1,2,4], try class_weight='balanced'.\n",
        "- Cache per-leg OOF and test preds to .npy; cache feature matrices/embeddings once and slice per fold.\n",
        "\n",
        "Blending (robust, shift-aware):\n",
        "- Rank space blending. Learn weights separately on last 2\u20133 chains; L2-normalize each weight vector and average them.\n",
        "- Apply 15\u201320% shrink toward uniform after averaging; greedy prune legs with ~0 or harmful weights.\n",
        "- Fallbacks: uniform rank-average of retained legs; backup = average of top-2 legs by forward-chain AUC.\n",
        "- Optional light calibration: preds = 0.9*preds + 0.1*rank(preds); clip final preds to [0.01, 0.99].\n",
        "\n",
        "Diagnostics:\n",
        "- AV only for analysis; report AV AUC with/without time; if extreme shift persists, increase time gap to 5\u20137 days and increase blend shrink.\n",
        "- Log per-chain AUC, elapsed time, and ensure zero requester overlap; print date ranges per split.\n",
        "- Inspect XGB importances and LR coefficients for sanity.\n",
        "\n",
        "Execution plan (milestones):\n",
        "1) Env check (GPU, versions) and torch stack guard (no installation conflicts).\n",
        "2) Load data; parse time; build groups; sort by time; quick sanity checks.\n",
        "3) Implement fold builder: purged forward-chaining (4 chains) with 3\u20135 day gap; persist folds and validate positive counts.\n",
        "4) Leg A: TF-IDF + LR (title+body+subs) under forward-chaining; cache OOF/test.\n",
        "5) Legs B\u2013D: ST embeddings (MiniLM, MPNet, E5) + XGB with GPU; 3-seed bag per leg; cache OOF/test.\n",
        "6) Optional (time permitting): SVD+meta XGB for diversity or swap MPNet\u2192bge-small-en-v1.5.\n",
        "7) Blend via multi-chain weight learning with shrink and pruning; produce primary and fallback submissions.\n",
        "8) Sanity-check submission distribution; clip and save submission.csv.\n",
        "\n",
        "Expert review checkpoints:\n",
        "- After folds implementation + smoke metrics.\n",
        "- After first leg OOF/test cached.\n",
        "- After all legs cached, before blending.\n",
        "- After blend results, before submission.\n",
        "\n",
        "Notes:\n",
        "- Refit every leg under this exact forward-chaining, group-purged CV; do not reuse non-time-aware OOF.\n",
        "- Always print progress per chain and flush; keep notebook small; cache artifacts."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "a90a82fd-ff9b-4a82-8c80-25c5e3c206c9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# One-time setup: Create isolated .venv with cu121 torch and register Jupyter kernel\n",
        "import sys, subprocess, os\n",
        "\n",
        "def sh(cmd):\n",
        "    print('$', cmd, flush=True)\n",
        "    subprocess.run(cmd, shell=True, check=True, executable='/bin/bash')\n",
        "\n",
        "# Create venv\n",
        "sh(f\"{sys.executable} -m venv .venv\")\n",
        "act = \"source .venv/bin/activate &&\"\n",
        "\n",
        "# Upgrade basics + ipykernel\n",
        "sh(f\"{act} python -m pip install --upgrade pip wheel setuptools ipykernel\")\n",
        "\n",
        "# Torch cu121 stack\n",
        "sh(f\"{act} python -m pip install --no-cache-dir --index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://pypi.org/simple torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1\")\n",
        "\n",
        "# NLP deps\n",
        "sh(f\"{act} python -m pip install --no-cache-dir transformers==4.44.2 sentence-transformers==3.0.1 accelerate==0.34.2 sentencepiece\")\n",
        "\n",
        "# Register kernel\n",
        "sh(f\"{act} python -m ipykernel install --user --name raop-venv-cu121 --display-name 'RAOP Venv (cu121)'\")\n",
        "\n",
        "print(\">>> Switch kernel to 'RAOP Venv (cu121)', restart, then verify:\", flush=True)\n",
        "print(\"import torch; print(torch.__version__, torch.version.cuda, torch.cuda.is_available())\", flush=True)\n",
        "print(\"from sentence_transformers import SentenceTransformer; print('ST OK')\", flush=True)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$ /usr/bin/python3.11 -m venv .venv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$ source .venv/bin/activate && python -m pip install --upgrade pip wheel setuptools ipykernel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in ./.venv/lib/python3.11/site-packages (22.0.2)\nCollecting pip\n  Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.8/1.8 MB 52.9 MB/s eta 0:00:00\nCollecting wheel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 72.5/72.5 KB 399.4 MB/s eta 0:00:00\nRequirement already satisfied: setuptools in ./.venv/lib/python3.11/site-packages (59.6.0)\nCollecting setuptools\n  Downloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.2/1.2 MB 264.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipykernel\n  Downloading ipykernel-6.30.1-py3-none-any.whl (117 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 117.5/117.5 KB 485.9 MB/s eta 0:00:00\nCollecting debugpy>=1.6.5\n  Downloading debugpy-1.8.17-cp311-cp311-manylinux_2_34_x86_64.whl (3.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.2/3.2 MB 243.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting comm>=0.1.1\n  Downloading comm-0.2.3-py3-none-any.whl (7.3 kB)\nCollecting jupyter-client>=8.0.0\n  Downloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 106.1/106.1 KB 466.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyzmq>=25\n  Downloading pyzmq-27.1.0-cp311-cp311-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (857 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 857.0/857.0 KB 389.5 MB/s eta 0:00:00\nCollecting psutil>=5.7\n  Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 291.2/291.2 KB 507.2 MB/s eta 0:00:00\nCollecting matplotlib-inline>=0.1\n  Downloading matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)\nCollecting tornado>=6.2\n  Downloading tornado-6.5.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 443.9/443.9 KB 487.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting packaging>=22\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 409.7 MB/s eta 0:00:00\nCollecting traitlets>=5.4.0\n  Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 85.4/85.4 KB 456.6 MB/s eta 0:00:00\nCollecting jupyter-core!=5.0.*,>=4.12\n  Downloading jupyter_core-5.8.1-py3-none-any.whl (28 kB)\nCollecting nest-asyncio>=1.4\n  Downloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\nCollecting ipython>=7.23.1\n  Downloading ipython-9.5.0-py3-none-any.whl (612 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 612.4/612.4 KB 297.0 MB/s eta 0:00:00\nCollecting typing_extensions>=4.6\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 328.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting decorator\n  Downloading decorator-5.2.1-py3-none-any.whl (9.2 kB)\nCollecting jedi>=0.16\n  Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.6/1.6 MB 532.5 MB/s eta 0:00:00\nCollecting prompt_toolkit<3.1.0,>=3.0.41\n  Downloading prompt_toolkit-3.0.52-py3-none-any.whl (391 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 391.4/391.4 KB 524.0 MB/s eta 0:00:00\nCollecting pexpect>4.3\n  Downloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 63.8/63.8 KB 443.2 MB/s eta 0:00:00\nCollecting ipython-pygments-lexers\n  Downloading ipython_pygments_lexers-1.1.1-py3-none-any.whl (8.1 kB)\nCollecting stack_data\n  Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)\nCollecting pygments>=2.4.0\n  Downloading pygments-2.19.2-py3-none-any.whl (1.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.2/1.2 MB 477.1 MB/s eta 0:00:00\nCollecting python-dateutil>=2.8.2\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 229.9/229.9 KB 482.7 MB/s eta 0:00:00\nCollecting platformdirs>=2.5\n  Downloading platformdirs-4.4.0-py3-none-any.whl (18 kB)\nCollecting parso<0.9.0,>=0.8.4\n  Downloading parso-0.8.5-py2.py3-none-any.whl (106 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 106.7/106.7 KB 445.1 MB/s eta 0:00:00\nCollecting ptyprocess>=0.5\n  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\nCollecting wcwidth\n  Downloading wcwidth-0.2.14-py2.py3-none-any.whl (37 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting six>=1.5\n  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\nCollecting asttokens>=2.1.0\n  Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)\nCollecting pure-eval\n  Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\nCollecting executing>=1.2.0\n  Downloading executing-2.2.1-py2.py3-none-any.whl (28 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: pure-eval, ptyprocess, wheel, wcwidth, typing_extensions, traitlets, tornado, six, setuptools, pyzmq, pygments, psutil, platformdirs, pip, pexpect, parso, packaging, nest-asyncio, executing, decorator, debugpy, comm, asttokens, stack_data, python-dateutil, prompt_toolkit, matplotlib-inline, jupyter-core, jedi, ipython-pygments-lexers, jupyter-client, ipython, ipykernel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Attempting uninstall: setuptools\n    Found existing installation: setuptools 59.6.0\n    Uninstalling setuptools-59.6.0:\n      Successfully uninstalled setuptools-59.6.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Attempting uninstall: pip\n    Found existing installation: pip 22.0.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Uninstalling pip-22.0.2:\n      Successfully uninstalled pip-22.0.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed asttokens-3.0.0 comm-0.2.3 debugpy-1.8.17 decorator-5.2.1 executing-2.2.1 ipykernel-6.30.1 ipython-9.5.0 ipython-pygments-lexers-1.1.1 jedi-0.19.2 jupyter-client-8.6.3 jupyter-core-5.8.1 matplotlib-inline-0.1.7 nest-asyncio-1.6.0 packaging-25.0 parso-0.8.5 pexpect-4.9.0 pip-25.2 platformdirs-4.4.0 prompt_toolkit-3.0.52 psutil-7.1.0 ptyprocess-0.7.0 pure-eval-0.2.3 pygments-2.19.2 python-dateutil-2.9.0.post0 pyzmq-27.1.0 setuptools-80.9.0 six-1.17.0 stack_data-0.6.3 tornado-6.5.2 traitlets-5.14.3 typing_extensions-4.15.0 wcwidth-0.2.14 wheel-0.45.1\n$ source .venv/bin/activate && python -m pip install --no-cache-dir --index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://pypi.org/simple torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121, https://pypi.org/simple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (799.0 MB)\n\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/799.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K     \u001b[91m\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m49.5/799.0 MB\u001b[0m \u001b[31m251.6 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K     \u001b[91m\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m102.2/799.0 MB\u001b[0m \u001b[31m256.8 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K     \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m154.4/799.0 MB\u001b[0m \u001b[31m257.8 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K     \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m204.5/799.0 MB\u001b[0m \u001b[31m255.4 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K     \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m256.6/799.0 MB\u001b[0m \u001b[31m256.1 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K     \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m309.3/799.0 MB\u001b[0m \u001b[31m258.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K     \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m364.1/799.0 MB\u001b[0m \u001b[31m260.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K     \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m418.1/799.0 MB\u001b[0m \u001b[31m261.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K     \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m469.0/799.0 MB\u001b[0m \u001b[31m262.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K     \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m522.7/799.0 MB\u001b[0m \u001b[31m264.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K     \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m575.1/799.0 MB\u001b[0m \u001b[31m263.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K     \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m623.9/799.0 MB\u001b[0m \u001b[31m258.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K     \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m672.1/799.0 MB\u001b[0m \u001b[31m253.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K     \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m723.3/799.0 MB\u001b[0m \u001b[31m254.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K     \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u001b[0m \u001b[32m773.3/799.0 MB\u001b[0m \u001b[31m249.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m799.0/799.0 MB\u001b[0m \u001b[31m249.2 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m\n\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.19.1\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/7.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m179.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/3.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m316.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting filelock (from torch==2.4.1)\n  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.11/site-packages (from torch==2.4.1) (4.15.0)\nCollecting sympy (from torch==2.4.1)\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting networkx (from torch==2.4.1)\n  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\nCollecting jinja2 (from torch==2.4.1)\n  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fsspec (from torch==2.4.1)\n  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.1)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.1)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.1)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.1)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.1)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.1)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.1)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.1)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.1)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton==3.0.0 (from torch==2.4.1)\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy (from torchvision==0.19.1)\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.19.1)\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1)\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.4.1)\n  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.4.1)\n  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\nDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/410.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m34.6/410.6 MB\u001b[0m \u001b[31m174.8 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m86.0/410.6 MB\u001b[0m \u001b[31m214.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m139.7/410.6 MB\u001b[0m \u001b[31m231.4 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m191.4/410.6 MB\u001b[0m \u001b[31m238.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m230.7/410.6 MB\u001b[0m \u001b[31m231.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m274.7/410.6 MB\u001b[0m \u001b[31m223.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m312.0/410.6 MB\u001b[0m \u001b[31m228.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m353.4/410.6 MB\u001b[0m \u001b[31m223.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u001b[0m \u001b[32m391.1/410.6 MB\u001b[0m \u001b[31m210.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m198.9 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/14.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m149.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/23.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m301.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/823.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m735.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/664.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m62.1/664.8 MB\u001b[0m \u001b[31m309.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m121.6/664.8 MB\u001b[0m \u001b[31m304.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m177.2/664.8 MB\u001b[0m \u001b[31m294.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m225.4/664.8 MB\u001b[0m \u001b[31m280.8 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m262.1/664.8 MB\u001b[0m \u001b[31m265.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m303.8/664.8 MB\u001b[0m \u001b[31m244.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m362.8/664.8 MB\u001b[0m \u001b[31m242.7 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m423.6/664.8 MB\u001b[0m \u001b[31m245.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m477.1/664.8 MB\u001b[0m \u001b[31m258.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m538.2/664.8 MB\u001b[0m \u001b[31m291.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m598.2/664.8 MB\u001b[0m \u001b[31m294.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u001b[0m \u001b[32m638.1/664.8 MB\u001b[0m \u001b[31m273.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m267.7 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/121.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m57.9/121.6 MB\u001b[0m \u001b[31m289.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u001b[0m \u001b[32m115.3/121.6 MB\u001b[0m \u001b[31m288.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m260.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/56.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u001b[0m \u001b[32m52.4/56.5 MB\u001b[0m \u001b[31m262.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m262.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/124.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m63.2/124.2 MB\u001b[0m \u001b[31m315.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m323.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/196.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m37.2/196.0 MB\u001b[0m \u001b[31m185.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m98.6/196.0 MB\u001b[0m \u001b[31m253.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m157.8/196.0 MB\u001b[0m \u001b[31m261.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m258.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/176.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m44.0/176.2 MB\u001b[0m \u001b[31m268.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m102.0/176.2 MB\u001b[0m \u001b[31m253.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u001b[0m \u001b[32m166.7/176.2 MB\u001b[0m \u001b[31m276.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m276.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\nDownloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/209.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m57.7/209.4 MB\u001b[0m \u001b[31m289.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m117.4/209.4 MB\u001b[0m \u001b[31m292.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m173.5/209.4 MB\u001b[0m \u001b[31m287.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m287.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/6.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m277.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading filelock-3.19.1-py3-none-any.whl (15 kB)\nDownloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\nDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\nDownloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\nDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m783.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/18.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m226.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/39.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m211.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/6.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m290.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/536.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m925.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: mpmath, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n\u001b[?25l\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 0/25\u001b[0m [mpmath]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/25\u001b[0m [sympy]\r\u001b[2K   \u001b[91m\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/25\u001b[0m [sympy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/25\u001b[0m [sympy]\r\u001b[2K   \u001b[91m\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/25\u001b[0m [sympy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/25\u001b[0m [sympy]\r\u001b[2K   \u001b[91m\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/25\u001b[0m [sympy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/25\u001b[0m [sympy]\r\u001b[2K   \u001b[91m\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/25\u001b[0m [sympy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/25\u001b[0m [sympy]\r\u001b[2K   \u001b[91m\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/25\u001b[0m [sympy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/25\u001b[0m [sympy]\r\u001b[2K   \u001b[91m\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/25\u001b[0m [sympy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/25\u001b[0m [sympy]\r\u001b[2K   \u001b[91m\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/25\u001b[0m [sympy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/25\u001b[0m [sympy]\r\u001b[2K   \u001b[91m\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/25\u001b[0m [sympy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/25\u001b[0m [sympy]\r\u001b[2K   \u001b[91m\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/25\u001b[0m [sympy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/25\u001b[0m [sympy]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 2/25\u001b[0m [pillow]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/25\u001b[0m [nvidia-nvjitlink-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/25\u001b[0m [nvidia-nvjitlink-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/25\u001b[0m [nvidia-nvjitlink-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 5/25\u001b[0m [nvidia-nccl-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 5/25\u001b[0m [nvidia-nccl-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 5/25\u001b[0m [nvidia-nccl-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 5/25\u001b[0m [nvidia-nccl-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 5/25\u001b[0m [nvidia-nccl-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 5/25\u001b[0m [nvidia-nccl-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 5/25\u001b[0m [nvidia-nccl-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 6/25\u001b[0m [nvidia-curand-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 6/25\u001b[0m [nvidia-curand-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 7/25\u001b[0m [nvidia-cufft-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 7/25\u001b[0m [nvidia-cufft-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 7/25\u001b[0m [nvidia-cufft-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 7/25\u001b[0m [nvidia-cufft-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 7/25\u001b[0m [nvidia-cufft-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 7/25\u001b[0m [nvidia-cufft-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 9/25\u001b[0m [nvidia-cuda-nvrtc-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 9/25\u001b[0m [nvidia-cuda-nvrtc-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m10/25\u001b[0m [nvidia-cuda-cupti-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11/25\u001b[0m [nvidia-cublas-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11/25\u001b[0m [nvidia-cublas-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11/25\u001b[0m [nvidia-cublas-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11/25\u001b[0m [nvidia-cublas-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11/25\u001b[0m [nvidia-cublas-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11/25\u001b[0m [nvidia-cublas-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11/25\u001b[0m [nvidia-cublas-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11/25\u001b[0m [nvidia-cublas-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11/25\u001b[0m [nvidia-cublas-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11/25\u001b[0m [nvidia-cublas-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11/25\u001b[0m [nvidia-cublas-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11/25\u001b[0m [nvidia-cublas-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11/25\u001b[0m [nvidia-cublas-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11/25\u001b[0m [nvidia-cublas-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11/25\u001b[0m [nvidia-cublas-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11/25\u001b[0m [nvidia-cublas-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11/25\u001b[0m [nvidia-cublas-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11/25\u001b[0m [nvidia-cublas-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11/25\u001b[0m [nvidia-cublas-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m12/25\u001b[0m [numpy]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m12/25\u001b[0m [numpy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m12/25\u001b[0m [numpy]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m12/25\u001b[0m [numpy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m12/25\u001b[0m [numpy]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m12/25\u001b[0m [numpy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m12/25\u001b[0m [numpy]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13/25\u001b[0m [networkx]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13/25\u001b[0m [networkx]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13/25\u001b[0m [networkx]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13/25\u001b[0m [networkx]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m15/25\u001b[0m [fsspec]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m17/25\u001b[0m [triton]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m17/25\u001b[0m [triton]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m17/25\u001b[0m [triton]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m17/25\u001b[0m [triton]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m17/25\u001b[0m [triton]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m17/25\u001b[0m [triton]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m17/25\u001b[0m [triton]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m17/25\u001b[0m [triton]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m17/25\u001b[0m [triton]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m17/25\u001b[0m [triton]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m17/25\u001b[0m [triton]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m17/25\u001b[0m [triton]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m18/25\u001b[0m [nvidia-cusparse-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m18/25\u001b[0m [nvidia-cusparse-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m18/25\u001b[0m [nvidia-cusparse-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m18/25\u001b[0m [nvidia-cusparse-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m18/25\u001b[0m [nvidia-cusparse-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m18/25\u001b[0m [nvidia-cusparse-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m18/25\u001b[0m [nvidia-cusparse-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m18/25\u001b[0m [nvidia-cusparse-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m18/25\u001b[0m [nvidia-cusparse-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m19/25\u001b[0m [nvidia-cudnn-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21/25\u001b[0m [nvidia-cusolver-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21/25\u001b[0m [nvidia-cusolver-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21/25\u001b[0m [nvidia-cusolver-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21/25\u001b[0m [nvidia-cusolver-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21/25\u001b[0m [nvidia-cusolver-cu12]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21/25\u001b[0m [nvidia-cusolver-cu12]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22/25\u001b[0m [torch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m23/25\u001b[0m [torchvision]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u001b[0m \u001b[32m24/25\u001b[0m [torchaudio]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m25/25\u001b[0m [torchaudio]\n\u001b[?25h\r\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pillow-11.3.0 sympy-1.14.0 torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121 triton-3.0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$ source .venv/bin/activate && python -m pip install --no-cache-dir transformers==4.44.2 sentence-transformers==3.0.1 accelerate==0.34.2 sentencepiece\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.44.2\n  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\nCollecting sentence-transformers==3.0.1\n  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\nCollecting accelerate==0.34.2\n  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\nCollecting sentencepiece\n  Downloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from transformers==4.44.2) (3.19.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.44.2)\n  Downloading huggingface_hub-0.35.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from transformers==4.44.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from transformers==4.44.2) (25.0)\nCollecting pyyaml>=5.1 (from transformers==4.44.2)\n  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting regex!=2019.12.17 (from transformers==4.44.2)\n  Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\nCollecting requests (from transformers==4.44.2)\n  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\nCollecting safetensors>=0.4.1 (from transformers==4.44.2)\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.44.2)\n  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting tqdm>=4.27 (from transformers==4.44.2)\n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\nRequirement already satisfied: torch>=1.11.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers==3.0.1) (2.4.1+cu121)\nCollecting scikit-learn (from sentence-transformers==3.0.1)\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\nCollecting scipy (from sentence-transformers==3.0.1)\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in ./.venv/lib/python3.11/site-packages (from sentence-transformers==3.0.1) (11.3.0)\nRequirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from accelerate==0.34.2) (7.1.0)\nRequirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (4.15.0)\nCollecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2)\n  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\nRequirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (1.14.0)\nRequirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (3.5)\nRequirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.1.105)\nRequirement already satisfied: triton==3.0.0 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (3.0.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers==3.0.1) (12.9.86)\nRequirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers==3.0.1) (3.0.3)\nCollecting charset_normalizer<4,>=2 (from requests->transformers==4.44.2)\n  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\nCollecting idna<4,>=2.5 (from requests->transformers==4.44.2)\n  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\nCollecting urllib3<3,>=1.21.1 (from requests->transformers==4.44.2)\n  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting certifi>=2017.4.17 (from requests->transformers==4.44.2)\n  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\nCollecting joblib>=1.2.0 (from scikit-learn->sentence-transformers==3.0.1)\n  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\nCollecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers==3.0.1)\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers==3.0.1) (1.3.0)\nDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/9.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m125.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\nDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\nDownloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/563.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m563.3/563.3 kB\u001b[0m \u001b[31m679.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m320.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/3.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m215.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m675.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/806.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m806.6/806.6 kB\u001b[0m \u001b[31m955.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/799.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m799.0/799.0 kB\u001b[0m \u001b[31m786.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\nDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\nDownloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\nDownloading idna-3.10-py3-none-any.whl (70 kB)\nDownloading urllib3-2.5.0-py3-none-any.whl (129 kB)\nDownloading certifi-2025.8.3-py3-none-any.whl (161 kB)\nDownloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/9.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m162.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\nDownloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/35.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u001b[0m \u001b[32m34.1/35.9 MB\u001b[0m \u001b[31m169.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m173.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: urllib3, tqdm, threadpoolctl, sentencepiece, scipy, safetensors, regex, pyyaml, joblib, idna, hf-xet, charset_normalizer, certifi, scikit-learn, requests, huggingface-hub, tokenizers, accelerate, transformers, sentence-transformers\n\u001b[?25l\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 0/20\u001b[0m [urllib3]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 0/20\u001b[0m [urllib3]\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 0/20\u001b[0m [urllib3]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 0/20\u001b[0m [urllib3]\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 0/20\u001b[0m [urllib3]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 0/20\u001b[0m [urllib3]\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 0/20\u001b[0m [urllib3]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 0/20\u001b[0m [urllib3]\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 0/20\u001b[0m [urllib3]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 0/20\u001b[0m [urllib3]\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 0/20\u001b[0m [urllib3]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 0/20\u001b[0m [urllib3]\r\u001b[2K   \u001b[91m\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/20\u001b[0m [tqdm]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/20\u001b[0m [tqdm]\r\u001b[2K   \u001b[91m\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/20\u001b[0m [tqdm]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/20\u001b[0m [tqdm]\r\u001b[2K   \u001b[91m\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/20\u001b[0m [tqdm]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/20\u001b[0m [tqdm]\r\u001b[2K   \u001b[91m\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/20\u001b[0m [tqdm]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/20\u001b[0m [tqdm]\r\u001b[2K   \u001b[91m\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/20\u001b[0m [tqdm]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/20\u001b[0m [tqdm]\r\u001b[2K   \u001b[91m\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/20\u001b[0m [tqdm]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/20\u001b[0m [tqdm]\r\u001b[2K   \u001b[91m\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/20\u001b[0m [tqdm]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/20\u001b[0m [tqdm]\r\u001b[2K   \u001b[91m\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/20\u001b[0m [tqdm]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/20\u001b[0m [tqdm]\r\u001b[2K   \u001b[91m\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/20\u001b[0m [tqdm]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/20\u001b[0m [tqdm]\r\u001b[2K   \u001b[91m\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/20\u001b[0m [tqdm]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 1/20\u001b[0m [tqdm]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 2/20\u001b[0m [threadpoolctl]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 4/20\u001b[0m [scipy]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 6/20\u001b[0m [regex]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m 8/20\u001b[0m [joblib]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11/20\u001b[0m [charset_normalizer]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13/20\u001b[0m [scikit-learn]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13/20\u001b[0m [scikit-learn]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13/20\u001b[0m [scikit-learn]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13/20\u001b[0m [scikit-learn]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13/20\u001b[0m [scikit-learn]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13/20\u001b[0m [scikit-learn]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13/20\u001b[0m [scikit-learn]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m15/20\u001b[0m [huggingface-hub]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m16/20\u001b[0m [tokenizers]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u001b[0m \u001b[32m18/20\u001b[0m [transformers]\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m20/20\u001b[0m [sentence-transformers]\n\u001b[?25h\r\u001b[1A\u001b[2KSuccessfully installed accelerate-0.34.2 certifi-2025.8.3 charset_normalizer-3.4.3 hf-xet-1.1.10 huggingface-hub-0.35.1 idna-3.10 joblib-1.5.2 pyyaml-6.0.3 regex-2025.9.18 requests-2.32.5 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 sentence-transformers-3.0.1 sentencepiece-0.2.1 threadpoolctl-3.6.0 tokenizers-0.19.1 tqdm-4.67.1 transformers-4.44.2 urllib3-2.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$ source .venv/bin/activate && python -m ipykernel install --user --name raop-venv-cu121 --display-name 'RAOP Venv (cu121)'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed kernelspec raop-venv-cu121 in /app/.local/share/jupyter/kernels/raop-venv-cu121\n>>> Switch kernel to 'RAOP Venv (cu121)', restart, then verify:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import torch; print(torch.__version__, torch.version.cuda, torch.cuda.is_available())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from sentence_transformers import SentenceTransformer; print('ST OK')\n"
          ]
        }
      ]
    },
    {
      "id": "d76b0757-1ff7-4963-a907-5dead883229e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Helper: Generate and cache E5 embeddings using the venv interpreter (works even if current kernel is base)\n",
        "import os, sys, subprocess, textwrap\n",
        "from pathlib import Path\n",
        "\n",
        "script_path = Path('e5_gen.py')\n",
        "script_code = textwrap.dedent('''\n",
        "import os, sys\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def load_json_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path, lines=True)\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return pd.read_json(path, lines=False)\n",
        "        except ValueError:\n",
        "            import json\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            if isinstance(data, dict) and 'data' in data:\n",
        "                data = data['data']\n",
        "            return pd.json_normalize(data)\n",
        "\n",
        "def build_text_cols(df):\n",
        "    title_col_candidates = ['request_title', 'title']\n",
        "    body_col_candidates = ['request_text', 'body', 'text']\n",
        "    def first_col(cols):\n",
        "        for c in cols:\n",
        "            if c in df.columns: return c\n",
        "        return None\n",
        "    tcol = first_col(title_col_candidates)\n",
        "    bcol = first_col(body_col_candidates)\n",
        "    t = df[tcol].fillna('') if tcol else pd.Series(['' for _ in range(len(df))])\n",
        "    b = df[bcol].fillna('') if bcol else pd.Series(['' for _ in range(len(df))])\n",
        "    return t.astype(str), b.astype(str)\n",
        "\n",
        "def main():\n",
        "    cache_dir = Path('emb_cache'); cache_dir.mkdir(exist_ok=True)\n",
        "    tr_path = cache_dir / 'emb_e5_train.npy'\n",
        "    te_path = cache_dir / 'emb_e5_test.npy'\n",
        "    if tr_path.exists() and te_path.exists():\n",
        "        arr_tr = np.load(tr_path); arr_te = np.load(te_path)\n",
        "        print('Embeddings already exist:', arr_tr.shape, arr_te.shape)\n",
        "        return\n",
        "    tr = load_json_df('train.json')\n",
        "    te = load_json_df('test.json')\n",
        "    time_col = 'unix_timestamp_of_request_utc' if 'unix_timestamp_of_request_utc' in tr.columns else tr.columns[0]\n",
        "    tr = tr.sort_values(time_col, kind='mergesort').reset_index(drop=True)\n",
        "    t_title, t_body = build_text_cols(tr)\n",
        "    te_title, te_body = build_text_cols(te)\n",
        "    tr_texts = ('passage: ' + (t_title + ' \\\\n ' + t_body)).tolist()\n",
        "    te_texts = ('passage: ' + (te_title + ' \\\\n ' + te_body)).tolist()\n",
        "    model_name = 'intfloat/e5-base-v2'\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print('Loading model:', model_name, 'on', device)\n",
        "    model = SentenceTransformer(model_name, device=device)\n",
        "    model.max_seq_length = 512\n",
        "    def embed(texts, batch_size=128):\n",
        "        return np.asarray(model.encode(texts, batch_size=batch_size, normalize_embeddings=True, show_progress_bar=True), dtype=np.float32)\n",
        "    print('Encoding train ...'); emb_tr = embed(tr_texts)\n",
        "    print('Encoding test ...'); emb_te = embed(te_texts)\n",
        "    np.save(tr_path, emb_tr); np.save(te_path, emb_te)\n",
        "    print('Saved:', tr_path, te_path, '| shapes:', emb_tr.shape, emb_te.shape)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "''')\n",
        "script_path.write_text(script_code)\n",
        "print('Wrote helper script:', script_path)\n",
        "\n",
        "# Execute with venv python to ensure proper torch/ST stack\n",
        "venv_py = Path('.venv/bin/python')\n",
        "assert venv_py.exists(), 'Venv python not found; run the venv setup cell first'\n",
        "\n",
        "# Ensure pandas/numpy are installed in the venv (required by e5_gen.py)\n",
        "print('Ensuring pandas/numpy in venv ...')\n",
        "subprocess.run([str(venv_py), '-m', 'pip', 'install', '--upgrade', 'pip', 'wheel', 'setuptools'], check=True)\n",
        "subprocess.run([str(venv_py), '-m', 'pip', 'install', 'pandas', 'numpy'], check=True)\n",
        "\n",
        "print('Running embeddings generation via', venv_py)\n",
        "proc = subprocess.run([str(venv_py), str(script_path)], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "print(proc.stdout)\n",
        "if proc.returncode != 0:\n",
        "    raise RuntimeError('Embedding generation failed; check logs above')\n",
        "print('E5 embeddings generation complete. You can now run Cell 10 to train XGB on cached embeddings.')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote helper script: e5_gen.py\nEnsuring pandas/numpy in venv ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in ./.venv/lib/python3.11/site-packages (25.2)\nRequirement already satisfied: wheel in ./.venv/lib/python3.11/site-packages (0.45.1)\nRequirement already satisfied: setuptools in ./.venv/lib/python3.11/site-packages (80.9.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas\n  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\nRequirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\nCollecting pytz>=2020.1 (from pandas)\n  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\nCollecting tzdata>=2022.7 (from pandas)\n  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nDownloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/12.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m252.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\nDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: pytz, tzdata, pandas\n\u001b[?25l\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/3\u001b[0m [pandas]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/3\u001b[0m [pandas]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/3\u001b[0m [pandas]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/3\u001b[0m [pandas]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/3\u001b[0m [pandas]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/3\u001b[0m [pandas]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/3\u001b[0m [pandas]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/3\u001b[0m [pandas]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/3\u001b[0m [pandas]\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3/3\u001b[0m [pandas]\n\u001b[?25h\r\u001b[1A\u001b[2KSuccessfully installed pandas-2.3.2 pytz-2025.2 tzdata-2025.2\nRunning embeddings generation via .venv/bin/python\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n\n0it [00:00, ?it/s]\n0it [00:00, ?it/s]\nLoading model: intfloat/e5-base-v2 on cuda\nEncoding train ...\n\nBatches:   0%|          | 0/23 [00:00<?, ?it/s]\nBatches:   4%|\u258d         | 1/23 [00:01<00:31,  1.43s/it]\nBatches:   9%|\u258a         | 2/23 [00:02<00:22,  1.09s/it]\nBatches:  13%|\u2588\u258e        | 3/23 [00:02<00:18,  1.10it/s]\nBatches:  17%|\u2588\u258b        | 4/23 [00:03<00:15,  1.23it/s]\nBatches:  22%|\u2588\u2588\u258f       | 5/23 [00:04<00:13,  1.36it/s]\nBatches:  26%|\u2588\u2588\u258c       | 6/23 [00:04<00:10,  1.58it/s]\nBatches:  30%|\u2588\u2588\u2588       | 7/23 [00:05<00:09,  1.77it/s]\nBatches:  35%|\u2588\u2588\u2588\u258d      | 8/23 [00:05<00:07,  1.98it/s]\nBatches:  39%|\u2588\u2588\u2588\u2589      | 9/23 [00:05<00:06,  2.19it/s]\nBatches:  43%|\u2588\u2588\u2588\u2588\u258e     | 10/23 [00:06<00:05,  2.42it/s]\nBatches:  48%|\u2588\u2588\u2588\u2588\u258a     | 11/23 [00:06<00:04,  2.69it/s]\nBatches:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 12/23 [00:06<00:03,  2.96it/s]\nBatches:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 13/23 [00:06<00:03,  3.21it/s]\nBatches:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 14/23 [00:07<00:02,  3.30it/s]\nBatches:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 15/23 [00:07<00:02,  3.54it/s]\nBatches:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 16/23 [00:07<00:01,  3.70it/s]\nBatches:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 17/23 [00:07<00:01,  3.87it/s]\nBatches:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 18/23 [00:08<00:01,  4.09it/s]\nBatches:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 19/23 [00:08<00:00,  4.54it/s]\nBatches:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 20/23 [00:08<00:00,  4.85it/s]\nBatches:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 21/23 [00:08<00:00,  5.02it/s]\nBatches:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 22/23 [00:08<00:00,  5.62it/s]\nBatches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 23/23 [00:08<00:00,  2.61it/s]\nEncoding test ...\n\nBatches:   0%|          | 0/10 [00:00<?, ?it/s]\nBatches:  10%|\u2588         | 1/10 [00:00<00:01,  7.90it/s]\nBatches:  30%|\u2588\u2588\u2588       | 3/10 [00:00<00:00, 10.49it/s]\nBatches:  50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:00<00:00, 11.76it/s]\nBatches:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:00<00:00, 13.62it/s]\nBatches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:00<00:00, 18.24it/s]\nBatches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:00<00:00, 15.05it/s]\nSaved: emb_cache/emb_e5_train.npy emb_cache/emb_e5_test.npy | shapes: (2878, 768) (1162, 768)\n\nE5 embeddings generation complete. You can now run Cell 10 to train XGB on cached embeddings.\n"
          ]
        }
      ]
    },
    {
      "id": "e034eec7-ce69-4f5d-80ef-40e1e4ba3970",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Torch repair v2: install cu121 stack into local vendor dir (read-only system fallback), guard sys.path\n",
        "import os, sys, site, shutil, subprocess, glob\n",
        "from pathlib import Path\n",
        "\n",
        "VENDOR = Path('vendor_pkgs'); VENDOR.mkdir(exist_ok=True)\n",
        "\n",
        "def sh(args):\n",
        "    print('$', ' '.join(args), flush=True)\n",
        "    subprocess.run(args, check=True)\n",
        "\n",
        "# 0) Disable user-site/shadowing for this process\n",
        "for k in ['PIP_TARGET','PYTHONPATH','PYTHONUSERBASE','PIP_USER']:\n",
        "    os.environ.pop(k, None)\n",
        "os.environ['PYTHONNOUSERSITE'] = '1'\n",
        "\n",
        "# 1) Remove pip-target dirs and *.pth that inject them\n",
        "for d in [os.path.expanduser('~/.pip-target'), '/app/.pip-target']:\n",
        "    if os.path.exists(d):\n",
        "        print('Removing', d); shutil.rmtree(d, ignore_errors=True)\n",
        "for sp in set(site.getsitepackages() + [site.getusersitepackages()]):\n",
        "    if isinstance(sp, str) and os.path.isdir(sp):\n",
        "        for pth in glob.glob(os.path.join(sp, '*.pth')):\n",
        "            try:\n",
        "                txt = open(pth, 'r', encoding='utf-8', errors='ignore').read()\n",
        "                if 'pip-target' in txt: print('Removing pth injector:', pth); os.remove(pth)\n",
        "            except Exception: pass\n",
        "\n",
        "# 2) Purge any torch/transformer remnants inside vendor (clean slate)\n",
        "for pat in ['torch*','torchvision*','torchaudio*','sentence_transformers*','sentence-transformers*','transformers*','accelerate*','tokenizers*','safetensors*','nvidia_*','triton*']:\n",
        "    for p in VENDOR.glob(pat):\n",
        "        if p.is_dir(): print('Removing dir', p); shutil.rmtree(p, ignore_errors=True)\n",
        "        elif p.is_file(): print('Removing file', p); p.unlink(missing_ok=True)\n",
        "\n",
        "# 3) Install exact CUDA 12.1 torch stack into vendor (writeable) and then NLP deps with constraints\n",
        "constraints = Path('constraints.txt'); constraints.write_text('torch==2.4.1\\ntorchvision==0.19.1\\ntorchaudio==2.4.1\\n')\n",
        "try:\n",
        "    sh([sys.executable, '-m', 'pip', 'install', '--no-cache-dir', '--target', str(VENDOR),\n",
        "        '--index-url', 'https://download.pytorch.org/whl/cu121', '--extra-index-url', 'https://pypi.org/simple',\n",
        "        'torch==2.4.1', 'torchvision==0.19.1', 'torchaudio==2.4.1'])\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print('Torch stack install failed:', e)\n",
        "sh([sys.executable, '-m', 'pip', 'install', '--no-cache-dir', '--target', str(VENDOR), '-c', str(constraints),\n",
        "    'sentence-transformers==3.0.1', 'transformers==4.44.2', 'accelerate==0.34.2', 'sentencepiece'])\n",
        "\n",
        "# 4) Runtime path guard: remove any pip-target paths; prepend vendor to sys.path\n",
        "sys.path = [p for p in sys.path if 'pip-target' not in p]\n",
        "if str(VENDOR) not in sys.path:\n",
        "    sys.path.insert(0, str(VENDOR))\n",
        "\n",
        "# 5) Sanity tests\n",
        "try:\n",
        "    import torch\n",
        "    print('torch:', getattr(torch, '__version__', None), '| cuda:', getattr(torch.version, 'cuda', None), '| cuda_available:', torch.cuda.is_available())\n",
        "    if torch.cuda.is_available():\n",
        "        print('gpu:', torch.cuda.get_device_name(0))\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    print('sentence-transformers import OK')\n",
        "except Exception as e:\n",
        "    print('Sanity check failed:', repr(e))\n",
        "\n",
        "print('Vendor path used:', VENDOR.resolve())\n",
        "print('>>> If torch.cuda.is_available() is False, still proceed with embedding CPU fallback temporarily, but prefer GPU. <<<')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing /app/.pip-target\nRemoving /app/.pip-target\n$ /usr/bin/python3.11 -m pip install --no-cache-dir --target vendor_pkgs --index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://pypi.org/simple torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121, https://pypi.org/simple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (799.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 MB 565.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.19.1\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.1/7.1 MB 524.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.4/3.4 MB 513.2 MB/s eta 0:00:00\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 126.7 MB/s eta 0:00:00\nCollecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 217.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 124.4 MB/s eta 0:00:00\nCollecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 173.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 165.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 272.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 432.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 120.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 194.0 MB/s eta 0:00:00\nCollecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 123.5 MB/s eta 0:00:00\nCollecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 168.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 388.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing-extensions>=4.8.0\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 340.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fsspec\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 438.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 467.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 283.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 486.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow!=8.3.*,>=5.3.0\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 163.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 101.8 MB/s eta 0:00:00\nCollecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 298.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 379.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.3 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pillow-11.3.0 sympy-1.14.0 torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121 triton-3.0.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$ /usr/bin/python3.11 -m pip install --no-cache-dir --target vendor_pkgs -c constraints.txt sentence-transformers==3.0.1 transformers==4.44.2 accelerate==0.34.2 sentencepiece\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers==3.0.1\n  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 227.1/227.1 KB 8.7 MB/s eta 0:00:00\nCollecting transformers==4.44.2\n  Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.5/9.5 MB 69.9 MB/s eta 0:00:00\nCollecting accelerate==0.34.2\n  Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 324.4/324.4 KB 443.9 MB/s eta 0:00:00\nCollecting sentencepiece\n  Downloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.4/1.4 MB 368.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.9/35.9 MB 220.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch>=1.11.0\n  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 797.1/797.1 MB 106.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.7/9.7 MB 247.1 MB/s eta 0:00:00\nCollecting huggingface-hub>=0.15.1\n  Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 563.3/563.3 KB 510.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 210.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Pillow\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 173.3 MB/s eta 0:00:00\nCollecting tqdm\n  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.5/78.5 KB 419.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting safetensors>=0.4.1\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 485.8/485.8 KB 523.2 MB/s eta 0:00:00\nCollecting pyyaml>=5.1\n  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 806.6/806.6 KB 504.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting regex!=2019.12.17\n  Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 KB 521.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers<0.20,>=0.19\n  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.6/3.6 MB 193.9 MB/s eta 0:00:00\nCollecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\nCollecting packaging>=20.0\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 373.9 MB/s eta 0:00:00\nCollecting requests\n  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.7/64.7 KB 423.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psutil\n  Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 291.2/291.2 KB 486.3 MB/s eta 0:00:00\nCollecting hf-xet<2.0.0,>=1.1.3\n  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.2/3.2 MB 500.8 MB/s eta 0:00:00\nCollecting fsspec>=2023.5.0\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 507.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing-extensions>=3.7.4.3\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 396.6 MB/s eta 0:00:00\nCollecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 198.9 MB/s eta 0:00:00\nCollecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 196.9 MB/s eta 0:00:00\nCollecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 443.8 MB/s eta 0:00:00\nCollecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 190.1 MB/s eta 0:00:00\nCollecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 192.9 MB/s eta 0:00:00\nCollecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 202.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 261.3 MB/s eta 0:00:00\nCollecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 376.7 MB/s eta 0:00:00\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 203.5 MB/s eta 0:00:00\nCollecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 174.0 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 202.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 567.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 397.2 MB/s eta 0:00:00\nCollecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 216.0 MB/s eta 0:00:00\nCollecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 500.2 MB/s eta 0:00:00\nCollecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 263.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting charset_normalizer<4,>=2\n  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 150.3/150.3 KB 493.1 MB/s eta 0:00:00\nCollecting urllib3<3,>=1.21.1\n  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 129.8/129.8 KB 478.3 MB/s eta 0:00:00\nCollecting certifi>=2017.4.17\n  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 161.2/161.2 KB 458.1 MB/s eta 0:00:00\nCollecting idna<4,>=2.5\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 70.4/70.4 KB 413.9 MB/s eta 0:00:00\nCollecting threadpoolctl>=3.1.0\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nCollecting joblib>=1.2.0\n  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 308.4/308.4 KB 490.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\nCollecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 502.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: mpmath, urllib3, typing-extensions, tqdm, threadpoolctl, sympy, sentencepiece, safetensors, regex, pyyaml, psutil, Pillow, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, joblib, idna, hf-xet, fsspec, filelock, charset_normalizer, certifi, triton, scipy, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, scikit-learn, nvidia-cusolver-cu12, huggingface-hub, torch, tokenizers, transformers, accelerate, sentence-transformers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.3 Pillow-11.3.0 accelerate-0.34.2 certifi-2025.8.3 charset_normalizer-3.4.3 filelock-3.19.1 fsspec-2025.9.0 hf-xet-1.1.10 huggingface-hub-0.35.1 idna-3.10 jinja2-3.1.6 joblib-1.5.2 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 packaging-25.0 psutil-7.1.0 pyyaml-6.0.3 regex-2025.9.18 requests-2.32.5 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 sentence-transformers-3.0.1 sentencepiece-0.2.1 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.19.1 torch-2.4.1 tqdm-4.67.1 transformers-4.44.2 triton-3.0.0 typing-extensions-4.15.0 urllib3-2.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/functorch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/torch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/torchgen already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/nvidia_cusolver_cu12-11.4.5.107.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/jinja2-3.1.6.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/jinja2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/nvidia_cudnn_cu12-9.1.0.70.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/nvidia_cusparse_cu12-12.1.0.106.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/triton-3.0.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/triton already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/filelock-3.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/filelock already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/fsspec-2025.9.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/fsspec already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/markupsafe-3.0.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/markupsafe already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/networkx-3.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/nvidia_cublas_cu12-12.1.3.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/nvidia_cuda_cupti_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/nvidia_cuda_nvrtc_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/nvidia_cuda_runtime_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/nvidia_cufft_cu12-11.0.2.54.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/nvidia_curand_cu12-10.3.2.106.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/nvidia_nccl_cu12-2.20.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/nvidia_nvjitlink_cu12-12.9.86.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/nvidia_nvtx_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/nvidia already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/pillow-11.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/pillow.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/PIL already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/sympy-1.14.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/sympy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/isympy.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/mpmath-1.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/mpmath already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/share already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanity check failed: AttributeError(\"module 'torch' has no attribute 'version'\")\nVendor path used: /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/vendor_pkgs\n>>> If torch.cuda.is_available() is False, still proceed with embedding CPU fallback temporarily, but prefer GPU. <<<\n"
          ]
        }
      ]
    },
    {
      "id": "29c06e11-69bb-45fc-930a-a9bf67dafb1c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Torch diagnostics: identify import path and attributes\n",
        "import sys\n",
        "print('First 5 sys.path entries:')\n",
        "for i,p in enumerate(sys.path[:5]):\n",
        "    print(i, p)\n",
        "try:\n",
        "    import torch\n",
        "    print('torch module:', torch)\n",
        "    print('torch __file__:', getattr(torch, '__file__', None))\n",
        "    print('has torch.__version__:', hasattr(torch, '__version__'))\n",
        "    print('has torch.version:', hasattr(torch, 'version'))\n",
        "    if hasattr(torch, 'version'):\n",
        "        print('torch.version:', torch.version)\n",
        "    print('has torch.cuda:', hasattr(torch, 'cuda'))\n",
        "    if hasattr(torch, 'cuda'):\n",
        "        print('torch.cuda.is_available():', getattr(torch.cuda, 'is_available', lambda: 'n/a')())\n",
        "except Exception as e:\n",
        "    print('Import torch failed:', repr(e))\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    import sentence_transformers as st\n",
        "    print('sentence_transformers __file__:', getattr(st, '__file__', None))\n",
        "except Exception as e:\n",
        "    print('Import sentence_transformers failed:', repr(e))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 sys.path entries:\n0 vendor_pkgs\n1 /app\n2 /app/src\n3 /usr/lib/python311.zip\n4 /usr/lib/python3.11\ntorch module: <module 'torch' (<_frozen_importlib_external.NamespaceLoader object at 0x7dbe59d048d0>)>\ntorch __file__: None\nhas torch.__version__: False\nhas torch.version: False\nhas torch.cuda: False\nImport sentence_transformers failed: ImportError(\"cannot import name 'Tensor' from 'torch' (unknown location)\")\n"
          ]
        }
      ]
    },
    {
      "id": "99abf81d-bcb7-488a-9798-1e3546c1df82",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Torch fix v3: remove shadow namespace dirs, force-reinstall into vendor, verify CUDA\n",
        "import os, sys, shutil, subprocess, glob, site\n",
        "from pathlib import Path\n",
        "\n",
        "VENDOR = Path('vendor_pkgs'); VENDOR.mkdir(exist_ok=True)\n",
        "\n",
        "def sh(args):\n",
        "    print('$', ' '.join(args), flush=True)\n",
        "    subprocess.run(args, check=True)\n",
        "\n",
        "# 1) Ensure vendor is first; drop any pip-target entries\n",
        "sys.path = [p for p in sys.path if 'pip-target' not in p]\n",
        "if str(VENDOR) not in sys.path:\n",
        "    sys.path.insert(0, str(VENDOR))\n",
        "print('sys.path[0:5]=', sys.path[:5])\n",
        "\n",
        "# 2) Locate and remove shadow 'torch' namespace dirs not under vendor (common cause of NamespaceLoader import) \n",
        "cwd = os.getcwd()\n",
        "removed = []\n",
        "for p in list(sys.path):\n",
        "    # Only consider writable, local paths (avoid system dirs)\n",
        "    if not isinstance(p, str):\n",
        "        continue\n",
        "    if not (p.startswith(cwd) or p.startswith('/app')):\n",
        "        continue\n",
        "    tdir = os.path.join(p, 'torch')\n",
        "    if os.path.isdir(tdir) and str(VENDOR) not in tdir:\n",
        "        # Heuristic: remove if missing __init__.py (namespace dir) or obviously not a proper torch package\n",
        "        has_init = os.path.exists(os.path.join(tdir, '__init__.py'))\n",
        "        if not has_init:\n",
        "            try:\n",
        "                print('Removing shadow torch dir:', tdir)\n",
        "                shutil.rmtree(tdir, ignore_errors=True)\n",
        "                removed.append(tdir)\n",
        "            except Exception as e:\n",
        "                print('Failed to remove', tdir, e)\n",
        "print('Removed shadow dirs:', removed)\n",
        "\n",
        "# 3) Force reinstall GPU torch stack into vendor\n",
        "constraints = Path('constraints.txt'); constraints.write_text('torch==2.4.1\\ntorchvision==0.19.1\\ntorchaudio==2.4.1\\n')\n",
        "try:\n",
        "    sh([sys.executable, '-m', 'pip', 'install', '--no-cache-dir', '--target', str(VENDOR),\n",
        "        '--upgrade', '--force-reinstall',\n",
        "        '--index-url', 'https://download.pytorch.org/whl/cu121', '--extra-index-url', 'https://pypi.org/simple',\n",
        "        'torch==2.4.1', 'torchvision==0.19.1', 'torchaudio==2.4.1'])\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print('Torch stack reinstall error:', e)\n",
        "try:\n",
        "    sh([sys.executable, '-m', 'pip', 'install', '--no-cache-dir', '--target', str(VENDOR), '-c', str(constraints),\n",
        "        '--upgrade', '--force-reinstall',\n",
        "        'sentence-transformers==3.0.1', 'transformers==4.44.2', 'accelerate==0.34.2', 'sentencepiece'])\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print('NLP deps reinstall error:', e)\n",
        "\n",
        "# 4) Final guard: vendor first on sys.path\n",
        "sys.path = [p for p in sys.path if 'pip-target' not in p]\n",
        "if sys.path[0] != str(VENDOR):\n",
        "    sys.path.insert(0, str(VENDOR))\n",
        "print('sys.path[0]=', sys.path[0])\n",
        "\n",
        "# 5) Sanity import and report source files\n",
        "try:\n",
        "    import importlib, types\n",
        "    torch = importlib.import_module('torch')\n",
        "    print('torch module file:', getattr(torch, '__file__', None))\n",
        "    print('torch has __version__?', hasattr(torch, '__version__'))\n",
        "    print('torch has version attr?', hasattr(torch, 'version'))\n",
        "    if hasattr(torch, '__version__'):\n",
        "        print('torch.__version__ =', torch.__version__)\n",
        "    if hasattr(torch, 'version') and hasattr(torch.version, 'cuda'):\n",
        "        print('torch.version.cuda =', torch.version.cuda)\n",
        "    if hasattr(torch, 'cuda') and hasattr(torch.cuda, 'is_available'):\n",
        "        print('torch.cuda.is_available() =', torch.cuda.is_available())\n",
        "    st = importlib.import_module('sentence_transformers')\n",
        "    print('sentence_transformers file:', getattr(st, '__file__', None))\n",
        "except Exception as e:\n",
        "    print('Sanity import failed:', repr(e))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sys.path[0:5]= ['vendor_pkgs', '/app', '/app/src', '/usr/lib/python311.zip', '/usr/lib/python3.11']\nRemoved shadow dirs: []\n$ /usr/bin/python3.11 -m pip install --no-cache-dir --target vendor_pkgs --upgrade --force-reinstall --index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://pypi.org/simple torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121, https://pypi.org/simple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (799.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 MB 243.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.19.1\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.1/7.1 MB 513.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.4/3.4 MB 260.0 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 222.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\nCollecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 236.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 207.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 194.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 552.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 231.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing-extensions>=4.8.0\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 364.5 MB/s eta 0:00:00\nCollecting fsspec\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 469.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 240.0 MB/s eta 0:00:00\nCollecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 468.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 244.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 505.4 MB/s eta 0:00:00\nCollecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 225.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 488.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 210.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 239.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 256.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 397.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow!=8.3.*,>=5.3.0\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 177.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 131.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\nCollecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 502.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.3 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pillow-11.3.0 sympy-1.14.0 torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121 triton-3.0.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$ /usr/bin/python3.11 -m pip install --no-cache-dir --target vendor_pkgs -c constraints.txt --upgrade --force-reinstall sentence-transformers==3.0.1 transformers==4.44.2 accelerate==0.34.2 sentencepiece\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers==3.0.1\n  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 227.1/227.1 KB 10.2 MB/s eta 0:00:00\nCollecting transformers==4.44.2\n  Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.5/9.5 MB 164.9 MB/s eta 0:00:00\nCollecting accelerate==0.34.2\n  Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 324.4/324.4 KB 483.2 MB/s eta 0:00:00\nCollecting sentencepiece\n  Downloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.4/1.4 MB 369.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tqdm\n  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.5/78.5 KB 406.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Pillow\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 194.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.9/35.9 MB 232.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 210.7 MB/s eta 0:00:00\nCollecting torch>=1.11.0\n  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 797.1/797.1 MB 152.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.7/9.7 MB 334.1 MB/s eta 0:00:00\nCollecting huggingface-hub>=0.15.1\n  Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 563.3/563.3 KB 496.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers<0.20,>=0.19\n  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.6/3.6 MB 187.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting safetensors>=0.4.1\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 485.8/485.8 KB 329.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting regex!=2019.12.17\n  Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 KB 255.3 MB/s eta 0:00:00\nCollecting requests\n  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.7/64.7 KB 332.8 MB/s eta 0:00:00\nCollecting pyyaml>=5.1\n  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 806.6/806.6 KB 506.3 MB/s eta 0:00:00\nCollecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\nCollecting packaging>=20.0\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 374.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psutil\n  Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 291.2/291.2 KB 484.8 MB/s eta 0:00:00\nCollecting typing-extensions>=3.7.4.3\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 325.2 MB/s eta 0:00:00\nCollecting hf-xet<2.0.0,>=1.1.3\n  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.2/3.2 MB 302.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fsspec>=2023.5.0\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 459.1 MB/s eta 0:00:00\nCollecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 174.3 MB/s eta 0:00:00\nCollecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 181.0 MB/s eta 0:00:00\nCollecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 214.2 MB/s eta 0:00:00\nCollecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 226.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 369.5 MB/s eta 0:00:00\nCollecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 239.4 MB/s eta 0:00:00\nCollecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 530.2 MB/s eta 0:00:00\nCollecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 180.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 451.1 MB/s eta 0:00:00\nCollecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 229.6 MB/s eta 0:00:00\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 220.6 MB/s eta 0:00:00\nCollecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 459.8 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 161.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 176.3 MB/s eta 0:00:00\nCollecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 243.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 177.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting charset_normalizer<4,>=2\n  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 150.3/150.3 KB 482.1 MB/s eta 0:00:00\nCollecting certifi>=2017.4.17\n  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 161.2/161.2 KB 487.1 MB/s eta 0:00:00\nCollecting urllib3<3,>=1.21.1\n  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 129.8/129.8 KB 437.3 MB/s eta 0:00:00\nCollecting idna<4,>=2.5\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 70.4/70.4 KB 448.8 MB/s eta 0:00:00\nCollecting joblib>=1.2.0\n  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 308.4/308.4 KB 70.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting threadpoolctl>=3.1.0\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nCollecting MarkupSafe>=2.0\n  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 519.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: mpmath, urllib3, typing-extensions, tqdm, threadpoolctl, sympy, sentencepiece, safetensors, regex, pyyaml, psutil, Pillow, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, joblib, idna, hf-xet, fsspec, filelock, charset_normalizer, certifi, triton, scipy, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, scikit-learn, nvidia-cusolver-cu12, huggingface-hub, torch, tokenizers, transformers, accelerate, sentence-transformers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.3 Pillow-11.3.0 accelerate-0.34.2 certifi-2025.8.3 charset_normalizer-3.4.3 filelock-3.19.1 fsspec-2025.9.0 hf-xet-1.1.10 huggingface-hub-0.35.1 idna-3.10 jinja2-3.1.6 joblib-1.5.2 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 packaging-25.0 psutil-7.1.0 pyyaml-6.0.3 regex-2025.9.18 requests-2.32.5 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 sentence-transformers-3.0.1 sentencepiece-0.2.1 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.19.1 torch-2.4.1 tqdm-4.67.1 transformers-4.44.2 triton-3.0.0 typing-extensions-4.15.0 urllib3-2.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sys.path[0]= vendor_pkgs\ntorch module file: None\ntorch has __version__? False\ntorch has version attr? False\nSanity import failed: ImportError(\"cannot import name 'Tensor' from 'torch' (unknown location)\")\n"
          ]
        }
      ]
    },
    {
      "id": "af2650b6-1b20-4d6d-8c83-d8e8b2ec85bc",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Runtime hardening: drop usersite/pip-target from sys.path, remove shadow torch, verify vendor torch import\n",
        "import os, sys, site, shutil, glob\n",
        "from pathlib import Path\n",
        "\n",
        "VENDOR = Path('vendor_pkgs')\n",
        "print('Pre sys.path[0:8]:', sys.path[:8])\n",
        "\n",
        "# 1) Disable usersite for this process and strip any usersite/pip-target paths from sys.path\n",
        "os.environ['PYTHONNOUSERSITE'] = '1'\n",
        "user_site = site.getusersitepackages() if hasattr(site, 'getusersitepackages') else None\n",
        "bad_substrings = ['.pip-target', str(user_site) if user_site else '']\n",
        "sys.path = [p for p in sys.path if not any(bs and bs in str(p) for bs in bad_substrings)]\n",
        "print('Post strip sys.path[0:8]:', sys.path[:8])\n",
        "print('User site path:', user_site)\n",
        "\n",
        "# 2) Physically remove ~/.pip-target to prevent accidental shadowing\n",
        "pt = os.path.expanduser('~/.pip-target')\n",
        "if os.path.exists(pt):\n",
        "    print('Removing ~/.pip-target recursively')\n",
        "    shutil.rmtree(pt, ignore_errors=True)\n",
        "\n",
        "# 3) Ensure vendor_pkgs is first on sys.path\n",
        "if str(VENDOR) not in sys.path:\n",
        "    sys.path.insert(0, str(VENDOR))\n",
        "else:\n",
        "    # Move to front if not already\n",
        "    sys.path.remove(str(VENDOR)); sys.path.insert(0, str(VENDOR))\n",
        "print('Final sys.path[0:5]:', sys.path[:5])\n",
        "\n",
        "# 4) Inspect vendor torch contents quickly\n",
        "torch_dir = VENDOR / 'torch'\n",
        "print('vendor torch exists:', torch_dir.exists(), '| has __init__:', (torch_dir / '__init__.py').exists())\n",
        "if torch_dir.exists():\n",
        "    sub = list(torch_dir.iterdir())[:10]\n",
        "    print('torch top entries:', [p.name for p in sub])\n",
        "\n",
        "# 5) Import torch and print details\n",
        "try:\n",
        "    import importlib\n",
        "    torch = importlib.import_module('torch')\n",
        "    print('torch __file__:', getattr(torch, '__file__', None))\n",
        "    print('torch __version__:', getattr(torch, '__version__', None))\n",
        "    has_cuda = hasattr(torch, 'cuda') and hasattr(torch.cuda, 'is_available') and torch.cuda.is_available()\n",
        "    print('CUDA available:', has_cuda)\n",
        "    if has_cuda:\n",
        "        try:\n",
        "            print('GPU:', torch.cuda.get_device_name(0))\n",
        "        except Exception as e:\n",
        "            print('GPU name fetch error:', repr(e))\n",
        "except Exception as e:\n",
        "    print('Import torch failed:', repr(e))\n",
        "\n",
        "# 6) Try sentence_transformers import (should resolve from vendor if installed there) \n",
        "try:\n",
        "    import sentence_transformers as st\n",
        "    print('sentence_transformers __file__:', getattr(st, '__file__', None))\n",
        "except Exception as e:\n",
        "    print('Import sentence_transformers failed:', repr(e))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre sys.path[0:8]: ['vendor_pkgs', '/app', '/app/src', '/usr/lib/python311.zip', '/usr/lib/python3.11', '/usr/lib/python3.11/lib-dynload', '', '/usr/local/lib/python3.11/dist-packages']\nPost strip sys.path[0:8]: ['vendor_pkgs', '/app', '/app/src', '/usr/lib/python311.zip', '/usr/lib/python3.11', '/usr/lib/python3.11/lib-dynload', '', '/usr/local/lib/python3.11/dist-packages']\nUser site path: /app/.pip-user/lib/python3.11/site-packages\nRemoving ~/.pip-target recursively\nFinal sys.path[0:5]: ['vendor_pkgs', '/app', '/app/src', '/usr/lib/python311.zip', '/usr/lib/python3.11']\nvendor torch exists: True | has __init__: True\ntorch top entries: ['random.py', 'signal', 'distributed', 'utils', 'onnx', '_decomp', 'testing', '_C.cpython-311-x86_64-linux-gnu.so', '_dynamo', '__init__.py']\ntorch __file__: None\ntorch __version__: None\nCUDA available: False\nImport sentence_transformers failed: ImportError(\"cannot import name 'Tensor' from 'torch' (unknown location)\")\n"
          ]
        }
      ]
    },
    {
      "id": "3554e048-017b-4be5-9280-881092659bf2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Env check: GPU, versions, basics\n",
        "import os, sys, subprocess, time, json, platform\n",
        "import numpy as np, pandas as pd\n",
        "print(\"Python:\", sys.version.split()[0], \"| Platform:\", platform.platform())\n",
        "print(\"Working dir:\", os.getcwd())\n",
        "\n",
        "def run(cmd):\n",
        "    print(\"$\", \" \".join(cmd), flush=True)\n",
        "    return subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "\n",
        "res = run(['bash','-lc','nvidia-smi || true'])\n",
        "print(res.stdout)\n",
        "gpu_ok = ('NVIDIA-SMI' in res.stdout) and ('Driver Version' in res.stdout)\n",
        "print(\"GPU available:\", gpu_ok)\n",
        "\n",
        "import sklearn, xgboost\n",
        "print(\"sklearn:\", sklearn.__version__)\n",
        "print(\"xgboost:\", xgboost.__version__)\n",
        "\n",
        "# Helper: timer context\n",
        "from contextlib import contextmanager\n",
        "@contextmanager\n",
        "def timer(msg):\n",
        "    t0 = time.time()\n",
        "    print(f\"[T0] {msg} ...\", flush=True)\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        dt = time.time() - t0\n",
        "        print(f\"[T+] {msg} done in {dt:.2f}s\", flush=True)\n",
        "\n",
        "# Seeds and constants\n",
        "SEEDS = [42, 1337, 2025]\n",
        "# Revised chains to improve C3 positives; reduce purge gap to 3 days\n",
        "CHAIN_SPLITS = [(0.0,0.60,0.80), (0.0,0.75,0.90), (0.0,0.80,1.00)]\n",
        "PURGE_GAP_DAYS = 3\n",
        "print(\"SEEDS:\", SEEDS)\n",
        "print(\"Chains:\", CHAIN_SPLITS, \"| Purge gap days:\", PURGE_GAP_DAYS)\n",
        "\n",
        "assert gpu_ok, \"GPU not available. Per competition_best_practices.md, exit immediately to avoid wasting time.\""
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.11.0rc1 | Platform: Linux-6.8.0-1031-azure-x86_64-with-glibc2.35\nWorking dir: /var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459\n$ bash -lc nvidia-smi || true\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Sep 29 09:05:43 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.06             Driver Version: 550.144.06     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10-24Q                 On  |   00000002:00:00.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |     711MiB /  24512MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\nGPU available: True\nsklearn: 1.7.2\nxgboost: 2.1.4\nSEEDS: [42, 1337, 2025]\nChains: [(0.0, 0.6, 0.8), (0.0, 0.75, 0.9), (0.0, 0.8, 1.0)] | Purge gap days: 3\n"
          ]
        }
      ]
    },
    {
      "id": "4a287298-2df5-4ff7-888f-bb2786ff3775",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build purged forward-chaining, group-purged folds and persist indices\n",
        "import os, json, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "with timer(\"Load train.json and basic parsing\"):\n",
        "    # Robust JSON loader: try JSON Lines first, then standard JSON array/object\n",
        "    df = None\n",
        "    try:\n",
        "        df = pd.read_json(\"train.json\", lines=True)\n",
        "    except ValueError:\n",
        "        try:\n",
        "            df = pd.read_json(\"train.json\", lines=False)\n",
        "        except ValueError:\n",
        "            # Final fallback: read whole file via json.load and normalize\n",
        "            with open(\"train.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "            if isinstance(data, dict) and \"data\" in data:\n",
        "                data = data[\"data\"]\n",
        "            df = pd.json_normalize(data)\n",
        "    assert df is not None and len(df) > 0, \"Failed to load train.json into a DataFrame\"\n",
        "    # Identify key columns with fallbacks\n",
        "    time_col_candidates = [\n",
        "        'unix_timestamp_of_request_utc', 'request_timestamp', 'created_utc', 'timestamp', 'time'\n",
        "    ]\n",
        "    user_col_candidates = ['requester_username', 'username', 'user']\n",
        "    label_col_candidates = ['requester_received_pizza', 'label', 'target', 'y']\n",
        "\n",
        "    def first_col(cols):\n",
        "        for c in cols:\n",
        "            if c in df.columns:\n",
        "                return c\n",
        "        raise KeyError(f\"Missing expected columns; have: {list(df.columns)[:20]} ...\")\n",
        "\n",
        "    TIME_COL = first_col(time_col_candidates)\n",
        "    GROUP_COL = first_col(user_col_candidates)\n",
        "    LABEL_COL = first_col(label_col_candidates)\n",
        "    print(\"Cols:\", dict(time=TIME_COL, group=GROUP_COL, label=LABEL_COL))\n",
        "\n",
        "    # Ensure numeric unix time (seconds) and datetime for readability\n",
        "    ts = pd.to_numeric(df[TIME_COL], errors='coerce').astype('Int64')\n",
        "    if ts.isna().any():\n",
        "        # If stored as datetime string, coerce\n",
        "        try:\n",
        "            ts2 = pd.to_datetime(df[TIME_COL], utc=True, errors='coerce')\n",
        "            ts = (ts2.view('int64') // 1_000_000_000).astype('Int64')\n",
        "        except Exception:\n",
        "            pass\n",
        "    ts = ts.fillna(ts.dropna().median()).astype(np.int64)\n",
        "    if ts.max() < 10_000_000_000:\n",
        "        unix_s = ts.values\n",
        "    else:\n",
        "        unix_s = (ts.values // 1_000_000_000).astype(np.int64)\n",
        "    df['_unix_s'] = unix_s\n",
        "    df['_dt'] = pd.to_datetime(df['_unix_s'], unit='s', utc=True)\n",
        "    # Clean label to 0/1\n",
        "    y = df[LABEL_COL]\n",
        "    if y.dtype == bool:\n",
        "        y = y.astype(np.int8)\n",
        "    elif y.dtype.name == 'object':\n",
        "        y = y.map({True:1, False:0, 'True':1, 'False':0, 'yes':1, 'no':0}).fillna(pd.to_numeric(y, errors='coerce')).fillna(0).astype(int)\n",
        "    else:\n",
        "        y = pd.to_numeric(y, errors='coerce').fillna(0).astype(int)\n",
        "    df['_y'] = y.clip(0,1).astype(np.int8)\n",
        "\n",
        "with timer(\"Sort by time and compute chain boundaries\"):\n",
        "    df = df.sort_values('_unix_s', kind='mergesort').reset_index(drop=True)\n",
        "    n = len(df)\n",
        "    print(\"Rows:\", n, \"| Positives:\", int(df['_y'].sum()))\n",
        "    print(\"Time range:\", df['_dt'].min(), \"->\", df['_dt'].max())\n",
        "    # Pre-compute position indices for split fractions\n",
        "    def frac_to_idx(f):\n",
        "        return int(round(f * n))\n",
        "    chains = []\n",
        "    for (tr_s, tr_e, va_e) in CHAIN_SPLITS:\n",
        "        tr_s_i, tr_e_i, va_e_i = frac_to_idx(tr_s), frac_to_idx(tr_e), frac_to_idx(va_e)\n",
        "        va_s_i = tr_e_i  # initial val start at train end fraction\n",
        "        chains.append((tr_s_i, tr_e_i, va_s_i, va_e_i))\n",
        "    print(\"Chain idx (train_start, train_end, val_start, val_end):\", chains)\n",
        "\n",
        "with timer(\"Construct purged, group-purged folds\"):\n",
        "    purge_gap_sec = int(PURGE_GAP_DAYS * 86400)\n",
        "    fold_dir = Path(\"folds\"); fold_dir.mkdir(exist_ok=True)\n",
        "    manifest = {\"time_col\": TIME_COL, \"group_col\": GROUP_COL, \"label_col\": LABEL_COL,\n",
        "                \"purge_gap_days\": PURGE_GAP_DAYS, \"chains\": []}\n",
        "    for ci, (tr_s_i, tr_e_i, va_s_i, va_e_i) in enumerate(chains, start=1):\n",
        "        # Base masks by position window\n",
        "        base_train_idx = np.arange(tr_s_i, tr_e_i)\n",
        "        base_val_idx = np.arange(va_s_i, va_e_i)\n",
        "        # Enforce purge gap: shift val start by time\n",
        "        train_end_ts = int(df.iloc[tr_e_i - 1]['_unix_s']) if tr_e_i > tr_s_i else int(df.iloc[0]['_unix_s'])\n",
        "        min_val_ts = train_end_ts + purge_gap_sec\n",
        "        # Find first index in base_val_idx with ts >= min_val_ts\n",
        "        val_ts = df.iloc[base_val_idx]['_unix_s'].values if len(base_val_idx) else np.array([])\n",
        "        if len(val_ts):\n",
        "            valid_mask_time = val_ts >= min_val_ts\n",
        "            base_val_idx = base_val_idx[valid_mask_time]\n",
        "        # Group purge: drop val rows whose group appears in train\n",
        "        tr_groups = set(df.iloc[base_train_idx][GROUP_COL].astype(str).values.tolist())\n",
        "        if len(base_val_idx):\n",
        "            val_groups = df.iloc[base_val_idx][GROUP_COL].astype(str).values\n",
        "            keep_mask = np.array([g not in tr_groups for g in val_groups], dtype=bool)\n",
        "            val_idx = base_val_idx[keep_mask]\n",
        "        else:\n",
        "            val_idx = base_val_idx\n",
        "        train_idx = base_train_idx\n",
        "        # Safety: ensure non-empty and enough positives in val\n",
        "        val_pos = int(df.iloc[val_idx]['_y'].sum()) if len(val_idx) else 0\n",
        "        train_pos = int(df.iloc[train_idx]['_y'].sum()) if len(train_idx) else 0\n",
        "        tr_start_dt = df.iloc[train_idx]['_dt'].min() if len(train_idx) else None\n",
        "        tr_end_dt = df.iloc[train_idx]['_dt'].max() if len(train_idx) else None\n",
        "        va_start_dt = df.iloc[val_idx]['_dt'].min() if len(val_idx) else None\n",
        "        va_end_dt = df.iloc[val_idx]['_dt'].max() if len(val_idx) else None\n",
        "        print(f\"Chain {ci}: train {len(train_idx)} (pos {train_pos}) [{tr_start_dt} -> {tr_end_dt}] | val {len(val_idx)} (pos {val_pos}) [{va_start_dt} -> {va_end_dt}]\")\n",
        "        if len(val_idx) == 0 or val_pos < 50:\n",
        "            print(f\"Warning: Chain {ci} has low/zero positives in val ({val_pos}). Consider widening window or reducing purge gap.\")\n",
        "        # Persist\n",
        "        np.save(fold_dir / f\"fc_chain{ci}_train_idx.npy\", train_idx)\n",
        "        np.save(fold_dir / f\"fc_chain{ci}_val_idx.npy\", val_idx)\n",
        "        chain_info = {\n",
        "            \"chain\": ci,\n",
        "            \"train_count\": int(len(train_idx)),\n",
        "            \"val_count\": int(len(val_idx)),\n",
        "            \"train_pos\": train_pos,\n",
        "            \"val_pos\": val_pos,\n",
        "            \"train_end_dt\": pd.to_datetime(train_end_ts, unit='s', utc=True).isoformat(),\n",
        "            \"min_val_dt\": pd.to_datetime(min_val_ts, unit='s', utc=True).isoformat(),\n",
        "            \"val_start_dt\": va_start_dt.isoformat() if va_start_dt is not None else None,\n",
        "            \"val_end_dt\": va_end_dt.isoformat() if va_end_dt is not None else None\n",
        "        }\n",
        "        manifest[\"chains\"].append(chain_info)\n",
        "\n",
        "with timer(\"Save fold manifest\"):\n",
        "    (fold_dir / \"manifest.json\").write_text(json.dumps(manifest, indent=2))\n",
        "    print(json.dumps(manifest, indent=2)[:800] + (\"...\" if len(json.dumps(manifest))>800 else \"\"))\n",
        "\n",
        "print(\"Folds saved under ./folds/. Reuse these indices across all legs.\")"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Load train.json and basic parsing ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cols: {'time': 'unix_timestamp_of_request_utc', 'group': 'requester_username', 'label': 'requester_received_pizza'}\n[T+] Load train.json and basic parsing done in 0.11s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Sort by time and compute chain boundaries ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows: 2878 | Positives: 715\nTime range: 2011-05-23 20:29:10+00:00 -> 2013-10-09 18:51:12+00:00\nChain idx (train_start, train_end, val_start, val_end): [(0, 1727, 1727, 2302), (0, 2158, 2158, 2590), (0, 2302, 2302, 2878)]\n[T+] Sort by time and compute chain boundaries done in 0.00s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Construct purged, group-purged folds ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 1: train 1727 (pos 470) [2011-05-23 20:29:10+00:00 -> 2012-10-17 20:30:50+00:00] | val 565 (pos 131) [2012-10-20 22:09:47+00:00 -> 2013-05-08 23:52:39+00:00]\nChain 2: train 2158 (pos 564) [2011-05-23 20:29:10+00:00 -> 2013-03-27 05:29:00+00:00] | val 427 (pos 103) [2013-03-30 10:03:22+00:00 -> 2013-08-05 04:54:13+00:00]\nChain 3: train 2302 (pos 603) [2011-05-23 20:29:10+00:00 -> 2013-05-08 23:52:39+00:00] | val 567 (pos 109) [2013-05-12 22:20:35+00:00 -> 2013-10-09 18:51:12+00:00]\n[T+] Construct purged, group-purged folds done in 0.01s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Save fold manifest ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n  \"time_col\": \"unix_timestamp_of_request_utc\",\n  \"group_col\": \"requester_username\",\n  \"label_col\": \"requester_received_pizza\",\n  \"purge_gap_days\": 3,\n  \"chains\": [\n    {\n      \"chain\": 1,\n      \"train_count\": 1727,\n      \"val_count\": 565,\n      \"train_pos\": 470,\n      \"val_pos\": 131,\n      \"train_end_dt\": \"2012-10-17T20:30:50+00:00\",\n      \"min_val_dt\": \"2012-10-20T20:30:50+00:00\",\n      \"val_start_dt\": \"2012-10-20T22:09:47+00:00\",\n      \"val_end_dt\": \"2013-05-08T23:52:39+00:00\"\n    },\n    {\n      \"chain\": 2,\n      \"train_count\": 2158,\n      \"val_count\": 427,\n      \"train_pos\": 564,\n      \"val_pos\": 103,\n      \"train_end_dt\": \"2013-03-27T05:29:00+00:00\",\n      \"min_val_dt\": \"2013-03-30T05:29:00+00:00\",\n      \"val_start_dt\": \"2013-03-30T10:03:22+00:00\",\n      \"val_end_dt\": \"2013-08-05T04:...\n[T+] Save fold manifest done in 0.00s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folds saved under ./folds/. Reuse these indices across all legs.\n"
          ]
        }
      ]
    },
    {
      "id": "1973be40-7ffa-42ce-b332-147c23be3137",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Leg A: TF-IDF (title x3 + body) + Logistic Regression under forward-chaining folds\n",
        "import json, gc, time, glob\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def load_json_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path, lines=True)\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return pd.read_json(path, lines=False)\n",
        "        except ValueError:\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            if isinstance(data, dict) and 'data' in data:\n",
        "                data = data['data']\n",
        "            return pd.json_normalize(data)\n",
        "\n",
        "def build_text(df):\n",
        "    title_col_candidates = ['request_title', 'title']\n",
        "    # Leak-safe: never use edit-aware body text\n",
        "    body_col_candidates = ['request_text', 'body', 'text']\n",
        "    def first_col(cols):\n",
        "        for c in cols:\n",
        "            if c in df.columns:\n",
        "                return c\n",
        "        return None\n",
        "    tcol = first_col(title_col_candidates)\n",
        "    bcol = first_col(body_col_candidates)\n",
        "    t = df[tcol].fillna(\"\") if tcol else pd.Series([\"\" for _ in range(len(df))])\n",
        "    b = df[bcol].fillna(\"\") if bcol else pd.Series([\"\" for _ in range(len(df))])\n",
        "    # Up-weight title by repeating x3\n",
        "    return (t + ' ' + t + ' ' + t + ' ' + b).astype(str)\n",
        "\n",
        "with timer(\"Load train/test and prepare text\"):\n",
        "    tr = load_json_df('train.json')\n",
        "    te = load_json_df('test.json')\n",
        "    # Columns discovered earlier:\n",
        "    time_col = 'unix_timestamp_of_request_utc' if 'unix_timestamp_of_request_utc' in tr.columns else tr.columns[0]\n",
        "    group_col = 'requester_username' if 'requester_username' in tr.columns else tr.columns[1]\n",
        "    label_col = 'requester_received_pizza' if 'requester_received_pizza' in tr.columns else 'label'\n",
        "    # Align order as in folds (sorted by time) to match saved indices\n",
        "    tr = tr.sort_values(time_col, kind='mergesort').reset_index(drop=True)\n",
        "    X_text_tr = build_text(tr)\n",
        "    X_text_te = build_text(te)\n",
        "    y = pd.to_numeric(tr[label_col], errors='coerce').fillna(0).astype(int).clip(0,1).values\n",
        "\n",
        "fold_dir = Path('folds')\n",
        "oof = np.zeros(len(tr), dtype=float)\n",
        "val_mask = np.zeros(len(tr), dtype=bool)\n",
        "test_preds_seeds = []\n",
        "params = dict(C_grid=[0.5,1,2,4], class_weight='balanced', max_features_word=200000, max_features_char=300000)\n",
        "print(\"Params:\", params)\n",
        "\n",
        "def fit_predict_fold(train_idx, val_idx, seed):\n",
        "    # Word and char TF-IDF separate, then hstack\n",
        "    word_vec = TfidfVectorizer(ngram_range=(1,2), analyzer='word', min_df=2, max_df=0.98,\n",
        "                               max_features=params['max_features_word'],\n",
        "                               dtype=np.float32)\n",
        "    char_vec = TfidfVectorizer(ngram_range=(3,6), analyzer='char_wb', min_df=2,\n",
        "                               max_features=params['max_features_char'],\n",
        "                               dtype=np.float32)\n",
        "    Xtr_word = word_vec.fit_transform(X_text_tr.iloc[train_idx])\n",
        "    Xtr_char = char_vec.fit_transform(X_text_tr.iloc[train_idx])\n",
        "    Xtr = sparse.hstack([Xtr_word, Xtr_char]).tocsr()\n",
        "    Xva_word = word_vec.transform(X_text_tr.iloc[val_idx])\n",
        "    Xva_char = char_vec.transform(X_text_tr.iloc[val_idx])\n",
        "    Xva = sparse.hstack([Xva_word, Xva_char]).tocsr()\n",
        "    Xte_word = word_vec.transform(X_text_te)\n",
        "    Xte_char = char_vec.transform(X_text_te)\n",
        "    Xte = sparse.hstack([Xte_word, Xte_char]).tocsr()\n",
        "    best_auc, best_pred_va, best_pred_te = -1.0, None, None\n",
        "    for C in params['C_grid']:\n",
        "        clf = LogisticRegression(solver='saga', penalty='l2', C=C,\n",
        "                                 class_weight=params['class_weight'],\n",
        "                                 random_state=seed, max_iter=2000, n_jobs=-1, verbose=0)\n",
        "        t0 = time.time()\n",
        "        clf.fit(Xtr, y[train_idx])\n",
        "        pva = clf.predict_proba(Xva)[:,1]\n",
        "        auc = roc_auc_score(y[val_idx], pva)\n",
        "        print(f\"  C={C} | AUC={auc:.5f} | fit+pred {time.time()-t0:.1f}s\", flush=True)\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_pred_va = auc, pva\n",
        "            best_pred_te = clf.predict_proba(Xte)[:,1]\n",
        "    # Free memory\n",
        "    del Xtr_word, Xtr_char, Xtr, Xva_word, Xva_char, Xva, Xte_word, Xte_char\n",
        "    gc.collect()\n",
        "    return best_pred_va, best_pred_te, best_auc\n",
        "\n",
        "with timer(\"Train across forward-chaining folds\"):\n",
        "    # Use manifest.json to avoid stale chain4 files\n",
        "    manifest_path = fold_dir / 'manifest.json'\n",
        "    if manifest_path.exists():\n",
        "        mf = json.loads(manifest_path.read_text())\n",
        "        chain_ids = [c['chain'] for c in mf.get('chains', [])]\n",
        "    else:\n",
        "        val_files = sorted(fold_dir.glob('fc_chain*_val_idx.npy'))\n",
        "        chain_ids = sorted(int(p.stem.split('chain')[1].split('_')[0]) for p in val_files)\n",
        "    print(\"Detected chains (manifest):\", chain_ids)\n",
        "    for ci in chain_ids:\n",
        "        tr_idx = np.load(fold_dir / f\"fc_chain{ci}_train_idx.npy\")\n",
        "        va_idx = np.load(fold_dir / f\"fc_chain{ci}_val_idx.npy\")\n",
        "        if len(va_idx) == 0:\n",
        "            print(f\"Chain {ci}: empty val; skipping\")\n",
        "            continue\n",
        "        print(f\"Chain {ci}: train {len(tr_idx)} | val {len(va_idx)}\", flush=True)\n",
        "        seed = SEEDS[0]\n",
        "        pva, pte, auc = fit_predict_fold(tr_idx, va_idx, seed)\n",
        "        oof[va_idx] = pva\n",
        "        val_mask[va_idx] = True\n",
        "        test_preds_seeds.append(pte)\n",
        "        print(f\"Chain {ci}: AUC={auc:.5f}\", flush=True)\n",
        "\n",
        "with timer(\"Evaluate OOF and save artifacts\"):\n",
        "    if val_mask.any():\n",
        "        oof_auc_val = roc_auc_score(y[val_mask], oof[val_mask])\n",
        "        print(\"OOF AUC (on validation rows only):\", round(oof_auc_val, 6))\n",
        "    else:\n",
        "        print(\"Warning: no validation rows in mask; OOF AUC not computed.\")\n",
        "    np.save('oof_lr_tfidf_fc.npy', oof)\n",
        "    # Average test predictions across chains (vectorizers differ per chain; this is a smoke check only)\n",
        "    if len(test_preds_seeds):\n",
        "        test_pred = np.mean(np.vstack(test_preds_seeds), axis=0)\n",
        "        np.save('test_lr_tfidf_fc.npy', test_pred)\n",
        "        # Also write a submission for quick smoke check\n",
        "        sub = pd.read_csv('sampleSubmission.csv')\n",
        "        sub['requester_received_pizza'] = np.clip(test_pred, 0.01, 0.99)\n",
        "        sub.to_csv('submission_lr_tfidf_fc.csv', index=False)\n",
        "        print('Saved: oof_lr_tfidf_fc.npy, test_lr_tfidf_fc.npy, submission_lr_tfidf_fc.csv')\n",
        "    else:\n",
        "        print('Warning: no test preds collected.')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Load train/test and prepare text ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Load train/test and prepare text done in 0.13s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params: {'C_grid': [0.5, 1, 2, 4], 'class_weight': 'balanced', 'max_features_word': 200000, 'max_features_char': 300000}\n[T0] Train across forward-chaining folds ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected chains (manifest): [1, 2, 3]\nChain 1: train 1727 | val 562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=0.5 | AUC=0.59809 | fit+pred 1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=1 | AUC=0.58855 | fit+pred 2.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=2 | AUC=0.57875 | fit+pred 3.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=4 | AUC=0.57216 | fit+pred 4.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 1: AUC=0.59809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 2: train 2302 | val 278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=0.5 | AUC=0.66337 | fit+pred 3.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=1 | AUC=0.66329 | fit+pred 4.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=2 | AUC=0.66042 | fit+pred 5.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=4 | AUC=0.65725 | fit+pred 6.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 2: AUC=0.66337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3: train 2590 | val 268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=0.5 | AUC=0.55753 | fit+pred 3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=1 | AUC=0.55519 | fit+pred 4.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=2 | AUC=0.55317 | fit+pred 5.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=4 | AUC=0.55317 | fit+pred 7.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3: AUC=0.55753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Train across forward-chaining folds done in 59.00s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Evaluate OOF and save artifacts ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF AUC (on validation rows only): 0.603979\nSaved: oof_lr_tfidf_fc.npy, test_lr_tfidf_fc.npy, submission_lr_tfidf_fc.csv\n[T+] Evaluate OOF and save artifacts done in 0.01s\n"
          ]
        }
      ]
    },
    {
      "id": "c71fb821-1557-418f-a66f-a22ffec64f74",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Leg B: E5-base-v2 embeddings + XGBoost (GPU, xgb.train+ES), 3-seed bag, tiny grid, with fold-safe per-row meta features + robust kNN-rate (multi-k + softmax + cross-model + recency-decay + bayesian smoothing, per-chain std) [REVERT: drop label variance & gap, tighten XGB]\n",
        "import os, sys, json, gc, math, shutil, subprocess, time, importlib, site\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import xgboost as xgb\n",
        "\n",
        "os.environ['PYTHONNOUSERSITE'] = '1'\n",
        "sys.path = [p for p in sys.path if ('vendor_pkgs' not in str(p)) and ('.pip-target' not in str(p))]\n",
        "\n",
        "def load_json_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path, lines=True)\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return pd.read_json(path, lines=False)\n",
        "        except ValueError:\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            if isinstance(data, dict) and 'data' in data:\n",
        "                data = data['data']\n",
        "            return pd.json_normalize(data)\n",
        "\n",
        "def build_text_cols(df):\n",
        "    title_col_candidates = ['request_title', 'title']\n",
        "    body_col_candidates = ['request_text', 'body', 'text']\n",
        "    def first_col(cols):\n",
        "        for c in cols:\n",
        "            if c in df.columns:\n",
        "                return c\n",
        "        return None\n",
        "    tcol = first_col(title_col_candidates)\n",
        "    bcol = first_col(body_col_candidates)\n",
        "    t = df[tcol].fillna(\"\") if tcol else pd.Series([\"\" for _ in range(len(df))])\n",
        "    b = df[bcol].fillna(\"\") if bcol else pd.Series([\"\" for _ in range(len(df))])\n",
        "    return t.astype(str), b.astype(str)\n",
        "\n",
        "def compute_or_load_e5_embeddings(tr, te, cache_dir=Path(\"emb_cache\")):\n",
        "    cache_dir.mkdir(exist_ok=True)\n",
        "    tr_path = cache_dir / \"emb_e5_train.npy\"\n",
        "    te_path = cache_dir / \"emb_e5_test.npy\"\n",
        "    if tr_path.exists() and te_path.exists():\n",
        "        print(\"Loading cached E5 embeddings ...\")\n",
        "        emb_tr = np.load(tr_path)\n",
        "        emb_te = np.load(te_path)\n",
        "        return emb_tr, emb_te\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    import torch\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model_name = \"intfloat/e5-base-v2\"\n",
        "    print(\"Loading model:\", model_name, \"on\", device)\n",
        "    model = SentenceTransformer(model_name, device=device)\n",
        "    model.max_seq_length = 512\n",
        "    t_title, t_body = build_text_cols(tr)\n",
        "    te_title, te_body = build_text_cols(te)\n",
        "    tr_texts = (\"passage: \" + (t_title.fillna(\"\") + \" \\n \" + t_body.fillna(\"\")).astype(str)).tolist()\n",
        "    te_texts = (\"passage: \" + (te_title.fillna(\"\") + \" \\n \" + te_body.fillna(\"\")).astype(str)).tolist()\n",
        "    def embed(texts, batch_size=128):\n",
        "        out = model.encode(texts, batch_size=batch_size, normalize_embeddings=True, show_progress_bar=True)\n",
        "        return np.asarray(out, dtype=np.float32)\n",
        "    emb_tr = embed(tr_texts); emb_te = embed(te_texts)\n",
        "    np.save(tr_path, emb_tr); np.save(te_path, emb_te)\n",
        "    print(\"Saved embeddings:\", tr_path, te_path)\n",
        "    return emb_tr, emb_te\n",
        "\n",
        "def per_row_meta(df):\n",
        "    # Leak-safe base meta only\n",
        "    title_col_candidates = ['request_title', 'title']\n",
        "    body_col_candidates = ['request_text', 'body', 'text']\n",
        "    time_col_candidates = ['unix_timestamp_of_request_utc', 'request_timestamp', 'created_utc', 'timestamp', 'time']\n",
        "    def first_col(cols):\n",
        "        for c in cols:\n",
        "            if c in df.columns: return c\n",
        "        return None\n",
        "    tcol = first_col(title_col_candidates); bcol = first_col(body_col_candidates); time_col = first_col(time_col_candidates)\n",
        "    t = df[tcol].fillna(\"\").astype(str) if tcol else pd.Series([\"\" for _ in range(len(df))])\n",
        "    b = df[bcol].fillna(\"\").astype(str) if bcol else pd.Series([\"\" for _ in range(len(df))])\n",
        "    def wc(s): return s.str.split().apply(len).astype(np.int32)\n",
        "    def cc(s): return s.str.len().astype(np.int32)\n",
        "    title_wc = wc(t); body_wc = wc(b); title_cc = cc(t); body_cc = cc(b)\n",
        "    ratio_tb = (title_cc / (body_cc + 1)).astype(np.float32)\n",
        "    uniq_ratio = (b.str.lower().str.split().apply(lambda x: len(set(x)) / (len(x)+1e-6))).astype(np.float32)\n",
        "    exclam = b.str.count('!').astype(np.int32); quest = b.str.count('\\?').astype(np.int32)\n",
        "    allcaps = b.apply(lambda s: sum(1 for w in s.split() if len(w)>=3 and w.isupper())).astype(np.int32)\n",
        "    has_url = (b.str.contains('http', case=False, na=False) | b.str.contains('www\\.', case=False, na=False)).astype(np.int8)\n",
        "    has_dollar = b.str.contains('\\u0024|\\$', case=False, na=False).astype(np.int8)\n",
        "    has_digit = b.str.contains('[0-9]', regex=True, na=False).astype(np.int8)\n",
        "    ts = pd.to_numeric(df[time_col], errors='coerce').fillna(0).astype(np.int64) if time_col else pd.Series(np.zeros(len(df), dtype=np.int64))\n",
        "    if ts.max() > 10_000_000_000: ts = (ts // 1_000_000_000).astype(np.int64)\n",
        "    dt = pd.to_datetime(ts, unit='s', utc=True)\n",
        "    month = dt.dt.month.astype(np.int16); wday = dt.dt.weekday.astype(np.int16); hour = dt.dt.hour.astype(np.int16)\n",
        "    feats = np.vstack([\n",
        "        title_wc, body_wc, title_cc, body_cc, ratio_tb, uniq_ratio,\n",
        "        exclam, quest, allcaps, has_url, has_dollar, has_digit,\n",
        "        month, wday, hour\n",
        "    ]).T.astype(np.float32)\n",
        "    return feats\n",
        "\n",
        "def _topk_idx(sims, k):\n",
        "    if k < sims.shape[1]:\n",
        "        return np.argpartition(-sims, kth=k-1, axis=1)[:, :k]\n",
        "    else:\n",
        "        return np.argsort(-sims, axis=1)\n",
        "\n",
        "def knn_rate_mean(pool_emb, pool_y, query_emb, k=50, self_exclude=False):\n",
        "    sims = query_emb @ pool_emb.T\n",
        "    if self_exclude and query_emb.shape[0] == pool_emb.shape[0]:\n",
        "        idx = np.arange(sims.shape[0]); sims[idx, idx] = -1e9\n",
        "    topk = _topk_idx(sims, k)\n",
        "    return pool_y[topk].mean(axis=1).astype(np.float32)\n",
        "\n",
        "def knn_rate_softmax(pool_emb, pool_y, query_emb, k=50, tau=0.12, self_exclude=False):\n",
        "    sims = query_emb @ pool_emb.T\n",
        "    if self_exclude and query_emb.shape[0] == pool_emb.shape[0]:\n",
        "        idx = np.arange(sims.shape[0]); sims[idx, idx] = -1e9\n",
        "    topk = _topk_idx(sims, k)\n",
        "    row_idx = np.arange(topk.shape[0])[:, None]\n",
        "    s_top = sims[row_idx, topk]\n",
        "    w = np.exp(s_top / max(1e-6, tau))\n",
        "    w /= (w.sum(axis=1, keepdims=True) + 1e-9)\n",
        "    y_top = pool_y[topk].astype(np.float32)\n",
        "    return (w * y_top).sum(axis=1).astype(np.float32)\n",
        "\n",
        "def knn_topk(pool_emb, query_emb, k=50, self_exclude=False):\n",
        "    sims = query_emb @ pool_emb.T\n",
        "    if self_exclude and query_emb.shape[0] == pool_emb.shape[0]:\n",
        "        idx = np.arange(sims.shape[0]); sims[idx, idx] = -1e9\n",
        "    topk = _topk_idx(sims, k)\n",
        "    return topk, sims\n",
        "\n",
        "def knn_rate_bayes(pool_emb, pool_y, query_emb, k=50, alpha=20.0, p_train=0.5, self_exclude=False):\n",
        "    topk, sims = knn_topk(pool_emb, query_emb, k=k, self_exclude=self_exclude)\n",
        "    y_top = pool_y[topk].astype(np.float32)\n",
        "    s = y_top.sum(axis=1)\n",
        "    denom = (k + alpha)\n",
        "    return ((s + alpha * p_train) / denom).astype(np.float32)\n",
        "\n",
        "def knn_rate_recency_decay(pool_emb, pool_y, query_emb, pool_ts_days, query_ts_days, k=50, lam_days=75.0, self_exclude=False):\n",
        "    topk, sims = knn_topk(pool_emb, query_emb, k=k, self_exclude=self_exclude)\n",
        "    row_idx = np.arange(topk.shape[0])[:, None]\n",
        "    qd = query_ts_days[row_idx[:,0]][:, None]\n",
        "    pdays = pool_ts_days[topk]\n",
        "    gaps = np.maximum(qd - pdays, 0.0)\n",
        "    w = np.exp(-gaps / max(lam_days, 1e-6))\n",
        "    y_top = pool_y[topk].astype(np.float32)\n",
        "    w_sum = w.sum(axis=1, keepdims=True) + 1e-9\n",
        "    return (w * y_top).sum(axis=1) / w_sum[:,0]\n",
        "\n",
        "from contextlib import contextmanager\n",
        "@contextmanager\n",
        "def timer(msg):\n",
        "    t0 = time.time(); print(f\"[T0] {msg} ...\", flush=True)\n",
        "    try: yield\n",
        "    finally: print(f\"[T+] {msg} done in {time.time()-t0:.2f}s\", flush=True)\n",
        "\n",
        "with timer(\"Load train/test and prepare inputs\"):\n",
        "    tr = load_json_df('train.json')\n",
        "    te = load_json_df('test.json')\n",
        "    time_col = 'unix_timestamp_of_request_utc' if 'unix_timestamp_of_request_utc' in tr.columns else tr.columns[0]\n",
        "    label_col = 'requester_received_pizza' if 'requester_received_pizza' in tr.columns else 'label'\n",
        "    tr = tr.sort_values(time_col, kind='mergesort').reset_index(drop=True)\n",
        "    y = pd.to_numeric(tr[label_col], errors='coerce').fillna(0).astype(int).clip(0,1).values\n",
        "    # timestamps -> days (float)\n",
        "    ts_tr = pd.to_numeric(tr[time_col], errors='coerce').fillna(0).values.astype(np.int64)\n",
        "    ts_te = pd.to_numeric(te[time_col], errors='coerce').fillna(0).values.astype(np.int64) if time_col in te.columns else np.zeros(len(te), dtype=np.int64)\n",
        "    if ts_tr.max() > 10_000_000_000: ts_tr = ts_tr // 1_000_000_000\n",
        "    if ts_te.max() > 10_000_000_000: ts_te = ts_te // 1_000_000_000\n",
        "    ts_tr_days = (ts_tr / 86400.0).astype(np.float32)\n",
        "    ts_te_days = (ts_te / 86400.0).astype(np.float32)\n",
        "    emb_tr, emb_te = compute_or_load_e5_embeddings(tr, te)\n",
        "    # Cross-model pool (BGE) for cross kNN-rate\n",
        "    bge_tr = np.load('emb_cache/emb_bge_train.npy').astype(np.float32)\n",
        "    bge_te = np.load('emb_cache/emb_bge_test.npy').astype(np.float32)\n",
        "    meta_tr = per_row_meta(tr)\n",
        "    meta_te = per_row_meta(te)\n",
        "    print(\"Shapes | emb:\", emb_tr.shape, emb_te.shape, \"meta:\", meta_tr.shape, meta_te.shape, \"| bge:\", bge_tr.shape, bge_te.shape)\n",
        "\n",
        "fold_dir = Path('folds')\n",
        "manifest_path = fold_dir / 'manifest.json'\n",
        "if manifest_path.exists():\n",
        "    mf = json.loads(manifest_path.read_text())\n",
        "    chain_ids = [c['chain'] for c in mf.get('chains', [])]\n",
        "else:\n",
        "    val_files = sorted(fold_dir.glob('fc_chain*_val_idx.npy'))\n",
        "    chain_ids = sorted(int(p.stem.split('chain')[1].split('_')[0]) for p in val_files)\n",
        "print(\"Chains detected (from manifest if available):\", chain_ids)\n",
        "\n",
        "oof = np.zeros(len(tr), dtype=np.float32)\n",
        "val_mask = np.zeros(len(tr), dtype=bool)\n",
        "test_preds_bag = []\n",
        "\n",
        "# Tighter regularization per expert guidance\n",
        "param_grid = [\n",
        "    dict(max_depth=4, eta=0.08, min_child_weight=5),\n",
        "    dict(max_depth=4, eta=0.05, min_child_weight=5),\n",
        "    dict(max_depth=4, eta=0.08, min_child_weight=8),\n",
        "]\n",
        "\n",
        "def predict_with_best(bst, dmat):\n",
        "    bi = getattr(bst, 'best_iteration', None)\n",
        "    if bi is not None:\n",
        "        return bst.predict(dmat, iteration_range=(0, int(bi)+1))\n",
        "    return bst.predict(dmat)\n",
        "\n",
        "def standardize_knn_feats(tr_mat, va_mat, te_mat):\n",
        "    mu = tr_mat.mean(axis=0); sd = tr_mat.std(axis=0); sd = np.where(sd < 1e-6, 1.0, sd)\n",
        "    tr_s = (tr_mat - mu) / sd\n",
        "    va_s = (va_mat - mu) / sd\n",
        "    te_s = (te_mat - mu) / sd\n",
        "    return tr_s.astype(np.float32), va_s.astype(np.float32), te_s.astype(np.float32)\n",
        "\n",
        "def train_one_chain_seed(ci, seed):\n",
        "    tr_idx = np.load(fold_dir / f\"fc_chain{ci}_train_idx.npy\"); va_idx = np.load(fold_dir / f\"fc_chain{ci}_val_idx.npy\")\n",
        "    if len(va_idx) == 0:\n",
        "        print(f\"Chain {ci} seed {seed}: empty val; skip\"); return None, None, None\n",
        "    Xtr_emb, ytr = emb_tr[tr_idx], y[tr_idx]\n",
        "    Xva_emb, yva = emb_tr[va_idx], y[va_idx]\n",
        "    # Same-model kNN rates: k=20,50,100 mean; softmax-weighted (k=50, tau=0.12)\n",
        "    kn_tr_k20 = knn_rate_mean(Xtr_emb, ytr, Xtr_emb, k=20, self_exclude=True).reshape(-1,1)\n",
        "    kn_va_k20 = knn_rate_mean(Xtr_emb, ytr, Xva_emb, k=20, self_exclude=False).reshape(-1,1)\n",
        "    kn_tr_k50 = knn_rate_mean(Xtr_emb, ytr, Xtr_emb, k=50, self_exclude=True).reshape(-1,1)\n",
        "    kn_va_k50 = knn_rate_mean(Xtr_emb, ytr, Xva_emb, k=50, self_exclude=False).reshape(-1,1)\n",
        "    kn_tr_k100 = knn_rate_mean(Xtr_emb, ytr, Xtr_emb, k=100, self_exclude=True).reshape(-1,1)\n",
        "    kn_va_k100 = knn_rate_mean(Xtr_emb, ytr, Xva_emb, k=100, self_exclude=False).reshape(-1,1)\n",
        "    kn_tr_soft = knn_rate_softmax(Xtr_emb, ytr, Xtr_emb, k=50, tau=0.12, self_exclude=True).reshape(-1,1)\n",
        "    kn_va_soft = knn_rate_softmax(Xtr_emb, ytr, Xva_emb, k=50, tau=0.12, self_exclude=False).reshape(-1,1)\n",
        "    # Cross-model rate using BGE embeddings (aligned by rows)\n",
        "    kn_tr_cross = knn_rate_mean(bge_tr[tr_idx], ytr, bge_tr[tr_idx], k=50, self_exclude=True).reshape(-1,1)\n",
        "    kn_va_cross = knn_rate_mean(bge_tr[tr_idx], ytr, bge_tr[va_idx], k=50, self_exclude=False).reshape(-1,1)\n",
        "    # Recency-decayed k=50 (lambda ~ 75 days)\n",
        "    tr_days = ts_tr_days[tr_idx]; va_days = ts_tr_days[va_idx]\n",
        "    kn_tr_dec = knn_rate_recency_decay(Xtr_emb, ytr, Xtr_emb, tr_days, tr_days, k=50, lam_days=75.0, self_exclude=True).reshape(-1,1)\n",
        "    kn_va_dec = knn_rate_recency_decay(Xtr_emb, ytr, Xva_emb, tr_days, va_days, k=50, lam_days=75.0, self_exclude=False).reshape(-1,1)\n",
        "    # Bayesian-smoothed k=50\n",
        "    p_train = float(ytr.mean())\n",
        "    kn_tr_bayes = knn_rate_bayes(Xtr_emb, ytr, Xtr_emb, k=50, alpha=20.0, p_train=p_train, self_exclude=True).reshape(-1,1)\n",
        "    kn_va_bayes = knn_rate_bayes(Xtr_emb, ytr, Xva_emb, k=50, alpha=20.0, p_train=p_train, self_exclude=False).reshape(-1,1)\n",
        "    # Test features against full train pool\n",
        "    kn_te_k20  = knn_rate_mean(emb_tr, y, emb_te, k=20,  self_exclude=False).reshape(-1,1)\n",
        "    kn_te_k50  = knn_rate_mean(emb_tr, y, emb_te, k=50,  self_exclude=False).reshape(-1,1)\n",
        "    kn_te_k100 = knn_rate_mean(emb_tr, y, emb_te, k=100, self_exclude=False).reshape(-1,1)\n",
        "    kn_te_soft = knn_rate_softmax(emb_tr, y, emb_te, k=50, tau=0.12, self_exclude=False).reshape(-1,1)\n",
        "    kn_te_cross = knn_rate_mean(bge_tr, y, bge_te, k=50, self_exclude=False).reshape(-1,1)\n",
        "    te_days = ts_te_days\n",
        "    kn_te_dec = knn_rate_recency_decay(emb_tr, y, emb_te, ts_tr_days, te_days, k=50, lam_days=75.0, self_exclude=False).reshape(-1,1)\n",
        "    p_full = float(y.mean())\n",
        "    kn_te_bayes = knn_rate_bayes(emb_tr, y, emb_te, k=50, alpha=20.0, p_train=p_full, self_exclude=False).reshape(-1,1)\n",
        "    # Assemble KNN features (REVERT: exclude label variance and density gap)\n",
        "    K_tr = np.hstack([kn_tr_k20, kn_tr_k50, kn_tr_k100, kn_tr_soft, kn_tr_cross, kn_tr_dec, kn_tr_bayes])\n",
        "    K_va = np.hstack([kn_va_k20, kn_va_k50, kn_va_k100, kn_va_soft, kn_va_cross, kn_va_dec, kn_va_bayes])\n",
        "    K_te = np.hstack([kn_te_k20, kn_te_k50, kn_te_k100, kn_te_soft, kn_te_cross, kn_te_dec, kn_te_bayes])\n",
        "    K_tr_s, K_va_s, K_te_s = standardize_knn_feats(K_tr, K_va, K_te)\n",
        "    # Final features\n",
        "    Xtr = np.hstack([Xtr_emb, meta_tr[tr_idx], K_tr_s]).astype(np.float32)\n",
        "    Xva = np.hstack([Xva_emb, meta_tr[va_idx], K_va_s]).astype(np.float32)\n",
        "    Xte = np.hstack([emb_te,   meta_te,        K_te_s]).astype(np.float32)\n",
        "    dtr = xgb.DMatrix(Xtr, label=ytr); dva = xgb.DMatrix(Xva, label=yva); dte = xgb.DMatrix(Xte)\n",
        "    pos = int((ytr==1).sum()); neg = int((ytr==0).sum()); spw = float(neg)/max(1.0, float(pos))\n",
        "    base = dict(tree_method='gpu_hist', objective='binary:logistic', eval_metric='auc',\n",
        "                subsample=0.8, colsample_bytree=0.8, reg_lambda=5.0, reg_alpha=0.2,\n",
        "                scale_pos_weight=spw, seed=seed)\n",
        "    best_auc = -1.0; best_pva = None; best_pte = None; best_desc = None\n",
        "    for g in param_grid:\n",
        "        params = base.copy(); params.update(g)\n",
        "        t0 = time.time()\n",
        "        bst = xgb.train(params, dtr, num_boost_round=4000, evals=[(dva,'val')], early_stopping_rounds=120, verbose_eval=False)\n",
        "        pva = predict_with_best(bst, dva)\n",
        "        auc = roc_auc_score(yva, pva)\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_pva = auc, pva\n",
        "            best_pte = predict_with_best(bst, dte)\n",
        "            best_desc = g.copy(); best_desc['best_it'] = getattr(bst, 'best_iteration', None); best_desc['secs'] = time.time()-t0\n",
        "    print(f\"Chain {ci} seed {seed}: best={best_desc} AUC={best_auc:.5f}\", flush=True)\n",
        "    return va_idx, best_pva, best_pte\n",
        "\n",
        "with timer(\"Train E5+XGB+robust kNN across chains and seeds\"):\n",
        "    SEEDS = [42, 1337, 2025]\n",
        "    for seed in SEEDS:\n",
        "        test_preds_per_chain = []\n",
        "        for ci in chain_ids:\n",
        "            res = train_one_chain_seed(ci, seed)\n",
        "            if res is None: continue\n",
        "            va_idx, pva, pte = res\n",
        "            if seed == SEEDS[0]:\n",
        "                oof[va_idx] = pva\n",
        "            else:\n",
        "                oof[va_idx] += pva\n",
        "            val_mask[va_idx] = True\n",
        "            test_preds_per_chain.append(pte)\n",
        "        if len(test_preds_per_chain):\n",
        "            test_preds_bag.append(np.mean(np.vstack(test_preds_per_chain), axis=0))\n",
        "\n",
        "with timer(\"Evaluate and save E5+XGB artifacts (reverted extras)\"):\n",
        "    if val_mask.any():\n",
        "        oof_avg = oof.copy(); oof_avg[val_mask] = oof_avg[val_mask] / 3.0\n",
        "        oof_auc_val = roc_auc_score(y[val_mask], oof_avg[val_mask])\n",
        "        print(\"E5+XGB OOF AUC (val rows only, seed-bag):\", round(oof_auc_val, 6))\n",
        "        np.save('oof_e5_xgb_fc.npy', oof_avg)\n",
        "    else:\n",
        "        print(\"Warning: no validation rows; OOF not computed.\")\n",
        "    if len(test_preds_bag):\n",
        "        test_pred = np.mean(np.vstack(test_preds_bag), axis=0)\n",
        "        np.save('test_e5_xgb_fc.npy', test_pred)\n",
        "        sub = pd.read_csv('sampleSubmission.csv')\n",
        "        sub['requester_received_pizza'] = np.clip(test_pred, 0.01, 0.99)\n",
        "        sub.to_csv('submission_e5_xgb_fc.csv', index=False)\n",
        "        print('Saved: oof_e5_xgb_fc.npy, test_e5_xgb_fc.npy, submission_e5_xgb_fc.csv')\n",
        "    else:\n",
        "        print('Warning: no test preds collected from seeds.')"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Load train/test and prepare inputs ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached E5 embeddings ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes | emb: (2878, 768) (1162, 768) meta: (2878, 15) (1162, 15) | bge: (2878, 384) (1162, 384)\n[T+] Load train/test and prepare inputs done in 0.23s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chains detected (from manifest if available): [1, 2, 3]\n[T0] Train E5+XGB+robust kNN across chains and seeds ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 1 seed 42: best={'max_depth': 4, 'eta': 0.05, 'min_child_weight': 5, 'best_it': 305, 'secs': 1.4255635738372803} AUC=0.63259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 2 seed 42: best={'max_depth': 4, 'eta': 0.05, 'min_child_weight': 5, 'best_it': 102, 'secs': 0.7586708068847656} AUC=0.67676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3 seed 42: best={'max_depth': 4, 'eta': 0.08, 'min_child_weight': 5, 'best_it': 97, 'secs': 0.7526278495788574} AUC=0.65312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 1 seed 1337: best={'max_depth': 4, 'eta': 0.05, 'min_child_weight': 5, 'best_it': 149, 'secs': 0.9070234298706055} AUC=0.61906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 2 seed 1337: best={'max_depth': 4, 'eta': 0.05, 'min_child_weight': 5, 'best_it': 60, 'secs': 0.6201951503753662} AUC=0.66900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3 seed 1337: best={'max_depth': 4, 'eta': 0.08, 'min_child_weight': 5, 'best_it': 96, 'secs': 0.7465951442718506} AUC=0.64619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 1 seed 2025: best={'max_depth': 4, 'eta': 0.08, 'min_child_weight': 8, 'best_it': 287, 'secs': 1.2900474071502686} AUC=0.63995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 2 seed 2025: best={'max_depth': 4, 'eta': 0.05, 'min_child_weight': 5, 'best_it': 40, 'secs': 0.563866376876831} AUC=0.68393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3 seed 2025: best={'max_depth': 4, 'eta': 0.08, 'min_child_weight': 8, 'best_it': 20, 'secs': 0.4767467975616455} AUC=0.66005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Train E5+XGB+robust kNN across chains and seeds done in 24.92s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Evaluate and save E5+XGB artifacts (reverted extras) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E5+XGB OOF AUC (val rows only, seed-bag): 0.610506\nSaved: oof_e5_xgb_fc.npy, test_e5_xgb_fc.npy, submission_e5_xgb_fc.csv\n[T+] Evaluate and save E5+XGB artifacts (reverted extras) done in 0.00s\n"
          ]
        }
      ]
    },
    {
      "id": "d94e280d-5a02-421d-8fb8-49e03b79d916",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Leg C: TF-IDF -> TruncatedSVD + XGBoost (GPU) under forward-chaining folds (migrated to xgb.train + ES)\n",
        "import json, gc, time\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import xgboost as xgb\n",
        "\n",
        "def load_json_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path, lines=True)\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return pd.read_json(path, lines=False)\n",
        "        except ValueError:\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            if isinstance(data, dict) and 'data' in data:\n",
        "                data = data['data']\n",
        "            return pd.json_normalize(data)\n",
        "\n",
        "def build_text_cols(df):\n",
        "    title_col_candidates = ['request_title', 'title']\n",
        "    body_col_candidates = ['request_text', 'body', 'text']\n",
        "    def first_col(cols):\n",
        "        for c in cols:\n",
        "            if c in df.columns: return c\n",
        "        return None\n",
        "    tcol = first_col(title_col_candidates)\n",
        "    bcol = first_col(body_col_candidates)\n",
        "    t = df[tcol].fillna(\"\") if tcol else pd.Series([\"\" for _ in range(len(df))])\n",
        "    b = df[bcol].fillna(\"\") if bcol else pd.Series([\"\" for _ in range(len(df))])\n",
        "    return t.astype(str), b.astype(str)\n",
        "\n",
        "def per_row_meta(df):\n",
        "    title_col_candidates = ['request_title', 'title']\n",
        "    body_col_candidates = ['request_text', 'body', 'text']\n",
        "    time_col_candidates = ['unix_timestamp_of_request_utc', 'request_timestamp', 'created_utc', 'timestamp', 'time']\n",
        "    def first_col(cols):\n",
        "        for c in cols:\n",
        "            if c in df.columns: return c\n",
        "        return None\n",
        "    tcol = first_col(title_col_candidates); bcol = first_col(body_col_candidates)\n",
        "    time_col = first_col(time_col_candidates)\n",
        "    t = df[tcol].fillna(\"\").astype(str) if tcol else pd.Series([\"\" for _ in range(len(df))])\n",
        "    b = df[bcol].fillna(\"\").astype(str) if bcol else pd.Series([\"\" for _ in range(len(df))])\n",
        "    def wc(s): return s.str.split().apply(len).astype(np.int32)\n",
        "    def cc(s): return s.str.len().astype(np.int32)\n",
        "    title_wc = wc(t); body_wc = wc(b); title_cc = cc(t); body_cc = cc(b)\n",
        "    ratio_tb = (title_cc / (body_cc + 1)).astype(np.float32)\n",
        "    uniq_ratio = (b.str.lower().str.split().apply(lambda x: len(set(x)) / (len(x)+1e-6))).astype(np.float32)\n",
        "    exclam = b.str.count('!').astype(np.int32); quest = b.str.count('\\?').astype(np.int32)\n",
        "    allcaps = b.apply(lambda s: sum(1 for w in s.split() if len(w)>=3 and w.isupper())).astype(np.int32)\n",
        "    has_url = (b.str.contains('http', case=False, na=False) | b.str.contains('www\\.', case=False, na=False)).astype(np.int8)\n",
        "    has_dollar = b.str.contains('\\u0024|\\$', case=False, na=False).astype(np.int8)\n",
        "    has_digit = b.str.contains('[0-9]', regex=True, na=False).astype(np.int8)\n",
        "    ts = pd.to_numeric(df[time_col], errors='coerce').fillna(0).astype(np.int64) if time_col else pd.Series(np.zeros(len(df), dtype=np.int64))\n",
        "    if ts.max() > 10_000_000_000: ts = (ts // 1_000_000_000).astype(np.int64)\n",
        "    dt = pd.to_datetime(ts, unit='s', utc=True)\n",
        "    month = dt.dt.month.astype(np.int16); wday = dt.dt.weekday.astype(np.int16); hour = dt.dt.hour.astype(np.int16)\n",
        "    feats = np.vstack([\n",
        "        title_wc, body_wc, title_cc, body_cc, ratio_tb, uniq_ratio,\n",
        "        exclam, quest, allcaps, has_url, has_dollar, has_digit,\n",
        "        month, wday, hour\n",
        "    ]).T.astype(np.float32)\n",
        "    return feats\n",
        "\n",
        "with timer(\"Load data and build text/meta\"):\n",
        "    tr = load_json_df('train.json')\n",
        "    te = load_json_df('test.json')\n",
        "    time_col = 'unix_timestamp_of_request_utc' if 'unix_timestamp_of_request_utc' in tr.columns else tr.columns[0]\n",
        "    label_col = 'requester_received_pizza' if 'requester_received_pizza' in tr.columns else 'label'\n",
        "    tr = tr.sort_values(time_col, kind='mergesort').reset_index(drop=True)\n",
        "    y = pd.to_numeric(tr[label_col], errors='coerce').fillna(0).astype(int).clip(0,1).values\n",
        "    t_title, t_body = build_text_cols(tr)\n",
        "    te_title, te_body = build_text_cols(te)\n",
        "    text_tr = (t_title + ' ' + t_title + ' ' + t_title + ' ' + t_body).astype(str)\n",
        "    text_te = (te_title + ' ' + te_body).astype(str)\n",
        "    meta_tr = per_row_meta(tr)\n",
        "    meta_te = per_row_meta(te)\n",
        "\n",
        "params = {\n",
        "    'svd_dim': 250,\n",
        "    'max_features_word': 200000,\n",
        "    'max_features_char': 200000\n",
        "}\n",
        "print(\"Params:\", params)\n",
        "\n",
        "fold_dir = Path('folds')\n",
        "manifest_path = fold_dir / 'manifest.json'\n",
        "if manifest_path.exists():\n",
        "    mf = json.loads(manifest_path.read_text())\n",
        "    chain_ids = [c['chain'] for c in mf.get('chains', [])]\n",
        "else:\n",
        "    val_files = sorted(fold_dir.glob('fc_chain*_val_idx.npy'))\n",
        "    chain_ids = sorted(int(p.stem.split('chain')[1].split('_')[0]) for p in val_files)\n",
        "print(\"Chains detected:\", chain_ids)\n",
        "\n",
        "oof = np.zeros(len(tr), dtype=np.float32)\n",
        "val_mask = np.zeros(len(tr), dtype=bool)\n",
        "test_preds_per_chain = []\n",
        "\n",
        "param_grid = [\n",
        "    dict(max_depth=4, eta=0.05, min_child_weight=3),\n",
        "    dict(max_depth=4, eta=0.08, min_child_weight=3),\n",
        "    dict(max_depth=5, eta=0.05, min_child_weight=3),\n",
        "    dict(max_depth=5, eta=0.05, min_child_weight=5),\n",
        "]\n",
        "\n",
        "def predict_with_best(bst, dmat):\n",
        "    bi = getattr(bst, 'best_iteration', None)\n",
        "    if bi is not None:\n",
        "        return bst.predict(dmat, iteration_range=(0, int(bi)+1))\n",
        "    return bst.predict(dmat)\n",
        "\n",
        "def fit_predict_chain(ci, include_meta=False):\n",
        "    tr_idx = np.load(fold_dir / f\"fc_chain{ci}_train_idx.npy\"); va_idx = np.load(fold_dir / f\"fc_chain{ci}_val_idx.npy\")\n",
        "    if len(va_idx) == 0:\n",
        "        print(f\"Chain {ci}: empty val; skip\"); return None\n",
        "    assert len(set(tr_idx.tolist()).intersection(set(va_idx.tolist()))) == 0, f\"Index overlap in chain {ci}\"\n",
        "    print(f\"Chain {ci}: vectorizing ...\", flush=True)\n",
        "    word_vec = TfidfVectorizer(ngram_range=(1,2), analyzer='word', min_df=2, max_df=0.98, max_features=params['max_features_word'], dtype=np.float32)\n",
        "    char_vec = TfidfVectorizer(ngram_range=(3,5), analyzer='char_wb', min_df=2, max_features=params['max_features_char'], dtype=np.float32)\n",
        "    Xtr_w = word_vec.fit_transform(text_tr.iloc[tr_idx]); Xva_w = word_vec.transform(text_tr.iloc[va_idx]); Xte_w = word_vec.transform(text_te)\n",
        "    Xtr_c = char_vec.fit_transform(text_tr.iloc[tr_idx]); Xva_c = char_vec.transform(text_tr.iloc[va_idx]); Xte_c = char_vec.transform(text_te)\n",
        "    Xtr_tf = sparse.hstack([Xtr_w, Xtr_c]).tocsr()\n",
        "    Xva_tf = sparse.hstack([Xva_w, Xva_c]).tocsr()\n",
        "    Xte_tf = sparse.hstack([Xte_w, Xte_c]).tocsr()\n",
        "    print(f\"Chain {ci}: SVD ...\", flush=True)\n",
        "    svd = TruncatedSVD(n_components=params['svd_dim'], random_state=42)\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    pipe = make_pipeline(svd, scaler)\n",
        "    Xtr_s = pipe.fit_transform(Xtr_tf).astype(np.float32)\n",
        "    Xva_s = pipe.transform(Xva_tf).astype(np.float32)\n",
        "    Xte_s = pipe.transform(Xte_tf).astype(np.float32)\n",
        "    if include_meta:\n",
        "        Xtr = np.hstack([Xtr_s, meta_tr[tr_idx]]).astype(np.float32)\n",
        "        Xva = np.hstack([Xva_s, meta_tr[va_idx]]).astype(np.float32)\n",
        "        Xte = np.hstack([Xte_s, meta_te]).astype(np.float32)\n",
        "    else:\n",
        "        Xtr, Xva, Xte = Xtr_s, Xva_s, Xte_s\n",
        "    ytr, yva = y[tr_idx], y[va_idx]\n",
        "    pos = int((ytr==1).sum()); neg = int((ytr==0).sum()); spw = float(neg)/max(1.0, float(pos))\n",
        "    dtr = xgb.DMatrix(Xtr, label=ytr); dva = xgb.DMatrix(Xva, label=yva); dte = xgb.DMatrix(Xte)\n",
        "    base = dict(tree_method='gpu_hist', objective='binary:logistic', eval_metric='auc',\n",
        "                subsample=0.85, colsample_bytree=0.9, reg_lambda=3.0, reg_alpha=0.1,\n",
        "                scale_pos_weight=spw, seed=42)\n",
        "    best_auc, best_pva, best_pte, best_desc = -1.0, None, None, None\n",
        "    for g in param_grid:\n",
        "        params_xgb = base.copy(); params_xgb.update(g)\n",
        "        t0 = time.time()\n",
        "        bst = xgb.train(params_xgb, dtr, num_boost_round=3000, evals=[(dva,'val')], early_stopping_rounds=75, verbose_eval=False)\n",
        "        pva = predict_with_best(bst, dva)\n",
        "        auc = roc_auc_score(yva, pva)\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_pva = auc, pva\n",
        "            best_pte = predict_with_best(bst, dte)\n",
        "            best_desc = g.copy(); best_desc['best_it'] = getattr(bst,'best_iteration',None); best_desc['secs'] = time.time()-t0\n",
        "    print(f\"Chain {ci}: best={best_desc} AUC={best_auc:.5f}\", flush=True)\n",
        "    return va_idx, best_pva, best_pte\n",
        "\n",
        "with timer(\"Train SVD+XGB across chains\"):\n",
        "    for ci in chain_ids:\n",
        "        res = fit_predict_chain(ci, include_meta=False)\n",
        "        if res is None: continue\n",
        "        va_idx, pva, pte = res\n",
        "        oof[va_idx] = pva\n",
        "        val_mask[va_idx] = True\n",
        "        test_preds_per_chain.append(pte)\n",
        "\n",
        "with timer(\"Evaluate and save SVD+XGB artifacts\"):\n",
        "    if val_mask.any():\n",
        "        oof_auc = roc_auc_score(y[val_mask], oof[val_mask])\n",
        "        print(\"SVD+XGB OOF AUC (val rows only):\", round(oof_auc, 6))\n",
        "        np.save('oof_tfidf_svd_xgb_fc.npy', oof)\n",
        "    else:\n",
        "        print('Warning: no validation rows; OOF not computed.')\n",
        "    if len(test_preds_per_chain):\n",
        "        test_pred = np.mean(np.vstack(test_preds_per_chain), axis=0)\n",
        "        np.save('test_tfidf_svd_xgb_fc.npy', test_pred)\n",
        "        sub = pd.read_csv('sampleSubmission.csv')\n",
        "        sub['requester_received_pizza'] = np.clip(test_pred, 0.01, 0.99)\n",
        "        sub.to_csv('submission_tfidf_svd_xgb_fc.csv', index=False)\n",
        "        print('Saved: oof_tfidf_svd_xgb_fc.npy, test_tfidf_svd_xgb_fc.npy, submission_tfidf_svd_xgb_fc.csv')\n",
        "    else:\n",
        "        print('Warning: no test preds collected.')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Load data and build text/meta ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Load data and build text/meta done in 0.23s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params: {'svd_dim': 250, 'max_features_word': 200000, 'max_features_char': 200000}\nChains detected: [1, 2, 3]\n[T0] Train SVD+XGB across chains ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 1: vectorizing ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 1: SVD ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 2: SVD ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 2: best={'max_depth': 5, 'eta': 0.05, 'min_child_weight': 5, 'best_it': 0, 'secs': 0.23083972930908203} AUC=0.69185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3: vectorizing ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3: SVD ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3: best={'max_depth': 5, 'eta': 0.05, 'min_child_weight': 5, 'best_it': 209, 'secs': 0.8223862648010254} AUC=0.51725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Train SVD+XGB across chains done in 17.52s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Evaluate and save SVD+XGB artifacts ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVD+XGB OOF AUC (val rows only): 0.591481\nSaved: oof_tfidf_svd_xgb_fc.npy, test_tfidf_svd_xgb_fc.npy, submission_tfidf_svd_xgb_fc.csv\n[T+] Evaluate and save SVD+XGB artifacts done in 0.00s\n"
          ]
        }
      ]
    },
    {
      "id": "a49a3960-84c4-4855-9ebd-5d195e7e497d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Blending v6: prune weak legs, heavier shrink/cap, auto-pick best by OOF AUC (rank space)\n",
        "import json, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def load_json_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path, lines=True)\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return pd.read_json(path, lines=False)\n",
        "        except ValueError:\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            if isinstance(data, dict) and 'data' in data:\n",
        "                data = data['data']\n",
        "            return pd.json_normalize(data)\n",
        "\n",
        "def load_oof_test(prefix):\n",
        "    oof = np.load(f\"oof_{prefix}.npy\").astype(np.float64)\n",
        "    test = np.load(f\"test_{prefix}.npy\").astype(np.float64)\n",
        "    return oof, test\n",
        "\n",
        "def rank_array(x):\n",
        "    order = np.argsort(x)\n",
        "    ranks = np.empty_like(order, dtype=np.float64)\n",
        "    ranks[order] = np.arange(len(x))\n",
        "    r = ranks / max(1.0, (len(x) - 1))\n",
        "    return np.clip(r, 0.01, 0.99)\n",
        "\n",
        "def simplex_grid(L, step=0.05):\n",
        "    alphas = np.arange(0.0, 1.0 + 1e-9, step)\n",
        "    if L == 1:\n",
        "        yield np.array([1.0])\n",
        "    elif L == 2:\n",
        "        for a in alphas:\n",
        "            yield np.array([a, 1.0 - a])\n",
        "    elif L == 3:\n",
        "        for a in alphas:\n",
        "            for b in np.arange(0.0, 1.0 - a + 1e-9, step):\n",
        "                c = 1.0 - a - b\n",
        "                if c < -1e-9: continue\n",
        "                yield np.array([a, b, max(0.0, c)])\n",
        "    elif L == 4:\n",
        "        for a in alphas:\n",
        "            for b in np.arange(0.0, 1.0 - a + 1e-9, step):\n",
        "                for c in np.arange(0.0, 1.0 - a - b + 1e-9, step):\n",
        "                    d = 1.0 - a - b - c\n",
        "                    if d < -1e-9: continue\n",
        "                    yield np.array([a, b, c, max(0.0, d)])\n",
        "    elif L == 5:\n",
        "        for a in alphas:\n",
        "            for b in np.arange(0.0, 1.0 - a + 1e-9, step):\n",
        "                for c in np.arange(0.0, 1.0 - a - b + 1e-9, step):\n",
        "                    for d in np.arange(0.0, 1.0 - a - b - c + 1e-9, step):\n",
        "                        e = 1.0 - a - b - c - d\n",
        "                        if e < -1e-9: continue\n",
        "                        yield np.array([a, b, c, d, max(0.0, e)])\n",
        "    else:\n",
        "        yield np.ones(L) / L\n",
        "\n",
        "def learn_simplex_weights(ranks_val, y_val, step=0.05):\n",
        "    L = ranks_val.shape[0]\n",
        "    best_auc, best_w = -1.0, None\n",
        "    for w in simplex_grid(L, step=step):\n",
        "        auc = roc_auc_score(y_val, np.dot(w, ranks_val))\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_w = auc, w.copy()\n",
        "    return best_w, best_auc\n",
        "\n",
        "def learn_lr_on_ranks(ranks_val, y_val, C_grid=(0.5,1,2,5)):\n",
        "    best_auc, best_w = -1.0, None\n",
        "    X = ranks_val.T\n",
        "    for C in C_grid:\n",
        "        clf = LogisticRegression(penalty='l2', C=C, fit_intercept=False, solver='lbfgs', max_iter=1000)\n",
        "        clf.fit(X, y_val)\n",
        "        w = clf.coef_.ravel().astype(float)\n",
        "        w = np.maximum(0.0, w)\n",
        "        if w.sum() == 0: continue\n",
        "        w = w / w.sum()\n",
        "        auc = roc_auc_score(y_val, np.dot(w, ranks_val))\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_w = auc, w.copy()\n",
        "    return best_w, best_auc\n",
        "\n",
        "def shrink_weights(w, shrink, L):\n",
        "    uniform = np.ones(L) / L\n",
        "    wf = (1.0 - shrink) * w + shrink * uniform\n",
        "    s = wf.sum()\n",
        "    return wf / (s if s > 0 else 1.0)\n",
        "\n",
        "def cap_and_norm(w, cap=0.15):\n",
        "    w = np.minimum(w, cap)\n",
        "    s = w.sum()\n",
        "    if s <= 0:\n",
        "        return np.ones_like(w) / len(w)\n",
        "    return w / s\n",
        "\n",
        "fold_dir = Path('folds')\n",
        "mf = json.loads((fold_dir / 'manifest.json').read_text())\n",
        "chains = [c['chain'] for c in mf['chains']]\n",
        "print('Chains (manifest):', chains)\n",
        "\n",
        "n = len(np.load('oof_lr_tfidf_fc.npy'))\n",
        "val_mask_all = np.zeros(n, dtype=bool)\n",
        "val_mask_c2 = np.zeros(n, dtype=bool)\n",
        "val_mask_c23 = np.zeros(n, dtype=bool)\n",
        "val_mask_c3 = np.zeros(n, dtype=bool)\n",
        "for ci in chains:\n",
        "    va_idx = np.load(fold_dir / f\"fc_chain{ci}_val_idx.npy\")\n",
        "    val_mask_all[va_idx] = True\n",
        "    if ci == 2: val_mask_c2[va_idx] = True\n",
        "    if ci in (2,3): val_mask_c23[va_idx] = True\n",
        "    if ci == 3: val_mask_c3[va_idx] = True\n",
        "\n",
        "# Pruned legs: drop mpnet_xgb_fc and tfidf_svd_xgb_fc per expert advice\n",
        "# Include DeBERTa FT, RoBERTa FT, meta, embeddings, TF-IDF legs; include wordlr_fc and nbsvm_fc\n",
        "all_leg_names = ['meta_xgb_fc', 'e5_xgb_fc', 'bge_xgb_fc', 'lr_tfidf_fc', 'wordlr_fc', 'charlr_fc', 'roberta_ft_fc', 'deberta_ft_fc', 'nbsvm_fc']\n",
        "legs = []\n",
        "for name in all_leg_names:\n",
        "    try:\n",
        "        oof, test = load_oof_test(name)\n",
        "        assert len(oof) == n\n",
        "        legs.append((name, oof, test))\n",
        "        print('Loaded leg:', name)\n",
        "    except Exception as e:\n",
        "        print('Skip leg:', name, '|', repr(e))\n",
        "assert len(legs) >= 3\n",
        "\n",
        "# Labels\n",
        "tr = load_json_df('train.json')\n",
        "time_col = mf.get('time_col', 'unix_timestamp_of_request_utc')\n",
        "label_col = mf.get('label_col', 'requester_received_pizza')\n",
        "tr = tr.sort_values(time_col, kind='mergesort').reset_index(drop=True)\n",
        "y = pd.to_numeric(tr[label_col], errors='coerce').fillna(0).astype(int).clip(0,1).values\n",
        "\n",
        "def prepare_rank_mats(cur_legs):\n",
        "    ranks_c2 = []; ranks_c23 = []; ranks_c3 = []; ranks_all = []; ranks_test = []; probs_test = []\n",
        "    for _, oof, test in cur_legs:\n",
        "        ranks_c2.append(rank_array(oof[val_mask_c2]))\n",
        "        ranks_c23.append(rank_array(oof[val_mask_c23]))\n",
        "        ranks_c3.append(rank_array(oof[val_mask_c3]))\n",
        "        r_all = np.zeros(n, dtype=np.float64); r_all[val_mask_all] = rank_array(oof[val_mask_all])\n",
        "        ranks_all.append(r_all)\n",
        "        ranks_test.append(rank_array(test))\n",
        "        probs_test.append(test.astype(np.float64))\n",
        "    return dict(\n",
        "        ranks_c2=np.vstack(ranks_c2),\n",
        "        ranks_c23=np.vstack(ranks_c23),\n",
        "        ranks_c3=np.vstack(ranks_c3),\n",
        "        ranks_all=np.vstack(ranks_all),\n",
        "        ranks_test=np.vstack(ranks_test),\n",
        "        probs_test=np.vstack(probs_test)\n",
        "    )\n",
        "\n",
        "legs_pruned = [(n,o,t) for (n,o,t) in legs]\n",
        "legs_transformers = [(n,o,t) for (n,o,t) in legs_pruned if n in ('e5_xgb_fc','bge_xgb_fc','roberta_ft_fc','deberta_ft_fc')]\n",
        "\n",
        "def optimize_simplex_on(mask_key, cur_legs, grid_step=0.05):\n",
        "    mats = prepare_rank_mats(cur_legs)\n",
        "    key_map = {'c23':'ranks_c23','c2':'ranks_c2','c3':'ranks_c3'}\n",
        "    R = mats[key_map[mask_key]]\n",
        "    w,_ = learn_simplex_weights(R, y[val_mask_c23 if mask_key=='c23' else (val_mask_c2 if mask_key=='c2' else val_mask_c3)], step=grid_step)\n",
        "    return w, mats\n",
        "\n",
        "def write_submission(tag, w, mats, cur_legs, shrink=0.38, cap=0.15):\n",
        "    L = len(cur_legs)\n",
        "    w = shrink_weights(w, shrink, L)\n",
        "    w = cap_and_norm(w, cap=cap)\n",
        "    ranks_all = mats['ranks_all']; ranks_test = mats['ranks_test']\n",
        "    oof_blend = np.dot(w, ranks_all)\n",
        "    oof_auc = roc_auc_score(y[val_mask_all], oof_blend[val_mask_all])\n",
        "    test_rank = np.dot(w, ranks_test)\n",
        "    test_final = test_rank\n",
        "    np.save(f'oof_blend_{tag}.npy', oof_blend)\n",
        "    np.save(f'test_blend_{tag}.npy', test_rank)\n",
        "    sub = pd.read_csv('sampleSubmission.csv')\n",
        "    sub['requester_received_pizza'] = test_final\n",
        "    fn = f'submission_blend_fc_{tag}.csv'\n",
        "    sub.to_csv(fn, index=False)\n",
        "    print(f'{tag}: OOF AUC={oof_auc:.6f} -> wrote {fn}')\n",
        "    return fn, oof_auc, w\n",
        "\n",
        "cands = []  # (tag, auc, fn)\n",
        "\n",
        "# A) Simplex on C2+3 (pruned legs), heavy shrink/cap\n",
        "mats_base = prepare_rank_mats(legs_pruned)\n",
        "wA,_ = learn_simplex_weights(mats_base['ranks_c23'], y[val_mask_c23], step=(0.02 if len(legs_pruned)<=4 else 0.05))\n",
        "if wA is not None:\n",
        "    fnA, aucA, _ = write_submission('A_simplex_c23_s38_c15', wA, mats_base, legs_pruned, shrink=0.38, cap=0.15)\n",
        "    cands.append(('A_simplex_c23_s38_c15', aucA, fnA))\n",
        "\n",
        "# B) LR on ranks C2+3 (pruned), heavy shrink/cap\n",
        "wB,_ = learn_lr_on_ranks(mats_base['ranks_c23'], y[val_mask_c23])\n",
        "if wB is not None:\n",
        "    fnB, aucB, _ = write_submission('B_lr_c23_s38_c15', wB, mats_base, legs_pruned, shrink=0.38, cap=0.15)\n",
        "    cands.append(('B_lr_c23_s38_c15', aucB, fnB))\n",
        "\n",
        "# C) Time-decayed 0.7*C3 + 0.3*C2 (pruned), heavy shrink/cap\n",
        "w2_td,_ = learn_simplex_weights(mats_base['ranks_c2'], y[val_mask_c2], step=(0.02 if len(legs_pruned)<=4 else 0.05))\n",
        "w3_td,_ = learn_simplex_weights(mats_base['ranks_c3'], y[val_mask_c3], step=(0.02 if len(legs_pruned)<=4 else 0.05))\n",
        "if (w2_td is not None) and (w3_td is not None):\n",
        "    def l2(v): return v / (np.linalg.norm(v) + 1e-12)\n",
        "    wC = 0.7*l2(w3_td) + 0.3*l2(w2_td)\n",
        "    wC = np.maximum(0.0, wC); wC = wC / wC.sum()\n",
        "    fnC, aucC, _ = write_submission('C_time_decay_70_30_s38_c15', wC, mats_base, legs_pruned, shrink=0.38, cap=0.15)\n",
        "    cands.append(('C_time_decay_70_30_s38_c15', aucC, fnC))\n",
        "\n",
        "# D) Transformers-only (E5, BGE, RoBERTa, DeBERTa) as a backup\n",
        "if len(legs_transformers) >= 2:\n",
        "    mats_tx = prepare_rank_mats(legs_transformers)\n",
        "    wD,_ = learn_simplex_weights(mats_tx['ranks_c23'], y[val_mask_c23], step=(0.02 if len(legs_transformers)<=4 else 0.05))\n",
        "    if wD is not None:\n",
        "        fnD, aucD, _ = write_submission('D_tx_only_c23_s38_c15', wD, mats_tx, legs_transformers, shrink=0.38, cap=0.15)\n",
        "        cands.append(('D_tx_only_c23_s38_c15', aucD, fnD))\n",
        "\n",
        "cands.sort(key=lambda x: x[1], reverse=True)\n",
        "print('Blend candidates (sorted by OOF AUC on all val rows after shrink/cap):')\n",
        "for name, auc, fn in cands:\n",
        "    print(' ', name, '| AUC=', round(auc,6), '| file=', fn)\n",
        "\n",
        "# Auto-pick best by OOF AUC\n",
        "primary = cands[0][2] if len(cands) else None\n",
        "if primary is not None:\n",
        "    pd.read_csv(primary).to_csv('submission_blend_fc.csv', index=False)\n",
        "    print('Primary submission_blend_fc.csv <-', primary)\n",
        "else:\n",
        "    print('No candidates produced; keeping previous submission_blend_fc.csv if exists')"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chains (manifest): [1, 2, 3]\nLoaded leg: meta_xgb_fc\nLoaded leg: e5_xgb_fc\nLoaded leg: bge_xgb_fc\nLoaded leg: lr_tfidf_fc\nLoaded leg: wordlr_fc\nLoaded leg: charlr_fc\nLoaded leg: roberta_ft_fc\nLoaded leg: deberta_ft_fc\nLoaded leg: nbsvm_fc\nA_simplex_c23_s38_c15: OOF AUC=0.641767 -> wrote submission_blend_fc_A_simplex_c23_s38_c15.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B_lr_c23_s38_c15: OOF AUC=0.640458 -> wrote submission_blend_fc_B_lr_c23_s38_c15.csv\nC_time_decay_70_30_s38_c15: OOF AUC=0.641767 -> wrote submission_blend_fc_C_time_decay_70_30_s38_c15.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D_tx_only_c23_s38_c15: OOF AUC=0.631174 -> wrote submission_blend_fc_D_tx_only_c23_s38_c15.csv\nBlend candidates (sorted by OOF AUC on all val rows after shrink/cap):\n  A_simplex_c23_s38_c15 | AUC= 0.641767 | file= submission_blend_fc_A_simplex_c23_s38_c15.csv\n  C_time_decay_70_30_s38_c15 | AUC= 0.641767 | file= submission_blend_fc_C_time_decay_70_30_s38_c15.csv\n  B_lr_c23_s38_c15 | AUC= 0.640458 | file= submission_blend_fc_B_lr_c23_s38_c15.csv\n  D_tx_only_c23_s38_c15 | AUC= 0.631174 | file= submission_blend_fc_D_tx_only_c23_s38_c15.csv\nPrimary submission_blend_fc.csv <- submission_blend_fc_A_simplex_c23_s38_c15.csv\n"
          ]
        }
      ]
    },
    {
      "id": "ff20b3a3-33d3-4a39-978a-002e04a03bf7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create submission.csv from the current primary blend selected by Cell 12\n",
        "import pandas as pd, os\n",
        "src = 'submission_blend_fc.csv'\n",
        "assert os.path.exists(src), f\"Primary blend file {src} not found; run Cell 12 first\"\n",
        "dst = 'submission.csv'\n",
        "pd.read_csv(src).to_csv(dst, index=False)\n",
        "print('Wrote submission.csv from', src)\n",
        "print(pd.read_csv(dst).head())"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv from submission_blend_fc.csv\n  request_id  requester_received_pizza\n0  t3_1aw5zf                  0.336286\n1   t3_roiuw                  0.573397\n2   t3_mjnbq                  0.594069\n3   t3_t8wd1                  0.546081\n4  t3_1m4zxu                  0.556294\n"
          ]
        }
      ]
    },
    {
      "id": "d81714c7-62d8-42ea-9893-8b41c26f2bab",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Leg M: Meta-only XGBoost (GPU) with strict leak bans + minimal safe features + sentinels + fold-safe user history\n",
        "import json, time, re\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import xgboost as xgb\n",
        "\n",
        "def load_json_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path, lines=True)\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return pd.read_json(path, lines=False)\n",
        "        except ValueError:\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            if isinstance(data, dict) and 'data' in data:\n",
        "                data = data['data']\n",
        "            return pd.json_normalize(data)\n",
        "\n",
        "def drop_banned_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    banned_tokens = [\n",
        "        'requester_received_pizza','received_pizza','target','label','y',\n",
        "        'request_text_edit_aware','edit','edited',\n",
        "        'retrieval',\n",
        "        'upvote','downvote','votes','karma','score',\n",
        "        'giver_username_if_known','number_of_recipients',\n",
        "        'account_age','days_since_first_post_on_raop','number_of_posts','number_of_comments','posts_on_raop','comments_in_raop',\n",
        "        'success'\n",
        "    ]\n",
        "    low = [c.lower() for c in df.columns]\n",
        "    keep = []\n",
        "    for c, cl in zip(df.columns, low):\n",
        "        if any(tok in cl for tok in banned_tokens):\n",
        "            continue\n",
        "        keep.append(c)\n",
        "    df2 = df[keep].copy()\n",
        "    assert 'request_text_edit_aware' not in df2.columns, 'edit-aware text present'\n",
        "    assert all('edit' not in c.lower() for c in df2.columns), 'any *edit* column present'\n",
        "    return df2\n",
        "\n",
        "def meta_features_minimal(df: pd.DataFrame) -> np.ndarray:\n",
        "    # Only from request_title and request_text + calendar + safe lexical flags. No requester_* fields.\n",
        "    def first_col(cols):\n",
        "        for c in cols:\n",
        "            if c in df.columns: return c\n",
        "        return None\n",
        "    tcol = first_col(['request_title','title'])\n",
        "    bcol = first_col(['request_text'])  # hard-ban body/text aliases to avoid edit-aware traps\n",
        "    # Guards\n",
        "    if bcol is None:\n",
        "        b = pd.Series(['']*len(df), index=df.index)\n",
        "    else:\n",
        "        assert 'edit' not in bcol.lower(), f'Body column is edit-aware: {bcol}'\n",
        "        b = df[bcol].fillna('').astype(str)\n",
        "    t = df[tcol].fillna('').astype(str) if tcol else pd.Series(['']*len(df), index=df.index)\n",
        "    # Base lengths and simple punctuation\n",
        "    def wc(s): return s.str.split().apply(len).astype(np.int32)\n",
        "    def cc(s): return s.str.len().astype(np.int32)\n",
        "    title_wc = wc(t); body_wc = wc(b); title_cc = cc(t); body_cc = cc(b)\n",
        "    ratio_tb = (title_cc / (body_cc + 1)).astype(np.float32)\n",
        "    uniq_ratio = (b.str.lower().str.split().apply(lambda x: len(set(x)) / (len(x)+1e-6))).astype(np.float32)\n",
        "    exclam = b.str.count('!').astype(np.int32); quest = b.str.count('\\?').astype(np.int32)\n",
        "    allcaps = b.apply(lambda s: sum(1 for w in s.split() if len(w)>=3 and w.isupper())).astype(np.int32)\n",
        "    # Calendar from unix_timestamp_of_request_utc (or similar)\n",
        "    time_col = None\n",
        "    for c in ['unix_timestamp_of_request_utc','request_timestamp','created_utc','timestamp','time']:\n",
        "        if c in df.columns: time_col = c; break\n",
        "    ts = pd.to_numeric(df[time_col], errors='coerce').fillna(0).astype(np.int64) if time_col else pd.Series(np.zeros(len(df), dtype=np.int64))\n",
        "    if ts.max() > 10_000_000_000: ts = (ts // 1_000_000_000).astype(np.int64)\n",
        "    dt = pd.to_datetime(ts, unit='s', utc=True)\n",
        "    month = dt.dt.month.astype(np.int16); wday = dt.dt.weekday.astype(np.int16); hour = dt.dt.hour.astype(np.int16)\n",
        "    # Safe lexical flags\n",
        "    lower_b = b.str.lower()\n",
        "    # URL patterns and counts\n",
        "    url_pat = r'https?://\\S+|www\\.[^\\s]+'\n",
        "    url_count = b.str.count(url_pat, flags=re.IGNORECASE).astype(np.int16)\n",
        "    has_url = (url_count > 0).astype(np.int8)\n",
        "    has_imgur = lower_b.str.contains('imgur.com', na=False).astype(np.int8)\n",
        "    tld_com = lower_b.str.contains('\\u002ecom|\\.com', na=False).astype(np.int8)\n",
        "    tld_org = lower_b.str.contains('\\u002eorg|\\.org', na=False).astype(np.int8)\n",
        "    tld_net = lower_b.str.contains('\\u002enet|\\.net', na=False).astype(np.int8)\n",
        "    # Currency and numbers\n",
        "    currency_pat = r'(\\$|usd|dollar|dollars)'\n",
        "    currency_count = lower_b.str.count(currency_pat, flags=re.IGNORECASE).astype(np.int16)\n",
        "    has_usd = lower_b.str.contains('usd', na=False).astype(np.int8)\n",
        "    has_dollar_word = lower_b.str.contains('dollar', na=False).astype(np.int8)\n",
        "    has_dollar_symbol = b.str.contains('\\u0024|\\$', na=False).astype(np.int8)\n",
        "    numbers_count = b.str.count(r'\\d').astype(np.int16)\n",
        "    # Politeness/thanks\n",
        "    please_thank_pat = r'(please|thank|thanks|grateful|appreciate)'\n",
        "    please_thank_count = lower_b.str.count(please_thank_pat, flags=re.IGNORECASE).astype(np.int16)\n",
        "    # First-person pronouns rate\n",
        "    fp_pat = r'\\b(i|me|my|mine|i\\'m|i\\'ve|i\\'d|i\\'ll)\\b'\n",
        "    fp_count = lower_b.str.count(fp_pat, flags=re.IGNORECASE).astype(np.int16)\n",
        "    first_person_rate = (fp_count / (body_wc.replace(0,1))).astype(np.float32)\n",
        "    # Hardship/urgency cues\n",
        "    hardship_pat = r'(rent|bill|bills|job|unemploy|hungry|broke|student|finals|paycheck|family|kids|today|tonight|asap|tldr)'\n",
        "    hardship_count = lower_b.str.count(hardship_pat, flags=re.IGNORECASE).astype(np.int16)\n",
        "    # Negations\n",
        "    neg_pat = r\"\\b(no|not|never|n't)\\b\"\n",
        "    negation_count = lower_b.str.count(neg_pat, flags=re.IGNORECASE).astype(np.int16)\n",
        "    # Title question mark flag\n",
        "    title_has_qmark = t.str.contains('\\?', na=False).astype(np.int8)\n",
        "    # Emoji count (basic range; broad approximation)\n",
        "    emoji_pat = r'[\\U0001F300-\\U0001F6FF\\U0001F900-\\U0001F9FF\\U0001F1E6-\\U0001F1FF\\u2600-\\u26FF\\u2700-\\u27BF]'\n",
        "    try:\n",
        "        emoji_count = b.str.count(emoji_pat).astype(np.int16)\n",
        "    except re.error:\n",
        "        # Some engines may not support the full range; fallback to zero\n",
        "        emoji_count = pd.Series([0]*len(b), index=b.index, dtype='int16')\n",
        "    # Assemble features\n",
        "    feats = np.vstack([\n",
        "        title_wc, body_wc, title_cc, body_cc, ratio_tb, uniq_ratio,\n",
        "        exclam, quest, allcaps,\n",
        "        month, wday, hour,\n",
        "        # lexical flags\n",
        "        url_count, has_url, has_imgur, tld_com, tld_org, tld_net,\n",
        "        currency_count, has_usd, has_dollar_word, has_dollar_symbol, numbers_count,\n",
        "        please_thank_count, first_person_rate, hardship_count, negation_count,\n",
        "        title_has_qmark, emoji_count\n",
        "    ]).T.astype(np.float32)\n",
        "    return feats\n",
        "\n",
        "def build_user_history_fold_safe(tr_all_sorted: pd.DataFrame, te_df: pd.DataFrame, group_col: str, time_col: str, label_col: str, fold_dir: Path, chains: list, alpha: float = 20.0) -> tuple[np.ndarray, np.ndarray]:\n",
        "    # Returns history features aligned to train rows (shape (n, F)) and test rows (shape (T, F))\n",
        "    n = len(tr_all_sorted); T = len(te_df)\n",
        "    H_tr = np.zeros((n, 4), dtype=np.float32)  # [log1p(prior_count), smoothed_rate, days_since_prev, log1p(days_since_first)]\n",
        "    # Global prior for smoothing uses train-only per chain; compute per-chain then fill\n",
        "    for ci in chains:\n",
        "        tr_idx = np.load(fold_dir / f'fc_chain{ci}_train_idx.npy')\n",
        "        va_idx = np.load(fold_dir / f'fc_chain{ci}_val_idx.npy')\n",
        "        if len(tr_idx) == 0: continue\n",
        "        sub_tr = tr_all_sorted.iloc[tr_idx].copy()\n",
        "        sub_tr = sub_tr.sort_values(time_col, kind='mergesort')\n",
        "        gp = sub_tr.groupby(group_col, sort=False, observed=True)\n",
        "        # Train-row features (per-row cum stats within train window)\n",
        "        prior_cnt = gp.cumcount().astype(np.int64).values  # count before\n",
        "        succ = pd.to_numeric(sub_tr[label_col], errors='coerce').fillna(0).astype(int).clip(0,1)\n",
        "        prior_succ = gp[succ.name].cumsum().shift(1).fillna(0).astype(np.int64).values\n",
        "        prev_ts = gp[time_col].shift(1).fillna(np.nan).values.astype('float64')\n",
        "        cur_ts = sub_tr[time_col].values.astype('float64')\n",
        "        days_since_prev = np.where(np.isnan(prev_ts), 0.0, (cur_ts - prev_ts) / 86400.0).astype(np.float32)\n",
        "        first_ts = gp[time_col].transform('min').values.astype('float64')\n",
        "        days_since_first = np.maximum((cur_ts - first_ts) / 86400.0, 0.0).astype(np.float32)\n",
        "        p_global = float(succ.mean()) if len(sub_tr) else 0.5\n",
        "        rate_sm = ((prior_succ + alpha * p_global) / (prior_cnt + alpha)).astype(np.float32)\n",
        "        H_tr_trwin = np.vstack([np.log1p(prior_cnt).astype(np.float32), rate_sm, days_since_prev, np.log1p(days_since_first)]).T.astype(np.float32)\n",
        "        # Map back to H_tr at train indices order\n",
        "        H_tr[tr_idx] = H_tr_trwin\n",
        "        # Validation rows get aggregate stats from train window only\n",
        "        if len(va_idx):\n",
        "            agg = gp.agg({label_col:'sum', time_col:['min','max','count']})\n",
        "            agg.columns = ['succ_sum','first_ts','last_ts','cnt']\n",
        "            # Build maps\n",
        "            succ_map = agg['succ_sum'].to_dict()\n",
        "            first_map = agg['first_ts'].to_dict()\n",
        "            last_map = agg['last_ts'].to_dict()\n",
        "            cnt_map = agg['cnt'].to_dict()\n",
        "            users_va = tr_all_sorted.iloc[va_idx][group_col].astype(str).values\n",
        "            cur_ts_va = tr_all_sorted.iloc[va_idx][time_col].values.astype('float64')\n",
        "            prior_cnt_va = np.array([cnt_map.get(u, 0) for u in users_va], dtype=np.int64)\n",
        "            prior_succ_va = np.array([succ_map.get(u, 0) for u in users_va], dtype=np.int64)\n",
        "            last_ts_va = np.array([last_map.get(u, cur_ts_va[i]) for i,u in enumerate(users_va)], dtype='float64')\n",
        "            first_ts_va = np.array([first_map.get(u, cur_ts_va[i]) for i,u in enumerate(users_va)], dtype='float64')\n",
        "            days_prev_va = np.maximum((cur_ts_va - last_ts_va)/86400.0, 0.0).astype(np.float32)\n",
        "            days_first_va = np.maximum((cur_ts_va - first_ts_va)/86400.0, 0.0).astype(np.float32)\n",
        "            rate_sm_va = ((prior_succ_va + alpha * p_global) / (prior_cnt_va + alpha)).astype(np.float32)\n",
        "            H_tr_va = np.vstack([np.log1p(prior_cnt_va).astype(np.float32), rate_sm_va, days_prev_va, np.log1p(days_first_va)]).T.astype(np.float32)\n",
        "            H_tr[va_idx] = H_tr_va\n",
        "    # Test features: build aggregates from full training (all rows) and apply to test\n",
        "    te_users = te_df.get('requester_username', None)\n",
        "    if te_users is None and 'username' in te_df.columns:\n",
        "        te_users = te_df['username']\n",
        "    if te_users is None:\n",
        "        te_users = pd.Series(['']*len(te_df))\n",
        "    tr_full = tr_all_sorted.copy()\n",
        "    gp_full = tr_full.groupby(group_col, sort=False, observed=True)\n",
        "    succ_full = pd.to_numeric(tr_full[label_col], errors='coerce').fillna(0).astype(int).clip(0,1)\n",
        "    agg_full = gp_full.agg({label_col:'sum', time_col:['min','max','count']})\n",
        "    agg_full.columns = ['succ_sum','first_ts','last_ts','cnt']\n",
        "    p_global_full = float(succ_full.mean()) if len(tr_full) else 0.5\n",
        "    succ_map_f = agg_full['succ_sum'].to_dict()\n",
        "    first_map_f = agg_full['first_ts'].to_dict()\n",
        "    last_map_f = agg_full['last_ts'].to_dict()\n",
        "    cnt_map_f = agg_full['cnt'].to_dict()\n",
        "    cur_ts_te = pd.to_numeric(te_df[time_col], errors='coerce').fillna(0).values.astype('float64') if time_col in te_df.columns else np.zeros(len(te_df), dtype='float64')\n",
        "    users_te = te_users.astype(str).values\n",
        "    prior_cnt_te = np.array([cnt_map_f.get(u, 0) for u in users_te], dtype=np.int64)\n",
        "    prior_succ_te = np.array([succ_map_f.get(u, 0) for u in users_te], dtype=np.int64)\n",
        "    last_ts_te = np.array([last_map_f.get(u, cur_ts_te[i]) for i,u in enumerate(users_te)], dtype='float64')\n",
        "    first_ts_te = np.array([first_map_f.get(u, cur_ts_te[i]) for i,u in enumerate(users_te)], dtype='float64')\n",
        "    days_prev_te = np.maximum((cur_ts_te - last_ts_te)/86400.0, 0.0).astype(np.float32)\n",
        "    days_first_te = np.maximum((cur_ts_te - first_ts_te)/86400.0, 0.0).astype(np.float32)\n",
        "    rate_sm_te = ((prior_succ_te + alpha * p_global_full) / (prior_cnt_te + alpha)).astype(np.float32)\n",
        "    H_te = np.vstack([np.log1p(prior_cnt_te).astype(np.float32), rate_sm_te, days_prev_te, np.log1p(days_first_te)]).T.astype(np.float32)\n",
        "    return H_tr, H_te\n",
        "\n",
        "from contextlib import contextmanager\n",
        "@contextmanager\n",
        "def timer(msg):\n",
        "    t0 = time.time(); print(f\"[T0] {msg} ...\", flush=True)\n",
        "    try: yield\n",
        "    finally: print(f\"[T+] {msg} done in {time.time()-t0:.2f}s\", flush=True)\n",
        "\n",
        "fold_dir = Path('folds')\n",
        "manifest_path = fold_dir / 'manifest.json'\n",
        "mf = json.loads(manifest_path.read_text()) if manifest_path.exists() else None\n",
        "\n",
        "with timer('Load data, apply hard bans, and build minimal-safe meta + user history (fold-safe)'):\n",
        "    tr_raw = load_json_df('train.json')\n",
        "    te_raw = load_json_df('test.json')\n",
        "    tr = drop_banned_columns(tr_raw)\n",
        "    te = drop_banned_columns(te_raw)\n",
        "    time_col = (mf.get('time_col') if mf else ('unix_timestamp_of_request_utc' if 'unix_timestamp_of_request_utc' in tr.columns else tr.columns[0]))\n",
        "    label_col = (mf.get('label_col') if mf else ('requester_received_pizza' if 'requester_received_pizza' in tr_raw.columns else 'label'))\n",
        "    group_col = (mf.get('group_col') if mf else ('requester_username' if 'requester_username' in tr_raw.columns else 'username'))\n",
        "    tr = tr.sort_values(time_col, kind='mergesort').reset_index(drop=True)\n",
        "    tr_raw_sorted = tr_raw.sort_values(time_col, kind='mergesort').reset_index(drop=True)\n",
        "    y = pd.to_numeric(tr_raw_sorted[label_col], errors='coerce').fillna(0).astype(int).clip(0,1).values\n",
        "    X_meta_tr_base = meta_features_minimal(tr)\n",
        "    X_meta_te_base = meta_features_minimal(te)\n",
        "    # Fold-safe user history features\n",
        "    if mf is not None:\n",
        "        chain_ids = [c['chain'] for c in mf.get('chains', [])]\n",
        "    else:\n",
        "        val_files = sorted(fold_dir.glob('fc_chain*_val_idx.npy'))\n",
        "        chain_ids = sorted(int(p.stem.split('chain')[1].split('_')[0]) for p in val_files)\n",
        "    H_tr, H_te = build_user_history_fold_safe(tr_raw_sorted, te_raw, group_col, time_col, label_col, fold_dir, chain_ids, alpha=20.0)\n",
        "    X_meta_tr = np.hstack([X_meta_tr_base, H_tr]).astype(np.float32)\n",
        "    X_meta_te = np.hstack([X_meta_te_base, H_te]).astype(np.float32)\n",
        "    print('Meta+History shapes:', X_meta_tr.shape, X_meta_te.shape)\n",
        "\n",
        "print('Chains detected for meta leg:', [c['chain'] for c in mf.get('chains', [])] if mf else 'unknown')\n",
        "\n",
        "# Sanity A: zero-sentinel to ensure pipeline isn't leaking\n",
        "def sanity_zero_check():\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    oof_zero = np.zeros(len(y), dtype=np.float32)\n",
        "    val_mask = np.zeros(len(y), dtype=bool)\n",
        "    if mf is not None:\n",
        "        for ci in [c['chain'] for c in mf.get('chains', [])]:\n",
        "            va_idx = np.load(fold_dir / f'fc_chain{ci}_val_idx.npy')\n",
        "            val_mask[va_idx] = True\n",
        "    try:\n",
        "        auc = roc_auc_score(y[val_mask], oof_zero[val_mask]) if val_mask.any() else 0.5\n",
        "    except Exception:\n",
        "        auc = 0.5\n",
        "    print('Sanity A (zeros) AUC on val rows:', round(auc, 6))\n",
        "    return auc\n",
        "\n",
        "# Sanity B: single-feature AUC sentinels\n",
        "def sanity_single_feature_scan(X):\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    val_mask = np.zeros(len(y), dtype=bool)\n",
        "    if mf is not None:\n",
        "        for ci in [c['chain'] for c in mf.get('chains', [])]:\n",
        "            va_idx = np.load(fold_dir / f'fc_chain{ci}_val_idx.npy')\n",
        "            val_mask[va_idx] = True\n",
        "    vmax = -1.0; vmax_j = -1\n",
        "    for j in range(X.shape[1]):\n",
        "        try:\n",
        "            auc = roc_auc_score(y[val_mask], X[val_mask, j])\n",
        "            if auc > vmax:\n",
        "                vmax, vmax_j = auc, j\n",
        "        except Exception:\n",
        "            pass\n",
        "    print('Sanity B: max single-feature AUC on val rows =', round(vmax,6), 'at feature', vmax_j)\n",
        "    if vmax >= 0.95:\n",
        "        raise RuntimeError(f'Feature leakage suspected: single feature AUC {vmax:.6f} >= 0.95')\n",
        "\n",
        "zero_auc = sanity_zero_check()\n",
        "sanity_single_feature_scan(X_meta_tr)\n",
        "\n",
        "oof = np.zeros(len(y), dtype=np.float32)\n",
        "val_mask = np.zeros(len(y), dtype=bool)\n",
        "test_preds = []\n",
        "\n",
        "param_grid = [\n",
        "    dict(max_depth=3, eta=0.05, min_child_weight=3),\n",
        "    dict(max_depth=4, eta=0.05, min_child_weight=3),\n",
        "    dict(max_depth=4, eta=0.08, min_child_weight=3),\n",
        "    dict(max_depth=5, eta=0.05, min_child_weight=5),\n",
        "]\n",
        "\n",
        "def predict_with_best(bst, dmat):\n",
        "    bi = getattr(bst, 'best_iteration', None)\n",
        "    if bi is not None:\n",
        "        return bst.predict(dmat, iteration_range=(0, int(bi)+1))\n",
        "    return bst.predict(dmat)\n",
        "\n",
        "def train_chain(ci):\n",
        "    tr_idx = np.load(fold_dir / f'fc_chain{ci}_train_idx.npy'); va_idx = np.load(fold_dir / f'fc_chain{ci}_val_idx.npy')\n",
        "    if len(va_idx) == 0:\n",
        "        print(f'Chain {ci}: empty val; skip'); return None\n",
        "    Xtr = X_meta_tr[tr_idx].astype(np.float32); Xva = X_meta_tr[va_idx].astype(np.float32); Xte = X_meta_te.astype(np.float32)\n",
        "    ytr_full, yva = y[tr_idx], y[va_idx]\n",
        "    dtr = xgb.DMatrix(Xtr, label=ytr_full); dva = xgb.DMatrix(Xva, label=yva); dte = xgb.DMatrix(Xte)\n",
        "    pos = int((ytr_full==1).sum()); neg = int((ytr_full==0).sum()); spw = float(neg)/max(1.0,float(pos))\n",
        "    base = dict(tree_method='gpu_hist', objective='binary:logistic', eval_metric='auc',\n",
        "                subsample=0.85, colsample_bytree=0.9, reg_lambda=3.0, reg_alpha=0.1,\n",
        "                scale_pos_weight=spw, seed=42)\n",
        "    best_auc, best_pva, best_pte, best_desc = -1.0, None, None, None\n",
        "    for g in param_grid:\n",
        "        params = base.copy(); params.update(g)\n",
        "        t0 = time.time()\n",
        "        bst = xgb.train(params, dtr, num_boost_round=3000, evals=[(dva,'val')], early_stopping_rounds=75, verbose_eval=False)\n",
        "        pva = predict_with_best(bst, dva)\n",
        "        auc = roc_auc_score(yva, pva)\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_pva = auc, pva\n",
        "            best_pte = predict_with_best(bst, dte)\n",
        "            best_desc = g.copy(); best_desc['best_it'] = getattr(bst,'best_iteration',None); best_desc['secs'] = time.time()-t0\n",
        "    print(f'Chain {ci}: best={best_desc} AUC={best_auc:.5f}', flush=True)\n",
        "    return va_idx, best_pva, best_pte\n",
        "\n",
        "with timer('Train Meta-XGB across chains (minimal + history + lexical)'):\n",
        "    if mf is not None:\n",
        "        chain_ids = [c['chain'] for c in mf.get('chains', [])]\n",
        "    else:\n",
        "        val_files = sorted(fold_dir.glob('fc_chain*_val_idx.npy'))\n",
        "        chain_ids = sorted(int(p.stem.split('chain')[1].split('_')[0]) for p in val_files)\n",
        "    for ci in chain_ids:\n",
        "        res = train_chain(ci)\n",
        "        if res is None: continue\n",
        "        va_idx, pva, pte = res\n",
        "        oof[va_idx] = pva\n",
        "        val_mask[va_idx] = True\n",
        "        test_preds.append(pte)\n",
        "\n",
        "with timer('Evaluate and save Meta-XGB artifacts (minimal+history+lexical)'):\n",
        "    if val_mask.any():\n",
        "        oof_auc = roc_auc_score(y[val_mask], oof[val_mask])\n",
        "        print('Meta-XGB (minimal+history+lexical) OOF AUC (val rows only):', round(oof_auc, 6))\n",
        "        np.save('oof_meta_xgb_fc.npy', oof)\n",
        "    else:\n",
        "        print('Warning: no validation rows; OOF not computed.')\n",
        "    if len(test_preds):\n",
        "        test_pred = np.mean(np.vstack(test_preds), axis=0)\n",
        "        np.save('test_meta_xgb_fc.npy', test_pred)\n",
        "        sub = pd.read_csv('sampleSubmission.csv')\n",
        "        sub['requester_received_pizza'] = np.clip(test_pred, 0.01, 0.99)\n",
        "        sub.to_csv('submission_meta_xgb_fc.csv', index=False)\n",
        "        print('Saved: oof_meta_xgb_fc.npy, test_meta_xgb_fc.npy, submission_meta_xgb_fc.csv')\n",
        "    else:\n",
        "        print('Warning: no test preds collected.')\n",
        "\n",
        "print()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Load data, apply hard bans, and build minimal-safe meta + user history (fold-safe) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta+History shapes: (2878, 33) (1162, 33)\n[T+] Load data, apply hard bans, and build minimal-safe meta + user history (fold-safe) done in 0.55s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chains detected for meta leg: [1, 2, 3]\nSanity A (zeros) AUC on val rows: 0.5\nSanity B: max single-feature AUC on val rows = 0.606255 at feature 1\n[T0] Train Meta-XGB across chains (minimal + history + lexical) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 1: best={'max_depth': 3, 'eta': 0.05, 'min_child_weight': 3, 'best_it': 14, 'secs': 0.15033745765686035} AUC=0.63402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 2: best={'max_depth': 5, 'eta': 0.05, 'min_child_weight': 5, 'best_it': 31, 'secs': 0.19341516494750977} AUC=0.66163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3: best={'max_depth': 5, 'eta': 0.05, 'min_child_weight': 5, 'best_it': 84, 'secs': 0.28555989265441895} AUC=0.53764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Train Meta-XGB across chains (minimal + history + lexical) done in 2.41s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Evaluate and save Meta-XGB artifacts (minimal+history+lexical) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-XGB (minimal+history+lexical) OOF AUC (val rows only): 0.619577\nSaved: oof_meta_xgb_fc.npy, test_meta_xgb_fc.npy, submission_meta_xgb_fc.csv\n[T+] Evaluate and save Meta-XGB artifacts (minimal+history+lexical) done in 0.00s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "id": "9d21875e-efb6-4a8e-af11-d469765c4515",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Helper: Generate and cache BGE-small embeddings via venv interpreter\n",
        "import os, sys, subprocess, textwrap\n",
        "from pathlib import Path\n",
        "\n",
        "script_path = Path('bge_gen.py')\n",
        "script_code = textwrap.dedent('''\n",
        "import os, sys\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def load_json_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path, lines=True)\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return pd.read_json(path, lines=False)\n",
        "        except ValueError:\n",
        "            import json\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            if isinstance(data, dict) and 'data' in data:\n",
        "                data = data['data']\n",
        "            return pd.json_normalize(data)\n",
        "\n",
        "def build_text_cols(df):\n",
        "    title_col_candidates = ['request_title', 'title']\n",
        "    body_col_candidates = ['request_text', 'body', 'text']\n",
        "    def first_col(cols):\n",
        "        for c in cols:\n",
        "            if c in df.columns: return c\n",
        "        return None\n",
        "    tcol = first_col(title_col_candidates)\n",
        "    bcol = first_col(body_col_candidates)\n",
        "    t = df[tcol].fillna('') if tcol else pd.Series(['' for _ in range(len(df))])\n",
        "    b = df[bcol].fillna('') if bcol else pd.Series(['' for _ in range(len(df))])\n",
        "    return t.astype(str), b.astype(str)\n",
        "\n",
        "def main():\n",
        "    cache_dir = Path('emb_cache'); cache_dir.mkdir(exist_ok=True)\n",
        "    tr_path = cache_dir / 'emb_bge_train.npy'\n",
        "    te_path = cache_dir / 'emb_bge_test.npy'\n",
        "    if tr_path.exists() and te_path.exists():\n",
        "        arr_tr = np.load(tr_path); arr_te = np.load(te_path)\n",
        "        print('BGE embeddings already exist:', arr_tr.shape, arr_te.shape)\n",
        "        return\n",
        "    tr = load_json_df('train.json')\n",
        "    te = load_json_df('test.json')\n",
        "    time_col = 'unix_timestamp_of_request_utc' if 'unix_timestamp_of_request_utc' in tr.columns else tr.columns[0]\n",
        "    tr = tr.sort_values(time_col, kind='mergesort').reset_index(drop=True)\n",
        "    t_title, t_body = build_text_cols(tr)\n",
        "    te_title, te_body = build_text_cols(te)\n",
        "    tr_texts = (t_title + ' \\\\n ' + t_body).tolist()\n",
        "    te_texts = (te_title + ' \\\\n ' + te_body).tolist()\n",
        "    model_name = 'BAAI/bge-small-en-v1.5'\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print('Loading model:', model_name, 'on', device)\n",
        "    model = SentenceTransformer(model_name, device=device)\n",
        "    model.max_seq_length = 512\n",
        "    def embed(texts, batch_size=128):\n",
        "        return np.asarray(model.encode(texts, batch_size=batch_size, normalize_embeddings=True, show_progress_bar=True), dtype=np.float32)\n",
        "    print('Encoding train ...'); emb_tr = embed(tr_texts)\n",
        "    print('Encoding test ...'); emb_te = embed(te_texts)\n",
        "    np.save(tr_path, emb_tr); np.save(te_path, emb_te)\n",
        "    print('Saved:', tr_path, te_path, '| shapes:', emb_tr.shape, emb_te.shape)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "''')\n",
        "script_path.write_text(script_code)\n",
        "print('Wrote helper script:', script_path)\n",
        "\n",
        "venv_py = Path('.venv/bin/python')\n",
        "assert venv_py.exists(), 'Venv python not found; run the venv setup cell first'\n",
        "\n",
        "print('Ensuring pandas/numpy in venv ...')\n",
        "subprocess.run([str(venv_py), '-m', 'pip', 'install', '--upgrade', 'pip', 'wheel', 'setuptools'], check=True)\n",
        "subprocess.run([str(venv_py), '-m', 'pip', 'install', 'pandas', 'numpy', 'sentence-transformers'], check=True)\n",
        "\n",
        "print('Running BGE embeddings generation via', venv_py)\n",
        "proc = subprocess.run([str(venv_py), str(script_path)], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "print(proc.stdout)\n",
        "if proc.returncode != 0:\n",
        "    raise RuntimeError('BGE embedding generation failed; check logs above')\n",
        "print('BGE embeddings generation complete. Train the BGE+XGB leg next.')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote helper script: bge_gen.py\nEnsuring pandas/numpy in venv ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in ./.venv/lib/python3.11/site-packages (25.2)\nRequirement already satisfied: wheel in ./.venv/lib/python3.11/site-packages (0.45.1)\nRequirement already satisfied: setuptools in ./.venv/lib/python3.11/site-packages (80.9.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (2.3.2)\nRequirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (1.26.4)\nRequirement already satisfied: sentence-transformers in ./.venv/lib/python3.11/site-packages (3.0.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.2)\nRequirement already satisfied: transformers<5.0.0,>=4.34.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (4.44.2)\nRequirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (2.4.1+cu121)\nRequirement already satisfied: scikit-learn in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (1.7.2)\nRequirement already satisfied: scipy in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (1.16.2)\nRequirement already satisfied: huggingface-hub>=0.15.1 in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (0.35.1)\nRequirement already satisfied: Pillow in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (11.3.0)\nRequirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (3.19.1)\nRequirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2025.9.18)\nRequirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2.32.5)\nRequirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\nRequirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (1.1.10)\nRequirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\nRequirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\nRequirement already satisfied: triton==3.0.0 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.0.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.9.86)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.34.0->sentence-transformers) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.34.0->sentence-transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.34.0->sentence-transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.34.0->sentence-transformers) (2025.8.3)\nRequirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nRunning BGE embeddings generation via .venv/bin/python\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: BAAI/bge-small-en-v1.5 on cuda\nEncoding train ...\n\nBatches:   0%|          | 0/23 [00:00<?, ?it/s]\nBatches:   4%|\u258d         | 1/23 [00:00<00:13,  1.66it/s]\nBatches:   9%|\u258a         | 2/23 [00:00<00:09,  2.32it/s]\nBatches:  13%|\u2588\u258e        | 3/23 [00:01<00:06,  2.88it/s]\nBatches:  17%|\u2588\u258b        | 4/23 [00:01<00:05,  3.29it/s]\nBatches:  22%|\u2588\u2588\u258f       | 5/23 [00:01<00:04,  3.73it/s]\nBatches:  26%|\u2588\u2588\u258c       | 6/23 [00:01<00:03,  4.45it/s]\nBatches:  30%|\u2588\u2588\u2588       | 7/23 [00:01<00:03,  5.05it/s]\nBatches:  35%|\u2588\u2588\u2588\u258d      | 8/23 [00:02<00:02,  5.75it/s]\nBatches:  39%|\u2588\u2588\u2588\u2589      | 9/23 [00:02<00:02,  6.35it/s]\nBatches:  43%|\u2588\u2588\u2588\u2588\u258e     | 10/23 [00:02<00:01,  7.02it/s]\nBatches:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 12/23 [00:02<00:01,  8.63it/s]\nBatches:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 14/23 [00:02<00:00,  9.57it/s]\nBatches:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 16/23 [00:02<00:00, 10.51it/s]\nBatches:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 18/23 [00:02<00:00, 11.43it/s]\nBatches:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 20/23 [00:03<00:00, 12.91it/s]\nBatches:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 22/23 [00:03<00:00, 14.42it/s]\nBatches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 23/23 [00:03<00:00,  7.37it/s]\nEncoding test ...\n\nBatches:   0%|          | 0/10 [00:00<?, ?it/s]\nBatches:  40%|\u2588\u2588\u2588\u2588      | 4/10 [00:00<00:00, 32.85it/s]\nBatches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:00<00:00, 46.48it/s]\nBatches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:00<00:00, 44.26it/s]\nSaved: emb_cache/emb_bge_train.npy emb_cache/emb_bge_test.npy | shapes: (2878, 384) (1162, 384)\n\nBGE embeddings generation complete. Train the BGE+XGB leg next.\n"
          ]
        }
      ]
    },
    {
      "id": "2cde5336-1ffd-4d3a-83fa-c19713eea231",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Leg B2: BGE-small-en-v1.5 embeddings + XGBoost (GPU), 3-seed bag, with robust kNN-rate (multi-k + softmax + cross-model + recency-decay + bayesian), per-chain std [REVERT EXTRAS; TIGHTER XGB]\n",
        "import os, sys, json, time\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import xgboost as xgb\n",
        "\n",
        "os.environ['PYTHONNOUSERSITE'] = '1'\n",
        "sys.path = [p for p in sys.path if ('vendor_pkgs' not in str(p)) and ('.pip-target' not in str(p))]\n",
        "\n",
        "def load_json_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path, lines=True)\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return pd.read_json(path, lines=False)\n",
        "        except ValueError:\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            if isinstance(data, dict) and 'data' in data:\n",
        "                data = data['data']\n",
        "            return pd.json_normalize(data)\n",
        "\n",
        "def per_row_meta(df):\n",
        "    title_col_candidates = ['request_title', 'title']\n",
        "    body_col_candidates = ['request_text', 'body', 'text']\n",
        "    time_col_candidates = ['unix_timestamp_of_request_utc', 'request_timestamp', 'created_utc', 'timestamp', 'time']\n",
        "    def first_col(cols):\n",
        "        for c in cols:\n",
        "            if c in df.columns: return c\n",
        "        return None\n",
        "    tcol = first_col(title_col_candidates); bcol = first_col(body_col_candidates)\n",
        "    time_col = first_col(time_col_candidates)\n",
        "    t = df[tcol].fillna(\"\").astype(str) if tcol else pd.Series([\"\" for _ in range(len(df))])\n",
        "    b = df[bcol].fillna(\"\").astype(str) if bcol else pd.Series([\"\" for _ in range(len(df))])\n",
        "    def wc(s): return s.str.split().apply(len).astype(np.int32)\n",
        "    def cc(s): return s.str.len().astype(np.int32)\n",
        "    title_wc = wc(t); body_wc = wc(b); title_cc = cc(t); body_cc = cc(b)\n",
        "    ratio_tb = (title_cc / (body_cc + 1)).astype(np.float32)\n",
        "    uniq_ratio = (b.str.lower().str.split().apply(lambda x: len(set(x)) / (len(x)+1e-6))).astype(np.float32)\n",
        "    exclam = b.str.count('!').astype(np.int32); quest = b.str.count('\\?').astype(np.int32)\n",
        "    allcaps = b.apply(lambda s: sum(1 for w in s.split() if len(w)>=3 and w.isupper())).astype(np.int32)\n",
        "    has_url = (b.str.contains('http', case=False, na=False) | b.str.contains('www\\.', case=False, na=False)).astype(np.int8)\n",
        "    has_dollar = b.str.contains('\\u0024|\\$', case=False, na=False).astype(np.int8)\n",
        "    has_digit = b.str.contains('[0-9]', regex=True, na=False).astype(np.int8)\n",
        "    ts = pd.to_numeric(df[time_col], errors='coerce').fillna(0).astype(np.int64) if time_col else pd.Series(np.zeros(len(df), dtype=np.int64))\n",
        "    if ts.max() > 10_000_000_000: ts = (ts // 1_000_000_000).astype(np.int64)\n",
        "    dt = pd.to_datetime(ts, unit='s', utc=True)\n",
        "    month = dt.dt.month.astype(np.int16); wday = dt.dt.weekday.astype(np.int16); hour = dt.dt.hour.astype(np.int16)\n",
        "    feats = np.vstack([\n",
        "        title_wc, body_wc, title_cc, body_cc, ratio_tb, uniq_ratio,\n",
        "        exclam, quest, allcaps, has_url, has_dollar, has_digit,\n",
        "        month, wday, hour\n",
        "    ]).T.astype(np.float32)\n",
        "    return feats\n",
        "\n",
        "def compute_or_load_bge_embeddings(cache_dir=Path(\"emb_cache\")):\n",
        "    tr_path = cache_dir / \"emb_bge_train.npy\"\n",
        "    te_path = cache_dir / \"emb_bge_test.npy\"\n",
        "    assert tr_path.exists() and te_path.exists(), \"BGE caches missing; run Cell 15 first\"\n",
        "    return np.load(tr_path).astype(np.float32), np.load(te_path).astype(np.float32)\n",
        "\n",
        "def _topk_idx(sims, k):\n",
        "    if k < sims.shape[1]:\n",
        "        return np.argpartition(-sims, kth=k-1, axis=1)[:, :k]\n",
        "    else:\n",
        "        return np.argsort(-sims, axis=1)\n",
        "\n",
        "def knn_rate_mean(pool_emb, pool_y, query_emb, k=50, self_exclude=False):\n",
        "    sims = query_emb @ pool_emb.T\n",
        "    if self_exclude and query_emb.shape[0] == pool_emb.shape[0]:\n",
        "        idx = np.arange(sims.shape[0]); sims[idx, idx] = -1e9\n",
        "    topk = _topk_idx(sims, k)\n",
        "    return pool_y[topk].mean(axis=1).astype(np.float32)\n",
        "\n",
        "def knn_rate_softmax(pool_emb, pool_y, query_emb, k=50, tau=0.12, self_exclude=False):\n",
        "    sims = query_emb @ pool_emb.T\n",
        "    if self_exclude and query_emb.shape[0] == pool_emb.shape[0]:\n",
        "        idx = np.arange(sims.shape[0]); sims[idx, idx] = -1e9\n",
        "    topk = _topk_idx(sims, k)\n",
        "    row_idx = np.arange(topk.shape[0])[:, None]\n",
        "    s_top = sims[row_idx, topk]\n",
        "    w = np.exp(s_top / max(1e-6, tau))\n",
        "    w /= (w.sum(axis=1, keepdims=True) + 1e-9)\n",
        "    y_top = pool_y[topk].astype(np.float32)\n",
        "    return (w * y_top).sum(axis=1).astype(np.float32)\n",
        "\n",
        "def knn_topk(pool_emb, query_emb, k=50, self_exclude=False):\n",
        "    sims = query_emb @ pool_emb.T\n",
        "    if self_exclude and query_emb.shape[0] == pool_emb.shape[0]:\n",
        "        idx = np.arange(sims.shape[0]); sims[idx, idx] = -1e9\n",
        "    topk = _topk_idx(sims, k)\n",
        "    return topk, sims\n",
        "\n",
        "def knn_rate_bayes(pool_emb, pool_y, query_emb, k=50, alpha=22.0, p_train=0.5, self_exclude=False):\n",
        "    topk, sims = knn_topk(pool_emb, query_emb, k=k, self_exclude=self_exclude)\n",
        "    y_top = pool_y[topk].astype(np.float32)\n",
        "    s = y_top.sum(axis=1)\n",
        "    denom = (k + alpha)\n",
        "    return ((s + alpha * p_train) / denom).astype(np.float32)\n",
        "\n",
        "def knn_rate_recency_decay(pool_emb, pool_y, query_emb, pool_ts_days, query_ts_days, k=50, lam_days=75.0, self_exclude=False):\n",
        "    topk, sims = knn_topk(pool_emb, query_emb, k=k, self_exclude=self_exclude)\n",
        "    row_idx = np.arange(topk.shape[0])[:, None]\n",
        "    qd = query_ts_days[row_idx[:,0]][:, None]\n",
        "    pdays = pool_ts_days[topk]\n",
        "    gaps = np.maximum(qd - pdays, 0.0)\n",
        "    w = np.exp(-gaps / max(lam_days, 1e-6))\n",
        "    y_top = pool_y[topk].astype(np.float32)\n",
        "    w_sum = w.sum(axis=1, keepdims=True) + 1e-9\n",
        "    return ((w * y_top).sum(axis=1) / w_sum[:,0]).astype(np.float32)\n",
        "\n",
        "from contextlib import contextmanager\n",
        "@contextmanager\n",
        "def timer(msg):\n",
        "    t0 = time.time(); print(f\"[T0] {msg} ...\", flush=True)\n",
        "    try: yield\n",
        "    finally: print(f\"[T+] {msg} done in {time.time()-t0:.2f}s\", flush=True)\n",
        "\n",
        "with timer(\"Load train/test and prepare inputs (BGE)\"):\n",
        "    tr = load_json_df('train.json')\n",
        "    te = load_json_df('test.json')\n",
        "    time_col = 'unix_timestamp_of_request_utc' if 'unix_timestamp_of_request_utc' in tr.columns else tr.columns[0]\n",
        "    label_col = 'requester_received_pizza' if 'requester_received_pizza' in tr.columns else 'label'\n",
        "    tr = tr.sort_values(time_col, kind='mergesort').reset_index(drop=True)\n",
        "    y = pd.to_numeric(tr[label_col], errors='coerce').fillna(0).astype(int).clip(0,1).values\n",
        "    # timestamps -> days\n",
        "    ts_tr = pd.to_numeric(tr[time_col], errors='coerce').fillna(0).values.astype(np.int64)\n",
        "    ts_te = pd.to_numeric(te[time_col], errors='coerce').fillna(0).values.astype(np.int64) if time_col in te.columns else np.zeros(len(te), dtype=np.int64)\n",
        "    if ts_tr.max() > 10_000_000_000: ts_tr = ts_tr // 1_000_000_000\n",
        "    if ts_te.max() > 10_000_000_000: ts_te = ts_te // 1_000_000_000\n",
        "    ts_tr_days = (ts_tr / 86400.0).astype(np.float32)\n",
        "    ts_te_days = (ts_te / 86400.0).astype(np.float32)\n",
        "    bge_tr, bge_te = compute_or_load_bge_embeddings()\n",
        "    # Cross-model pool for reverse cross kNN-rate (E5 space)\n",
        "    e5_tr = np.load('emb_cache/emb_e5_train.npy').astype(np.float32)\n",
        "    e5_te = np.load('emb_cache/emb_e5_test.npy').astype(np.float32)\n",
        "    meta_tr = per_row_meta(tr)\n",
        "    meta_te = per_row_meta(te)\n",
        "    print(\"Shapes | bge:\", bge_tr.shape, bge_te.shape, \"meta:\", meta_tr.shape, meta_te.shape, \"| e5:\", e5_tr.shape, e5_te.shape)\n",
        "\n",
        "fold_dir = Path('folds')\n",
        "manifest_path = fold_dir / 'manifest.json'\n",
        "if manifest_path.exists():\n",
        "    mf = json.loads(manifest_path.read_text())\n",
        "    chain_ids = [c['chain'] for c in mf.get('chains', [])]\n",
        "else:\n",
        "    val_files = sorted(fold_dir.glob('fc_chain*_val_idx.npy'))\n",
        "    chain_ids = sorted(int(p.stem.split('chain')[1].split('_')[0]) for p in val_files)\n",
        "print(\"Chains detected (from manifest if available):\", chain_ids)\n",
        "\n",
        "oof = np.zeros(len(tr), dtype=np.float32)\n",
        "val_mask = np.zeros(len(tr), dtype=bool)\n",
        "test_preds_bag = []\n",
        "\n",
        "# Tighter regularization per expert guidance\n",
        "param_grid = [\n",
        "    dict(max_depth=4, eta=0.08, min_child_weight=5),\n",
        "    dict(max_depth=4, eta=0.05, min_child_weight=5),\n",
        "    dict(max_depth=4, eta=0.08, min_child_weight=8),\n",
        "]\n",
        "\n",
        "def predict_with_best(bst, dmat):\n",
        "    bi = getattr(bst, 'best_iteration', None)\n",
        "    if bi is not None:\n",
        "        return bst.predict(dmat, iteration_range=(0, int(bi)+1))\n",
        "    return bst.predict(dmat)\n",
        "\n",
        "def standardize_knn_feats(tr_mat, va_mat, te_mat):\n",
        "    mu = tr_mat.mean(axis=0); sd = tr_mat.std(axis=0); sd = np.where(sd < 1e-6, 1.0, sd)\n",
        "    tr_s = (tr_mat - mu) / sd\n",
        "    va_s = (va_mat - mu) / sd\n",
        "    te_s = (te_mat - mu) / sd\n",
        "    return tr_s.astype(np.float32), va_s.astype(np.float32), te_s.astype(np.float32)\n",
        "\n",
        "def train_one_chain_seed(ci, seed):\n",
        "    tr_idx = np.load(fold_dir / f\"fc_chain{ci}_train_idx.npy\"); va_idx = np.load(fold_dir / f\"fc_chain{ci}_val_idx.npy\")\n",
        "    if len(va_idx) == 0:\n",
        "        print(f\"Chain {ci} seed {seed}: empty val; skip\"); return None, None, None\n",
        "    Xtr_emb, ytr = bge_tr[tr_idx], y[tr_idx]\n",
        "    Xva_emb, yva = bge_tr[va_idx], y[va_idx]\n",
        "    # Same-model kNN rates in BGE space: multi-k and softmax (tau=0.12)\n",
        "    kn_tr_k20 = knn_rate_mean(Xtr_emb, ytr, Xtr_emb, k=20, self_exclude=True).reshape(-1,1)\n",
        "    kn_va_k20 = knn_rate_mean(Xtr_emb, ytr, Xva_emb, k=20, self_exclude=False).reshape(-1,1)\n",
        "    kn_tr_k50 = knn_rate_mean(Xtr_emb, ytr, Xtr_emb, k=50, self_exclude=True).reshape(-1,1)\n",
        "    kn_va_k50 = knn_rate_mean(Xtr_emb, ytr, Xva_emb, k=50, self_exclude=False).reshape(-1,1)\n",
        "    kn_tr_k100 = knn_rate_mean(Xtr_emb, ytr, Xtr_emb, k=100, self_exclude=True).reshape(-1,1)\n",
        "    kn_va_k100 = knn_rate_mean(Xtr_emb, ytr, Xva_emb, k=100, self_exclude=False).reshape(-1,1)\n",
        "    kn_tr_soft = knn_rate_softmax(Xtr_emb, ytr, Xtr_emb, k=50, tau=0.12, self_exclude=True).reshape(-1,1)\n",
        "    kn_va_soft = knn_rate_softmax(Xtr_emb, ytr, Xva_emb, k=50, tau=0.12, self_exclude=False).reshape(-1,1)\n",
        "    # Cross-model (reverse) in E5 space\n",
        "    kn_tr_cross = knn_rate_mean(e5_tr[tr_idx], ytr, e5_tr[tr_idx], k=50, self_exclude=True).reshape(-1,1)\n",
        "    kn_va_cross = knn_rate_mean(e5_tr[tr_idx], ytr, e5_tr[va_idx], k=50, self_exclude=False).reshape(-1,1)\n",
        "    # Recency-decayed k=50 (lambda ~75 days) in BGE space\n",
        "    tr_days = ts_tr_days[tr_idx]; va_days = ts_tr_days[va_idx]\n",
        "    kn_tr_dec = knn_rate_recency_decay(Xtr_emb, ytr, Xtr_emb, tr_days, tr_days, k=50, lam_days=75.0, self_exclude=True).reshape(-1,1)\n",
        "    kn_va_dec = knn_rate_recency_decay(Xtr_emb, ytr, Xva_emb, tr_days, va_days, k=50, lam_days=75.0, self_exclude=False).reshape(-1,1)\n",
        "    # Bayesian-smoothed k=50 in BGE space\n",
        "    p_train = float(ytr.mean())\n",
        "    kn_tr_bayes = knn_rate_bayes(Xtr_emb, ytr, Xtr_emb, k=50, alpha=22.0, p_train=p_train, self_exclude=True).reshape(-1,1)\n",
        "    kn_va_bayes = knn_rate_bayes(Xtr_emb, ytr, Xva_emb, k=50, alpha=22.0, p_train=p_train, self_exclude=False).reshape(-1,1)\n",
        "    # Test features using full train pool\n",
        "    kn_te_k20  = knn_rate_mean(bge_tr, y, bge_te, k=20,  self_exclude=False).reshape(-1,1)\n",
        "    kn_te_k50  = knn_rate_mean(bge_tr, y, bge_te, k=50,  self_exclude=False).reshape(-1,1)\n",
        "    kn_te_k100 = knn_rate_mean(bge_tr, y, bge_te, k=100, self_exclude=False).reshape(-1,1)\n",
        "    kn_te_soft = knn_rate_softmax(bge_tr, y, bge_te, k=50, tau=0.12, self_exclude=False).reshape(-1,1)\n",
        "    kn_te_cross = knn_rate_mean(e5_tr, y, e5_te, k=50, self_exclude=False).reshape(-1,1)\n",
        "    te_days = ts_te_days\n",
        "    kn_te_dec = knn_rate_recency_decay(bge_tr, y, bge_te, ts_tr_days, te_days, k=50, lam_days=75.0, self_exclude=False).reshape(-1,1)\n",
        "    p_full = float(y.mean())\n",
        "    kn_te_bayes = knn_rate_bayes(bge_tr, y, bge_te, k=50, alpha=22.0, p_train=p_full, self_exclude=False).reshape(-1,1)\n",
        "    # Assemble and standardize kNN features per chain (EXTRAS REMOVED)\n",
        "    K_tr = np.hstack([kn_tr_k20, kn_tr_k50, kn_tr_k100, kn_tr_soft, kn_tr_cross, kn_tr_dec, kn_tr_bayes])\n",
        "    K_va = np.hstack([kn_va_k20, kn_va_k50, kn_va_k100, kn_va_soft, kn_va_cross, kn_va_dec, kn_va_bayes])\n",
        "    K_te = np.hstack([kn_te_k20, kn_te_k50, kn_te_k100, kn_te_soft, kn_te_cross, kn_te_dec, kn_te_bayes])\n",
        "    K_tr_s, K_va_s, K_te_s = standardize_knn_feats(K_tr, K_va, K_te)\n",
        "    # Final features\n",
        "    Xtr = np.hstack([Xtr_emb, meta_tr[tr_idx], K_tr_s]).astype(np.float32)\n",
        "    Xva = np.hstack([Xva_emb, meta_tr[va_idx], K_va_s]).astype(np.float32)\n",
        "    Xte = np.hstack([bge_te,   meta_te,        K_te_s]).astype(np.float32)\n",
        "    dtr = xgb.DMatrix(Xtr, label=ytr); dva = xgb.DMatrix(Xva, label=yva); dte = xgb.DMatrix(Xte)\n",
        "    pos = int((ytr==1).sum()); neg = int((ytr==0).sum()); spw = float(neg)/max(1.0, float(pos))\n",
        "    base = dict(tree_method='gpu_hist', objective='binary:logistic', eval_metric='auc',\n",
        "                subsample=0.8, colsample_bytree=0.8, reg_lambda=5.0, reg_alpha=0.2,\n",
        "                scale_pos_weight=spw, seed=seed)\n",
        "    best_auc = -1.0; best_pva = None; best_pte = None; best_desc = None\n",
        "    for g in param_grid:\n",
        "        params = base.copy(); params.update(g)\n",
        "        t0 = time.time()\n",
        "        bst = xgb.train(params, dtr, num_boost_round=4000, evals=[(dva,'val')], early_stopping_rounds=120, verbose_eval=False)\n",
        "        pva = predict_with_best(bst, dva)\n",
        "        auc = roc_auc_score(yva, pva)\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_pva = auc, pva\n",
        "            best_pte = predict_with_best(bst, dte)\n",
        "            best_desc = g.copy(); best_desc['best_it'] = getattr(bst, 'best_iteration', None); best_desc['secs'] = time.time()-t0\n",
        "    print(f\"Chain {ci} seed {seed}: best={best_desc} AUC={best_auc:.5f}\", flush=True)\n",
        "    return va_idx, best_pva, best_pte\n",
        "\n",
        "with timer(\"Train BGE+XGB+robust kNN across chains and seeds\"):\n",
        "    SEEDS = [42, 1337, 2025]\n",
        "    for seed in SEEDS:\n",
        "        test_preds_per_chain = []\n",
        "        for ci in chain_ids:\n",
        "            res = train_one_chain_seed(ci, seed)\n",
        "            if res is None: continue\n",
        "            va_idx, pva, pte = res\n",
        "            if seed == SEEDS[0]:\n",
        "                oof[va_idx] = pva\n",
        "            else:\n",
        "                oof[va_idx] += pva\n",
        "            val_mask[va_idx] = True\n",
        "            test_preds_per_chain.append(pte)\n",
        "        if len(test_preds_per_chain):\n",
        "            test_preds_bag.append(np.mean(np.vstack(test_preds_per_chain), axis=0))\n",
        "\n",
        "with timer(\"Evaluate and save BGE+XGB artifacts (reverted extras)\"):\n",
        "    if val_mask.any():\n",
        "        oof_avg = oof.copy(); oof_avg[val_mask] = oof_avg[val_mask] / 3.0\n",
        "        oof_auc_val = roc_auc_score(y[val_mask], oof_avg[val_mask])\n",
        "        print(\"BGE+XGB OOF AUC (val rows only, seed-bag):\", round(oof_auc_val, 6))\n",
        "        np.save('oof_bge_xgb_fc.npy', oof_avg)\n",
        "    else:\n",
        "        print(\"Warning: no validation rows; OOF not computed.\")\n",
        "    if len(test_preds_bag):\n",
        "        test_pred = np.mean(np.vstack(test_preds_bag), axis=0)\n",
        "        np.save('test_bge_xgb_fc.npy', test_pred)\n",
        "        sub = pd.read_csv('sampleSubmission.csv')\n",
        "        sub['requester_received_pizza'] = np.clip(test_pred, 0.01, 0.99)\n",
        "        sub.to_csv('submission_bge_xgb_fc.csv', index=False)\n",
        "        print('Saved: oof_bge_xgb_fc.npy, test_bge_xgb_fc.npy, submission_bge_xgb_fc.csv')\n",
        "    else:\n",
        "        print('Warning: no test preds collected from seeds.')"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Load train/test and prepare inputs (BGE) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes | bge: (2878, 384) (1162, 384) meta: (2878, 15) (1162, 15) | e5: (2878, 768) (1162, 768)\n[T+] Load train/test and prepare inputs (BGE) done in 0.23s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chains detected (from manifest if available): [1, 2, 3]\n[T0] Train BGE+XGB+robust kNN across chains and seeds ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 1 seed 42: best={'max_depth': 4, 'eta': 0.05, 'min_child_weight': 5, 'best_it': 111, 'secs': 0.5622589588165283} AUC=0.62390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 2 seed 42: best={'max_depth': 4, 'eta': 0.05, 'min_child_weight': 5, 'best_it': 20, 'secs': 0.34802865982055664} AUC=0.70032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3 seed 42: best={'max_depth': 4, 'eta': 0.08, 'min_child_weight': 8, 'best_it': 80, 'secs': 0.48722362518310547} AUC=0.63433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 1 seed 1337: best={'max_depth': 4, 'eta': 0.05, 'min_child_weight': 5, 'best_it': 12, 'secs': 0.32511377334594727} AUC=0.61522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 2 seed 1337: best={'max_depth': 4, 'eta': 0.08, 'min_child_weight': 8, 'best_it': 39, 'secs': 0.3844156265258789} AUC=0.68168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3 seed 1337: best={'max_depth': 4, 'eta': 0.08, 'min_child_weight': 5, 'best_it': 58, 'secs': 0.45854854583740234} AUC=0.62802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 1 seed 2025: best={'max_depth': 4, 'eta': 0.05, 'min_child_weight': 5, 'best_it': 19, 'secs': 0.3417055606842041} AUC=0.60752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 2 seed 2025: best={'max_depth': 4, 'eta': 0.08, 'min_child_weight': 5, 'best_it': 18, 'secs': 0.3515007495880127} AUC=0.67218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3 seed 2025: best={'max_depth': 4, 'eta': 0.08, 'min_child_weight': 8, 'best_it': 116, 'secs': 0.57157301902771} AUC=0.61422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Train BGE+XGB+robust kNN across chains and seeds done in 17.62s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Evaluate and save BGE+XGB artifacts (reverted extras) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BGE+XGB OOF AUC (val rows only, seed-bag): 0.614021\nSaved: oof_bge_xgb_fc.npy, test_bge_xgb_fc.npy, submission_bge_xgb_fc.csv\n[T+] Evaluate and save BGE+XGB artifacts (reverted extras) done in 0.00s\n"
          ]
        }
      ]
    },
    {
      "id": "b51e29a9-12ea-4585-aef0-567a0d31ffe7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# kNN neighbor-rate features from E5 and BGE embeddings (per-chain, leak-safe); saves OOF/test features\n",
        "import json, time\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "def load_json_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path, lines=True)\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return pd.read_json(path, lines=False)\n",
        "        except ValueError:\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            if isinstance(data, dict) and 'data' in data:\n",
        "                data = data['data']\n",
        "            return pd.json_normalize(data)\n",
        "\n",
        "def timer_log(msg):\n",
        "    print(f\"[T0] {msg} ...\", flush=True)\n",
        "    return time.time()\n",
        "\n",
        "def timer_done(t0, msg):\n",
        "    print(f\"[T+] {msg} done in {time.time()-t0:.2f}s\", flush=True)\n",
        "\n",
        "fold_dir = Path('folds')\n",
        "mf = json.loads((fold_dir / 'manifest.json').read_text())\n",
        "chains = [c['chain'] for c in mf['chains']]\n",
        "print('Chains (manifest):', chains)\n",
        "\n",
        "# Labels aligned to sorted-by-time order\n",
        "tr = load_json_df('train.json')\n",
        "time_col = mf.get('time_col', 'unix_timestamp_of_request_utc')\n",
        "label_col = mf.get('label_col', 'requester_received_pizza')\n",
        "tr = tr.sort_values(time_col, kind='mergesort').reset_index(drop=True)\n",
        "y = pd.to_numeric(tr[label_col], errors='coerce').fillna(0).astype(int).clip(0,1).values.astype(np.int8)\n",
        "n = len(tr)\n",
        "\n",
        "# Load normalized embeddings (already normalized in cache generation)\n",
        "emb_dir = Path('emb_cache')\n",
        "e5_tr = np.load(emb_dir / 'emb_e5_train.npy').astype(np.float32)\n",
        "e5_te = np.load(emb_dir / 'emb_e5_test.npy').astype(np.float32)\n",
        "bge_tr = np.load(emb_dir / 'emb_bge_train.npy').astype(np.float32)\n",
        "bge_te = np.load(emb_dir / 'emb_bge_test.npy').astype(np.float32)\n",
        "assert e5_tr.shape[0] == n and bge_tr.shape[0] == n\n",
        "print('Emb shapes | e5:', e5_tr.shape, e5_te.shape, '| bge:', bge_tr.shape, bge_te.shape)\n",
        "\n",
        "def knn_rate_foldsafe(emb_tr, emb_te, y, k=50):\n",
        "    \"\"\"Compute per-chain OOF neighbor success-rate and test neighbor success-rate.\n",
        "    - For each chain c: use emb_tr[train_idx] as neighbor pool; for val_idx rows,\n",
        "      compute cosine sims to pool, take top-k indices, average y of neighbors.\n",
        "    - For test: use full training pool emb_tr (all rows) against emb_te.\n",
        "    Returns:\n",
        "      oof_rate: shape (n,), filled at validation indices only;\n",
        "      test_rate: shape (len(test),).\n",
        "    \"\"\"\n",
        "    oof_rate = np.zeros(n, dtype=np.float32)\n",
        "    val_mask_all = np.zeros(n, dtype=bool)\n",
        "    # Per-chain OOF\n",
        "    for ci in chains:\n",
        "        tr_idx = np.load(fold_dir / f\"fc_chain{ci}_train_idx.npy\")\n",
        "        va_idx = np.load(fold_dir / f\"fc_chain{ci}_val_idx.npy\")\n",
        "        if len(va_idx) == 0:\n",
        "            continue\n",
        "        val_mask_all[va_idx] = True\n",
        "        X_pool = emb_tr[tr_idx]  # (M, D)\n",
        "        X_q = emb_tr[va_idx]     # (V, D)\n",
        "        # Cosine sim via dot (embeddings are normalized)\n",
        "        t0 = timer_log(f'Chain {ci} kNN-rate: pool {X_pool.shape}, queries {X_q.shape}, k={k}')\n",
        "        sims = X_q @ X_pool.T  # (V, M)\n",
        "        # top-k indices per row\n",
        "        if k < sims.shape[1]:\n",
        "            topk_idx = np.argpartition(-sims, kth=k-1, axis=1)[:, :k]\n",
        "        else:\n",
        "            topk_idx = np.argsort(-sims, axis=1)\n",
        "        # Gather neighbor labels and average\n",
        "        yn = y[tr_idx]\n",
        "        rates = yn[topk_idx].mean(axis=1).astype(np.float32)\n",
        "        oof_rate[va_idx] = rates\n",
        "        timer_done(t0, f'Chain {ci} kNN-rate')\n",
        "    # Test using full train as pool\n",
        "    t0 = timer_log('Test kNN-rate using full train pool')\n",
        "    sims_te = emb_te @ emb_tr.T  # (T, N)\n",
        "    if k < sims_te.shape[1]:\n",
        "        topk_te = np.argpartition(-sims_te, kth=k-1, axis=1)[:, :k]\n",
        "    else:\n",
        "        topk_te = np.argsort(-sims_te, axis=1)\n",
        "    test_rate = y[topk_te].mean(axis=1).astype(np.float32)\n",
        "    timer_done(t0, 'Test kNN-rate')\n",
        "    return oof_rate, test_rate, val_mask_all\n",
        "\n",
        "for name, (tr_emb, te_emb) in {\n",
        "    'e5': (e5_tr, e5_te),\n",
        "    'bge': (bge_tr, bge_te),\n",
        "}.items():\n",
        "    t0 = timer_log(f'Compute kNN-rate for {name}')\n",
        "    oof_rate, test_rate, valmask = knn_rate_foldsafe(tr_emb, te_emb, y, k=50)\n",
        "    # Save artifacts\n",
        "    np.save(f'knnrate_{name}_oof.npy', oof_rate)\n",
        "    np.save(f'knnrate_{name}_test.npy', test_rate)\n",
        "    # Quick diagnostics\n",
        "    try:\n",
        "        from sklearn.metrics import roc_auc_score\n",
        "        auc = roc_auc_score(y[valmask], oof_rate[valmask]) if valmask.any() else float('nan')\n",
        "        print(f'{name} kNN-rate OOF AUC (val rows): {auc:.6f}')\n",
        "    except Exception as e:\n",
        "        print('AUC diag failed for', name, '|', repr(e))\n",
        "    timer_done(t0, f'Compute kNN-rate for {name}')\n",
        "\n",
        "print('Saved: knnrate_e5_oof.npy, knnrate_e5_test.npy, knnrate_bge_oof.npy, knnrate_bge_test.npy')\n",
        "print('Next: append these features to E5/BGE/Meta legs and retrain, then reblend.')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chains (manifest): [1, 2, 3]\nEmb shapes | e5: (2878, 768) (1162, 768) | bge: (2878, 384) (1162, 384)\n[T0] Compute kNN-rate for e5 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Chain 1 kNN-rate: pool (1727, 768), queries (562, 768), k=50 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Chain 1 kNN-rate done in 0.01s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Chain 2 kNN-rate: pool (2302, 768), queries (278, 768), k=50 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Chain 2 kNN-rate done in 0.01s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Chain 3 kNN-rate: pool (2590, 768), queries (268, 768), k=50 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Chain 3 kNN-rate done in 0.01s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Test kNN-rate using full train pool ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Test kNN-rate done in 0.05s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e5 kNN-rate OOF AUC (val rows): 0.568997\n[T+] Compute kNN-rate for e5 done in 0.08s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Compute kNN-rate for bge ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Chain 1 kNN-rate: pool (1727, 384), queries (562, 384), k=50 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Chain 1 kNN-rate done in 0.01s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Chain 2 kNN-rate: pool (2302, 384), queries (278, 384), k=50 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Chain 2 kNN-rate done in 0.01s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Chain 3 kNN-rate: pool (2590, 384), queries (268, 384), k=50 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Chain 3 kNN-rate done in 0.01s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Test kNN-rate using full train pool ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Test kNN-rate done in 0.03s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bge kNN-rate OOF AUC (val rows): 0.570418\n[T+] Compute kNN-rate for bge done in 0.07s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: knnrate_e5_oof.npy, knnrate_e5_test.npy, knnrate_bge_oof.npy, knnrate_bge_test.npy\nNext: append these features to E5/BGE/Meta legs and retrain, then reblend.\n"
          ]
        }
      ]
    },
    {
      "id": "754ee1a3-7f05-453b-9456-47c23b9ff8a3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Leg B3: MPNet (multi-qa-mpnet-base-dot-v1) embeddings + XGBoost with robust kNN features (multi-k mean, softmax tau=0.12, recency lam=75, bayes, cross-model), per-chain std [REVERT EXTRAS; TIGHTER XGB]\n",
        "import os, sys, json, time, subprocess, textwrap\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import xgboost as xgb\n",
        "\n",
        "os.environ['PYTHONNOUSERSITE'] = '1'\n",
        "sys.path = [p for p in sys.path if ('vendor_pkgs' not in str(p)) and ('.pip-target' not in str(p))]\n",
        "\n",
        "def load_json_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path, lines=True)\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return pd.read_json(path, lines=False)\n",
        "        except ValueError:\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            if isinstance(data, dict) and 'data' in data:\n",
        "                data = data['data']\n",
        "            return pd.json_normalize(data)\n",
        "\n",
        "def per_row_meta(df):\n",
        "    title_col_candidates = ['request_title', 'title']\n",
        "    body_col_candidates = ['request_text', 'body', 'text']\n",
        "    time_col_candidates = ['unix_timestamp_of_request_utc', 'request_timestamp', 'created_utc', 'timestamp', 'time']\n",
        "    def first_col(cols):\n",
        "        for c in cols:\n",
        "            if c in df.columns: return c\n",
        "        return None\n",
        "    tcol = first_col(title_col_candidates); bcol = first_col(body_col_candidates)\n",
        "    time_col = first_col(time_col_candidates)\n",
        "    t = df[tcol].fillna(\"\").astype(str) if tcol else pd.Series([\"\" for _ in range(len(df))])\n",
        "    b = df[bcol].fillna(\"\").astype(str) if bcol else pd.Series([\"\" for _ in range(len(df))])\n",
        "    def wc(s): return s.str.split().apply(len).astype(np.int32)\n",
        "    def cc(s): return s.str.len().astype(np.int32)\n",
        "    title_wc = wc(t); body_wc = wc(b); title_cc = cc(t); body_cc = cc(b)\n",
        "    ratio_tb = (title_cc / (body_cc + 1)).astype(np.float32)\n",
        "    uniq_ratio = (b.str.lower().str.split().apply(lambda x: len(set(x)) / (len(x)+1e-6))).astype(np.float32)\n",
        "    exclam = b.str.count('!').astype(np.int32); quest = b.str.count('\\?').astype(np.int32)\n",
        "    allcaps = b.apply(lambda s: sum(1 for w in s.split() if len(w)>=3 and w.isupper())).astype(np.int32)\n",
        "    has_url = (b.str.contains('http', case=False, na=False) | b.str.contains('www\\.', case=False, na=False)).astype(np.int8)\n",
        "    has_dollar = b.str.contains('\\u0024|\\$', case=False, na=False).astype(np.int8)\n",
        "    has_digit = b.str.contains('[0-9]', regex=True, na=False).astype(np.int8)\n",
        "    ts = pd.to_numeric(df[time_col], errors='coerce').fillna(0).astype(np.int64) if time_col else pd.Series(np.zeros(len(df), dtype=np.int64))\n",
        "    if ts.max() > 10_000_000_000: ts = (ts // 1_000_000_000).astype(np.int64)\n",
        "    dt = pd.to_datetime(ts, unit='s', utc=True)\n",
        "    month = dt.dt.month.astype(np.int16); wday = dt.dt.weekday.astype(np.int16); hour = dt.dt.hour.astype(np.int16)\n",
        "    feats = np.vstack([\n",
        "        title_wc, body_wc, title_cc, body_cc, ratio_tb, uniq_ratio,\n",
        "        exclam, quest, allcaps, has_url, has_dollar, has_digit,\n",
        "        month, wday, hour\n",
        "    ]).T.astype(np.float32)\n",
        "    return feats\n",
        "\n",
        "def ensure_mpnet_embeddings():\n",
        "    cache_dir = Path('emb_cache'); cache_dir.mkdir(exist_ok=True)\n",
        "    tr_path = cache_dir / 'emb_mpnet_train.npy'\n",
        "    te_path = cache_dir / 'emb_mpnet_test.npy'\n",
        "    if tr_path.exists() and te_path.exists():\n",
        "        return\n",
        "    script_path = Path('mpnet_gen.py')\n",
        "    code = textwrap.dedent('''\n",
        "import json, numpy as np, pandas as pd, torch\n",
        "from pathlib import Path\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def load_json_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path, lines=True)\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return pd.read_json(path, lines=False)\n",
        "        except ValueError:\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            if isinstance(data, dict) and 'data' in data:\n",
        "                data = data['data']\n",
        "            return pd.json_normalize(data)\n",
        "\n",
        "def first_col(df, cols):\n",
        "    for c in cols:\n",
        "        if c in df.columns: return c\n",
        "    return None\n",
        "\n",
        "def main():\n",
        "    cache_dir = Path('emb_cache'); cache_dir.mkdir(exist_ok=True)\n",
        "    tr = load_json_df('train.json')\n",
        "    te = load_json_df('test.json')\n",
        "    tcol = first_col(tr, ['request_title','title'])\n",
        "    bcol = first_col(tr, ['request_text','body','text'])\n",
        "    t = tr[tcol].fillna('') if tcol else pd.Series(['']*len(tr))\n",
        "    b = tr[bcol].fillna('') if bcol else pd.Series(['']*len(tr))\n",
        "    tt = te[tcol].fillna('') if tcol in te.columns else pd.Series(['']*len(te))\n",
        "    tb = te[bcol].fillna('') if bcol in te.columns else pd.Series(['']*len(te))\n",
        "    tr_txt = (t + ' \\n ' + b).astype(str).tolist()\n",
        "    te_txt = (tt + ' \\n ' + tb).astype(str).tolist()\n",
        "    model = SentenceTransformer('multi-qa-mpnet-base-dot-v1', device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.max_seq_length = 512\n",
        "    def enc(texts, bs=128):\n",
        "        return np.asarray(model.encode(texts, batch_size=bs, normalize_embeddings=True, show_progress_bar=True), dtype=np.float32)\n",
        "    emb_tr = enc(tr_txt); emb_te = enc(te_txt)\n",
        "    np.save(cache_dir/'emb_mpnet_train.npy', emb_tr)\n",
        "    np.save(cache_dir/'emb_mpnet_test.npy', emb_te)\n",
        "    print('Saved MPNet embeddings:', emb_tr.shape, emb_te.shape)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "''')\n",
        "    script_path.write_text(code)\n",
        "    venv_py = Path('.venv/bin/python')\n",
        "    assert venv_py.exists(), 'Missing .venv python; run setup cell 1 first'\n",
        "    subprocess.run([str(venv_py), '-m', 'pip', 'install', '--upgrade', 'pip', 'wheel', 'setuptools'], check=True)\n",
        "    subprocess.run([str(venv_py), '-m', 'pip', 'install', 'pandas', 'numpy', 'sentence-transformers'], check=True)\n",
        "    print('Generating MPNet embeddings via venv ...', flush=True)\n",
        "    proc = subprocess.run([str(venv_py), str(script_path)], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    print(proc.stdout)\n",
        "    if proc.returncode != 0:\n",
        "        raise RuntimeError('MPNet embedding generation failed')\n",
        "\n",
        "def _topk_idx(sims, k):\n",
        "    if k < sims.shape[1]:\n",
        "        return np.argpartition(-sims, kth=k-1, axis=1)[:, :k]\n",
        "    else:\n",
        "        return np.argsort(-sims, axis=1)\n",
        "\n",
        "def knn_rate_mean(pool_emb, pool_y, query_emb, k=50, self_exclude=False):\n",
        "    sims = query_emb @ pool_emb.T\n",
        "    if self_exclude and query_emb.shape[0] == pool_emb.shape[0]:\n",
        "        idx = np.arange(sims.shape[0]); sims[idx, idx] = -1e9\n",
        "    topk = _topk_idx(sims, k)\n",
        "    return pool_y[topk].mean(axis=1).astype(np.float32)\n",
        "\n",
        "def knn_rate_softmax(pool_emb, pool_y, query_emb, k=50, tau=0.12, self_exclude=False):\n",
        "    sims = query_emb @ pool_emb.T\n",
        "    if self_exclude and query_emb.shape[0] == pool_emb.shape[0]:\n",
        "        idx = np.arange(sims.shape[0]); sims[idx, idx] = -1e9\n",
        "    topk = _topk_idx(sims, k)\n",
        "    row_idx = np.arange(topk.shape[0])[:, None]\n",
        "    s_top = sims[row_idx, topk]\n",
        "    w = np.exp(s_top / max(1e-6, tau))\n",
        "    w /= (w.sum(axis=1, keepdims=True) + 1e-9)\n",
        "    y_top = pool_y[topk].astype(np.float32)\n",
        "    return (w * y_top).sum(axis=1).astype(np.float32)\n",
        "\n",
        "def knn_topk(pool_emb, query_emb, k=50, self_exclude=False):\n",
        "    sims = query_emb @ pool_emb.T\n",
        "    if self_exclude and query_emb.shape[0] == pool_emb.shape[0]:\n",
        "        idx = np.arange(sims.shape[0]); sims[idx, idx] = -1e9\n",
        "    topk = _topk_idx(sims, k)\n",
        "    return topk, sims\n",
        "\n",
        "def knn_rate_bayes(pool_emb, pool_y, query_emb, k=50, alpha=22.0, p_train=0.5, self_exclude=False):\n",
        "    topk, sims = knn_topk(pool_emb, query_emb, k=k, self_exclude=self_exclude)\n",
        "    y_top = pool_y[topk].astype(np.float32)\n",
        "    s = y_top.sum(axis=1)\n",
        "    denom = (k + alpha)\n",
        "    return ((s + alpha * p_train) / denom).astype(np.float32)\n",
        "\n",
        "def knn_rate_recency_decay(pool_emb, pool_y, query_emb, pool_ts_days, query_ts_days, k=50, lam_days=75.0, self_exclude=False):\n",
        "    topk, sims = knn_topk(pool_emb, query_emb, k=k, self_exclude=self_exclude)\n",
        "    row_idx = np.arange(topk.shape[0])[:, None]\n",
        "    qd = query_ts_days[row_idx[:,0]][:, None]\n",
        "    pdays = pool_ts_days[topk]\n",
        "    gaps = np.maximum(qd - pdays, 0.0)\n",
        "    w = np.exp(-gaps / max(lam_days, 1e-6))\n",
        "    y_top = pool_y[topk].astype(np.float32)\n",
        "    w_sum = w.sum(axis=1, keepdims=True) + 1e-9\n",
        "    return ((w * y_top).sum(axis=1) / w_sum[:,0]).astype(np.float32)\n",
        "\n",
        "from contextlib import contextmanager\n",
        "@contextmanager\n",
        "def timer(msg):\n",
        "    t0 = time.time(); print(f\"[T0] {msg} ...\", flush=True)\n",
        "    try: yield\n",
        "    finally: print(f\"[T+] {msg} done in {time.time()-t0:.2f}s\", flush=True)\n",
        "\n",
        "with timer('Load train/test and prepare inputs (MPNet)'):\n",
        "    tr = load_json_df('train.json')\n",
        "    te = load_json_df('test.json')\n",
        "    time_col = 'unix_timestamp_of_request_utc' if 'unix_timestamp_of_request_utc' in tr.columns else tr.columns[0]\n",
        "    label_col = 'requester_received_pizza' if 'requester_received_pizza' in tr.columns else 'label'\n",
        "    tr = tr.sort_values(time_col, kind='mergesort').reset_index(drop=True)\n",
        "    y = pd.to_numeric(tr[label_col], errors='coerce').fillna(0).astype(int).clip(0,1).values\n",
        "    ts_tr = pd.to_numeric(tr[time_col], errors='coerce').fillna(0).values.astype(np.int64)\n",
        "    ts_te = pd.to_numeric(te[time_col], errors='coerce').fillna(0).values.astype(np.int64) if time_col in te.columns else np.zeros(len(te), dtype=np.int64)\n",
        "    if ts_tr.max() > 10_000_000_000: ts_tr = ts_tr // 1_000_000_000\n",
        "    if ts_te.max() > 10_000_000_000: ts_te = ts_te // 1_000_000_000\n",
        "    ts_tr_days = (ts_tr / 86400.0).astype(np.float32)\n",
        "    ts_te_days = (ts_te / 86400.0).astype(np.float32)\n",
        "    ensure_mpnet_embeddings()\n",
        "    mp_tr = np.load('emb_cache/emb_mpnet_train.npy').astype(np.float32)\n",
        "    mp_te = np.load('emb_cache/emb_mpnet_test.npy').astype(np.float32)\n",
        "    e5_tr = np.load('emb_cache/emb_e5_train.npy').astype(np.float32)\n",
        "    e5_te = np.load('emb_cache/emb_e5_test.npy').astype(np.float32)\n",
        "    bge_tr = np.load('emb_cache/emb_bge_train.npy').astype(np.float32)\n",
        "    bge_te = np.load('emb_cache/emb_bge_test.npy').astype(np.float32)\n",
        "    meta_tr = per_row_meta(tr); meta_te = per_row_meta(te)\n",
        "    print('Shapes | mpnet:', mp_tr.shape, mp_te.shape, 'meta:', meta_tr.shape, meta_te.shape, '| e5:', e5_tr.shape, '| bge:', bge_tr.shape)\n",
        "\n",
        "fold_dir = Path('folds')\n",
        "manifest_path = fold_dir / 'manifest.json'\n",
        "if manifest_path.exists():\n",
        "    mf = json.loads(manifest_path.read_text())\n",
        "    chain_ids = [c['chain'] for c in mf.get('chains', [])]\n",
        "else:\n",
        "    val_files = sorted(fold_dir.glob('fc_chain*_val_idx.npy'))\n",
        "    chain_ids = sorted(int(p.stem.split('chain')[1].split('_')[0]) for p in val_files)\n",
        "print('Chains detected (from manifest if available):', chain_ids)\n",
        "\n",
        "oof = np.zeros(len(tr), dtype=np.float32)\n",
        "val_mask = np.zeros(len(tr), dtype=bool)\n",
        "test_preds_bag = []\n",
        "\n",
        "# Tighter XGB regularization\n",
        "param_grid = [\n",
        "    dict(max_depth=4, eta=0.08, min_child_weight=5),\n",
        "    dict(max_depth=4, eta=0.05, min_child_weight=5),\n",
        "    dict(max_depth=4, eta=0.08, min_child_weight=8),\n",
        "]\n",
        "\n",
        "def predict_with_best(bst, dmat):\n",
        "    bi = getattr(bst, 'best_iteration', None)\n",
        "    if bi is not None:\n",
        "        return bst.predict(dmat, iteration_range=(0, int(bi)+1))\n",
        "    return bst.predict(dmat)\n",
        "\n",
        "def standardize_knn_feats(tr_mat, va_mat, te_mat):\n",
        "    mu = tr_mat.mean(axis=0); sd = tr_mat.std(axis=0); sd = np.where(sd < 1e-6, 1.0, sd)\n",
        "    tr_s = (tr_mat - mu) / sd\n",
        "    va_s = (va_mat - mu) / sd\n",
        "    te_s = (te_mat - mu) / sd\n",
        "    return tr_s.astype(np.float32), va_s.astype(np.float32), te_s.astype(np.float32)\n",
        "\n",
        "def train_one_chain_seed(ci, seed):\n",
        "    tr_idx = np.load(fold_dir / f\"fc_chain{ci}_train_idx.npy\"); va_idx = np.load(fold_dir / f\"fc_chain{ci}_val_idx.npy\")\n",
        "    if len(va_idx) == 0:\n",
        "        print(f'Chain {ci} seed {seed}: empty val; skip'); return None, None, None\n",
        "    Xtr_emb, ytr = mp_tr[tr_idx], y[tr_idx]\n",
        "    Xva_emb, yva = mp_tr[va_idx], y[va_idx]\n",
        "    # Same-space kNN features (k=20/50/100 mean, softmax tau=0.12, cross-model (E5), recency-decay 75d, Bayes)\n",
        "    kn_tr_k20 = knn_rate_mean(Xtr_emb, ytr, Xtr_emb, k=20, self_exclude=True).reshape(-1,1)\n",
        "    kn_va_k20 = knn_rate_mean(Xtr_emb, ytr, Xva_emb, k=20, self_exclude=False).reshape(-1,1)\n",
        "    kn_tr_k50 = knn_rate_mean(Xtr_emb, ytr, Xtr_emb, k=50, self_exclude=True).reshape(-1,1)\n",
        "    kn_va_k50 = knn_rate_mean(Xtr_emb, ytr, Xva_emb, k=50, self_exclude=False).reshape(-1,1)\n",
        "    kn_tr_k100 = knn_rate_mean(Xtr_emb, ytr, Xtr_emb, k=100, self_exclude=True).reshape(-1,1)\n",
        "    kn_va_k100 = knn_rate_mean(Xtr_emb, ytr, Xva_emb, k=100, self_exclude=False).reshape(-1,1)\n",
        "    kn_tr_soft = knn_rate_softmax(Xtr_emb, ytr, Xtr_emb, k=50, tau=0.12, self_exclude=True).reshape(-1,1)\n",
        "    kn_va_soft = knn_rate_softmax(Xtr_emb, ytr, Xva_emb, k=50, tau=0.12, self_exclude=False).reshape(-1,1)\n",
        "    # Cross-model (E5 space)\n",
        "    kn_tr_cross = knn_rate_mean(e5_tr[tr_idx], ytr, e5_tr[tr_idx], k=50, self_exclude=True).reshape(-1,1)\n",
        "    kn_va_cross = knn_rate_mean(e5_tr[tr_idx], ytr, e5_tr[va_idx], k=50, self_exclude=False).reshape(-1,1)\n",
        "    # Recency-decayed k=50 (lam_days=75.0)\n",
        "    tr_days = ts_tr_days[tr_idx]; va_days = ts_tr_days[va_idx]\n",
        "    kn_tr_dec = knn_rate_recency_decay(Xtr_emb, ytr, Xtr_emb, tr_days, tr_days, k=50, lam_days=75.0, self_exclude=True).reshape(-1,1)\n",
        "    kn_va_dec = knn_rate_recency_decay(Xtr_emb, ytr, Xva_emb, tr_days, va_days, k=50, lam_days=75.0, self_exclude=False).reshape(-1,1)\n",
        "    # Bayesian-smoothed k=50\n",
        "    p_train = float(ytr.mean())\n",
        "    kn_tr_bayes = knn_rate_bayes(Xtr_emb, ytr, Xtr_emb, k=50, alpha=22.0, p_train=p_train, self_exclude=True).reshape(-1,1)\n",
        "    kn_va_bayes = knn_rate_bayes(Xtr_emb, ytr, Xva_emb, k=50, alpha=22.0, p_train=p_train, self_exclude=False).reshape(-1,1)\n",
        "    # Test features with full train pool\n",
        "    kn_te_k20  = knn_rate_mean(mp_tr, y, mp_te, k=20,  self_exclude=False).reshape(-1,1)\n",
        "    kn_te_k50  = knn_rate_mean(mp_tr, y, mp_te, k=50,  self_exclude=False).reshape(-1,1)\n",
        "    kn_te_k100 = knn_rate_mean(mp_tr, y, mp_te, k=100, self_exclude=False).reshape(-1,1)\n",
        "    kn_te_soft = knn_rate_softmax(mp_tr, y, mp_te, k=50, tau=0.12, self_exclude=False).reshape(-1,1)\n",
        "    kn_te_cross = knn_rate_mean(e5_tr, y, e5_te, k=50, self_exclude=False).reshape(-1,1)\n",
        "    kn_te_dec = knn_rate_recency_decay(mp_tr, y, mp_te, ts_tr_days, ts_te_days, k=50, lam_days=75.0, self_exclude=False).reshape(-1,1)\n",
        "    kn_te_bayes = knn_rate_bayes(mp_tr, y, mp_te, k=50, alpha=22.0, p_train=float(y.mean()), self_exclude=False).reshape(-1,1)\n",
        "    # Stack and standardize kNN features (7 features)\n",
        "    K_tr = np.hstack([kn_tr_k20, kn_tr_k50, kn_tr_k100, kn_tr_soft, kn_tr_cross, kn_tr_dec, kn_tr_bayes])\n",
        "    K_va = np.hstack([kn_va_k20, kn_va_k50, kn_va_k100, kn_va_soft, kn_va_cross, kn_va_dec, kn_va_bayes])\n",
        "    K_te = np.hstack([kn_te_k20, kn_te_k50, kn_te_k100, kn_te_soft, kn_te_cross, kn_te_dec, kn_te_bayes])\n",
        "    K_tr_s, K_va_s, K_te_s = standardize_knn_feats(K_tr, K_va, K_te)\n",
        "    # Final matrices\n",
        "    Xtr = np.hstack([Xtr_emb, meta_tr[tr_idx], K_tr_s]).astype(np.float32)\n",
        "    Xva = np.hstack([Xva_emb, meta_tr[va_idx], K_va_s]).astype(np.float32)\n",
        "    Xte = np.hstack([mp_te,   meta_te,        K_te_s]).astype(np.float32)\n",
        "    dtr = xgb.DMatrix(Xtr, label=ytr); dva = xgb.DMatrix(Xva, label=yva); dte = xgb.DMatrix(Xte)\n",
        "    pos = int((ytr==1).sum()); neg = int((ytr==0).sum()); spw = float(neg)/max(1.0, float(pos))\n",
        "    base = dict(tree_method='gpu_hist', objective='binary:logistic', eval_metric='auc',\n",
        "                subsample=0.8, colsample_bytree=0.8, reg_lambda=5.0, reg_alpha=0.2,\n",
        "                scale_pos_weight=spw, seed=seed)\n",
        "    best_auc=-1.0; best_pva=None; best_pte=None; best_desc=None\n",
        "    for g in param_grid:\n",
        "        params = base.copy(); params.update(g)\n",
        "        t0=time.time()\n",
        "        bst = xgb.train(params, dtr, num_boost_round=4000, evals=[(dva,'val')], early_stopping_rounds=120, verbose_eval=False)\n",
        "        pva = predict_with_best(bst, dva)\n",
        "        auc = roc_auc_score(yva, pva)\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_pva = auc, pva\n",
        "            best_pte = predict_with_best(bst, dte)\n",
        "            best_desc = g.copy(); best_desc['best_it']=getattr(bst,'best_iteration',None); best_desc['secs']=time.time()-t0\n",
        "    print(f'Chain {ci} seed {seed}: best={best_desc} AUC={best_auc:.5f}', flush=True)\n",
        "    return va_idx, best_pva, best_pte\n",
        "\n",
        "with timer('Train MPNet+XGB+robust kNN across chains and seeds'):\n",
        "    SEEDS=[42,1337,2025]\n",
        "    for seed in SEEDS:\n",
        "        test_preds_per_chain=[]\n",
        "        for ci in chain_ids:\n",
        "            res = train_one_chain_seed(ci, seed)\n",
        "            if res is None: continue\n",
        "            va_idx, pva, pte = res\n",
        "            if seed == SEEDS[0]:\n",
        "                oof[va_idx] = pva\n",
        "            else:\n",
        "                oof[va_idx] += pva\n",
        "            val_mask[va_idx] = True\n",
        "            test_preds_per_chain.append(pte)\n",
        "        if len(test_preds_per_chain):\n",
        "            test_preds_bag.append(np.mean(np.vstack(test_preds_per_chain), axis=0))\n",
        "\n",
        "with timer('Evaluate and save MPNet+XGB artifacts (reverted extras)'):\n",
        "    if val_mask.any():\n",
        "        oof_avg = oof.copy(); oof_avg[val_mask] = oof_avg[val_mask]/3.0\n",
        "        print('MPNet+XGB OOF AUC (val rows only, seed-bag):', round(roc_auc_score(y[val_mask], oof_avg[val_mask]),6))\n",
        "        np.save('oof_mpnet_xgb_fc.npy', oof_avg)\n",
        "    else:\n",
        "        print('Warning: no validation rows; OOF not computed.')\n",
        "    if len(test_preds_bag):\n",
        "        test_pred = np.mean(np.vstack(test_preds_bag), axis=0)\n",
        "        np.save('test_mpnet_xgb_fc.npy', test_pred)\n",
        "        sub = pd.read_csv('sampleSubmission.csv')\n",
        "        sub['requester_received_pizza'] = np.clip(test_pred, 0.01, 0.99)\n",
        "        sub.to_csv('submission_mpnet_xgb_fc.csv', index=False)\n",
        "        print('Saved: oof_mpnet_xgb_fc.npy, test_mpnet_xgb_fc.npy, submission_mpnet_xgb_fc.csv')\n",
        "    else:\n",
        "        print('Warning: no test preds collected.')"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Load train/test and prepare inputs (MPNet) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes | mpnet: (2878, 768) (1162, 768) meta: (2878, 15) (1162, 15) | e5: (2878, 768) | bge: (2878, 384)\n[T+] Load train/test and prepare inputs (MPNet) done in 0.23s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chains detected (from manifest if available): [1, 2, 3]\n[T0] Train MPNet+XGB+robust kNN across chains and seeds ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 1 seed 42: best={'max_depth': 4, 'eta': 0.08, 'min_child_weight': 8, 'best_it': 5, 'secs': 0.42093634605407715} AUC=0.61705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 2 seed 42: best={'max_depth': 4, 'eta': 0.05, 'min_child_weight': 5, 'best_it': 8, 'secs': 0.4458937644958496} AUC=0.65417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3 seed 42: best={'max_depth': 4, 'eta': 0.08, 'min_child_weight': 5, 'best_it': 6, 'secs': 0.4651782512664795} AUC=0.61171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 1 seed 1337: best={'max_depth': 4, 'eta': 0.05, 'min_child_weight': 5, 'best_it': 11, 'secs': 0.45186328887939453} AUC=0.63065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 2 seed 1337: best={'max_depth': 4, 'eta': 0.08, 'min_child_weight': 8, 'best_it': 6, 'secs': 0.4221508502960205} AUC=0.63841\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3 seed 1337: best={'max_depth': 4, 'eta': 0.08, 'min_child_weight': 5, 'best_it': 19, 'secs': 0.5003876686096191} AUC=0.59252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 1 seed 2025: best={'max_depth': 4, 'eta': 0.08, 'min_child_weight': 5, 'best_it': 14, 'secs': 0.46810150146484375} AUC=0.60003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 2 seed 2025: best={'max_depth': 4, 'eta': 0.05, 'min_child_weight': 5, 'best_it': 16, 'secs': 0.4728114604949951} AUC=0.66742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3 seed 2025: best={'max_depth': 4, 'eta': 0.08, 'min_child_weight': 5, 'best_it': 31, 'secs': 0.5465712547302246} AUC=0.62047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Train MPNet+XGB+robust kNN across chains and seeds done in 20.28s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Evaluate and save MPNet+XGB artifacts (reverted extras) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MPNet+XGB OOF AUC (val rows only, seed-bag): 0.589321\nSaved: oof_mpnet_xgb_fc.npy, test_mpnet_xgb_fc.npy, submission_mpnet_xgb_fc.csv\n[T+] Evaluate and save MPNet+XGB artifacts (reverted extras) done in 0.00s\n"
          ]
        }
      ]
    },
    {
      "id": "ada96bff-e63d-4ddd-9461-88ec07abbfe3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# New Leg: RoBERTa-base fine-tuning (title+body pairs) per forward chain via venv Trainer; caches OOF/test\n",
        "import os, json, textwrap, subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# Set SMOKE=0 for full run (all chains, 3 epochs, max_len=384, seeds [42,1337]); set to '1' only for quick smoke.\n",
        "os.environ['SMOKE'] = os.environ.get('SMOKE', '0')  # force default to full run unless explicitly set\n",
        "\n",
        "script = Path('roberta_ft_fc.py')\n",
        "code = textwrap.dedent('''\n",
        "import os, json, numpy as np, pandas as pd, torch, math, random, glob\n",
        "from pathlib import Path\n",
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,\n",
        "                          DataCollatorWithPadding, EarlyStoppingCallback)\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed);\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def load_json_df(path):\n",
        "    try: return pd.read_json(path, lines=True)\n",
        "    except ValueError:\n",
        "        try: return pd.read_json(path, lines=False)\n",
        "        except ValueError:\n",
        "            with open(path,'r',encoding='utf-8') as f: data=json.load(f)\n",
        "            if isinstance(data, dict) and 'data' in data: data=data['data']\n",
        "            return pd.json_normalize(data)\n",
        "\n",
        "def first_col(df, cols):\n",
        "    for c in cols:\n",
        "        if c in df.columns: return c\n",
        "    return None\n",
        "\n",
        "def extract_title_body(df):\n",
        "    tcol = first_col(df, ['request_title','title'])\n",
        "    bcol = first_col(df, ['request_text','body','text'])\n",
        "    t = df[tcol].fillna('') if tcol else pd.Series(['']*len(df))\n",
        "    b = df[bcol].fillna('') if bcol else pd.Series(['']*len(df))\n",
        "    return t.astype(str).tolist(), b.astype(str).tolist()\n",
        "\n",
        "class PairDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.enc = encodings\n",
        "        self.labels = None if labels is None else np.array(labels, dtype=np.int64)\n",
        "    def __len__(self):\n",
        "        return len(self.enc['input_ids'])\n",
        "    def __getitem__(self, i):\n",
        "        item = {k: torch.tensor(self.enc[k][i]) for k in self.enc.keys()}\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(int(self.labels[i]))\n",
        "        return item\n",
        "\n",
        "def rank01(x: np.ndarray) -> np.ndarray:\n",
        "    order = np.argsort(x)\n",
        "    ranks = np.empty_like(order, dtype=np.float64)\n",
        "    ranks[order] = np.arange(len(x))\n",
        "    r = ranks / max(1.0, (len(x) - 1))\n",
        "    return np.clip(r, 0.0, 1.0).astype(np.float32)\n",
        "\n",
        "def softmax_np(logits: np.ndarray) -> np.ndarray:\n",
        "    if logits.ndim == 1:\n",
        "        z = logits - np.max(logits)\n",
        "        e = np.exp(z)\n",
        "        return e / (e.sum() + 1e-12)\n",
        "    z = logits - logits.max(axis=1, keepdims=True)\n",
        "    e = np.exp(z)\n",
        "    return e / (e.sum(axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    if logits.ndim == 2 and logits.shape[1] == 2:\n",
        "        probs1 = softmax_np(logits)[:,1]\n",
        "    else:\n",
        "        # binary single-logit case\n",
        "        probs1 = 1.0/(1.0+np.exp(-logits.squeeze()))\n",
        "    try: auc = roc_auc_score(labels, probs1)\n",
        "    except Exception: auc = 0.5\n",
        "    return {'auc': float(auc)}\n",
        "\n",
        "def latest_checkpoint_dir(outdir: Path):\n",
        "    cks = sorted([p for p in outdir.glob('checkpoint-*') if p.is_dir()], key=lambda p: int(p.name.split('-')[-1]))\n",
        "    return cks[-1] if len(cks) else None\n",
        "\n",
        "def main():\n",
        "    fold_dir = Path('folds')\n",
        "    mf = json.loads((fold_dir/'manifest.json').read_text())\n",
        "    chains_all = [c['chain'] for c in mf['chains']]\n",
        "    tr = load_json_df('train.json')\n",
        "    te = load_json_df('test.json')\n",
        "    time_col = mf.get('time_col','unix_timestamp_of_request_utc')\n",
        "    label_col = mf.get('label_col','requester_received_pizza')\n",
        "    tr = tr.sort_values(time_col, kind='mergesort').reset_index(drop=True)\n",
        "    y = pd.to_numeric(tr[label_col], errors='coerce').fillna(0).astype(int).clip(0,1).values\n",
        "    tr_titles, tr_bodies = extract_title_body(tr)\n",
        "    te_titles, te_bodies = extract_title_body(te)\n",
        "\n",
        "    smoke = os.environ.get('SMOKE','0') == '1'\n",
        "    pred_only = os.environ.get('PRED_ONLY','0') == '1'\n",
        "    if smoke:\n",
        "        chains = [max(chains_all)]  # C3 only\n",
        "        max_length = 256\n",
        "        num_epochs = 1\n",
        "        seeds = [42]\n",
        "    else:\n",
        "        chains = chains_all\n",
        "        max_length = 384\n",
        "        num_epochs = 3\n",
        "        seeds = [42, 1337]\n",
        "\n",
        "    model_name = 'roberta-base'\n",
        "    tok = AutoTokenizer.from_pretrained(model_name)\n",
        "    collate = DataCollatorWithPadding(tokenizer=tok)\n",
        "\n",
        "    oof = np.zeros(len(tr), dtype=np.float32)\n",
        "    val_mask = np.zeros(len(tr), dtype=bool)\n",
        "    test_pred_seeds = []  # each entry: rank-avg across chains for a seed\n",
        "\n",
        "    for seed in seeds:\n",
        "        set_seed(seed)\n",
        "        test_preds_per_chain = []\n",
        "        for ci in chains:\n",
        "            tr_idx = np.load(fold_dir / f'fc_chain{ci}_train_idx.npy')\n",
        "            va_idx = np.load(fold_dir / f'fc_chain{ci}_val_idx.npy')\n",
        "            if len(va_idx) == 0:\n",
        "                continue\n",
        "            val_mask[va_idx] = True\n",
        "            enc_tr = tok([tr_titles[i] for i in tr_idx], [tr_bodies[i] for i in tr_idx], truncation=True, padding=False, max_length=max_length)\n",
        "            enc_va = tok([tr_titles[i] for i in va_idx], [tr_bodies[i] for i in va_idx], truncation=True, padding=False, max_length=max_length)\n",
        "            enc_te = tok(te_titles, te_bodies, truncation=True, padding=False, max_length=max_length)\n",
        "            ds_tr = PairDataset(enc_tr, y[tr_idx])\n",
        "            ds_va = PairDataset(enc_va, y[va_idx])\n",
        "            ds_te = PairDataset(enc_te, None)\n",
        "            out_dir = Path(f'./roberta_fc_c{ci}_s{seed}')\n",
        "            out_dir.mkdir(exist_ok=True, parents=True)\n",
        "            ckpt = latest_checkpoint_dir(out_dir)\n",
        "            model = None\n",
        "            if pred_only and (ckpt is not None):\n",
        "                model = AutoModelForSequenceClassification.from_pretrained(str(ckpt))\n",
        "            else:\n",
        "                model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "                if hasattr(model, 'gradient_checkpointing_enable'):\n",
        "                    model.gradient_checkpointing_enable()\n",
        "                eff_bs = 16 * 2\n",
        "                eval_steps = max(100, len(tr_idx) // max(1, eff_bs*2))\n",
        "                args = TrainingArguments(\n",
        "                    output_dir=str(out_dir),\n",
        "                    learning_rate=2e-5,\n",
        "                    weight_decay=0.01,\n",
        "                    lr_scheduler_type='cosine',\n",
        "                    warmup_ratio=0.06,\n",
        "                    per_device_train_batch_size=16,\n",
        "                    gradient_accumulation_steps=2,\n",
        "                    per_device_eval_batch_size=64,\n",
        "                    num_train_epochs=num_epochs,\n",
        "                    evaluation_strategy='steps',\n",
        "                    eval_steps=eval_steps,\n",
        "                    save_strategy='steps',\n",
        "                    save_steps=eval_steps,\n",
        "                    save_total_limit=1,\n",
        "                    logging_steps=max(50, eval_steps//2),\n",
        "                    load_best_model_at_end=True,\n",
        "                    metric_for_best_model='auc',\n",
        "                    greater_is_better=True,\n",
        "                    fp16=torch.cuda.is_available(),\n",
        "                    disable_tqdm=True,\n",
        "                    seed=seed\n",
        "                )\n",
        "                trainer = Trainer(\n",
        "                    model=model,\n",
        "                    args=args,\n",
        "                    train_dataset=ds_tr,\n",
        "                    eval_dataset=ds_va,\n",
        "                    tokenizer=tok,\n",
        "                    data_collator=collate,\n",
        "                    compute_metrics=compute_metrics,\n",
        "                    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
        "                )\n",
        "                trainer.train()\n",
        "                ckpt = latest_checkpoint_dir(out_dir) or out_dir\n",
        "                model.save_pretrained(str(ckpt))\n",
        "            # Predict val/test with best/last checkpoint\n",
        "            model.eval()\n",
        "            trainer_pred = Trainer(model=model, tokenizer=tok, data_collator=collate)\n",
        "            logits_va = trainer_pred.predict(ds_va).predictions\n",
        "            if logits_va.ndim == 2 and logits_va.shape[1] == 2:\n",
        "                pva = softmax_np(logits_va)[:,1].astype(np.float32)\n",
        "            else:\n",
        "                pva = (1.0/(1.0+np.exp(-logits_va.squeeze()))).astype(np.float32)\n",
        "            oof[va_idx] = (oof[va_idx] + pva) if (seed != seeds[0]) else pva\n",
        "            try:\n",
        "                auc = roc_auc_score(y[va_idx], pva)\n",
        "                print(f'Chain {ci} seed {seed}: val AUC={auc:.6f}')\n",
        "            except Exception:\n",
        "                pass\n",
        "            logits_te = trainer_pred.predict(ds_te).predictions\n",
        "            if logits_te.ndim == 2 and logits_te.shape[1] == 2:\n",
        "                pte = softmax_np(logits_te)[:,1].astype(np.float32)\n",
        "            else:\n",
        "                pte = (1.0/(1.0+np.exp(-logits_te.squeeze()))).astype(np.float32)\n",
        "            test_preds_per_chain.append(rank01(pte))\n",
        "        if len(test_preds_per_chain):\n",
        "            test_rank_seed = np.mean(np.vstack(test_preds_per_chain), axis=0)\n",
        "            test_pred_seeds.append(test_rank_seed.astype(np.float32))\n",
        "\n",
        "    if len(seeds) > 1:\n",
        "        oof[val_mask] = oof[val_mask] / float(len(seeds))\n",
        "\n",
        "    if val_mask.any():\n",
        "        try:\n",
        "            auc_oof = roc_auc_score(y[val_mask], oof[val_mask])\n",
        "            print('RoBERTa FT OOF AUC (val rows):', round(float(auc_oof),6))\n",
        "        except Exception:\n",
        "            pass\n",
        "    np.save('oof_roberta_ft_fc.npy', oof.astype(np.float32))\n",
        "    if len(test_pred_seeds):\n",
        "        test_rank_final = np.mean(np.vstack(test_pred_seeds), axis=0)\n",
        "        np.save('test_roberta_ft_fc.npy', test_rank_final.astype(np.float32))\n",
        "        sub = pd.read_csv('sampleSubmission.csv')\n",
        "        sub['requester_received_pizza'] = np.clip(test_rank_final, 0.01, 0.99)\n",
        "        sub.to_csv('submission_roberta_ft_fc.csv', index=False)\n",
        "        print('Saved: oof_roberta_ft_fc.npy, test_roberta_ft_fc.npy, submission_roberta_ft_fc.csv')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "''')\n",
        "script.write_text(code)\n",
        "print('Wrote roberta_ft_fc.py')\n",
        "\n",
        "# Run via venv (has working torch/cu121 and HF stack)\n",
        "venv_py = Path('.venv/bin/python')\n",
        "assert venv_py.exists(), 'Missing .venv python; run setup cell 1 first'\n",
        "env = os.environ.copy()\n",
        "env['SMOKE'] = '0'  # force full run in subprocess regardless of parent env\n",
        "env['PRED_ONLY'] = env.get('PRED_ONLY', '1')  # default to predict-only to quickly fix probability extraction\n",
        "print('SMOKE mode =', env.get('SMOKE','0'), '| PRED_ONLY =', env.get('PRED_ONLY','0'))\n",
        "print('Launching fine-tune/predict with', venv_py)\n",
        "proc = subprocess.run([str(venv_py), str(script)], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, env=env)\n",
        "print(proc.stdout)\n",
        "if proc.returncode != 0:\n",
        "    raise RuntimeError('RoBERTa FT/predict failed')\n",
        "print('RoBERTa FT leg updated (softmax probs). Re-run Cell 12 to reblend.')"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote roberta_ft_fc.py\nSMOKE mode = 0 | PRED_ONLY = 1\nLaunching fine-tune/predict with .venv/bin/python\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n\n  0%|          | 0/71 [00:00<?, ?it/s]\n  7%|\u258b         | 5/71 [00:00<00:01, 39.16it/s]\n 13%|\u2588\u258e        | 9/71 [00:00<00:01, 33.02it/s]\n 18%|\u2588\u258a        | 13/71 [00:00<00:02, 27.47it/s]\n 23%|\u2588\u2588\u258e       | 16/71 [00:00<00:02, 27.17it/s]\n 28%|\u2588\u2588\u258a       | 20/71 [00:00<00:01, 29.24it/s]\n 34%|\u2588\u2588\u2588\u258d      | 24/71 [00:00<00:01, 30.71it/s]\n 39%|\u2588\u2588\u2588\u2589      | 28/71 [00:00<00:01, 31.13it/s]\n 45%|\u2588\u2588\u2588\u2588\u258c     | 32/71 [00:01<00:01, 32.60it/s]\n 51%|\u2588\u2588\u2588\u2588\u2588     | 36/71 [00:01<00:01, 28.68it/s]\n 55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 39/71 [00:01<00:01, 28.02it/s]\n 59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 42/71 [00:01<00:01, 27.98it/s]\n 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 45/71 [00:01<00:00, 26.68it/s]\n 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 49/71 [00:01<00:00, 26.15it/s]\n 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 53/71 [00:01<00:00, 26.43it/s]\n 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 56/71 [00:01<00:00, 25.08it/s]\n 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 59/71 [00:02<00:00, 25.20it/s]\n 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 62/71 [00:02<00:00, 25.29it/s]\n 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 65/71 [00:02<00:00, 24.01it/s]\n 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 68/71 [00:02<00:00, 22.85it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71/71 [00:02<00:00, 27.13it/s]\nChain 1 seed 42: val AUC=0.625708\n\n  0%|          | 0/146 [00:00<?, ?it/s]\n  9%|\u2589         | 13/146 [00:00<00:01, 127.31it/s]\n 18%|\u2588\u258a        | 26/146 [00:00<00:00, 127.89it/s]\n 27%|\u2588\u2588\u258b       | 40/146 [00:00<00:00, 130.12it/s]\n 37%|\u2588\u2588\u2588\u258b      | 54/146 [00:00<00:00, 132.78it/s]\n 47%|\u2588\u2588\u2588\u2588\u258b     | 68/146 [00:00<00:00, 128.89it/s]\n 55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 81/146 [00:00<00:00, 127.46it/s]\n 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 94/146 [00:00<00:00, 126.15it/s]\n 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 107/146 [00:00<00:00, 127.06it/s]\n 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 120/146 [00:00<00:00, 124.83it/s]\n 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 133/146 [00:01<00:00, 124.42it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 146/146 [00:01<00:00, 125.93it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 146/146 [00:01<00:00, 126.98it/s]\n\n  0%|          | 0/54 [00:00<?, ?it/s]\n  7%|\u258b         | 4/54 [00:00<00:01, 38.97it/s]\n 15%|\u2588\u258d        | 8/54 [00:00<00:01, 27.01it/s]\n 20%|\u2588\u2588        | 11/54 [00:00<00:01, 27.07it/s]\n 26%|\u2588\u2588\u258c       | 14/54 [00:00<00:01, 22.71it/s]\n 31%|\u2588\u2588\u2588\u258f      | 17/54 [00:00<00:01, 24.23it/s]\n 37%|\u2588\u2588\u2588\u258b      | 20/54 [00:00<00:01, 22.75it/s]\n 44%|\u2588\u2588\u2588\u2588\u258d     | 24/54 [00:00<00:01, 27.13it/s]\n 50%|\u2588\u2588\u2588\u2588\u2588     | 27/54 [00:01<00:00, 27.79it/s]\n 56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 30/54 [00:01<00:00, 26.75it/s]\n 61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 33/54 [00:01<00:00, 27.51it/s]\n 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 37/54 [00:01<00:00, 27.36it/s]\n 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 40/54 [00:01<00:00, 25.97it/s]\n 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 43/54 [00:01<00:00, 26.95it/s]\n 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 46/54 [00:01<00:00, 26.39it/s]\n 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 49/54 [00:01<00:00, 25.18it/s]\n 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 52/54 [00:02<00:00, 23.48it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 54/54 [00:02<00:00, 25.64it/s]\nChain 2 seed 42: val AUC=0.668824\n\n  0%|          | 0/146 [00:00<?, ?it/s]\n  9%|\u2589         | 13/146 [00:00<00:01, 125.63it/s]\n 18%|\u2588\u258a        | 26/146 [00:00<00:00, 126.56it/s]\n 27%|\u2588\u2588\u258b       | 40/146 [00:00<00:00, 129.53it/s]\n 37%|\u2588\u2588\u2588\u258b      | 54/146 [00:00<00:00, 132.20it/s]\n 47%|\u2588\u2588\u2588\u2588\u258b     | 68/146 [00:00<00:00, 128.47it/s]\n 55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 81/146 [00:00<00:00, 127.37it/s]\n 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 94/146 [00:00<00:00, 126.41it/s]\n 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 107/146 [00:00<00:00, 127.13it/s]\n 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 120/146 [00:00<00:00, 124.80it/s]\n 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 133/146 [00:01<00:00, 124.64it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 146/146 [00:01<00:00, 126.10it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 146/146 [00:01<00:00, 126.86it/s]\n\n  0%|          | 0/71 [00:00<?, ?it/s]\n  6%|\u258c         | 4/71 [00:00<00:01, 34.98it/s]\n 11%|\u2588\u258f        | 8/71 [00:00<00:01, 32.73it/s]\n 17%|\u2588\u258b        | 12/71 [00:00<00:01, 31.82it/s]\n 23%|\u2588\u2588\u258e       | 16/71 [00:00<00:01, 31.51it/s]\n 28%|\u2588\u2588\u258a       | 20/71 [00:00<00:01, 29.43it/s]\n 32%|\u2588\u2588\u2588\u258f      | 23/71 [00:00<00:01, 27.62it/s]\n 37%|\u2588\u2588\u2588\u258b      | 26/71 [00:00<00:01, 25.68it/s]\n 41%|\u2588\u2588\u2588\u2588      | 29/71 [00:01<00:01, 23.81it/s]\n 45%|\u2588\u2588\u2588\u2588\u258c     | 32/71 [00:01<00:01, 22.95it/s]\n 49%|\u2588\u2588\u2588\u2588\u2589     | 35/71 [00:01<00:01, 20.29it/s]\n 54%|\u2588\u2588\u2588\u2588\u2588\u258e    | 38/71 [00:01<00:01, 21.23it/s]\n 59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 42/71 [00:01<00:01, 23.80it/s]\n 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 45/71 [00:01<00:01, 21.74it/s]\n 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 48/71 [00:01<00:01, 22.37it/s]\n 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 51/71 [00:02<00:00, 24.02it/s]\n 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 54/71 [00:02<00:00, 24.49it/s]\n 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 57/71 [00:02<00:00, 24.36it/s]\n 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 61/71 [00:02<00:00, 26.97it/s]\n 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 64/71 [00:02<00:00, 24.42it/s]\n 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 68/71 [00:02<00:00, 26.68it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71/71 [00:02<00:00, 25.67it/s]\nChain 3 seed 42: val AUC=0.622832\n\n  0%|          | 0/146 [00:00<?, ?it/s]\n  9%|\u2589         | 13/146 [00:00<00:01, 125.96it/s]\n 18%|\u2588\u258a        | 26/146 [00:00<00:00, 126.28it/s]\n 27%|\u2588\u2588\u258b       | 40/146 [00:00<00:00, 128.57it/s]\n 37%|\u2588\u2588\u2588\u258b      | 54/146 [00:00<00:00, 131.23it/s]\n 47%|\u2588\u2588\u2588\u2588\u258b     | 68/146 [00:00<00:00, 127.62it/s]\n 55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 81/146 [00:00<00:00, 125.77it/s]\n 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 94/146 [00:00<00:00, 124.73it/s]\n 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 107/146 [00:00<00:00, 125.42it/s]\n 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 120/146 [00:00<00:00, 123.20it/s]\n 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 133/146 [00:01<00:00, 123.48it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 146/146 [00:01<00:00, 125.20it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 146/146 [00:01<00:00, 125.74it/s]\n\n  0%|          | 0/71 [00:00<?, ?it/s]\n  6%|\u258c         | 4/71 [00:00<00:01, 39.21it/s]\n 11%|\u2588\u258f        | 8/71 [00:00<00:01, 31.79it/s]\n 17%|\u2588\u258b        | 12/71 [00:00<00:01, 29.84it/s]\n 23%|\u2588\u2588\u258e       | 16/71 [00:00<00:02, 26.63it/s]\n 28%|\u2588\u2588\u258a       | 20/71 [00:00<00:01, 28.74it/s]\n 32%|\u2588\u2588\u2588\u258f      | 23/71 [00:00<00:01, 29.05it/s]\n 38%|\u2588\u2588\u2588\u258a      | 27/71 [00:00<00:01, 30.45it/s]\n 44%|\u2588\u2588\u2588\u2588\u258e     | 31/71 [00:01<00:01, 31.67it/s]\n 49%|\u2588\u2588\u2588\u2588\u2589     | 35/71 [00:01<00:01, 29.84it/s]\n 55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 39/71 [00:01<00:01, 27.92it/s]\n 59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 42/71 [00:01<00:01, 27.85it/s]\n 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 45/71 [00:01<00:00, 26.60it/s]\n 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 49/71 [00:01<00:00, 26.01it/s]\n 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 53/71 [00:01<00:00, 26.04it/s]\n 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 56/71 [00:02<00:00, 24.73it/s]\n 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 59/71 [00:02<00:00, 24.88it/s]\n 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 62/71 [00:02<00:00, 25.06it/s]\n 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 65/71 [00:02<00:00, 23.73it/s]\n 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 68/71 [00:02<00:00, 22.61it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71/71 [00:02<00:00, 26.83it/s]\nChain 1 seed 1337: val AUC=0.631759\n\n  0%|          | 0/146 [00:00<?, ?it/s]\n  9%|\u2589         | 13/146 [00:00<00:01, 126.33it/s]\n 18%|\u2588\u258a        | 26/146 [00:00<00:00, 126.51it/s]\n 27%|\u2588\u2588\u258b       | 40/146 [00:00<00:00, 128.72it/s]\n 37%|\u2588\u2588\u2588\u258b      | 54/146 [00:00<00:00, 131.06it/s]\n 47%|\u2588\u2588\u2588\u2588\u258b     | 68/146 [00:00<00:00, 127.54it/s]\n 55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 81/146 [00:00<00:00, 126.25it/s]\n 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 94/146 [00:00<00:00, 124.67it/s]\n 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 107/146 [00:00<00:00, 125.64it/s]\n 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 120/146 [00:00<00:00, 123.55it/s]\n 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 133/146 [00:01<00:00, 123.28it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 146/146 [00:01<00:00, 124.71it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 146/146 [00:01<00:00, 125.67it/s]\n\n  0%|          | 0/54 [00:00<?, ?it/s]\n  7%|\u258b         | 4/54 [00:00<00:01, 38.61it/s]\n 15%|\u2588\u258d        | 8/54 [00:00<00:01, 26.74it/s]\n 20%|\u2588\u2588        | 11/54 [00:00<00:01, 26.72it/s]\n 26%|\u2588\u2588\u258c       | 14/54 [00:00<00:01, 22.44it/s]\n 31%|\u2588\u2588\u2588\u258f      | 17/54 [00:00<00:01, 23.89it/s]\n 37%|\u2588\u2588\u2588\u258b      | 20/54 [00:00<00:01, 22.38it/s]\n 44%|\u2588\u2588\u2588\u2588\u258d     | 24/54 [00:00<00:01, 26.63it/s]\n 50%|\u2588\u2588\u2588\u2588\u2588     | 27/54 [00:01<00:00, 27.22it/s]\n 56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 30/54 [00:01<00:00, 26.27it/s]\n 61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 33/54 [00:01<00:00, 27.08it/s]\n 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 37/54 [00:01<00:00, 27.00it/s]\n 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 40/54 [00:01<00:00, 25.66it/s]\n 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 43/54 [00:01<00:00, 26.67it/s]\n 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 46/54 [00:01<00:00, 25.97it/s]\n 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 49/54 [00:01<00:00, 24.77it/s]\n 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 52/54 [00:02<00:00, 23.14it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 54/54 [00:02<00:00, 25.27it/s]\nChain 2 seed 1337: val AUC=0.663520\n\n  0%|          | 0/146 [00:00<?, ?it/s]\n  9%|\u2589         | 13/146 [00:00<00:01, 125.32it/s]\n 18%|\u2588\u258a        | 26/146 [00:00<00:00, 126.39it/s]\n 27%|\u2588\u2588\u258b       | 40/146 [00:00<00:00, 128.67it/s]\n 37%|\u2588\u2588\u2588\u258b      | 54/146 [00:00<00:00, 131.28it/s]\n 47%|\u2588\u2588\u2588\u2588\u258b     | 68/146 [00:00<00:00, 127.54it/s]\n 55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 81/146 [00:00<00:00, 125.85it/s]\n 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 94/146 [00:00<00:00, 124.36it/s]\n 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 107/146 [00:00<00:00, 125.12it/s]\n 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 120/146 [00:00<00:00, 122.65it/s]\n 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 133/146 [00:01<00:00, 122.77it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 146/146 [00:01<00:00, 125.50it/s]\n\n  0%|          | 0/71 [00:00<?, ?it/s]\n  6%|\u258c         | 4/71 [00:00<00:01, 34.39it/s]\n 11%|\u2588\u258f        | 8/71 [00:00<00:01, 32.64it/s]\n 17%|\u2588\u258b        | 12/71 [00:00<00:01, 31.57it/s]\n 23%|\u2588\u2588\u258e       | 16/71 [00:00<00:01, 31.31it/s]\n 28%|\u2588\u2588\u258a       | 20/71 [00:00<00:01, 29.16it/s]\n 32%|\u2588\u2588\u2588\u258f      | 23/71 [00:00<00:01, 27.43it/s]\n 37%|\u2588\u2588\u2588\u258b      | 26/71 [00:00<00:01, 25.59it/s]\n 41%|\u2588\u2588\u2588\u2588      | 29/71 [00:01<00:01, 23.69it/s]\n 45%|\u2588\u2588\u2588\u2588\u258c     | 32/71 [00:01<00:01, 22.83it/s]\n 49%|\u2588\u2588\u2588\u2588\u2589     | 35/71 [00:01<00:01, 20.21it/s]\n 54%|\u2588\u2588\u2588\u2588\u2588\u258e    | 38/71 [00:01<00:01, 21.06it/s]\n 59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 42/71 [00:01<00:01, 23.62it/s]\n 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 45/71 [00:01<00:01, 21.53it/s]\n 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 48/71 [00:01<00:01, 22.18it/s]\n 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 51/71 [00:02<00:00, 23.83it/s]\n 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 54/71 [00:02<00:00, 24.30it/s]\n 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 57/71 [00:02<00:00, 24.10it/s]\n 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 61/71 [00:02<00:00, 26.85it/s]\n 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 64/71 [00:02<00:00, 24.32it/s]\n 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 67/71 [00:02<00:00, 25.68it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71/71 [00:02<00:00, 28.92it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71/71 [00:02<00:00, 25.48it/s]\nChain 3 seed 1337: val AUC=0.624554\n\n  0%|          | 0/146 [00:00<?, ?it/s]\n  9%|\u2589         | 13/146 [00:00<00:01, 123.19it/s]\n 18%|\u2588\u258a        | 26/146 [00:00<00:00, 125.71it/s]\n 27%|\u2588\u2588\u258b       | 40/146 [00:00<00:00, 128.12it/s]\n 37%|\u2588\u2588\u2588\u258b      | 54/146 [00:00<00:00, 130.96it/s]\n 47%|\u2588\u2588\u2588\u2588\u258b     | 68/146 [00:00<00:00, 127.37it/s]\n 55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 81/146 [00:00<00:00, 125.95it/s]\n 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 94/146 [00:00<00:00, 124.68it/s]\n 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 107/146 [00:00<00:00, 125.42it/s]\n 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 120/146 [00:00<00:00, 122.52it/s]\n 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 133/146 [00:01<00:00, 122.51it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 146/146 [00:01<00:00, 124.37it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 146/146 [00:01<00:00, 125.19it/s]\nRoBERTa FT OOF AUC (val rows): 0.6288\nSaved: oof_roberta_ft_fc.npy, test_roberta_ft_fc.npy, submission_roberta_ft_fc.csv\n\nRoBERTa FT leg updated (softmax probs). Re-run Cell 12 to reblend.\n"
          ]
        }
      ]
    },
    {
      "id": "91b1aa54-3a35-4470-b67b-301cb6b722f0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# New Leg: DeBERTa-v3-base fine-tuning (title+body pairs), forward-chaining, 2 seeds; caches OOF/test\n",
        "import os, json, textwrap, subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "script = Path('deberta_ft_fc.py')\n",
        "code = textwrap.dedent('''\n",
        "import os, json, numpy as np, pandas as pd, torch, random\n",
        "from pathlib import Path\n",
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,\n",
        "                          DataCollatorWithPadding, EarlyStoppingCallback)\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def load_json_df(path):\n",
        "    try: return pd.read_json(path, lines=True)\n",
        "    except ValueError:\n",
        "        try: return pd.read_json(path, lines=False)\n",
        "        except ValueError:\n",
        "            with open(path,'r',encoding='utf-8') as f: data=json.load(f)\n",
        "            if isinstance(data, dict) and 'data' in data: data=data['data']\n",
        "            return pd.json_normalize(data)\n",
        "\n",
        "def first_col(df, cols):\n",
        "    for c in cols:\n",
        "        if c in df.columns: return c\n",
        "    return None\n",
        "\n",
        "def extract_title_body(df):\n",
        "    tcol = first_col(df, ['request_title','title'])\n",
        "    bcol = first_col(df, ['request_text','body','text'])\n",
        "    t = df[tcol].fillna('') if tcol else pd.Series(['']*len(df))\n",
        "    b = df[bcol].fillna('') if bcol else pd.Series(['']*len(df))\n",
        "    return t.astype(str).tolist(), b.astype(str).tolist()\n",
        "\n",
        "class PairDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.enc = encodings\n",
        "        self.labels = None if labels is None else np.array(labels, dtype=np.int64)\n",
        "    def __len__(self):\n",
        "        return len(self.enc['input_ids'])\n",
        "    def __getitem__(self, i):\n",
        "        item = {k: torch.tensor(self.enc[k][i]) for k in self.enc.keys()}\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(int(self.labels[i]))\n",
        "        return item\n",
        "\n",
        "def softmax_np(logits: np.ndarray) -> np.ndarray:\n",
        "    if logits.ndim == 1:\n",
        "        z = logits - np.max(logits); e = np.exp(z); return e / (e.sum() + 1e-12)\n",
        "    z = logits - logits.max(axis=1, keepdims=True)\n",
        "    e = np.exp(z)\n",
        "    return e / (e.sum(axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    p1 = softmax_np(logits)[:,1] if (logits.ndim==2 and logits.shape[1]==2) else 1.0/(1.0+np.exp(-logits.squeeze()))\n",
        "    try: auc = roc_auc_score(labels, p1)\n",
        "    except Exception: auc = 0.5\n",
        "    return {'auc': float(auc)}\n",
        "\n",
        "def latest_checkpoint_dir(outdir: Path):\n",
        "    cks = sorted([p for p in outdir.glob('checkpoint-*') if p.is_dir()], key=lambda p: int(p.name.split('-')[-1]))\n",
        "    return cks[-1] if len(cks) else None\n",
        "\n",
        "def main():\n",
        "    fold_dir = Path('folds')\n",
        "    mf = json.loads((fold_dir/'manifest.json').read_text())\n",
        "    chains_all = [c['chain'] for c in mf['chains']]\n",
        "    tr = load_json_df('train.json'); te = load_json_df('test.json')\n",
        "    time_col = mf.get('time_col','unix_timestamp_of_request_utc')\n",
        "    label_col = mf.get('label_col','requester_received_pizza')\n",
        "    tr = tr.sort_values(time_col, kind='mergesort').reset_index(drop=True)\n",
        "    y = pd.to_numeric(tr[label_col], errors='coerce').fillna(0).astype(int).clip(0,1).values\n",
        "    tr_titles, tr_bodies = extract_title_body(tr); te_titles, te_bodies = extract_title_body(te)\n",
        "\n",
        "    max_length = int(os.environ.get('MAX_LEN','384'))\n",
        "    seeds = [int(s) for s in os.environ.get('SEEDS','42,1337').split(',')]\n",
        "\n",
        "    model_name = 'microsoft/deberta-v3-base'\n",
        "    tok = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
        "    collate = DataCollatorWithPadding(tokenizer=tok)\n",
        "\n",
        "    oof = np.zeros(len(tr), dtype=np.float32); val_mask = np.zeros(len(tr), dtype=bool)\n",
        "    test_pred_seeds = []\n",
        "\n",
        "    for seed in seeds:\n",
        "        set_seed(seed)\n",
        "        test_preds_per_chain = []\n",
        "        for ci in chains_all:\n",
        "            tr_idx = np.load(fold_dir / f'fc_chain{ci}_train_idx.npy')\n",
        "            va_idx = np.load(fold_dir / f'fc_chain{ci}_val_idx.npy')\n",
        "            if len(va_idx)==0: continue\n",
        "            val_mask[va_idx] = True\n",
        "            enc_tr = tok([tr_titles[i] for i in tr_idx], [tr_bodies[i] for i in tr_idx], truncation=True, padding=False, max_length=max_length)\n",
        "            enc_va = tok([tr_titles[i] for i in va_idx], [tr_bodies[i] for i in va_idx], truncation=True, padding=False, max_length=max_length)\n",
        "            enc_te = tok(te_titles, te_bodies, truncation=True, padding=False, max_length=max_length)\n",
        "            ds_tr = PairDataset(enc_tr, y[tr_idx]); ds_va = PairDataset(enc_va, y[va_idx]); ds_te = PairDataset(enc_te, None)\n",
        "            out_dir = Path(f'./deberta_fc_c{ci}_s{seed}'); out_dir.mkdir(exist_ok=True, parents=True)\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "            if hasattr(model, 'gradient_checkpointing_enable'): model.gradient_checkpointing_enable()\n",
        "            eff_bs = 16*2\n",
        "            eval_steps = max(100, len(tr_idx)//max(1, eff_bs*2))\n",
        "            args = TrainingArguments(\n",
        "                output_dir=str(out_dir),\n",
        "                learning_rate=float(os.environ.get('LR','1.5e-5')),\n",
        "                weight_decay=0.01,\n",
        "                lr_scheduler_type='cosine',\n",
        "                warmup_ratio=float(os.environ.get('WARMUP','0.10')),\n",
        "                per_device_train_batch_size=16,\n",
        "                gradient_accumulation_steps=2,\n",
        "                per_device_eval_batch_size=64,\n",
        "                num_train_epochs=int(os.environ.get('EPOCHS','3')),\n",
        "                evaluation_strategy='steps',\n",
        "                eval_steps=eval_steps,\n",
        "                save_strategy='steps',\n",
        "                save_steps=eval_steps,\n",
        "                save_total_limit=1,\n",
        "                logging_steps=max(50, eval_steps//2),\n",
        "                load_best_model_at_end=True,\n",
        "                metric_for_best_model='auc',\n",
        "                greater_is_better=True,\n",
        "                fp16=torch.cuda.is_available(),\n",
        "                disable_tqdm=True,\n",
        "                seed=seed\n",
        "            )\n",
        "            trainer = Trainer(model=model, args=args, train_dataset=ds_tr, eval_dataset=ds_va, tokenizer=tok,\n",
        "                              data_collator=collate, compute_metrics=compute_metrics,\n",
        "                              callbacks=[EarlyStoppingCallback(early_stopping_patience=1)])\n",
        "            trainer.train()\n",
        "            # Predict best\n",
        "            model.eval(); pred_trainer = Trainer(model=model, tokenizer=tok, data_collator=collate)\n",
        "            logits_va = pred_trainer.predict(ds_va).predictions\n",
        "            pva = softmax_np(logits_va)[:,1].astype(np.float32) if (logits_va.ndim==2 and logits_va.shape[1]==2) else (1.0/(1.0+np.exp(-logits_va.squeeze()))).astype(np.float32)\n",
        "            oof[va_idx] = (oof[va_idx]+pva) if (seed!=seeds[0]) else pva\n",
        "            try: print(f'Chain {ci} seed {seed}: val AUC={roc_auc_score(y[va_idx], pva):.6f}')\n",
        "            except Exception: pass\n",
        "            logits_te = pred_trainer.predict(ds_te).predictions\n",
        "            pte = softmax_np(logits_te)[:,1].astype(np.float32) if (logits_te.ndim==2 and logits_te.shape[1]==2) else (1.0/(1.0+np.exp(-logits_te.squeeze()))).astype(np.float32)\n",
        "            # rank per chain then average\n",
        "            order = np.argsort(pte); ranks = np.empty_like(order, dtype=np.float64); ranks[order] = np.arange(len(pte)); ranks = ranks/ max(1,(len(pte)-1));\n",
        "            test_preds_per_chain.append(ranks.astype(np.float32))\n",
        "        if len(test_preds_per_chain):\n",
        "            test_pred_seeds.append(np.mean(np.vstack(test_preds_per_chain), axis=0).astype(np.float32))\n",
        "\n",
        "    if len(seeds)>1: oof[val_mask] = oof[val_mask]/float(len(seeds))\n",
        "    if val_mask.any():\n",
        "        try: print('DeBERTa FT OOF AUC (val rows):', round(float(roc_auc_score(y[val_mask], oof[val_mask])),6))\n",
        "        except Exception: pass\n",
        "    np.save('oof_deberta_ft_fc.npy', oof.astype(np.float32))\n",
        "    if len(test_pred_seeds):\n",
        "        test_rank_final = np.mean(np.vstack(test_pred_seeds), axis=0).astype(np.float32)\n",
        "        np.save('test_deberta_ft_fc.npy', test_rank_final)\n",
        "        sub = pd.read_csv('sampleSubmission.csv')\n",
        "        sub['requester_received_pizza'] = np.clip(test_rank_final, 0.01, 0.99)\n",
        "        sub.to_csv('submission_deberta_ft_fc.csv', index=False)\n",
        "        print('Saved: oof_deberta_ft_fc.npy, test_deberta_ft_fc.npy, submission_deberta_ft_fc.csv')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "''')\n",
        "script.write_text(code)\n",
        "print('Wrote deberta_ft_fc.py')\n",
        "\n",
        "# Launch training via venv\n",
        "venv_py = Path('.venv/bin/python')\n",
        "assert venv_py.exists(), 'Missing .venv python; run the venv setup cell 1 first'\n",
        "env = os.environ.copy()\n",
        "env.setdefault('EPOCHS','3'); env.setdefault('LR','1.5e-5'); env.setdefault('WARMUP','0.10'); env.setdefault('MAX_LEN','384'); env.setdefault('SEEDS','42,1337')\n",
        "print('Launching DeBERTa FT with', venv_py, '| MAX_LEN=', env['MAX_LEN'], '| SEEDS=', env['SEEDS'])\n",
        "proc = subprocess.run([str(venv_py), str(script)], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, env=env)\n",
        "print(proc.stdout)\n",
        "if proc.returncode != 0:\n",
        "    raise RuntimeError('DeBERTa FT failed')\n",
        "print('DeBERTa FT leg complete. Next: modify Cell 12 to include deberta_ft_fc and reblend.')"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote deberta_ft_fc.py\nLaunching DeBERTa FT with .venv/bin/python | MAX_LEN= 384 | SEEDS= 42,1337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n{'loss': 0.6298, 'grad_norm': 0.8112905025482178, 'learning_rate': 1.3163282134179156e-05, 'epoch': 0.9259259259259259}\n{'loss': 0.5782, 'grad_norm': 1.306274175643921, 'learning_rate': 5.808469303639544e-06, 'epoch': 1.8518518518518519}\n{'eval_loss': 0.5310297012329102, 'eval_auc': 0.6309230660991312, 'eval_runtime': 2.5753, 'eval_samples_per_second': 219.389, 'eval_steps_per_second': 3.495, 'epoch': 1.8518518518518519}\n{'loss': 0.5645, 'grad_norm': 2.180481433868408, 'learning_rate': 2.520633540647824e-07, 'epoch': 2.7777777777777777}\n{'train_runtime': 111.8724, 'train_samples_per_second': 46.312, 'train_steps_per_second': 1.448, 'train_loss': 0.5888488086653344, 'epoch': 3.0}\n\n  0%|          | 0/71 [00:00<?, ?it/s]\n  8%|\u258a         | 6/71 [00:00<00:01, 54.78it/s]\n 17%|\u2588\u258b        | 12/71 [00:00<00:01, 45.64it/s]\n 24%|\u2588\u2588\u258d       | 17/71 [00:00<00:01, 41.97it/s]\n 31%|\u2588\u2588\u2588       | 22/71 [00:00<00:01, 41.35it/s]\n 39%|\u2588\u2588\u2588\u2589      | 28/71 [00:00<00:00, 46.27it/s]\n 46%|\u2588\u2588\u2588\u2588\u258b     | 33/71 [00:00<00:00, 44.44it/s]\n 54%|\u2588\u2588\u2588\u2588\u2588\u258e    | 38/71 [00:00<00:00, 41.59it/s]\n 61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 43/71 [00:01<00:00, 40.10it/s]\n 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 48/71 [00:01<00:00, 41.80it/s]\n 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 53/71 [00:01<00:00, 38.42it/s]\n 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 57/71 [00:01<00:00, 37.98it/s]\n 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 61/71 [00:01<00:00, 35.53it/s]\n 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 65/71 [00:01<00:00, 35.14it/s]\n 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 69/71 [00:01<00:00, 32.88it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71/71 [00:01<00:00, 39.33it/s]\nChain 1 seed 42: val AUC=0.631011\n\n  0%|          | 0/146 [00:00<?, ?it/s]\n  5%|\u258c         | 8/146 [00:00<00:01, 77.05it/s]\n 11%|\u2588         | 16/146 [00:00<00:01, 71.51it/s]\n 16%|\u2588\u258b        | 24/146 [00:00<00:01, 70.04it/s]\n 22%|\u2588\u2588\u258f       | 32/146 [00:00<00:01, 69.32it/s]\n 27%|\u2588\u2588\u258b       | 39/146 [00:00<00:01, 68.60it/s]\n 32%|\u2588\u2588\u2588\u258f      | 46/146 [00:00<00:01, 68.41it/s]\n 36%|\u2588\u2588\u2588\u258b      | 53/146 [00:00<00:01, 68.20it/s]\n 41%|\u2588\u2588\u2588\u2588      | 60/146 [00:00<00:01, 68.02it/s]\n 46%|\u2588\u2588\u2588\u2588\u258c     | 67/146 [00:00<00:01, 67.70it/s]\n 51%|\u2588\u2588\u2588\u2588\u2588     | 74/146 [00:01<00:01, 67.62it/s]\n 55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 81/146 [00:01<00:00, 67.44it/s]\n 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 88/146 [00:01<00:00, 67.42it/s]\n 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 95/146 [00:01<00:00, 67.50it/s]\n 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 102/146 [00:01<00:00, 67.57it/s]\n 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 109/146 [00:01<00:00, 67.48it/s]\n 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 116/146 [00:01<00:00, 67.56it/s]\n 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 123/146 [00:01<00:00, 67.29it/s]\n 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 130/146 [00:01<00:00, 67.28it/s]\n 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 137/146 [00:02<00:00, 67.52it/s]\n 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 144/146 [00:02<00:00, 67.64it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 146/146 [00:02<00:00, 68.07it/s]\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n{'loss': 0.6061, 'grad_norm': 0.9765859842300415, 'learning_rate': 1.4059647803545468e-05, 'epoch': 0.7407407407407407}\n{'loss': 0.5476, 'grad_norm': 3.459491014480591, 'learning_rate': 8.931067465324087e-06, 'epoch': 1.4814814814814814}\n{'eval_loss': 0.5296604633331299, 'eval_auc': 0.657811938151744, 'eval_runtime': 2.2598, 'eval_samples_per_second': 188.951, 'eval_steps_per_second': 3.098, 'epoch': 1.4814814814814814}\n{'loss': 0.5503, 'grad_norm': 1.0355061292648315, 'learning_rate': 2.7800970671262205e-06, 'epoch': 2.2222222222222223}\n{'loss': 0.5432, 'grad_norm': 1.0031300783157349, 'learning_rate': 1.1422863270654781e-09, 'epoch': 2.962962962962963}\n{'eval_loss': 0.5234611630439758, 'eval_auc': 0.6631008030684407, 'eval_runtime': 2.2657, 'eval_samples_per_second': 188.46, 'eval_steps_per_second': 3.09, 'epoch': 2.962962962962963}\n{'train_runtime': 138.1696, 'train_samples_per_second': 46.855, 'train_steps_per_second': 1.455, 'train_loss': 0.5616611746413198, 'epoch': 2.977777777777778}\n\n  0%|          | 0/54 [00:00<?, ?it/s]\n 11%|\u2588         | 6/54 [00:00<00:01, 42.68it/s]\n 20%|\u2588\u2588        | 11/54 [00:00<00:01, 37.51it/s]\n 28%|\u2588\u2588\u258a       | 15/54 [00:00<00:01, 33.68it/s]\n 35%|\u2588\u2588\u2588\u258c      | 19/54 [00:00<00:01, 33.74it/s]\n 44%|\u2588\u2588\u2588\u2588\u258d     | 24/54 [00:00<00:00, 37.30it/s]\n 54%|\u2588\u2588\u2588\u2588\u2588\u258e    | 29/54 [00:00<00:00, 37.52it/s]\n 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 34/54 [00:00<00:00, 40.29it/s]\n 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 39/54 [00:01<00:00, 38.25it/s]\n 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 44/54 [00:01<00:00, 38.76it/s]\n 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 48/54 [00:01<00:00, 35.74it/s]\n 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 52/54 [00:01<00:00, 34.18it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 54/54 [00:01<00:00, 36.19it/s]\nChain 2 seed 42: val AUC=0.663191\n\n  0%|          | 0/146 [00:00<?, ?it/s]\n  5%|\u258c         | 8/146 [00:00<00:01, 76.70it/s]\n 11%|\u2588         | 16/146 [00:00<00:01, 71.22it/s]\n 16%|\u2588\u258b        | 24/146 [00:00<00:01, 69.80it/s]\n 21%|\u2588\u2588        | 31/146 [00:00<00:01, 69.05it/s]\n 26%|\u2588\u2588\u258c       | 38/146 [00:00<00:01, 68.57it/s]\n 31%|\u2588\u2588\u2588       | 45/146 [00:00<00:01, 68.44it/s]\n 36%|\u2588\u2588\u2588\u258c      | 52/146 [00:00<00:01, 68.40it/s]\n 40%|\u2588\u2588\u2588\u2588      | 59/146 [00:00<00:01, 68.27it/s]\n 45%|\u2588\u2588\u2588\u2588\u258c     | 66/146 [00:00<00:01, 68.13it/s]\n 50%|\u2588\u2588\u2588\u2588\u2588     | 73/146 [00:01<00:01, 67.96it/s]\n 55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 80/146 [00:01<00:00, 67.81it/s]\n 60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 87/146 [00:01<00:00, 67.84it/s]\n 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 94/146 [00:01<00:00, 67.87it/s]\n 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 101/146 [00:01<00:00, 67.93it/s]\n 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 108/146 [00:01<00:00, 67.91it/s]\n 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 115/146 [00:01<00:00, 67.86it/s]\n 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 122/146 [00:01<00:00, 67.47it/s]\n 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 129/146 [00:01<00:00, 67.42it/s]\n 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 136/146 [00:01<00:00, 67.42it/s]\n 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 143/146 [00:02<00:00, 67.45it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 146/146 [00:02<00:00, 68.18it/s]\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n{'loss': 0.5986, 'grad_norm': 1.5100879669189453, 'learning_rate': 1.4242138058140465e-05, 'epoch': 0.6944444444444444}\n{'loss': 0.5681, 'grad_norm': 1.5867063999176025, 'learning_rate': 9.886822354705831e-06, 'epoch': 1.3888888888888888}\n{'eval_loss': 0.4772893190383911, 'eval_auc': 0.61348703978206, 'eval_runtime': 2.6606, 'eval_samples_per_second': 213.106, 'eval_steps_per_second': 3.383, 'epoch': 1.3888888888888888}\n{'loss': 0.5343, 'grad_norm': 1.5814400911331177, 'learning_rate': 3.998042185370695e-06, 'epoch': 2.0833333333333335}\n{'loss': 0.5416, 'grad_norm': 2.080927848815918, 'learning_rate': 2.824105017447737e-07, 'epoch': 2.7777777777777777}\n{'eval_loss': 0.4820746183395386, 'eval_auc': 0.6281398982412565, 'eval_runtime': 2.6658, 'eval_samples_per_second': 212.69, 'eval_steps_per_second': 3.376, 'epoch': 2.7777777777777777}\n{'train_runtime': 146.3701, 'train_samples_per_second': 47.182, 'train_steps_per_second': 1.476, 'train_loss': 0.5589235800283926, 'epoch': 3.0}\n\n  0%|          | 0/71 [00:00<?, ?it/s]\n  8%|\u258a         | 6/71 [00:00<00:01, 51.19it/s]\n 17%|\u2588\u258b        | 12/71 [00:00<00:01, 46.17it/s]\n 24%|\u2588\u2588\u258d       | 17/71 [00:00<00:01, 47.13it/s]\n 31%|\u2588\u2588\u2588       | 22/71 [00:00<00:01, 38.93it/s]\n 38%|\u2588\u2588\u2588\u258a      | 27/71 [00:00<00:01, 36.61it/s]\n 44%|\u2588\u2588\u2588\u2588\u258e     | 31/71 [00:00<00:01, 36.62it/s]\n 49%|\u2588\u2588\u2588\u2588\u2589     | 35/71 [00:00<00:01, 29.54it/s]\n 55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 39/71 [00:01<00:01, 31.46it/s]\n 61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 43/71 [00:01<00:00, 32.61it/s]\n 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 47/71 [00:01<00:00, 31.73it/s]\n 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 52/71 [00:01<00:00, 34.64it/s]\n 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 56/71 [00:01<00:00, 34.64it/s]\n 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 62/71 [00:01<00:00, 38.70it/s]\n 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 66/71 [00:01<00:00, 38.39it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71/71 [00:01<00:00, 37.54it/s]\nChain 3 seed 42: val AUC=0.628090\n\n  0%|          | 0/146 [00:00<?, ?it/s]\n  5%|\u258c         | 8/146 [00:00<00:01, 75.81it/s]\n 11%|\u2588         | 16/146 [00:00<00:01, 70.41it/s]\n 16%|\u2588\u258b        | 24/146 [00:00<00:01, 68.95it/s]\n 21%|\u2588\u2588        | 31/146 [00:00<00:01, 68.27it/s]\n 26%|\u2588\u2588\u258c       | 38/146 [00:00<00:01, 67.87it/s]\n 31%|\u2588\u2588\u2588       | 45/146 [00:00<00:01, 67.62it/s]\n 36%|\u2588\u2588\u2588\u258c      | 52/146 [00:00<00:01, 67.57it/s]\n 40%|\u2588\u2588\u2588\u2588      | 59/146 [00:00<00:01, 67.47it/s]\n 45%|\u2588\u2588\u2588\u2588\u258c     | 66/146 [00:00<00:01, 67.37it/s]\n 50%|\u2588\u2588\u2588\u2588\u2588     | 73/146 [00:01<00:01, 67.16it/s]\n 55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 80/146 [00:01<00:00, 67.02it/s]\n 60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 87/146 [00:01<00:00, 67.06it/s]\n 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 94/146 [00:01<00:00, 67.10it/s]\n 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 101/146 [00:01<00:00, 67.16it/s]\n 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 108/146 [00:01<00:00, 67.13it/s]\n 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 115/146 [00:01<00:00, 66.95it/s]\n 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 122/146 [00:01<00:00, 66.58it/s]\n 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 129/146 [00:01<00:00, 66.54it/s]\n 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 136/146 [00:02<00:00, 66.63it/s]\n 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 143/146 [00:02<00:00, 66.67it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 146/146 [00:02<00:00, 67.37it/s]\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n{'loss': 0.6016, 'grad_norm': 0.6803420782089233, 'learning_rate': 1.3163282134179156e-05, 'epoch': 0.9259259259259259}\n{'loss': 0.5799, 'grad_norm': 2.5123109817504883, 'learning_rate': 5.808469303639544e-06, 'epoch': 1.8518518518518519}\n{'eval_loss': 0.5243703126907349, 'eval_auc': 0.6338780033067154, 'eval_runtime': 2.6322, 'eval_samples_per_second': 214.651, 'eval_steps_per_second': 3.419, 'epoch': 1.8518518518518519}\n{'loss': 0.5528, 'grad_norm': 1.4628024101257324, 'learning_rate': 2.520633540647824e-07, 'epoch': 2.7777777777777777}\n{'train_runtime': 114.0995, 'train_samples_per_second': 45.408, 'train_steps_per_second': 1.42, 'train_loss': 0.5767955367947802, 'epoch': 3.0}\n\n  0%|          | 0/71 [00:00<?, ?it/s]\n  8%|\u258a         | 6/71 [00:00<00:01, 53.29it/s]\n 17%|\u2588\u258b        | 12/71 [00:00<00:01, 44.67it/s]\n 24%|\u2588\u2588\u258d       | 17/71 [00:00<00:01, 41.23it/s]\n 31%|\u2588\u2588\u2588       | 22/71 [00:00<00:01, 40.63it/s]\n 39%|\u2588\u2588\u2588\u2589      | 28/71 [00:00<00:00, 45.40it/s]\n 46%|\u2588\u2588\u2588\u2588\u258b     | 33/71 [00:00<00:00, 43.68it/s]\n 54%|\u2588\u2588\u2588\u2588\u2588\u258e    | 38/71 [00:00<00:00, 40.83it/s]\n 61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 43/71 [00:01<00:00, 39.39it/s]\n 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 48/71 [00:01<00:00, 41.12it/s]\n 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 53/71 [00:01<00:00, 37.93it/s]\n 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 57/71 [00:01<00:00, 37.45it/s]\n 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 61/71 [00:01<00:00, 34.98it/s]\n 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 65/71 [00:01<00:00, 34.53it/s]\n 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 69/71 [00:01<00:00, 32.28it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71/71 [00:01<00:00, 38.65it/s]\nChain 1 seed 1337: val AUC=0.633878\n\n  0%|          | 0/146 [00:00<?, ?it/s]\n  5%|\u258c         | 8/146 [00:00<00:01, 75.62it/s]\n 11%|\u2588         | 16/146 [00:00<00:01, 70.05it/s]\n 16%|\u2588\u258b        | 24/146 [00:00<00:01, 68.58it/s]\n 21%|\u2588\u2588        | 31/146 [00:00<00:01, 67.86it/s]\n 26%|\u2588\u2588\u258c       | 38/146 [00:00<00:01, 67.50it/s]\n 31%|\u2588\u2588\u2588       | 45/146 [00:00<00:01, 67.34it/s]\n 36%|\u2588\u2588\u2588\u258c      | 52/146 [00:00<00:01, 67.24it/s]\n 40%|\u2588\u2588\u2588\u2588      | 59/146 [00:00<00:01, 67.08it/s]\n 45%|\u2588\u2588\u2588\u2588\u258c     | 66/146 [00:00<00:01, 66.87it/s]\n 50%|\u2588\u2588\u2588\u2588\u2588     | 73/146 [00:01<00:01, 66.82it/s]\n 55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 80/146 [00:01<00:00, 66.69it/s]\n 60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 87/146 [00:01<00:00, 66.74it/s]\n 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 94/146 [00:01<00:00, 66.76it/s]\n 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 101/146 [00:01<00:00, 66.80it/s]\n 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 108/146 [00:01<00:00, 66.80it/s]\n 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 115/146 [00:01<00:00, 66.78it/s]\n 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 122/146 [00:01<00:00, 66.49it/s]\n 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 129/146 [00:01<00:00, 66.49it/s]\n 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 136/146 [00:02<00:00, 66.55it/s]\n 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 143/146 [00:02<00:00, 66.63it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 146/146 [00:02<00:00, 67.13it/s]\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n{'loss': 0.5939, 'grad_norm': 0.6648640036582947, 'learning_rate': 1.4059647803545468e-05, 'epoch': 0.7407407407407407}\n{'loss': 0.5706, 'grad_norm': 0.8470618724822998, 'learning_rate': 8.931067465324087e-06, 'epoch': 1.4814814814814814}\n{'eval_loss': 0.5341517329216003, 'eval_auc': 0.654590674817212, 'eval_runtime': 2.2754, 'eval_samples_per_second': 187.659, 'eval_steps_per_second': 3.076, 'epoch': 1.4814814814814814}\n{'loss': 0.5647, 'grad_norm': 1.8869961500167847, 'learning_rate': 2.7800970671262205e-06, 'epoch': 2.2222222222222223}\n{'loss': 0.5373, 'grad_norm': 3.078315019607544, 'learning_rate': 1.1422863270654781e-09, 'epoch': 2.962962962962963}\n{'eval_loss': 0.5265312790870667, 'eval_auc': 0.6717457749011148, 'eval_runtime': 2.2813, 'eval_samples_per_second': 187.173, 'eval_steps_per_second': 3.068, 'epoch': 2.962962962962963}\n{'train_runtime': 139.1801, 'train_samples_per_second': 46.515, 'train_steps_per_second': 1.444, 'train_loss': 0.565843446041221, 'epoch': 2.977777777777778}\n\n  0%|          | 0/54 [00:00<?, ?it/s]\n 11%|\u2588         | 6/54 [00:00<00:01, 42.59it/s]\n 20%|\u2588\u2588        | 11/54 [00:00<00:01, 37.22it/s]\n 28%|\u2588\u2588\u258a       | 15/54 [00:00<00:01, 33.31it/s]\n 35%|\u2588\u2588\u2588\u258c      | 19/54 [00:00<00:01, 33.36it/s]\n 44%|\u2588\u2588\u2588\u2588\u258d     | 24/54 [00:00<00:00, 36.89it/s]\n 54%|\u2588\u2588\u2588\u2588\u2588\u258e    | 29/54 [00:00<00:00, 37.09it/s]\n 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 34/54 [00:00<00:00, 39.64it/s]\n 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 39/54 [00:01<00:00, 37.84it/s]\n 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 44/54 [00:01<00:00, 38.40it/s]\n 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 48/54 [00:01<00:00, 35.46it/s]\n 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 52/54 [00:01<00:00, 33.91it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 54/54 [00:01<00:00, 35.84it/s]\nChain 2 seed 1337: val AUC=0.671761\n\n  0%|          | 0/146 [00:00<?, ?it/s]\n  5%|\u258c         | 8/146 [00:00<00:01, 75.82it/s]\n 11%|\u2588         | 16/146 [00:00<00:01, 70.21it/s]\n 16%|\u2588\u258b        | 24/146 [00:00<00:01, 68.70it/s]\n 21%|\u2588\u2588        | 31/146 [00:00<00:01, 68.00it/s]\n 26%|\u2588\u2588\u258c       | 38/146 [00:00<00:01, 67.63it/s]\n 31%|\u2588\u2588\u2588       | 45/146 [00:00<00:01, 67.50it/s]\n 36%|\u2588\u2588\u2588\u258c      | 52/146 [00:00<00:01, 67.41it/s]\n 40%|\u2588\u2588\u2588\u2588      | 59/146 [00:00<00:01, 67.32it/s]\n 45%|\u2588\u2588\u2588\u2588\u258c     | 66/146 [00:00<00:01, 67.18it/s]\n 50%|\u2588\u2588\u2588\u2588\u2588     | 73/146 [00:01<00:01, 67.00it/s]\n 55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 80/146 [00:01<00:00, 66.84it/s]\n 60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 87/146 [00:01<00:00, 66.86it/s]\n 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 94/146 [00:01<00:00, 66.92it/s]\n 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 101/146 [00:01<00:00, 66.99it/s]\n 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 108/146 [00:01<00:00, 66.99it/s]\n 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 115/146 [00:01<00:00, 66.95it/s]\n 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 122/146 [00:01<00:00, 66.67it/s]\n 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 129/146 [00:01<00:00, 66.69it/s]\n 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 136/146 [00:02<00:00, 66.77it/s]\n 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 143/146 [00:02<00:00, 66.54it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 146/146 [00:02<00:00, 67.27it/s]\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/var/lib/simon/agent_run_states/random-acts-of-pizza-20250929-001459/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n{'loss': 0.5921, 'grad_norm': 1.0197969675064087, 'learning_rate': 1.4242138058140465e-05, 'epoch': 0.6944444444444444}\n{'loss': 0.5712, 'grad_norm': 1.2150202989578247, 'learning_rate': 9.77137556923463e-06, 'epoch': 1.3888888888888888}\n{'eval_loss': 0.4763762652873993, 'eval_auc': 0.6325267417170787, 'eval_runtime': 2.671, 'eval_samples_per_second': 212.279, 'eval_steps_per_second': 3.37, 'epoch': 1.3888888888888888}\n{'loss': 0.5539, 'grad_norm': 1.129792332649231, 'learning_rate': 3.891105416318962e-06, 'epoch': 2.0833333333333335}\n{'loss': 0.5292, 'grad_norm': 3.185598611831665, 'learning_rate': 2.503436226554165e-07, 'epoch': 2.7777777777777777}\n{'eval_loss': 0.4743827283382416, 'eval_auc': 0.6480509594968151, 'eval_runtime': 2.6717, 'eval_samples_per_second': 212.225, 'eval_steps_per_second': 3.369, 'epoch': 2.7777777777777777}\n{'train_runtime': 148.4348, 'train_samples_per_second': 46.525, 'train_steps_per_second': 1.455, 'train_loss': 0.5600734640050817, 'epoch': 3.0}\n\n  0%|          | 0/71 [00:00<?, ?it/s]\n  8%|\u258a         | 6/71 [00:00<00:01, 50.68it/s]\n 17%|\u2588\u258b        | 12/71 [00:00<00:01, 45.68it/s]\n 24%|\u2588\u2588\u258d       | 17/71 [00:00<00:01, 46.73it/s]\n 31%|\u2588\u2588\u2588       | 22/71 [00:00<00:01, 39.00it/s]\n 38%|\u2588\u2588\u2588\u258a      | 27/71 [00:00<00:01, 36.56it/s]\n 44%|\u2588\u2588\u2588\u2588\u258e     | 31/71 [00:00<00:01, 36.60it/s]\n 49%|\u2588\u2588\u2588\u2588\u2589     | 35/71 [00:00<00:01, 29.49it/s]\n 55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 39/71 [00:01<00:01, 31.37it/s]\n 61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 43/71 [00:01<00:00, 32.56it/s]\n 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 47/71 [00:01<00:00, 31.55it/s]\n 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 52/71 [00:01<00:00, 34.47it/s]\n 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 56/71 [00:01<00:00, 34.46it/s]\n 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 61/71 [00:01<00:00, 38.44it/s]\n 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 65/71 [00:01<00:00, 38.54it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71/71 [00:01<00:00, 42.93it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71/71 [00:01<00:00, 37.42it/s]\nChain 3 seed 1337: val AUC=0.648071\n\n  0%|          | 0/146 [00:00<?, ?it/s]\n  5%|\u258c         | 8/146 [00:00<00:01, 76.30it/s]\n 11%|\u2588         | 16/146 [00:00<00:01, 70.71it/s]\n 16%|\u2588\u258b        | 24/146 [00:00<00:01, 69.31it/s]\n 21%|\u2588\u2588        | 31/146 [00:00<00:01, 68.71it/s]\n 26%|\u2588\u2588\u258c       | 38/146 [00:00<00:01, 68.11it/s]\n 31%|\u2588\u2588\u2588       | 45/146 [00:00<00:01, 67.62it/s]\n 36%|\u2588\u2588\u2588\u258c      | 52/146 [00:00<00:01, 67.41it/s]\n 40%|\u2588\u2588\u2588\u2588      | 59/146 [00:00<00:01, 67.23it/s]\n 45%|\u2588\u2588\u2588\u2588\u258c     | 66/146 [00:00<00:01, 67.04it/s]\n 50%|\u2588\u2588\u2588\u2588\u2588     | 73/146 [00:01<00:01, 66.70it/s]\n 55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 80/146 [00:01<00:00, 66.45it/s]\n 60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 87/146 [00:01<00:00, 66.48it/s]\n 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 94/146 [00:01<00:00, 66.54it/s]\n 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 101/146 [00:01<00:00, 66.61it/s]\n 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 108/146 [00:01<00:00, 66.61it/s]\n 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 115/146 [00:01<00:00, 66.63it/s]\n 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 122/146 [00:01<00:00, 66.41it/s]\n 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 129/146 [00:01<00:00, 66.48it/s]\n 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 136/146 [00:02<00:00, 66.58it/s]\n 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 143/146 [00:02<00:00, 66.70it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 146/146 [00:02<00:00, 67.21it/s]\nDeBERTa FT OOF AUC (val rows): 0.628745\nSaved: oof_deberta_ft_fc.npy, test_deberta_ft_fc.npy, submission_deberta_ft_fc.csv\n\nDeBERTa FT leg complete. Next: modify Cell 12 to include deberta_ft_fc and reblend.\n"
          ]
        }
      ]
    },
    {
      "id": "5df61739-2185-4641-a245-cd1b2eea72ca",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Leg: NB-SVM (word+char TF-IDF, title x3 + body), forward-chain, per-chain vectorizers, C in {0.5,1,2,4}\n",
        "import json, re, time, gc\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def load_json_df(path):\n",
        "    try:\n",
        "        return pd.read_json(path, lines=True)\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return pd.read_json(path, lines=False)\n",
        "        except ValueError:\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            if isinstance(data, dict) and 'data' in data:\n",
        "                data = data['data']\n",
        "            return pd.json_normalize(data)\n",
        "\n",
        "def build_text(df):\n",
        "    title_col_candidates = ['request_title', 'title']\n",
        "    body_col_candidates = ['request_text', 'body', 'text']\n",
        "    def first_col(cols):\n",
        "        for c in cols:\n",
        "            if c in df.columns: return c\n",
        "        return None\n",
        "    tcol = first_col(title_col_candidates); bcol = first_col(body_col_candidates)\n",
        "    t = df[tcol].fillna(\"\") if tcol else pd.Series([\"\" for _ in range(len(df))])\n",
        "    b = df[bcol].fillna(\"\") if bcol else pd.Series([\"\" for _ in range(len(df))])\n",
        "    # lowercase; normalize URLs and digits\n",
        "    t = t.astype(str).str.lower(); b = b.astype(str).str.lower()\n",
        "    url_pat = r'https?://\\S+|www\\.[^\\s]+'\n",
        "    t = t.str.replace(url_pat, ' URL ', regex=True).str.replace(r'\\d', '0', regex=True)\n",
        "    b = b.str.replace(url_pat, ' URL ', regex=True).str.replace(r'\\d', '0', regex=True)\n",
        "    # Up-weight title by repeating x3\n",
        "    return (t + ' ' + t + ' ' + t + ' ' + b)\n",
        "\n",
        "def nb_log_ratio_from_matrix(Xtr, ytr, alpha=1.0):\n",
        "    # Compute log P(w|pos) - log P(w|neg) using per-class summed TF-IDF weights with additive smoothing.\n",
        "    ytr = np.asarray(ytr).astype(np.int8)\n",
        "    pos_idx = np.where(ytr == 1)[0]\n",
        "    neg_idx = np.where(ytr == 0)[0]\n",
        "    Xc = Xtr.tocsc()\n",
        "    pos_counts = np.array(Xc[pos_idx, :].sum(axis=0)).ravel().astype(np.float64)\n",
        "    neg_counts = np.array(Xc[neg_idx, :].sum(axis=0)).ravel().astype(np.float64)\n",
        "    pos_counts += alpha; neg_counts += alpha\n",
        "    pos_probs = pos_counts / (pos_counts.sum() + 1e-12)\n",
        "    neg_probs = neg_counts / (neg_counts.sum() + 1e-12)\n",
        "    r = np.log(pos_probs + 1e-12) - np.log(neg_probs + 1e-12)\n",
        "    return r.astype(np.float32)\n",
        "\n",
        "from contextlib import contextmanager\n",
        "@contextmanager\n",
        "def timer(msg):\n",
        "    t0 = time.time(); print(f\"[T0] {msg} ...\", flush=True)\n",
        "    try: yield\n",
        "    finally: print(f\"[T+] {msg} done in {time.time()-t0:.2f}s\", flush=True)\n",
        "\n",
        "with timer('Load data and align by time'):\n",
        "    tr = load_json_df('train.json'); te = load_json_df('test.json')\n",
        "    mf = json.loads(Path('folds/manifest.json').read_text())\n",
        "    time_col = mf.get('time_col','unix_timestamp_of_request_utc')\n",
        "    label_col = mf.get('label_col','requester_received_pizza')\n",
        "    tr = tr.sort_values(time_col, kind='mergesort').reset_index(drop=True)\n",
        "    y = pd.to_numeric(tr[label_col], errors='coerce').fillna(0).astype(int).clip(0,1).values\n",
        "    X_text_tr = build_text(tr)\n",
        "    X_text_te = build_text(te)\n",
        "\n",
        "fold_dir = Path('folds')\n",
        "mf = json.loads((fold_dir / 'manifest.json').read_text())\n",
        "chains = [c['chain'] for c in mf['chains']]\n",
        "print('Chains:', chains)\n",
        "\n",
        "oof = np.zeros(len(tr), dtype=np.float32)\n",
        "val_mask = np.zeros(len(tr), dtype=bool)\n",
        "test_preds_accum = []\n",
        "\n",
        "params = dict(\n",
        "    word_max_features=220000,\n",
        "    char_max_features=280000,\n",
        "    C_grid=[0.5, 1.0, 2.0, 4.0]\n",
        ")\n",
        "print('Params:', params)\n",
        "\n",
        "def fit_chain(ci, seed=42):\n",
        "    tr_idx = np.load(fold_dir / f\"fc_chain{ci}_train_idx.npy\"); va_idx = np.load(fold_dir / f\"fc_chain{ci}_val_idx.npy\")\n",
        "    if len(va_idx) == 0:\n",
        "        print(f'Chain {ci}: empty val; skip'); return None\n",
        "    print(f'Chain {ci}: train {len(tr_idx)} | val {len(va_idx)}', flush=True)\n",
        "    # TF-IDF vectorizers (per-chain, fit on train only); strip_accents unicode\n",
        "    word_vec = TfidfVectorizer(analyzer='word', ngram_range=(1,2), min_df=2, max_df=0.98,\n",
        "                               max_features=params['word_max_features'], lowercase=False,\n",
        "                               strip_accents='unicode', dtype=np.float32)\n",
        "    char_vec = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,6), min_df=2,\n",
        "                               max_features=params['char_max_features'], lowercase=False,\n",
        "                               strip_accents='unicode', dtype=np.float32)\n",
        "    with timer(f'Chain {ci}: vectorize'):\n",
        "        Xtr_w = word_vec.fit_transform(X_text_tr.iloc[tr_idx])\n",
        "        Xva_w = word_vec.transform(X_text_tr.iloc[va_idx])\n",
        "        Xte_w = word_vec.transform(X_text_te)\n",
        "        Xtr_c = char_vec.fit_transform(X_text_tr.iloc[tr_idx])\n",
        "        Xva_c = char_vec.transform(X_text_tr.iloc[va_idx])\n",
        "        Xte_c = char_vec.transform(X_text_te)\n",
        "        Xtr = sparse.hstack([Xtr_w, Xtr_c]).tocsr()\n",
        "        Xva = sparse.hstack([Xva_w, Xva_c]).tocsr()\n",
        "        Xte = sparse.hstack([Xte_w, Xte_c]).tocsr()\n",
        "    ytr, yva = y[tr_idx], y[va_idx]\n",
        "    # NB-SVM transform via manual log ratio on TF-IDF matrix\n",
        "    with timer(f'Chain {ci}: NB log-ratio transform'):\n",
        "        r = nb_log_ratio_from_matrix(Xtr, ytr, alpha=1.0)\n",
        "        Xtr_nb = Xtr.multiply(r)\n",
        "        Xva_nb = Xva.multiply(r)\n",
        "        Xte_nb = Xte.multiply(r)\n",
        "    best_auc, best_pva, best_pte, bestC = -1.0, None, None, None\n",
        "    for C in params['C_grid']:\n",
        "        clf = LogisticRegression(solver='saga', penalty='l2', C=C,\n",
        "                                 class_weight='balanced',\n",
        "                                 random_state=seed, max_iter=3000, n_jobs=-1, verbose=0)\n",
        "        t0 = time.time(); clf.fit(Xtr_nb, ytr); pva = clf.predict_proba(Xva_nb)[:,1]\n",
        "        auc = roc_auc_score(yva, pva)\n",
        "        print(f'  C={C} | AUC={auc:.6f} | secs={time.time()-t0:.2f}', flush=True)\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_pva, best_pte, bestC = auc, pva, clf.predict_proba(Xte_nb)[:,1], C\n",
        "    print(f'Chain {ci}: best C={bestC} | AUC={best_auc:.6f}', flush=True)\n",
        "    del Xtr_w, Xva_w, Xte_w, Xtr_c, Xva_c, Xte_c, Xtr, Xva, Xte, Xtr_nb, Xva_nb, Xte_nb; gc.collect()\n",
        "    return va_idx, best_pva.astype(np.float32), best_pte.astype(np.float32)\n",
        "\n",
        "with timer('Train NB-SVM across forward chains'):\n",
        "    for ci in chains:\n",
        "        res = fit_chain(ci, seed=42)\n",
        "        if res is None: continue\n",
        "        va_idx, pva, pte = res\n",
        "        oof[va_idx] = pva\n",
        "        val_mask[va_idx] = True\n",
        "        test_preds_accum.append(pte)\n",
        "\n",
        "with timer('Evaluate and save NB-SVM artifacts'):\n",
        "    if val_mask.any():\n",
        "        oof_auc = roc_auc_score(y[val_mask], oof[val_mask])\n",
        "        print('NB-SVM OOF AUC (val rows only):', round(oof_auc, 6))\n",
        "    else:\n",
        "        print('Warning: no validation rows; OOF not computed.')\n",
        "    np.save('oof_nbsvm_fc.npy', oof.astype(np.float32))\n",
        "    if len(test_preds_accum):\n",
        "        test_pred = np.mean(np.vstack(test_preds_accum), axis=0).astype(np.float32)\n",
        "        np.save('test_nbsvm_fc.npy', test_pred)\n",
        "        sub = pd.read_csv('sampleSubmission.csv')\n",
        "        sub['requester_received_pizza'] = np.clip(test_pred, 0.01, 0.99)\n",
        "        sub.to_csv('submission_nbsvm_fc.csv', index=False)\n",
        "        print('Saved: oof_nbsvm_fc.npy, test_nbsvm_fc.npy, submission_nbsvm_fc.csv')\n",
        "    else:\n",
        "        print('Warning: no test preds collected.')"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Load data and align by time ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Load data and align by time done in 0.16s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chains: [1, 2, 3]\nParams: {'word_max_features': 220000, 'char_max_features': 280000, 'C_grid': [0.5, 1.0, 2.0, 4.0]}\n[T0] Train NB-SVM across forward chains ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 1: train 1727 | val 565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Chain 1: vectorize ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Chain 1: vectorize done in 1.42s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Chain 1: NB log-ratio transform ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Chain 1: NB log-ratio transform done in 0.04s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=0.5 | AUC=0.604830 | secs=0.46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=1.0 | AUC=0.599043 | secs=0.47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=2.0 | AUC=0.590301 | secs=0.46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=4.0 | AUC=0.579766 | secs=0.66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 1: best C=0.5 | AUC=0.604830\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 2: train 2158 | val 427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Chain 2: vectorize ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Chain 2: vectorize done in 1.57s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=0.5 | AUC=0.610632 | secs=0.55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=1.0 | AUC=0.600593 | secs=0.60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=2.0 | AUC=0.589716 | secs=0.65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=4.0 | AUC=0.580007 | secs=1.73\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 2: best C=0.5 | AUC=0.610632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3: train 2302 | val 567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Chain 3: vectorize ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Chain 3: vectorize done in 1.70s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Chain 3: NB log-ratio transform ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Chain 3: NB log-ratio transform done in 0.05s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=0.5 | AUC=0.591042 | secs=0.58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=1.0 | AUC=0.583530 | secs=0.61\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=2.0 | AUC=0.573955 | secs=0.76\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=4.0 | AUC=0.563759 | secs=1.81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3: best C=0.5 | AUC=0.591042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Train NB-SVM across forward chains done in 14.54s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Evaluate and save NB-SVM artifacts ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM OOF AUC (val rows only): 0.590052\nSaved: oof_nbsvm_fc.npy, test_nbsvm_fc.npy, submission_nbsvm_fc.csv\n[T+] Evaluate and save NB-SVM artifacts done in 0.00s\n"
          ]
        }
      ]
    },
    {
      "id": "48313ac2-df4d-4ef1-b42e-adfd983d8461",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# NB-SVM via venv script: counts->MNB r, apply r to TF-IDF (same vocab), per-chain fit\n",
        "import os, textwrap, subprocess, json\n",
        "from pathlib import Path\n",
        "\n",
        "script = Path('nbsvm_fc.py')\n",
        "code = textwrap.dedent('''\n",
        "import os, json, time, gc, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def load_json_df(path):\n",
        "    try: return pd.read_json(path, lines=True)\n",
        "    except ValueError:\n",
        "        try: return pd.read_json(path, lines=False)\n",
        "        except ValueError:\n",
        "            with open(path,'r',encoding='utf-8') as f: data=json.load(f)\n",
        "            if isinstance(data, dict) and 'data' in data: data=data['data']\n",
        "            return pd.json_normalize(data)\n",
        "\n",
        "def build_text(df):\n",
        "    def first_col(cols):\n",
        "        for c in cols:\n",
        "            if c in df.columns: return c\n",
        "        return None\n",
        "    tcol = first_col(['request_title','title'])\n",
        "    bcol = first_col(['request_text','body','text'])\n",
        "    t = df[tcol].fillna('') if tcol else pd.Series(['']*len(df))\n",
        "    b = df[bcol].fillna('') if bcol else pd.Series(['']*len(df))\n",
        "    t = t.astype(str).str.lower()\n",
        "    b = b.astype(str).str.lower()\n",
        "    url_pat = r'https?://\\S+|www\\.[^\\s]+'\n",
        "    t = t.str.replace(url_pat, ' URL ', regex=True).str.replace(r'\\d', '0', regex=True)\n",
        "    b = b.str.replace(url_pat, ' URL ', regex=True).str.replace(r'\\d', '0', regex=True)\n",
        "    return (t + ' ' + t + ' ' + t + ' ' + b)\n",
        "\n",
        "def timer(msg):\n",
        "    t0 = time.time(); print(f\"[T0] {msg} ...\", flush=True);\n",
        "    return t0\n",
        "def done(t0, msg):\n",
        "    print(f\"[T+] {msg} done in {time.time()-t0:.2f}s\", flush=True)\n",
        "\n",
        "def main():\n",
        "    fold_dir = Path('folds')\n",
        "    mf = json.loads((fold_dir/'manifest.json').read_text())\n",
        "    chains = [c['chain'] for c in mf['chains']]\n",
        "    tr = load_json_df('train.json'); te = load_json_df('test.json')\n",
        "    time_col = mf.get('time_col','unix_timestamp_of_request_utc')\n",
        "    label_col = mf.get('label_col','requester_received_pizza')\n",
        "    tr = tr.sort_values(time_col, kind='mergesort').reset_index(drop=True)\n",
        "    y = pd.to_numeric(tr[label_col], errors='coerce').fillna(0).astype(int).clip(0,1).values\n",
        "    X_text_tr = build_text(tr); X_text_te = build_text(te)\n",
        "    print('Chains:', chains)\n",
        "\n",
        "    oof = np.zeros(len(tr), dtype=np.float32); val_mask = np.zeros(len(tr), dtype=bool)\n",
        "    test_preds = []\n",
        "    params = dict(word_max_features=240000, char_max_features=300000, C_grid=[0.25,0.5,1.0,2.0,4.0])\n",
        "    print('Params:', params)\n",
        "\n",
        "    for ci in chains:\n",
        "        tr_idx = np.load(fold_dir / f\"fc_chain{ci}_train_idx.npy\"); va_idx = np.load(fold_dir / f\"fc_chain{ci}_val_idx.npy\")\n",
        "        if len(va_idx) == 0:\n",
        "            print(f'Chain {ci}: empty val; skip'); continue\n",
        "        print(f'Chain {ci}: train {len(tr_idx)} | val {len(va_idx)}', flush=True)\n",
        "        # 1) Count vectorizers (fit on train only) with binary=True for stability\n",
        "        word_cv = CountVectorizer(analyzer='word', ngram_range=(1,2), min_df=2, max_df=0.98,\n",
        "                                  max_features=params['word_max_features'], lowercase=False, strip_accents='unicode', binary=True)\n",
        "        char_cv = CountVectorizer(analyzer='char_wb', ngram_range=(3,6), min_df=2,\n",
        "                                  max_features=params['char_max_features'], lowercase=False, strip_accents='unicode', binary=True)\n",
        "        t0 = timer(f'Chain {ci}: fit counts')\n",
        "        Xtr_w_cnt = word_cv.fit_transform(X_text_tr.iloc[tr_idx])\n",
        "        Xva_w_cnt = word_cv.transform(X_text_tr.iloc[va_idx])\n",
        "        Xte_w_cnt = word_cv.transform(X_text_te)\n",
        "        Xtr_c_cnt = char_cv.fit_transform(X_text_tr.iloc[tr_idx])\n",
        "        Xva_c_cnt = char_cv.transform(X_text_tr.iloc[va_idx])\n",
        "        Xte_c_cnt = char_cv.transform(X_text_te)\n",
        "        Xtr_cnt = sparse.hstack([Xtr_w_cnt, Xtr_c_cnt]).tocsr()\n",
        "        Xva_cnt = sparse.hstack([Xva_w_cnt, Xva_c_cnt]).tocsr()\n",
        "        Xte_cnt = sparse.hstack([Xte_w_cnt, Xte_c_cnt]).tocsr()\n",
        "        done(t0, f'Chain {ci}: fit counts')\n",
        "        # 2) TF-IDF with same vocabularies (align columns), using sublinear_tf=True\n",
        "        word_tf = TfidfVectorizer(analyzer='word', ngram_range=(1,2), min_df=2, max_df=0.98,\n",
        "                                  vocabulary=word_cv.vocabulary_, lowercase=False, strip_accents='unicode', dtype=np.float32, sublinear_tf=True)\n",
        "        char_tf = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,6), min_df=2,\n",
        "                                  vocabulary=char_cv.vocabulary_, lowercase=False, strip_accents='unicode', dtype=np.float32, sublinear_tf=True)\n",
        "        t0 = timer(f'Chain {ci}: fit/transform TF-IDF (vocab-aligned)')\n",
        "        Xtr_w_tf = word_tf.fit_transform(X_text_tr.iloc[tr_idx])\n",
        "        Xva_w_tf = word_tf.transform(X_text_tr.iloc[va_idx])\n",
        "        Xte_w_tf = word_tf.transform(X_text_te)\n",
        "        Xtr_c_tf = char_tf.fit_transform(X_text_tr.iloc[tr_idx])\n",
        "        Xva_c_tf = char_tf.transform(X_text_tr.iloc[va_idx])\n",
        "        Xte_c_tf = char_tf.transform(X_text_te)\n",
        "        Xtr_tf = sparse.hstack([Xtr_w_tf, Xtr_c_tf]).tocsr()\n",
        "        Xva_tf = sparse.hstack([Xva_w_tf, Xva_c_tf]).tocsr()\n",
        "        Xte_tf = sparse.hstack([Xte_w_tf, Xte_c_tf]).tocsr()\n",
        "        done(t0, f'Chain {ci}: TF-IDF')\n",
        "        # 3) NB step on counts\n",
        "        t0 = timer(f'Chain {ci}: NB fit (counts) and r compute')\n",
        "        mnb = MultinomialNB(alpha=1.0); mnb.fit(Xtr_cnt, y[tr_idx])\n",
        "        r = (mnb.feature_log_prob_[1] - mnb.feature_log_prob_[0]).astype(np.float32)\n",
        "        r = np.clip(r, -8.0, 8.0)\n",
        "        done(t0, f'Chain {ci}: NB fit')\n",
        "        # 4) Apply r to TF-IDF features\n",
        "        Xtr_nb = Xtr_tf.multiply(r); Xva_nb = Xva_tf.multiply(r); Xte_nb = Xte_tf.multiply(r)\n",
        "        # 5) LR over NB-weighted features; grid over C\n",
        "        best_auc, best_pva, best_pte, bestC = -1.0, None, None, None\n",
        "        for C in params['C_grid']:\n",
        "            clf = LogisticRegression(solver='saga', penalty='l2', C=C, class_weight=None,\n",
        "                                     random_state=42, max_iter=4000, n_jobs=-1, verbose=0)\n",
        "            tfit = time.time(); clf.fit(Xtr_nb, y[tr_idx]); pva = clf.predict_proba(Xva_nb)[:,1]\n",
        "            auc = roc_auc_score(y[va_idx], pva)\n",
        "            print(f'  C={C} | AUC={auc:.6f} | secs={time.time()-tfit:.2f}', flush=True)\n",
        "            if auc > best_auc:\n",
        "                best_auc, best_pva, best_pte, bestC = auc, pva, clf.predict_proba(Xte_nb)[:,1], C\n",
        "        print(f'Chain {ci}: best C={bestC} | AUC={best_auc:.6f}', flush=True)\n",
        "        oof = globals().setdefault('oof', np.zeros(len(tr), dtype=np.float32))\n",
        "        val_mask = globals().setdefault('val_mask', np.zeros(len(tr), dtype=bool))\n",
        "        test_preds = globals().setdefault('test_preds', [])\n",
        "        oof[va_idx] = best_pva.astype(np.float32); val_mask[va_idx] = True; test_preds.append(best_pte.astype(np.float32))\n",
        "        del Xtr_w_cnt, Xva_w_cnt, Xte_w_cnt, Xtr_c_cnt, Xva_c_cnt, Xte_c_cnt, Xtr_cnt, Xva_cnt, Xte_cnt\n",
        "        del Xtr_w_tf, Xva_w_tf, Xte_w_tf, Xtr_c_tf, Xva_c_tf, Xte_c_tf, Xtr_tf, Xva_tf, Xte_tf, Xtr_nb, Xva_nb, Xte_nb\n",
        "        gc.collect()\n",
        "\n",
        "    oof = globals().get('oof')\n",
        "    val_mask = globals().get('val_mask')\n",
        "    test_preds = globals().get('test_preds', [])\n",
        "    if val_mask is not None and val_mask.any():\n",
        "        oof_auc = roc_auc_score(y[val_mask], oof[val_mask])\n",
        "        print('NB-SVM (venv, counts->MNB r, tfidf LR, binary counts, sublinear TF) OOF AUC:', round(oof_auc, 6))\n",
        "    np.save('oof_nbsvm_fc.npy', oof.astype(np.float32))\n",
        "    if len(test_preds):\n",
        "        test_pred = np.mean(np.vstack(test_preds), axis=0).astype(np.float32)\n",
        "        np.save('test_nbsvm_fc.npy', test_pred)\n",
        "        sub = pd.read_csv('sampleSubmission.csv')\n",
        "        sub['requester_received_pizza'] = np.clip(test_pred, 0.01, 0.99)\n",
        "        sub.to_csv('submission_nbsvm_fc.csv', index=False)\n",
        "        print('Saved: oof_nbsvm_fc.npy, test_nbsvm_fc.npy, submission_nbsvm_fc.csv')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "''')\n",
        "script.write_text(code)\n",
        "print('Wrote nbsvm_fc.py')\n",
        "\n",
        "# Run with venv python to avoid base-kernel sklearn issues\n",
        "venv_py = Path('.venv/bin/python'); assert venv_py.exists(), 'Missing .venv python; run setup first'\n",
        "print('Launching NB-SVM via', venv_py)\n",
        "proc = subprocess.run([str(venv_py), str(script)], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "print(proc.stdout)\n",
        "if proc.returncode != 0:\n",
        "    raise RuntimeError('nbsvm_fc.py failed; check logs above')\n",
        "print('NB-SVM venv run complete. Re-run Cell 12 to reblend.')"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote nbsvm_fc.py\nLaunching NB-SVM via .venv/bin/python\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chains: [1, 2, 3]\nParams: {'word_max_features': 240000, 'char_max_features': 300000, 'C_grid': [0.25, 0.5, 1.0, 2.0, 4.0]}\nChain 1: train 1727 | val 565\n[T0] Chain 1: fit counts ...\n[T+] Chain 1: fit counts done in 1.42s\n[T0] Chain 1: fit/transform TF-IDF (vocab-aligned) ...\n[T+] Chain 1: TF-IDF done in 1.34s\n[T0] Chain 1: NB fit (counts) and r compute ...\n[T+] Chain 1: NB fit done in 0.01s\n  C=0.25 | AUC=0.578622 | secs=0.46\n  C=0.5 | AUC=0.579256 | secs=0.76\n  C=1.0 | AUC=0.578605 | secs=1.28\n  C=2.0 | AUC=0.576336 | secs=1.98\n  C=4.0 | AUC=0.571728 | secs=2.52\nChain 1: best C=0.5 | AUC=0.579256\nChain 2: train 2158 | val 427\n[T0] Chain 2: fit counts ...\n[T+] Chain 2: fit counts done in 1.54s\n[T0] Chain 2: fit/transform TF-IDF (vocab-aligned) ...\n[T+] Chain 2: TF-IDF done in 1.46s\n[T0] Chain 2: NB fit (counts) and r compute ...\n[T+] Chain 2: NB fit done in 0.01s\n  C=0.25 | AUC=0.589716 | secs=1.09\n  C=0.5 | AUC=0.590555 | secs=1.43\n  C=1.0 | AUC=0.590165 | secs=2.18\n  C=2.0 | AUC=0.588038 | secs=2.91\n  C=4.0 | AUC=0.583663 | secs=3.72\nChain 2: best C=0.5 | AUC=0.590555\nChain 3: train 2302 | val 567\n[T0] Chain 3: fit counts ...\n[T+] Chain 3: fit counts done in 1.69s\n[T0] Chain 3: fit/transform TF-IDF (vocab-aligned) ...\n[T+] Chain 3: TF-IDF done in 1.58s\n[T0] Chain 3: NB fit (counts) and r compute ...\n[T+] Chain 3: NB fit done in 0.01s\n  C=0.25 | AUC=0.624795 | secs=1.26\n  C=0.5 | AUC=0.622992 | secs=1.74\n  C=1.0 | AUC=0.620027 | secs=2.18\n  C=2.0 | AUC=0.616382 | secs=3.10\n  C=4.0 | AUC=0.612576 | secs=3.93\nChain 3: best C=0.25 | AUC=0.624795\nNB-SVM (venv, counts->MNB r, tfidf LR, binary counts, sublinear TF) OOF AUC: 0.581379\nSaved: oof_nbsvm_fc.npy, test_nbsvm_fc.npy, submission_nbsvm_fc.csv\n\nNB-SVM venv run complete. Re-run Cell 12 to reblend.\n"
          ]
        }
      ]
    },
    {
      "id": "141622ba-11b9-4809-a881-41d4887a9dc7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# New Leg: Char-only TF-IDF + LR (char_wb 3-6), forward-chain per-chain fit\n",
        "import json, time, gc\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def load_json_df(path):\n",
        "    try: return pd.read_json(path, lines=True)\n",
        "    except ValueError:\n",
        "        try: return pd.read_json(path, lines=False)\n",
        "        except ValueError:\n",
        "            import json as _json\n",
        "            with open(path, 'r', encoding='utf-8') as f: data=_json.load(f)\n",
        "            if isinstance(data, dict) and 'data' in data: data=data['data']\n",
        "            return pd.json_normalize(data)\n",
        "\n",
        "def build_text(df):\n",
        "    def first_col(cols):\n",
        "        for c in cols:\n",
        "            if c in df.columns: return c\n",
        "        return None\n",
        "    tcol = first_col(['request_title','title'])\n",
        "    bcol = first_col(['request_text','body','text'])\n",
        "    t = df[tcol].fillna('') if tcol else pd.Series(['']*len(df))\n",
        "    b = df[bcol].fillna('') if bcol else pd.Series(['']*len(df))\n",
        "    t = t.astype(str).str.lower()\n",
        "    b = b.astype(str).str.lower()\n",
        "    url_pat = r'https?://\\S+|www\\.[^\\s]+'\n",
        "    t = t.str.replace(url_pat, ' URL ', regex=True).str.replace(r'\\d', '0', regex=True)\n",
        "    b = b.str.replace(url_pat, ' URL ', regex=True).str.replace(r'\\d', '0', regex=True)\n",
        "    return (t + ' ' + t + ' ' + t + ' ' + b)\n",
        "\n",
        "from contextlib import contextmanager\n",
        "@contextmanager\n",
        "def timer(msg):\n",
        "    t0 = time.time(); print(f\"[T0] {msg} ...\", flush=True)\n",
        "    try: yield\n",
        "    finally: print(f\"[T+] {msg} done in {time.time()-t0:.2f}s\", flush=True)\n",
        "\n",
        "with timer('Load data and align by time'):\n",
        "    tr = load_json_df('train.json'); te = load_json_df('test.json')\n",
        "    mf = json.loads(Path('folds/manifest.json').read_text())\n",
        "    time_col = mf.get('time_col','unix_timestamp_of_request_utc')\n",
        "    label_col = mf.get('label_col','requester_received_pizza')\n",
        "    tr = tr.sort_values(time_col, kind='mergesort').reset_index(drop=True)\n",
        "    y = pd.to_numeric(tr[label_col], errors='coerce').fillna(0).astype(int).clip(0,1).values\n",
        "    X_text_tr = build_text(tr)\n",
        "    X_text_te = build_text(te)\n",
        "\n",
        "fold_dir = Path('folds')\n",
        "mf = json.loads((fold_dir / 'manifest.json').read_text())\n",
        "chains = [c['chain'] for c in mf['chains']]\n",
        "print('Chains:', chains)\n",
        "\n",
        "oof = np.zeros(len(tr), dtype=np.float32)\n",
        "val_mask = np.zeros(len(tr), dtype=bool)\n",
        "test_preds_accum = []\n",
        "\n",
        "params = dict(\n",
        "    char_max_features=300000,\n",
        "    C_grid=[1.0, 2.0, 4.0]\n",
        ")\n",
        "print('Params:', params)\n",
        "\n",
        "def fit_chain(ci, seed=42):\n",
        "    tr_idx = np.load(fold_dir / f\"fc_chain{ci}_train_idx.npy\"); va_idx = np.load(fold_dir / f\"fc_chain{ci}_val_idx.npy\")\n",
        "    if len(va_idx) == 0:\n",
        "        print(f'Chain {ci}: empty val; skip'); return None\n",
        "    print(f'Chain {ci}: train {len(tr_idx)} | val {len(va_idx)}', flush=True)\n",
        "    char_vec = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,6), min_df=2,\n",
        "                               max_features=params['char_max_features'], lowercase=False,\n",
        "                               strip_accents='unicode', dtype=np.float32)\n",
        "    with timer(f'Chain {ci}: vectorize char TF-IDF'):\n",
        "        Xtr = char_vec.fit_transform(X_text_tr.iloc[tr_idx]).tocsr()\n",
        "        Xva = char_vec.transform(X_text_tr.iloc[va_idx]).tocsr()\n",
        "        Xte = char_vec.transform(X_text_te).tocsr()\n",
        "    ytr, yva = y[tr_idx], y[va_idx]\n",
        "    best_auc, best_pva, best_pte, bestC = -1.0, None, None, None\n",
        "    for C in params['C_grid']:\n",
        "        clf = LogisticRegression(solver='saga', penalty='l2', C=C,\n",
        "                                 class_weight=None,\n",
        "                                 random_state=seed, max_iter=3000, n_jobs=-1, verbose=0)\n",
        "        t0 = time.time(); clf.fit(Xtr, ytr); pva = clf.predict_proba(Xva)[:,1]\n",
        "        auc = roc_auc_score(yva, pva)\n",
        "        print(f'  C={C} | AUC={auc:.6f} | secs={time.time()-t0:.2f}', flush=True)\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_pva, best_pte, bestC = auc, pva, clf.predict_proba(Xte)[:,1], C\n",
        "    print(f'Chain {ci}: best C={bestC} | AUC={best_auc:.6f}', flush=True)\n",
        "    del Xtr, Xva, Xte; gc.collect()\n",
        "    return va_idx, best_pva.astype(np.float32), best_pte.astype(np.float32)\n",
        "\n",
        "with timer('Train Char-LR across forward chains'):\n",
        "    for ci in chains:\n",
        "        res = fit_chain(ci, seed=42)\n",
        "        if res is None: continue\n",
        "        va_idx, pva, pte = res\n",
        "        oof[va_idx] = pva\n",
        "        val_mask[va_idx] = True\n",
        "        test_preds_accum.append(pte)\n",
        "\n",
        "with timer('Evaluate and save Char-LR artifacts'):\n",
        "    if val_mask.any():\n",
        "        oof_auc = roc_auc_score(y[val_mask], oof[val_mask])\n",
        "        print('Char-LR OOF AUC (val rows only):', round(oof_auc, 6))\n",
        "    else:\n",
        "        print('Warning: no validation rows; OOF not computed.')\n",
        "    np.save('oof_charlr_fc.npy', oof.astype(np.float32))\n",
        "    if len(test_preds_accum):\n",
        "        test_pred = np.mean(np.vstack(test_preds_accum), axis=0).astype(np.float32)\n",
        "        np.save('test_charlr_fc.npy', test_pred)\n",
        "        sub = pd.read_csv('sampleSubmission.csv')\n",
        "        sub['requester_received_pizza'] = np.clip(test_pred, 0.01, 0.99)\n",
        "        sub.to_csv('submission_charlr_fc.csv', index=False)\n",
        "        print('Saved: oof_charlr_fc.npy, test_charlr_fc.npy, submission_charlr_fc.csv')\n",
        "    else:\n",
        "        print('Warning: no test preds collected.')"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Load data and align by time ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Load data and align by time done in 0.17s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chains: [1, 2, 3]\nParams: {'char_max_features': 300000, 'C_grid': [1.0, 2.0, 4.0]}\n[T0] Train Char-LR across forward chains ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 1: train 1727 | val 565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Chain 1: vectorize char TF-IDF ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Chain 1: vectorize char TF-IDF done in 1.14s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=1.0 | AUC=0.594136 | secs=3.12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=2.0 | AUC=0.581507 | secs=3.63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=4.0 | AUC=0.569916 | secs=4.17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 1: best C=1.0 | AUC=0.594136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 2: train 2158 | val 427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Chain 2: vectorize char TF-IDF ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Chain 2: vectorize char TF-IDF done in 1.26s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=1.0 | AUC=0.633255 | secs=4.29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=2.0 | AUC=0.624206 | secs=4.67\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=4.0 | AUC=0.615007 | secs=5.20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 2: best C=1.0 | AUC=0.633255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3: train 2302 | val 567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Chain 3: vectorize char TF-IDF ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Chain 3: vectorize char TF-IDF done in 1.37s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=1.0 | AUC=0.621910 | secs=4.41\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=2.0 | AUC=0.616682 | secs=4.97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=4.0 | AUC=0.609711 | secs=5.40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3: best C=1.0 | AUC=0.621910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Train Char-LR across forward chains done in 44.03s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Evaluate and save Char-LR artifacts ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char-LR OOF AUC (val rows only): 0.60305\nSaved: oof_charlr_fc.npy, test_charlr_fc.npy, submission_charlr_fc.csv\n[T+] Evaluate and save Char-LR artifacts done in 0.01s\n"
          ]
        }
      ]
    },
    {
      "id": "08848c0e-4454-4879-88f0-bb92f781e0e1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Leg: Word-only TF-IDF + LR (word 1-2, lowercase, sublinear), forward-chain per-chain fit\n",
        "import json, time, gc\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def load_json_df(path):\n",
        "    try: return pd.read_json(path, lines=True)\n",
        "    except ValueError:\n",
        "        try: return pd.read_json(path, lines=False)\n",
        "        except ValueError:\n",
        "            import json as _json\n",
        "            with open(path, 'r', encoding='utf-8') as f: data=_json.load(f)\n",
        "            if isinstance(data, dict) and 'data' in data: data=data['data']\n",
        "            return pd.json_normalize(data)\n",
        "\n",
        "def build_text(df):\n",
        "    def first_col(cols):\n",
        "        for c in cols:\n",
        "            if c in df.columns: return c\n",
        "        return None\n",
        "    tcol = first_col(['request_title','title'])\n",
        "    bcol = first_col(['request_text','body','text'])\n",
        "    t = df[tcol].fillna('') if tcol else pd.Series(['']*len(df))\n",
        "    b = df[bcol].fillna('') if bcol else pd.Series(['']*len(df))\n",
        "    t = t.astype(str).str.lower()\n",
        "    b = b.astype(str).str.lower()\n",
        "    url_pat = r'https?://\\S+|www\\.[^\\s]+'\n",
        "    t = t.str.replace(url_pat, ' url ', regex=True).str.replace(r'\\d', '0', regex=True)\n",
        "    b = b.str.replace(url_pat, ' url ', regex=True).str.replace(r'\\d', '0', regex=True)\n",
        "    return (t + ' ' + t + ' ' + t + ' ' + b)\n",
        "\n",
        "from contextlib import contextmanager\n",
        "@contextmanager\n",
        "def timer(msg):\n",
        "    t0 = time.time(); print(f\"[T0] {msg} ...\", flush=True)\n",
        "    try: yield\n",
        "    finally: print(f\"[T+] {msg} done in {time.time()-t0:.2f}s\", flush=True)\n",
        "\n",
        "with timer('Load data and align by time'):\n",
        "    tr = load_json_df('train.json'); te = load_json_df('test.json')\n",
        "    mf = json.loads(Path('folds/manifest.json').read_text())\n",
        "    time_col = mf.get('time_col','unix_timestamp_of_request_utc')\n",
        "    label_col = mf.get('label_col','requester_received_pizza')\n",
        "    tr = tr.sort_values(time_col, kind='mergesort').reset_index(drop=True)\n",
        "    y = pd.to_numeric(tr[label_col], errors='coerce').fillna(0).astype(int).clip(0,1).values\n",
        "    X_text_tr = build_text(tr)\n",
        "    X_text_te = build_text(te)\n",
        "\n",
        "fold_dir = Path('folds')\n",
        "mf = json.loads((fold_dir / 'manifest.json').read_text())\n",
        "chains = [c['chain'] for c in mf['chains']]\n",
        "print('Chains:', chains)\n",
        "\n",
        "oof = np.zeros(len(tr), dtype=np.float32)\n",
        "val_mask = np.zeros(len(tr), dtype=bool)\n",
        "test_preds_accum = []\n",
        "\n",
        "params = dict(\n",
        "    word_max_features=250000,\n",
        "    min_df=2,\n",
        "    max_df=0.995,\n",
        "    C_grid=[1.0, 2.0, 4.0, 8.0, 12.0]\n",
        ")\n",
        "print('Params:', params)\n",
        "\n",
        "def fit_chain(ci, seed=42):\n",
        "    tr_idx = np.load(fold_dir / f\"fc_chain{ci}_train_idx.npy\"); va_idx = np.load(fold_dir / f\"fc_chain{ci}_val_idx.npy\")\n",
        "    if len(va_idx) == 0:\n",
        "        print(f'Chain {ci}: empty val; skip'); return None\n",
        "    print(f'Chain {ci}: train {len(tr_idx)} | val {len(va_idx)}', flush=True)\n",
        "    word_vec = TfidfVectorizer(analyzer='word', ngram_range=(1,2),\n",
        "                               min_df=params['min_df'], max_df=params['max_df'],\n",
        "                               max_features=params['word_max_features'],\n",
        "                               lowercase=True, strip_accents='unicode',\n",
        "                               dtype=np.float32, sublinear_tf=True)\n",
        "    with timer(f'Chain {ci}: vectorize word TF-IDF (word-only)'):\n",
        "        Xtr = word_vec.fit_transform(X_text_tr.iloc[tr_idx]).tocsr()\n",
        "        Xva = word_vec.transform(X_text_tr.iloc[va_idx]).tocsr()\n",
        "        Xte = word_vec.transform(X_text_te).tocsr()\n",
        "    ytr, yva = y[tr_idx], y[va_idx]\n",
        "    best_auc, best_pva, best_pte, bestC = -1.0, None, None, None\n",
        "    for C in params['C_grid']:\n",
        "        clf = LogisticRegression(solver='saga', penalty='l2', C=C,\n",
        "                                 class_weight=None,\n",
        "                                 random_state=seed, max_iter=3000, n_jobs=-1, verbose=0)\n",
        "        t0 = time.time(); clf.fit(Xtr, ytr); pva = clf.predict_proba(Xva)[:,1]\n",
        "        auc = roc_auc_score(yva, pva)\n",
        "        print(f'  C={C} | AUC={auc:.6f} | secs={time.time()-t0:.2f}', flush=True)\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_pva, best_pte, bestC = auc, pva, clf.predict_proba(Xte)[:,1], C\n",
        "    print(f'Chain {ci}: best C={bestC} | AUC={best_auc:.6f}', flush=True)\n",
        "    del Xtr, Xva, Xte; gc.collect()\n",
        "    return va_idx, best_pva.astype(np.float32), best_pte.astype(np.float32)\n",
        "\n",
        "with timer('Train Word-only TF-IDF LR across forward chains'):\n",
        "    for ci in chains:\n",
        "        res = fit_chain(ci, seed=42)\n",
        "        if res is None: continue\n",
        "        va_idx, pva, pte = res\n",
        "        oof[va_idx] = pva\n",
        "        val_mask[va_idx] = True\n",
        "        test_preds_accum.append(pte)\n",
        "\n",
        "with timer('Evaluate and save Word-only TF-IDF LR artifacts'):\n",
        "    if val_mask.any():\n",
        "        oof_auc = roc_auc_score(y[val_mask], oof[val_mask])\n",
        "        print('Word-only TF-IDF LR OOF AUC (val rows only):', round(oof_auc, 6))\n",
        "    else:\n",
        "        print('Warning: no validation rows; OOF not computed.')\n",
        "    np.save('oof_wordlr_fc.npy', oof.astype(np.float32))\n",
        "    if len(test_preds_accum):\n",
        "        test_pred = np.mean(np.vstack(test_preds_accum), axis=0).astype(np.float32)\n",
        "        np.save('test_wordlr_fc.npy', test_pred)\n",
        "        sub = pd.read_csv('sampleSubmission.csv')\n",
        "        sub['requester_received_pizza'] = np.clip(test_pred, 0.01, 0.99)\n",
        "        sub.to_csv('submission_wordlr_fc.csv', index=False)\n",
        "        print('Saved: oof_wordlr_fc.npy, test_wordlr_fc.npy, submission_wordlr_fc.csv')\n",
        "    else:\n",
        "        print('Warning: no test preds collected.')"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Load data and align by time ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Load data and align by time done in 0.16s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chains: [1, 2, 3]\nParams: {'word_max_features': 250000, 'min_df': 2, 'max_df': 0.995, 'C_grid': [1.0, 2.0, 4.0, 8.0, 12.0]}\n[T0] Train Word-only TF-IDF LR across forward chains ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 1: train 1727 | val 565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Chain 1: vectorize word TF-IDF (word-only) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Chain 1: vectorize word TF-IDF (word-only) done in 0.27s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=1.0 | AUC=0.592940 | secs=0.47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=2.0 | AUC=0.588754 | secs=0.60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=4.0 | AUC=0.584040 | secs=0.74\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=8.0 | AUC=0.580452 | secs=0.88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=12.0 | AUC=0.578271 | secs=1.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 1: best C=1.0 | AUC=0.592940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 2: train 2158 | val 427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Chain 2: vectorize word TF-IDF (word-only) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Chain 2: vectorize word TF-IDF (word-only) done in 0.30s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=1.0 | AUC=0.635173 | secs=0.77\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=2.0 | AUC=0.632686 | secs=0.89\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=4.0 | AUC=0.631218 | secs=1.08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=8.0 | AUC=0.628371 | secs=1.23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=12.0 | AUC=0.628191 | secs=1.33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 2: best C=1.0 | AUC=0.635173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3: train 2302 | val 567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Chain 3: vectorize word TF-IDF (word-only) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Chain 3: vectorize word TF-IDF (word-only) done in 0.33s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=1.0 | AUC=0.620548 | secs=0.77\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=2.0 | AUC=0.617583 | secs=0.91\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=4.0 | AUC=0.615380 | secs=1.12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=8.0 | AUC=0.613757 | secs=1.30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  C=12.0 | AUC=0.612365 | secs=1.44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain 3: best C=1.0 | AUC=0.620548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T+] Train Word-only TF-IDF LR across forward chains done in 15.82s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T0] Evaluate and save Word-only TF-IDF LR artifacts ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word-only TF-IDF LR OOF AUC (val rows only): 0.598862\nSaved: oof_wordlr_fc.npy, test_wordlr_fc.npy, submission_wordlr_fc.csv\n[T+] Evaluate and save Word-only TF-IDF LR artifacts done in 0.01s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}