{
  "cells": [
    {
      "id": "7ddac395-e0bd-47cb-ba02-f1a6158d2ed0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quantile LGBM v3: 5-seed bag for q50 delta (OOF + Test); reuse v2 bands for sigma later\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "np.random.seed(42)\n",
        "pd.set_option('display.max_columns', 200)\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "# Helpers\n",
        "def prepare_baseline_table(df):\n",
        "    base = (df.sort_values(['Patient','Weeks']).groupby('Patient', as_index=False).first())\n",
        "    base = base[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "        columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "    return base\n",
        "\n",
        "def one_hot_fit(df, cols):\n",
        "    return {c: sorted(df[c].dropna().astype(str).unique().tolist()) for c in cols}\n",
        "\n",
        "def one_hot_transform(df, cats):\n",
        "    out = df.copy()\n",
        "    for c, values in cats.items():\n",
        "        col = df[c].astype(str)\n",
        "        for v in values:\n",
        "            out[f'{c}__{v}'] = (col == v).astype(np.int8)\n",
        "    return out\n",
        "\n",
        "def ecdf_rank_fit(x):\n",
        "    xs = np.sort(np.asarray(x, dtype=float))\n",
        "    return xs\n",
        "\n",
        "def ecdf_rank_transform(x, xs):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    idx = np.searchsorted(xs, x, side='right')\n",
        "    return idx / max(len(xs), 1)\n",
        "\n",
        "def build_slope_features(base_df, ecdf_basefvc=None, ecdf_percent=None, cats=None, fit=False):\n",
        "    b = base_df.copy()\n",
        "    b['log_Base_FVC'] = np.log1p(np.maximum(b['Base_FVC'].astype(float), 1.0))\n",
        "    b['BaseFVC_over_Age'] = b['Base_FVC'].astype(float) / np.maximum(b['Age'].astype(float), 1.0)\n",
        "    b['PercentBase_over_Age'] = b['Percent_at_base'].astype(float) / np.maximum(b['Age'].astype(float), 1.0)\n",
        "    if fit:\n",
        "        ecdf_basefvc = ecdf_rank_fit(b['Base_FVC'].values)\n",
        "        ecdf_percent = ecdf_rank_fit(b['Percent_at_base'].values)\n",
        "    b['BaseFVC_ecdf'] = ecdf_rank_transform(b['Base_FVC'].values, ecdf_basefvc)\n",
        "    b['Percent_ecdf'] = ecdf_rank_transform(b['Percent_at_base'].values, ecdf_percent)\n",
        "    if fit:\n",
        "        cats = one_hot_fit(b, ['Sex','SmokingStatus'])\n",
        "    b = one_hot_transform(b, cats)\n",
        "    num_cols = ['Age','Base_FVC','log_Base_FVC','Percent_at_base','BaseFVC_over_Age','PercentBase_over_Age','BaseFVC_ecdf','Percent_ecdf']\n",
        "    cat_cols = [c for c in b.columns if c.startswith('Sex__') or c.startswith('SmokingStatus__')]\n",
        "    feat_cols = num_cols + cat_cols\n",
        "    return b, feat_cols, ecdf_basefvc, ecdf_percent, cats\n",
        "\n",
        "def compute_patient_slopes(df, patient_col='Patient', week_col='Weeks', target_col='FVC'):\n",
        "    slopes = {}\n",
        "    for pid, g in df.groupby(patient_col):\n",
        "        if g.shape[0] >= 2:\n",
        "            x = g[week_col].values.astype(float); y = g[target_col].values.astype(float)\n",
        "            xm = x.mean(); ym = y.mean()\n",
        "            denom = ((x - xm)**2).sum()\n",
        "            slope = ((x - xm) * (y - ym)).sum() / denom if denom > 0 else 0.0\n",
        "            slopes[pid] = slope\n",
        "    return slopes\n",
        "\n",
        "def build_q_features(grid_df, base_df, ecdf_bf=None, ecdf_pc=None, cats=None, fit=False):\n",
        "    d = grid_df.merge(base_df[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "    d['dist'] = (d['Weeks'] - d['Base_Week']).astype(float)\n",
        "    d = d[d['dist'] >= 0].copy()\n",
        "    d['abs_dist'] = d['dist'].abs()\n",
        "    d['log1p_abs_dist'] = np.log1p(d['abs_dist'])\n",
        "    d['dist_cap'] = d['dist'].clip(0, 30)\n",
        "    d['dist_short'] = d['dist'].clip(0, 5)\n",
        "    d['dist_mid'] = (d['dist'] - 5).clip(lower=0, upper=10)\n",
        "    d['dist_long'] = (d['dist'] - 15).clip(lower=0)\n",
        "    d['dist2'] = d['dist']**2\n",
        "    d['dist3'] = d['dist']**3\n",
        "    d['Base_FVC'] = d['Base_FVC'].astype(float)\n",
        "    d['Percent_at_base'] = d['Percent_at_base'].astype(float).clip(30, 120)\n",
        "    d['Age'] = d['Age'].astype(float)\n",
        "    d['log_Base_FVC'] = np.log1p(np.maximum(d['Base_FVC'], 1.0))\n",
        "    d['Age_x_Percent'] = d['Age'] * d['Percent_at_base']\n",
        "    d['BaseFVC_x_dist'] = d['Base_FVC'] * d['dist']\n",
        "    d['dist_x_Age'] = d['dist'] * d['Age']\n",
        "    d['dist_x_Percent'] = d['dist'] * d['Percent_at_base']\n",
        "    d['BaseFVC_x_dshort'] = d['Base_FVC'] * d['dist_short']\n",
        "    d['BaseFVC_x_dmid'] = d['Base_FVC'] * d['dist_mid']\n",
        "    d['BaseFVC_x_dlong'] = d['Base_FVC'] * d['dist_long']\n",
        "    if fit:\n",
        "        ecdf_bf = ecdf_rank_fit(d['Base_FVC'].values)\n",
        "        ecdf_pc = ecdf_rank_fit(d['Percent_at_base'].values)\n",
        "        cats = one_hot_fit(d, ['Sex','SmokingStatus'])\n",
        "    d['BaseFVC_ecdf'] = ecdf_rank_transform(d['Base_FVC'].values, ecdf_bf)\n",
        "    d['Percent_ecdf'] = ecdf_rank_transform(d['Percent_at_base'].values, ecdf_pc)\n",
        "    d = one_hot_transform(d, cats)\n",
        "    d['BFV_decile'] = np.floor(d['BaseFVC_ecdf'] * 10).clip(0, 9).astype(int)\n",
        "    for k in range(10):\n",
        "        d[f'BFV_decile__{k}'] = (d['BFV_decile'] == k).astype(np.int8)\n",
        "    feat_cols = [\n",
        "        'Age','Base_FVC','log_Base_FVC','Percent_at_base','BaseFVC_ecdf','Percent_ecdf',\n",
        "        'dist','abs_dist','log1p_abs_dist','dist_cap','dist_short','dist_mid','dist_long','dist2','dist3',\n",
        "        'Age_x_Percent','BaseFVC_x_dist','dist_x_Age','dist_x_Percent','BaseFVC_x_dshort','BaseFVC_x_dmid','BaseFVC_x_dlong','s_hat'\n",
        "    ] + [c for c in d.columns if c.startswith('Sex__') or c.startswith('SmokingStatus__') or c.startswith('BFV_decile__')]\n",
        "    for c in feat_cols:\n",
        "        if c not in d.columns: d[c] = 0.0\n",
        "    return d, feat_cols, ecdf_bf, ecdf_pc, cats\n",
        "\n",
        "def fit_s_hat_fold(trn_df, base_trn):\n",
        "    slopes_trn = compute_patient_slopes(trn_df)\n",
        "    slope_labels_trn = pd.DataFrame({'Patient': list(slopes_trn.keys()), 's_label': list(slopes_trn.values())})\n",
        "    base_trn_lab = base_trn.merge(slope_labels_trn, on='Patient', how='left')\n",
        "    bf_trn, f_cols_s, ecdf_bf_s, ecdf_pc_s, cats_s = build_slope_features(base_trn_lab, fit=True)\n",
        "    scaler_s = StandardScaler(with_mean=True, with_std=True).fit(bf_trn[f_cols_s].values.astype(float))\n",
        "    Xs_tr = scaler_s.transform(bf_trn[f_cols_s].values.astype(float))\n",
        "    y_s = bf_trn['s_label'].fillna(0.0).values.astype(float)\n",
        "    ridge = Ridge(alpha=1.0, random_state=42).fit(Xs_tr, y_s)\n",
        "    knn = KNeighborsRegressor(n_neighbors=9, weights='distance').fit(Xs_tr, y_s)\n",
        "    q_lo, q_hi = np.percentile(y_s, [5,95])\n",
        "    def get_s_hat_map(base_df_patients):\n",
        "        bf_pred, _, _, _, _ = build_slope_features(base_df_patients, ecdf_bf_s, ecdf_pc_s, cats_s, fit=False)\n",
        "        Xs = scaler_s.transform(bf_pred[f_cols_s].values.astype(float))\n",
        "        s = 0.8*ridge.predict(Xs) + 0.2*knn.predict(Xs)\n",
        "        s = np.clip(s, q_lo, q_hi)\n",
        "        return dict(zip(bf_pred['Patient'].values, s))\n",
        "    return get_s_hat_map\n",
        "\n",
        "# Config per expert\n",
        "seeds = [1337, 2027, 3037, 4242, 5151]\n",
        "params = dict(objective='quantile', metric='quantile',\n",
        "              n_estimators=2600, learning_rate=0.032,\n",
        "              num_leaves=31, max_depth=6, min_data_in_leaf=24,\n",
        "              subsample=0.75, colsample_bytree=0.75,\n",
        "              reg_alpha=0.1, reg_lambda=0.2, n_jobs=-1, verbose=-1)\n",
        "\n",
        "# OOF frame to collect q50 deltas\n",
        "oof_df = train[['Patient','Weeks','FVC']].copy()\n",
        "oof_df['q50_delta_oof'] = np.nan\n",
        "\n",
        "# Static TEST grid and index map\n",
        "grid_te = ss.copy()\n",
        "parts = grid_te['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid_te['Patient'] = parts[0]; grid_te['Weeks'] = parts[1].astype(int)\n",
        "test_base = test[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "    columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid_te_idx = grid_te[['Patient','Weeks']].copy()\n",
        "grid_te_idx['ss_idx'] = np.arange(grid_te_idx.shape[0], dtype=int)\n",
        "\n",
        "# Accumulators for TEST q50 delta\n",
        "test_pred_sum = np.zeros(ss.shape[0], dtype=float)\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "groups = train['Patient'].values\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    tf = time.time()\n",
        "    trn_df = train.iloc[trn_idx].copy(); val_df = train.iloc[val_idx].copy()\n",
        "    base_trn = prepare_baseline_table(trn_df)\n",
        "    base_val = prepare_baseline_table(val_df)\n",
        "    # s_hat maps\n",
        "    get_s_hat_map = fit_s_hat_fold(trn_df, base_trn)\n",
        "    s_map_trn = get_s_hat_map(base_trn)\n",
        "    s_map_val = get_s_hat_map(base_val)\n",
        "    base_test = grid_te[['Patient']].drop_duplicates().merge(test_base.drop_duplicates('Patient'), on='Patient', how='left')\n",
        "    s_map_test = get_s_hat_map(base_test[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']])\n",
        "\n",
        "    # Build future-only train/val with s_hat\n",
        "    trn = trn_df.merge(base_trn[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "    val = val_df.merge(base_val[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "    trn['dist'] = (trn['Weeks'] - trn['Base_Week']).astype(float); trn = trn[trn['dist'] >= 0].copy()\n",
        "    val['dist'] = (val['Weeks'] - val['Base_Week']).astype(float); val = val[val['dist'] >= 0].copy()\n",
        "    trn['s_hat'] = trn['Patient'].map(s_map_trn).astype(float).fillna(0.0)\n",
        "    val['s_hat'] = val['Patient'].map(s_map_val).astype(float).fillna(0.0)\n",
        "\n",
        "    # Features (fit on TRAIN fold)\n",
        "    trn_feat, feat_cols, ecdf_bf, ecdf_pc, cats = build_q_features(trn[['Patient','Weeks']].copy(), base_trn, fit=True)\n",
        "    trn_feat['s_hat'] = trn_feat['Patient'].map(s_map_trn).astype(float).fillna(0.0)\n",
        "    val_feat, _, _, _, _ = build_q_features(val[['Patient','Weeks']].copy(), base_val, ecdf_bf, ecdf_pc, cats, fit=False)\n",
        "    val_feat['s_hat'] = val_feat['Patient'].map(s_map_val).astype(float).fillna(0.0)\n",
        "\n",
        "    # Align features with labels strictly\n",
        "    trn_feat_aligned = trn_feat.merge(trn[['Patient','Weeks','FVC']], on=['Patient','Weeks'], how='inner')\n",
        "    val_feat_aligned = val_feat.merge(val[['Patient','Weeks','FVC']], on=['Patient','Weeks'], how='inner')\n",
        "\n",
        "    y_tr_delta = (trn_feat_aligned['FVC'].astype(float).values - trn_feat_aligned['Base_FVC'].astype(float).values)\n",
        "    y_va_delta = (val_feat_aligned['FVC'].astype(float).values - val_feat_aligned['Base_FVC'].astype(float).values)\n",
        "    X_tr = trn_feat_aligned[feat_cols].values.astype(float)\n",
        "    X_va = val_feat_aligned[feat_cols].values.astype(float)\n",
        "\n",
        "    if X_tr.shape[0] == 0 or X_va.shape[0] == 0:\n",
        "        print(f'[v3 Fold {fold}] skipped (X_tr={X_tr.shape[0]}, X_va={X_va.shape[0]})', flush=True)\n",
        "        del trn_df, val_df, trn, val, trn_feat, val_feat, trn_feat_aligned, val_feat_aligned\n",
        "        gc.collect()\n",
        "        continue\n",
        "\n",
        "    # Build TEST features under TRAIN-fold transforms; align to ss via index map\n",
        "    te_feat, _, _, _, _ = build_q_features(grid_te[['Patient','Weeks']].copy(), test_base, ecdf_bf, ecdf_pc, cats, fit=False)\n",
        "    te_feat['s_hat'] = te_feat['Patient'].map(s_map_test).astype(float).fillna(0.0)\n",
        "    X_te = te_feat[feat_cols].values.astype(float)\n",
        "    te_keys = te_feat[['Patient','Weeks']].copy().merge(grid_te_idx, on=['Patient','Weeks'], how='left')\n",
        "    te_idx = te_keys['ss_idx'].values.astype(int)\n",
        "\n",
        "    # Seed bagging for q50 (alpha=0.5); jitter lr by \u00b10.002 per seed index\n",
        "    val_pred_sum = np.zeros(X_va.shape[0], dtype=float)\n",
        "    test_pred_sum_fold = np.zeros(ss.shape[0], dtype=float)\n",
        "\n",
        "    for si, sd in enumerate(seeds):\n",
        "        lr = params['learning_rate'] + (0.002 if (si % 2 == 0) else -0.002)\n",
        "        mdl = LGBMRegressor(**{**params, 'alpha': 0.5, 'learning_rate': lr}, random_state=sd)\n",
        "        mdl.fit(X_tr, y_tr_delta,\n",
        "                eval_set=[(X_va, y_va_delta)],\n",
        "                eval_metric='quantile',\n",
        "                callbacks=[lgb.early_stopping(200, verbose=False)])\n",
        "        val_pred_sum += mdl.predict(X_va, num_iteration=mdl.best_iteration_)\n",
        "        pred_te = mdl.predict(X_te, num_iteration=mdl.best_iteration_)\n",
        "        # Scatter into full-sized array using te_idx\n",
        "        test_pred_sum_fold[te_idx] += pred_te\n",
        "        del mdl\n",
        "    # Average over seeds\n",
        "    val_pred_avg = val_pred_sum / max(len(seeds), 1)\n",
        "    test_pred_avg_fold = test_pred_sum_fold / max(len(seeds), 1)\n",
        "\n",
        "    # Write OOF deltas back by keys\n",
        "    keys = val_feat_aligned[['Patient','Weeks']].reset_index(drop=True)\n",
        "    block = pd.DataFrame({'Patient': keys['Patient'].astype(str), 'Weeks': keys['Weeks'].astype(int), 'q50_delta_oof': val_pred_avg})\n",
        "    oof_df = oof_df.merge(block, on=['Patient','Weeks'], how='left', suffixes=('','_new'))\n",
        "    oof_df['q50_delta_oof'] = oof_df['q50_delta_oof'].fillna(oof_df['q50_delta_oof_new'])\n",
        "    oof_df.drop(columns=['q50_delta_oof_new'], inplace=True)\n",
        "\n",
        "    # Accumulate TEST deltas (fold-average)\n",
        "    test_pred_sum += (test_pred_avg_fold / 5.0)  # average across 5 folds\n",
        "\n",
        "    print(f'[v3 Fold {fold}] trn={X_tr.shape[0]} val={X_va.shape[0]} elapsed={time.time()-tf:.2f}s', flush=True)\n",
        "    del trn_df, val_df, trn, val, trn_feat, val_feat, trn_feat_aligned, val_feat_aligned, X_tr, X_va, X_te, te_feat, te_idx, te_keys\n",
        "    gc.collect()\n",
        "\n",
        "# Save OOF v3 (q50 only) with Base_FVC for downstream reconstruction\n",
        "train_base = prepare_baseline_table(train)\n",
        "oof_save = oof_df.dropna(subset=['q50_delta_oof']).merge(train_base[['Patient','Base_Week','Base_FVC']], on='Patient', how='left')\n",
        "oof_save.to_csv('oof_quantile_lgbm_v3.csv', index=False)\n",
        "\n",
        "# Save TEST q50 deltas aligned to ss; reusing v2 q20/q80 later for sigma is allowed\n",
        "pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'q50_d': test_pred_sum.astype(float)}).to_csv('pred_quantile_deltas_v3.csv', index=False)\n",
        "\n",
        "print(f'Saved oof_quantile_lgbm_v3.csv and pred_quantile_deltas_v3.csv. Total elapsed {time.time()-t0:.1f}s')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v3 Fold 1] trn=1124 val=284 elapsed=1.42s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v3 Fold 2] trn=1127 val=281 elapsed=0.86s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v3 Fold 3] trn=1129 val=279 elapsed=0.92s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v3 Fold 4] trn=1129 val=279 elapsed=1.21s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v3 Fold 5] trn=1123 val=285 elapsed=1.02s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_quantile_lgbm_v3.csv and pred_quantile_deltas_v3.csv. Total elapsed 5.7s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}