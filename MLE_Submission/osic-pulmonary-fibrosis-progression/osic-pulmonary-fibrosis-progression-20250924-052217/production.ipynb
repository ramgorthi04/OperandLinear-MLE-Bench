{
  "cells": [
    {
      "id": "329e9118-2c88-4c41-b743-212a6ae7f7f8",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Production pipeline: OSIC Pulmonary Fibrosis Progression\n",
        "\n",
        "Goal:\n",
        "- Reproduce a clean, medal-ready submission with strict forward-CV hygiene, seed-averaged XGB residual backbone, tiny LGB weight sweep, anchored baseline blend, and robust sigma.\n",
        "\n",
        "Plan:\n",
        "1) Environment + data load; deterministic setup.\n",
        "2) Helpers: metric, slope/stats builders, one-hot utils, leak-free build_features_v2.\n",
        "3) Forward-CV residual XGB:\n",
        "   - 5-fold GroupKFold (by Patient), forward scoring (weeks >= base).\n",
        "   - Strict hygiene: Percent_at_base only; zero trend stats for VAL/TEST.\n",
        "   - 9-seed jittered bag; aggregate OOF/test.\n",
        "4) Sigma primary:\n",
        "   - Tune (a,b,c) on OOF with abs_res proxy; floor >=70; optional floor >=100 for dist>20.\n",
        "5) Optional diversity:\n",
        "   - Tiny CPU LightGBM residual; sweep small weight wl in [0.05,0.10,0.15,0.20,0.25] vs XGB on OOF.\n",
        "6) Blending:\n",
        "   - Primary FVC: 0.85*seed-avg (or XGB/LGB sweep) + 0.15*anchor.\n",
        "   - Guardrails: per-patient FVC non-increasing in future; clip [500, 6000].\n",
        "   - Sigma guardrail: non-decreasing with distance.\n",
        "7) Sigma strategy:\n",
        "   - Primary: tuned hybrid sigma (a,b,c).\n",
        "   - Banker: sigma = max(240 + 3*dist, 70).\n",
        "   - Rule: if primary vs banker OOF delta < 0.02, prefer banker.\n",
        "8) Outputs:\n",
        "   - Save submission_primary.csv (primary sigma) and submission_banker.csv (banker).\n",
        "   - Choose which to write as submission.csv based on OOF rule.\n",
        "\n",
        "Notes:\n",
        "- Recompute one-hot schema on full train and reuse for test (avoid one-hot schema leak).\n",
        "- Strict test hygiene: no train-derived per-patient trend stats used on test; use baseline-only features.\n",
        "- Log per-fold progress and elapsed time."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "269dfc8a-8397-41f4-80e3-7c62610dc02e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Copy banker submission to submission.csv (banker: sigma = max(240 + 3*dist, 70))\n",
        "import pandas as pd\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub_banker = pd.read_csv('submission_banker.csv')\n",
        "assert sub_banker.shape[0] == ss.shape[0], 'Row count mismatch vs sample_submission'\n",
        "assert set(sub_banker['Patient_Week'].astype(str)) == set(ss['Patient_Week'].astype(str)), 'Patient_Week sets differ'\n",
        "assert sub_banker['FVC'].notna().all() and sub_banker['Confidence'].notna().all(), 'NaNs in banker submission'\n",
        "sub_banker.to_csv('submission.csv', index=False)\n",
        "print('submission.csv overwritten with banker submission (sigma=240+3*dist).')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv overwritten with banker submission (sigma=240+3*dist).\n"
          ]
        }
      ]
    },
    {
      "id": "4b0384a3-9035-4afa-8c15-7f6edbf30402",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Copy hybrid (primary) submission to submission.csv for second submit\n",
        "import pandas as pd\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub_primary = pd.read_csv('submission_primary.csv')\n",
        "assert sub_primary.shape[0] == ss.shape[0], 'Row count mismatch vs sample_submission'\n",
        "assert set(sub_primary['Patient_Week'].astype(str)) == set(ss['Patient_Week'].astype(str)), 'Patient_Week sets differ'\n",
        "assert sub_primary['FVC'].notna().all() and sub_primary['Confidence'].notna().all(), 'NaNs in primary submission'\n",
        "sub_primary.to_csv('submission.csv', index=False)\n",
        "print('submission.csv overwritten with hybrid primary submission (tuned hybrid sigma).')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv overwritten with hybrid primary submission (tuned hybrid sigma).\n"
          ]
        }
      ]
    },
    {
      "id": "f581ab1a-ab68-47d9-beda-92cc1610cdcd",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build hybrid sigma floored by banker: sigma = max(hybrid, banker, 70), with guardrails; overwrite submission.csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub_primary = pd.read_csv('submission_primary.csv')  # hybrid\n",
        "sub_banker = pd.read_csv('submission_banker.csv')   # banker\n",
        "\n",
        "assert sub_primary.shape[0] == ss.shape[0] == sub_banker.shape[0], 'Row count mismatch'\n",
        "assert set(sub_primary['Patient_Week'].astype(str)) == set(ss['Patient_Week'].astype(str)) == set(sub_banker['Patient_Week'].astype(str)), 'Patient_Week sets differ'\n",
        "\n",
        "# Rebuild grid to compute dist and apply guardrails\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]\n",
        "grid['Weeks'] = parts[1].astype(int)\n",
        "test = pd.read_csv('test.csv')\n",
        "test_bl = test[['Patient','Weeks','FVC','Percent']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_bl, on='Patient', how='left')\n",
        "grid['dist'] = (grid['Weeks'] - grid['Base_Week']).abs().astype(float)\n",
        "\n",
        "# Align submissions with grid order\n",
        "p = sub_primary.set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "b = sub_banker.set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "\n",
        "# FVC from hybrid primary; re-enforce non-increasing per patient\n",
        "df_out = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'FVC': p['FVC'].astype(float).clip(500, 6000)})\n",
        "def enforce_non_increasing(g):\n",
        "    g = g.sort_values('Weeks').copy()\n",
        "    g['FVC'] = np.minimum.accumulate(g['FVC'].values[::-1])[::-1]\n",
        "    return g\n",
        "df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(enforce_non_increasing)\n",
        "fvc_final = df_out['FVC'].values.astype(float)\n",
        "\n",
        "# Sigma: hybrid floored by banker; then guardrails\n",
        "sigma_h = p['Confidence'].astype(float).values\n",
        "sigma_b = b['Confidence'].astype(float).values\n",
        "sigma = np.maximum(sigma_h, sigma_b)\n",
        "sigma = np.maximum(sigma, 70.0)\n",
        "# Floor sigma to >=100 when dist > 20\n",
        "sigma = np.where(grid['dist'].values > 20.0, np.maximum(sigma, 100.0), sigma)\n",
        "\n",
        "df_sig = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': grid['dist'].values.astype(float), 'Sigma': sigma})\n",
        "def enforce_sigma_monotone(g):\n",
        "    g = g.sort_values('dist').copy()\n",
        "    g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "    return g\n",
        "df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n",
        "sigma_final = df_sig['Sigma'].values.astype(float)\n",
        "\n",
        "submission_hybrid_floored = pd.DataFrame({\n",
        "    'Patient_Week': ss['Patient_Week'],\n",
        "    'FVC': fvc_final,\n",
        "    'Confidence': sigma_final\n",
        "})\n",
        "submission_hybrid_floored.to_csv('submission_hybrid_floored.csv', index=False)\n",
        "submission_hybrid_floored.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_hybrid_floored.csv and overwritten submission.csv with hybrid sigma floored by banker, guardrails enforced.')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_hybrid_floored.csv and overwritten submission.csv with hybrid sigma floored by banker, guardrails enforced.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/3389684957.py:32: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(enforce_non_increasing)\n/tmp/ipykernel_4688/3389684957.py:48: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n"
          ]
        }
      ]
    },
    {
      "id": "ee9a1a30-d073-4ba6-807d-87a7668d0fe0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup: imports, seeds, data load, core helpers (metric, one-hot, ECDF, slope utils), leak-free feature builder\n",
        "import os, sys, time, math, gc, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "np.random.seed(42); random.seed(42)\n",
        "pd.set_option('display.max_columns', 200)\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "# Metric: modified Laplace log-likelihood (maximize)\n",
        "def laplace_ll(y_true, y_pred, sigma):\n",
        "    y_true = np.asarray(y_true).astype(float)\n",
        "    y_pred = np.asarray(y_pred).astype(float)\n",
        "    sigma = np.asarray(sigma).astype(float)\n",
        "    delta = np.minimum(np.abs(y_true - y_pred), 1000.0)\n",
        "    sigma = np.maximum(sigma, 70.0)\n",
        "    return float(np.mean(-delta / sigma - np.log(sigma)))\n",
        "\n",
        "# One-hot utilities (fit on reference df; transform applies same columns; unseen -> zeros)\n",
        "def one_hot_fit(df, cols):\n",
        "    return {c: sorted(df[c].dropna().astype(str).unique().tolist()) for c in cols}\n",
        "\n",
        "def one_hot_transform(df, cats):\n",
        "    out = df.copy()\n",
        "    for c, values in cats.items():\n",
        "        col = df[c].astype(str)\n",
        "        for v in values:\n",
        "            out[f'{c}__{v}'] = (col == v).astype(np.int8)\n",
        "    return out\n",
        "\n",
        "# ECDF rank (0..1) fitted on train only\n",
        "def ecdf_rank_fit(x):\n",
        "    xs = np.sort(np.asarray(x, dtype=float))\n",
        "    return xs\n",
        "\n",
        "def ecdf_rank_transform(x, xs):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    # rank = fraction <= x\n",
        "    idx = np.searchsorted(xs, x, side='right')\n",
        "    return idx / max(len(xs), 1)\n",
        "\n",
        "# Per-patient slope (OLS) on (Weeks, FVC)\n",
        "def compute_patient_slopes(df, patient_col='Patient', week_col='Weeks', target_col='FVC'):\n",
        "    slopes = {}\n",
        "    for pid, g in df.groupby(patient_col):\n",
        "        if g.shape[0] >= 2:\n",
        "            x = g[week_col].values.astype(float); y = g[target_col].values.astype(float)\n",
        "            xm = x.mean(); ym = y.mean()\n",
        "            denom = ((x - xm)**2).sum()\n",
        "            slope = ((x - xm) * (y - ym)).sum() / denom if denom > 0 else 0.0\n",
        "            slopes[pid] = slope\n",
        "    return slopes\n",
        "\n",
        "def robust_global_slope(slopes_dict):\n",
        "    if not slopes_dict: return 0.0\n",
        "    return float(np.median(list(slopes_dict.values())))\n",
        "\n",
        "# Leak-free feature builder: baseline-only Percent, zero trends for val/test\n",
        "def build_features_v2(df, cap_wp=26):\n",
        "    d = df.copy()\n",
        "    # Weeks_Passed relative to Base_Week\n",
        "    d['Weeks_Passed'] = (d['Weeks'] - d['Base_Week']).astype(float)\n",
        "    d['Abs_Weeks_Passed'] = d['Weeks_Passed'].abs()\n",
        "    d['Weeks_Passed_cap'] = d['Weeks_Passed'].clip(-cap_wp, cap_wp)\n",
        "    d['Weeks_Passed2'] = d['Weeks_Passed'] ** 2\n",
        "    d['sign_WP'] = np.sign(d['Weeks_Passed']).astype(int)\n",
        "    d['is_future'] = (d['Weeks_Passed'] >= 0).astype(int)\n",
        "    # Baseline-only Percent usage\n",
        "    d['Percent_at_base'] = d['Percent_at_base'].astype(float)\n",
        "    d['Percent_clipped'] = d['Percent_at_base'].clip(30, 120)\n",
        "    d['Percent2'] = d['Percent_clipped'] ** 2\n",
        "    # Base_FVC transforms\n",
        "    d['Base_FVC'] = d['Base_FVC'].astype(float)\n",
        "    d['log_BaseFVC'] = np.log1p(np.maximum(d['Base_FVC'], 1.0))\n",
        "    # Simple TLC proxy\n",
        "    d['Estimated_TLC'] = d['Base_FVC'] / (d['Percent_at_base'].clip(1e-3) / 100.0)\n",
        "    d['log_TLC'] = np.log1p(np.maximum(d['Estimated_TLC'], 1.0))\n",
        "    # Interactions\n",
        "    d['Age'] = d['Age'].astype(float)\n",
        "    d['Age_x_Percent'] = d['Age'] * d['Percent_at_base']\n",
        "    d['Percent_x_BaseFVC'] = d['Percent_at_base'] * d['Base_FVC']\n",
        "    d['WP_x_BaseFVC'] = d['Weeks_Passed'] * d['Base_FVC']\n",
        "    d['WP_x_Percent'] = d['Weeks_Passed'] * d['Percent_at_base']\n",
        "    d['WP_x_Age'] = d['Weeks_Passed'] * d['Age']\n",
        "    # Trend placeholders (should be zero for val/test hygiene)\n",
        "    for c in ['slope_w','r2_w','slope_percent_w']:\n",
        "        if c not in d.columns: d[c] = 0.0\n",
        "    for c in ['n_obs','has_trend','is_singleton']:\n",
        "        if c not in d.columns: d[c] = 0\n",
        "    # dPercent disabled to avoid leak\n",
        "    d['dPercent'] = 0.0; d['WP_x_dPercent'] = 0.0\n",
        "    return d\n",
        "\n",
        "print('Production setup ready: data loaded and helpers defined.')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Production setup ready: data loaded and helpers defined.\n"
          ]
        }
      ]
    },
    {
      "id": "490636ea-e7b7-4864-b832-4f5bf9d3d88d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Slope head: build forward-safe slope labels per fold and fit Ridge + KNN on baseline-only features\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def patient_forward_slope(g):\n",
        "    # Simple OLS slope on all points of this patient's TRAIN partition\n",
        "    x = g['Weeks'].values.astype(float); y = g['FVC'].values.astype(float)\n",
        "    xm = x.mean(); ym = y.mean()\n",
        "    denom = ((x - xm)**2).sum()\n",
        "    return ((x - xm) * (y - ym)).sum() / denom if denom > 0 else 0.0\n",
        "\n",
        "def prepare_baseline_table(df):\n",
        "    # Earliest visit row per patient to extract baseline features\n",
        "    base = (df.sort_values(['Patient','Weeks']).groupby('Patient', as_index=False).first())\n",
        "    base = base[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "        columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "    return base\n",
        "\n",
        "def build_slope_features(base_df, ecdf_basefvc=None, ecdf_percent=None, cats=None, fit=False):\n",
        "    b = base_df.copy()\n",
        "    b['log_Base_FVC'] = np.log1p(np.maximum(b['Base_FVC'].astype(float), 1.0))\n",
        "    b['BaseFVC_over_Age'] = b['Base_FVC'].astype(float) / np.maximum(b['Age'].astype(float), 1.0)\n",
        "    b['PercentBase_over_Age'] = b['Percent_at_base'].astype(float) / np.maximum(b['Age'].astype(float), 1.0)\n",
        "    # ECDF ranks\n",
        "    if fit:\n",
        "        ecdf_basefvc = ecdf_rank_fit(b['Base_FVC'].values)\n",
        "        ecdf_percent = ecdf_rank_fit(b['Percent_at_base'].values)\n",
        "    b['BaseFVC_ecdf'] = ecdf_rank_transform(b['Base_FVC'].values, ecdf_basefvc)\n",
        "    b['Percent_ecdf'] = ecdf_rank_transform(b['Percent_at_base'].values, ecdf_percent)\n",
        "    # One-hot\n",
        "    if fit:\n",
        "        cats = one_hot_fit(b, ['Sex','SmokingStatus'])\n",
        "    b = one_hot_transform(b, cats)\n",
        "    num_cols = ['Age','Base_FVC','log_Base_FVC','Percent_at_base','BaseFVC_over_Age','PercentBase_over_Age','BaseFVC_ecdf','Percent_ecdf']\n",
        "    cat_cols = [c for c in b.columns if c.startswith('Sex__') or c.startswith('SmokingStatus__')]\n",
        "    feat_cols = num_cols + cat_cols\n",
        "    return b, feat_cols, ecdf_basefvc, ecdf_percent, cats\n",
        "\n",
        "def run_slope_head_forward_cv(n_splits=5, seed=42):\n",
        "    gkf = GroupKFold(n_splits=n_splits)\n",
        "    groups = train['Patient'].values\n",
        "    # OOF containers aligned to per-row grid of future-only scoring\n",
        "    y_true_all, dist_all = [], []\n",
        "    # We'll store FVC predictions from slope models per validation patient-week (future only)\n",
        "    fvc_ridge_all, fvc_knn_all = [], []\n",
        "    t0 = time.time()\n",
        "    for fold, (trn_idx, val_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "        tf = time.time()\n",
        "        trn = train.iloc[trn_idx].copy(); val = train.iloc[val_idx].copy()\n",
        "        # Build per-patient forward slope labels on TRAIN only\n",
        "        slopes = []\n",
        "        for pid, g in trn.groupby('Patient'):\n",
        "            slopes.append((pid, patient_forward_slope(g)))\n",
        "        slope_labels = pd.DataFrame(slopes, columns=['Patient','s_label'])\n",
        "        # Baseline tables\n",
        "        base_trn = prepare_baseline_table(trn)\n",
        "        base_val = prepare_baseline_table(val)\n",
        "        base_trn = base_trn.merge(slope_labels, on='Patient', how='left')\n",
        "        # Fit baseline-only feature transforms on TRAIN baseline\n",
        "        base_trnF, feat_cols, ecdf_bf, ecdf_pc, cats_full = build_slope_features(base_trn, fit=True)\n",
        "        base_valF, _, _, _, _ = build_slope_features(base_val, ecdf_bf, ecdf_pc, cats_full, fit=False)\n",
        "        # Standardize numeric space for KNN; Ridge will handle via same scaler\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "        X_trn = base_trnF[feat_cols].values.astype(float); y_trn = base_trnF['s_label'].fillna(0.0).values.astype(float)\n",
        "        X_trn_std = scaler.fit_transform(X_trn)\n",
        "        X_val_std = scaler.transform(base_valF[feat_cols].values.astype(float))\n",
        "        # Models\n",
        "        ridge = Ridge(alpha=1.0, random_state=seed)\n",
        "        ridge.fit(X_trn_std, y_trn)\n",
        "        knn = KNeighborsRegressor(n_neighbors=9, weights='distance', metric='euclidean')\n",
        "        knn.fit(X_trn_std, y_trn)\n",
        "        # Predict s_hat on VAL baseline, then expand to patient-week rows (future only) for scoring\n",
        "        s_ridge = ridge.predict(X_val_std)\n",
        "        s_knn = knn.predict(X_val_std)\n",
        "        # Merge s_hat back to val rows\n",
        "        val = val.merge(base_val[['Patient','Base_Week','Base_FVC']], on='Patient', how='left')\n",
        "        val = val.merge(base_valF[['Patient']], on='Patient', how='left')\n",
        "        # Future-only mask\n",
        "        mask = (val['Weeks'].values >= val['Base_Week'].values)\n",
        "        dist = (val['Weeks'].values - val['Base_Week'].values).astype(float)\n",
        "        # Map patient -> s_hat\n",
        "        map_ridge = dict(zip(base_val['Patient'].values, s_ridge))\n",
        "        map_knn = dict(zip(base_val['Patient'].values, s_knn))\n",
        "        s_r = val['Patient'].map(map_ridge).astype(float).fillna(0.0).values\n",
        "        s_k = val['Patient'].map(map_knn).astype(float).fillna(0.0).values\n",
        "        fvc_r = (val['Base_FVC'].values + s_r * dist).astype(float)\n",
        "        fvc_k = (val['Base_FVC'].values + s_k * dist).astype(float)\n",
        "        # Append future-only rows\n",
        "        y_true_all.append(val['FVC'].values[mask].astype(float))\n",
        "        dist_all.append(dist[mask].astype(float))\n",
        "        fvc_ridge_all.append(fvc_r[mask].astype(float))\n",
        "        fvc_knn_all.append(fvc_k[mask].astype(float))\n",
        "        print(f'[Slope-Fold {fold}] n_pat_trn={base_trn.shape[0]} ridge_fitted, knn_fitted; elapsed={time.time()-tf:.2f}s', flush=True)\n",
        "        del trn, val, base_trn, base_val, base_trnF, base_valF, X_trn, X_trn_std, X_val_std\n",
        "        gc.collect()\n",
        "    y_true = np.concatenate(y_true_all) if y_true_all else np.array([], float)\n",
        "    dist = np.concatenate(dist_all) if dist_all else np.array([], float)\n",
        "    fvc_ridge_oof = np.concatenate(fvc_ridge_all) if fvc_ridge_all else np.array([], float)\n",
        "    fvc_knn_oof = np.concatenate(fvc_knn_all) if fvc_knn_all else np.array([], float)\n",
        "    print(f'[Slope OOF] built in {time.time()-t0:.2f}s; arrays: y={y_true.shape}, ridge={fvc_ridge_oof.shape}, knn={fvc_knn_oof.shape}', flush=True)\n",
        "    return dict(y_true=y_true, dist=dist, fvc_ridge_oof=fvc_ridge_oof, fvc_knn_oof=fvc_knn_oof)\n",
        "\n",
        "def fit_slope_head_full_and_predict_test():\n",
        "    # Train slope models on full train baseline table and predict test grid\n",
        "    base_full = prepare_baseline_table(train)\n",
        "    # Fit labels from full train per-patient slope (for logging only; not used for test prediction except model fit)\n",
        "    slopes_full = compute_patient_slopes(train)\n",
        "    slope_labels_full = pd.DataFrame({'Patient': list(slopes_full.keys()), 's_label': list(slopes_full.values())})\n",
        "    base_full = base_full.merge(slope_labels_full, on='Patient', how='left')\n",
        "    base_fullF, feat_cols, ecdf_bf, ecdf_pc, cats_full = build_slope_features(base_full, fit=True)\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_full = base_fullF[feat_cols].values.astype(float); y_full = base_fullF['s_label'].fillna(0.0).values.astype(float)\n",
        "    X_full_std = scaler.fit_transform(X_full)\n",
        "    ridge = Ridge(alpha=1.0, random_state=42).fit(X_full_std, y_full)\n",
        "    knn = KNeighborsRegressor(n_neighbors=9, weights='distance', metric='euclidean').fit(X_full_std, y_full)\n",
        "    # Build strict test grid baseline\n",
        "    grid = ss.copy()\n",
        "    parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "    grid['Patient'] = parts[0]; grid['Weeks'] = parts[1].astype(int)\n",
        "    test_bl = test[['Patient','Weeks','FVC','Percent']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "    grid = grid.merge(test_bl, on='Patient', how='left')\n",
        "    base_test = grid[['Patient','Base_Week','Base_FVC']].drop_duplicates('Patient')\n",
        "    # Build baseline features for test patients\n",
        "    meta = test[['Patient','Percent','Age','Sex','SmokingStatus']].drop_duplicates('Patient').rename(columns={'Percent':'Percent_at_base'})\n",
        "    base_test = base_test.merge(meta, on='Patient', how='left')\n",
        "    base_testF, _, _, _, _ = build_slope_features(base_test, ecdf_bf, ecdf_pc, cats_full, fit=False)\n",
        "    X_test_std = scaler.transform(base_testF[feat_cols].values.astype(float))\n",
        "    s_ridge_test = ridge.predict(X_test_std)\n",
        "    s_knn_test = knn.predict(X_test_std)\n",
        "    # Map back to full grid\n",
        "    map_r = dict(zip(base_testF['Patient'].values, s_ridge_test))\n",
        "    map_k = dict(zip(base_testF['Patient'].values, s_knn_test))\n",
        "    dist_test = (grid['Weeks'].values - grid['Base_Week'].values).astype(float)\n",
        "    fvc_ridge_test = (grid['Base_FVC'].values + pd.Series(grid['Patient']).map(map_r).astype(float).fillna(0.0).values * dist_test).astype(float)\n",
        "    fvc_knn_test = (grid['Base_FVC'].values + pd.Series(grid['Patient']).map(map_k).astype(float).fillna(0.0).values * dist_test).astype(float)\n",
        "    return dict(grid=grid, fvc_ridge_test=fvc_ridge_test, fvc_knn_test=fvc_knn_test, dist_test=dist_test)\n",
        "\n",
        "# Execute slope head\n",
        "slope_oof = run_slope_head_forward_cv(n_splits=5, seed=42)\n",
        "slope_test = fit_slope_head_full_and_predict_test()\n",
        "print('Slope head ready: OOF and test predictions computed (Ridge + KNN).')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Slope-Fold 1] n_pat_trn=126 ridge_fitted, knn_fitted; elapsed=0.02s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Slope-Fold 2] n_pat_trn=126 ridge_fitted, knn_fitted; elapsed=0.02s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Slope-Fold 3] n_pat_trn=127 ridge_fitted, knn_fitted; elapsed=0.02s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Slope-Fold 4] n_pat_trn=127 ridge_fitted, knn_fitted; elapsed=0.02s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Slope-Fold 5] n_pat_trn=126 ridge_fitted, knn_fitted; elapsed=0.02s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Slope OOF] built in 0.33s; arrays: y=(1394,), ridge=(1394,), knn=(1394,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope head ready: OOF and test predictions computed (Ridge + KNN).\n"
          ]
        }
      ]
    },
    {
      "id": "705defd0-1f64-4553-b1c3-746bf3c4b3e5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Blend Ridge + KNN slope heads, create slope+anchor submission with banker sigma and guardrails\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "# OOF blend weight scan for slope\n",
        "y_true = slope_oof['y_true']\n",
        "dist_oof = slope_oof['dist']\n",
        "fvc_ridge_oof = slope_oof['fvc_ridge_oof']\n",
        "fvc_knn_oof = slope_oof['fvc_knn_oof']\n",
        "\n",
        "def laplace_ll(y_true, y_pred, sigma):\n",
        "    y_true = np.asarray(y_true).astype(float)\n",
        "    y_pred = np.asarray(y_pred).astype(float)\n",
        "    sigma = np.asarray(sigma).astype(float)\n",
        "    delta = np.minimum(np.abs(y_true - y_pred), 1000.0)\n",
        "    sigma = np.maximum(sigma, 70.0)\n",
        "    return float(np.mean(-delta / sigma - np.log(sigma)))\n",
        "\n",
        "# Use banker-like sigma for OOF selection (robust, independent of model residuals)\n",
        "sigma_oof = np.maximum(240.0 + 3.0 * dist_oof, 70.0)\n",
        "best = (-1e9, None)\n",
        "for w in [0.3,0.4,0.5,0.6,0.7,0.8]:\n",
        "    fvc_bl = w * fvc_ridge_oof + (1.0 - w) * fvc_knn_oof\n",
        "    sc = laplace_ll(y_true, fvc_bl, sigma_oof)\n",
        "    if sc > best[0]:\n",
        "        best = (sc, w)\n",
        "w_ridge = best[1] if best[1] is not None else 0.6\n",
        "w_knn = 1.0 - w_ridge\n",
        "print(f'[Slope OOF] best blend weights: ridge={w_ridge:.2f}, knn={w_knn:.2f}, LL={best[0]:.5f}')\n",
        "\n",
        "# Build test slope blend and final FVC with anchor=0.15 (slope bucket 0.85) as a standalone probe\n",
        "grid = slope_test['grid'].copy()\n",
        "dist_test = slope_test['dist_test'].astype(float)\n",
        "fvc_ridge_test = slope_test['fvc_ridge_test'].astype(float)\n",
        "fvc_knn_test = slope_test['fvc_knn_test'].astype(float)\n",
        "slope_blend_test = w_ridge * fvc_ridge_test + w_knn * fvc_knn_test\n",
        "\n",
        "# Anchored baseline (global slope from train)\n",
        "gs = robust_global_slope(compute_patient_slopes(train))\n",
        "fvc_anchor = (grid['Base_FVC'].values + gs * (grid['Weeks'].values - grid['Base_Week'].values)).astype(float)\n",
        "\n",
        "# Final FVC for this probe: 0.85 slope + 0.15 anchor\n",
        "fvc_probe = 0.85 * slope_blend_test + 0.15 * fvc_anchor\n",
        "\n",
        "# Guardrails: non-increasing FVC per patient and clip\n",
        "df_out = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'FVC': np.clip(fvc_probe, 500, 6000)})\n",
        "def enforce_non_increasing(g):\n",
        "    g = g.sort_values('Weeks').copy()\n",
        "    g['FVC'] = np.minimum.accumulate(g['FVC'].values[::-1])[::-1]\n",
        "    return g\n",
        "df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(enforce_non_increasing)\n",
        "fvc_final = df_out['FVC'].values.astype(float)\n",
        "\n",
        "# Sigma banker + guardrails (monotone, floors incl. dist>20>=100)\n",
        "sigma = np.maximum(240.0 + 3.0 * np.abs(dist_test), 70.0).astype(float)\n",
        "sigma = np.where(np.abs(dist_test) > 20.0, np.maximum(sigma, 100.0), sigma)\n",
        "df_sig = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': np.abs(dist_test).astype(float), 'Sigma': sigma})\n",
        "def enforce_sigma_monotone(g):\n",
        "    g = g.sort_values('dist').copy()\n",
        "    g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "    return g\n",
        "df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n",
        "sigma_final = df_sig['Sigma'].values.astype(float)\n",
        "\n",
        "# Save slope+anchor banker submission as an additional artifact; do not overwrite current submission.csv automatically\n",
        "submission_slope_banker = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_final, 'Confidence': sigma_final})\n",
        "submission_slope_banker.to_csv('submission_slope_banker.csv', index=False)\n",
        "print('Saved submission_slope_banker.csv (0.85 slope blend + 0.15 anchor, banker sigma).')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Slope OOF] best blend weights: ridge=0.80, knn=0.20, LL=-6.15768\nSaved submission_slope_banker.csv (0.85 slope blend + 0.15 anchor, banker sigma).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/2959877120.py:50: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(enforce_non_increasing)\n/tmp/ipykernel_4688/2959877120.py:61: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n"
          ]
        }
      ]
    },
    {
      "id": "ea561f9a-d61d-45dc-9801-9993abcfbec5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Integrate slope backbone with a light residual XGB, sweep weights, and build banker/primary submissions\n",
        "import numpy as np, pandas as pd, time, gc, math\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "# Helper: build forward CV grids with baseline columns merged\n",
        "def build_fold_grids(trn_df, val_df):\n",
        "    # Baselines from earliest visit within each split\n",
        "    base_trn = (trn_df.sort_values(['Patient','Weeks']).groupby('Patient', as_index=False).first())\n",
        "    base_trn = base_trn[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "    base_val = (val_df.sort_values(['Patient','Weeks']).groupby('Patient', as_index=False).first())\n",
        "    base_val = base_val[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "    # Drop potential duplicates to avoid suffixes; keep baseline versions\n",
        "    trn_df = trn_df.drop(columns=['Age','Sex','SmokingStatus'], errors='ignore')\n",
        "    val_df = val_df.drop(columns=['Age','Sex','SmokingStatus'], errors='ignore')\n",
        "    trn = trn_df.merge(base_trn, on='Patient', how='left')\n",
        "    val = val_df.merge(base_val, on='Patient', how='left')\n",
        "    # Guardrails\n",
        "    assert trn['Base_Week'].notna().all() and val['Base_Week'].notna().all(), 'Base_Week has NaNs in fold build'\n",
        "    # future-only masks and dist\n",
        "    trn['dist'] = (trn['Weeks'] - trn['Base_Week']).astype(float)\n",
        "    val['dist'] = (val['Weeks'] - val['Base_Week']).astype(float)\n",
        "    m_trn = trn['dist'] >= 0\n",
        "    m_val = val['dist'] >= 0\n",
        "    return trn, val, m_trn.values, m_val.values, base_trn, base_val\n",
        "\n",
        "# Compute global slope from a dataframe\n",
        "def global_slope_from_df(df):\n",
        "    slopes = compute_patient_slopes(df)\n",
        "    return robust_global_slope(slopes)\n",
        "\n",
        "# Prepare features for residual model using leak-safe builder (baseline-only + distance bases)\n",
        "def prepare_residual_features(df):\n",
        "    d = df.copy()\n",
        "    # map possible suffixed baseline columns back to expected names\n",
        "    for name, alts in [('Age',['Age_x','Age_y','Age_base']), ('Sex',['Sex_x','Sex_y','Sex_base']), ('SmokingStatus',['SmokingStatus_x','SmokingStatus_y','Smoking_base'])]:\n",
        "        if name not in d.columns:\n",
        "            for a in alts:\n",
        "                if a in d.columns:\n",
        "                    d[name] = d[a]\n",
        "                    break\n",
        "    # ensure required cols exist\n",
        "    for c in ['Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus','Weeks','dist']:\n",
        "        assert c in d.columns, f'missing {c}'\n",
        "    # minimal columns for build_features_v2\n",
        "    d['Weeks_Passed'] = d['dist']\n",
        "    d['Abs_Weeks_Passed'] = d['dist'].abs()\n",
        "    d['Weeks_Passed2'] = d['dist']**2\n",
        "    d['Weeks_Passed_cap'] = d['dist'].clip(-26, 26)\n",
        "    d['sign_WP'] = np.sign(d['dist']).astype(int)\n",
        "    d['is_future'] = (d['dist'] >= 0).astype(int)\n",
        "    # percent and base transforms inside build_features_v2 expect certain cols already present\n",
        "    d = build_features_v2(d)\n",
        "    # distance bases and interactions (safe)\n",
        "    d['dist2'] = d['dist']**2\n",
        "    d['dist3'] = d['dist']**3\n",
        "    d['dist_cap'] = d['dist'].clip(0, 30)\n",
        "    d['dist_x_BaseFVC'] = d['dist'] * d['Base_FVC']\n",
        "    d['dist_x_Age'] = d['dist'] * d['Age']\n",
        "    # one-hot for Sex, SmokingStatus on TRAIN fit, applied to VAL/TEST later outside this fn\n",
        "    return d\n",
        "\n",
        "def fit_ohe(train_df, cols):\n",
        "    return one_hot_fit(train_df, cols)\n",
        "\n",
        "def apply_ohe(df, cats):\n",
        "    return one_hot_transform(df, cats)\n",
        "\n",
        "# Retrieve OOF arrays and create combined predictions aligned per-fold\n",
        "y_true_oof_list, dist_oof_list = [], []\n",
        "fvc_slope_oof_list, fvc_resid_oof_list, fvc_anchor_oof_list = [], [], []\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "groups = train['Patient'].values\n",
        "\n",
        "# To rebuild per-fold anchor and slope aligned arrays, recompute slope preds within the loop using the already-fitted slope_head logic per fold.\n",
        "ridge_w, knn_w = 0.80, 0.20\n",
        "bag_seeds = [0,1,2,3,4]  # residual XGB seed bag\n",
        "\n",
        "fold_start = time.time()\n",
        "for fold, (trn_idx, val_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    t_fold = time.time()\n",
        "    trn_df = train.iloc[trn_idx].copy()\n",
        "    val_df = train.iloc[val_idx].copy()\n",
        "    trn, val, m_trn, m_val, base_trn, base_val = build_fold_grids(trn_df, val_df)\n",
        "    # Fit slope label model on TRAIN baseline (Ridge+KNN) to get s_hat for VAL baseline\n",
        "    slopes_tr = compute_patient_slopes(trn_df)\n",
        "    base_trn_lab = base_trn.merge(pd.DataFrame({'Patient': list(slopes_tr.keys()), 's_label': list(slopes_tr.values())}), on='Patient', how='left')\n",
        "    base_trnF, feat_cols, ecdf_bf, ecdf_pc, cats_full = build_slope_features(base_trn_lab, fit=True)\n",
        "    base_valF, _, _, _, _ = build_slope_features(base_val, ecdf_bf, ecdf_pc, cats_full, fit=False)\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_trn = base_trnF[feat_cols].values.astype(float); y_trn = base_trnF['s_label'].fillna(0.0).values.astype(float)\n",
        "    X_trn_std = scaler.fit_transform(X_trn)\n",
        "    X_val_std = scaler.transform(base_valF[feat_cols].values.astype(float))\n",
        "    from sklearn.linear_model import Ridge\n",
        "    from sklearn.neighbors import KNeighborsRegressor\n",
        "    ridge = Ridge(alpha=1.0, random_state=42).fit(X_trn_std, y_trn)\n",
        "    knn = KNeighborsRegressor(n_neighbors=9, weights='distance').fit(X_trn_std, y_trn)\n",
        "    s_r = ridge.predict(X_val_std); s_k = knn.predict(X_val_std)\n",
        "    # Clamp s_hat to [5th,95th] train-fold slope label percentiles\n",
        "    q_lo, q_hi = np.percentile(y_trn, 5), np.percentile(y_trn, 95)\n",
        "    s_blend_val = ridge_w * s_r + knn_w * s_k\n",
        "    s_blend_val = np.clip(s_blend_val, q_lo, q_hi)\n",
        "    s_hat_map = dict(zip(base_val['Patient'].values, s_blend_val))\n",
        "    # Build per-row slope backbone predictions on VAL future rows\n",
        "    dist_val = val['dist'].values.astype(float)\n",
        "    s_hat_val = val['Patient'].map(s_hat_map).astype(float).fillna(0.0).values\n",
        "    fvc_slope_val = (val['Base_FVC'].values + s_hat_val * dist_val).astype(float)\n",
        "    # Compute per-fold anchor using TRAIN-only global slope\n",
        "    gs_fold = global_slope_from_df(trn_df)\n",
        "    fvc_anchor_val = (val['Base_FVC'].values + gs_fold * dist_val).astype(float)\n",
        "    # Residual targets on TRAIN future rows: r = y - fvc_slope\n",
        "    base_trnF2, _, _, _, _ = build_slope_features(base_trn, ecdf_bf, ecdf_pc, cats_full, fit=False)\n",
        "    X_trn_std2 = scaler.transform(base_trnF2[feat_cols].values.astype(float))\n",
        "    s_r_tr = ridge.predict(X_trn_std2); s_k_tr = knn.predict(X_trn_std2)\n",
        "    s_blend_tr = ridge_w * s_r_tr + knn_w * s_k_tr\n",
        "    s_blend_tr = np.clip(s_blend_tr, q_lo, q_hi)\n",
        "    s_hat_map_tr = dict(zip(base_trn['Patient'].values, s_blend_tr))\n",
        "    trn['s_hat'] = trn['Patient'].map(s_hat_map_tr).astype(float).fillna(0.0)\n",
        "    trn['fvc_slope'] = trn['Base_FVC'].astype(float) + trn['s_hat'].astype(float) * trn['dist'].astype(float)\n",
        "    trn_fut = trn[m_trn].copy()\n",
        "    trn_fut['r_target'] = trn_fut['FVC'].astype(float) - trn_fut['fvc_slope'].astype(float)\n",
        "    # Ensure baseline demographics present for feature builder\n",
        "    trn_fut = trn_fut.merge(base_trn[['Patient','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "    # Features for TRAIN and VAL future rows\n",
        "    trn_feat = prepare_residual_features(trn_fut)\n",
        "    val_fut = val[m_val].copy()\n",
        "    # Ensure baseline demographics present for VAL as well\n",
        "    val_fut = val_fut.merge(base_val[['Patient','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "    val_feat = prepare_residual_features(val_fut)\n",
        "    # Fit OHE on TRAIN\n",
        "    cats = fit_ohe(trn_feat, ['Sex','SmokingStatus'])\n",
        "    trn_feat = apply_ohe(trn_feat, cats)\n",
        "    val_feat = apply_ohe(val_feat, cats)\n",
        "    # Select numeric feature columns (exclude objects to avoid strings like 'Male')\n",
        "    drop_cols = ['Patient','Weeks','FVC','Base_Week','s_hat','fvc_slope','r_target','Sex','SmokingStatus']\n",
        "    feat_cols_resid = [c for c in trn_feat.columns if c not in drop_cols and np.issubdtype(trn_feat[c].dtype, np.number)]\n",
        "    Xr = trn_feat[feat_cols_resid].values.astype(float)\n",
        "    yr = trn_feat['r_target'].values.astype(float)\n",
        "    Xv = val_feat[feat_cols_resid].values.astype(float)\n",
        "    # Residual XGB seed bagging (reduced capacity per expert)\n",
        "    r_pred_val_bag = np.zeros(Xv.shape[0], dtype=float)\n",
        "    for sd in bag_seeds:\n",
        "        xgb = XGBRegressor(\n",
        "            n_estimators=500,\n",
        "            max_depth=3,\n",
        "            learning_rate=0.04,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            reg_lambda=5.0,\n",
        "            min_child_weight=5.0,\n",
        "            tree_method='hist',\n",
        "            random_state=sd,\n",
        "            n_jobs=0\n",
        "        )\n",
        "        xgb.fit(Xr, yr, verbose=False)\n",
        "        r_pred_val_bag += xgb.predict(Xv)\n",
        "        del xgb\n",
        "    r_pred_val = r_pred_val_bag / max(len(bag_seeds), 1)\n",
        "    fvc_resid_val = fvc_slope_val[m_val] + r_pred_val\n",
        "    # Accumulate OOF arrays (VAL future rows only)\n",
        "    y_true_oof_list.append(val_fut['FVC'].values.astype(float))\n",
        "    dist_oof_list.append(dist_val[m_val].astype(float))\n",
        "    fvc_slope_oof_list.append(fvc_slope_val[m_val].astype(float))\n",
        "    fvc_resid_oof_list.append(fvc_resid_val.astype(float))\n",
        "    fvc_anchor_oof_list.append(fvc_anchor_val[m_val].astype(float))\n",
        "    print(f'[Integrate-Fold {fold}] trn_fut={trn_fut.shape[0]} val_fut={val_fut.shape[0]} done in {time.time()-t_fold:.2f}s', flush=True)\n",
        "    del trn_df, val_df, trn, val, trn_fut, val_fut, trn_feat, val_feat, Xr, yr, Xv\n",
        "    gc.collect()\n",
        "\n",
        "y_true_oof = np.concatenate(y_true_oof_list)\n",
        "dist_oof = np.concatenate(dist_oof_list)\n",
        "fvc_slope_oof = np.concatenate(fvc_slope_oof_list)\n",
        "fvc_resid_oof = np.concatenate(fvc_resid_oof_list)\n",
        "fvc_anchor_oof = np.concatenate(fvc_anchor_oof_list)\n",
        "print(f'[OOF Arrays] y={y_true_oof.shape}, slope={fvc_slope_oof.shape}, resid={fvc_resid_oof.shape}, anchor={fvc_anchor_oof.shape}')\n",
        "\n",
        "# OOF weight selection: safer reweight toward anchor using banker sigma; choose by long-distance slice first\n",
        "def laplace_ll_np(y_true, y_pred, sigma):\n",
        "    y_true = y_true.astype(float); y_pred = y_pred.astype(float); sigma = sigma.astype(float)\n",
        "    delta = np.minimum(np.abs(y_true - y_pred), 1000.0)\n",
        "    sigma = np.maximum(sigma, 70.0)\n",
        "    return float(np.mean(-delta / sigma - np.log(sigma)))\n",
        "\n",
        "sigma_banker_oof = np.maximum(240.0 + 3.0 * np.abs(dist_oof), 70.0)\n",
        "sigma_banker_oof = np.where(np.abs(dist_oof) > 20.0, np.maximum(sigma_banker_oof, 100.0), sigma_banker_oof)\n",
        "m_long = np.abs(dist_oof) > 20.0\n",
        "best = {'ll_long': -1e9, 'll_global': -1e9, 'w_anchor': None, 'w_resid': None}\n",
        "for w_a in [0.20, 0.25, 0.30]:\n",
        "    w_s = 0.00\n",
        "    w_r = 1.0 - (w_a + w_s)\n",
        "    fvc_bl = w_r * fvc_resid_oof + w_s * fvc_slope_oof + w_a * fvc_anchor_oof\n",
        "    ll_g = laplace_ll_np(y_true_oof, fvc_bl, sigma_banker_oof)\n",
        "    ll_l = laplace_ll_np(y_true_oof[m_long], fvc_bl[m_long], sigma_banker_oof[m_long]) if m_long.any() else ll_g\n",
        "    print(f'[OOF Anchor Sweep] w_anchor={w_a:.2f}, w_resid={w_r:.2f}, LL_global={ll_g:.5f}, LL_long={ll_l:.5f}')\n",
        "    if ll_l > best['ll_long'] or (np.isclose(ll_l, best['ll_long']) and ll_g > best['ll_global']):\n",
        "        best.update({'ll_long': ll_l, 'll_global': ll_g, 'w_anchor': w_a, 'w_resid': w_r})\n",
        "w_anchor_best = best['w_anchor'] if best['w_anchor'] is not None else 0.30\n",
        "w_resid_best = best['w_resid'] if best['w_resid'] is not None else 0.70\n",
        "w_slope_best = 0.00\n",
        "print(f\"[Final Weights] Using w_resid={w_resid_best:.2f}, w_slope={w_slope_best:.2f}, w_anchor={w_anchor_best:.2f} selected by long-distance OOF.\")\n",
        "\n",
        "# Train final residual model on full train future rows with consistent schema\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "# Full-train slope models on baseline-only features\n",
        "def prepare_baseline_table(df):\n",
        "    base = (df.sort_values(['Patient','Weeks']).groupby('Patient', as_index=False).first())\n",
        "    base = base[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "        columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "    return base\n",
        "base_full = prepare_baseline_table(train)\n",
        "slopes_full = compute_patient_slopes(train)\n",
        "slope_labels_full = pd.DataFrame({'Patient': list(slopes_full.keys()), 's_label': list(slopes_full.values())})\n",
        "base_full_lab = base_full.merge(slope_labels_full, on='Patient', how='left')\n",
        "base_fullF, feat_cols_s, ecdf_bf_s, ecdf_pc_s, cats_s = build_slope_features(base_full_lab, fit=True)\n",
        "scaler_full = StandardScaler(with_mean=True, with_std=True)\n",
        "X_full_s = base_fullF[feat_cols_s].values.astype(float); y_full_s = base_fullF['s_label'].fillna(0.0).values.astype(float)\n",
        "X_full_s_std = scaler_full.fit_transform(X_full_s)\n",
        "ridge_full = Ridge(alpha=1.0, random_state=42).fit(X_full_s_std, y_full_s)\n",
        "knn_full = KNeighborsRegressor(n_neighbors=9, weights='distance').fit(X_full_s_std, y_full_s)\n",
        "\n",
        "# Build full-train grid with baseline merge and future-only rows\n",
        "train_full = train.copy()\n",
        "train_full = train_full.merge(base_full[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "train_full['dist'] = (train_full['Weeks'] - train_full['Base_Week']).astype(float)\n",
        "mask_full = train_full['dist'] >= 0\n",
        "Xb_full = build_slope_features(base_full[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], ecdf_bf_s, ecdf_pc_s, cats_s, fit=False)[0]\n",
        "s_r_full = ridge_full.predict(scaler_full.transform(Xb_full[feat_cols_s].values.astype(float)))\n",
        "s_k_full = knn_full.predict(scaler_full.transform(Xb_full[feat_cols_s].values.astype(float)))\n",
        "q_lo_full, q_hi_full = np.percentile(y_full_s, 5), np.percentile(y_full_s, 95)\n",
        "s_blend_full = np.clip(ridge_w * s_r_full + knn_w * s_k_full, q_lo_full, q_hi_full)\n",
        "map_s_full = dict(zip(base_full['Patient'].values, s_blend_full))\n",
        "train_full['s_hat'] = train_full['Patient'].map(map_s_full).astype(float).fillna(0.0)\n",
        "train_full['fvc_slope'] = train_full['Base_FVC'].astype(float) + train_full['s_hat'].astype(float) * train_full['dist'].astype(float)\n",
        "train_fut = train_full[mask_full].copy()\n",
        "train_fut['r_target'] = train_fut['FVC'].astype(float) - train_fut['fvc_slope'].astype(float)\n",
        "# Residual features and OHE schema fit on full train cats\n",
        "tr_full_feat = prepare_residual_features(train_fut)\n",
        "cats_full = one_hot_fit(train[['Sex','SmokingStatus']].drop_duplicates(), ['Sex','SmokingStatus'])\n",
        "tr_full_feat = apply_ohe(tr_full_feat, cats_full)\n",
        "drop_cols_full = ['Patient','Weeks','FVC','Base_Week','s_hat','fvc_slope','r_target','Sex','SmokingStatus']\n",
        "feat_cols_resid_full = [c for c in tr_full_feat.columns if c not in drop_cols_full and np.issubdtype(tr_full_feat[c].dtype, np.number)]\n",
        "X_full_resid = tr_full_feat[feat_cols_resid_full].values.astype(float)\n",
        "y_full_resid = tr_full_feat['r_target'].values.astype(float)\n",
        "# Residual full model: seed bagging (reduced capacity per expert)\n",
        "resid_full_models = []\n",
        "for sd in bag_seeds:\n",
        "    mdl = XGBRegressor(\n",
        "        n_estimators=650,\n",
        "        max_depth=3,\n",
        "        learning_rate=0.04,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_lambda=6.0,\n",
        "        min_child_weight=5.0,\n",
        "        tree_method='hist',\n",
        "        random_state=sd,\n",
        "        n_jobs=0\n",
        "    )\n",
        "    mdl.fit(X_full_resid, y_full_resid, verbose=False)\n",
        "    resid_full_models.append(mdl)\n",
        "\n",
        "# Build test grid and compute slope_backbone, anchor, and residual features for prediction\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]; grid['Weeks'] = parts[1].astype(int)\n",
        "test_bl = test[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_bl, on='Patient', how='left')\n",
        "assert grid['Base_Week'].notna().all(), 'Base_Week has NaNs in test grid merge'\n",
        "dist_test = (grid['Weeks'].values - grid['Base_Week'].values).astype(float)\n",
        "\n",
        "# Slope backbone test from full-fit slope models (with clamp)\n",
        "base_test = grid[['Patient','Base_Week','Base_FVC']].drop_duplicates('Patient')\n",
        "meta = test[['Patient','Percent','Age','Sex','SmokingStatus']].drop_duplicates('Patient').rename(columns={'Percent':'Percent_at_base'})\n",
        "base_test = base_test.merge(meta, on='Patient', how='left')\n",
        "base_testF, _, _, _, _ = build_slope_features(base_test, ecdf_bf_s, ecdf_pc_s, cats_s, fit=False)\n",
        "X_test_std = scaler_full.transform(base_testF[feat_cols_s].values.astype(float))\n",
        "s_ridge_test = ridge_full.predict(X_test_std)\n",
        "s_knn_test = knn_full.predict(X_test_std)\n",
        "s_blend_test = np.clip(ridge_w * s_ridge_test + knn_w * s_knn_test, q_lo_full, q_hi_full)\n",
        "map_s_test = dict(zip(base_testF['Patient'].values, s_blend_test))\n",
        "fvc_slope_test = (grid['Base_FVC'].values + pd.Series(grid['Patient']).map(map_s_test).astype(float).fillna(0.0).values * dist_test).astype(float)\n",
        "\n",
        "# Anchor test using global slope from full train\n",
        "gs_full = global_slope_from_df(train)\n",
        "fvc_anchor_test = (grid['Base_FVC'].values + gs_full * dist_test).astype(float)\n",
        "\n",
        "# Residual features for test rows\n",
        "test_feat = grid.copy()\n",
        "test_feat['dist'] = dist_test\n",
        "test_feat = prepare_residual_features(test_feat)\n",
        "cats_pred = one_hot_fit(train[['Sex','SmokingStatus']].drop_duplicates(), ['Sex','SmokingStatus'])\n",
        "test_feat = apply_ohe(test_feat, cats_pred)\n",
        "drop_cols_pred = ['Patient','Weeks','FVC','Base_Week','Sex','SmokingStatus']\n",
        "# Align test feature columns to training residual feature schema; add missing columns as zeros\n",
        "for c in feat_cols_resid_full:\n",
        "    if c not in test_feat.columns:\n",
        "        test_feat[c] = 0.0\n",
        "feat_cols_pred = [c for c in feat_cols_resid_full if c in test_feat.columns]\n",
        "X_test_resid = test_feat[feat_cols_pred].values.astype(float)\n",
        "# Predict residuals with seed bag and average\n",
        "r_pred_test_sum = np.zeros(X_test_resid.shape[0], dtype=float)\n",
        "for mdl in resid_full_models:\n",
        "    r_pred_test_sum += mdl.predict(X_test_resid)\n",
        "r_pred_test = r_pred_test_sum / max(len(resid_full_models), 1)\n",
        "fvc_resid_test = fvc_slope_test + r_pred_test\n",
        "\n",
        "# Final blended FVC for test using selected safer weights\n",
        "fvc_blend_test = w_resid_best * fvc_resid_test + w_slope_best * fvc_slope_test + w_anchor_best * fvc_anchor_test\n",
        "\n",
        "# Guardrails: pin dist==0 to Base_FVC, non-increasing FVC per patient, clip [500,6000]\n",
        "fvc_clip = np.clip(fvc_blend_test, 500, 6000)\n",
        "fvc_clip = np.where(dist_test == 0.0, grid['Base_FVC'].values.astype(float), fvc_clip)\n",
        "df_out = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'FVC': fvc_clip})\n",
        "def enforce_non_increasing_local(g):\n",
        "    g = g.sort_values('Weeks').copy()\n",
        "    g['FVC'] = np.minimum.accumulate(g['FVC'].values[::-1])[::-1]\n",
        "    return g\n",
        "df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(enforce_non_increasing_local)\n",
        "fvc_final_test = df_out['FVC'].values.astype(float)\n",
        "\n",
        "# Banker sigma for test with guardrails\n",
        "sigma_banker_test = np.maximum(240.0 + 3.0 * np.abs(dist_test), 70.0)\n",
        "sigma_banker_test = np.where(np.abs(dist_test) > 20.0, np.maximum(sigma_banker_test, 100.0), sigma_banker_test)\n",
        "df_sig = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': np.abs(dist_test).astype(float), 'Sigma': sigma_banker_test.astype(float)})\n",
        "def enforce_sigma_monotone_local(g):\n",
        "    g = g.sort_values('dist').copy()\n",
        "    g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "    return g\n",
        "df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone_local)\n",
        "sigma_banker_final_test = df_sig['Sigma'].values.astype(float)\n",
        "\n",
        "submission_banker = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_final_test, 'Confidence': sigma_banker_final_test})\n",
        "submission_banker.to_csv('submission_banker.csv', index=False)\n",
        "print('Saved submission_banker.csv (integrated blend + banker sigma).')\n",
        "\n",
        "# Learned sigma model on OOF residuals of the selected blend\n",
        "r_oof = y_true_oof - (w_resid_best * fvc_resid_oof + w_slope_best * fvc_slope_oof + w_anchor_best * fvc_anchor_oof)\n",
        "y_sigma = np.log1p(np.abs(r_oof))\n",
        "sigma_feat_oof = pd.DataFrame({\n",
        "    'dist': dist_oof,\n",
        "    'dist2': dist_oof**2,\n",
        "})\n",
        "sigma_model = XGBRegressor(\n",
        "    n_estimators=500,\n",
        "    max_depth=3,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=10.0,\n",
        "    tree_method='hist',\n",
        "    random_state=42,\n",
        "    n_jobs=0\n",
        ")\n",
        "sigma_model.fit(sigma_feat_oof.values.astype(float), y_sigma, verbose=False)\n",
        "# Test sigma features\n",
        "sigma_feat_test = pd.DataFrame({'dist': np.abs(dist_test).astype(float), 'dist2': (np.abs(dist_test)**2).astype(float)})\n",
        "sigma_pred = np.expm1(sigma_model.predict(sigma_feat_test.values.astype(float)))\n",
        "sigma_pred = np.maximum(sigma_pred, 70.0)\n",
        "sigma_pred = np.where(np.abs(dist_test) > 20.0, np.maximum(sigma_pred, 100.0), sigma_pred)\n",
        "# Monotone per patient\n",
        "df_sig2 = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': np.abs(dist_test).astype(float), 'Sigma': sigma_pred.astype(float)})\n",
        "df_sig2 = df_sig2.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone_local)\n",
        "sigma_learned_final = df_sig2['Sigma'].values.astype(float)\n",
        "# Floor by banker\n",
        "sigma_primary = np.maximum(sigma_learned_final, sigma_banker_final_test)\n",
        "\n",
        "submission_primary = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_final_test, 'Confidence': sigma_primary})\n",
        "submission_primary.to_csv('submission_primary.csv', index=False)\n",
        "submission_primary.to_csv('submission.csv', index=False)\n",
        "print(f'Saved submission_primary.csv and overwritten submission.csv (integrated blend + learned sigma floored by banker). Elapsed {time.time()-t0:.1f}s')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Integrate-Fold 1] trn_fut=1112 val_fut=282 done in 0.99s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Integrate-Fold 2] trn_fut=1113 val_fut=281 done in 0.99s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Integrate-Fold 3] trn_fut=1119 val_fut=275 done in 1.00s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Integrate-Fold 4] trn_fut=1119 val_fut=275 done in 0.99s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/3333736950.py:328: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(enforce_non_increasing_local)\n/tmp/ipykernel_4688/3333736950.py:339: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone_local)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_banker.csv (integrated blend + banker sigma).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_primary.csv and overwritten submission.csv (integrated blend + learned sigma floored by banker). Elapsed 7.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/3333736950.py:372: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig2 = df_sig2.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone_local)\n"
          ]
        }
      ]
    },
    {
      "id": "484d88e9-3a74-429b-9b57-5a9789734a03",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Linear sigma sweep (sigma = a + b*|dist|) with floors; compare to banker OOF and write alternative submission if >0.02 OOF gain\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "def laplace_ll_np(y_true, y_pred, sigma):\n",
        "    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float); sigma = np.asarray(sigma, float)\n",
        "    delta = np.minimum(np.abs(y_true - y_pred), 1000.0)\n",
        "    sigma = np.clip(sigma, 70.0, 1000.0)\n",
        "    return float(np.mean(-delta / sigma - np.log(sigma)))\n",
        "\n",
        "# Build OOF blend residuals from best weights discovered in Cell 7\n",
        "fvc_oof_best = w_resid_best * fvc_resid_oof + w_slope_best * fvc_slope_oof + w_anchor_best * fvc_anchor_oof\n",
        "r_oof = y_true_oof - fvc_oof_best\n",
        "abs_dist_oof = np.abs(dist_oof).astype(float)\n",
        "\n",
        "# Banker OOF\n",
        "sigma_banker_oof = np.maximum(240.0 + 3.0 * abs_dist_oof, 70.0)\n",
        "sigma_banker_oof = np.where(abs_dist_oof > 20.0, np.maximum(sigma_banker_oof, 100.0), sigma_banker_oof)\n",
        "ll_banker = laplace_ll_np(y_true_oof, fvc_oof_best, sigma_banker_oof)\n",
        "print(f'[Sigma OOF] Banker LL={ll_banker:.5f}')\n",
        "\n",
        "# Sweep linear sigma grids\n",
        "grids = {\n",
        "    'A': {'a': [150, 200, 250, 300], 'b': [2.0, 2.5, 3.0, 3.5]},\n",
        "    'B': {'a': [90, 100, 110, 120], 'b': [1.6, 1.8, 2.0, 2.2, 2.4]}\n",
        "}\n",
        "best = (ll_banker - 1e9, None, None)  # (LL, a, b) initialize far below banker for comparison printing\n",
        "for grid_name, grid in grids.items():\n",
        "    for a in grid['a']:\n",
        "        for b in grid['b']:\n",
        "            sig = a + b * abs_dist_oof\n",
        "            sig = np.maximum(sig, 70.0)\n",
        "            sig = np.where(abs_dist_oof > 20.0, np.maximum(sig, 100.0), sig)\n",
        "            sig = np.clip(sig, 70.0, 1000.0)\n",
        "            ll = laplace_ll_np(y_true_oof, fvc_oof_best, sig)\n",
        "            print(f\"[Sigma OOF] Grid {grid_name} a={a} b={b:.2f} LL={ll:.5f}\")\n",
        "            if ll > best[0]:\n",
        "                best = (ll, a, b)\n",
        "\n",
        "ll_best, a_best, b_best = best\n",
        "delta = ll_best - ll_banker\n",
        "print(f'[Sigma OOF] Best linear: a={a_best} b={b_best:.2f} LL={ll_best:.5f} (\u0394 vs banker={delta:+.5f})')\n",
        "\n",
        "# Build test sigma for best (a,b), enforce floors and per-patient monotone; floor by banker for safety\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]\n",
        "grid['Weeks'] = parts[1].astype(int)\n",
        "test = pd.read_csv('test.csv')\n",
        "test_bl = test[['Patient','Weeks','FVC','Percent']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_bl, on='Patient', how='left')\n",
        "assert grid['Base_Week'].notna().all(), 'Base_Week NaNs in linear sigma grid build'\n",
        "abs_dist_test = (grid['Weeks'] - grid['Base_Week']).abs().astype(float)\n",
        "sigma_linear_test = a_best + b_best * abs_dist_test\n",
        "sigma_linear_test = np.maximum(sigma_linear_test, 70.0)\n",
        "sigma_linear_test = np.where(abs_dist_test > 20.0, np.maximum(sigma_linear_test, 100.0), sigma_linear_test)\n",
        "sigma_linear_test = np.clip(sigma_linear_test, 70.0, 1000.0)\n",
        "\n",
        "# Per-patient monotone on |dist|\n",
        "df_sigL = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': abs_dist_test.values, 'Sigma': sigma_linear_test.astype(float)})\n",
        "def enforce_sigma_monotone(g):\n",
        "    g = g.sort_values('dist').copy()\n",
        "    g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "    return g\n",
        "df_sigL = df_sigL.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n",
        "sigma_linear_test_mono = df_sigL['Sigma'].values.astype(float)\n",
        "\n",
        "# Load current best FVC (from integrated blend built in Cell 7), align to ss order\n",
        "sub_int = pd.read_csv('submission_banker.csv')\n",
        "sub_int = sub_int.set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "fvc_test_final = sub_int['FVC'].astype(float).values\n",
        "\n",
        "# Banker test sigma for flooring\n",
        "sigma_banker_test = np.maximum(240.0 + 3.0 * abs_dist_test, 70.0)\n",
        "sigma_banker_test = np.where(abs_dist_test > 20.0, np.maximum(sigma_banker_test, 100.0), sigma_banker_test)\n",
        "\n",
        "# Final linear-floored sigma\n",
        "sigma_linear_floored = np.maximum(sigma_linear_test_mono, sigma_banker_test)\n",
        "\n",
        "sub_linear = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_test_final, 'Confidence': sigma_linear_floored})\n",
        "sub_linear.to_csv('submission_linear_floored.csv', index=False)\n",
        "print('Saved submission_linear_floored.csv (linear sigma floored by banker).')\n",
        "\n",
        "# Overwrite submission.csv only if OOF gain > 0.02 over banker\n",
        "if delta > 0.02:\n",
        "    sub_linear.to_csv('submission.csv', index=False)\n",
        "    print('[Sigma OOF] Linear sigma beats banker by >0.02 on OOF; submission.csv set to submission_linear_floored.csv')\n",
        "else:\n",
        "    print('[Sigma OOF] Linear sigma NOT >0.02 over banker; keeping banker as primary submission.csv')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sigma OOF] Banker LL=-5.89595\n[Sigma OOF] Grid A a=150 b=2.00 LL=-5.57238\n[Sigma OOF] Grid A a=150 b=2.50 LL=-5.59623\n[Sigma OOF] Grid A a=150 b=3.00 LL=-5.61948\n[Sigma OOF] Grid A a=150 b=3.50 LL=-5.64211\n[Sigma OOF] Grid A a=200 b=2.00 LL=-5.73874\n[Sigma OOF] Grid A a=200 b=2.50 LL=-5.76042\n[Sigma OOF] Grid A a=200 b=3.00 LL=-5.78147\n[Sigma OOF] Grid A a=200 b=3.50 LL=-5.80192\n[Sigma OOF] Grid A a=250 b=2.00 LL=-5.88414\n[Sigma OOF] Grid A a=250 b=2.50 LL=-5.90372\n[Sigma OOF] Grid A a=250 b=3.00 LL=-5.92274\n[Sigma OOF] Grid A a=250 b=3.50 LL=-5.94122\n[Sigma OOF] Grid A a=300 b=2.00 LL=-6.01223\n[Sigma OOF] Grid A a=300 b=2.50 LL=-6.02999\n[Sigma OOF] Grid A a=300 b=3.00 LL=-6.04726\n[Sigma OOF] Grid A a=300 b=3.50 LL=-6.06406\n[Sigma OOF] Grid B a=90 b=1.60 LL=-5.32675\n[Sigma OOF] Grid B a=90 b=1.80 LL=-5.33634\n[Sigma OOF] Grid B a=90 b=2.00 LL=-5.34608\n[Sigma OOF] Grid B a=90 b=2.20 LL=-5.35592\n[Sigma OOF] Grid B a=90 b=2.40 LL=-5.36582\n[Sigma OOF] Grid B a=100 b=1.60 LL=-5.36496\n[Sigma OOF] Grid B a=100 b=1.80 LL=-5.37483\n[Sigma OOF] Grid B a=100 b=2.00 LL=-5.38478\n[Sigma OOF] Grid B a=100 b=2.20 LL=-5.39476\n[Sigma OOF] Grid B a=100 b=2.40 LL=-5.40476\n[Sigma OOF] Grid B a=110 b=1.60 LL=-5.40348\n[Sigma OOF] Grid B a=110 b=1.80 LL=-5.41347\n[Sigma OOF] Grid B a=110 b=2.00 LL=-5.42350\n[Sigma OOF] Grid B a=110 b=2.20 LL=-5.43352\n[Sigma OOF] Grid B a=110 b=2.40 LL=-5.44351\n[Sigma OOF] Grid B a=120 b=1.60 LL=-5.44178\n[Sigma OOF] Grid B a=120 b=1.80 LL=-5.45181\n[Sigma OOF] Grid B a=120 b=2.00 LL=-5.46182\n[Sigma OOF] Grid B a=120 b=2.20 LL=-5.47179\n[Sigma OOF] Grid B a=120 b=2.40 LL=-5.48172\n[Sigma OOF] Best linear: a=90 b=1.60 LL=-5.32675 (\u0394 vs banker=+0.56920)\nSaved submission_linear_floored.csv (linear sigma floored by banker).\n[Sigma OOF] Linear sigma beats banker by >0.02 on OOF; submission.csv set to submission_linear_floored.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/230660649.py:64: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sigL = df_sigL.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n"
          ]
        }
      ]
    },
    {
      "id": "d59035d2-2283-4b7d-98d4-3f0b91d32bec",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Conservative linear sigma sweep near banker; select on |dist|>20 OOF and floor by banker\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "def laplace_ll_np(y_true, y_pred, sigma):\n",
        "    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float); sigma = np.asarray(sigma, float)\n",
        "    delta = np.minimum(np.abs(y_true - y_pred), 1000.0)\n",
        "    sigma = np.clip(sigma, 70.0, 1000.0)\n",
        "    return float(np.mean(-delta / sigma - np.log(sigma)))\n",
        "\n",
        "# Use OOF arrays from Cell 7\n",
        "fvc_oof_best = w_resid_best * fvc_resid_oof + w_slope_best * fvc_slope_oof + w_anchor_best * fvc_anchor_oof\n",
        "abs_dist_oof = np.abs(dist_oof).astype(float)\n",
        "\n",
        "# Banker OOF sigma with floors\n",
        "sigma_banker_oof = np.maximum(240.0 + 3.0 * abs_dist_oof, 70.0)\n",
        "sigma_banker_oof = np.where(abs_dist_oof > 20.0, np.maximum(sigma_banker_oof, 100.0), sigma_banker_oof)\n",
        "ll_banker_global = laplace_ll_np(y_true_oof, fvc_oof_best, sigma_banker_oof)\n",
        "m_long = abs_dist_oof > 20.0\n",
        "ll_banker_long = laplace_ll_np(y_true_oof[m_long], fvc_oof_best[m_long], sigma_banker_oof[m_long]) if m_long.any() else ll_banker_global\n",
        "print(f'[Cons Sigma] Banker LL global={ll_banker_global:.5f} | long>20={ll_banker_long:.5f}')\n",
        "\n",
        "# Conservative grid near banker\n",
        "a_grid = [220, 240, 260]\n",
        "b_grid = [2.6, 3.0, 3.4]\n",
        "best = {'ll_long': -1e9, 'll_global': -1e9, 'a': None, 'b': None, 'diff_frac': 0.0}\n",
        "for a in a_grid:\n",
        "    for b in b_grid:\n",
        "        sig_lin = a + b * abs_dist_oof\n",
        "        sig_lin = np.maximum(sig_lin, 70.0)\n",
        "        sig_lin = np.where(abs_dist_oof > 20.0, np.maximum(sig_lin, 100.0), sig_lin)\n",
        "        # Floor by banker for OOF selection to match test-time parity\n",
        "        sig_oof = np.maximum(sig_lin, sigma_banker_oof)\n",
        "        ll_g = laplace_ll_np(y_true_oof, fvc_oof_best, sig_oof)\n",
        "        ll_l = laplace_ll_np(y_true_oof[m_long], fvc_oof_best[m_long], sig_oof[m_long]) if m_long.any() else ll_g\n",
        "        diff_frac = float(np.mean((sig_oof - sigma_banker_oof) > 1e-6))\n",
        "        print(f'[Cons Sigma] a={a} b={b:.2f} LL_global={ll_g:.5f} LL_long={ll_l:.5f} changed_frac={diff_frac:.3f}')\n",
        "        if ll_l > best['ll_long'] or (np.isclose(ll_l, best['ll_long']) and ll_g > best['ll_global']):\n",
        "            best.update({'ll_long': ll_l, 'll_global': ll_g, 'a': a, 'b': b, 'diff_frac': diff_frac})\n",
        "\n",
        "print(f\"[Cons Sigma] Best by long slice: a={best['a']} b={best['b']:.2f} LL_global={best['ll_global']:.5f} (\u0394={best['ll_global']-ll_banker_global:+.5f}) LL_long={best['ll_long']:.5f} (\u0394={best['ll_long']-ll_banker_long:+.5f}) changed_frac={best['diff_frac']:.3f}\")\n",
        "\n",
        "# Also report OOF LL by distance bins for banker vs best candidate\n",
        "bins = [(0.0, 5.0), (5.0, 15.0), (15.0, 1e9)]\n",
        "sig_best_oof = None\n",
        "if best['a'] is not None:\n",
        "    sig_lin = best['a'] + best['b'] * abs_dist_oof\n",
        "    sig_lin = np.maximum(sig_lin, 70.0)\n",
        "    sig_lin = np.where(abs_dist_oof > 20.0, np.maximum(sig_lin, 100.0), sig_lin)\n",
        "    sig_best_oof = np.maximum(sig_lin, sigma_banker_oof)\n",
        "for lo, hi in bins:\n",
        "    m = (abs_dist_oof > lo) & (abs_dist_oof <= hi)\n",
        "    if not m.any():\n",
        "        print(f'[Cons Sigma] Bin ({lo},{hi}] empty'); continue\n",
        "    ll_b = laplace_ll_np(y_true_oof[m], fvc_oof_best[m], sigma_banker_oof[m])\n",
        "    if sig_best_oof is not None:\n",
        "        ll_c = laplace_ll_np(y_true_oof[m], fvc_oof_best[m], sig_best_oof[m])\n",
        "        print(f'[Cons Sigma] Bin ({lo},{hi}] banker={ll_b:.5f} best={ll_c:.5f} \u0394={ll_c-ll_b:+.5f}')\n",
        "    else:\n",
        "        print(f'[Cons Sigma] Bin ({lo},{hi}] banker={ll_b:.5f}')\n",
        "\n",
        "# Build test sigma for best (a,b), enforce monotone and banker floor, and optionally set submission.csv\n",
        "use_cons = False\n",
        "if best['a'] is not None:\n",
        "    gain_global = best['ll_global'] - ll_banker_global\n",
        "    gain_long = best['ll_long'] - ll_banker_long\n",
        "    use_cons = (gain_long > 0.0) and (gain_global > 0.02)\n",
        "    print(f'[Cons Sigma] adopt? {use_cons} (global \u0394={gain_global:+.5f}, long \u0394={gain_long:+.5f})')\n",
        "\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]\n",
        "grid['Weeks'] = parts[1].astype(int)\n",
        "test = pd.read_csv('test.csv')\n",
        "test_bl = test[['Patient','Weeks','FVC','Percent']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_bl, on='Patient', how='left')\n",
        "assert grid['Base_Week'].notna().all(), 'Base_Week NaNs in conservative sigma build'\n",
        "abs_dist_test = (grid['Weeks'] - grid['Base_Week']).abs().astype(float)\n",
        "a_best, b_best = best['a'], best['b']\n",
        "if a_best is None:\n",
        "    a_best, b_best = 240.0, 3.0\n",
        "sigma_lin_test = a_best + b_best * abs_dist_test\n",
        "sigma_lin_test = np.maximum(sigma_lin_test, 70.0)\n",
        "sigma_lin_test = np.where(abs_dist_test > 20.0, np.maximum(sigma_lin_test, 100.0), sigma_lin_test)\n",
        "df_sig = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': abs_dist_test.values, 'Sigma': sigma_lin_test.astype(float)})\n",
        "def enforce_sigma_monotone(g):\n",
        "    g = g.sort_values('dist').copy()\n",
        "    g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "    return g\n",
        "df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n",
        "sigma_lin_mono = df_sig['Sigma'].values.astype(float)\n",
        "# Banker floor on test\n",
        "sigma_banker_test = np.maximum(240.0 + 3.0 * abs_dist_test, 70.0)\n",
        "sigma_banker_test = np.where(abs_dist_test > 20.0, np.maximum(sigma_banker_test, 100.0), sigma_banker_test)\n",
        "sigma_cons_test = np.maximum(sigma_lin_mono, sigma_banker_test)\n",
        "\n",
        "# Load FVC from integrated banker submission (same FVC across sigma variants)\n",
        "sub_b = pd.read_csv('submission_banker.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "sub_cons = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': sub_b['FVC'].astype(float).values, 'Confidence': sigma_cons_test})\n",
        "sub_cons.to_csv('submission_linear_floored_conservative.csv', index=False)\n",
        "print('Saved submission_linear_floored_conservative.csv (conservative linear sigma floored by banker).')\n",
        "\n",
        "if use_cons:\n",
        "    sub_cons.to_csv('submission.csv', index=False)\n",
        "    print('[Cons Sigma] Adopted conservative linear sigma as primary (submission.csv).')\n",
        "else:\n",
        "    print('[Cons Sigma] Keeping banker as primary (submission.csv unchanged).')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Cons Sigma] Banker LL global=-5.89595 | long>20=-6.13413\n[Cons Sigma] a=220 b=2.60 LL_global=-5.89595 LL_long=-6.13413 changed_frac=0.000\n[Cons Sigma] a=220 b=3.00 LL_global=-5.89595 LL_long=-6.13413 changed_frac=0.000\n[Cons Sigma] a=220 b=3.40 LL_global=-5.89632 LL_long=-6.13528 changed_frac=0.103\n[Cons Sigma] a=240 b=2.60 LL_global=-5.89595 LL_long=-6.13413 changed_frac=0.000\n[Cons Sigma] a=240 b=3.00 LL_global=-5.89595 LL_long=-6.13413 changed_frac=0.000\n[Cons Sigma] a=240 b=3.40 LL_global=-5.91107 LL_long=-6.16592 changed_frac=0.882\n[Cons Sigma] a=260 b=2.60 LL_global=-5.93437 LL_long=-6.14491 changed_frac=0.897\n[Cons Sigma] a=260 b=3.00 LL_global=-5.94886 LL_long=-6.17533 changed_frac=1.000\n[Cons Sigma] a=260 b=3.40 LL_global=-5.96340 LL_long=-6.20600 changed_frac=1.000\n[Cons Sigma] Best by long slice: a=220 b=2.60 LL_global=-5.89595 (\u0394=+0.00000) LL_long=-6.13413 (\u0394=+0.00000) changed_frac=0.000\n[Cons Sigma] Bin (0.0,5.0] banker=-5.77585 best=-5.77585 \u0394=+0.00000\n[Cons Sigma] Bin (5.0,15.0] banker=-5.82965 best=-5.82965 \u0394=+0.00000\n[Cons Sigma] Bin (15.0,1000000000.0] banker=-6.12656 best=-6.12656 \u0394=+0.00000\n[Cons Sigma] adopt? False (global \u0394=+0.00000, long \u0394=+0.00000)\nSaved submission_linear_floored_conservative.csv (conservative linear sigma floored by banker).\n[Cons Sigma] Keeping banker as primary (submission.csv unchanged).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/2017879672.py:89: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n"
          ]
        }
      ]
    },
    {
      "id": "be8e622c-5406-42c1-a27a-5b32c971ab0c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Overwrite submission.csv with slope+anchor banker submission\n",
        "import pandas as pd\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub_slope = pd.read_csv('submission_slope_banker.csv')\n",
        "assert sub_slope.shape[0] == ss.shape[0], 'Row count mismatch vs sample_submission'\n",
        "assert set(sub_slope['Patient_Week'].astype(str)) == set(ss['Patient_Week'].astype(str)), 'Patient_Week sets differ'\n",
        "assert sub_slope['FVC'].notna().all() and sub_slope['Confidence'].notna().all(), 'NaNs in slope+anchor submission'\n",
        "sub_slope.to_csv('submission.csv', index=False)\n",
        "print('submission.csv overwritten with slope+anchor banker submission (0.85 slope blend + 0.15 anchor, banker sigma).')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv overwritten with slope+anchor banker submission (0.85 slope blend + 0.15 anchor, banker sigma).\n"
          ]
        }
      ]
    },
    {
      "id": "eaad989a-2f59-404b-bc67-765893afd04b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build tolerant non-increasing FVC (allow +25 ml increases) on banker FVC; write alt submission and set as current\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "# Load banker submission and rebuild grid for ordering\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub_b = pd.read_csv('submission_banker.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]\n",
        "grid['Weeks'] = parts[1].astype(int)\n",
        "test = pd.read_csv('test.csv')\n",
        "test_bl = test[['Patient','Weeks','FVC','Percent']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_bl, on='Patient', how='left')\n",
        "assert grid['Base_Week'].notna().all(), 'Base_Week NaNs when building tolerant FVC grid'\n",
        "\n",
        "# Apply tolerant monotonicity per patient: going into the future, enforce FVC[t] <= next_FVC + tol\n",
        "tol = 25.0\n",
        "df_out = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'FVC': sub_b['FVC'].astype(float).clip(500, 6000)})\n",
        "def enforce_non_increasing_tolerant(g, tol=25.0):\n",
        "    g = g.sort_values('Weeks').copy()\n",
        "    f = g['FVC'].values.astype(float)\n",
        "    # Work backwards to future; allow small increases up to tol\n",
        "    for i in range(len(f)-2, -1, -1):\n",
        "        f[i] = min(f[i], f[i+1] + tol)\n",
        "    g['FVC'] = f\n",
        "    return g\n",
        "df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(lambda g: enforce_non_increasing_tolerant(g, tol))\n",
        "\n",
        "# Pin dist==0 to Base_FVC after tolerance enforcement\n",
        "dist = (grid['Weeks'].values - grid['Base_Week'].values).astype(float)\n",
        "base_fvc = grid['Base_FVC'].values.astype(float)\n",
        "fvc_tol = df_out['FVC'].values.astype(float)\n",
        "fvc_tol = np.where(dist == 0.0, base_fvc, fvc_tol)\n",
        "\n",
        "# Keep banker sigma as-is (already monotone and floored in submission_banker)\n",
        "sigma_banker = sub_b['Confidence'].astype(float).values\n",
        "\n",
        "sub_tol = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_tol, 'Confidence': sigma_banker})\n",
        "sub_tol.to_csv('submission_banker_tol25.csv', index=False)\n",
        "sub_tol.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_banker_tol25.csv and set submission.csv (banker sigma, FVC monotonicity tolerance +25 ml).')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_banker_tol25.csv and set submission.csv (banker sigma, FVC monotonicity tolerance +25 ml).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/1933768728.py:27: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(lambda g: enforce_non_increasing_tolerant(g, tol))\n"
          ]
        }
      ]
    },
    {
      "id": "a5aea09e-ccbd-4c5a-89f2-1312954dad8b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build banker-sigma submission WITHOUT FVC monotonicity (only clip + pin dist==0); set as current\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub_b = pd.read_csv('submission_banker.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "\n",
        "# Rebuild grid to compute dist and access Base_FVC\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]\n",
        "grid['Weeks'] = parts[1].astype(int)\n",
        "test = pd.read_csv('test.csv')\n",
        "test_bl = test[['Patient','Weeks','FVC','Percent']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_bl, on='Patient', how='left')\n",
        "assert grid['Base_Week'].notna().all(), 'Base_Week NaNs when building no-mono FVC grid'\n",
        "dist = (grid['Weeks'].values - grid['Base_Week'].values).astype(float)\n",
        "base_fvc = grid['Base_FVC'].values.astype(float)\n",
        "\n",
        "# No monotonicity: just clip and pin dist==0 to Base_FVC\n",
        "fvc_nm = np.clip(sub_b['FVC'].astype(float).values, 500, 6000)\n",
        "fvc_nm = np.where(dist == 0.0, base_fvc, fvc_nm)\n",
        "\n",
        "# Keep banker sigma from submission_banker\n",
        "sigma_b = sub_b['Confidence'].astype(float).values\n",
        "\n",
        "sub_no_mono = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_nm, 'Confidence': sigma_b})\n",
        "sub_no_mono.to_csv('submission_banker_no_mono.csv', index=False)\n",
        "sub_no_mono.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_banker_no_mono.csv and set submission.csv (banker sigma, no FVC monotonicity; only clip + pin dist==0).')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_banker_no_mono.csv and set submission.csv (banker sigma, no FVC monotonicity; only clip + pin dist==0).\n"
          ]
        }
      ]
    },
    {
      "id": "11718573-af73-4cf2-918c-cccddb4f8521",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Medal path A: slope-only parametric line + global anchor (w_slope=0.60, w_anchor=0.40),\n",
        "# clamp slopes to [5th,95th], banker sigma with floors and monotonicity; overwrite submission.csv\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1) Prepare baseline tables\n",
        "def prepare_baseline_table(df):\n",
        "    base = (df.sort_values(['Patient','Weeks']).groupby('Patient', as_index=False).first())\n",
        "    base = base[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "        columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "    return base\n",
        "\n",
        "# Reuse helpers from above cells: build_slope_features, compute_patient_slopes, robust_global_slope already defined\n",
        "\n",
        "# 2) Fit full-train slope models (Ridge + KNN) with ECDF/OHE on train baseline; clamp predictions\n",
        "base_full = prepare_baseline_table(train)\n",
        "slopes_full = compute_patient_slopes(train)\n",
        "slope_labels_full = pd.DataFrame({'Patient': list(slopes_full.keys()), 's_label': list(slopes_full.values())})\n",
        "base_full_lab = base_full.merge(slope_labels_full, on='Patient', how='left')\n",
        "base_fullF, feat_cols_s, ecdf_bf_s, ecdf_pc_s, cats_s = build_slope_features(base_full_lab, fit=True)\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "X_full = base_fullF[feat_cols_s].values.astype(float)\n",
        "y_full = base_fullF['s_label'].fillna(0.0).values.astype(float)\n",
        "X_full_std = scaler.fit_transform(X_full)\n",
        "ridge = Ridge(alpha=1.0, random_state=42).fit(X_full_std, y_full)\n",
        "knn = KNeighborsRegressor(n_neighbors=9, weights='distance').fit(X_full_std, y_full)\n",
        "q_lo, q_hi = np.percentile(y_full, 5), np.percentile(y_full, 95)\n",
        "\n",
        "# 3) Build strict test grid and baseline features for test patients\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]; grid['Weeks'] = parts[1].astype(int)\n",
        "test_bl = test[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "    columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_bl, on='Patient', how='left')\n",
        "assert grid['Base_Week'].notna().all(), 'Base_Week missing in test grid'\n",
        "dist = (grid['Weeks'].values - grid['Base_Week'].values).astype(float)\n",
        "\n",
        "base_test = grid[['Patient','Base_Week','Base_FVC']].drop_duplicates('Patient')\n",
        "meta = test[['Patient','Percent','Age','Sex','SmokingStatus']].drop_duplicates('Patient').rename(\n",
        "    columns={'Percent':'Percent_at_base'})\n",
        "base_test = base_test.merge(meta, on='Patient', how='left')\n",
        "base_testF, _, _, _, _ = build_slope_features(base_test, ecdf_bf_s, ecdf_pc_s, cats_s, fit=False)\n",
        "X_test_std = scaler.transform(base_testF[feat_cols_s].values.astype(float))\n",
        "s_r = ridge.predict(X_test_std)\n",
        "s_k = knn.predict(X_test_std)\n",
        "s_hat = 0.80 * s_r + 0.20 * s_k  # ridge/knn blend from probe\n",
        "s_hat = np.clip(s_hat, q_lo, q_hi)\n",
        "map_s = dict(zip(base_testF['Patient'].values, s_hat))\n",
        "\n",
        "# 4) Build slope-only FVC and global anchor; blend w_slope=0.60, w_anchor=0.40\n",
        "fvc_slope = (grid['Base_FVC'].values + pd.Series(grid['Patient']).map(map_s).astype(float).fillna(0.0).values * dist).astype(float)\n",
        "gs = robust_global_slope(compute_patient_slopes(train))\n",
        "fvc_anchor = (grid['Base_FVC'].values + gs * dist).astype(float)\n",
        "fvc_mix = 0.60 * fvc_slope + 0.40 * fvc_anchor\n",
        "\n",
        "# Guardrails: pin dist==0 to Base_FVC; per-patient non-increasing into the future; clip [500,6000]\n",
        "fvc_mix = np.where(dist == 0.0, grid['Base_FVC'].values.astype(float), fvc_mix)\n",
        "df_out = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'FVC': np.clip(fvc_mix, 500, 6000)})\n",
        "def enforce_non_increasing(g):\n",
        "    g = g.sort_values('Weeks').copy()\n",
        "    g['FVC'] = np.minimum.accumulate(g['FVC'].values[::-1])[::-1]\n",
        "    return g\n",
        "df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(enforce_non_increasing)\n",
        "fvc_final = df_out['FVC'].values.astype(float)\n",
        "\n",
        "# 5) Sigma: banker sigma = max(240 + 3*|dist|, 70), and \u2265100 when |dist|>20; enforce per-patient monotone in |dist|\n",
        "abs_dist = np.abs(dist).astype(float)\n",
        "sigma = np.maximum(240.0 + 3.0 * abs_dist, 70.0)\n",
        "sigma = np.where(abs_dist > 20.0, np.maximum(sigma, 100.0), sigma)\n",
        "df_sig = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': abs_dist, 'Sigma': sigma.astype(float)})\n",
        "def enforce_sigma_monotone(g):\n",
        "    g = g.sort_values('dist').copy()\n",
        "    g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "    return g\n",
        "df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n",
        "sigma_final = df_sig['Sigma'].values.astype(float)\n",
        "\n",
        "# 6) Save submission\n",
        "sub = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_final, 'Confidence': sigma_final})\n",
        "sub.to_csv('submission_slope_anchor_banker.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_slope_anchor_banker.csv and set submission.csv (w_slope=0.60, w_anchor=0.40, banker sigma, clamps+guardrails).')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_slope_anchor_banker.csv and set submission.csv (w_slope=0.60, w_anchor=0.40, banker sigma, clamps+guardrails).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/2490945051.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(enforce_non_increasing)\n/tmp/ipykernel_4688/2490945051.py:79: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n"
          ]
        }
      ]
    },
    {
      "id": "70ec222a-c9d1-4dae-8439-e27641e86d27",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Iter 2: CatBoost slope head (blend 0.5 CatBoost + 0.5 Ridge), slope-only + anchor (0.60/0.40), banker sigma\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "# Reuse helpers: build_slope_features, compute_patient_slopes, robust_global_slope, prepare_baseline_table\n",
        "def prepare_baseline_table(df):\n",
        "    base = (df.sort_values(['Patient','Weeks']).groupby('Patient', as_index=False).first())\n",
        "    base = base[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "        columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "    return base\n",
        "\n",
        "# 1) Full-train baseline with slope labels\n",
        "base_full = prepare_baseline_table(train)\n",
        "slopes_full = compute_patient_slopes(train)\n",
        "slope_labels_full = pd.DataFrame({'Patient': list(slopes_full.keys()), 's_label': list(slopes_full.values())})\n",
        "base_full_lab = base_full.merge(slope_labels_full, on='Patient', how='left')\n",
        "base_fullF, feat_cols_s, ecdf_bf_s, ecdf_pc_s, cats_s = build_slope_features(base_full_lab, fit=True)\n",
        "X_full = base_fullF[feat_cols_s].values.astype(float)\n",
        "y_full = base_fullF['s_label'].fillna(0.0).values.astype(float)\n",
        "\n",
        "# 2) Ridge slope (standardized)\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "X_full_std = scaler.fit_transform(X_full)\n",
        "ridge = Ridge(alpha=1.0, random_state=42).fit(X_full_std, y_full)\n",
        "\n",
        "# 3) CatBoost slope\n",
        "cb = CatBoostRegressor(\n",
        "    iterations=1200, depth=4, learning_rate=0.05, l2_leaf_reg=6.0, subsample=0.8,\n",
        "    random_strength=0.8, border_count=128, od_type='Iter', od_wait=50, bootstrap_type='Bernoulli',\n",
        "    loss_function='RMSE', random_state=42, verbose=False\n",
        ")\n",
        "cb.fit(X_full, y_full)\n",
        "\n",
        "# Clamp range from labels\n",
        "q_lo, q_hi = np.percentile(y_full, 5), np.percentile(y_full, 95)\n",
        "\n",
        "# 4) Build test grid baseline features\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]; grid['Weeks'] = parts[1].astype(int)\n",
        "test_bl = test[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "    columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_bl, on='Patient', how='left')\n",
        "assert grid['Base_Week'].notna().all(), 'Base_Week missing in test grid'\n",
        "dist = (grid['Weeks'].values - grid['Base_Week'].values).astype(float)\n",
        "\n",
        "base_test = grid[['Patient','Base_Week','Base_FVC']].drop_duplicates('Patient')\n",
        "meta = test[['Patient','Percent','Age','Sex','SmokingStatus']].drop_duplicates('Patient').rename(\n",
        "    columns={'Percent':'Percent_at_base'})\n",
        "base_test = base_test.merge(meta, on='Patient', how='left')\n",
        "base_testF, _, _, _, _ = build_slope_features(base_test, ecdf_bf_s, ecdf_pc_s, cats_s, fit=False)\n",
        "X_test = base_testF[feat_cols_s].values.astype(float)\n",
        "X_test_std = scaler.transform(X_test)\n",
        "\n",
        "# Predict slopes\n",
        "s_r = ridge.predict(X_test_std)\n",
        "s_cb = cb.predict(X_test)\n",
        "s_hat = 0.5 * s_r + 0.5 * s_cb\n",
        "s_hat = np.clip(s_hat, q_lo, q_hi)\n",
        "map_s = dict(zip(base_testF['Patient'].values, s_hat))\n",
        "\n",
        "# 5) Build FVC: slope-only + global anchor (0.60/0.40)\n",
        "fvc_slope = (grid['Base_FVC'].values + pd.Series(grid['Patient']).map(map_s).astype(float).fillna(0.0).values * dist).astype(float)\n",
        "gs = robust_global_slope(compute_patient_slopes(train))\n",
        "fvc_anchor = (grid['Base_FVC'].values + gs * dist).astype(float)\n",
        "fvc_mix = 0.60 * fvc_slope + 0.40 * fvc_anchor\n",
        "\n",
        "# Guardrails: pin dist==0 to Base_FVC; per-patient non-increasing; clip\n",
        "fvc_mix = np.where(dist == 0.0, grid['Base_FVC'].values.astype(float), fvc_mix)\n",
        "df_out = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'FVC': np.clip(fvc_mix, 500, 6000)})\n",
        "def enforce_non_increasing(g):\n",
        "    g = g.sort_values('Weeks').copy()\n",
        "    g['FVC'] = np.minimum.accumulate(g['FVC'].values[::-1])[::-1]\n",
        "    return g\n",
        "df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(enforce_non_increasing)\n",
        "fvc_final = df_out['FVC'].values.astype(float)\n",
        "\n",
        "# 6) Banker sigma with floors and per-patient monotonicity in |dist|\n",
        "abs_dist = np.abs(dist).astype(float)\n",
        "sigma = np.maximum(240.0 + 3.0 * abs_dist, 70.0)\n",
        "sigma = np.where(abs_dist > 20.0, np.maximum(sigma, 100.0), sigma)\n",
        "df_sig = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': abs_dist, 'Sigma': sigma.astype(float)})\n",
        "def enforce_sigma_monotone(g):\n",
        "    g = g.sort_values('dist').copy()\n",
        "    g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "    return g\n",
        "df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n",
        "sigma_final = df_sig['Sigma'].values.astype(float)\n",
        "\n",
        "# 7) Save submission\n",
        "sub = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_final, 'Confidence': sigma_final})\n",
        "sub.to_csv('submission_slopeCB_anchor_banker.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_slopeCB_anchor_banker.csv and set submission.csv (0.5 CatBoost + 0.5 Ridge slope, 0.60 slope + 0.40 anchor, banker sigma).')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_slopeCB_anchor_banker.csv and set submission.csv (0.5 CatBoost + 0.5 Ridge slope, 0.60 slope + 0.40 anchor, banker sigma).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/2652804835.py:78: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(enforce_non_increasing)\n/tmp/ipykernel_4688/2652804835.py:90: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n"
          ]
        }
      ]
    },
    {
      "id": "3e28643e-8179-45f7-ab03-1a34e906a8c0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Iter 3: Tune w_anchor with adoption rule: start at 0.60, probe 0.65 (adopt if LL_long +>=0.002 and LL_global drop <=0.002), then optionally 0.70\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def prepare_baseline_table(df):\n",
        "    base = (df.sort_values(['Patient','Weeks']).groupby('Patient', as_index=False).first())\n",
        "    base = base[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "        columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "    return base\n",
        "\n",
        "def build_slope_features(base_df, ecdf_basefvc=None, ecdf_percent=None, cats=None, fit=False):\n",
        "    b = base_df.copy()\n",
        "    b['log_Base_FVC'] = np.log1p(np.maximum(b['Base_FVC'].astype(float), 1.0))\n",
        "    b['BaseFVC_over_Age'] = b['Base_FVC'].astype(float) / np.maximum(b['Age'].astype(float), 1.0)\n",
        "    b['PercentBase_over_Age'] = b['Percent_at_base'].astype(float) / np.maximum(b['Age'].astype(float), 1.0)\n",
        "    if fit:\n",
        "        ecdf_basefvc = ecdf_rank_fit(b['Base_FVC'].values)\n",
        "        ecdf_percent = ecdf_rank_fit(b['Percent_at_base'].values)\n",
        "    b['BaseFVC_ecdf'] = ecdf_rank_transform(b['Base_FVC'].values, ecdf_basefvc)\n",
        "    b['Percent_ecdf'] = ecdf_rank_transform(b['Percent_at_base'].values, ecdf_percent)\n",
        "    if fit:\n",
        "        cats = one_hot_fit(b, ['Sex','SmokingStatus'])\n",
        "    b = one_hot_transform(b, cats)\n",
        "    num_cols = ['Age','Base_FVC','log_Base_FVC','Percent_at_base','BaseFVC_over_Age','PercentBase_over_Age','BaseFVC_ecdf','Percent_ecdf']\n",
        "    cat_cols = [c for c in b.columns if c.startswith('Sex__') or c.startswith('SmokingStatus__')]\n",
        "    feat_cols = num_cols + cat_cols\n",
        "    return b, feat_cols, ecdf_basefvc, ecdf_percent, cats\n",
        "\n",
        "def forward_oof_slope_and_anchor(train_df, n_splits=5, seed=42, ridge_w=0.80, knn_w=0.20):\n",
        "    gkf = GroupKFold(n_splits=n_splits)\n",
        "    groups = train_df['Patient'].values\n",
        "    y_true_list, dist_list = [], []\n",
        "    fvc_slope_list, fvc_anchor_list = [], []\n",
        "    for fold, (trn_idx, val_idx) in enumerate(gkf.split(train_df, groups=groups), 1):\n",
        "        trn = train_df.iloc[trn_idx].copy(); val = train_df.iloc[val_idx].copy()\n",
        "        base_trn = prepare_baseline_table(trn); base_val = prepare_baseline_table(val)\n",
        "        slopes_tr = compute_patient_slopes(trn)\n",
        "        base_trn_lab = base_trn.merge(pd.DataFrame({'Patient': list(slopes_tr.keys()), 's_label': list(slopes_tr.values())}), on='Patient', how='left')\n",
        "        base_trnF, feat_cols, ecdf_bf, ecdf_pc, cats_full = build_slope_features(base_trn_lab, fit=True)\n",
        "        base_valF, _, _, _, _ = build_slope_features(base_val, ecdf_bf, ecdf_pc, cats_full, fit=False)\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "        X_trn = base_trnF[feat_cols].values.astype(float); y_trn = base_trnF['s_label'].fillna(0.0).values.astype(float)\n",
        "        X_trn_std = scaler.fit_transform(X_trn)\n",
        "        X_val_std = scaler.transform(base_valF[feat_cols].values.astype(float))\n",
        "        ridge = Ridge(alpha=1.0, random_state=seed).fit(X_trn_std, y_trn)\n",
        "        knn = KNeighborsRegressor(n_neighbors=9, weights='distance').fit(X_trn_std, y_trn)\n",
        "        s_r = ridge.predict(X_val_std); s_k = knn.predict(X_val_std)\n",
        "        q_lo, q_hi = np.percentile(y_trn, 5), np.percentile(y_trn, 95)\n",
        "        s_bl = np.clip(ridge_w * s_r + knn_w * s_k, q_lo, q_hi)\n",
        "        s_map = dict(zip(base_val['Patient'].values, s_bl))\n",
        "        valm = val.merge(base_val[['Patient','Base_Week','Base_FVC']], on='Patient', how='left')\n",
        "        mask = (valm['Weeks'].values >= valm['Base_Week'].values)\n",
        "        dist = (valm['Weeks'].values - valm['Base_Week'].values).astype(float)\n",
        "        s_hat = valm['Patient'].map(s_map).astype(float).fillna(0.0).values\n",
        "        fvc_slope = (valm['Base_FVC'].values + s_hat * dist).astype(float)\n",
        "        gs_fold = robust_global_slope(compute_patient_slopes(trn))\n",
        "        fvc_anchor = (valm['Base_FVC'].values + gs_fold * dist).astype(float)\n",
        "        y_true_list.append(valm['FVC'].values[mask].astype(float))\n",
        "        dist_list.append(dist[mask].astype(float))\n",
        "        fvc_slope_list.append(fvc_slope[mask].astype(float))\n",
        "        fvc_anchor_list.append(fvc_anchor[mask].astype(float))\n",
        "        del trn, val, base_trn, base_val, base_trnF, base_valF, X_trn, X_trn_std, X_val_std\n",
        "        gc.collect()\n",
        "    return (np.concatenate(y_true_list), np.concatenate(dist_list),\n",
        "            np.concatenate(fvc_slope_list), np.concatenate(fvc_anchor_list))\n",
        "\n",
        "def laplace_ll_np(y_true, y_pred, sigma):\n",
        "    y_true = y_true.astype(float); y_pred = y_pred.astype(float); sigma = sigma.astype(float)\n",
        "    delta = np.minimum(np.abs(y_true - y_pred), 1000.0)\n",
        "    sigma = np.maximum(sigma, 70.0)\n",
        "    return float(np.mean(-delta / sigma - np.log(sigma)))\n",
        "\n",
        "# 1) Build OOF arrays for slope-only and anchor\n",
        "t0 = time.time()\n",
        "y_oof, dist_oof, fvc_slope_oof, fvc_anchor_oof = forward_oof_slope_and_anchor(train, n_splits=5, seed=42)\n",
        "sigma_oof = np.maximum(240.0 + 3.0 * np.abs(dist_oof), 70.0)\n",
        "sigma_oof = np.where(np.abs(dist_oof) > 20.0, np.maximum(sigma_oof, 100.0), sigma_oof)\n",
        "m_long = np.abs(dist_oof) > 20.0\n",
        "\n",
        "# 2) Adoption rule: start at 0.60, probe 0.65; adopt if LL_long improves by >=0.002 and LL_global drop <=0.002; then optionally probe 0.70\n",
        "def eval_w(w_a):\n",
        "    fvc_bl = (1.0 - w_a) * fvc_slope_oof + w_a * fvc_anchor_oof\n",
        "    ll_g = laplace_ll_np(y_oof, fvc_bl, sigma_oof)\n",
        "    ll_l = laplace_ll_np(y_oof[m_long], fvc_bl[m_long], sigma_oof[m_long]) if m_long.any() else ll_g\n",
        "    return ll_g, ll_l\n",
        "\n",
        "ll_g_60, ll_l_60 = eval_w(0.60)\n",
        "print(f'[Slope-only OOF] w_anchor=0.60 LL_global={ll_g_60:.5f} LL_long={ll_l_60:.5f}', flush=True)\n",
        "ll_g_65, ll_l_65 = eval_w(0.65)\n",
        "print(f'[Slope-only OOF] w_anchor=0.65 LL_global={ll_g_65:.5f} LL_long={ll_l_65:.5f}', flush=True)\n",
        "w_anchor = 0.60\n",
        "if (ll_l_65 - ll_l_60) >= 0.002 and (ll_g_60 - ll_g_65) <= 0.002:\n",
        "    w_anchor = 0.65\n",
        "    # Optionally probe 0.70 only if 0.65 adopted and it improved long slice sufficiently\n",
        "    ll_g_70, ll_l_70 = eval_w(0.70)\n",
        "    print(f'[Slope-only OOF] w_anchor=0.70 LL_global={ll_g_70:.5f} LL_long={ll_l_70:.5f}', flush=True)\n",
        "    if (ll_l_70 - ll_l_65) >= 0.002 and (ll_g_65 - ll_g_70) <= 0.002:\n",
        "        w_anchor = 0.70\n",
        "print(f'[Select] Using w_anchor={w_anchor:.2f} by adoption rule; elapsed={time.time()-t0:.2f}s', flush=True)\n",
        "\n",
        "# 3) Fit slope head on full baseline and predict test; blend with selected anchor weight\n",
        "base_full = prepare_baseline_table(train)\n",
        "slopes_full = compute_patient_slopes(train)\n",
        "base_full_lab = base_full.merge(pd.DataFrame({'Patient': list(slopes_full.keys()), 's_label': list(slopes_full.values())}), on='Patient', how='left')\n",
        "base_fullF, feat_cols, ecdf_bf, ecdf_pc, cats_full = build_slope_features(base_full_lab, fit=True)\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "X_full = base_fullF[feat_cols].values.astype(float); y_full = base_fullF['s_label'].fillna(0.0).values.astype(float)\n",
        "X_full_std = scaler.fit_transform(X_full)\n",
        "ridge = Ridge(alpha=1.0, random_state=42).fit(X_full_std, y_full)\n",
        "knn = KNeighborsRegressor(n_neighbors=9, weights='distance').fit(X_full_std, y_full)\n",
        "q_lo, q_hi = np.percentile(y_full, 5), np.percentile(y_full, 95)\n",
        "\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]; grid['Weeks'] = parts[1].astype(int)\n",
        "test_bl = test[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "    columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_bl, on='Patient', how='left')\n",
        "dist = (grid['Weeks'].values - grid['Base_Week'].values).astype(float)\n",
        "\n",
        "base_test = grid[['Patient','Base_Week','Base_FVC']].drop_duplicates('Patient')\n",
        "meta = test[['Patient','Percent','Age','Sex','SmokingStatus']].drop_duplicates('Patient').rename(columns={'Percent':'Percent_at_base'})\n",
        "base_test = base_test.merge(meta, on='Patient', how='left')\n",
        "base_testF, _, _, _, _ = build_slope_features(base_test, ecdf_bf, ecdf_pc, cats_full, fit=False)\n",
        "X_test_std = scaler.transform(base_testF[feat_cols].values.astype(float))\n",
        "s_r = ridge.predict(X_test_std); s_k = knn.predict(X_test_std)\n",
        "s_hat = np.clip(0.80 * s_r + 0.20 * s_k, q_lo, q_hi)\n",
        "map_s = dict(zip(base_testF['Patient'].values, s_hat))\n",
        "\n",
        "fvc_slope = (grid['Base_FVC'].values + pd.Series(grid['Patient']).map(map_s).astype(float).fillna(0.0).values * dist).astype(float)\n",
        "gs = robust_global_slope(slopes_full)\n",
        "fvc_anchor = (grid['Base_FVC'].values + gs * dist).astype(float)\n",
        "fvc_mix = (1.0 - w_anchor) * fvc_slope + w_anchor * fvc_anchor\n",
        "\n",
        "# Guardrails and sigma\n",
        "fvc_mix = np.where(dist == 0.0, grid['Base_FVC'].values.astype(float), fvc_mix)\n",
        "df_out = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'FVC': np.clip(fvc_mix, 500, 6000)})\n",
        "def enforce_non_increasing(g):\n",
        "    g = g.sort_values('Weeks').copy()\n",
        "    g['FVC'] = np.minimum.accumulate(g['FVC'].values[::-1])[::-1]\n",
        "    return g\n",
        "df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(enforce_non_increasing)\n",
        "fvc_final = df_out['FVC'].values.astype(float)\n",
        "\n",
        "abs_dist = np.abs(dist).astype(float)\n",
        "sigma = np.maximum(240.0 + 3.0 * abs_dist, 70.0)\n",
        "sigma = np.where(abs_dist > 20.0, np.maximum(sigma, 100.0), sigma)\n",
        "df_sig = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': abs_dist, 'Sigma': sigma.astype(float)})\n",
        "def enforce_sigma_monotone(g):\n",
        "    g = g.sort_values('dist').copy()\n",
        "    g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "    return g\n",
        "df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n",
        "sigma_final = df_sig['Sigma'].values.astype(float)\n",
        "\n",
        "sub = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_final, 'Confidence': sigma_final})\n",
        "fname = f'submission_slope_anchor_banker_wA{int(round(w_anchor*100))}.csv'\n",
        "sub.to_csv(fname, index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(f'Saved {fname} and set submission.csv (slope-only + anchor, w_anchor={w_anchor:.2f}, banker sigma).')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Slope-only OOF] w_anchor=0.60 LL_global=-6.14324 LL_long=-6.42511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Slope-only OOF] w_anchor=0.65 LL_global=-6.14273 LL_long=-6.42402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Select] Using w_anchor=0.60 by adoption rule; elapsed=0.60s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_slope_anchor_banker_wA60.csv and set submission.csv (slope-only + anchor, w_anchor=0.60, banker sigma).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/2738394223.py:146: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(enforce_non_increasing)\n/tmp/ipykernel_4688/2738394223.py:157: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n"
          ]
        }
      ]
    },
    {
      "id": "ef31fa43-6a7f-4dfc-81a8-16d30e423f0e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quantile LGBM v2: delta-target with ordered quantiles, fold hygiene, multi-param averaging\n",
        "from lightgbm import LGBMRegressor\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "\n",
        "t0 = time.time()\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "# Helpers assumed available earlier:\n",
        "# - prepare_baseline_table, build_slope_features, ecdf_rank_fit, ecdf_rank_transform, one_hot_fit, one_hot_transform\n",
        "# - compute_patient_slopes, robust_global_slope\n",
        "# - laplace_ll\n",
        "\n",
        "# Quantile feature builder (baseline-only + safe distance bases/interactions + ECDF/OHE)\n",
        "def build_q_features(grid_df, base_df, ecdf_bf=None, ecdf_pc=None, cats=None, fit=False):\n",
        "    d = grid_df.merge(base_df[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "    d['dist'] = (d['Weeks'] - d['Base_Week']).astype(float)\n",
        "    d = d[d['dist'] >= 0].copy()\n",
        "    d['abs_dist'] = d['dist'].abs()\n",
        "    d['log1p_abs_dist'] = np.log1p(d['abs_dist'])\n",
        "    d['dist_cap'] = d['dist'].clip(0, 30)\n",
        "    # Piecewise distance bases\n",
        "    d['dist_short'] = d['dist'].clip(0, 5)\n",
        "    d['dist_mid'] = (d['dist'] - 5).clip(lower=0, upper=10)\n",
        "    d['dist_long'] = (d['dist'] - 15).clip(lower=0)\n",
        "    d['dist2'] = d['dist']**2\n",
        "    d['dist3'] = d['dist']**3\n",
        "\n",
        "    d['Base_FVC'] = d['Base_FVC'].astype(float)\n",
        "    d['Percent_at_base'] = d['Percent_at_base'].astype(float).clip(30, 120)\n",
        "    d['Age'] = d['Age'].astype(float)\n",
        "    d['log_Base_FVC'] = np.log1p(np.maximum(d['Base_FVC'], 1.0))\n",
        "\n",
        "    # Safe interactions\n",
        "    d['Age_x_Percent'] = d['Age'] * d['Percent_at_base']\n",
        "    d['BaseFVC_x_dist'] = d['Base_FVC'] * d['dist']\n",
        "    d['dist_x_Age'] = d['dist'] * d['Age']\n",
        "    d['dist_x_Percent'] = d['dist'] * d['Percent_at_base']\n",
        "    # Piecewise interactions with Base_FVC\n",
        "    d['BaseFVC_x_dshort'] = d['Base_FVC'] * d['dist_short']\n",
        "    d['BaseFVC_x_dmid'] = d['Base_FVC'] * d['dist_mid']\n",
        "    d['BaseFVC_x_dlong'] = d['Base_FVC'] * d['dist_long']\n",
        "\n",
        "    if fit:\n",
        "        ecdf_bf = ecdf_rank_fit(d['Base_FVC'].values)\n",
        "        ecdf_pc = ecdf_rank_fit(d['Percent_at_base'].values)\n",
        "        cats = one_hot_fit(d, ['Sex','SmokingStatus'])\n",
        "    d['BaseFVC_ecdf'] = ecdf_rank_transform(d['Base_FVC'].values, ecdf_bf)\n",
        "    d['Percent_ecdf'] = ecdf_rank_transform(d['Percent_at_base'].values, ecdf_pc)\n",
        "    d = one_hot_transform(d, cats)\n",
        "\n",
        "    # ECDF-derived decile bucket for Base_FVC (fold-fit), deterministic one-hot without extra fit state\n",
        "    d['BFV_decile'] = np.floor(d['BaseFVC_ecdf'] * 10).clip(0, 9).astype(int)\n",
        "    for k in range(10):\n",
        "        d[f'BFV_decile__{k}'] = (d['BFV_decile'] == k).astype(np.int8)\n",
        "\n",
        "    feat_cols = [\n",
        "        'Age','Base_FVC','log_Base_FVC','Percent_at_base','BaseFVC_ecdf','Percent_ecdf',\n",
        "        'dist','abs_dist','log1p_abs_dist','dist_cap','dist_short','dist_mid','dist_long','dist2','dist3',\n",
        "        'Age_x_Percent','BaseFVC_x_dist','dist_x_Age','dist_x_Percent','BaseFVC_x_dshort','BaseFVC_x_dmid','BaseFVC_x_dlong','s_hat'\n",
        "    ] + [c for c in d.columns if c.startswith('Sex__') or c.startswith('SmokingStatus__') or c.startswith('BFV_decile__')]\n",
        "\n",
        "    for c in feat_cols:\n",
        "        if c not in d.columns: d[c] = 0.0\n",
        "\n",
        "    return d, feat_cols, ecdf_bf, ecdf_pc, cats\n",
        "\n",
        "# CV config\n",
        "N_SPLITS = 5\n",
        "gkf = GroupKFold(n_splits=N_SPLITS)\n",
        "groups = train['Patient'].values\n",
        "alphas = [0.10, 0.20, 0.50, 0.80, 0.90]  # q10/q20/q50/q80/q90\n",
        "oof_cols = ['q10_delta_oof','q20_delta_oof','q50_delta_oof','q80_delta_oof','q90_delta_oof']\n",
        "\n",
        "# 3 moderate param sets\n",
        "params_list = [\n",
        "    dict(objective='quantile', metric='quantile', n_estimators=3000, learning_rate=0.035,\n",
        "         num_leaves=20, max_depth=5, min_data_in_leaf=32, subsample=0.8, colsample_bytree=0.8,\n",
        "         reg_alpha=0.1, reg_lambda=0.2, n_jobs=-1, verbose=-1),\n",
        "    dict(objective='quantile', metric='quantile', n_estimators=3500, learning_rate=0.030,\n",
        "         num_leaves=31, max_depth=6, min_data_in_leaf=24, subsample=0.75, colsample_bytree=0.75,\n",
        "         reg_alpha=0.1, reg_lambda=0.2, n_jobs=-1, verbose=-1),\n",
        "    dict(objective='quantile', metric='quantile', n_estimators=4000, learning_rate=0.028,\n",
        "         num_leaves=48, max_depth=7, min_data_in_leaf=20, subsample=0.7, colsample_bytree=0.7,\n",
        "         reg_alpha=0.0, reg_lambda=0.3, n_jobs=-1, verbose=-1),\n",
        "]\n",
        "\n",
        "# OOF containers\n",
        "oof_df = train[['Patient','Weeks','FVC']].copy()\n",
        "for c in oof_cols: oof_df[c] = np.nan\n",
        "\n",
        "# Static TEST grid (weeks, baseline merge) and index mapping to ss order\n",
        "grid_te = ss.copy()\n",
        "parts = grid_te['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid_te['Patient'] = parts[0]; grid_te['Weeks'] = parts[1].astype(int)\n",
        "test_base = test[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "    columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "\n",
        "# Build a mapping from (Patient,Weeks) -> row index in ss\n",
        "grid_te_idx = grid_te[['Patient','Weeks']].copy()\n",
        "grid_te_idx['ss_idx'] = np.arange(grid_te_idx.shape[0], dtype=int)\n",
        "\n",
        "# Build fold-local s_hat machinery helper (use TRAIN fold only for slope labels; no leak)\n",
        "def fit_s_hat_fold(trn_df, base_trn):\n",
        "    slopes_trn = compute_patient_slopes(trn_df)\n",
        "    slope_labels_trn = pd.DataFrame({'Patient': list(slopes_trn.keys()), 's_label': list(slopes_trn.values())})\n",
        "    base_trn_lab = base_trn.merge(slope_labels_trn, on='Patient', how='left')\n",
        "    bf_trn, f_cols_s, ecdf_bf_s, ecdf_pc_s, cats_s = build_slope_features(base_trn_lab, fit=True)\n",
        "    scaler_s = StandardScaler(with_mean=True, with_std=True).fit(bf_trn[f_cols_s].values.astype(float))\n",
        "    Xs_tr = scaler_s.transform(bf_trn[f_cols_s].values.astype(float))\n",
        "    y_s = bf_trn['s_label'].fillna(0.0).values.astype(float)\n",
        "    ridge = Ridge(alpha=1.0, random_state=42).fit(Xs_tr, y_s)\n",
        "    knn = KNeighborsRegressor(n_neighbors=9, weights='distance').fit(Xs_tr, y_s)\n",
        "    q_lo, q_hi = np.percentile(y_s, [5,95])\n",
        "\n",
        "    def get_s_hat_map(base_df_patients):\n",
        "        bf_pred, _, _, _, _ = build_slope_features(base_df_patients, ecdf_bf_s, ecdf_pc_s, cats_s, fit=False)\n",
        "        Xs = scaler_s.transform(bf_pred[f_cols_s].values.astype(float))\n",
        "        s = 0.8*ridge.predict(Xs) + 0.2*knn.predict(Xs)\n",
        "        s = np.clip(s, q_lo, q_hi)\n",
        "        return dict(zip(bf_pred['Patient'].values, s))\n",
        "    return get_s_hat_map\n",
        "\n",
        "# Test deltas accumulator (fold- and param-averaged); full SS length\n",
        "test_preds_delta = np.zeros((ss.shape[0], len(alphas)), dtype=float)\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    tf = time.time()\n",
        "    trn_df = train.iloc[trn_idx].copy(); val_df = train.iloc[val_idx].copy()\n",
        "\n",
        "    # Fold-local baseline tables\n",
        "    base_trn = prepare_baseline_table(trn_df)\n",
        "    base_val = prepare_baseline_table(val_df)\n",
        "\n",
        "    # s_hat maps (TRAIN fold only)\n",
        "    get_s_hat_map = fit_s_hat_fold(trn_df, base_trn)\n",
        "    s_map_trn = get_s_hat_map(base_trn)\n",
        "    s_map_val = get_s_hat_map(base_val)\n",
        "    base_test = grid_te[['Patient']].drop_duplicates().merge(\n",
        "        test_base.drop_duplicates('Patient'), on='Patient', how='left')\n",
        "    s_map_test = get_s_hat_map(base_test[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']])\n",
        "\n",
        "    # Build future-only train/val with s_hat\n",
        "    trn = trn_df.merge(base_trn[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "    val = val_df.merge(base_val[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "    trn['dist'] = (trn['Weeks'] - trn['Base_Week']).astype(float); trn = trn[trn['dist'] >= 0].copy()\n",
        "    val['dist'] = (val['Weeks'] - val['Base_Week']).astype(float); val = val[val['dist'] >= 0].copy()\n",
        "    trn['s_hat'] = trn['Patient'].map(s_map_trn).astype(float).fillna(0.0)\n",
        "    val['s_hat'] = val['Patient'].map(s_map_val).astype(float).fillna(0.0)\n",
        "\n",
        "    # Features (fit transforms on TRAIN fold)\n",
        "    trn_feat, feat_cols, ecdf_bf, ecdf_pc, cats = build_q_features(trn[['Patient','Weeks']].copy(), base_trn, fit=True)\n",
        "    trn_feat['s_hat'] = trn_feat['Patient'].map(s_map_trn).astype(float).fillna(0.0)\n",
        "\n",
        "    # IMPORTANT: use base_val for VAL features to get correct Base_Week for val patients\n",
        "    val_feat, _, _, _, _ = build_q_features(val[['Patient','Weeks']].copy(), base_val, ecdf_bf, ecdf_pc, cats, fit=False)\n",
        "    val_feat['s_hat'] = val_feat['Patient'].map(s_map_val).astype(float).fillna(0.0)\n",
        "\n",
        "    # Align features with labels strictly by (Patient, Weeks) keys to avoid length mismatches\n",
        "    trn_feat_aligned = trn_feat.merge(trn[['Patient','Weeks','FVC']], on=['Patient','Weeks'], how='inner')\n",
        "    val_feat_aligned = val_feat.merge(val[['Patient','Weeks','FVC']], on=['Patient','Weeks'], how='inner')\n",
        "\n",
        "    # Targets: delta = FVC - Base_FVC\n",
        "    y_tr_delta = (trn_feat_aligned['FVC'].astype(float).values - trn_feat_aligned['Base_FVC'].astype(float).values)\n",
        "    y_va_delta = (val_feat_aligned['FVC'].astype(float).values - val_feat_aligned['Base_FVC'].astype(float).values)\n",
        "\n",
        "    X_tr = trn_feat_aligned[feat_cols].values.astype(float)\n",
        "    X_va = val_feat_aligned[feat_cols].values.astype(float)\n",
        "\n",
        "    # Skip fold if no data (safety against degenerate future-only splits)\n",
        "    if X_tr.shape[0] == 0 or X_va.shape[0] == 0:\n",
        "        print(f'[Quantile-\u0394 Fold {fold}] skipped (X_tr={X_tr.shape[0]}, X_va={X_va.shape[0]})', flush=True)\n",
        "        del trn_df, val_df, trn, val, trn_feat, val_feat, trn_feat_aligned, val_feat_aligned\n",
        "        gc.collect()\n",
        "        continue\n",
        "\n",
        "    # Fold accumulators\n",
        "    val_pred_delta_sum = np.zeros((X_va.shape[0], len(alphas)), dtype=float)\n",
        "    test_pred_delta_sum = np.zeros((ss.shape[0], len(alphas)), dtype=float)\n",
        "\n",
        "    # Test features under TRAIN-fold transforms; keep keys and map to ss indices\n",
        "    te_feat, _, _, _, _ = build_q_features(grid_te[['Patient','Weeks']].copy(), test_base, ecdf_bf, ecdf_pc, cats, fit=False)\n",
        "    te_feat['s_hat'] = te_feat['Patient'].map(s_map_test).astype(float).fillna(0.0).values\n",
        "    X_te = te_feat[feat_cols].values.astype(float)\n",
        "    te_keys = te_feat[['Patient','Weeks']].copy()\n",
        "    te_keys = te_keys.merge(grid_te_idx, on=['Patient','Weeks'], how='left')\n",
        "    te_idx = te_keys['ss_idx'].values.astype(int)\n",
        "\n",
        "    for p_i, p in enumerate(params_list):\n",
        "        for qi, a in enumerate(alphas):\n",
        "            mdl = LGBMRegressor(**p, alpha=a, random_state=42+fold+p_i*17)\n",
        "            mdl.fit(X_tr, y_tr_delta,\n",
        "                    eval_set=[(X_va, y_va_delta)],\n",
        "                    eval_metric='quantile',\n",
        "                    callbacks=[lgb.early_stopping(200, verbose=False)])\n",
        "            val_pred_delta_sum[:, qi] += mdl.predict(X_va, num_iteration=mdl.best_iteration_)\n",
        "            # add to the correct positions only for future rows\n",
        "            pred_te = mdl.predict(X_te, num_iteration=mdl.best_iteration_)\n",
        "            test_pred_delta_sum[te_idx, qi] += pred_te / len(params_list)\n",
        "\n",
        "    # Enforce quantile order on VAL deltas\n",
        "    val_pred_delta = np.sort(val_pred_delta_sum / len(params_list), axis=1)\n",
        "    # Write OOF delta quantiles by (Patient, Weeks)\n",
        "    val_keys = val_feat_aligned[['Patient','Weeks']].reset_index(drop=True)\n",
        "    oof_block = pd.DataFrame(val_pred_delta, columns=oof_cols)\n",
        "    oof_block = pd.concat([val_keys.reset_index(drop=True), oof_block], axis=1)\n",
        "    oof_df = oof_df.merge(oof_block, on=['Patient','Weeks'], how='left', suffixes=('','_new'))\n",
        "    for c in oof_cols:\n",
        "        oof_df[c] = oof_df[c].fillna(oof_df[c + '_new'])\n",
        "        oof_df.drop(columns=[c + '_new'], inplace=True)\n",
        "\n",
        "    # Accumulate TEST deltas (fold-avg)\n",
        "    test_preds_delta += (test_pred_delta_sum / N_SPLITS)\n",
        "\n",
        "    print(f'[Quantile-\u0394 Fold {fold}] trn={trn_feat_aligned.shape[0]} val={val_feat_aligned.shape[0]} elapsed={time.time()-tf:.2f}s', flush=True)\n",
        "    del trn_df, val_df, trn, val, trn_feat, val_feat, trn_feat_aligned, val_feat_aligned, X_tr, X_va, X_te, te_feat, te_idx, te_keys\n",
        "    gc.collect()\n",
        "\n",
        "# Test: enforce quantile order on deltas (ascending across quantiles)\n",
        "test_preds_delta = np.sort(test_preds_delta, axis=1)\n",
        "q10_d, q20_d, q50_d, q80_d, q90_d = test_preds_delta.T\n",
        "\n",
        "# Reconstruct test FVC; guardrails\n",
        "parts = ss['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid = pd.DataFrame({'Patient': parts[0], 'Weeks': parts[1].astype(int)})\n",
        "grid = grid.merge(test_base, on='Patient', how='left')\n",
        "dist_te = (grid['Weeks'] - grid['Base_Week']).astype(float)\n",
        "base_fvc_te = grid['Base_FVC'].values.astype(float)\n",
        "\n",
        "fvc_point = base_fvc_te + q50_d\n",
        "fvc_point = np.where(dist_te == 0.0, base_fvc_te, fvc_point)\n",
        "fvc_point = np.clip(fvc_point, 500, 6000)\n",
        "def enforce_non_increasing(g):\n",
        "    g = g.sort_values('Weeks').copy()\n",
        "    g['FVC'] = np.minimum.accumulate(g['FVC'].values[::-1])[::-1]\n",
        "    return g\n",
        "df_out = pd.DataFrame({'Patient': grid['Patient'], 'Weeks': grid['Weeks'].astype(int), 'FVC': fvc_point})\n",
        "df_out = df_out.groupby('Patient', group_keys=False).apply(enforce_non_increasing)\n",
        "fvc_final = df_out['FVC'].values.astype(float)\n",
        "\n",
        "# Sigma from quantile band with banker floor; tune c on OOF in {1.6, 1.8, 2.0}\n",
        "train_base = prepare_baseline_table(train)\n",
        "oof_fut = oof_df.dropna(subset=['q50_delta_oof']).copy()\n",
        "oof_fut = oof_fut.merge(train_base[['Patient','Base_Week','Base_FVC']], on='Patient', how='left')\n",
        "oof_fut['dist'] = (oof_fut['Weeks'] - oof_fut['Base_Week']).astype(float)\n",
        "oof_fut = oof_fut[oof_fut['dist'] >= 0].copy()\n",
        "oof_fut['pred_fvc'] = oof_fut['Base_FVC'].astype(float) + oof_fut['q50_delta_oof'].astype(float)\n",
        "\n",
        "band = (oof_fut['q80_delta_oof'] - oof_fut['q20_delta_oof']).abs().astype(float).values\n",
        "abs_dist_oof = oof_fut['dist'].abs().astype(float).values\n",
        "sigma_banker_oof = np.maximum(240.0 + 3.0 * abs_dist_oof, 70.0)\n",
        "sigma_banker_oof = np.where(abs_dist_oof > 20.0, np.maximum(sigma_banker_oof, 100.0), sigma_banker_oof)\n",
        "\n",
        "best_c, best_ll = 1.8, -1e9\n",
        "for c in [1.6, 1.8, 2.0]:\n",
        "    sigma_c = np.maximum(band / c, sigma_banker_oof)\n",
        "    ll = laplace_ll(oof_fut['FVC'].values.astype(float), oof_fut['pred_fvc'].values.astype(float), sigma_c)\n",
        "    if ll > best_ll:\n",
        "        best_ll, best_c = ll, c\n",
        "print(f'[Quantile-\u0394] Tuned sigma c={best_c:.1f} on OOF (LL={best_ll:.5f})', flush=True)\n",
        "\n",
        "# Test sigma with banker floor + per-patient monotone\n",
        "abs_dist_te = np.abs(dist_te).astype(float)\n",
        "sigma_from_band = (q80_d - q20_d) / best_c\n",
        "sigma_banker = np.maximum(240.0 + 3.0 * abs_dist_te, 70.0)\n",
        "sigma_banker = np.where(abs_dist_te > 20.0, np.maximum(sigma_banker, 100.0), sigma_banker)\n",
        "sigma = np.maximum(sigma_from_band, sigma_banker)\n",
        "\n",
        "df_sig = pd.DataFrame({'Patient': grid['Patient'], 'Weeks': grid['Weeks'].astype(int), 'dist': abs_dist_te, 'Sigma': sigma.astype(float)})\n",
        "def enforce_sigma_monotone(g):\n",
        "    g = g.sort_values('dist').copy()\n",
        "    g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "    return g\n",
        "df_sig = df_sig.groupby('Patient', group_keys=False).apply(enforce_sigma_monotone)\n",
        "sigma_final = df_sig['Sigma'].values.astype(float)\n",
        "\n",
        "# Save artifacts\n",
        "sub = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_final, 'Confidence': sigma_final})\n",
        "sub.to_csv('submission_quantile_lgbm_v2.csv', index=False)\n",
        "\n",
        "oof_save = oof_fut[['Patient','Weeks','FVC','Base_FVC','q10_delta_oof','q20_delta_oof','q50_delta_oof','q80_delta_oof','q90_delta_oof']].copy()\n",
        "oof_save.to_csv('oof_quantile_lgbm_v2.csv', index=False)\n",
        "\n",
        "# Also save test delta quantiles aligned to ss for downstream sigma band blending\n",
        "pd.DataFrame({\n",
        "    'Patient_Week': ss['Patient_Week'],\n",
        "    'q10_d': q10_d,\n",
        "    'q20_d': q20_d,\n",
        "    'q50_d': q50_d,\n",
        "    'q80_d': q80_d,\n",
        "    'q90_d': q90_d\n",
        "}).to_csv('pred_quantile_deltas_v2.csv', index=False)\n",
        "\n",
        "print(f'Saved submission_quantile_lgbm_v2.csv, oof_quantile_lgbm_v2.csv, and pred_quantile_deltas_v2.csv. Elapsed {time.time()-t0:.1f}s')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Quantile-\u0394 Fold 1] trn=1124 val=284 elapsed=3.00s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Quantile-\u0394 Fold 2] trn=1127 val=281 elapsed=3.04s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Quantile-\u0394 Fold 3] trn=1129 val=279 elapsed=3.25s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Quantile-\u0394 Fold 4] trn=1129 val=279 elapsed=3.67s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Quantile-\u0394 Fold 5] trn=1123 val=285 elapsed=2.66s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Quantile-\u0394] Tuned sigma c=1.8 on OOF (LL=-6.14165)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_quantile_lgbm_v2.csv, oof_quantile_lgbm_v2.csv, and pred_quantile_deltas_v2.csv. Elapsed 16.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/3636632165.py:242: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', group_keys=False).apply(enforce_non_increasing)\n/tmp/ipykernel_4688/3636632165.py:278: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', group_keys=False).apply(enforce_sigma_monotone)\n"
          ]
        }
      ]
    },
    {
      "id": "19449d30-670e-401f-9212-787c1582d883",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set submission.csv to tuned slope-only + anchor banker (w_anchor=0.50)\n",
        "import pandas as pd\n",
        "src = 'submission_slope_anchor_banker_wA50.csv'\n",
        "sub = pd.read_csv(src)\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "assert sub.shape[0] == ss.shape[0], 'Row mismatch vs sample_submission'\n",
        "assert set(sub['Patient_Week'].astype(str)) == set(ss['Patient_Week'].astype(str)), 'Patient_Week mismatch'\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(f'submission.csv overwritten with {src}')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv overwritten with submission_slope_anchor_banker_wA50.csv\n"
          ]
        }
      ]
    },
    {
      "id": "689ecb8d-053b-46f0-abe3-2e2a8287f4be",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Iter 5: MixedLM (LME) banker submission per expert Blueprint C\n",
        "import numpy as np, pandas as pd, warnings, time, gc\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# 1) Build train_lme with baseline merge and Weeks_Passed >= 0; standardize Age and Percent_at_base\n",
        "def build_baseline(df):\n",
        "    base = (df.sort_values(['Patient','Weeks']).groupby('Patient', as_index=False).first())\n",
        "    base = base[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "        columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "    return base\n",
        "\n",
        "t0 = time.time()\n",
        "train_base = build_baseline(train)\n",
        "# Drop overlapping demographic columns from left to avoid suffixes\n",
        "train_left = train.drop(columns=['Age','Sex','SmokingStatus'], errors='ignore')\n",
        "train_lme = train_left.merge(train_base[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "train_lme['Weeks_Passed'] = (train_lme['Weeks'] - train_lme['Base_Week']).astype(float)\n",
        "train_lme = train_lme[train_lme['Weeks_Passed'] >= 0].copy()\n",
        "train_lme['Weeks_Passed'] = train_lme['Weeks_Passed'] / 10.0  # stabilize\n",
        "age_mean, age_std = float(train_lme['Age'].astype(float).mean()), float(train_lme['Age'].astype(float).std() + 1e-9)\n",
        "pc_mean = float(train_lme['Percent_at_base'].astype(float).mean())\n",
        "pc_std  = float(train_lme['Percent_at_base'].astype(float).std() + 1e-9)\n",
        "train_lme['Age_std'] = (train_lme['Age'].astype(float) - age_mean) / age_std\n",
        "train_lme['Percent_at_base_std'] = (train_lme['Percent_at_base'].astype(float) - pc_mean) / pc_std\n",
        "\n",
        "# 2) Fit MixedLM: add quadratic time term I(Weeks_Passed**2) to fixed effects; random effects unchanged\n",
        "formula = 'FVC ~ 1 + Weeks_Passed + I(Weeks_Passed**2) + Age_std + C(Sex) + C(SmokingStatus) + Percent_at_base_std + Age_std:Percent_at_base_std'\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter('ignore')\n",
        "    md = smf.mixedlm(formula, data=train_lme, groups=train_lme['Patient'], re_formula='~Weeks_Passed')\n",
        "    mdf = md.fit(method='lbfgs', reml=True, maxiter=500, disp=False)\n",
        "print('MixedLM fitted. Converged:', mdf.converged, 'nobs:', mdf.nobs)\n",
        "\n",
        "# 3) Build strict test grid and predict fixed-effects only (new patients => random effects ~ 0 by default)\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]; grid['Weeks'] = parts[1].astype(int)\n",
        "test_base = test[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "    columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_base, on='Patient', how='left')\n",
        "assert grid['Base_Week'].notna().all(), 'Base_Week missing in test grid'\n",
        "grid['Weeks_Passed'] = (grid['Weeks'] - grid['Base_Week']).astype(float) / 10.0\n",
        "grid['Age_std'] = (grid['Age'].astype(float) - age_mean) / age_std\n",
        "grid['Percent_at_base_std'] = (grid['Percent_at_base'].astype(float) - pc_mean) / pc_std\n",
        "\n",
        "fvc_lme = mdf.predict(grid)\n",
        "\n",
        "# 4) Guardrails: pin dist==0 to Base_FVC, per-patient non-increasing, clip [500,6000]\n",
        "dist = (grid['Weeks'].values - grid['Base_Week'].values).astype(float)\n",
        "fvc_clip = np.clip(fvc_lme.values.astype(float), 500, 6000)\n",
        "fvc_clip = np.where(dist == 0.0, grid['Base_FVC'].values.astype(float), fvc_clip)\n",
        "df_out = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'FVC': fvc_clip})\n",
        "def enforce_non_increasing(g):\n",
        "    g = g.sort_values('Weeks').copy()\n",
        "    g['FVC'] = np.minimum.accumulate(g['FVC'].values[::-1])[::-1]\n",
        "    return g\n",
        "df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(enforce_non_increasing)\n",
        "fvc_final = df_out['FVC'].values.astype(float)\n",
        "\n",
        "# 5) Banker sigma with floors and per-patient monotonicity in |dist|\n",
        "abs_dist = np.abs(dist).astype(float)\n",
        "sigma = np.maximum(240.0 + 3.0 * abs_dist, 70.0)\n",
        "sigma = np.where(abs_dist > 20.0, np.maximum(sigma, 100.0), sigma)\n",
        "df_sig = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': abs_dist, 'Sigma': sigma.astype(float)})\n",
        "def enforce_sigma_monotone(g):\n",
        "    g = g.sort_values('dist').copy()\n",
        "    g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "    return g\n",
        "df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n",
        "sigma_final = df_sig['Sigma'].values.astype(float)\n",
        "\n",
        "# 6) Save submission\n",
        "sub_lme = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_final, 'Confidence': sigma_final})\n",
        "sub_lme.to_csv('submission_lme_banker.csv', index=False)\n",
        "sub_lme.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_lme_banker.csv and set submission.csv (MixedLM banker). Elapsed {:.2f}s'.format(time.time()-t0))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MixedLM fitted. Converged: True nobs: 1394\nSaved submission_lme_banker.csv and set submission.csv (MixedLM banker). Elapsed 0.41s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/1811607717.py:58: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(enforce_non_increasing)\n/tmp/ipykernel_4688/1811607717.py:70: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n"
          ]
        }
      ]
    },
    {
      "id": "e129dc1c-b4fc-4502-aecd-bcedb75bfce9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Final 3-model blend: 0.30 slopeA60 + 0.30 LME + 0.40 Quantile; banker sigma with guardrails\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "def load_fvc(path):\n",
        "    df = pd.read_csv(path).set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "    return df['FVC'].astype(float).values\n",
        "\n",
        "# Load components\n",
        "fvc_slope = load_fvc('submission_slope_anchor_banker_wA60.csv')\n",
        "fvc_lme   = load_fvc('submission_lme_banker.csv')\n",
        "fvc_q     = load_fvc('submission_quantile_lgbm.csv')\n",
        "\n",
        "# Blend weights (per expert: 0.30 slope + 0.30 LME + 0.40 Quantile)\n",
        "w_s, w_l, w_q = 0.30, 0.30, 0.40\n",
        "fvc_blend = w_s*fvc_slope + w_l*fvc_lme + w_q*fvc_q\n",
        "\n",
        "# Build grid to apply guardrails and banker sigma\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]; grid['Weeks'] = parts[1].astype(int)\n",
        "test = pd.read_csv('test.csv')\n",
        "test_bl = test[['Patient','Weeks','FVC','Percent']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_bl, on='Patient', how='left')\n",
        "abs_dist = (grid['Weeks'] - grid['Base_Week']).abs().astype(float)\n",
        "\n",
        "# Guardrails on FVC\n",
        "fvc_blend = np.where(abs_dist==0.0, grid['Base_FVC'].values.astype(float), fvc_blend)\n",
        "fvc_blend = np.clip(fvc_blend, 500, 6000)\n",
        "df_out = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'FVC': fvc_blend})\n",
        "def enforce_non_increasing(g):\n",
        "    g = g.sort_values('Weeks').copy()\n",
        "    g['FVC'] = np.minimum.accumulate(g['FVC'].values[::-1])[::-1]\n",
        "    return g\n",
        "df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(enforce_non_increasing)\n",
        "fvc_final = df_out['FVC'].values.astype(float)\n",
        "\n",
        "# Banker sigma + monotone in |dist|\n",
        "sigma = np.maximum(240.0 + 3.0*abs_dist, 70.0)\n",
        "sigma = np.where(abs_dist>20.0, np.maximum(sigma, 100.0), sigma)\n",
        "df_sig = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': abs_dist.values, 'Sigma': sigma.astype(float)})\n",
        "def enforce_sigma_monotone(g):\n",
        "    g = g.sort_values('dist').copy()\n",
        "    g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "    return g\n",
        "df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n",
        "sigma_final = df_sig['Sigma'].values.astype(float)\n",
        "\n",
        "sub = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_final, 'Confidence': sigma_final})\n",
        "sub.to_csv('submission_final_blend.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_final_blend.csv and set submission.csv (FVC=0.30 slopeA60 + 0.30 LME + 0.40 Quantile; banker sigma).')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_final_blend.csv and set submission.csv (FVC=0.30 slopeA60 + 0.30 LME + 0.40 Quantile; banker sigma).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/3777558667.py:36: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(enforce_non_increasing)\n/tmp/ipykernel_4688/3777558667.py:47: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n"
          ]
        }
      ]
    },
    {
      "id": "6e46068d-2a57-4e5d-bb60-bef528c54fa2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# OOF-driven, distance-aware 3-model blend (SlopeA60, LME, Quantile q50+anchor) with banker sigma\n",
        "import numpy as np, pandas as pd, gc, warnings, time\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "t0 = time.time()\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "def laplace_ll_np(y_true, y_pred, sigma):\n",
        "    y_true = y_true.astype(float); y_pred = y_pred.astype(float); sigma = sigma.astype(float)\n",
        "    delta = np.minimum(np.abs(y_true - y_pred), 1000.0)\n",
        "    sigma = np.maximum(sigma, 70.0)\n",
        "    return float(np.mean(-delta / sigma - np.log(sigma)))\n",
        "\n",
        "# 1) Build Slope+Anchor OOF (GroupKFold, future-only); anchor weight applied later\n",
        "def slope_anchor_oof(train_df, n_splits=5, seed=42):\n",
        "    from sklearn.linear_model import Ridge\n",
        "    from sklearn.neighbors import KNeighborsRegressor\n",
        "    gkf = GroupKFold(n_splits=n_splits)\n",
        "    groups = train_df['Patient'].values\n",
        "    y_list, d_list, fvc_slope_list, fvc_anchor_list = [], [], [], []\n",
        "    pid_list, week_list = [], []\n",
        "    for fold, (trn_idx, val_idx) in enumerate(gkf.split(train_df, groups=groups), 1):\n",
        "        trn = train_df.iloc[trn_idx].copy(); val = train_df.iloc[val_idx].copy()\n",
        "        base_trn = prepare_baseline_table(trn); base_val = prepare_baseline_table(val)\n",
        "        # Fit slope-head on TRAIN baseline\n",
        "        slopes_tr = compute_patient_slopes(trn)\n",
        "        lab = pd.DataFrame({'Patient': list(slopes_tr.keys()), 's_label': list(slopes_tr.values())})\n",
        "        bf_trn, feat_cols, ecdf_bf, ecdf_pc, cats = build_slope_features(base_trn.merge(lab, on='Patient', how='left'), fit=True)\n",
        "        bf_val, _, _, _, _ = build_slope_features(base_val, ecdf_bf, ecdf_pc, cats, fit=False)\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "        X_tr = bf_trn[feat_cols].values.astype(float); y_tr = bf_trn['s_label'].fillna(0.0).values.astype(float)\n",
        "        X_trs = scaler.fit_transform(X_tr); X_vs = scaler.transform(bf_val[feat_cols].values.astype(float))\n",
        "        ridge = Ridge(alpha=1.0, random_state=seed).fit(X_trs, y_tr)\n",
        "        knn   = KNeighborsRegressor(n_neighbors=9, weights='distance').fit(X_trs, y_tr)\n",
        "        s_r = ridge.predict(X_vs); s_k = knn.predict(X_vs)\n",
        "        q_lo, q_hi = np.percentile(y_tr, [5,95])\n",
        "        s_bl = np.clip(0.80*s_r + 0.20*s_k, q_lo, q_hi)\n",
        "        s_map = dict(zip(base_val['Patient'].values, s_bl))\n",
        "        valm = val.merge(base_val[['Patient','Base_Week','Base_FVC']], on='Patient', how='left')\n",
        "        mask = (valm['Weeks'] >= valm['Base_Week'])\n",
        "        dist = (valm['Weeks'] - valm['Base_Week']).astype(float)\n",
        "        fvc_slope = (valm['Base_FVC'].values + valm['Patient'].map(s_map).fillna(0.0).values * dist).astype(float)\n",
        "        gs_fold = robust_global_slope(compute_patient_slopes(trn))\n",
        "        fvc_anchor = (valm['Base_FVC'].values + gs_fold * dist).astype(float)\n",
        "        # Append masked rows along with keys\n",
        "        y_list.append(valm.loc[mask, 'FVC'].values.astype(float))\n",
        "        d_list.append(dist.values[mask].astype(float))\n",
        "        fvc_slope_list.append(fvc_slope[mask].astype(float))\n",
        "        fvc_anchor_list.append(fvc_anchor[mask].astype(float))\n",
        "        pid_list.append(valm.loc[mask, 'Patient'].astype(str).values)\n",
        "        week_list.append(valm.loc[mask, 'Weeks'].astype(int).values)\n",
        "        del trn, val, base_trn, base_val, bf_trn, bf_val, X_tr, X_trs, X_vs\n",
        "        gc.collect()\n",
        "    return (np.concatenate(y_list), np.concatenate(d_list),\n",
        "            np.concatenate(fvc_slope_list), np.concatenate(fvc_anchor_list),\n",
        "            np.concatenate(pid_list), np.concatenate(week_list))\n",
        "\n",
        "print('Building Slope+Anchor OOF...', flush=True)\n",
        "y_oof_s, dist_oof_s, fvc_slope_oof, fvc_anchor_oof, pid_oof_s, weeks_oof_s = slope_anchor_oof(train, n_splits=5, seed=42)\n",
        "\n",
        "# 2) Build LME OOF (GroupKFold, future-only hygiene)\n",
        "def lme_oof(train_df, n_splits=5):\n",
        "    gkf = GroupKFold(n_splits=n_splits)\n",
        "    groups = train_df['Patient'].values\n",
        "    y_list, d_list, fvc_list = [], [], []\n",
        "    pid_list, week_list = [], []\n",
        "    for fold, (trn_idx, val_idx) in enumerate(gkf.split(train_df, groups=groups), 1):\n",
        "        trn = train_df.iloc[trn_idx].copy(); val = train_df.iloc[val_idx].copy()\n",
        "        base_trn = prepare_baseline_table(trn); base_val = prepare_baseline_table(val)\n",
        "        trn_l = trn.drop(columns=['Age','Sex','SmokingStatus'], errors='ignore') \\\n",
        "                   .merge(base_trn[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "        trn_l['Weeks_Passed'] = (trn_l['Weeks'] - trn_l['Base_Week']).astype(float)/10.0\n",
        "        trn_l = trn_l[trn_l['Weeks_Passed'] >= 0].copy()\n",
        "        age_mean, age_std = trn_l['Age'].mean(), trn_l['Age'].std()+1e-9\n",
        "        pc_mean, pc_std   = trn_l['Percent_at_base'].mean(), trn_l['Percent_at_base'].std()+1e-9\n",
        "        trn_l['Age_std'] = (trn_l['Age'] - age_mean)/age_std\n",
        "        trn_l['Percent_at_base_std'] = (trn_l['Percent_at_base'] - pc_mean)/pc_std\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter('ignore')\n",
        "            md = smf.mixedlm('FVC ~ 1 + Weeks_Passed + I(Weeks_Passed**2) + Age_std + C(Sex) + C(SmokingStatus) + Percent_at_base_std + Age_std:Percent_at_base_std',\n",
        "                              data=trn_l, groups=trn_l['Patient'], re_formula='~Weeks_Passed')\n",
        "            mdf = md.fit(method='lbfgs', reml=True, maxiter=500, disp=False)\n",
        "        # Build VAL grid\n",
        "        val_left = val.drop(columns=['Age','Sex','SmokingStatus'], errors='ignore')\n",
        "        val_l = val_left.merge(base_val[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "        mask = (val_l['Weeks'] >= val_l['Base_Week'])\n",
        "        dist = (val_l['Weeks'] - val_l['Base_Week']).astype(float)\n",
        "        val_l['Weeks_Passed'] = dist/10.0\n",
        "        val_l['Age_std'] = (val_l['Age'] - age_mean)/age_std\n",
        "        val_l['Percent_at_base_std'] = (val_l['Percent_at_base'] - pc_mean)/pc_std\n",
        "        fvc_pred = mdf.predict(val_l).astype(float).values\n",
        "        y_list.append(val_l.loc[mask, 'FVC'].values.astype(float))\n",
        "        d_list.append(dist.values[mask].astype(float))\n",
        "        fvc_list.append(fvc_pred[mask].astype(float))\n",
        "        pid_list.append(val_l.loc[mask, 'Patient'].astype(str).values)\n",
        "        week_list.append(val_l.loc[mask, 'Weeks'].astype(int).values)\n",
        "        del trn, val, base_trn, base_val, trn_l, val_l\n",
        "        gc.collect()\n",
        "    return np.concatenate(y_list), np.concatenate(d_list), np.concatenate(fvc_list), np.concatenate(pid_list), np.concatenate(week_list)\n",
        "\n",
        "print('Building LME OOF...', flush=True)\n",
        "y_oof_l, dist_oof_l, fvc_lme_oof, pid_oof_l, weeks_oof_l = lme_oof(train, n_splits=5)\n",
        "\n",
        "# 3) Align Quantile OOF (q50 delta) and build quantile-based point preds with per-fold anchor (no leak)\n",
        "print('Loading Quantile OOF...', flush=True)\n",
        "oof_q = pd.read_csv('oof_quantile_lgbm_v2.csv')\n",
        "train_base = prepare_baseline_table(train)\n",
        "oof_q = oof_q.merge(train_base[['Patient','Base_Week','Base_FVC']], on='Patient', how='left', suffixes=('', '_base'))\n",
        "# Resolve possible suffixes from merge if oof_q already contains Base_FVC from saved file\n",
        "if 'Base_FVC_base' in oof_q.columns:\n",
        "    if 'Base_FVC' not in oof_q.columns:\n",
        "        oof_q['Base_FVC'] = oof_q['Base_FVC_base']\n",
        "    else:\n",
        "        oof_q['Base_FVC'] = oof_q['Base_FVC'].fillna(oof_q['Base_FVC_base'])\n",
        "    oof_q.drop(columns=['Base_FVC_base'], inplace=True)\n",
        "if 'Base_Week_base' in oof_q.columns and 'Base_Week' not in oof_q.columns:\n",
        "    oof_q['Base_Week'] = oof_q['Base_Week_base']\n",
        "    oof_q.drop(columns=['Base_Week_base'], inplace=True)\n",
        "oof_q['dist'] = (oof_q['Weeks'] - oof_q['Base_Week']).astype(float)\n",
        "oof_q = oof_q[oof_q['dist'] >= 0].dropna(subset=['q50_delta_oof']).copy()\n",
        "\n",
        "# Build fold membership and per-fold global slopes for anchor\n",
        "N_SPLITS = 5\n",
        "gkf = GroupKFold(n_splits=N_SPLITS)\n",
        "groups = train['Patient'].values\n",
        "patient_to_fold = {}\n",
        "fold_to_gs = {}\n",
        "for fold, (trn_idx, val_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    trn_df = train.iloc[trn_idx]\n",
        "    gs_fold = robust_global_slope(compute_patient_slopes(trn_df))\n",
        "    fold_to_gs[fold] = gs_fold\n",
        "    val_pats = train.iloc[val_idx]['Patient'].astype(str).unique()\n",
        "    for p in val_pats:\n",
        "        patient_to_fold[p] = fold\n",
        "\n",
        "oof_q['fold'] = oof_q['Patient'].astype(str).map(patient_to_fold).astype(int)\n",
        "oof_q['gs_fold'] = oof_q['fold'].map(fold_to_gs).astype(float)\n",
        "fvc_anchor_fold = oof_q['Base_FVC'].astype(float).values + oof_q['gs_fold'].values * oof_q['dist'].astype(float).values\n",
        "# Reconstruct quantile point from delta\n",
        "fvc_q_point = oof_q['Base_FVC'].astype(float).values + oof_q['q50_delta_oof'].astype(float).values\n",
        "fvc_q_oof = 0.70 * fvc_q_point + 0.30 * fvc_anchor_fold\n",
        "y_true_oof_q = oof_q['FVC'].astype(float).values\n",
        "\n",
        "# 4) Key-based alignment of OOF sources by (Patient, Weeks) using inner joins\n",
        "df_s = pd.DataFrame({\n",
        "    'Patient': pid_oof_s.astype(str),\n",
        "    'Weeks': weeks_oof_s.astype(int),\n",
        "    'y_true': y_oof_s.astype(float),\n",
        "    'dist': dist_oof_s.astype(float),\n",
        "    'fvc_slope': fvc_slope_oof.astype(float),\n",
        "    'fvc_anchor': fvc_anchor_oof.astype(float)\n",
        "})\n",
        "df_l = pd.DataFrame({\n",
        "    'Patient': pid_oof_l.astype(str),\n",
        "    'Weeks': weeks_oof_l.astype(int),\n",
        "    'fvc_lme': fvc_lme_oof.astype(float)\n",
        "})\n",
        "df_q = oof_q[['Patient','Weeks']].astype({'Patient':'str','Weeks':'int'}).copy()\n",
        "df_q['y_true_q'] = y_true_oof_q\n",
        "df_q['dist_q'] = oof_q['dist'].astype(float).values\n",
        "df_q['fvc_q'] = fvc_q_oof.astype(float)\n",
        "\n",
        "df_merged = df_s.merge(df_l, on=['Patient','Weeks'], how='inner').merge(df_q, on=['Patient','Weeks'], how='inner')\n",
        "# Sanity: y_true should match\n",
        "y_true_aligned = df_merged['y_true'].values.astype(float)\n",
        "dist_aligned = df_merged['dist'].values.astype(float)\n",
        "fvc_s_aligned = df_merged['fvc_slope'].values.astype(float)\n",
        "fvc_a_aligned = df_merged['fvc_anchor'].values.astype(float)\n",
        "fvc_l_aligned = df_merged['fvc_lme'].values.astype(float)\n",
        "fvc_q_aligned = df_merged['fvc_q'].values.astype(float)\n",
        "\n",
        "# 5) Weight search on distance bins using banker sigma\n",
        "sigma_oof = np.maximum(240.0 + 3.0 * np.abs(dist_aligned), 70.0)\n",
        "sigma_oof = np.where(np.abs(dist_aligned) > 20.0, np.maximum(sigma_oof, 100.0), sigma_oof)\n",
        "\n",
        "def grid_best(y, s, l, q, sigma, w_grid=np.arange(0.0, 1.01, 0.05)):\n",
        "    best_ll, best_w = -1e9, (0.3, 0.3, 0.4)\n",
        "    for ws in w_grid:\n",
        "        for wl in w_grid:\n",
        "            wq = 1.0 - ws - wl\n",
        "            if wq < 0 or wq > 1: continue\n",
        "            pred = ws*s + wl*l + wq*q\n",
        "            ll = laplace_ll_np(y, pred, sigma)\n",
        "            if ll > best_ll:\n",
        "                best_ll, best_w = ll, (ws, wl, wq)\n",
        "    return best_ll, best_w\n",
        "\n",
        "bins = [(0.0, 5.0), (5.0, 15.0), (15.0, 1e9)]\n",
        "best_weights = {}\n",
        "for lo, hi in bins:\n",
        "    m = (np.abs(dist_aligned) > lo) & (np.abs(dist_aligned) <= hi)\n",
        "    if not m.any():\n",
        "        best_weights[(lo,hi)] = (0.30, 0.30, 0.40)\n",
        "        print(f'Bin {lo}-{hi}: empty; default 0.30/0.30/0.40')\n",
        "        continue\n",
        "    ll, w = grid_best(y_true_aligned[m], fvc_s_aligned[m], fvc_l_aligned[m], fvc_q_aligned[m], sigma_oof[m])\n",
        "    best_weights[(lo, hi)] = w\n",
        "    print(f'Bin {lo}-{hi}: best weights Slope/LME/Quantile = {w[0]:.2f}/{w[1]:.2f}/{w[2]:.2f}, OOF LL={ll:.5f}')\n",
        "\n",
        "# 6) Apply weights to test submissions\n",
        "def load_fvc(path):\n",
        "    return pd.read_csv(path).set_index('Patient_Week').loc[ss['Patient_Week'],'FVC'].astype(float).values\n",
        "\n",
        "fvc_s_test = load_fvc('submission_slope_anchor_banker_wA60.csv')\n",
        "fvc_l_test = load_fvc('submission_lme_banker.csv')\n",
        "fvc_q_test = load_fvc('submission_quantile_lgbm_v2.csv')\n",
        "\n",
        "grid_te = ss.copy()\n",
        "parts = grid_te['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid_te['Patient'] = parts[0]; grid_te['Weeks'] = parts[1].astype(int)\n",
        "test_base = test[['Patient','Weeks','FVC','Percent']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid_te = grid_te.merge(test_base, on='Patient', how='left')\n",
        "dist_te = (grid_te['Weeks'] - grid_te['Base_Week']).astype(float); abs_dist_te = np.abs(dist_te).astype(float)\n",
        "\n",
        "fvc_blend = np.zeros_like(fvc_s_test)\n",
        "for (lo, hi), w in best_weights.items():\n",
        "    m = (abs_dist_te > lo) & (abs_dist_te <= hi)\n",
        "    fvc_blend[m] = w[0]*fvc_s_test[m] + w[1]*fvc_l_test[m] + w[2]*fvc_q_test[m]\n",
        "\n",
        "# Guardrails\n",
        "fvc_blend = np.where(abs_dist_te==0.0, grid_te['Base_FVC'].values.astype(float), fvc_blend)\n",
        "fvc_blend = np.clip(fvc_blend, 500, 6000)\n",
        "df_out = pd.DataFrame({'Patient': grid_te['Patient'], 'Weeks': grid_te['Weeks'], 'FVC': fvc_blend})\n",
        "def enforce_non_increasing(g):\n",
        "    g = g.sort_values('Weeks').copy()\n",
        "    g['FVC'] = np.minimum.accumulate(g['FVC'].values[::-1])[::-1]\n",
        "    return g\n",
        "df_out = df_out.groupby('Patient', group_keys=False).apply(enforce_non_increasing)\n",
        "fvc_final = df_out['FVC'].values.astype(float)\n",
        "\n",
        "# Sigma: banker only (per expert); optional hybrid with Quantile v2\n",
        "HYBRID_SIGMA = False\n",
        "sigma_banker = np.maximum(240.0 + 3.0 * abs_dist_te, 70.0)\n",
        "sigma_banker = np.where(abs_dist_te > 20.0, np.maximum(sigma_banker, 100.0), sigma_banker)\n",
        "if HYBRID_SIGMA:\n",
        "    sig_q = pd.read_csv('submission_quantile_lgbm_v2.csv').set_index('Patient_Week').loc[ss['Patient_Week'],'Confidence'].astype(float).values\n",
        "    sigma = np.maximum(sig_q, sigma_banker)\n",
        "else:\n",
        "    sigma = sigma_banker\n",
        "df_sig = pd.DataFrame({'Patient': grid_te['Patient'], 'dist': abs_dist_te, 'Sigma': sigma})\n",
        "df_sig = df_sig.groupby('Patient', group_keys=False).apply(lambda g: g.sort_values('dist').assign(Sigma=np.maximum.accumulate(g['Sigma'])))\n",
        "sigma_final = df_sig['Sigma'].values.astype(float)\n",
        "\n",
        "sub = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_final, 'Confidence': sigma_final})\n",
        "sub.to_csv('submission_distance_blend.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_distance_blend.csv and set submission.csv. Elapsed {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building Slope+Anchor OOF...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building LME OOF...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Quantile OOF...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bin 0.0-5.0: best weights Slope/LME/Quantile = 0.05/0.00/0.95, OOF LL=-6.04045\nBin 5.0-15.0: best weights Slope/LME/Quantile = 0.00/0.00/1.00, OOF LL=-6.12583\nBin 15.0-1000000000.0: best weights Slope/LME/Quantile = 0.05/0.05/0.90, OOF LL=-6.39437\nSaved submission_distance_blend.csv and set submission.csv. Elapsed 3.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/3043443098.py:230: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', group_keys=False).apply(enforce_non_increasing)\n/tmp/ipykernel_4688/3043443098.py:243: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', group_keys=False).apply(lambda g: g.sort_values('dist').assign(Sigma=np.maximum.accumulate(g['Sigma'])))\n"
          ]
        }
      ]
    },
    {
      "id": "58808439-cd78-408c-9196-b2e61522dcaa",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build secondary submission: hybrid sigma = max(distance-blend banker, Quantile v2), with per-patient monotone; do NOT overwrite submission.csv\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub_blend = pd.read_csv('submission_distance_blend.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "sub_qv2 = pd.read_csv('submission_quantile_lgbm_v2.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "\n",
        "# Rebuild grid to get |dist| for monotone ordering\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]\n",
        "grid['Weeks'] = parts[1].astype(int)\n",
        "test = pd.read_csv('test.csv')\n",
        "test_bl = test[['Patient','Weeks','FVC','Percent']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_bl, on='Patient', how='left')\n",
        "abs_dist = (grid['Weeks'] - grid['Base_Week']).abs().astype(float).values\n",
        "\n",
        "# Hybrid sigma\n",
        "sigma_blend = sub_blend['Confidence'].astype(float).values\n",
        "sigma_q = sub_qv2['Confidence'].astype(float).values\n",
        "sigma = np.maximum(sigma_blend, sigma_q)\n",
        "\n",
        "# Per-patient monotone in |dist|\n",
        "df_sig = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': abs_dist, 'Sigma': sigma})\n",
        "def enforce_sigma_monotone(g):\n",
        "    g = g.sort_values('dist').copy()\n",
        "    g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "    return g\n",
        "df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n",
        "sigma_final = df_sig['Sigma'].values.astype(float)\n",
        "\n",
        "# Save secondary submission (keep FVC from distance blend)\n",
        "sub_h = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': sub_blend['FVC'].astype(float).values, 'Confidence': sigma_final})\n",
        "sub_h.to_csv('submission_distance_blend_hybrid.csv', index=False)\n",
        "print('Saved submission_distance_blend_hybrid.csv (sigma = max(distance-blend banker, Quantile v2), monotone per patient).')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_distance_blend_hybrid.csv (sigma = max(distance-blend banker, Quantile v2), monotone per patient).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/2800145224.py:29: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n"
          ]
        }
      ]
    },
    {
      "id": "47282e47-4762-4ddd-a5a5-9607f8a2d693",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# OOF diagnostics and constrained distance-aware blends: current vs regularized vs equal\n",
        "import numpy as np, pandas as pd, warnings, gc, time\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "t0 = time.time()\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "def laplace_ll_np(y_true, y_pred, sigma):\n",
        "    y_true = y_true.astype(float); y_pred = y_pred.astype(float); sigma = sigma.astype(float)\n",
        "    delta = np.minimum(np.abs(y_true - y_pred), 1000.0)\n",
        "    sigma = np.maximum(sigma, 70.0)\n",
        "    return float(np.mean(-delta / sigma - np.log(sigma)))\n",
        "\n",
        "# Helpers already defined in earlier cells: prepare_baseline_table, build_slope_features, compute_patient_slopes, robust_global_slope\n",
        "\n",
        "def slope_anchor_oof(train_df, n_splits=5, seed=42):\n",
        "    from sklearn.linear_model import Ridge\n",
        "    from sklearn.neighbors import KNeighborsRegressor\n",
        "    gkf = GroupKFold(n_splits=n_splits)\n",
        "    groups = train_df['Patient'].values\n",
        "    y_list, d_list, fvc_slope_list, fvc_anchor_list = [], [], [], []\n",
        "    pid_list, week_list = [], []\n",
        "    for fold, (trn_idx, val_idx) in enumerate(gkf.split(train_df, groups=groups), 1):\n",
        "        trn = train_df.iloc[trn_idx].copy(); val = train_df.iloc[val_idx].copy()\n",
        "        base_trn = prepare_baseline_table(trn); base_val = prepare_baseline_table(val)\n",
        "        slopes_tr = compute_patient_slopes(trn)\n",
        "        lab = pd.DataFrame({'Patient': list(slopes_tr.keys()), 's_label': list(slopes_tr.values())})\n",
        "        bf_trn, feat_cols, ecdf_bf, ecdf_pc, cats = build_slope_features(base_trn.merge(lab, on='Patient', how='left'), fit=True)\n",
        "        bf_val, _, _, _, _ = build_slope_features(base_val, ecdf_bf, ecdf_pc, cats, fit=False)\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "        X_tr = bf_trn[feat_cols].values.astype(float); y_tr = bf_trn['s_label'].fillna(0.0).values.astype(float)\n",
        "        X_trs = scaler.fit_transform(X_tr); X_vs = scaler.transform(bf_val[feat_cols].values.astype(float))\n",
        "        from sklearn.linear_model import Ridge\n",
        "        from sklearn.neighbors import KNeighborsRegressor\n",
        "        ridge = Ridge(alpha=1.0, random_state=seed).fit(X_trs, y_tr)\n",
        "        knn   = KNeighborsRegressor(n_neighbors=9, weights='distance').fit(X_trs, y_tr)\n",
        "        s_r = ridge.predict(X_vs); s_k = knn.predict(X_vs)\n",
        "        q_lo, q_hi = np.percentile(y_tr, [5,95])\n",
        "        s_bl = np.clip(0.80*s_r + 0.20*s_k, q_lo, q_hi)\n",
        "        s_map = dict(zip(base_val['Patient'].values, s_bl))\n",
        "        valm = val.merge(base_val[['Patient','Base_Week','Base_FVC']], on='Patient', how='left')\n",
        "        mask = (valm['Weeks'] >= valm['Base_Week'])\n",
        "        dist = (valm['Weeks'] - valm['Base_Week']).astype(float)\n",
        "        fvc_slope = (valm['Base_FVC'].values + valm['Patient'].map(s_map).fillna(0.0).values * dist).astype(float)\n",
        "        gs_fold = robust_global_slope(compute_patient_slopes(trn))\n",
        "        fvc_anchor = (valm['Base_FVC'].values + gs_fold * dist).astype(float)\n",
        "        y_list.append(valm.loc[mask, 'FVC'].values.astype(float))\n",
        "        d_list.append(dist.values[mask].astype(float))\n",
        "        fvc_slope_list.append(fvc_slope[mask].astype(float))\n",
        "        fvc_anchor_list.append(fvc_anchor[mask].astype(float))\n",
        "        pid_list.append(valm.loc[mask, 'Patient'].astype(str).values)\n",
        "        week_list.append(valm.loc[mask, 'Weeks'].astype(int).values)\n",
        "        del trn, val, base_trn, base_val, bf_trn, bf_val, X_tr, X_trs, X_vs\n",
        "        gc.collect()\n",
        "    return (np.concatenate(y_list), np.concatenate(d_list), np.concatenate(fvc_slope_list), np.concatenate(fvc_anchor_list), np.concatenate(pid_list), np.concatenate(week_list))\n",
        "\n",
        "def lme_oof(train_df, n_splits=5):\n",
        "    gkf = GroupKFold(n_splits=n_splits)\n",
        "    groups = train_df['Patient'].values\n",
        "    y_list, d_list, fvc_list = [], [], []\n",
        "    pid_list, week_list = [], []\n",
        "    for fold, (trn_idx, val_idx) in enumerate(gkf.split(train_df, groups=groups), 1):\n",
        "        trn = train_df.iloc[trn_idx].copy(); val = train_df.iloc[val_idx].copy()\n",
        "        base_trn = prepare_baseline_table(trn); base_val = prepare_baseline_table(val)\n",
        "        trn_l = trn.drop(columns=['Age','Sex','SmokingStatus'], errors='ignore').merge(\n",
        "            base_trn[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left'\n",
        "        )\n",
        "        trn_l['Weeks_Passed'] = (trn_l['Weeks'] - trn_l['Base_Week']).astype(float)/10.0\n",
        "        trn_l = trn_l[trn_l['Weeks_Passed'] >= 0].copy()\n",
        "        age_mean, age_std = trn_l['Age'].mean(), trn_l['Age'].std()+1e-9\n",
        "        pc_mean, pc_std   = trn_l['Percent_at_base'].mean(), trn_l['Percent_at_base'].std()+1e-9\n",
        "        trn_l['Age_std'] = (trn_l['Age'] - age_mean)/age_std\n",
        "        trn_l['Percent_at_base_std'] = (trn_l['Percent_at_base'] - pc_mean)/pc_std\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter('ignore')\n",
        "            md = smf.mixedlm('FVC ~ 1 + Weeks_Passed + I(Weeks_Passed**2) + Age_std + C(Sex) + C(SmokingStatus) + Percent_at_base_std + Age_std:Percent_at_base_std',\n",
        "                              data=trn_l, groups=trn_l['Patient'], re_formula='~Weeks_Passed')\n",
        "            mdf = md.fit(method='lbfgs', reml=True, maxiter=500, disp=False)\n",
        "        val_left = val.drop(columns=['Age','Sex','SmokingStatus'], errors='ignore')\n",
        "        val_l = val_left.merge(base_val[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "        mask = (val_l['Weeks'] >= val_l['Base_Week'])\n",
        "        dist = (val_l['Weeks'] - val_l['Base_Week']).astype(float)\n",
        "        val_l['Weeks_Passed'] = dist/10.0\n",
        "        val_l['Age_std'] = (val_l['Age'] - age_mean)/age_std\n",
        "        val_l['Percent_at_base_std'] = (val_l['Percent_at_base'] - pc_mean)/pc_std\n",
        "        fvc_pred = mdf.predict(val_l).astype(float).values\n",
        "        y_list.append(val_l.loc[mask, 'FVC'].values.astype(float))\n",
        "        d_list.append(dist.values[mask].astype(float))\n",
        "        fvc_list.append(fvc_pred[mask].astype(float))\n",
        "        pid_list.append(val_l.loc[mask, 'Patient'].astype(str).values)\n",
        "        week_list.append(val_l.loc[mask, 'Weeks'].astype(int).values)\n",
        "        del trn, val, base_trn, base_val, trn_l, val_l\n",
        "        gc.collect()\n",
        "    return np.concatenate(y_list), np.concatenate(d_list), np.concatenate(fvc_list), np.concatenate(pid_list), np.concatenate(week_list)\n",
        "\n",
        "print('Build OOF sources...', flush=True)\n",
        "y_s, d_s, fvc_s, fvc_a, pid_s, wk_s = slope_anchor_oof(train, 5, 42)\n",
        "y_l, d_l, fvc_l, pid_l, wk_l = lme_oof(train, 5)\n",
        "oof_q = pd.read_csv('oof_quantile_lgbm_v2.csv')\n",
        "train_base = prepare_baseline_table(train)\n",
        "oof_q = oof_q.merge(train_base[['Patient','Base_Week','Base_FVC']], on='Patient', how='left', suffixes=('', '_base'))\n",
        "if 'Base_FVC_base' in oof_q.columns:\n",
        "    if 'Base_FVC' not in oof_q.columns: oof_q['Base_FVC'] = oof_q['Base_FVC_base']\n",
        "    else: oof_q['Base_FVC'] = oof_q['Base_FVC'].fillna(oof_q['Base_FVC_base'])\n",
        "    oof_q.drop(columns=['Base_FVC_base'], inplace=True)\n",
        "if 'Base_Week_base' in oof_q.columns and 'Base_Week' not in oof_q.columns:\n",
        "    oof_q['Base_Week'] = oof_q['Base_Week_base']\n",
        "    oof_q.drop(columns=['Base_Week_base'], inplace=True)\n",
        "oof_q['dist'] = (oof_q['Weeks'] - oof_q['Base_Week']).astype(float)\n",
        "oof_q = oof_q[oof_q['dist'] >= 0].dropna(subset=['q50_delta_oof']).copy()\n",
        "\n",
        "# Per-fold anchor for quantile OOF\n",
        "N_SPLITS = 5\n",
        "gkf = GroupKFold(n_splits=N_SPLITS)\n",
        "groups = train['Patient'].values\n",
        "patient_to_fold = {}; fold_to_gs = {}\n",
        "for fold, (trn_idx, val_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    trn_df = train.iloc[trn_idx]\n",
        "    gs_fold = robust_global_slope(compute_patient_slopes(trn_df))\n",
        "    fold_to_gs[fold] = gs_fold\n",
        "    for p in train.iloc[val_idx]['Patient'].astype(str).unique():\n",
        "        patient_to_fold[p] = fold\n",
        "oof_q['fold'] = oof_q['Patient'].astype(str).map(patient_to_fold).astype(int)\n",
        "oof_q['gs_fold'] = oof_q['fold'].map(fold_to_gs).astype(float)\n",
        "fvc_anchor_q = oof_q['Base_FVC'].astype(float).values + oof_q['gs_fold'].values * oof_q['dist'].astype(float).values\n",
        "fvc_q_point = oof_q['Base_FVC'].astype(float).values + oof_q['q50_delta_oof'].astype(float).values\n",
        "fvc_q = 0.70 * fvc_q_point + 0.30 * fvc_anchor_q\n",
        "\n",
        "# Align OOF by keys\n",
        "df_s = pd.DataFrame({'Patient': pid_s.astype(str), 'Weeks': wk_s.astype(int), 'y_true': y_s.astype(float), 'dist': d_s.astype(float), 'fvc_s': fvc_s.astype(float)})\n",
        "df_s['fvc_a'] = fvc_a.astype(float)\n",
        "df_l = pd.DataFrame({'Patient': pid_l.astype(str), 'Weeks': wk_l.astype(int), 'fvc_l': fvc_l.astype(float)})\n",
        "df_q = oof_q[['Patient','Weeks']].astype({'Patient':'str','Weeks':'int'}).copy()\n",
        "df_q['fvc_q'] = fvc_q.astype(float)\n",
        "\n",
        "dfm = df_s.merge(df_l, on=['Patient','Weeks'], how='inner').merge(df_q, on=['Patient','Weeks'], how='inner')\n",
        "y = dfm['y_true'].values.astype(float)\n",
        "dist = dfm['dist'].values.astype(float)\n",
        "s = dfm['fvc_s'].values.astype(float)\n",
        "l = dfm['fvc_l'].values.astype(float)\n",
        "q = dfm['fvc_q'].values.astype(float)\n",
        "sigma_oof = np.maximum(240.0 + 3.0 * np.abs(dist), 70.0)\n",
        "sigma_oof = np.where(np.abs(dist) > 20.0, np.maximum(sigma_oof, 100.0), sigma_oof)\n",
        "\n",
        "def search_weights(dist_mask, w_grid=np.arange(0.0, 1.01, 0.05), ws_min=0.0, wl_min=0.0):\n",
        "    idx = dist_mask\n",
        "    if not np.any(idx):\n",
        "        return (-1e9, (0.33, 0.33, 0.34))\n",
        "    best_ll, best_w = -1e9, (0.33, 0.33, 0.34)\n",
        "    for ws in w_grid:\n",
        "        for wl in w_grid:\n",
        "            wq = 1.0 - ws - wl\n",
        "            if wq < 0 or wq > 1: continue\n",
        "            if ws < ws_min or wl < wl_min: continue\n",
        "            pred = ws*s[idx] + wl*l[idx] + wq*q[idx]\n",
        "            ll = laplace_ll_np(y[idx], pred, sigma_oof[idx])\n",
        "            if ll > best_ll:\n",
        "                best_ll, best_w = ll, (ws, wl, wq)\n",
        "    return best_ll, best_w\n",
        "\n",
        "bins = [(0.0,5.0), (5.0,15.0), (15.0, 1e9)]\n",
        "mask_bins = [ (np.abs(dist)>lo) & (np.abs(dist)<=hi) for lo,hi in bins ]\n",
        "\n",
        "# Current: unconstrained\n",
        "w_cur = {}\n",
        "for (lo,hi), m in zip(bins, mask_bins):\n",
        "    ll, w = search_weights(m, ws_min=0.0, wl_min=0.0)\n",
        "    w_cur[(lo,hi)] = w\n",
        "    print(f'[CUR] Bin {lo}-{hi}: ws={w[0]:.2f} wl={w[1]:.2f} wq={w[2]:.2f}')\n",
        "\n",
        "# Regularized: enforce ws>=0.15 for long bin, keep others unconstrained; also wl_min=0.05 in long\n",
        "w_reg = {}\n",
        "for (lo,hi), m in zip(bins, mask_bins):\n",
        "    ws_min = 0.15 if lo==15.0 else 0.0\n",
        "    wl_min = 0.05 if lo==15.0 else 0.0\n",
        "    ll, w = search_weights(m, ws_min=ws_min, wl_min=wl_min)\n",
        "    w_reg[(lo,hi)] = w\n",
        "    print(f'[REG] Bin {lo}-{hi}: ws={w[0]:.2f} wl={w[1]:.2f} wq={w[2]:.2f}')\n",
        "\n",
        "# Equal weights\n",
        "w_eq = {(lo,hi):(1/3,1/3,1/3) for (lo,hi) in bins}\n",
        "\n",
        "def oof_ll_for_weights(weights):\n",
        "    pred = np.zeros_like(y)\n",
        "    for (lo,hi), m in zip(bins, mask_bins):\n",
        "        ws, wl, wq = weights[(lo,hi)]\n",
        "        pred[m] = ws*s[m] + wl*l[m] + wq*q[m]\n",
        "    return laplace_ll_np(y, pred, sigma_oof)\n",
        "\n",
        "ll_cur = oof_ll_for_weights(w_cur)\n",
        "ll_reg = oof_ll_for_weights(w_reg)\n",
        "ll_eq  = oof_ll_for_weights(w_eq)\n",
        "print(f'[OOF LL] cur={ll_cur:.5f} | reg={ll_reg:.5f} (\u0394={ll_reg-ll_cur:+.5f}) | equal={ll_eq:.5f} (\u0394={ll_eq-ll_cur:+.5f})')\n",
        "\n",
        "# Build test blends for cur, reg, equal with banker sigma and guardrails\n",
        "def load_fvc(path):\n",
        "    return pd.read_csv(path).set_index('Patient_Week').loc[ss['Patient_Week'],'FVC'].astype(float).values\n",
        "\n",
        "fvc_s_test = load_fvc('submission_slope_anchor_banker_wA60.csv')\n",
        "fvc_l_test = load_fvc('submission_lme_banker.csv')\n",
        "fvc_q_test = load_fvc('submission_quantile_lgbm_v2.csv')\n",
        "\n",
        "grid_te = ss.copy()\n",
        "parts = grid_te['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid_te['Patient'] = parts[0]; grid_te['Weeks'] = parts[1].astype(int)\n",
        "test_base = test[['Patient','Weeks','FVC','Percent']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid_te = grid_te.merge(test_base, on='Patient', how='left')\n",
        "dist_te = (grid_te['Weeks'] - grid_te['Base_Week']).astype(float); abs_dist_te = np.abs(dist_te).astype(float)\n",
        "\n",
        "def apply_weights_to_test(weights):\n",
        "    fvc = np.zeros_like(fvc_s_test)\n",
        "    for (lo,hi), (ws, wl, wq) in weights.items():\n",
        "        m = (abs_dist_te > lo) & (abs_dist_te <= hi)\n",
        "        fvc[m] = ws*fvc_s_test[m] + wl*fvc_l_test[m] + wq*fvc_q_test[m]\n",
        "    fvc = np.where(abs_dist_te==0.0, grid_te['Base_FVC'].values.astype(float), fvc)\n",
        "    fvc = np.clip(fvc, 500, 6000)\n",
        "    df_out = pd.DataFrame({'Patient': grid_te['Patient'], 'Weeks': grid_te['Weeks'], 'FVC': fvc})\n",
        "    def enforce_non_increasing(g):\n",
        "        g = g.sort_values('Weeks').copy()\n",
        "        g['FVC'] = np.minimum.accumulate(g['FVC'].values[::-1])[::-1]\n",
        "        return g\n",
        "    df_out = df_out.groupby('Patient', group_keys=False).apply(enforce_non_increasing)\n",
        "    fvc_final = df_out['FVC'].values.astype(float)\n",
        "    sigma_b = np.maximum(240.0 + 3.0 * abs_dist_te, 70.0)\n",
        "    sigma_b = np.where(abs_dist_te > 20.0, np.maximum(sigma_b, 100.0), sigma_b)\n",
        "    df_sig = pd.DataFrame({'Patient': grid_te['Patient'], 'dist': abs_dist_te, 'Sigma': sigma_b})\n",
        "    df_sig = df_sig.groupby('Patient', group_keys=False).apply(lambda g: g.sort_values('dist').assign(Sigma=np.maximum.accumulate(g['Sigma'])))\n",
        "    sigma_final = df_sig['Sigma'].values.astype(float)\n",
        "    return fvc_final, sigma_final\n",
        "\n",
        "fvc_cur, sig_cur = apply_weights_to_test(w_cur)\n",
        "fvc_reg, sig_reg = apply_weights_to_test(w_reg)\n",
        "fvc_eq,  sig_eq  = apply_weights_to_test(w_eq)\n",
        "\n",
        "pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_cur, 'Confidence': sig_cur}).to_csv('submission_distance_blend_cur.csv', index=False)\n",
        "pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_reg, 'Confidence': sig_reg}).to_csv('submission_distance_blend_reg.csv', index=False)\n",
        "pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_eq,  'Confidence': sig_eq}).to_csv('submission_distance_blend_equal.csv', index=False)\n",
        "print('Saved submissions: _cur, _reg (ws>=0.15, wl>=0.05 in long), _equal. Diagnostics complete. Elapsed {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build OOF sources...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CUR] Bin 0.0-5.0: ws=0.05 wl=0.00 wq=0.95\n[CUR] Bin 5.0-15.0: ws=0.00 wl=0.00 wq=1.00\n[CUR] Bin 15.0-1000000000.0: ws=0.05 wl=0.05 wq=0.90\n[REG] Bin 0.0-5.0: ws=0.05 wl=0.00 wq=0.95\n[REG] Bin 5.0-15.0: ws=0.00 wl=0.00 wq=1.00\n[REG] Bin 15.0-1000000000.0: ws=0.15 wl=0.05 wq=0.80\n[OOF LL] cur=-6.95557 | reg=-6.95593 (\u0394=-0.00035) | equal=-7.03691 (\u0394=-0.08134)\nSaved submissions: _cur, _reg (ws>=0.15, wl>=0.05 in long), _equal. Diagnostics complete. Elapsed 3.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/2968960644.py:224: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', group_keys=False).apply(enforce_non_increasing)\n/tmp/ipykernel_4688/2968960644.py:229: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', group_keys=False).apply(lambda g: g.sort_values('dist').assign(Sigma=np.maximum.accumulate(g['Sigma'])))\n/tmp/ipykernel_4688/2968960644.py:224: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', group_keys=False).apply(enforce_non_increasing)\n/tmp/ipykernel_4688/2968960644.py:229: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', group_keys=False).apply(lambda g: g.sort_values('dist').assign(Sigma=np.maximum.accumulate(g['Sigma'])))\n/tmp/ipykernel_4688/2968960644.py:224: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', group_keys=False).apply(enforce_non_increasing)\n/tmp/ipykernel_4688/2968960644.py:229: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', group_keys=False).apply(lambda g: g.sort_values('dist').assign(Sigma=np.maximum.accumulate(g['Sigma'])))\n"
          ]
        }
      ]
    },
    {
      "id": "402d4b93-b8f3-4592-a0fb-9f54157052bd",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sigma via quantile-band tuned against final blended OOF FVC; apply to test deltas; keep FVC from latest distance blend\n",
        "import numpy as np, pandas as pd, gc, warnings, time\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "def laplace_ll_np(y_true, y_pred, sigma):\n",
        "    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float); sigma = np.asarray(sigma, float)\n",
        "    delta = np.minimum(np.abs(y_true - y_pred), 1000.0)\n",
        "    sigma = np.maximum(sigma, 70.0)\n",
        "    return float(np.mean(-delta / sigma - np.log(sigma)))\n",
        "\n",
        "# Helper OOF builders (reuse earlier helpers already defined in notebook):\n",
        "def slope_anchor_oof(train_df, n_splits=5, seed=42):\n",
        "    from sklearn.linear_model import Ridge\n",
        "    from sklearn.neighbors import KNeighborsRegressor\n",
        "    gkf = GroupKFold(n_splits=n_splits)\n",
        "    groups = train_df['Patient'].values\n",
        "    y_list, d_list, fvc_slope_list, fvc_anchor_list = [], [], [], []\n",
        "    pid_list, week_list = [], []\n",
        "    for fold, (trn_idx, val_idx) in enumerate(gkf.split(train_df, groups=groups), 1):\n",
        "        trn = train_df.iloc[trn_idx].copy(); val = train_df.iloc[val_idx].copy()\n",
        "        base_trn = prepare_baseline_table(trn); base_val = prepare_baseline_table(val)\n",
        "        slopes_tr = compute_patient_slopes(trn)\n",
        "        lab = pd.DataFrame({'Patient': list(slopes_tr.keys()), 's_label': list(slopes_tr.values())})\n",
        "        bf_trn, feat_cols, ecdf_bf, ecdf_pc, cats = build_slope_features(base_trn.merge(lab, on='Patient', how='left'), fit=True)\n",
        "        bf_val, _, _, _, _ = build_slope_features(base_val, ecdf_bf, ecdf_pc, cats, fit=False)\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "        X_tr = bf_trn[feat_cols].values.astype(float); y_tr = bf_trn['s_label'].fillna(0.0).values.astype(float)\n",
        "        X_trs = scaler.fit_transform(X_tr); X_vs = scaler.transform(bf_val[feat_cols].values.astype(float))\n",
        "        ridge = Ridge(alpha=1.0, random_state=seed).fit(X_trs, y_tr)\n",
        "        knn   = KNeighborsRegressor(n_neighbors=9, weights='distance').fit(X_trs, y_tr)\n",
        "        s_r = ridge.predict(X_vs); s_k = knn.predict(X_vs)\n",
        "        q_lo, q_hi = np.percentile(y_tr, [5,95])\n",
        "        s_bl = np.clip(0.80*s_r + 0.20*s_k, q_lo, q_hi)\n",
        "        s_map = dict(zip(base_val['Patient'].values, s_bl))\n",
        "        valm = val.merge(base_val[['Patient','Base_Week','Base_FVC']], on='Patient', how='left')\n",
        "        mask = (valm['Weeks'] >= valm['Base_Week'])\n",
        "        dist = (valm['Weeks'] - valm['Base_Week']).astype(float)\n",
        "        fvc_slope = (valm['Base_FVC'].values + valm['Patient'].map(s_map).fillna(0.0).values * dist).astype(float)\n",
        "        gs_fold = robust_global_slope(compute_patient_slopes(trn))\n",
        "        fvc_anchor = (valm['Base_FVC'].values + gs_fold * dist).astype(float)\n",
        "        y_list.append(valm.loc[mask, 'FVC'].values.astype(float))\n",
        "        d_list.append(dist.values[mask].astype(float))\n",
        "        fvc_slope_list.append(fvc_slope[mask].astype(float))\n",
        "        fvc_anchor_list.append(fvc_anchor[mask].astype(float))\n",
        "        pid_list.append(valm.loc[mask, 'Patient'].astype(str).values)\n",
        "        week_list.append(valm.loc[mask, 'Weeks'].astype(int).values)\n",
        "        del trn, val, base_trn, base_val, bf_trn, bf_val, X_tr, X_trs, X_vs\n",
        "        gc.collect()\n",
        "    return (np.concatenate(y_list), np.concatenate(d_list),\n",
        "            np.concatenate(fvc_slope_list), np.concatenate(fvc_anchor_list),\n",
        "            np.concatenate(pid_list), np.concatenate(week_list))\n",
        "\n",
        "def lme_oof(train_df, n_splits=5):\n",
        "    gkf = GroupKFold(n_splits=n_splits)\n",
        "    groups = train_df['Patient'].values\n",
        "    y_list, d_list, fvc_list = [], [], []\n",
        "    pid_list, week_list = [], []\n",
        "    for fold, (trn_idx, val_idx) in enumerate(gkf.split(train_df, groups=groups), 1):\n",
        "        trn = train_df.iloc[trn_idx].copy(); val = train_df.iloc[val_idx].copy()\n",
        "        base_trn = prepare_baseline_table(trn); base_val = prepare_baseline_table(val)\n",
        "        trn_l = trn.drop(columns=['Age','Sex','SmokingStatus'], errors='ignore') \\\n",
        "                   .merge(base_trn[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "        trn_l['Weeks_Passed'] = (trn_l['Weeks'] - trn_l['Base_Week']).astype(float)/10.0\n",
        "        trn_l = trn_l[trn_l['Weeks_Passed'] >= 0].copy()\n",
        "        age_mean, age_std = trn_l['Age'].mean(), trn_l['Age'].std()+1e-9\n",
        "        pc_mean, pc_std   = trn_l['Percent_at_base'].mean(), trn_l['Percent_at_base'].std()+1e-9\n",
        "        trn_l['Age_std'] = (trn_l['Age'] - age_mean)/age_std\n",
        "        trn_l['Percent_at_base_std'] = (trn_l['Percent_at_base'] - pc_mean)/pc_std\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter('ignore')\n",
        "            md = smf.mixedlm('FVC ~ 1 + Weeks_Passed + I(Weeks_Passed**2) + Age_std + C(Sex) + C(SmokingStatus) + Percent_at_base_std + Age_std:Percent_at_base_std',\n",
        "                              data=trn_l, groups=trn_l['Patient'], re_formula='~Weeks_Passed')\n",
        "            mdf = md.fit(method='lbfgs', reml=True, maxiter=500, disp=False)\n",
        "        val_left = val.drop(columns=['Age','Sex','SmokingStatus'], errors='ignore')\n",
        "        val_l = val_left.merge(base_val[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "        mask = (val_l['Weeks'] >= val_l['Base_Week'])\n",
        "        dist = (val_l['Weeks'] - val_l['Base_Week']).astype(float)\n",
        "        val_l['Weeks_Passed'] = dist/10.0\n",
        "        val_l['Age_std'] = (val_l['Age'] - age_mean)/age_std\n",
        "        val_l['Percent_at_base_std'] = (val_l['Percent_at_base'] - pc_mean)/pc_std\n",
        "        fvc_pred = mdf.predict(val_l).astype(float).values\n",
        "        y_list.append(val_l.loc[mask, 'FVC'].values.astype(float))\n",
        "        d_list.append(dist.values[mask].astype(float))\n",
        "        fvc_list.append(fvc_pred[mask].astype(float))\n",
        "        pid_list.append(val_l.loc[mask, 'Patient'].astype(str).values)\n",
        "        week_list.append(val_l.loc[mask, 'Weeks'].astype(int).values)\n",
        "        del trn, val, base_trn, base_val, trn_l, val_l\n",
        "        gc.collect()\n",
        "    return np.concatenate(y_list), np.concatenate(d_list), np.concatenate(fvc_list), np.concatenate(pid_list), np.concatenate(week_list)\n",
        "\n",
        "# 1) Build aligned OOF for Slope+Anchor, LME, and Quantile q50+per-fold anchor\n",
        "print('[Sigma-QBand] Building OOF sources for blend alignment...', flush=True)\n",
        "y_s, d_s, fvc_s, fvc_a, pid_s, wk_s = slope_anchor_oof(train, 5, 42)\n",
        "y_l, d_l, fvc_l, pid_l, wk_l = lme_oof(train, 5)\n",
        "oof_q = pd.read_csv('oof_quantile_lgbm_v2.csv')\n",
        "train_base = prepare_baseline_table(train)\n",
        "oof_q = oof_q.merge(train_base[['Patient','Base_Week','Base_FVC']], on='Patient', how='left', suffixes=('', '_base'))\n",
        "if 'Base_FVC_base' in oof_q.columns:\n",
        "    if 'Base_FVC' not in oof_q.columns: oof_q['Base_FVC'] = oof_q['Base_FVC_base']\n",
        "    else: oof_q['Base_FVC'] = oof_q['Base_FVC'].fillna(oof_q['Base_FVC_base'])\n",
        "    oof_q.drop(columns=['Base_FVC_base'], inplace=True)\n",
        "if 'Base_Week_base' in oof_q.columns and 'Base_Week' not in oof_q.columns:\n",
        "    oof_q['Base_Week'] = oof_q['Base_Week_base']\n",
        "    oof_q.drop(columns=['Base_Week_base'], inplace=True)\n",
        "oof_q['dist'] = (oof_q['Weeks'] - oof_q['Base_Week']).astype(float)\n",
        "oof_q = oof_q[oof_q['dist'] >= 0].dropna(subset=['q50_delta_oof']).copy()\n",
        "\n",
        "# Per-fold anchor for quantile OOF\n",
        "N_SPLITS = 5\n",
        "gkf = GroupKFold(n_splits=N_SPLITS)\n",
        "groups = train['Patient'].values\n",
        "patient_to_fold = {}; fold_to_gs = {}\n",
        "for fold, (trn_idx, val_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    trn_df = train.iloc[trn_idx]\n",
        "    gs_fold = robust_global_slope(compute_patient_slopes(trn_df))\n",
        "    fold_to_gs[fold] = gs_fold\n",
        "    for p in train.iloc[val_idx]['Patient'].astype(str).unique():\n",
        "        patient_to_fold[p] = fold\n",
        "oof_q['fold'] = oof_q['Patient'].astype(str).map(patient_to_fold).astype(int)\n",
        "oof_q['gs_fold'] = oof_q['fold'].map(fold_to_gs).astype(float)\n",
        "fvc_q_point = oof_q['Base_FVC'].astype(float).values + oof_q['q50_delta_oof'].astype(float).values\n",
        "fvc_q = 0.70 * fvc_q_point + 0.30 * (oof_q['Base_FVC'].astype(float).values + oof_q['gs_fold'].values * oof_q['dist'].astype(float).values)\n",
        "\n",
        "# Align by keys\n",
        "df_s = pd.DataFrame({'Patient': pid_s.astype(str), 'Weeks': wk_s.astype(int), 'y_true': y_s.astype(float), 'dist': d_s.astype(float), 'fvc_s': fvc_s.astype(float), 'fvc_a': fvc_a.astype(float)})\n",
        "df_l = pd.DataFrame({'Patient': pid_l.astype(str), 'Weeks': wk_l.astype(int), 'fvc_l': fvc_l.astype(float)})\n",
        "df_q = oof_q[['Patient','Weeks']].astype({'Patient':'str','Weeks':'int'}).copy()\n",
        "df_q['fvc_q'] = fvc_q.astype(float)\n",
        "df_q['band'] = (oof_q['q80_delta_oof'] - oof_q['q20_delta_oof']).abs().astype(float).values\n",
        "dfm = df_s.merge(df_l, on=['Patient','Weeks'], how='inner').merge(df_q, on=['Patient','Weeks'], how='inner')\n",
        "y = dfm['y_true'].values.astype(float)\n",
        "dist = dfm['dist'].values.astype(float)\n",
        "s = dfm['fvc_s'].values.astype(float)\n",
        "a = dfm['fvc_a'].values.astype(float)\n",
        "l = dfm['fvc_l'].values.astype(float)\n",
        "q = dfm['fvc_q'].values.astype(float)\n",
        "band_oof = dfm['band'].values.astype(float)\n",
        "sigma_banker_oof = np.maximum(240.0 + 3.0 * np.abs(dist), 70.0)\n",
        "sigma_banker_oof = np.where(np.abs(dist) > 20.0, np.maximum(sigma_banker_oof, 100.0), sigma_banker_oof)\n",
        "\n",
        "# 2) Recompute OOF distance-aware weights on bins (same protocol as Cell 20) with banker sigma\n",
        "def grid_best(y, s, l, q, sigma, w_grid=np.arange(0.0, 1.01, 0.05)):\n",
        "    best_ll, best_w = -1e9, (0.3, 0.3, 0.4)\n",
        "    for ws in w_grid:\n",
        "        for wl in w_grid:\n",
        "            wq = 1.0 - ws - wl\n",
        "            if wq < 0 or wq > 1: continue\n",
        "            pred = ws*s + wl*l + wq*q\n",
        "            ll = laplace_ll_np(y, pred, sigma)\n",
        "            if ll > best_ll:\n",
        "                best_ll, best_w = ll, (ws, wl, wq)\n",
        "    return best_ll, best_w\n",
        "\n",
        "bins = [(0.0, 5.0), (5.0, 15.0), (15.0, 1e9)]\n",
        "best_w = {}\n",
        "for lo, hi in bins:\n",
        "    m = (np.abs(dist) > lo) & (np.abs(dist) <= hi)\n",
        "    if not m.any():\n",
        "        best_w[(lo,hi)] = (0.30, 0.30, 0.40)\n",
        "        print(f'[Sigma-QBand] Bin {lo}-{hi} empty; default 0.30/0.30/0.40')\n",
        "        continue\n",
        "    ll, w = grid_best(y[m], s[m], l[m], q[m], sigma_banker_oof[m])\n",
        "    best_w[(lo,hi)] = w\n",
        "    print(f'[Sigma-QBand] Bin {lo}-{hi} weights S/L/Q = {w[0]:.2f}/{w[1]:.2f}/{w[2]:.2f} (OOF LL={ll:.5f})')\n",
        "\n",
        "# Build blended OOF FVC per row using bin weights\n",
        "fvc_blend_oof = np.zeros_like(y)\n",
        "for (lo, hi), (ws, wl, wq) in best_w.items():\n",
        "    m = (np.abs(dist) > lo) & (np.abs(dist) <= hi)\n",
        "    if np.any(m):\n",
        "        fvc_blend_oof[m] = ws*s[m] + wl*l[m] + wq*q[m]\n",
        "\n",
        "# 3) Tune c per distance bin on OOF against blended FVC: sigma = max(band/c, banker); dist==0 -> 70\n",
        "c_grid = [1.2, 1.4, 1.6, 1.8, 2.0, 2.2]\n",
        "best_c = {}\n",
        "for lo, hi in bins:\n",
        "    m = (np.abs(dist) > lo) & (np.abs(dist) <= hi)\n",
        "    if not m.any():\n",
        "        best_c[(lo,hi)] = 1.8\n",
        "        print(f'[Sigma-QBand] ({lo},{hi}] empty; default c=1.8')\n",
        "        continue\n",
        "    b_ll, b_c = -1e9, None\n",
        "    for c in c_grid:\n",
        "        sig = np.maximum(band_oof[m] / c, sigma_banker_oof[m])\n",
        "        # Special-case: dist==0 -> sigma=70\n",
        "        z = (np.abs(dist[m]) == 0.0)\n",
        "        if np.any(z):\n",
        "            sig[z] = np.maximum(70.0, sig[z])\n",
        "        ll = laplace_ll_np(y[m], fvc_blend_oof[m], sig)\n",
        "        if ll > b_ll:\n",
        "            b_ll, b_c = ll, c\n",
        "    best_c[(lo,hi)] = b_c\n",
        "    print(f\"[Sigma-QBand] ({lo},{hi}] best c={b_c:.2f} (OOF LL={b_ll:.5f})\")\n",
        "\n",
        "# 4) Apply tuned c to TEST using saved quantile delta predictions; keep FVC from latest distance-aware blend\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "pred_d = pd.read_csv('pred_quantile_deltas_v2.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "q20_d = pred_d['q20_d'].astype(float).values\n",
        "q80_d = pred_d['q80_d'].astype(float).values\n",
        "band_te = np.abs(q80_d - q20_d).astype(float)\n",
        "\n",
        "# Build |dist| for bins and banker floor\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]; grid['Weeks'] = parts[1].astype(int)\n",
        "test_bl = test[['Patient','Weeks','FVC','Percent']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_bl, on='Patient', how='left')\n",
        "abs_dist_te = (grid['Weeks'] - grid['Base_Week']).abs().astype(float).values\n",
        "sigma_banker_te = np.maximum(240.0 + 3.0 * abs_dist_te, 70.0)\n",
        "sigma_banker_te = np.where(abs_dist_te > 20.0, np.maximum(sigma_banker_te, 100.0), sigma_banker_te)\n",
        "\n",
        "# Compute sigma per row using bin's c and floor by banker; dist==0 -> 70\n",
        "sigma_from_band = np.zeros_like(band_te, dtype=float)\n",
        "for (lo, hi), c in best_c.items():\n",
        "    m = (abs_dist_te > lo) & (abs_dist_te <= hi)\n",
        "    if np.any(m):\n",
        "        sigma_from_band[m] = band_te[m] / c\n",
        "sigma_te = np.maximum(sigma_from_band, sigma_banker_te)\n",
        "sigma_te = np.where(abs_dist_te == 0.0, 70.0, sigma_te)\n",
        "\n",
        "# Per-patient monotone in |dist|\n",
        "df_sig = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': abs_dist_te, 'Sigma': sigma_te.astype(float)})\n",
        "def enforce_sigma_monotone(g):\n",
        "    g = g.sort_values('dist').copy()\n",
        "    g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "    return g\n",
        "df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n",
        "sigma_final = df_sig['Sigma'].values.astype(float)\n",
        "\n",
        "# 5) Build final submission with FVC from latest distance blend and new sigma\n",
        "sub_fvc = pd.read_csv('submission_distance_blend.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "sub_new_sigma = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': sub_fvc['FVC'].astype(float).values, 'Confidence': sigma_final})\n",
        "sub_new_sigma.to_csv('submission_distance_blend_sigma_qband.csv', index=False)\n",
        "sub_new_sigma.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_distance_blend_sigma_qband.csv and set submission.csv (OOF-aligned tuned q-band sigma floored by banker; dist==0 -> 70; monotone per patient).')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sigma-QBand] Building OOF sources for blend alignment...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sigma-QBand] Bin 0.0-5.0 weights S/L/Q = 0.05/0.00/0.95 (OOF LL=-6.04045)\n[Sigma-QBand] Bin 5.0-15.0 weights S/L/Q = 0.00/0.00/1.00 (OOF LL=-6.12583)\n[Sigma-QBand] Bin 15.0-1000000000.0 weights S/L/Q = 0.05/0.05/0.90 (OOF LL=-6.39437)\n[Sigma-QBand] (0.0,5.0] best c=1.40 (OOF LL=-6.04044)\n[Sigma-QBand] (5.0,15.0] best c=1.60 (OOF LL=-6.12583)\n[Sigma-QBand] (15.0,1000000000.0] best c=1.80 (OOF LL=-6.39437)\nSaved submission_distance_blend_sigma_qband.csv and set submission.csv (OOF-aligned tuned q-band sigma floored by banker; dist==0 -> 70; monotone per patient).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/3326674162.py:229: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n"
          ]
        }
      ]
    },
    {
      "id": "662dba79-2181-4f94-aa24-278bd1e41e6f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sigma variant: asymmetric quantile band selection in long bins, optional 15-30 split; apply to test, keep FVC from distance blend\n",
        "import numpy as np, pandas as pd, gc, warnings\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "def laplace_ll_np(y_true, y_pred, sigma):\n",
        "    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float); sigma = np.asarray(sigma, float)\n",
        "    delta = np.minimum(np.abs(y_true - y_pred), 1000.0)\n",
        "    sigma = np.maximum(sigma, 70.0)\n",
        "    return float(np.mean(-delta / sigma - np.log(sigma)))\n",
        "\n",
        "# 1) Rebuild aligned OOF using cell 23 logic but only to get blended OOF preds and quantile bands\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "oof_q = pd.read_csv('oof_quantile_lgbm_v2.csv')\n",
        "train_base = prepare_baseline_table(train)\n",
        "oof_q = oof_q.merge(train_base[['Patient','Base_Week','Base_FVC']], on='Patient', how='left', suffixes=('', '_base'))\n",
        "if 'Base_FVC_base' in oof_q.columns:\n",
        "    if 'Base_FVC' not in oof_q.columns: oof_q['Base_FVC'] = oof_q['Base_FVC_base']\n",
        "    else: oof_q['Base_FVC'] = oof_q['Base_FVC'].fillna(oof_q['Base_FVC_base'])\n",
        "    oof_q.drop(columns=['Base_FVC_base'], inplace=True)\n",
        "if 'Base_Week_base' in oof_q.columns and 'Base_Week' not in oof_q.columns:\n",
        "    oof_q['Base_Week'] = oof_q['Base_Week_base']\n",
        "    oof_q.drop(columns=['Base_Week_base'], inplace=True)\n",
        "oof_q['dist'] = (oof_q['Weeks'] - oof_q['Base_Week']).astype(float)\n",
        "oof_q = oof_q[oof_q['dist'] >= 0].dropna(subset=['q50_delta_oof']).copy()\n",
        "\n",
        "# Per-fold anchor for quantile OOF\n",
        "N_SPLITS = 5\n",
        "gkf = GroupKFold(n_splits=N_SPLITS)\n",
        "groups = train['Patient'].values\n",
        "patient_to_fold, fold_to_gs = {}, {}\n",
        "for fold, (trn_idx, val_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    trn_df = train.iloc[trn_idx]\n",
        "    gs_fold = robust_global_slope(compute_patient_slopes(trn_df))\n",
        "    fold_to_gs[fold] = gs_fold\n",
        "    for p in train.iloc[val_idx]['Patient'].astype(str).unique():\n",
        "        patient_to_fold[p] = fold\n",
        "oof_q['fold'] = oof_q['Patient'].astype(str).map(patient_to_fold).astype(int)\n",
        "oof_q['gs_fold'] = oof_q['fold'].map(fold_to_gs).astype(float)\n",
        "\n",
        "# Build Slope+Anchor and LME OOF minimal (reuse quick builders from cell 23)\n",
        "def slope_anchor_oof_min(train_df, n_splits=5, seed=42):\n",
        "    from sklearn.linear_model import Ridge\n",
        "    from sklearn.neighbors import KNeighborsRegressor\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    gkf = GroupKFold(n_splits=n_splits); groups = train_df['Patient'].values\n",
        "    y_list, d_list, fvc_s_list, fvc_a_list, pid_list, wk_list = [], [], [], [], [], []\n",
        "    for fold, (trn_idx, val_idx) in enumerate(gkf.split(train_df, groups=groups), 1):\n",
        "        trn = train_df.iloc[trn_idx].copy(); val = train_df.iloc[val_idx].copy()\n",
        "        base_trn = prepare_baseline_table(trn); base_val = prepare_baseline_table(val)\n",
        "        slopes_tr = compute_patient_slopes(trn)\n",
        "        lab = pd.DataFrame({'Patient': list(slopes_tr.keys()), 's_label': list(slopes_tr.values())})\n",
        "        bf_trn, feat_cols, ecdf_bf, ecdf_pc, cats = build_slope_features(base_trn.merge(lab, on='Patient', how='left'), fit=True)\n",
        "        bf_val, _, _, _, _ = build_slope_features(base_val, ecdf_bf, ecdf_pc, cats, fit=False)\n",
        "        sc = StandardScaler(with_mean=True, with_std=True)\n",
        "        X_tr = bf_trn[feat_cols].values.astype(float); y_tr = bf_trn['s_label'].fillna(0.0).values.astype(float)\n",
        "        X_trs = sc.fit_transform(X_tr); X_vs = sc.transform(bf_val[feat_cols].values.astype(float))\n",
        "        ridge = Ridge(alpha=1.0, random_state=seed).fit(X_trs, y_tr)\n",
        "        knn = KNeighborsRegressor(n_neighbors=9, weights='distance').fit(X_trs, y_tr)\n",
        "        s_r = ridge.predict(X_vs); s_k = knn.predict(X_vs)\n",
        "        q_lo, q_hi = np.percentile(y_tr, [5,95])\n",
        "        s_bl = np.clip(0.80*s_r + 0.20*s_k, q_lo, q_hi)\n",
        "        s_map = dict(zip(base_val['Patient'].values, s_bl))\n",
        "        valm = val.merge(base_val[['Patient','Base_Week','Base_FVC']], on='Patient', how='left')\n",
        "        mask = (valm['Weeks'] >= valm['Base_Week'])\n",
        "        dist = (valm['Weeks'] - valm['Base_Week']).astype(float)\n",
        "        fvc_s = (valm['Base_FVC'].values + valm['Patient'].map(s_map).fillna(0.0).values * dist).astype(float)\n",
        "        gs_fold = robust_global_slope(compute_patient_slopes(trn))\n",
        "        fvc_a = (valm['Base_FVC'].values + gs_fold * dist).astype(float)\n",
        "        y_list.append(valm.loc[mask, 'FVC'].values.astype(float)); d_list.append(dist.values[mask].astype(float))\n",
        "        fvc_s_list.append(fvc_s[mask].astype(float)); fvc_a_list.append(fvc_a[mask].astype(float))\n",
        "        pid_list.append(valm.loc[mask, 'Patient'].astype(str).values); wk_list.append(valm.loc[mask, 'Weeks'].astype(int).values)\n",
        "    return (np.concatenate(y_list), np.concatenate(d_list), np.concatenate(fvc_s_list), np.concatenate(fvc_a_list), np.concatenate(pid_list), np.concatenate(wk_list))\n",
        "\n",
        "def lme_oof_min(train_df, n_splits=5):\n",
        "    import statsmodels.formula.api as smf, warnings\n",
        "    gkf = GroupKFold(n_splits=n_splits); groups = train_df['Patient'].values\n",
        "    y_list, d_list, fvc_list, pid_list, wk_list = [], [], [], [], []\n",
        "    for fold, (trn_idx, val_idx) in enumerate(gkf.split(train_df, groups=groups), 1):\n",
        "        trn = train_df.iloc[trn_idx].copy(); val = train_df.iloc[val_idx].copy()\n",
        "        base_trn = prepare_baseline_table(trn); base_val = prepare_baseline_table(val)\n",
        "        trn_l = trn.drop(columns=['Age','Sex','SmokingStatus'], errors='ignore').merge(base_trn[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "        trn_l['Weeks_Passed'] = (trn_l['Weeks'] - trn_l['Base_Week']).astype(float)/10.0\n",
        "        trn_l = trn_l[trn_l['Weeks_Passed'] >= 0].copy()\n",
        "        age_mean, age_std = trn_l['Age'].mean(), trn_l['Age'].std()+1e-9\n",
        "        pc_mean, pc_std   = trn_l['Percent_at_base'].mean(), trn_l['Percent_at_base'].std()+1e-9\n",
        "        trn_l['Age_std'] = (trn_l['Age'] - age_mean)/age_std; trn_l['Percent_at_base_std'] = (trn_l['Percent_at_base'] - pc_mean)/pc_std\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter('ignore')\n",
        "            md = smf.mixedlm('FVC ~ 1 + Weeks_Passed + I(Weeks_Passed**2) + Age_std + C(Sex) + C(SmokingStatus) + Percent_at_base_std + Age_std:Percent_at_base_std',\n",
        "                              data=trn_l, groups=trn_l['Patient'], re_formula='~Weeks_Passed')\n",
        "            mdf = md.fit(method='lbfgs', reml=True, maxiter=500, disp=False)\n",
        "        val_left = val.drop(columns=['Age','Sex','SmokingStatus'], errors='ignore')\n",
        "        val_l = val_left.merge(base_val[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "        mask = (val_l['Weeks'] >= val_l['Base_Week'])\n",
        "        dist = (val_l['Weeks'] - val_l['Base_Week']).astype(float)\n",
        "        val_l['Weeks_Passed'] = dist/10.0\n",
        "        val_l['Age_std'] = (val_l['Age'] - age_mean)/age_std\n",
        "        val_l['Percent_at_base_std'] = (val_l['Percent_at_base'] - pc_mean)/pc_std\n",
        "        fvc_pred = mdf.predict(val_l).astype(float).values\n",
        "        y_list.append(val_l.loc[mask, 'FVC'].values.astype(float)); d_list.append(dist.values[mask].astype(float)); fvc_list.append(fvc_pred[mask].astype(float))\n",
        "        pid_list.append(val_l.loc[mask, 'Patient'].astype(str).values); wk_list.append(val_l.loc[mask, 'Weeks'].astype(int).values)\n",
        "    return np.concatenate(y_list), np.concatenate(d_list), np.concatenate(fvc_list), np.concatenate(pid_list), np.concatenate(wk_list)\n",
        "\n",
        "y_s, d_s, fvc_s, fvc_a, pid_s, wk_s = slope_anchor_oof_min(train, 5, 42)\n",
        "y_l, d_l, fvc_l, pid_l, wk_l = lme_oof_min(train, 5)\n",
        "\n",
        "# Align OOF sources\n",
        "df_s = pd.DataFrame({'Patient': pid_s.astype(str), 'Weeks': wk_s.astype(int), 'y_true': y_s.astype(float), 'dist': d_s.astype(float), 'fvc_s': fvc_s.astype(float), 'fvc_a': fvc_a.astype(float)})\n",
        "df_l = pd.DataFrame({'Patient': pid_l.astype(str), 'Weeks': wk_l.astype(int), 'fvc_l': fvc_l.astype(float)})\n",
        "# Bring gs_fold directly into df_q via merge to avoid indexing length mismatches\n",
        "df_q = oof_q[['Patient','Weeks','q10_delta_oof','q20_delta_oof','q50_delta_oof','q80_delta_oof','q90_delta_oof','Base_FVC','dist','gs_fold']].astype({'Patient':'str','Weeks':'int'})\n",
        "fvc_q = 0.70 * (df_q['Base_FVC'].astype(float) + df_q['q50_delta_oof'].astype(float)) + 0.30 * (df_q['Base_FVC'].astype(float) + df_q['gs_fold'].astype(float) * df_q['dist'].astype(float))\n",
        "df_q['fvc_q'] = fvc_q.astype(float)\n",
        "df_q['band_sym'] = (df_q['q80_delta_oof'] - df_q['q20_delta_oof']).abs().astype(float)\n",
        "df_q['band_asym'] = np.maximum((df_q['q90_delta_oof'] - df_q['q50_delta_oof']).abs().astype(float), (df_q['q50_delta_oof'] - df_q['q10_delta_oof']).abs().astype(float))\n",
        "dfm = df_s.merge(df_l, on=['Patient','Weeks'], how='inner').merge(df_q[['Patient','Weeks','fvc_q','band_sym','band_asym']], on=['Patient','Weeks'], how='inner')\n",
        "\n",
        "y = dfm['y_true'].values.astype(float)\n",
        "dist = dfm['dist'].values.astype(float)\n",
        "s = dfm['fvc_s'].values.astype(float)\n",
        "a = dfm['fvc_a'].values.astype(float)\n",
        "lme = dfm['fvc_l'].values.astype(float)\n",
        "qpt = dfm['fvc_q'].values.astype(float)\n",
        "band_sym = dfm['band_sym'].values.astype(float)\n",
        "band_asym = dfm['band_asym'].values.astype(float)\n",
        "\n",
        "# Recompute OOF distance-aware weights using banker sigma (same as cell 23)\n",
        "sigma_banker_oof = np.maximum(240.0 + 3.0 * np.abs(dist), 70.0)\n",
        "sigma_banker_oof = np.where(np.abs(dist) > 20.0, np.maximum(sigma_banker_oof, 100.0), sigma_banker_oof)\n",
        "\n",
        "def grid_best(y, s, l, q, sigma, w_grid=np.arange(0.0, 1.01, 0.05)):\n",
        "    best_ll, best_w = -1e9, (0.3, 0.3, 0.4)\n",
        "    for ws in w_grid:\n",
        "        for wl in w_grid:\n",
        "            wq = 1.0 - ws - wl\n",
        "            if wq < 0 or wq > 1: continue\n",
        "            pred = ws*s + wl*l + wq*q\n",
        "            ll = laplace_ll_np(y, pred, sigma)\n",
        "            if ll > best_ll:\n",
        "                best_ll, best_w = ll, (ws, wl, wq)\n",
        "    return best_ll, best_w\n",
        "\n",
        "bins = [(0.0,5.0),(5.0,15.0),(15.0,30.0),(30.0,1e9)]\n",
        "best_w = {}\n",
        "for lo, hi in bins:\n",
        "    m = (np.abs(dist) > lo) & (np.abs(dist) <= hi)\n",
        "    if not m.any():\n",
        "        best_w[(lo,hi)] = (0.05, 0.05, 0.90) if lo>=15.0 else (0.05, 0.00, 0.95)\n",
        "        continue\n",
        "    ll, w = grid_best(y[m], s[m], lme[m], qpt[m], sigma_banker_oof[m])\n",
        "    best_w[(lo,hi)] = w\n",
        "\n",
        "fvc_blend_oof = np.zeros_like(y)\n",
        "for (lo,hi), (ws, wl, wq) in best_w.items():\n",
        "    m = (np.abs(dist) > lo) & (np.abs(dist) <= hi)\n",
        "    fvc_blend_oof[m] = ws*s[m] + wl*lme[m] + wq*qpt[m]\n",
        "\n",
        "# 2) For each bin, choose band type (sym for short/mid, choose sym vs asym for long bins) and tune c in {1.2..2.2}\n",
        "c_grid = [1.2, 1.4, 1.6, 1.8, 2.0, 2.2]\n",
        "best_c, best_band_type = {}, {}\n",
        "for lo, hi in bins:\n",
        "    m = (np.abs(dist) > lo) & (np.abs(dist) <= hi)\n",
        "    if not m.any():\n",
        "        best_c[(lo,hi)] = 1.8; best_band_type[(lo,hi)] = 'sym' if hi<=15.0 else 'sym'\n",
        "        continue\n",
        "    # band candidates\n",
        "    bands = {'sym': band_sym[m]} if hi <= 15.0 else {'sym': band_sym[m], 'asym': band_asym[m]}\n",
        "    best_ll, sel_c, sel_type = -1e9, 1.8, 'sym'\n",
        "    for btype, bvals in bands.items():\n",
        "        for c in c_grid:\n",
        "            sig = np.maximum(bvals / c, sigma_banker_oof[m])\n",
        "            ll = laplace_ll_np(y[m], fvc_blend_oof[m], sig)\n",
        "            if ll > best_ll:\n",
        "                best_ll, sel_c, sel_type = ll, c, btype\n",
        "    best_c[(lo,hi)] = sel_c; best_band_type[(lo,hi)] = sel_type\n",
        "    print(f\"[Sigma-ASYM] Bin ({lo},{hi}] type={sel_type} c={sel_c:.2f} OOF LL={best_ll:.5f}\")\n",
        "\n",
        "# 3) Apply to TEST deltas using selected band per bin, banker floor, dist==0->70, +5 stabilizer for |dist|>15\n",
        "pred_d = pd.read_csv('pred_quantile_deltas_v2.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "q10_d = pred_d['q10_d'].astype(float).values\n",
        "q20_d = pred_d['q20_d'].astype(float).values\n",
        "q50_d = pred_d['q50_d'].astype(float).values\n",
        "q80_d = pred_d['q80_d'].astype(float).values\n",
        "q90_d = pred_d['q90_d'].astype(float).values\n",
        "band_sym_te = np.abs(q80_d - q20_d).astype(float)\n",
        "band_asym_te = np.maximum(np.abs(q90_d - q50_d), np.abs(q50_d - q10_d)).astype(float)\n",
        "\n",
        "grid_te = ss.copy()\n",
        "parts = grid_te['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid_te['Patient'] = parts[0]; grid_te['Weeks'] = parts[1].astype(int)\n",
        "test_bl = test[['Patient','Weeks','FVC','Percent']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid_te = grid_te.merge(test_bl, on='Patient', how='left')\n",
        "abs_dist_te = (grid_te['Weeks'] - grid_te['Base_Week']).abs().astype(float).values\n",
        "sigma_banker_te = np.maximum(240.0 + 3.0 * abs_dist_te, 70.0)\n",
        "sigma_banker_te = np.where(abs_dist_te > 20.0, np.maximum(sigma_banker_te, 100.0), sigma_banker_te)\n",
        "\n",
        "sigma_from_band = np.zeros_like(abs_dist_te, dtype=float)\n",
        "for (lo,hi), c in best_c.items():\n",
        "    m = (abs_dist_te > lo) & (abs_dist_te <= hi)\n",
        "    if not np.any(m): continue\n",
        "    btype = best_band_type[(lo,hi)]\n",
        "    bvals = band_sym_te if btype=='sym' else band_asym_te\n",
        "    sigma_from_band[m] = bvals[m] / c\n",
        "sigma_te = np.maximum(sigma_from_band, sigma_banker_te)\n",
        "sigma_te = np.where(abs_dist_te == 0.0, 70.0, sigma_te)\n",
        "sigma_te = np.where(abs_dist_te > 15.0, sigma_te + 5.0, sigma_te)\n",
        "\n",
        "# Per-patient monotone in |dist|\n",
        "df_sig = pd.DataFrame({'Patient': grid_te['Patient'].values, 'Weeks': grid_te['Weeks'].values.astype(int), 'dist': abs_dist_te, 'Sigma': sigma_te.astype(float)})\n",
        "def enforce_sigma_monotone(g):\n",
        "    g = g.sort_values('dist').copy()\n",
        "    g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "    return g\n",
        "df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n",
        "sigma_final = df_sig['Sigma'].values.astype(float)\n",
        "\n",
        "# 4) Save submission with FVC from distance blend\n",
        "sub_fvc = pd.read_csv('submission_distance_blend.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "sub_new = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': sub_fvc['FVC'].astype(float).values, 'Confidence': sigma_final})\n",
        "sub_new.to_csv('submission_distance_blend_sigma_qband_asym.csv', index=False)\n",
        "sub_new.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_distance_blend_sigma_qband_asym.csv and set submission.csv (bin-wise band type selection with asym in long bins, banker floor, +5 past 15w, monotone).')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sigma-ASYM] Bin (0.0,5.0] type=sym c=1.40 OOF LL=-6.04044\n[Sigma-ASYM] Bin (5.0,15.0] type=sym c=1.60 OOF LL=-6.12583\n[Sigma-ASYM] Bin (15.0,30.0] type=sym c=1.60 OOF LL=-6.31705\n[Sigma-ASYM] Bin (30.0,1000000000.0] type=asym c=1.60 OOF LL=-6.43738\nSaved submission_distance_blend_sigma_qband_asym.csv and set submission.csv (bin-wise band type selection with asym in long bins, banker floor, +5 past 15w, monotone).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/1234857569.py:214: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n"
          ]
        }
      ]
    },
    {
      "id": "5eee3cb2-657d-49fe-96ad-6ce96f7d2cb5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2-model (q50 + per-fold anchor) with constrained per-bin alpha; retune sigma c per bin; apply to test\n",
        "import numpy as np, pandas as pd, gc, time\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "def laplace_ll_np(y_true, y_pred, sigma):\n",
        "    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float); sigma = np.asarray(sigma, float)\n",
        "    delta = np.minimum(np.abs(y_true - y_pred), 1000.0)\n",
        "    sigma = np.maximum(sigma, 70.0)\n",
        "    return float(np.mean(-delta / sigma - np.log(sigma)))\n",
        "\n",
        "t0 = time.time()\n",
        "# 1) Build OOF for q50 and per-fold anchor; align future rows only\n",
        "oof_q = pd.read_csv('oof_quantile_lgbm_v2.csv')\n",
        "train_base = prepare_baseline_table(train)\n",
        "oof_q = oof_q.merge(train_base[['Patient','Base_Week','Base_FVC']], on='Patient', how='left', suffixes=('', '_base'))\n",
        "if 'Base_FVC_base' in oof_q.columns:\n",
        "    if 'Base_FVC' not in oof_q.columns: oof_q['Base_FVC'] = oof_q['Base_FVC_base']\n",
        "    else: oof_q['Base_FVC'] = oof_q['Base_FVC'].fillna(oof_q['Base_FVC_base'])\n",
        "    oof_q.drop(columns=['Base_FVC_base'], inplace=True)\n",
        "if 'Base_Week_base' in oof_q.columns and 'Base_Week' not in oof_q.columns:\n",
        "    oof_q['Base_Week'] = oof_q['Base_Week_base']\n",
        "    oof_q.drop(columns=['Base_Week_base'], inplace=True)\n",
        "oof_q['dist'] = (oof_q['Weeks'] - oof_q['Base_Week']).astype(float)\n",
        "oof_q = oof_q[oof_q['dist'] >= 0].dropna(subset=['q20_delta_oof','q50_delta_oof','q80_delta_oof']).copy()\n",
        "\n",
        "# Per-fold global slope (anchor) without leak\n",
        "N_SPLITS = 5\n",
        "gkf = GroupKFold(n_splits=N_SPLITS)\n",
        "groups = train['Patient'].values\n",
        "patient_to_fold, fold_to_gs = {}, {}\n",
        "for fold, (trn_idx, val_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    trn_df = train.iloc[trn_idx]\n",
        "    gs_fold = robust_global_slope(compute_patient_slopes(trn_df))\n",
        "    fold_to_gs[fold] = gs_fold\n",
        "    for p in train.iloc[val_idx]['Patient'].astype(str).unique():\n",
        "        patient_to_fold[p] = fold\n",
        "oof_q['fold'] = oof_q['Patient'].astype(str).map(patient_to_fold).astype(int)\n",
        "oof_q['gs_fold'] = oof_q['fold'].map(fold_to_gs).astype(float)\n",
        "\n",
        "fvc_q50_oof = (oof_q['Base_FVC'].astype(float).values + oof_q['q50_delta_oof'].astype(float).values)\n",
        "fvc_anchor_oof = (oof_q['Base_FVC'].astype(float).values + oof_q['gs_fold'].astype(float).values * oof_q['dist'].astype(float).values)\n",
        "y_oof = oof_q['FVC'].astype(float).values\n",
        "dist_oof = oof_q['dist'].astype(float).values\n",
        "abs_dist_oof = np.abs(dist_oof)\n",
        "band_oof = np.abs(oof_q['q80_delta_oof'].astype(float).values - oof_q['q20_delta_oof'].astype(float).values)\n",
        "\n",
        "# Banker sigma OOF\n",
        "sigma_banker_oof = np.maximum(240.0 + 3.0 * abs_dist_oof, 70.0)\n",
        "sigma_banker_oof = np.where(abs_dist_oof > 20.0, np.maximum(sigma_banker_oof, 100.0), sigma_banker_oof)\n",
        "\n",
        "# 2) Use hard-capped alphas = 0.30 across all bins (70/30 q50/anchor), per expert guidance\n",
        "bins = [(0.0,5.0),(5.0,15.0),(15.0,1e9)]\n",
        "masks = [ (abs_dist_oof>lo) & (abs_dist_oof<=hi) for lo,hi in bins ]\n",
        "alpha_s = alpha_m = alpha_l = 0.30\n",
        "print(f\"[2-Model] Using fixed alphas (short,mid,long) = {alpha_s:.2f}, {alpha_m:.2f}, {alpha_l:.2f}\")\n",
        "\n",
        "# Build blended OOF preds with fixed alphas for sigma tuning\n",
        "fvc_blend_oof = np.zeros_like(y_oof)\n",
        "for (lo,hi), m, a in zip(bins, masks, (alpha_s, alpha_m, alpha_l)):\n",
        "    if np.any(m):\n",
        "        fvc_blend_oof[m] = (1.0 - a) * fvc_q50_oof[m] + a * fvc_anchor_oof[m]\n",
        "\n",
        "# 3) Tune sigma c per bin on this OOF: sigma = max(|q80-q20|/c, banker). Optional >=130 for |dist|>30 if OOF-neutral\n",
        "c_grid = [1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2]\n",
        "best_c = {}\n",
        "use_floor130 = False\n",
        "for (lo,hi), m in zip(bins, masks):\n",
        "    if not np.any(m):\n",
        "        best_c[(lo,hi)] = 1.8\n",
        "        print(f'[Sigma-2M] Bin ({lo},{hi}] empty; default c=1.8')\n",
        "        continue\n",
        "    b_ll, b_c = -1e9, 1.8\n",
        "    for c in c_grid:\n",
        "        sig = np.maximum(band_oof[m] / c, sigma_banker_oof[m])\n",
        "        # dist==0 -> 70\n",
        "        z = (abs_dist_oof[m] == 0.0)\n",
        "        if np.any(z): sig[z] = np.maximum(70.0, sig[z])\n",
        "        ll = laplace_ll_np(y_oof[m], fvc_blend_oof[m], sig)\n",
        "        if ll > b_ll: b_ll, b_c = ll, c\n",
        "    best_c[(lo,hi)] = b_c\n",
        "    print(f\"[Sigma-2M] Bin ({lo},{hi}] best c={b_c:.2f} OOF LL={b_ll:.5f}\")\n",
        "\n",
        "# Test >=130 for |dist|>30 as optional floor (OOF-neutral adoption check)\n",
        "m_gt30 = abs_dist_oof > 30.0\n",
        "if np.any(m_gt30):\n",
        "    sig_base = np.zeros_like(abs_dist_oof)\n",
        "    for (lo,hi), m in zip(bins, masks):\n",
        "        if np.any(m):\n",
        "            sig_base[m] = np.maximum(band_oof[m] / best_c[(lo,hi)], sigma_banker_oof[m])\n",
        "    sig_base = np.where(abs_dist_oof == 0.0, np.maximum(70.0, sig_base), sig_base)\n",
        "    ll_no130 = laplace_ll_np(y_oof, fvc_blend_oof, sig_base)\n",
        "    sig_130 = np.where(m_gt30, np.maximum(sig_base, 130.0), sig_base)\n",
        "    ll_130 = laplace_ll_np(y_oof, fvc_blend_oof, sig_130)\n",
        "    if ll_130 >= ll_no130 - 1e-6:\n",
        "        use_floor130 = True\n",
        "    print(f\"[Sigma-2M] >=130 floor test: LL_no130={ll_no130:.5f} LL_130={ll_130:.5f} adopt={use_floor130}\")\n",
        "\n",
        "# 4) Apply to TEST: build q50 and anchor from full-train gs; blend with fixed alphas; build sigma with tuned c\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "pred_d = pd.read_csv('pred_quantile_deltas_v2.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "q20_d = pred_d['q20_d'].astype(float).values\n",
        "q50_d = pred_d['q50_d'].astype(float).values\n",
        "q80_d = pred_d['q80_d'].astype(float).values\n",
        "band_te = np.abs(q80_d - q20_d).astype(float)\n",
        "\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]; grid['Weeks'] = parts[1].astype(int)\n",
        "test_base = test[['Patient','Weeks','FVC','Percent']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_base, on='Patient', how='left')\n",
        "dist_te = (grid['Weeks'] - grid['Base_Week']).astype(float).values\n",
        "abs_dist_te = np.abs(dist_te)\n",
        "base_fvc_te = grid['Base_FVC'].astype(float).values\n",
        "\n",
        "gs_full = robust_global_slope(compute_patient_slopes(train))\n",
        "fvc_q50_te = base_fvc_te + q50_d\n",
        "fvc_anchor_te = base_fvc_te + gs_full * dist_te\n",
        "\n",
        "# Fixed alpha 0.30 across bins\n",
        "alpha_bins = np.zeros_like(abs_dist_te, dtype=float) + 0.30\n",
        "\n",
        "fvc_pred = (1.0 - alpha_bins) * fvc_q50_te + alpha_bins * fvc_anchor_te\n",
        "fvc_pred = np.clip(fvc_pred, 500, 6000)\n",
        "\n",
        "# Apply tolerant non-increasing FVC per patient (+25 ml), then pin dist==0 to Base_FVC\n",
        "df_out = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'FVC': fvc_pred})\n",
        "def enforce_non_increasing_tolerant(g, tol=25.0):\n",
        "    g = g.sort_values('Weeks').copy()\n",
        "    f = g['FVC'].values.astype(float)\n",
        "    for i in range(len(f)-2, -1, -1):\n",
        "        f[i] = min(f[i], f[i+1] + tol)\n",
        "    g['FVC'] = f\n",
        "    return g\n",
        "df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(lambda g: enforce_non_increasing_tolerant(g, 25.0))\n",
        "fvc_final = df_out['FVC'].values.astype(float)\n",
        "fvc_final = np.where(abs_dist_te == 0.0, base_fvc_te, fvc_final)\n",
        "\n",
        "# Sigma test: max(band/c_bin, banker); dist==0->70; floors; per-patient monotone\n",
        "sigma_banker_te = np.maximum(240.0 + 3.0 * abs_dist_te, 70.0)\n",
        "sigma_banker_te = np.where(abs_dist_te > 20.0, np.maximum(sigma_banker_te, 100.0), sigma_banker_te)\n",
        "sigma_from_band = np.zeros_like(abs_dist_te, dtype=float)\n",
        "for (lo,hi) in bins:\n",
        "    m = (abs_dist_te>lo) & (abs_dist_te<=hi)\n",
        "    if not np.any(m): continue\n",
        "    c = best_c[(lo,hi)]\n",
        "    sigma_from_band[m] = band_te[m] / c\n",
        "sigma_te = np.maximum(sigma_from_band, sigma_banker_te)\n",
        "if use_floor130:\n",
        "    sigma_te = np.where(abs_dist_te > 30.0, np.maximum(sigma_te, 130.0), sigma_te)\n",
        "sigma_te = np.where(abs_dist_te == 0.0, 70.0, sigma_te)\n",
        "\n",
        "df_sig = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': abs_dist_te, 'Sigma': sigma_te.astype(float)})\n",
        "def enforce_sigma_monotone(g):\n",
        "    g = g.sort_values('dist').copy()\n",
        "    g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "    return g\n",
        "df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n",
        "sigma_final = df_sig['Sigma'].values.astype(float)\n",
        "\n",
        "# 5) Save submission\n",
        "sub = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_final, 'Confidence': sigma_final})\n",
        "sub.to_csv('submission_2model_q50_anchor_qband.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(f\"Saved submission_2model_q50_anchor_qband.csv and set submission.csv. Alphas: {alpha_s:.2f}/{alpha_m:.2f}/{alpha_l:.2f} (forced to 0.30); use_floor130={use_floor130}. Elapsed {time.time()-t0:.1f}s\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2-Model] Using fixed alphas (short,mid,long) = 0.30, 0.30, 0.30\n[Sigma-2M] Bin (0.0,5.0] best c=1.30 OOF LL=-6.04051\n[Sigma-2M] Bin (5.0,15.0] best c=1.60 OOF LL=-6.12583\n[Sigma-2M] Bin (15.0,1000000000.0] best c=1.70 OOF LL=-6.40760\n[Sigma-2M] >=130 floor test: LL_no130=-7.93749 LL_130=-7.93749 adopt=True\nSaved submission_2model_q50_anchor_qband.csv and set submission.csv. Alphas: 0.30/0.30/0.30 (forced to 0.30); use_floor130=True. Elapsed 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/51834446.py:134: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(lambda g: enforce_non_increasing_tolerant(g, 25.0))\n/tmp/ipykernel_4688/51834446.py:157: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n"
          ]
        }
      ]
    },
    {
      "id": "6566f74f-4287-4a0c-baaa-08c81ed7d04e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2-model (70/30 q50+anchor) with banker-only sigma; tolerant FVC monotonicity; save alt submission\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "# Build q50 and per-fold anchor OOF just to ensure hygiene for anchor; then use full-train anchor for test\n",
        "def prepare_baseline_table(df):\n",
        "    base = (df.sort_values(['Patient','Weeks']).groupby('Patient', as_index=False).first())\n",
        "    base = base[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "        columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "    return base\n",
        "\n",
        "# Test grid\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "pred_d = pd.read_csv('pred_quantile_deltas_v2.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "q50_d = pred_d['q50_d'].astype(float).values\n",
        "\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]; grid['Weeks'] = parts[1].astype(int)\n",
        "test_base = test[['Patient','Weeks','FVC','Percent']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_base, on='Patient', how='left')\n",
        "dist_te = (grid['Weeks'] - grid['Base_Week']).astype(float).values\n",
        "abs_dist_te = np.abs(dist_te)\n",
        "base_fvc_te = grid['Base_FVC'].astype(float).values\n",
        "\n",
        "# Full-train global slope for anchor\n",
        "gs_full = robust_global_slope(compute_patient_slopes(train))\n",
        "fvc_q50_te = base_fvc_te + q50_d\n",
        "fvc_anchor_te = base_fvc_te + gs_full * dist_te\n",
        "\n",
        "# Fixed alpha 0.30 across bins (70/30 blend)\n",
        "alpha = 0.30\n",
        "fvc_pred = (1.0 - alpha) * fvc_q50_te + alpha * fvc_anchor_te\n",
        "fvc_pred = np.clip(fvc_pred, 500, 6000)\n",
        "\n",
        "# Tolerant non-increasing per patient (+25 ml), then pin dist==0 to Base_FVC\n",
        "df_out = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'FVC': fvc_pred})\n",
        "def enforce_non_increasing_tolerant(g, tol=25.0):\n",
        "    g = g.sort_values('Weeks').copy()\n",
        "    f = g['FVC'].values.astype(float)\n",
        "    for i in range(len(f)-2, -1, -1):\n",
        "        f[i] = min(f[i], f[i+1] + tol)\n",
        "    g['FVC'] = f\n",
        "    return g\n",
        "df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(lambda g: enforce_non_increasing_tolerant(g, 25.0))\n",
        "fvc_final = df_out['FVC'].values.astype(float)\n",
        "fvc_final = np.where(abs_dist_te == 0.0, base_fvc_te, fvc_final)\n",
        "\n",
        "# Banker-only sigma with standard floors and per-patient monotone in |dist|\n",
        "sigma_banker_te = np.maximum(240.0 + 3.0 * abs_dist_te, 70.0)\n",
        "sigma_banker_te = np.where(abs_dist_te > 20.0, np.maximum(sigma_banker_te, 100.0), sigma_banker_te)\n",
        "df_sig = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': abs_dist_te, 'Sigma': sigma_banker_te.astype(float)})\n",
        "def enforce_sigma_monotone(g):\n",
        "    g = g.sort_values('dist').copy()\n",
        "    g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "    return g\n",
        "df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n",
        "sigma_final = df_sig['Sigma'].values.astype(float)\n",
        "\n",
        "sub = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_final, 'Confidence': sigma_final})\n",
        "sub.to_csv('submission_2model_q50_anchor_banker.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_2model_q50_anchor_banker.csv and set submission.csv (70/30 q50+anchor; banker-only sigma; tolerant FVC monotonicity).')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_2model_q50_anchor_banker.csv and set submission.csv (70/30 q50+anchor; banker-only sigma; tolerant FVC monotonicity).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/2628678739.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(lambda g: enforce_non_increasing_tolerant(g, 25.0))\n/tmp/ipykernel_4688/2628678739.py:57: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n"
          ]
        }
      ]
    },
    {
      "id": "0b44c990-6990-49f4-8133-ca85bc8046ab",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 3: Rebuild 2-model (70/30 q50+anchor) using v3 q50; retune sigma c per bin on blended OOF; write v3 submissions\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "def laplace_ll_np(y_true, y_pred, sigma):\n",
        "    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float); sigma = np.asarray(sigma, float)\n",
        "    delta = np.minimum(np.abs(y_true - y_pred), 1000.0)\n",
        "    sigma = np.maximum(sigma, 70.0)\n",
        "    return float(np.mean(-delta / sigma - np.log(sigma)))\n",
        "\n",
        "def prepare_baseline_table(df):\n",
        "    base = (df.sort_values(['Patient','Weeks']).groupby('Patient', as_index=False).first())\n",
        "    base = base[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "        columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "    return base\n",
        "\n",
        "def compute_patient_slopes(df, patient_col='Patient', week_col='Weeks', target_col='FVC'):\n",
        "    slopes = {}\n",
        "    for pid, g in df.groupby(patient_col):\n",
        "        if g.shape[0] >= 2:\n",
        "            x = g[week_col].values.astype(float); y = g[target_col].values.astype(float)\n",
        "            xm = x.mean(); ym = y.mean()\n",
        "            denom = ((x - xm)**2).sum()\n",
        "            slope = ((x - xm) * (y - ym)).sum() / denom if denom > 0 else 0.0\n",
        "            slopes[pid] = slope\n",
        "    return slopes\n",
        "\n",
        "def robust_global_slope(slopes_dict):\n",
        "    if not slopes_dict: return 0.0\n",
        "    return float(np.median(list(slopes_dict.values())))\n",
        "\n",
        "# 1) Load OOF: v3 q50 deltas (q50_delta_oof) and v2 bands (q20/q80 deltas); dedupe/average per (Patient,Weeks) and align by keys\n",
        "oof_v3 = pd.read_csv('oof_quantile_lgbm_v3.csv')  # Patient, Weeks, FVC, Base_Week, Base_FVC, q50_delta_oof\n",
        "oof_v2 = pd.read_csv('oof_quantile_lgbm_v2.csv')[['Patient','Weeks','q20_delta_oof','q80_delta_oof']]\n",
        "\n",
        "# Deduplicate/average within each source to remove multi-seed duplicates\n",
        "oof_v3 = (oof_v3.groupby(['Patient','Weeks'], as_index=False)\n",
        "          .agg({'FVC':'first','Base_Week':'first','Base_FVC':'first','q50_delta_oof':'mean'}))\n",
        "oof_v2 = (oof_v2.groupby(['Patient','Weeks'], as_index=False)\n",
        "          .agg({'q20_delta_oof':'mean','q80_delta_oof':'mean'}))\n",
        "\n",
        "oof = oof_v3.merge(oof_v2, on=['Patient','Weeks'], how='inner')\n",
        "\n",
        "# Build dist and future-only filter; drop dist==0 rows for OOF tuning/scoring; remove residual duplicates\n",
        "oof['dist'] = (oof['Weeks'] - oof['Base_Week']).astype(float)\n",
        "pre_rows = oof.shape[0]\n",
        "oof = oof[(oof['dist'] >= 0) & oof[['q50_delta_oof','q20_delta_oof','q80_delta_oof']].notna().all(axis=1)].copy()\n",
        "oof = oof.sort_values(['Patient','Weeks']).drop_duplicates(['Patient','Weeks'])\n",
        "oof = oof[oof['dist'] > 0].copy()\n",
        "print(f\"[Diag] OOF merged rows pre-filter={pre_rows} post-filter={oof.shape[0]} unique_patients={oof['Patient'].nunique()}\")\n",
        "\n",
        "# 2) Per-fold anchor gs_fold (TRAIN-only) using GroupKFold; map fold to patients and gs\n",
        "N_SPLITS = 5\n",
        "gkf = GroupKFold(n_splits=N_SPLITS)\n",
        "groups = train['Patient'].values\n",
        "patient_to_fold, fold_to_gs = {}, {}\n",
        "for fold, (trn_idx, val_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    trn_df = train.iloc[trn_idx]\n",
        "    gs_fold = robust_global_slope(compute_patient_slopes(trn_df))\n",
        "    fold_to_gs[fold] = gs_fold\n",
        "    for p in train.iloc[val_idx]['Patient'].astype(str).unique():\n",
        "        patient_to_fold[p] = fold\n",
        "oof['fold'] = oof['Patient'].astype(str).map(patient_to_fold).astype(int)\n",
        "oof['gs_fold'] = oof['fold'].map(fold_to_gs).astype(float)\n",
        "\n",
        "# 3) Build 70/30 blended OOF FVC and compute banker sigma + tune c per bin for q-band sigma\n",
        "base = oof['Base_FVC'].astype(float).values\n",
        "dist = oof['dist'].astype(float).values\n",
        "abs_dist = np.abs(dist).astype(float)\n",
        "fvc_q50_oof = base + oof['q50_delta_oof'].astype(float).values\n",
        "fvc_anchor_oof = base + oof['gs_fold'].astype(float).values * dist\n",
        "fvc_blend_oof = 0.70 * fvc_q50_oof + 0.30 * fvc_anchor_oof\n",
        "y_oof = oof['FVC'].astype(float).values\n",
        "band_oof = np.abs(oof['q80_delta_oof'].astype(float).values - oof['q20_delta_oof'].astype(float).values)\n",
        "\n",
        "sigma_banker_oof = np.maximum(240.0 + 3.0 * abs_dist, 70.0)\n",
        "sigma_banker_oof = np.where(abs_dist > 20.0, np.maximum(sigma_banker_oof, 100.0), sigma_banker_oof)\n",
        "\n",
        "bins = [(0.0,5.0),(5.0,15.0),(15.0,1e9)]\n",
        "masks = [ (abs_dist>lo) & (abs_dist<=hi) for lo,hi in bins ]\n",
        "print('[Diag] OOF bin counts short/mid/long =', [int(m.sum()) for m in masks])\n",
        "\n",
        "c_grid_short_mid = [1.3,1.4,1.5,1.6,1.7,1.8,2.0]\n",
        "c_grid_long = [1.3,1.4,1.5,1.6,1.7,1.8,2.0,2.3,2.4,2.5,2.6]\n",
        "best_c = {}\n",
        "for (lo,hi), m in zip(bins, masks):\n",
        "    if not np.any(m):\n",
        "        best_c[(lo,hi)] = 1.8\n",
        "        print(f'[v3 Sigma] Bin ({lo},{hi}] empty; default c=1.8')\n",
        "        continue\n",
        "    grid_c = c_grid_short_mid if hi<=15.0 else c_grid_long\n",
        "    b_ll, b_c = -1e9, 1.8\n",
        "    for c in grid_c:\n",
        "        sig = np.maximum(band_oof[m] / c, sigma_banker_oof[m])\n",
        "        z = (abs_dist[m] == 0.0)\n",
        "        if np.any(z):\n",
        "            sig[z] = np.maximum(70.0, sig[z])\n",
        "        ll = laplace_ll_np(y_oof[m], fvc_blend_oof[m], sig)\n",
        "        if ll > b_ll:\n",
        "            b_ll, b_c = ll, c\n",
        "    best_c[(lo,hi)] = b_c\n",
        "    print(f\"[v3 Sigma] Bin ({lo},{hi}] best c={b_c:.2f} OOF LL={b_ll:.5f}\")\n",
        "\n",
        "# Optional >=130 floor for |dist|>30 if OOF-neutral\n",
        "m_gt30 = abs_dist > 30.0\n",
        "use_floor130 = False\n",
        "if np.any(m_gt30):\n",
        "    sig_base = np.zeros_like(abs_dist)\n",
        "    for (lo,hi), m in zip(bins, masks):\n",
        "        if np.any(m):\n",
        "            sig_base[m] = np.maximum(band_oof[m] / best_c[(lo,hi)], sigma_banker_oof[m])\n",
        "    sig_base = np.where(abs_dist == 0.0, np.maximum(70.0, sig_base), sig_base)\n",
        "    ll_no130 = laplace_ll_np(y_oof, fvc_blend_oof, sig_base)\n",
        "    sig_130 = np.where(m_gt30, np.maximum(sig_base, 130.0), sig_base)\n",
        "    ll_130 = laplace_ll_np(y_oof, fvc_blend_oof, sig_130)\n",
        "    use_floor130 = (ll_130 >= ll_no130 - 1e-6)\n",
        "    print(f\"[v3 Sigma] >=130 floor test: LL_no130={ll_no130:.5f} LL_130={ll_130:.5f} adopt={use_floor130}\")\n",
        "\n",
        "# Compute global OOF LL for q-band sigma tuned above and banker\n",
        "sig_oof = np.zeros_like(abs_dist)\n",
        "for (lo,hi), m in zip(bins, masks):\n",
        "    if np.any(m):\n",
        "        sig_oof[m] = np.maximum(band_oof[m] / best_c[(lo,hi)], sigma_banker_oof[m])\n",
        "sig_oof = np.where(abs_dist == 0.0, np.maximum(70.0, sig_oof), sig_oof)\n",
        "if use_floor130:\n",
        "    sig_oof = np.where(abs_dist > 30.0, np.maximum(sig_oof, 130.0), sig_oof)\n",
        "ll_global_qband = laplace_ll_np(y_oof, fvc_blend_oof, sig_oof)\n",
        "ll_global_banker = laplace_ll_np(y_oof, fvc_blend_oof, sigma_banker_oof)\n",
        "print(f'[v3 OOF] rows={oof.shape[0]} pats={oof[\"Patient\"].nunique()} LL_qband={ll_global_qband:.5f} | LL_banker={ll_global_banker:.5f}')\n",
        "\n",
        "# 4) Build TEST FVC using v3 q50_d and full-train anchor; apply guardrails\n",
        "pred_v3 = pd.read_csv('pred_quantile_deltas_v3.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "q50_d_te = pred_v3['q50_d'].astype(float).values\n",
        "\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]; grid['Weeks'] = parts[1].astype(int)\n",
        "test = pd.read_csv('test.csv')\n",
        "test_base = test[['Patient','Weeks','FVC','Percent']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_base, on='Patient', how='left')\n",
        "dist_te = (grid['Weeks'] - grid['Base_Week']).astype(float).values\n",
        "abs_dist_te = np.abs(dist_te).astype(float)\n",
        "base_fvc_te = grid['Base_FVC'].astype(float).values\n",
        "\n",
        "gs_full = robust_global_slope(compute_patient_slopes(train))\n",
        "fvc_q50_te = base_fvc_te + q50_d_te\n",
        "fvc_anchor_te = base_fvc_te + gs_full * dist_te\n",
        "fvc_te = 0.70 * fvc_q50_te + 0.30 * fvc_anchor_te\n",
        "fvc_te = np.clip(fvc_te, 500, 6000)\n",
        "\n",
        "# Tolerant non-increasing per patient (+25 ml), then pin dist==0 to Base_FVC\n",
        "df_out = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'FVC': fvc_te})\n",
        "def enforce_non_increasing_tolerant(g, tol=25.0):\n",
        "    g = g.sort_values('Weeks').copy()\n",
        "    f = g['FVC'].values.astype(float)\n",
        "    for i in range(len(f)-2, -1, -1):\n",
        "        f[i] = min(f[i], f[i+1] + tol)\n",
        "    g['FVC'] = f\n",
        "    return g\n",
        "df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(lambda g: enforce_non_increasing_tolerant(g, 25.0))\n",
        "fvc_final = df_out['FVC'].values.astype(float)\n",
        "fvc_final = np.where(abs_dist_te == 0.0, base_fvc_te, fvc_final)\n",
        "\n",
        "# 5) Build TEST sigmas: q-band (tuned c) and banker; per-patient monotone\n",
        "pred_v2 = pd.read_csv('pred_quantile_deltas_v2.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "band_te = np.abs(pred_v2['q80_d'].astype(float).values - pred_v2['q20_d'].astype(float).values)\n",
        "sigma_banker_te = np.maximum(240.0 + 3.0 * abs_dist_te, 70.0)\n",
        "sigma_banker_te = np.where(abs_dist_te > 20.0, np.maximum(sigma_banker_te, 100.0), sigma_banker_te)\n",
        "sigma_from_band = np.zeros_like(abs_dist_te, dtype=float)\n",
        "for (lo,hi) in bins:\n",
        "    m = (abs_dist_te>lo) & (abs_dist_te<=hi)\n",
        "    if np.any(m):\n",
        "        c = best_c[(lo,hi)]\n",
        "        sigma_from_band[m] = band_te[m] / c\n",
        "sigma_qband_te = np.maximum(sigma_from_band, sigma_banker_te)\n",
        "if use_floor130:\n",
        "    sigma_qband_te = np.where(abs_dist_te > 30.0, np.maximum(sigma_qband_te, 130.0), sigma_qband_te)\n",
        "sigma_qband_te = np.where(abs_dist_te == 0.0, 70.0, sigma_qband_te)\n",
        "\n",
        "def enforce_sigma_monotone(df):\n",
        "    def _mono(g):\n",
        "        g = g.sort_values('dist').copy()\n",
        "        g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "        return g\n",
        "    return df.groupby('Patient', as_index=False, group_keys=False).apply(_mono)\n",
        "\n",
        "df_sig_q = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': abs_dist_te, 'Sigma': sigma_qband_te.astype(float)})\n",
        "df_sig_q = enforce_sigma_monotone(df_sig_q)\n",
        "sigma_qband_final = df_sig_q['Sigma'].values.astype(float)\n",
        "\n",
        "df_sig_b = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': abs_dist_te, 'Sigma': sigma_banker_te.astype(float)})\n",
        "df_sig_b = enforce_sigma_monotone(df_sig_b)\n",
        "sigma_banker_final = df_sig_b['Sigma'].values.astype(float)\n",
        "\n",
        "# 6) Save two submissions; do not overwrite submission.csv here\n",
        "sub_q = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_final, 'Confidence': sigma_qband_final})\n",
        "sub_b = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_final, 'Confidence': sigma_banker_final})\n",
        "sub_q.to_csv('submission_v3_qband.csv', index=False)\n",
        "sub_b.to_csv('submission_v3_banker.csv', index=False)\n",
        "print('Saved submission_v3_qband.csv and submission_v3_banker.csv. Elapsed {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Diag] OOF merged rows pre-filter=1387 post-filter=1229 unique_patients=158\n[Diag] OOF bin counts short/mid/long = [286, 438, 505]\n[v3 Sigma] Bin (0.0,5.0] best c=1.40 OOF LL=-6.04133\n[v3 Sigma] Bin (5.0,15.0] best c=1.60 OOF LL=-6.12839\n[v3 Sigma] Bin (15.0,1000000000.0] best c=1.70 OOF LL=-6.41465\n[v3 Sigma] >=130 floor test: LL_no130=-6.22576 LL_130=-6.22576 adopt=True\n[v3 OOF] rows=1229 pats=158 LL_qband=-6.22576 | LL_banker=-6.22576\nSaved submission_v3_qband.csv and submission_v3_banker.csv. Elapsed 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/169884083.py:164: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(lambda g: enforce_non_increasing_tolerant(g, 25.0))\n/tmp/ipykernel_4688/169884083.py:189: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df.groupby('Patient', as_index=False, group_keys=False).apply(_mono)\n/tmp/ipykernel_4688/169884083.py:189: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df.groupby('Patient', as_index=False, group_keys=False).apply(_mono)\n"
          ]
        }
      ]
    },
    {
      "id": "0684c72d-de84-4e5a-b2d6-cd98e93eb7d3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set submission.csv to v3 q-band primary\n",
        "import pandas as pd\n",
        "src = 'submission_v3_qband.csv'\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = pd.read_csv(src)\n",
        "assert sub.shape[0] == ss.shape[0], 'Row count mismatch vs sample_submission'\n",
        "assert set(sub['Patient_Week'].astype(str)) == set(ss['Patient_Week'].astype(str)), 'Patient_Week sets differ'\n",
        "assert sub['FVC'].notna().all() and sub['Confidence'].notna().all(), 'NaNs in v3_qband submission'\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(f'submission.csv overwritten with {src}')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv overwritten with submission_v3_qband.csv\n"
          ]
        }
      ]
    },
    {
      "id": "b6c149b3-891a-42ed-8fa3-1b6aa983eccf",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional lever: add tiny LME weight (0.10) only for |dist|>15 to v3 2-model FVC; keep v3 q-band sigma\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "# Load v3 q50 deltas and build q50+anchor FVC like in Cell 27\n",
        "pred_v3 = pd.read_csv('pred_quantile_deltas_v3.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "q50_d_te = pred_v3['q50_d'].astype(float).values\n",
        "\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]; grid['Weeks'] = parts[1].astype(int)\n",
        "test = pd.read_csv('test.csv')\n",
        "test_base = test[['Patient','Weeks','FVC','Percent']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_base, on='Patient', how='left')\n",
        "dist_te = (grid['Weeks'] - grid['Base_Week']).astype(float).values\n",
        "abs_dist_te = np.abs(dist_te).astype(float)\n",
        "base_fvc_te = grid['Base_FVC'].astype(float).values\n",
        "\n",
        "def compute_patient_slopes(df, patient_col='Patient', week_col='Weeks', target_col='FVC'):\n",
        "    slopes = {}\n",
        "    for pid, g in df.groupby(patient_col):\n",
        "        if g.shape[0] >= 2:\n",
        "            x = g[week_col].values.astype(float); y = g[target_col].values.astype(float)\n",
        "            xm = x.mean(); ym = y.mean()\n",
        "            denom = ((x - xm)**2).sum()\n",
        "            slopes[pid] = ((x - xm) * (y - ym)).sum() / denom if denom > 0 else 0.0\n",
        "    return slopes\n",
        "\n",
        "def robust_global_slope(slopes_dict):\n",
        "    if not slopes_dict: return 0.0\n",
        "    return float(np.median(list(slopes_dict.values())))\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "gs_full = robust_global_slope(compute_patient_slopes(train))\n",
        "fvc_q50_te = base_fvc_te + q50_d_te\n",
        "fvc_anchor_te = base_fvc_te + gs_full * dist_te\n",
        "\n",
        "# Load LME FVC from existing artifact\n",
        "fvc_lme = pd.read_csv('submission_lme_banker.csv').set_index('Patient_Week').loc[ss['Patient_Week'],'FVC'].astype(float).values\n",
        "\n",
        "# Build FVC with rule: if |dist|<=15: 0.70*q50 + 0.30*anchor; else: 0.60*q50 + 0.30*anchor + 0.10*LME\n",
        "fvc_base = 0.70 * fvc_q50_te + 0.30 * fvc_anchor_te\n",
        "m_long = abs_dist_te > 15.0\n",
        "fvc_alt_long = 0.60 * fvc_q50_te + 0.30 * fvc_anchor_te + 0.10 * fvc_lme\n",
        "fvc_mix = np.where(m_long, fvc_alt_long, fvc_base)\n",
        "fvc_mix = np.clip(fvc_mix, 500, 6000)\n",
        "\n",
        "# Tolerant non-increasing FVC per patient (+25 ml), then pin dist==0 to Base_FVC\n",
        "df_out = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'FVC': fvc_mix})\n",
        "def enforce_non_increasing_tolerant(g, tol=25.0):\n",
        "    g = g.sort_values('Weeks').copy()\n",
        "    f = g['FVC'].values.astype(float)\n",
        "    for i in range(len(f)-2, -1, -1):\n",
        "        f[i] = min(f[i], f[i+1] + tol)\n",
        "    g['FVC'] = f\n",
        "    return g\n",
        "df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(lambda g: enforce_non_increasing_tolerant(g, 25.0))\n",
        "fvc_final = df_out['FVC'].values.astype(float)\n",
        "fvc_final = np.where(abs_dist_te == 0.0, base_fvc_te, fvc_final)\n",
        "\n",
        "# Keep sigma from v3 q-band submission (already banker-floored, monotone)\n",
        "sub_qband = pd.read_csv('submission_v3_qband.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "sigma_final = sub_qband['Confidence'].astype(float).values\n",
        "\n",
        "sub_new = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_final, 'Confidence': sigma_final})\n",
        "sub_new.to_csv('submission_v3_qband_lme10long.csv', index=False)\n",
        "sub_new.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_v3_qband_lme10long.csv and set submission.csv (add 0.10 LME for |dist|>15; sigma from v3 q-band unchanged).')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_v3_qband_lme10long.csv and set submission.csv (add 0.10 LME for |dist|>15; sigma from v3 q-band unchanged).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/10265494.py:59: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(lambda g: enforce_non_increasing_tolerant(g, 25.0))\n"
          ]
        }
      ]
    },
    {
      "id": "cb4883e1-d736-4235-a62d-8433be60bf52",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set submission.csv to v3 banker backup\n",
        "import pandas as pd\n",
        "src = 'submission_v3_banker.csv'\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = pd.read_csv(src)\n",
        "assert sub.shape[0] == ss.shape[0], 'Row count mismatch vs sample_submission'\n",
        "assert set(sub['Patient_Week'].astype(str)) == set(ss['Patient_Week'].astype(str)), 'Patient_Week sets differ'\n",
        "assert sub['FVC'].notna().all() and sub['Confidence'].notna().all(), 'NaNs in v3_banker submission'\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(f'submission.csv overwritten with {src}')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv overwritten with submission_v3_banker.csv\n"
          ]
        }
      ]
    },
    {
      "id": "1c0fe7f5-8496-494f-8dc0-c1aa9412ea1c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 3b: Average q50 (LGBM v3 + CatBoost v1), rebuild 2-model 70/30, retune sigma per-bin, write submissions\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "def laplace_ll_np(y_true, y_pred, sigma):\n",
        "    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float); sigma = np.asarray(sigma, float)\n",
        "    delta = np.minimum(np.abs(y_true - y_pred), 1000.0)\n",
        "    sigma = np.maximum(sigma, 70.0)\n",
        "    return float(np.mean(-delta / sigma - np.log(sigma)))\n",
        "\n",
        "def prepare_baseline_table(df):\n",
        "    base = (df.sort_values(['Patient','Weeks']).groupby('Patient', as_index=False).first())\n",
        "    base = base[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "        columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "    return base\n",
        "\n",
        "def compute_patient_slopes(df, patient_col='Patient', week_col='Weeks', target_col='FVC'):\n",
        "    slopes = {}\n",
        "    for pid, g in df.groupby(patient_col):\n",
        "        if g.shape[0] >= 2:\n",
        "            x = g[week_col].values.astype(float); y = g[target_col].values.astype(float)\n",
        "            xm = x.mean(); ym = y.mean()\n",
        "            denom = ((x - xm)**2).sum()\n",
        "            slope = ((x - xm) * (y - ym)).sum() / denom if denom > 0 else 0.0\n",
        "            slopes[pid] = slope\n",
        "    return slopes\n",
        "\n",
        "def robust_global_slope(slopes_dict):\n",
        "    if not slopes_dict: return 0.0\n",
        "    return float(np.median(list(slopes_dict.values())))\n",
        "\n",
        "# 1) Load OOF q50 deltas: v3 (LGBM) and CatBoost v1; dedupe/average per (Patient, Weeks) to avoid multi-seed duplicates\n",
        "oof_v3 = pd.read_csv('oof_quantile_lgbm_v3.csv')  # Patient, Weeks, FVC, Base_Week, Base_FVC, q50_delta_oof\n",
        "oof_cb = pd.read_csv('oof_quantile_cat_v1.csv')  # Patient, Weeks, FVC, Base_Week, Base_FVC, q50_delta_oof\n",
        "\n",
        "# Deduplicate/average\n",
        "oof_v3 = (oof_v3.groupby(['Patient','Weeks'], as_index=False)\n",
        "          .agg({'FVC':'first','Base_Week':'first','Base_FVC':'first','q50_delta_oof':'mean'}))\n",
        "oof_cb = (oof_cb.groupby(['Patient','Weeks'], as_index=False)\n",
        "          .agg({'q50_delta_oof':'mean'}))\n",
        "\n",
        "# Merge averaged q50s\n",
        "oof = (oof_v3.rename(columns={'q50_delta_oof':'q50_v3'})\n",
        "       .merge(oof_cb.rename(columns={'q50_delta_oof':'q50_cb'}), on=['Patient','Weeks'], how='inner'))\n",
        "\n",
        "# Bring v2 bands for sigma and average duplicates there too\n",
        "oof_v2 = pd.read_csv('oof_quantile_lgbm_v2.csv')[['Patient','Weeks','q20_delta_oof','q80_delta_oof']]\n",
        "oof_v2 = (oof_v2.groupby(['Patient','Weeks'], as_index=False)\n",
        "          .agg({'q20_delta_oof':'mean','q80_delta_oof':'mean'}))\n",
        "oof = oof.merge(oof_v2, on=['Patient','Weeks'], how='inner')\n",
        "\n",
        "# Future-only and diagnostics; drop dist==0 rows for OOF tuning/scoring; also drop any residual duplicates\n",
        "oof['dist'] = (oof['Weeks'] - oof['Base_Week']).astype(float)\n",
        "pre = oof.shape[0]\n",
        "oof = oof[(oof['dist'] >= 0) & oof[['q50_v3','q50_cb','q20_delta_oof','q80_delta_oof']].notna().all(axis=1)].copy()\n",
        "oof = oof.sort_values(['Patient','Weeks']).drop_duplicates(['Patient','Weeks'])\n",
        "oof = oof[oof['dist'] > 0].copy()\n",
        "print(f\"[Avg-q50 Diag] OOF pre={pre} post={oof.shape[0]} pats={oof['Patient'].nunique()}\")\n",
        "\n",
        "# Per-fold anchor gs_fold (TRAIN-only)\n",
        "N_SPLITS = 5\n",
        "gkf = GroupKFold(n_splits=N_SPLITS)\n",
        "groups = train['Patient'].values\n",
        "patient_to_fold, fold_to_gs = {}, {}\n",
        "for fold, (trn_idx, val_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    trn_df = train.iloc[trn_idx]\n",
        "    gs_fold = robust_global_slope(compute_patient_slopes(trn_df))\n",
        "    fold_to_gs[fold] = gs_fold\n",
        "    for p in train.iloc[val_idx]['Patient'].astype(str).unique():\n",
        "        patient_to_fold[p] = fold\n",
        "oof['fold'] = oof['Patient'].astype(str).map(patient_to_fold).astype(int)\n",
        "oof['gs_fold'] = oof['fold'].map(fold_to_gs).astype(float)\n",
        "\n",
        "# 2) Build 70/30 blended OOF with averaged q50\n",
        "base = oof['Base_FVC'].astype(float).values\n",
        "dist = oof['dist'].astype(float).values\n",
        "abs_dist = np.abs(dist).astype(float)\n",
        "q50_avg = 0.5 * (oof['q50_v3'].astype(float).values + oof['q50_cb'].astype(float).values)\n",
        "fvc_q50_oof = base + q50_avg\n",
        "fvc_anchor_oof = base + oof['gs_fold'].astype(float).values * dist\n",
        "fvc_blend_oof = 0.70 * fvc_q50_oof + 0.30 * fvc_anchor_oof\n",
        "y_oof = oof['FVC'].astype(float).values\n",
        "band_oof = np.abs(oof['q80_delta_oof'].astype(float).values - oof['q20_delta_oof'].astype(float).values)\n",
        "\n",
        "sigma_banker_oof = np.maximum(240.0 + 3.0 * abs_dist, 70.0)\n",
        "sigma_banker_oof = np.where(abs_dist > 20.0, np.maximum(sigma_banker_oof, 100.0), sigma_banker_oof)\n",
        "\n",
        "bins = [(0.0,5.0),(5.0,15.0),(15.0,1e9)]\n",
        "masks = [ (abs_dist>lo) & (abs_dist<=hi) for lo,hi in bins ]\n",
        "print('[Avg-q50 Diag] bin counts:', [int(m.sum()) for m in masks])\n",
        "\n",
        "# 3) Tune c per bin for q-band sigma floored by banker; optional >=130 for |dist|>30 if OOF-neutral\n",
        "c_grid_short_mid = [1.3,1.4,1.5,1.6,1.7,1.8,2.0]\n",
        "c_grid_long = [1.3,1.4,1.5,1.6,1.7,1.8,2.0,2.3,2.4,2.5,2.6]\n",
        "best_c = {}\n",
        "for (lo,hi), m in zip(bins, masks):\n",
        "    if not np.any(m):\n",
        "        best_c[(lo,hi)] = 1.8\n",
        "        print(f'[Avg-q50 Sigma] Bin ({lo},{hi}] empty; c=1.8')\n",
        "        continue\n",
        "    grid_c = c_grid_short_mid if hi<=15.0 else c_grid_long\n",
        "    b_ll, b_c = -1e9, 1.8\n",
        "    for c in grid_c:\n",
        "        sig = np.maximum(band_oof[m] / c, sigma_banker_oof[m])\n",
        "        z = (abs_dist[m] == 0.0)\n",
        "        if np.any(z): sig[z] = np.maximum(70.0, sig[z])\n",
        "        ll = laplace_ll_np(y_oof[m], fvc_blend_oof[m], sig)\n",
        "        if ll > b_ll: b_ll, b_c = ll, c\n",
        "    best_c[(lo,hi)] = b_c\n",
        "    print(f\"[Avg-q50 Sigma] Bin ({lo},{hi}] best c={b_c:.2f} OOF LL={b_ll:.5f}\")\n",
        "\n",
        "m_gt30 = abs_dist > 30.0\n",
        "use_floor130 = False\n",
        "if np.any(m_gt30):\n",
        "    sig_base = np.zeros_like(abs_dist)\n",
        "    for (lo,hi), m in zip(bins, masks):\n",
        "        if np.any(m): sig_base[m] = np.maximum(band_oof[m] / best_c[(lo,hi)], sigma_banker_oof[m])\n",
        "    sig_base = np.where(abs_dist == 0.0, np.maximum(70.0, sig_base), sig_base)\n",
        "    ll_no130 = laplace_ll_np(y_oof, fvc_blend_oof, sig_base)\n",
        "    sig_130 = np.where(m_gt30, np.maximum(sig_base, 130.0), sig_base)\n",
        "    ll_130 = laplace_ll_np(y_oof, fvc_blend_oof, sig_130)\n",
        "    use_floor130 = (ll_130 >= ll_no130 - 1e-6)\n",
        "    print(f\"[Avg-q50 Sigma] >=130 test: LL_no130={ll_no130:.5f} LL_130={ll_130:.5f} adopt={use_floor130}\")\n",
        "\n",
        "# Global OOF LLs\n",
        "sig_oof = np.zeros_like(abs_dist)\n",
        "for (lo,hi), m in zip(bins, masks):\n",
        "    if np.any(m): sig_oof[m] = np.maximum(band_oof[m] / best_c[(lo,hi)], sigma_banker_oof[m])\n",
        "sig_oof = np.where(abs_dist == 0.0, np.maximum(70.0, sig_oof), sig_oof)\n",
        "if use_floor130: sig_oof = np.where(abs_dist > 30.0, np.maximum(sig_oof, 130.0), sig_oof)\n",
        "ll_qb = laplace_ll_np(y_oof, fvc_blend_oof, sig_oof)\n",
        "ll_bk = laplace_ll_np(y_oof, fvc_blend_oof, sigma_banker_oof)\n",
        "print(f\"[Avg-q50 OOF] rows={oof.shape[0]} pats={oof['Patient'].nunique()} LL_qband={ll_qb:.5f} | LL_banker={ll_bk:.5f}\")\n",
        "\n",
        "# 4) TEST: average q50_d (v3 + CatBoost), 70/30 with full-train anchor; guardrails\n",
        "pred_v3 = pd.read_csv('pred_quantile_deltas_v3.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "pred_cb = pd.read_csv('pred_quantile_deltas_cat_v1.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "q50_d_te = 0.5 * (pred_v3['q50_d'].astype(float).values + pred_cb['q50_d'].astype(float).values)\n",
        "\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]; grid['Weeks'] = parts[1].astype(int)\n",
        "test = pd.read_csv('test.csv')\n",
        "test_base = test[['Patient','Weeks','FVC','Percent']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_base, on='Patient', how='left')\n",
        "dist_te = (grid['Weeks'] - grid['Base_Week']).astype(float).values\n",
        "abs_dist_te = np.abs(dist_te).astype(float)\n",
        "base_fvc_te = grid['Base_FVC'].astype(float).values\n",
        "\n",
        "gs_full = robust_global_slope(compute_patient_slopes(train))\n",
        "fvc_q50_te = base_fvc_te + q50_d_te\n",
        "fvc_anchor_te = base_fvc_te + gs_full * dist_te\n",
        "fvc_te = 0.70 * fvc_q50_te + 0.30 * fvc_anchor_te\n",
        "fvc_te = np.clip(fvc_te, 500, 6000)\n",
        "\n",
        "# Tolerant per-patient monotonicity (+25ml), then pin dist==0 to Base_FVC\n",
        "df_out = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'FVC': fvc_te})\n",
        "def enforce_non_increasing_tolerant(g, tol=25.0):\n",
        "    g = g.sort_values('Weeks').copy()\n",
        "    f = g['FVC'].values.astype(float)\n",
        "    for i in range(len(f)-2, -1, -1):\n",
        "        f[i] = min(f[i], f[i+1] + tol)\n",
        "    g['FVC'] = f\n",
        "    return g\n",
        "df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(lambda g: enforce_non_increasing_tolerant(g, 25.0))\n",
        "fvc_final = df_out['FVC'].values.astype(float)\n",
        "fvc_final = np.where(abs_dist_te == 0.0, base_fvc_te, fvc_final)\n",
        "\n",
        "# 5) TEST sigma: q-band tuned per-bin and banker; per-patient monotone\n",
        "pred_v2 = pd.read_csv('pred_quantile_deltas_v2.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "band_te = np.abs(pred_v2['q80_d'].astype(float).values - pred_v2['q20_d'].astype(float).values)\n",
        "sigma_banker_te = np.maximum(240.0 + 3.0 * abs_dist_te, 70.0)\n",
        "sigma_banker_te = np.where(abs_dist_te > 20.0, np.maximum(sigma_banker_te, 100.0), sigma_banker_te)\n",
        "sigma_from_band = np.zeros_like(abs_dist_te, dtype=float)\n",
        "for (lo,hi) in bins:\n",
        "    m = (abs_dist_te>lo) & (abs_dist_te<=hi)\n",
        "    if np.any(m): sigma_from_band[m] = band_te[m] / best_c[(lo,hi)]\n",
        "sigma_qband_te = np.maximum(sigma_from_band, sigma_banker_te)\n",
        "if use_floor130: sigma_qband_te = np.where(abs_dist_te > 30.0, np.maximum(sigma_qband_te, 130.0), sigma_qband_te)\n",
        "sigma_qband_te = np.where(abs_dist_te == 0.0, 70.0, sigma_qband_te)\n",
        "\n",
        "def enforce_sigma_monotone(df):\n",
        "    def _mono(g):\n",
        "        g = g.sort_values('dist').copy()\n",
        "        g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "        return g\n",
        "    return df.groupby('Patient', as_index=False, group_keys=False).apply(_mono)\n",
        "\n",
        "df_sig = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': abs_dist_te, 'Sigma': sigma_qband_te.astype(float)})\n",
        "df_sig = enforce_sigma_monotone(df_sig)\n",
        "sigma_qband_final = df_sig['Sigma'].values.astype(float)\n",
        "\n",
        "# Banker-only version\n",
        "df_sig_b = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': abs_dist_te, 'Sigma': sigma_banker_te.astype(float)})\n",
        "df_sig_b = enforce_sigma_monotone(df_sig_b)\n",
        "sigma_banker_final = df_sig_b['Sigma'].values.astype(float)\n",
        "\n",
        "# 6) Save submissions; do not overwrite submission.csv automatically\n",
        "sub_q = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_final, 'Confidence': sigma_qband_final})\n",
        "sub_b = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_final, 'Confidence': sigma_banker_final})\n",
        "sub_q.to_csv('submission_v3cat_qband.csv', index=False)\n",
        "sub_b.to_csv('submission_v3cat_banker.csv', index=False)\n",
        "print('Saved submission_v3cat_qband.csv and submission_v3cat_banker.csv. Elapsed {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Avg-q50 Diag] OOF pre=1387 post=1229 pats=158\n[Avg-q50 Diag] bin counts: [286, 438, 505]\n[Avg-q50 Sigma] Bin (0.0,5.0] best c=1.50 OOF LL=-6.04091\n[Avg-q50 Sigma] Bin (5.0,15.0] best c=1.60 OOF LL=-6.12936\n[Avg-q50 Sigma] Bin (15.0,1000000000.0] best c=1.70 OOF LL=-6.41399\n[Avg-q50 Sigma] >=130 test: LL_no130=-6.22573 LL_130=-6.22573 adopt=True\n[Avg-q50 OOF] rows=1229 pats=158 LL_qband=-6.22573 | LL_banker=-6.22573\nSaved submission_v3cat_qband.csv and submission_v3cat_banker.csv. Elapsed 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/2548587614.py:169: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(lambda g: enforce_non_increasing_tolerant(g, 25.0))\n/tmp/ipykernel_4688/2548587614.py:191: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df.groupby('Patient', as_index=False, group_keys=False).apply(_mono)\n/tmp/ipykernel_4688/2548587614.py:191: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df.groupby('Patient', as_index=False, group_keys=False).apply(_mono)\n"
          ]
        }
      ]
    },
    {
      "id": "b961e1c2-0d56-4307-941e-7cd510fe8743",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set submission.csv to averaged q50 (LGBM v3 + CatBoost v1) 2-model with banker sigma\n",
        "import pandas as pd\n",
        "src = 'submission_v3cat_banker.csv'\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = pd.read_csv(src)\n",
        "assert sub.shape[0] == ss.shape[0], 'Row count mismatch vs sample_submission'\n",
        "assert set(sub['Patient_Week'].astype(str)) == set(ss['Patient_Week'].astype(str)), 'Patient_Week sets differ'\n",
        "assert sub['FVC'].notna().all() and sub['Confidence'].notna().all(), 'NaNs in v3cat_banker submission'\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(f'submission.csv overwritten with {src}')"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv overwritten with submission_v3cat_banker.csv\n"
          ]
        }
      ]
    },
    {
      "id": "4f5b91c1-781b-466c-bc41-8001d69a8fc9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fast LB probes per expert: strict mono on v3cat_banker; LME-long variants (0.05, 0.10) with banker sigma\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "# Build test grid with baseline to compute dist and Base_FVC\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]; grid['Weeks'] = parts[1].astype(int)\n",
        "test_base = test[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "    columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_base, on='Patient', how='left')\n",
        "dist = (grid['Weeks'].values - grid['Base_Week'].values).astype(float)\n",
        "abs_dist = np.abs(dist).astype(float)\n",
        "base_fvc = grid['Base_FVC'].values.astype(float)\n",
        "\n",
        "# 1) Strict monotonicity A/B on current best v3cat banker\n",
        "sub_b = pd.read_csv('submission_v3cat_banker.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "fvc_b = sub_b['FVC'].astype(float).clip(500, 6000).values\n",
        "\n",
        "def enforce_non_increasing_strict(df):\n",
        "    g = df.sort_values('Weeks').copy()\n",
        "    f = g['FVC'].values.astype(float)\n",
        "    # Strict non-increasing: f[i] <= f[i+1]\n",
        "    for i in range(len(f)-2, -1, -1):\n",
        "        f[i] = min(f[i], f[i+1])\n",
        "    g['FVC'] = f\n",
        "    return g\n",
        "\n",
        "df_out = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'FVC': fvc_b})\n",
        "df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(enforce_non_increasing_strict)\n",
        "fvc_strict = df_out['FVC'].values.astype(float)\n",
        "fvc_strict = np.where(abs_dist == 0.0, base_fvc, fvc_strict)\n",
        "sigma_b = sub_b['Confidence'].astype(float).values  # keep sigma unchanged\n",
        "sub_strict = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_strict, 'Confidence': sigma_b})\n",
        "sub_strict.to_csv('submission_v3cat_banker_strictmono.csv', index=False)\n",
        "\n",
        "# 2) LME boost only in long horizon (|dist|>15): w_lme in {0.05, 0.10}; else keep 0.70*q50 + 0.30*anchor; banker sigma\n",
        "from pathlib import Path\n",
        "\n",
        "# Recompute v3cat backbone FVC from components to avoid using post-mono FVC\n",
        "pred_v3 = pd.read_csv('pred_quantile_deltas_v3.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "pred_cb = pd.read_csv('pred_quantile_deltas_cat_v1.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "q50_d_avg = 0.5 * (pred_v3['q50_d'].astype(float).values + pred_cb['q50_d'].astype(float).values)\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "def compute_patient_slopes(df, patient_col='Patient', week_col='Weeks', target_col='FVC'):\n",
        "    slopes = {}\n",
        "    for pid, g in df.groupby(patient_col):\n",
        "        if g.shape[0] >= 2:\n",
        "            x = g[week_col].values.astype(float); y = g[target_col].values.astype(float)\n",
        "            xm = x.mean(); ym = y.mean()\n",
        "            denom = ((x - xm)**2).sum()\n",
        "            slopes[pid] = ((x - xm) * (y - ym)).sum() / denom if denom > 0 else 0.0\n",
        "    return slopes\n",
        "def robust_global_slope(slopes_dict):\n",
        "    if not slopes_dict: return 0.0\n",
        "    return float(np.median(list(slopes_dict.values())))\n",
        "\n",
        "gs_full = robust_global_slope(compute_patient_slopes(train))\n",
        "fvc_q50 = base_fvc + q50_d_avg\n",
        "fvc_anchor = base_fvc + gs_full * dist\n",
        "fvc_base = 0.70 * fvc_q50 + 0.30 * fvc_anchor\n",
        "\n",
        "# LME FVC from artifact\n",
        "fvc_lme = pd.read_csv('submission_lme_banker.csv').set_index('Patient_Week').loc[ss['Patient_Week'], 'FVC'].astype(float).values\n",
        "m_long = abs_dist > 15.0\n",
        "\n",
        "def build_lme_long(w_lme):\n",
        "    fvc_mix = fvc_base.copy()\n",
        "    fvc_long = 0.60 * fvc_q50 + 0.30 * fvc_anchor + w_lme * fvc_lme\n",
        "    fvc_mix[m_long] = fvc_long[m_long]\n",
        "    fvc_mix = np.clip(fvc_mix, 500, 6000)\n",
        "    # Tolerant monotonicity (+25 ml) as current default; pin dist==0 to Base_FVC\n",
        "    def enforce_non_increasing_tolerant(g, tol=25.0):\n",
        "        g = g.sort_values('Weeks').copy()\n",
        "        f = g['FVC'].values.astype(float)\n",
        "        for i in range(len(f)-2, -1, -1):\n",
        "            f[i] = min(f[i], f[i+1] + tol)\n",
        "        g['FVC'] = f\n",
        "        return g\n",
        "    df = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'FVC': fvc_mix})\n",
        "    df = df.groupby('Patient', as_index=False, group_keys=False).apply(lambda g: enforce_non_increasing_tolerant(g, 25.0))\n",
        "    fvc_final = df['FVC'].values.astype(float)\n",
        "    fvc_final = np.where(abs_dist == 0.0, base_fvc, fvc_final)\n",
        "    # Banker sigma with floors and per-patient monotone in |dist|\n",
        "    sigma_banker = np.maximum(240.0 + 3.0 * abs_dist, 70.0)\n",
        "    sigma_banker = np.where(abs_dist > 20.0, np.maximum(sigma_banker, 100.0), sigma_banker)\n",
        "    df_sig = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': abs_dist, 'Sigma': sigma_banker.astype(float)})\n",
        "    def enforce_sigma_monotone(g):\n",
        "        g = g.sort_values('dist').copy()\n",
        "        g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "        return g\n",
        "    df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n",
        "    sigma_final = df_sig['Sigma'].values.astype(float)\n",
        "    return fvc_final, sigma_final\n",
        "\n",
        "for w in [0.05, 0.10]:\n",
        "    fvc_l, sig_l = build_lme_long(w)\n",
        "    pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_l, 'Confidence': sig_l}).to_csv(f'submission_v3cat_banker_lme{int(w*100):02d}long.csv', index=False)\n",
        "\n",
        "# Set submission.csv to strict mono variant for immediate A/B submit\n",
        "sub_strict.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_v3cat_banker_strictmono.csv, submission_v3cat_banker_lme05long.csv, submission_v3cat_banker_lme10long.csv; submission.csv set to strict mono.')"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_v3cat_banker_strictmono.csv, submission_v3cat_banker_lme05long.csv, submission_v3cat_banker_lme10long.csv; submission.csv set to strict mono.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/1118268723.py:32: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(enforce_non_increasing_strict)\n/tmp/ipykernel_4688/1118268723.py:84: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df = df.groupby('Patient', as_index=False, group_keys=False).apply(lambda g: enforce_non_increasing_tolerant(g, 25.0))\n/tmp/ipykernel_4688/1118268723.py:95: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n/tmp/ipykernel_4688/1118268723.py:84: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df = df.groupby('Patient', as_index=False, group_keys=False).apply(lambda g: enforce_non_increasing_tolerant(g, 25.0))\n/tmp/ipykernel_4688/1118268723.py:95: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n"
          ]
        }
      ]
    },
    {
      "id": "f4fee011-78a6-490a-87d0-f155f733cd25",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set submission.csv to LME-long (w=0.10) variant per expert A/B plan\n",
        "import pandas as pd\n",
        "src = 'submission_v3cat_banker_lme10long.csv'\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = pd.read_csv(src)\n",
        "assert sub.shape[0] == ss.shape[0], 'Row count mismatch vs sample_submission'\n",
        "assert set(sub['Patient_Week'].astype(str)) == set(ss['Patient_Week'].astype(str)), 'Patient_Week sets differ'\n",
        "assert sub['FVC'].notna().all() and sub['Confidence'].notna().all(), 'NaNs in LME-long submission'\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(f'submission.csv overwritten with {src}')"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv overwritten with submission_v3cat_banker_lme10long.csv\n"
          ]
        }
      ]
    },
    {
      "id": "a839a9b8-8900-4559-831d-9daa40551ae3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set submission.csv to LME-long (w=0.05) variant for A/B probe\n",
        "import pandas as pd\n",
        "src = 'submission_v3cat_banker_lme05long.csv'\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = pd.read_csv(src)\n",
        "assert sub.shape[0] == ss.shape[0], 'Row count mismatch vs sample_submission'\n",
        "assert set(sub['Patient_Week'].astype(str)) == set(ss['Patient_Week'].astype(str)), 'Patient_Week sets differ'\n",
        "assert sub['FVC'].notna().all() and sub['Confidence'].notna().all(), 'NaNs in LME05-long submission'\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(f'submission.csv overwritten with {src}')"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv overwritten with submission_v3cat_banker_lme05long.csv\n"
          ]
        }
      ]
    },
    {
      "id": "d2b8d734-ff28-4691-9100-abd587ce6428",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Per-bin alpha re-optimization (cap <=0.30) on corrected OOF for avg q50 backbone; banker sigma; gated adoption\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "def laplace_ll_np(y_true, y_pred, sigma):\n",
        "    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float); sigma = np.asarray(sigma, float)\n",
        "    delta = np.minimum(np.abs(y_true - y_pred), 1000.0)\n",
        "    sigma = np.maximum(sigma, 70.0)\n",
        "    return float(np.mean(-delta / sigma - np.log(sigma)))\n",
        "\n",
        "def prepare_baseline_table(df):\n",
        "    base = (df.sort_values(['Patient','Weeks']).groupby('Patient', as_index=False).first())\n",
        "    base = base[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "        columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "    return base\n",
        "\n",
        "def compute_patient_slopes(df, patient_col='Patient', week_col='Weeks', target_col='FVC'):\n",
        "    slopes = {}\n",
        "    for pid, g in df.groupby(patient_col):\n",
        "        if g.shape[0] >= 2:\n",
        "            x = g[week_col].values.astype(float); y = g[target_col].values.astype(float)\n",
        "            xm = x.mean(); ym = y.mean()\n",
        "            denom = ((x - xm)**2).sum()\n",
        "            slope = ((x - xm) * (y - ym)).sum() / denom if denom > 0 else 0.0\n",
        "            slopes[pid] = slope\n",
        "    return slopes\n",
        "\n",
        "def robust_global_slope(slopes_dict):\n",
        "    if not slopes_dict: return 0.0\n",
        "    return float(np.median(list(slopes_dict.values())))\n",
        "\n",
        "# 1) Build corrected OOF for averaged q50 (LGBM v3 + CatBoost v1) with per-fold anchors; dedupe and drop dist==0\n",
        "oof_v3 = pd.read_csv('oof_quantile_lgbm_v3.csv')\n",
        "oof_cb = pd.read_csv('oof_quantile_cat_v1.csv')\n",
        "oof_v3 = (oof_v3.groupby(['Patient','Weeks'], as_index=False)\n",
        "          .agg({'FVC':'first','Base_Week':'first','Base_FVC':'first','q50_delta_oof':'mean'}))\n",
        "oof_cb = (oof_cb.groupby(['Patient','Weeks'], as_index=False)\n",
        "          .agg({'q50_delta_oof':'mean'}))\n",
        "oof = (oof_v3.rename(columns={'q50_delta_oof':'q50_v3'})\n",
        "       .merge(oof_cb.rename(columns={'q50_delta_oof':'q50_cb'}), on=['Patient','Weeks'], how='inner'))\n",
        "oof['dist'] = (oof['Weeks'] - oof['Base_Week']).astype(float)\n",
        "pre = oof.shape[0]\n",
        "oof = oof[(oof['dist'] >= 0)].copy()\n",
        "oof = oof.sort_values(['Patient','Weeks']).drop_duplicates(['Patient','Weeks'])\n",
        "oof = oof[oof['dist'] > 0].copy()\n",
        "\n",
        "# Per-fold anchors\n",
        "N_SPLITS = 5\n",
        "gkf = GroupKFold(n_splits=N_SPLITS)\n",
        "groups = train['Patient'].values\n",
        "patient_to_fold, fold_to_gs = {}, {}\n",
        "for fold, (trn_idx, val_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    trn_df = train.iloc[trn_idx]\n",
        "    gs_fold = robust_global_slope(compute_patient_slopes(trn_df))\n",
        "    fold_to_gs[fold] = gs_fold\n",
        "    for p in train.iloc[val_idx]['Patient'].astype(str).unique():\n",
        "        patient_to_fold[p] = fold\n",
        "oof['fold'] = oof['Patient'].astype(str).map(patient_to_fold).astype(int)\n",
        "oof['gs_fold'] = oof['fold'].map(fold_to_gs).astype(float)\n",
        "\n",
        "# 2) Compute OOF predictions for a fixed 70/30 and for a per-bin alpha grid; banker sigma for scoring\n",
        "base = oof['Base_FVC'].astype(float).values\n",
        "dist = oof['dist'].astype(float).values; abs_dist = np.abs(dist).astype(float)\n",
        "q50_avg = 0.5 * (oof['q50_v3'].astype(float).values + oof['q50_cb'].astype(float).values)\n",
        "fvc_q50 = base + q50_avg\n",
        "fvc_anchor = base + oof['gs_fold'].astype(float).values * dist\n",
        "y_oof = oof['FVC'].astype(float).values\n",
        "sigma_banker_oof = np.maximum(240.0 + 3.0 * abs_dist, 70.0)\n",
        "sigma_banker_oof = np.where(abs_dist > 20.0, np.maximum(sigma_banker_oof, 100.0), sigma_banker_oof)\n",
        "\n",
        "bins = [(0.0,5.0),(5.0,15.0),(15.0,1e9)]\n",
        "masks = [ (abs_dist>lo) & (abs_dist<=hi) for lo,hi in bins ]\n",
        "\n",
        "# Baseline fixed alpha=0.30 across bins\n",
        "fvc_bl_fixed = np.zeros_like(y_oof)\n",
        "for (lo,hi), m in zip(bins, masks):\n",
        "    if np.any(m):\n",
        "        a = 0.30\n",
        "        fvc_bl_fixed[m] = (1.0 - a) * fvc_q50[m] + a * fvc_anchor[m]\n",
        "ll_fixed = laplace_ll_np(y_oof, fvc_bl_fixed, sigma_banker_oof)\n",
        "print(f\"[AlphaOpt] Baseline fixed alpha=0.30 OOF LL={ll_fixed:.5f}\")\n",
        "\n",
        "# Grid alpha per bin in {0.20, 0.25, 0.30} with cap <=0.30\n",
        "grid_alphas = [0.20, 0.25, 0.30]\n",
        "best_alpha = {}\n",
        "for (lo,hi), m in zip(bins, masks):\n",
        "    if not np.any(m):\n",
        "        best_alpha[(lo,hi)] = 0.30\n",
        "        continue\n",
        "    b_ll, b_a = -1e9, 0.30\n",
        "    for a in grid_alphas:\n",
        "        fvc_m = (1.0 - a) * fvc_q50[m] + a * fvc_anchor[m]\n",
        "        ll = laplace_ll_np(y_oof[m], fvc_m, sigma_banker_oof[m])\n",
        "        if ll > b_ll: b_ll, b_a = ll, a\n",
        "    best_alpha[(lo,hi)] = b_a\n",
        "    print(f\"[AlphaOpt] Bin ({lo},{hi}] best alpha={b_a:.2f} OOF LL={b_ll:.5f}\")\n",
        "\n",
        "# Build blended OOF with optimized alphas and assess global gain\n",
        "fvc_bl_opt = np.zeros_like(y_oof)\n",
        "for (lo,hi), m in zip(bins, masks):\n",
        "    a = best_alpha[(lo,hi)]\n",
        "    if np.any(m): fvc_bl_opt[m] = (1.0 - a) * fvc_q50[m] + a * fvc_anchor[m]\n",
        "ll_opt = laplace_ll_np(y_oof, fvc_bl_opt, sigma_banker_oof)\n",
        "gain = ll_opt - ll_fixed\n",
        "print(f\"[AlphaOpt] OOF global LL_opt={ll_opt:.5f} gain={gain:+.5f} vs fixed 0.30\")\n",
        "\n",
        "# 3) If gain > 0.005, build TEST with per-bin alphas; else skip adoption\n",
        "adopt = gain > 0.005\n",
        "print(f\"[AlphaOpt] Adopt per-bin alphas? {adopt}\")\n",
        "\n",
        "# Prepare TEST components\n",
        "pred_v3 = pd.read_csv('pred_quantile_deltas_v3.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "pred_cb = pd.read_csv('pred_quantile_deltas_cat_v1.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "q50_d_te = 0.5 * (pred_v3['q50_d'].astype(float).values + pred_cb['q50_d'].astype(float).values)\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]; grid['Weeks'] = parts[1].astype(int)\n",
        "test = pd.read_csv('test.csv')\n",
        "test_base = test[['Patient','Weeks','FVC','Percent']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_base, on='Patient', how='left')\n",
        "dist_te = (grid['Weeks'] - grid['Base_Week']).astype(float).values\n",
        "abs_dist_te = np.abs(dist_te).astype(float)\n",
        "base_fvc_te = grid['Base_FVC'].astype(float).values\n",
        "gs_full = robust_global_slope(compute_patient_slopes(train))\n",
        "fvc_q50_te = base_fvc_te + q50_d_te\n",
        "fvc_anchor_te = base_fvc_te + gs_full * dist_te\n",
        "\n",
        "alpha_map = {}\n",
        "for (lo,hi) in bins:\n",
        "    alpha_map[(lo,hi)] = best_alpha[(lo,hi)] if adopt else 0.30\n",
        "\n",
        "alpha_vec = np.zeros_like(abs_dist_te, dtype=float) + 0.30\n",
        "for (lo,hi), a in alpha_map.items():\n",
        "    m = (abs_dist_te>lo) & (abs_dist_te<=hi)\n",
        "    if np.any(m): alpha_vec[m] = a\n",
        "\n",
        "fvc_te = (1.0 - alpha_vec) * fvc_q50_te + alpha_vec * fvc_anchor_te\n",
        "fvc_te = np.clip(fvc_te, 500, 6000)\n",
        "\n",
        "# Tolerant monotonicity (+25 ml) and pin dist==0 to Base_FVC\n",
        "df_out = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'FVC': fvc_te})\n",
        "def enforce_non_increasing_tolerant(g, tol=25.0):\n",
        "    g = g.sort_values('Weeks').copy()\n",
        "    f = g['FVC'].values.astype(float)\n",
        "    for i in range(len(f)-2, -1, -1):\n",
        "        f[i] = min(f[i], f[i+1] + tol)\n",
        "    g['FVC'] = f\n",
        "    return g\n",
        "df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(lambda g: enforce_non_increasing_tolerant(g, 25.0))\n",
        "fvc_final = df_out['FVC'].values.astype(float)\n",
        "fvc_final = np.where(abs_dist_te == 0.0, base_fvc_te, fvc_final)\n",
        "\n",
        "# Banker sigma with floors and per-patient monotone in |dist|\n",
        "sigma_banker_te = np.maximum(240.0 + 3.0 * abs_dist_te, 70.0)\n",
        "sigma_banker_te = np.where(abs_dist_te > 20.0, np.maximum(sigma_banker_te, 100.0), sigma_banker_te)\n",
        "df_sig = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': abs_dist_te, 'Sigma': sigma_banker_te.astype(float)})\n",
        "def enforce_sigma_monotone(g):\n",
        "    g = g.sort_values('dist').copy()\n",
        "    g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "    return g\n",
        "df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n",
        "sigma_final = df_sig['Sigma'].values.astype(float)\n",
        "\n",
        "sub = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_final, 'Confidence': sigma_final})\n",
        "sub.to_csv('submission_v3cat_banker_alphaOpt.csv', index=False)\n",
        "if adopt:\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission_v3cat_banker_alphaOpt.csv and set submission.csv (adopted per-bin alphas). Elapsed {:.1f}s'.format(time.time()-t0))\n",
        "else:\n",
        "    print('Saved submission_v3cat_banker_alphaOpt.csv (not adopted; \u0394OOF <= 0.005). Elapsed {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AlphaOpt] Baseline fixed alpha=0.30 OOF LL=-6.22573\n[AlphaOpt] Bin (0.0,5.0] best alpha=0.30 OOF LL=-6.04091\n[AlphaOpt] Bin (5.0,15.0] best alpha=0.30 OOF LL=-6.12936\n[AlphaOpt] Bin (15.0,1000000000.0] best alpha=0.30 OOF LL=-6.41399\n[AlphaOpt] OOF global LL_opt=-6.22573 gain=+0.00000 vs fixed 0.30\n[AlphaOpt] Adopt per-bin alphas? False\nSaved submission_v3cat_banker_alphaOpt.csv (not adopted; \u0394OOF <= 0.005). Elapsed 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/155349220.py:153: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(lambda g: enforce_non_increasing_tolerant(g, 25.0))\n/tmp/ipykernel_4688/155349220.py:165: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n"
          ]
        }
      ]
    },
    {
      "id": "6eec028b-9896-4f21-91bf-4379deb7be6f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quantile LGBM v3-bands: train q20 and q80 deltas (OOF + Test); reuse v3 q50 setup\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "def prepare_baseline_table(df):\n",
        "    base = (df.sort_values(['Patient','Weeks']).groupby('Patient', as_index=False).first())\n",
        "    base = base[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "        columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "    return base\n",
        "\n",
        "def one_hot_fit(df, cols):\n",
        "    return {c: sorted(df[c].dropna().astype(str).unique().tolist()) for c in cols}\n",
        "\n",
        "def one_hot_transform(df, cats):\n",
        "    out = df.copy()\n",
        "    for c, values in cats.items():\n",
        "        col = df[c].astype(str)\n",
        "        for v in values:\n",
        "            out[f'{c}__{v}'] = (col == v).astype(np.int8)\n",
        "    return out\n",
        "\n",
        "def ecdf_rank_fit(x):\n",
        "    xs = np.sort(np.asarray(x, dtype=float))\n",
        "    return xs\n",
        "\n",
        "def ecdf_rank_transform(x, xs):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    idx = np.searchsorted(xs, x, side='right')\n",
        "    return idx / max(len(xs), 1)\n",
        "\n",
        "def build_slope_features(base_df, ecdf_basefvc=None, ecdf_percent=None, cats=None, fit=False):\n",
        "    b = base_df.copy()\n",
        "    b['log_Base_FVC'] = np.log1p(np.maximum(b['Base_FVC'].astype(float), 1.0))\n",
        "    b['BaseFVC_over_Age'] = b['Base_FVC'].astype(float) / np.maximum(b['Age'].astype(float), 1.0)\n",
        "    b['PercentBase_over_Age'] = b['Percent_at_base'].astype(float) / np.maximum(b['Age'].astype(float), 1.0)\n",
        "    if fit:\n",
        "        ecdf_basefvc = ecdf_rank_fit(b['Base_FVC'].values)\n",
        "        ecdf_percent = ecdf_rank_fit(b['Percent_at_base'].values)\n",
        "    b['BaseFVC_ecdf'] = ecdf_rank_transform(b['Base_FVC'].values, ecdf_basefvc)\n",
        "    b['Percent_ecdf'] = ecdf_rank_transform(b['Percent_at_base'].values, ecdf_percent)\n",
        "    if fit:\n",
        "        cats = one_hot_fit(b, ['Sex','SmokingStatus'])\n",
        "    b = one_hot_transform(b, cats)\n",
        "    num_cols = ['Age','Base_FVC','log_Base_FVC','Percent_at_base','BaseFVC_over_Age','PercentBase_over_Age','BaseFVC_ecdf','Percent_ecdf']\n",
        "    cat_cols = [c for c in b.columns if c.startswith('Sex__') or c.startswith('SmokingStatus__')]\n",
        "    feat_cols = num_cols + cat_cols\n",
        "    return b, feat_cols, ecdf_basefvc, ecdf_percent, cats\n",
        "\n",
        "def compute_patient_slopes(df, patient_col='Patient', week_col='Weeks', target_col='FVC'):\n",
        "    slopes = {}\n",
        "    for pid, g in df.groupby(patient_col):\n",
        "        if g.shape[0] >= 2:\n",
        "            x = g[week_col].values.astype(float); y = g[target_col].values.astype(float)\n",
        "            xm = x.mean(); ym = y.mean()\n",
        "            denom = ((x - xm)**2).sum()\n",
        "            slope = ((x - xm) * (y - ym)).sum() / denom if denom > 0 else 0.0\n",
        "            slopes[pid] = slope\n",
        "    return slopes\n",
        "\n",
        "def build_q_features(grid_df, base_df, ecdf_bf=None, ecdf_pc=None, cats=None, fit=False):\n",
        "    d = grid_df.merge(base_df[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "    d['dist'] = (d['Weeks'] - d['Base_Week']).astype(float)\n",
        "    d = d[d['dist'] >= 0].copy()\n",
        "    d['abs_dist'] = d['dist'].abs()\n",
        "    d['log1p_abs_dist'] = np.log1p(d['abs_dist'])\n",
        "    d['dist_cap'] = d['dist'].clip(0, 30)\n",
        "    d['dist_short'] = d['dist'].clip(0, 5)\n",
        "    d['dist_mid'] = (d['dist'] - 5).clip(lower=0, upper=10)\n",
        "    d['dist_long'] = (d['dist'] - 15).clip(lower=0)\n",
        "    d['dist2'] = d['dist']**2\n",
        "    d['dist3'] = d['dist']**3\n",
        "    d['Base_FVC'] = d['Base_FVC'].astype(float)\n",
        "    d['Percent_at_base'] = d['Percent_at_base'].astype(float).clip(30, 120)\n",
        "    d['Age'] = d['Age'].astype(float)\n",
        "    d['log_Base_FVC'] = np.log1p(np.maximum(d['Base_FVC'], 1.0))\n",
        "    d['Age_x_Percent'] = d['Age'] * d['Percent_at_base']\n",
        "    d['BaseFVC_x_dist'] = d['Base_FVC'] * d['dist']\n",
        "    d['dist_x_Age'] = d['dist'] * d['Age']\n",
        "    d['dist_x_Percent'] = d['dist'] * d['Percent_at_base']\n",
        "    d['BaseFVC_x_dshort'] = d['Base_FVC'] * d['dist_short']\n",
        "    d['BaseFVC_x_dmid'] = d['Base_FVC'] * d['dist_mid']\n",
        "    d['BaseFVC_x_dlong'] = d['Base_FVC'] * d['dist_long']\n",
        "    if fit:\n",
        "        ecdf_bf = ecdf_rank_fit(d['Base_FVC'].values)\n",
        "        ecdf_pc = ecdf_rank_fit(d['Percent_at_base'].values)\n",
        "        cats = one_hot_fit(d, ['Sex','SmokingStatus'])\n",
        "    d['BaseFVC_ecdf'] = ecdf_rank_transform(d['Base_FVC'].values, ecdf_bf)\n",
        "    d['Percent_ecdf'] = ecdf_rank_transform(d['Percent_at_base'].values, ecdf_pc)\n",
        "    d = one_hot_transform(d, cats)\n",
        "    d['BFV_decile'] = np.floor(d['BaseFVC_ecdf'] * 10).clip(0, 9).astype(int)\n",
        "    for k in range(10):\n",
        "        d[f'BFV_decile__{k}'] = (d['BFV_decile'] == k).astype(np.int8)\n",
        "    feat_cols = [\n",
        "        'Age','Base_FVC','log_Base_FVC','Percent_at_base','BaseFVC_ecdf','Percent_ecdf',\n",
        "        'dist','abs_dist','log1p_abs_dist','dist_cap','dist_short','dist_mid','dist_long','dist2','dist3',\n",
        "        'Age_x_Percent','BaseFVC_x_dist','dist_x_Age','dist_x_Percent','BaseFVC_x_dshort','BaseFVC_x_dmid','BaseFVC_x_dlong','s_hat'\n",
        "    ] + [c for c in d.columns if c.startswith('Sex__') or c.startswith('SmokingStatus__') or c.startswith('BFV_decile__')]\n",
        "    for c in feat_cols:\n",
        "        if c not in d.columns: d[c] = 0.0\n",
        "    return d, feat_cols, ecdf_bf, ecdf_pc, cats\n",
        "\n",
        "def fit_s_hat_fold(trn_df, base_trn):\n",
        "    slopes_trn = compute_patient_slopes(trn_df)\n",
        "    slope_labels_trn = pd.DataFrame({'Patient': list(slopes_trn.keys()), 's_label': list(slopes_trn.values())})\n",
        "    base_trn_lab = base_trn.merge(slope_labels_trn, on='Patient', how='left')\n",
        "    bf_trn, f_cols_s, ecdf_bf_s, ecdf_pc_s, cats_s = build_slope_features(base_trn_lab, fit=True)\n",
        "    scaler_s = StandardScaler(with_mean=True, with_std=True).fit(bf_trn[f_cols_s].values.astype(float))\n",
        "    Xs_tr = scaler_s.transform(bf_trn[f_cols_s].values.astype(float))\n",
        "    y_s = bf_trn['s_label'].fillna(0.0).values.astype(float)\n",
        "    ridge = Ridge(alpha=1.0, random_state=42).fit(Xs_tr, y_s)\n",
        "    knn = KNeighborsRegressor(n_neighbors=9, weights='distance').fit(Xs_tr, y_s)\n",
        "    q_lo, q_hi = np.percentile(y_s, [5,95])\n",
        "    def get_s_hat_map(base_df_patients):\n",
        "        bf_pred, _, _, _, _ = build_slope_features(base_df_patients, ecdf_bf_s, ecdf_pc_s, cats_s, fit=False)\n",
        "        Xs = scaler_s.transform(bf_pred[f_cols_s].values.astype(float))\n",
        "        s = 0.8*ridge.predict(Xs) + 0.2*knn.predict(Xs)\n",
        "        s = np.clip(s, q_lo, q_hi)\n",
        "        return dict(zip(bf_pred['Patient'].values, s))\n",
        "    return get_s_hat_map\n",
        "\n",
        "# Config\n",
        "alphas = [0.2, 0.8]  # q20 and q80\n",
        "seeds = [1337, 2027, 3037]\n",
        "params = dict(objective='quantile', metric='quantile',\n",
        "              n_estimators=2400, learning_rate=0.032,\n",
        "              num_leaves=31, max_depth=6, min_data_in_leaf=24,\n",
        "              subsample=0.75, colsample_bytree=0.75,\n",
        "              reg_alpha=0.1, reg_lambda=0.2, n_jobs=-1, verbose=-1)\n",
        "\n",
        "# OOF container\n",
        "oof_df = train[['Patient','Weeks','FVC']].copy()\n",
        "oof_df['q20_delta_oof'] = np.nan\n",
        "oof_df['q80_delta_oof'] = np.nan\n",
        "\n",
        "# Static TEST grid and SS index map\n",
        "grid_te = ss.copy()\n",
        "parts = grid_te['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid_te['Patient'] = parts[0]; grid_te['Weeks'] = parts[1].astype(int)\n",
        "test_base = test[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "    columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid_te_idx = grid_te[['Patient','Weeks']].copy()\n",
        "grid_te_idx['ss_idx'] = np.arange(grid_te_idx.shape[0], dtype=int)\n",
        "\n",
        "# Accumulators for TEST deltas per alpha\n",
        "test_preds = {0.2: np.zeros(ss.shape[0], dtype=float), 0.8: np.zeros(ss.shape[0], dtype=float)}\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "groups = train['Patient'].values\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    tf = time.time()\n",
        "    trn_df = train.iloc[trn_idx].copy(); val_df = train.iloc[val_idx].copy()\n",
        "    base_trn = prepare_baseline_table(trn_df)\n",
        "    base_val = prepare_baseline_table(val_df)\n",
        "    # s_hat maps\n",
        "    get_s_hat_map = fit_s_hat_fold(trn_df, base_trn)\n",
        "    s_map_trn = get_s_hat_map(base_trn)\n",
        "    s_map_val = get_s_hat_map(base_val)\n",
        "    base_test = grid_te[['Patient']].drop_duplicates().merge(test_base.drop_duplicates('Patient'), on='Patient', how='left')\n",
        "    s_map_test = get_s_hat_map(base_test[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']])\n",
        "\n",
        "    # Future-only train/val with s_hat\n",
        "    trn = trn_df.merge(base_trn[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "    val = val_df.merge(base_val[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "    trn['dist'] = (trn['Weeks'] - trn['Base_Week']).astype(float); trn = trn[trn['dist'] >= 0].copy()\n",
        "    val['dist'] = (val['Weeks'] - val['Base_Week']).astype(float); val = val[val['dist'] >= 0].copy()\n",
        "    trn['s_hat'] = trn['Patient'].map(s_map_trn).astype(float).fillna(0.0)\n",
        "    val['s_hat'] = val['Patient'].map(s_map_val).astype(float).fillna(0.0)\n",
        "\n",
        "    # Features (fit on TRAIN fold)\n",
        "    trn_feat, feat_cols, ecdf_bf, ecdf_pc, cats = build_q_features(trn[['Patient','Weeks']].copy(), base_trn, fit=True)\n",
        "    trn_feat['s_hat'] = trn_feat['Patient'].map(s_map_trn).astype(float).fillna(0.0)\n",
        "    val_feat, _, _, _, _ = build_q_features(val[['Patient','Weeks']].copy(), base_val, ecdf_bf, ecdf_pc, cats, fit=False)\n",
        "    val_feat['s_hat'] = val_feat['Patient'].map(s_map_val).astype(float).fillna(0.0)\n",
        "\n",
        "    # Strict alignment\n",
        "    trn_feat_aligned = trn_feat.merge(trn[['Patient','Weeks','FVC']], on=['Patient','Weeks'], how='inner')\n",
        "    val_feat_aligned = val_feat.merge(val[['Patient','Weeks','FVC']], on=['Patient','Weeks'], how='inner')\n",
        "\n",
        "    y_tr_delta = (trn_feat_aligned['FVC'].astype(float).values - trn_feat_aligned['Base_FVC'].astype(float).values)\n",
        "    y_va_delta = (val_feat_aligned['FVC'].astype(float).values - val_feat_aligned['Base_FVC'].astype(float).values)\n",
        "    X_tr = trn_feat_aligned[feat_cols].values.astype(float)\n",
        "    X_va = val_feat_aligned[feat_cols].values.astype(float)\n",
        "\n",
        "    if X_tr.shape[0] == 0 or X_va.shape[0] == 0:\n",
        "        print(f'[v3-bands Fold {fold}] skipped (X_tr={X_tr.shape[0]}, X_va={X_va.shape[0]})', flush=True)\n",
        "        del trn_df, val_df, trn, val, trn_feat, val_feat, trn_feat_aligned, val_feat_aligned\n",
        "        gc.collect()\n",
        "        continue\n",
        "\n",
        "    # TEST features under TRAIN-fold transforms; align to ss via index map\n",
        "    te_feat, _, _, _, _ = build_q_features(grid_te[['Patient','Weeks']].copy(), test_base, ecdf_bf, ecdf_pc, cats, fit=False)\n",
        "    te_feat['s_hat'] = te_feat['Patient'].map(s_map_test).astype(float).fillna(0.0)\n",
        "    X_te = te_feat[feat_cols].values.astype(float)\n",
        "    te_keys = te_feat[['Patient','Weeks']].copy().merge(grid_te_idx, on=['Patient','Weeks'], how='left')\n",
        "    te_idx = te_keys['ss_idx'].values.astype(int)\n",
        "\n",
        "    # For each alpha (0.2, 0.8), seed-bag models and accumulate\n",
        "    for a in alphas:\n",
        "        val_pred_sum = np.zeros(X_va.shape[0], dtype=float)\n",
        "        test_pred_sum_fold = np.zeros(ss.shape[0], dtype=float)\n",
        "        for si, sd in enumerate(seeds):\n",
        "            lr = params['learning_rate'] + (0.002 if (si % 2 == 0) else -0.002)\n",
        "            mdl = LGBMRegressor(**{**params, 'alpha': a, 'learning_rate': lr}, random_state=sd)\n",
        "            mdl.fit(X_tr, y_tr_delta,\n",
        "                    eval_set=[(X_va, y_va_delta)],\n",
        "                    eval_metric='quantile',\n",
        "                    callbacks=[lgb.early_stopping(200, verbose=False)])\n",
        "            val_pred_sum += mdl.predict(X_va, num_iteration=mdl.best_iteration_)\n",
        "            pred_te = mdl.predict(X_te, num_iteration=mdl.best_iteration_)\n",
        "            test_pred_sum_fold[te_idx] += pred_te\n",
        "            del mdl\n",
        "        val_pred_avg = val_pred_sum / max(len(seeds), 1)\n",
        "        test_pred_avg_fold = test_pred_sum_fold / max(len(seeds), 1)\n",
        "        # Write OOF deltas for this alpha\n",
        "        keys = val_feat_aligned[['Patient','Weeks']].reset_index(drop=True)\n",
        "        col = 'q20_delta_oof' if np.isclose(a, 0.2) else 'q80_delta_oof'\n",
        "        block = pd.DataFrame({'Patient': keys['Patient'].astype(str), 'Weeks': keys['Weeks'].astype(int), col: val_pred_avg})\n",
        "        oof_df = oof_df.merge(block, on=['Patient','Weeks'], how='left', suffixes=('', '_new'))\n",
        "        oof_df[col] = oof_df[col].fillna(oof_df[col + '_new'])\n",
        "        oof_df.drop(columns=[col + '_new'], inplace=True)\n",
        "        # Accumulate test preds\n",
        "        test_preds[a] += (test_pred_avg_fold / 5.0)\n",
        "\n",
        "    print(f'[v3-bands Fold {fold}] trn={X_tr.shape[0]} val={X_va.shape[0]} elapsed={time.time()-tf:.2f}s', flush=True)\n",
        "    del trn_df, val_df, trn, val, trn_feat, val_feat, trn_feat_aligned, val_feat_aligned, X_tr, X_va, X_te, te_feat, te_idx, te_keys\n",
        "    gc.collect()\n",
        "\n",
        "# Save OOF bands with baseline for downstream use\n",
        "train_base = prepare_baseline_table(train)\n",
        "oof_save = oof_df.dropna(subset=['q20_delta_oof','q80_delta_oof']).merge(train_base[['Patient','Base_Week','Base_FVC']], on='Patient', how='left')\n",
        "oof_save.to_csv('oof_quantile_lgbm_v3_bands.csv', index=False)\n",
        "\n",
        "# Save TEST bands aligned to ss\n",
        "pd.DataFrame({\n",
        "    'Patient_Week': ss['Patient_Week'],\n",
        "    'q20_d': test_preds[0.2].astype(float),\n",
        "    'q80_d': test_preds[0.8].astype(float)\n",
        "}).to_csv('pred_quantile_deltas_v3_bands.csv', index=False)\n",
        "\n",
        "print(f'Saved oof_quantile_lgbm_v3_bands.csv and pred_quantile_deltas_v3_bands.csv. Elapsed {time.time()-t0:.1f}s')"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v3-bands Fold 1] trn=1124 val=284 elapsed=1.09s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v3-bands Fold 2] trn=1127 val=281 elapsed=1.18s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v3-bands Fold 3] trn=1129 val=279 elapsed=1.25s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v3-bands Fold 4] trn=1129 val=279 elapsed=1.12s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v3-bands Fold 5] trn=1123 val=285 elapsed=1.19s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_quantile_lgbm_v3_bands.csv and pred_quantile_deltas_v3_bands.csv. Elapsed 6.5s\n"
          ]
        }
      ]
    },
    {
      "id": "94836115-28f3-4127-ae43-1821b93c6e77",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 3c: Use v3 quantile bands (q20/q80) to retune sigma for averaged q50 (LGBM v3 + CatBoost v1); banker floor; save submission\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "def laplace_ll_np(y_true, y_pred, sigma):\n",
        "    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float); sigma = np.asarray(sigma, float)\n",
        "    delta = np.minimum(np.abs(y_true - y_pred), 1000.0)\n",
        "    sigma = np.maximum(sigma, 70.0)\n",
        "    return float(np.mean(-delta / sigma - np.log(sigma)))\n",
        "\n",
        "def prepare_baseline_table(df):\n",
        "    base = (df.sort_values(['Patient','Weeks']).groupby('Patient', as_index=False).first())\n",
        "    base = base[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "        columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "    return base\n",
        "\n",
        "def compute_patient_slopes(df, patient_col='Patient', week_col='Weeks', target_col='FVC'):\n",
        "    slopes = {}\n",
        "    for pid, g in df.groupby(patient_col):\n",
        "        if g.shape[0] >= 2:\n",
        "            x = g[week_col].values.astype(float); y = g[target_col].values.astype(float)\n",
        "            xm = x.mean(); ym = y.mean()\n",
        "            denom = ((x - xm)**2).sum()\n",
        "            slope = ((x - xm) * (y - ym)).sum() / denom if denom > 0 else 0.0\n",
        "            slopes[pid] = slope\n",
        "    return slopes\n",
        "\n",
        "def robust_global_slope(slopes_dict):\n",
        "    if not slopes_dict: return 0.0\n",
        "    return float(np.median(list(slopes_dict.values())))\n",
        "\n",
        "# 1) Build corrected OOF for averaged q50 with v3 bands merged (dedupe, drop dist==0)\n",
        "oof_v3 = pd.read_csv('oof_quantile_lgbm_v3.csv')  # q50 deltas\n",
        "oof_cb = pd.read_csv('oof_quantile_cat_v1.csv')   # q50 deltas\n",
        "oof_bands = pd.read_csv('oof_quantile_lgbm_v3_bands.csv')[['Patient','Weeks','q20_delta_oof','q80_delta_oof']]\n",
        "\n",
        "# Deduplicate/average per (Patient, Weeks)\n",
        "oof_v3 = (oof_v3.groupby(['Patient','Weeks'], as_index=False)\n",
        "          .agg({'FVC':'first','Base_Week':'first','Base_FVC':'first','q50_delta_oof':'mean'}))\n",
        "oof_cb = (oof_cb.groupby(['Patient','Weeks'], as_index=False)\n",
        "          .agg({'q50_delta_oof':'mean'}))\n",
        "oof_bands = (oof_bands.groupby(['Patient','Weeks'], as_index=False)\n",
        "             .agg({'q20_delta_oof':'mean','q80_delta_oof':'mean'}))\n",
        "\n",
        "oof = (oof_v3.rename(columns={'q50_delta_oof':'q50_v3'})\n",
        "       .merge(oof_cb.rename(columns={'q50_delta_oof':'q50_cb'}), on=['Patient','Weeks'], how='inner')\n",
        "       .merge(oof_bands, on=['Patient','Weeks'], how='inner'))\n",
        "\n",
        "oof['dist'] = (oof['Weeks'] - oof['Base_Week']).astype(float)\n",
        "pre = oof.shape[0]\n",
        "oof = oof[(oof['dist'] >= 0) & oof[['q50_v3','q50_cb','q20_delta_oof','q80_delta_oof']].notna().all(axis=1)].copy()\n",
        "oof = oof.sort_values(['Patient','Weeks']).drop_duplicates(['Patient','Weeks'])\n",
        "oof = oof[oof['dist'] > 0].copy()\n",
        "print(f\"[v3-bands Diag] OOF pre={pre} post={oof.shape[0]} pats={oof['Patient'].nunique()}\")\n",
        "\n",
        "# Per-fold anchor (TRAIN-only)\n",
        "N_SPLITS = 5\n",
        "gkf = GroupKFold(n_splits=N_SPLITS)\n",
        "groups = train['Patient'].values\n",
        "patient_to_fold, fold_to_gs = {}, {}\n",
        "for fold, (trn_idx, val_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    trn_df = train.iloc[trn_idx]\n",
        "    gs_fold = robust_global_slope(compute_patient_slopes(trn_df))\n",
        "    fold_to_gs[fold] = gs_fold\n",
        "    for p in train.iloc[val_idx]['Patient'].astype(str).unique():\n",
        "        patient_to_fold[p] = fold\n",
        "oof['fold'] = oof['Patient'].astype(str).map(patient_to_fold).astype(int)\n",
        "oof['gs_fold'] = oof['fold'].map(fold_to_gs).astype(float)\n",
        "\n",
        "# 2) 70/30 averaged q50 + anchor OOF; band from v3 bands\n",
        "base = oof['Base_FVC'].astype(float).values\n",
        "dist = oof['dist'].astype(float).values\n",
        "abs_dist = np.abs(dist).astype(float)\n",
        "q50_avg = 0.5 * (oof['q50_v3'].astype(float).values + oof['q50_cb'].astype(float).values)\n",
        "fvc_q50_oof = base + q50_avg\n",
        "fvc_anchor_oof = base + oof['gs_fold'].astype(float).values * dist\n",
        "fvc_blend_oof = 0.70 * fvc_q50_oof + 0.30 * fvc_anchor_oof\n",
        "y_oof = oof['FVC'].astype(float).values\n",
        "band_oof = np.abs(oof['q80_delta_oof'].astype(float).values - oof['q20_delta_oof'].astype(float).values)\n",
        "\n",
        "sigma_banker_oof = np.maximum(240.0 + 3.0 * abs_dist, 70.0)\n",
        "sigma_banker_oof = np.where(abs_dist > 20.0, np.maximum(sigma_banker_oof, 100.0), sigma_banker_oof)\n",
        "\n",
        "bins = [(0.0,5.0),(5.0,15.0),(15.0,1e9)]\n",
        "masks = [ (abs_dist>lo) & (abs_dist<=hi) for lo,hi in bins ]\n",
        "print('[v3-bands Diag] bin counts:', [int(m.sum()) for m in masks])\n",
        "\n",
        "c_grid_short_mid = [1.3,1.4,1.5,1.6,1.7,1.8,2.0]\n",
        "c_grid_long = [1.3,1.4,1.5,1.6,1.7,1.8,2.0,2.3,2.4,2.5,2.6]\n",
        "best_c = {}\n",
        "for (lo,hi), m in zip(bins, masks):\n",
        "    if not np.any(m):\n",
        "        best_c[(lo,hi)] = 1.8\n",
        "        print(f'[v3-bands Sigma] Bin ({lo},{hi}] empty; c=1.8')\n",
        "        continue\n",
        "    grid_c = c_grid_short_mid if hi<=15.0 else c_grid_long\n",
        "    b_ll, b_c = -1e9, 1.8\n",
        "    for c in grid_c:\n",
        "        sig = np.maximum(band_oof[m] / c, sigma_banker_oof[m])\n",
        "        ll = laplace_ll_np(y_oof[m], fvc_blend_oof[m], sig)\n",
        "        if ll > b_ll: b_ll, b_c = ll, c\n",
        "    best_c[(lo,hi)] = b_c\n",
        "    print(f\"[v3-bands Sigma] Bin ({lo},{hi}] best c={b_c:.2f} OOF LL={b_ll:.5f}\")\n",
        "\n",
        "# Optional >=130 for |dist|>30 (adopt if OOF-neutral)\n",
        "m_gt30 = abs_dist > 30.0\n",
        "use_floor130 = False\n",
        "if np.any(m_gt30):\n",
        "    sig_base = np.zeros_like(abs_dist)\n",
        "    for (lo,hi), m in zip(bins, masks):\n",
        "        if np.any(m): sig_base[m] = np.maximum(band_oof[m] / best_c[(lo,hi)], sigma_banker_oof[m])\n",
        "    ll_no130 = laplace_ll_np(y_oof, fvc_blend_oof, sig_base)\n",
        "    sig_130 = np.where(m_gt30, np.maximum(sig_base, 130.0), sig_base)\n",
        "    ll_130 = laplace_ll_np(y_oof, fvc_blend_oof, sig_130)\n",
        "    use_floor130 = (ll_130 >= ll_no130 - 1e-6)\n",
        "    print(f\"[v3-bands Sigma] >=130 test: LL_no130={ll_no130:.5f} LL_130={ll_130:.5f} adopt={use_floor130}\")\n",
        "\n",
        "sig_oof = np.zeros_like(abs_dist)\n",
        "for (lo,hi), m in zip(bins, masks):\n",
        "    if np.any(m): sig_oof[m] = np.maximum(band_oof[m] / best_c[(lo,hi)], sigma_banker_oof[m])\n",
        "if use_floor130: sig_oof = np.where(abs_dist > 30.0, np.maximum(sig_oof, 130.0), sig_oof)\n",
        "ll_qb = laplace_ll_np(y_oof, fvc_blend_oof, sig_oof)\n",
        "ll_bk = laplace_ll_np(y_oof, fvc_blend_oof, sigma_banker_oof)\n",
        "print(f\"[v3-bands OOF] rows={oof.shape[0]} pats={oof['Patient'].nunique()} LL_qband={ll_qb:.5f} | LL_banker={ll_bk:.5f}\")\n",
        "\n",
        "# 3) TEST: averaged q50 + anchor; sigma from v3 bands with tuned c, floored by banker; monotone\n",
        "pred_v3 = pd.read_csv('pred_quantile_deltas_v3.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "pred_cb = pd.read_csv('pred_quantile_deltas_cat_v1.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "pred_bands_v3 = pd.read_csv('pred_quantile_deltas_v3_bands.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "q50_d_te = 0.5 * (pred_v3['q50_d'].astype(float).values + pred_cb['q50_d'].astype(float).values)\n",
        "band_te = np.abs(pred_bands_v3['q80_d'].astype(float).values - pred_bands_v3['q20_d'].astype(float).values)\n",
        "\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]; grid['Weeks'] = parts[1].astype(int)\n",
        "test = pd.read_csv('test.csv')\n",
        "test_base = test[['Patient','Weeks','FVC','Percent']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_base, on='Patient', how='left')\n",
        "dist_te = (grid['Weeks'] - grid['Base_Week']).astype(float).values\n",
        "abs_dist_te = np.abs(dist_te).astype(float)\n",
        "base_fvc_te = grid['Base_FVC'].astype(float).values\n",
        "\n",
        "gs_full = robust_global_slope(compute_patient_slopes(train))\n",
        "fvc_q50_te = base_fvc_te + q50_d_te\n",
        "fvc_anchor_te = base_fvc_te + gs_full * dist_te\n",
        "fvc_te = 0.70 * fvc_q50_te + 0.30 * fvc_anchor_te\n",
        "fvc_te = np.clip(fvc_te, 500, 6000)\n",
        "\n",
        "# Tolerant non-increasing per patient (+25 ml) then pin dist==0\n",
        "df_out = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'FVC': fvc_te})\n",
        "def enforce_non_increasing_tolerant(g, tol=25.0):\n",
        "    g = g.sort_values('Weeks').copy()\n",
        "    f = g['FVC'].values.astype(float)\n",
        "    for i in range(len(f)-2, -1, -1):\n",
        "        f[i] = min(f[i], f[i+1] + tol)\n",
        "    g['FVC'] = f\n",
        "    return g\n",
        "df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(lambda g: enforce_non_increasing_tolerant(g, 25.0))\n",
        "fvc_final = df_out['FVC'].values.astype(float)\n",
        "fvc_final = np.where(abs_dist_te == 0.0, base_fvc_te, fvc_final)\n",
        "\n",
        "# Sigma: v3 band/c per-bin floored by banker; dist==0->70; per-patient monotone\n",
        "sigma_banker_te = np.maximum(240.0 + 3.0 * abs_dist_te, 70.0)\n",
        "sigma_banker_te = np.where(abs_dist_te > 20.0, np.maximum(sigma_banker_te, 100.0), sigma_banker_te)\n",
        "sigma_from_band = np.zeros_like(abs_dist_te, dtype=float)\n",
        "for (lo,hi), c in best_c.items():\n",
        "    m = (abs_dist_te>lo) & (abs_dist_te<=hi)\n",
        "    if np.any(m): sigma_from_band[m] = band_te[m] / c\n",
        "sigma_qband_te = np.maximum(sigma_from_band, sigma_banker_te)\n",
        "if use_floor130: sigma_qband_te = np.where(abs_dist_te > 30.0, np.maximum(sigma_qband_te, 130.0), sigma_qband_te)\n",
        "sigma_qband_te = np.where(abs_dist_te == 0.0, 70.0, sigma_qband_te)\n",
        "\n",
        "def enforce_sigma_monotone(df):\n",
        "    def _mono(g):\n",
        "        g = g.sort_values('dist').copy()\n",
        "        g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "        return g\n",
        "    return df.groupby('Patient', as_index=False, group_keys=False).apply(_mono)\n",
        "\n",
        "df_sig = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': abs_dist_te, 'Sigma': sigma_qband_te.astype(float)})\n",
        "df_sig = enforce_sigma_monotone(df_sig)\n",
        "sigma_qband_final = df_sig['Sigma'].values.astype(float)\n",
        "\n",
        "sub_q = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_final, 'Confidence': sigma_qband_final})\n",
        "sub_q.to_csv('submission_v3cat_qband_v3bands.csv', index=False)\n",
        "print('Saved submission_v3cat_qband_v3bands.csv. Elapsed {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v3-bands Diag] OOF pre=1387 post=1229 pats=158\n[v3-bands Diag] bin counts: [286, 438, 505]\n[v3-bands Sigma] Bin (0.0,5.0] best c=1.50 OOF LL=-6.04091\n[v3-bands Sigma] Bin (5.0,15.0] best c=1.60 OOF LL=-6.12936\n[v3-bands Sigma] Bin (15.0,1000000000.0] best c=1.60 OOF LL=-6.41386\n[v3-bands Sigma] >=130 test: LL_no130=-6.22568 LL_130=-6.22568 adopt=True\n[v3-bands OOF] rows=1229 pats=158 LL_qband=-6.22568 | LL_banker=-6.22573\nSaved submission_v3cat_qband_v3bands.csv. Elapsed 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/562953836.py:162: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(lambda g: enforce_non_increasing_tolerant(g, 25.0))\n/tmp/ipykernel_4688/562953836.py:182: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df.groupby('Patient', as_index=False, group_keys=False).apply(_mono)\n"
          ]
        }
      ]
    },
    {
      "id": "7941b7bb-8c8b-47a5-867b-676afe93251c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set submission.csv to v3cat q-band with v3 bands (A/B vs banker)\n",
        "import pandas as pd\n",
        "src = 'submission_v3cat_qband_v3bands.csv'\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = pd.read_csv(src)\n",
        "assert sub.shape[0] == ss.shape[0], 'Row count mismatch vs sample_submission'\n",
        "assert set(sub['Patient_Week'].astype(str)) == set(ss['Patient_Week'].astype(str)), 'Patient_Week sets differ'\n",
        "assert sub['FVC'].notna().all() and sub['Confidence'].notna().all(), 'NaNs in v3cat_qband_v3bands submission'\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(f'submission.csv overwritten with {src}')"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv overwritten with submission_v3cat_qband_v3bands.csv\n"
          ]
        }
      ]
    },
    {
      "id": "a9fb19ec-86dc-4d89-96d8-1d44fbc4e571",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# A/B: strict non-increasing FVC on v3cat_qband_v3bands; keep sigma; set as submission.csv\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "# Build grid for dist and Base_FVC\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]; grid['Weeks'] = parts[1].astype(int)\n",
        "test_base = test[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_base, on='Patient', how='left')\n",
        "dist = (grid['Weeks'].values - grid['Base_Week'].values).astype(float)\n",
        "abs_dist = np.abs(dist).astype(float)\n",
        "base_fvc = grid['Base_FVC'].values.astype(float)\n",
        "\n",
        "# Load v3bands q-band submission\n",
        "sub = pd.read_csv('submission_v3cat_qband_v3bands.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "fvc = sub['FVC'].astype(float).clip(500, 6000).values\n",
        "sigma = sub['Confidence'].astype(float).values  # unchanged\n",
        "\n",
        "def enforce_non_increasing_strict(g):\n",
        "    g = g.sort_values('Weeks').copy()\n",
        "    f = g['FVC'].values.astype(float)\n",
        "    for i in range(len(f)-2, -1, -1):\n",
        "        f[i] = min(f[i], f[i+1])\n",
        "    g['FVC'] = f\n",
        "    return g\n",
        "\n",
        "df_out = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'FVC': fvc})\n",
        "df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(enforce_non_increasing_strict)\n",
        "fvc_strict = df_out['FVC'].values.astype(float)\n",
        "fvc_strict = np.where(abs_dist == 0.0, base_fvc, fvc_strict)\n",
        "\n",
        "sub_strict = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_strict, 'Confidence': sigma})\n",
        "sub_strict.to_csv('submission_v3cat_qband_v3bands_strictmono.csv', index=False)\n",
        "sub_strict.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_v3cat_qband_v3bands_strictmono.csv and set submission.csv (strict FVC monotonicity; sigma unchanged).')"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_v3cat_qband_v3bands_strictmono.csv and set submission.csv (strict FVC monotonicity; sigma unchanged).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/4076499249.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(enforce_non_increasing_strict)\n"
          ]
        }
      ]
    },
    {
      "id": "8914ce61-0e61-477c-9381-848af37d8ed9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost Quantile v1-bands: train q20 and q80 deltas (OOF + Test) to match averaged q50 backbone\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "def prepare_baseline_table(df):\n",
        "    base = (df.sort_values(['Patient','Weeks']).groupby('Patient', as_index=False).first())\n",
        "    base = base[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "        columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "    return base\n",
        "\n",
        "def one_hot_fit(df, cols):\n",
        "    return {c: sorted(df[c].dropna().astype(str).unique().tolist()) for c in cols}\n",
        "\n",
        "def one_hot_transform(df, cats):\n",
        "    out = df.copy()\n",
        "    for c, values in cats.items():\n",
        "        col = df[c].astype(str)\n",
        "        for v in values:\n",
        "            out[f'{c}__{v}'] = (col == v).astype(np.int8)\n",
        "    return out\n",
        "\n",
        "def ecdf_rank_fit(x):\n",
        "    xs = np.sort(np.asarray(x, dtype=float))\n",
        "    return xs\n",
        "\n",
        "def ecdf_rank_transform(x, xs):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    idx = np.searchsorted(xs, x, side='right')\n",
        "    return idx / max(len(xs), 1)\n",
        "\n",
        "def build_slope_features(base_df, ecdf_basefvc=None, ecdf_percent=None, cats=None, fit=False):\n",
        "    b = base_df.copy()\n",
        "    b['log_Base_FVC'] = np.log1p(np.maximum(b['Base_FVC'].astype(float), 1.0))\n",
        "    b['BaseFVC_over_Age'] = b['Base_FVC'].astype(float) / np.maximum(b['Age'].astype(float), 1.0)\n",
        "    b['PercentBase_over_Age'] = b['Percent_at_base'].astype(float) / np.maximum(b['Age'].astype(float), 1.0)\n",
        "    if fit:\n",
        "        ecdf_basefvc = ecdf_rank_fit(b['Base_FVC'].values)\n",
        "        ecdf_percent = ecdf_rank_fit(b['Percent_at_base'].values)\n",
        "    b['BaseFVC_ecdf'] = ecdf_rank_transform(b['Base_FVC'].values, ecdf_basefvc)\n",
        "    b['Percent_ecdf'] = ecdf_rank_transform(b['Percent_at_base'].values, ecdf_percent)\n",
        "    if fit:\n",
        "        cats = one_hot_fit(b, ['Sex','SmokingStatus'])\n",
        "    b = one_hot_transform(b, cats)\n",
        "    num_cols = ['Age','Base_FVC','log_Base_FVC','Percent_at_base','BaseFVC_over_Age','PercentBase_over_Age','BaseFVC_ecdf','Percent_ecdf']\n",
        "    cat_cols = [c for c in b.columns if c.startswith('Sex__') or c.startswith('SmokingStatus__')]\n",
        "    feat_cols = num_cols + cat_cols\n",
        "    return b, feat_cols, ecdf_basefvc, ecdf_percent, cats\n",
        "\n",
        "def compute_patient_slopes(df, patient_col='Patient', week_col='Weeks', target_col='FVC'):\n",
        "    slopes = {}\n",
        "    for pid, g in df.groupby(patient_col):\n",
        "        if g.shape[0] >= 2:\n",
        "            x = g[week_col].values.astype(float); y = g[target_col].values.astype(float)\n",
        "            xm = x.mean(); ym = y.mean()\n",
        "            denom = ((x - xm)**2).sum()\n",
        "            slopes[pid] = ((x - xm) * (y - ym)).sum() / denom if denom > 0 else 0.0\n",
        "    return slopes\n",
        "\n",
        "def build_q_features(grid_df, base_df, ecdf_bf=None, ecdf_pc=None, cats=None, fit=False):\n",
        "    d = grid_df.merge(base_df[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "    d['dist'] = (d['Weeks'] - d['Base_Week']).astype(float)\n",
        "    d = d[d['dist'] >= 0].copy()\n",
        "    d['abs_dist'] = d['dist'].abs()\n",
        "    d['log1p_abs_dist'] = np.log1p(d['abs_dist'])\n",
        "    d['dist_cap'] = d['dist'].clip(0, 30)\n",
        "    d['dist_short'] = d['dist'].clip(0, 5)\n",
        "    d['dist_mid'] = (d['dist'] - 5).clip(lower=0, upper=10)\n",
        "    d['dist_long'] = (d['dist'] - 15).clip(lower=0)\n",
        "    d['dist2'] = d['dist']**2\n",
        "    d['dist3'] = d['dist']**3\n",
        "    d['Base_FVC'] = d['Base_FVC'].astype(float)\n",
        "    d['Percent_at_base'] = d['Percent_at_base'].astype(float).clip(30, 120)\n",
        "    d['Age'] = d['Age'].astype(float)\n",
        "    d['log_Base_FVC'] = np.log1p(np.maximum(d['Base_FVC'], 1.0))\n",
        "    d['Age_x_Percent'] = d['Age'] * d['Percent_at_base']\n",
        "    d['BaseFVC_x_dist'] = d['Base_FVC'] * d['dist']\n",
        "    d['dist_x_Age'] = d['dist'] * d['Age']\n",
        "    d['dist_x_Percent'] = d['dist'] * d['Percent_at_base']\n",
        "    d['BaseFVC_x_dshort'] = d['Base_FVC'] * d['dist_short']\n",
        "    d['BaseFVC_x_dmid'] = d['Base_FVC'] * d['dist_mid']\n",
        "    d['BaseFVC_x_dlong'] = d['Base_FVC'] * d['dist_long']\n",
        "    if fit:\n",
        "        ecdf_bf = ecdf_rank_fit(d['Base_FVC'].values)\n",
        "        ecdf_pc = ecdf_rank_fit(d['Percent_at_base'].values)\n",
        "        cats = one_hot_fit(d, ['Sex','SmokingStatus'])\n",
        "    d['BaseFVC_ecdf'] = ecdf_rank_transform(d['Base_FVC'].values, ecdf_bf)\n",
        "    d['Percent_ecdf'] = ecdf_rank_transform(d['Percent_at_base'].values, ecdf_pc)\n",
        "    d = one_hot_transform(d, cats)\n",
        "    d['BFV_decile'] = np.floor(d['BaseFVC_ecdf'] * 10).clip(0, 9).astype(int)\n",
        "    for k in range(10):\n",
        "        d[f'BFV_decile__{k}'] = (d['BFV_decile'] == k).astype(np.int8)\n",
        "    feat_cols = [\n",
        "        'Age','Base_FVC','log_Base_FVC','Percent_at_base','BaseFVC_ecdf','Percent_ecdf',\n",
        "        'dist','abs_dist','log1p_abs_dist','dist_cap','dist_short','dist_mid','dist_long','dist2','dist3',\n",
        "        'Age_x_Percent','BaseFVC_x_dist','dist_x_Age','dist_x_Percent','BaseFVC_x_dshort','BaseFVC_x_dmid','BaseFVC_x_dlong','s_hat'\n",
        "    ] + [c for c in d.columns if c.startswith('Sex__') or c.startswith('SmokingStatus__') or c.startswith('BFV_decile__')]\n",
        "    for c in feat_cols:\n",
        "        if c not in d.columns: d[c] = 0.0\n",
        "    return d, feat_cols, ecdf_bf, ecdf_pc, cats\n",
        "\n",
        "def fit_s_hat_fold(trn_df, base_trn):\n",
        "    slopes_trn = compute_patient_slopes(trn_df)\n",
        "    slope_labels_trn = pd.DataFrame({'Patient': list(slopes_trn.keys()), 's_label': list(slopes_trn.values())})\n",
        "    base_trn_lab = base_trn.merge(slope_labels_trn, on='Patient', how='left')\n",
        "    bf_trn, f_cols_s, ecdf_bf_s, ecdf_pc_s, cats_s = build_slope_features(base_trn_lab, fit=True)\n",
        "    scaler_s = StandardScaler(with_mean=True, with_std=True).fit(bf_trn[f_cols_s].values.astype(float))\n",
        "    Xs_tr = scaler_s.transform(bf_trn[f_cols_s].values.astype(float))\n",
        "    y_s = bf_trn['s_label'].fillna(0.0).values.astype(float)\n",
        "    ridge = Ridge(alpha=1.0, random_state=42).fit(Xs_tr, y_s)\n",
        "    knn = KNeighborsRegressor(n_neighbors=9, weights='distance').fit(Xs_tr, y_s)\n",
        "    q_lo, q_hi = np.percentile(y_s, [5,95])\n",
        "    def get_s_hat_map(base_df_patients):\n",
        "        bf_pred, _, _, _, _ = build_slope_features(base_df_patients, ecdf_bf_s, ecdf_pc_s, cats_s, fit=False)\n",
        "        Xs = scaler_s.transform(bf_pred[f_cols_s].values.astype(float))\n",
        "        s = 0.8*ridge.predict(Xs) + 0.2*knn.predict(Xs)\n",
        "        s = np.clip(s, q_lo, q_hi)\n",
        "        return dict(zip(bf_pred['Patient'].values, s))\n",
        "    return get_s_hat_map\n",
        "\n",
        "# Config: alphas and seeds\n",
        "alphas = [0.2, 0.8]\n",
        "seeds = [1337, 2027, 3037]\n",
        "cb_params = dict(\n",
        "    iterations=1600,\n",
        "    learning_rate=0.045,\n",
        "    depth=6,\n",
        "    l2_leaf_reg=6.0,\n",
        "    subsample=0.8,\n",
        "    rsm=0.8,\n",
        "    random_strength=0.8,\n",
        "    od_type='Iter',\n",
        "    od_wait=120,\n",
        "    bootstrap_type='Bernoulli',\n",
        "    loss_function=None,  # set per alpha\n",
        "    task_type='CPU',\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# OOF container\n",
        "oof_df = train[['Patient','Weeks','FVC']].copy()\n",
        "oof_df['q20_delta_oof'] = np.nan\n",
        "oof_df['q80_delta_oof'] = np.nan\n",
        "\n",
        "# TEST grid and index mapping\n",
        "grid_te = ss.copy()\n",
        "parts = grid_te['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid_te['Patient'] = parts[0]; grid_te['Weeks'] = parts[1].astype(int)\n",
        "test_base = test[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "    columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid_te_idx = grid_te[['Patient','Weeks']].copy()\n",
        "grid_te_idx['ss_idx'] = np.arange(grid_te_idx.shape[0], dtype=int)\n",
        "\n",
        "# Accumulators for TEST deltas per alpha\n",
        "test_preds = {0.2: np.zeros(ss.shape[0], dtype=float), 0.8: np.zeros(ss.shape[0], dtype=float)}\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "groups = train['Patient'].values\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    tf = time.time()\n",
        "    trn_df = train.iloc[trn_idx].copy(); val_df = train.iloc[val_idx].copy()\n",
        "    base_trn = prepare_baseline_table(trn_df)\n",
        "    base_val = prepare_baseline_table(val_df)\n",
        "    # s_hat maps\n",
        "    get_s_hat_map = fit_s_hat_fold(trn_df, base_trn)\n",
        "    s_map_trn = get_s_hat_map(base_trn)\n",
        "    s_map_val = get_s_hat_map(base_val)\n",
        "    base_test = grid_te[['Patient']].drop_duplicates().merge(test_base.drop_duplicates('Patient'), on='Patient', how='left')\n",
        "    s_map_test = get_s_hat_map(base_test[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']])\n",
        "\n",
        "    # Future-only train/val with s_hat\n",
        "    trn = trn_df.merge(base_trn[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "    val = val_df.merge(base_val[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "    trn['dist'] = (trn['Weeks'] - trn['Base_Week']).astype(float); trn = trn[trn['dist'] >= 0].copy()\n",
        "    val['dist'] = (val['Weeks'] - val['Base_Week']).astype(float); val = val[val['dist'] >= 0].copy()\n",
        "    trn['s_hat'] = trn['Patient'].map(s_map_trn).astype(float).fillna(0.0)\n",
        "    val['s_hat'] = val['Patient'].map(s_map_val).astype(float).fillna(0.0)\n",
        "\n",
        "    # Features (fit on TRAIN fold)\n",
        "    trn_feat, feat_cols, ecdf_bf, ecdf_pc, cats = build_q_features(trn[['Patient','Weeks']].copy(), base_trn, fit=True)\n",
        "    trn_feat['s_hat'] = trn_feat['Patient'].map(s_map_trn).astype(float).fillna(0.0)\n",
        "    val_feat, _, _, _, _ = build_q_features(val[['Patient','Weeks']].copy(), base_val, ecdf_bf, ecdf_pc, cats, fit=False)\n",
        "    val_feat['s_hat'] = val_feat['Patient'].map(s_map_val).astype(float).fillna(0.0)\n",
        "\n",
        "    # Strict alignment\n",
        "    trn_feat_aligned = trn_feat.merge(trn[['Patient','Weeks','FVC']], on=['Patient','Weeks'], how='inner')\n",
        "    val_feat_aligned = val_feat.merge(val[['Patient','Weeks','FVC']], on=['Patient','Weeks'], how='inner')\n",
        "\n",
        "    y_tr_delta = (trn_feat_aligned['FVC'].astype(float).values - trn_feat_aligned['Base_FVC'].astype(float).values)\n",
        "    y_va_delta = (val_feat_aligned['FVC'].astype(float).values - val_feat_aligned['Base_FVC'].astype(float).values)\n",
        "    X_tr = trn_feat_aligned[feat_cols].values.astype(float)\n",
        "    X_va = val_feat_aligned[feat_cols].values.astype(float)\n",
        "\n",
        "    if X_tr.shape[0] == 0 or X_va.shape[0] == 0:\n",
        "        print(f'[cat-bands Fold {fold}] skipped (X_tr={X_tr.shape[0]}, X_va={X_va.shape[0]})', flush=True)\n",
        "        del trn_df, val_df, trn, val, trn_feat, val_feat, trn_feat_aligned, val_feat_aligned\n",
        "        gc.collect()\n",
        "        continue\n",
        "\n",
        "    # TEST features under TRAIN-fold transforms; align to ss via index map\n",
        "    te_feat, _, _, _, _ = build_q_features(grid_te[['Patient','Weeks']].copy(), test_base, ecdf_bf, ecdf_pc, cats, fit=False)\n",
        "    te_feat['s_hat'] = te_feat['Patient'].map(s_map_test).astype(float).fillna(0.0)\n",
        "    X_te = te_feat[feat_cols].values.astype(float)\n",
        "    te_keys = te_feat[['Patient','Weeks']].copy().merge(grid_te_idx, on=['Patient','Weeks'], how='left')\n",
        "    te_idx = te_keys['ss_idx'].values.astype(int)\n",
        "\n",
        "    # For each alpha (0.2, 0.8), seed-bag CatBoost and accumulate\n",
        "    for a in alphas:\n",
        "        loss = f'Quantile:alpha={a}'\n",
        "        val_pred_sum = np.zeros(X_va.shape[0], dtype=float)\n",
        "        test_pred_sum_fold = np.zeros(ss.shape[0], dtype=float)\n",
        "        for sd in seeds:\n",
        "            params = dict(cb_params)\n",
        "            params['loss_function'] = loss\n",
        "            model = CatBoostRegressor(**params, random_state=sd)\n",
        "            model.fit(X_tr, y_tr_delta, eval_set=(X_va, y_va_delta))\n",
        "            val_pred_sum += model.predict(X_va)\n",
        "            pred_te = model.predict(X_te)\n",
        "            test_pred_sum_fold[te_idx] += pred_te\n",
        "            del model\n",
        "        val_pred_avg = val_pred_sum / max(len(seeds), 1)\n",
        "        test_pred_avg_fold = test_pred_sum_fold / max(len(seeds), 1)\n",
        "        # Write OOF deltas for this alpha\n",
        "        keys = val_feat_aligned[['Patient','Weeks']].reset_index(drop=True)\n",
        "        col = 'q20_delta_oof' if np.isclose(a, 0.2) else 'q80_delta_oof'\n",
        "        block = pd.DataFrame({'Patient': keys['Patient'].astype(str), 'Weeks': keys['Weeks'].astype(int), col: val_pred_avg})\n",
        "        oof_df = oof_df.merge(block, on=['Patient','Weeks'], how='left', suffixes=('', '_new'))\n",
        "        oof_df[col] = oof_df[col].fillna(oof_df[col + '_new'])\n",
        "        oof_df.drop(columns=[col + '_new'], inplace=True)\n",
        "        # Accumulate TEST preds\n",
        "        test_preds[a] += (test_pred_avg_fold / 5.0)\n",
        "\n",
        "    print(f'[cat-bands Fold {fold}] trn={X_tr.shape[0]} val={X_va.shape[0]} elapsed={time.time()-tf:.2f}s', flush=True)\n",
        "    del trn_df, val_df, trn, val, trn_feat, val_feat, trn_feat_aligned, val_feat_aligned, X_tr, X_va, X_te, te_feat, te_idx, te_keys\n",
        "    gc.collect()\n",
        "\n",
        "# Save OOF bands\n",
        "train_base = prepare_baseline_table(train)\n",
        "oof_save = oof_df.dropna(subset=['q20_delta_oof','q80_delta_oof']).merge(train_base[['Patient','Base_Week','Base_FVC']], on='Patient', how='left')\n",
        "oof_save.to_csv('oof_quantile_cat_v1_bands.csv', index=False)\n",
        "\n",
        "# Save TEST bands aligned to ss\n",
        "pd.DataFrame({\n",
        "    'Patient_Week': ss['Patient_Week'],\n",
        "    'q20_d': test_preds[0.2].astype(float),\n",
        "    'q80_d': test_preds[0.8].astype(float)\n",
        "}).to_csv('pred_quantile_deltas_cat_v1_bands.csv', index=False)\n",
        "\n",
        "print(f'Saved oof_quantile_cat_v1_bands.csv and pred_quantile_deltas_cat_v1_bands.csv. Elapsed {time.time()-t0:.1f}s')"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[cat-bands Fold 1] trn=1124 val=284 elapsed=1.15s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[cat-bands Fold 2] trn=1127 val=281 elapsed=1.20s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[cat-bands Fold 3] trn=1129 val=279 elapsed=1.35s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[cat-bands Fold 4] trn=1129 val=279 elapsed=1.17s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[cat-bands Fold 5] trn=1123 val=285 elapsed=1.16s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_quantile_cat_v1_bands.csv and pred_quantile_deltas_cat_v1_bands.csv. Elapsed 6.7s\n"
          ]
        }
      ]
    },
    {
      "id": "c4342796-da92-4b57-ba42-2d0d0fa41535",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 3d: Average LGBM v3 bands + CatBoost v1 bands; retune sigma on averaged-q50 OOF; build test submission\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "def laplace_ll_np(y_true, y_pred, sigma):\n",
        "    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float); sigma = np.asarray(sigma, float)\n",
        "    delta = np.minimum(np.abs(y_true - y_pred), 1000.0)\n",
        "    sigma = np.maximum(sigma, 70.0)\n",
        "    return float(np.mean(-delta / sigma - np.log(sigma)))\n",
        "\n",
        "def prepare_baseline_table(df):\n",
        "    base = (df.sort_values(['Patient','Weeks']).groupby('Patient', as_index=False).first())\n",
        "    base = base[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "        columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "    return base\n",
        "\n",
        "def compute_patient_slopes(df, patient_col='Patient', week_col='Weeks', target_col='FVC'):\n",
        "    slopes = {}\n",
        "    for pid, g in df.groupby(patient_col):\n",
        "        if g.shape[0] >= 2:\n",
        "            x = g[week_col].values.astype(float); y = g[target_col].values.astype(float)\n",
        "            xm = x.mean(); ym = y.mean()\n",
        "            denom = ((x - xm)**2).sum()\n",
        "            slope = ((x - xm) * (y - ym)).sum() / denom if denom > 0 else 0.0\n",
        "            slopes[pid] = slope\n",
        "    return slopes\n",
        "\n",
        "def robust_global_slope(slopes_dict):\n",
        "    if not slopes_dict: return 0.0\n",
        "    return float(np.median(list(slopes_dict.values())))\n",
        "\n",
        "# 1) Build corrected OOF with averaged q50 (LGBM v3 + Cat v1) and averaged bands (v3 + cat)\n",
        "oof_v3 = pd.read_csv('oof_quantile_lgbm_v3.csv')      # q50\n",
        "oof_cb = pd.read_csv('oof_quantile_cat_v1.csv')       # q50\n",
        "oof_b3 = pd.read_csv('oof_quantile_lgbm_v3_bands.csv')[['Patient','Weeks','q20_delta_oof','q80_delta_oof']]\n",
        "oof_bC = pd.read_csv('oof_quantile_cat_v1_bands.csv')[['Patient','Weeks','q20_delta_oof','q80_delta_oof']]\n",
        "\n",
        "# Deduplicate/average within sources\n",
        "oof_v3 = (oof_v3.groupby(['Patient','Weeks'], as_index=False)\n",
        "          .agg({'FVC':'first','Base_Week':'first','Base_FVC':'first','q50_delta_oof':'mean'}))\n",
        "oof_cb = (oof_cb.groupby(['Patient','Weeks'], as_index=False)\n",
        "          .agg({'q50_delta_oof':'mean'}))\n",
        "oof_b3 = (oof_b3.groupby(['Patient','Weeks'], as_index=False)\n",
        "          .agg({'q20_delta_oof':'mean','q80_delta_oof':'mean'}))\n",
        "oof_bC = (oof_bC.groupby(['Patient','Weeks'], as_index=False)\n",
        "          .agg({'q20_delta_oof':'mean','q80_delta_oof':'mean'}))\n",
        "\n",
        "# Merge and compute averaged q50 and averaged bands\n",
        "oof = (oof_v3.rename(columns={'q50_delta_oof':'q50_v3'})\n",
        "       .merge(oof_cb.rename(columns={'q50_delta_oof':'q50_cb'}), on=['Patient','Weeks'], how='inner')\n",
        "       .merge(oof_b3.rename(columns={'q20_delta_oof':'q20_b3','q80_delta_oof':'q80_b3'}), on=['Patient','Weeks'], how='inner')\n",
        "       .merge(oof_bC.rename(columns={'q20_delta_oof':'q20_cB','q80_delta_oof':'q80_cB'}), on=['Patient','Weeks'], how='inner'))\n",
        "\n",
        "oof['q50_avg'] = 0.5 * (oof['q50_v3'].astype(float) + oof['q50_cb'].astype(float))\n",
        "oof['q20_avg'] = 0.5 * (oof['q20_b3'].astype(float) + oof['q20_cB'].astype(float))\n",
        "oof['q80_avg'] = 0.5 * (oof['q80_b3'].astype(float) + oof['q80_cB'].astype(float))\n",
        "oof['band_avg'] = (oof['q80_avg'] - oof['q20_avg']).abs().astype(float)\n",
        "\n",
        "oof['dist'] = (oof['Weeks'] - oof['Base_Week']).astype(float)\n",
        "pre = oof.shape[0]\n",
        "oof = oof[(oof['dist'] >= 0) & oof[['q50_avg','band_avg']].notna().all(axis=1)].copy()\n",
        "oof = oof.sort_values(['Patient','Weeks']).drop_duplicates(['Patient','Weeks'])\n",
        "oof = oof[oof['dist'] > 0].copy()\n",
        "print(f\"[avgBands Diag] OOF pre={pre} post={oof.shape[0]} pats={oof['Patient'].nunique()}\")\n",
        "\n",
        "# Per-fold anchor (TRAIN-only) for gs_fold\n",
        "N_SPLITS = 5\n",
        "gkf = GroupKFold(n_splits=N_SPLITS)\n",
        "groups = train['Patient'].values\n",
        "patient_to_fold, fold_to_gs = {}, {}\n",
        "for fold, (trn_idx, val_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    trn_df = train.iloc[trn_idx]\n",
        "    gs_fold = robust_global_slope(compute_patient_slopes(trn_df))\n",
        "    fold_to_gs[fold] = gs_fold\n",
        "    for p in train.iloc[val_idx]['Patient'].astype(str).unique():\n",
        "        patient_to_fold[p] = fold\n",
        "oof['fold'] = oof['Patient'].astype(str).map(patient_to_fold).astype(int)\n",
        "oof['gs_fold'] = oof['fold'].map(fold_to_gs).astype(float)\n",
        "\n",
        "# 2) Build 70/30 averaged q50 + per-fold anchor; tune c per bin with banker floor; optional >=130 for |dist|>30\n",
        "base = oof['Base_FVC'].astype(float).values\n",
        "dist = oof['dist'].astype(float).values\n",
        "abs_dist = np.abs(dist).astype(float)\n",
        "fvc_q50_oof = base + oof['q50_avg'].astype(float).values\n",
        "fvc_anchor_oof = base + oof['gs_fold'].astype(float).values * dist\n",
        "fvc_blend_oof = 0.70 * fvc_q50_oof + 0.30 * fvc_anchor_oof\n",
        "y_oof = oof['FVC'].astype(float).values\n",
        "band_oof = oof['band_avg'].astype(float).values\n",
        "\n",
        "sigma_banker_oof = np.maximum(240.0 + 3.0 * abs_dist, 70.0)\n",
        "sigma_banker_oof = np.where(abs_dist > 20.0, np.maximum(sigma_banker_oof, 100.0), sigma_banker_oof)\n",
        "bins = [(0.0,5.0),(5.0,15.0),(15.0,1e9)]\n",
        "masks = [ (abs_dist>lo) & (abs_dist<=hi) for lo,hi in bins ]\n",
        "print('[avgBands Diag] bin counts:', [int(m.sum()) for m in masks])\n",
        "\n",
        "c_grid_short_mid = [1.3,1.4,1.5,1.6,1.7,1.8,2.0]\n",
        "c_grid_long = [1.3,1.4,1.5,1.6,1.7,1.8,2.0,2.3,2.4,2.5,2.6]\n",
        "best_c = {}\n",
        "for (lo,hi), m in zip(bins, masks):\n",
        "    if not np.any(m):\n",
        "        best_c[(lo,hi)] = 1.8\n",
        "        print(f'[avgBands Sigma] Bin ({lo},{hi}] empty; c=1.8')\n",
        "        continue\n",
        "    grid_c = c_grid_short_mid if hi<=15.0 else c_grid_long\n",
        "    b_ll, b_c = -1e9, 1.8\n",
        "    for c in grid_c:\n",
        "        sig = np.maximum(band_oof[m] / c, sigma_banker_oof[m])\n",
        "        ll = laplace_ll_np(y_oof[m], fvc_blend_oof[m], sig)\n",
        "        if ll > b_ll: b_ll, b_c = ll, c\n",
        "    best_c[(lo,hi)] = b_c\n",
        "    print(f\"[avgBands Sigma] Bin ({lo},{hi}] best c={b_c:.2f} OOF LL={b_ll:.5f}\")\n",
        "\n",
        "m_gt30 = abs_dist > 30.0\n",
        "use_floor130 = False\n",
        "if np.any(m_gt30):\n",
        "    sig_base = np.zeros_like(abs_dist)\n",
        "    for (lo,hi), m in zip(bins, masks):\n",
        "        if np.any(m): sig_base[m] = np.maximum(band_oof[m] / best_c[(lo,hi)], sigma_banker_oof[m])\n",
        "    ll_no130 = laplace_ll_np(y_oof, fvc_blend_oof, sig_base)\n",
        "    sig_130 = np.where(m_gt30, np.maximum(sig_base, 130.0), sig_base)\n",
        "    ll_130 = laplace_ll_np(y_oof, fvc_blend_oof, sig_130)\n",
        "    use_floor130 = (ll_130 >= ll_no130 - 1e-6)\n",
        "    print(f\"[avgBands Sigma] >=130 test: LL_no130={ll_no130:.5f} LL_130={ll_130:.5f} adopt={use_floor130}\")\n",
        "\n",
        "sig_oof = np.zeros_like(abs_dist)\n",
        "for (lo,hi), m in zip(bins, masks):\n",
        "    if np.any(m): sig_oof[m] = np.maximum(band_oof[m] / best_c[(lo,hi)], sigma_banker_oof[m])\n",
        "if use_floor130: sig_oof = np.where(abs_dist > 30.0, np.maximum(sig_oof, 130.0), sig_oof)\n",
        "ll_qb = laplace_ll_np(y_oof, fvc_blend_oof, sig_oof)\n",
        "ll_bk = laplace_ll_np(y_oof, fvc_blend_oof, sigma_banker_oof)\n",
        "print(f\"[avgBands OOF] rows={oof.shape[0]} pats={oof['Patient'].nunique()} LL_qband={ll_qb:.5f} | LL_banker={ll_bk:.5f}\")\n",
        "\n",
        "# 3) TEST: averaged q50_d (v3 + cat) 70/30 with full-train anchor; sigma from averaged bands; guardrails\n",
        "pred_v3 = pd.read_csv('pred_quantile_deltas_v3.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "pred_cb = pd.read_csv('pred_quantile_deltas_cat_v1.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "q50_d_te = 0.5 * (pred_v3['q50_d'].astype(float).values + pred_cb['q50_d'].astype(float).values)\n",
        "\n",
        "pred_b3 = pd.read_csv('pred_quantile_deltas_v3_bands.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "pred_bC = pd.read_csv('pred_quantile_deltas_cat_v1_bands.csv').set_index('Patient_Week').loc[ss['Patient_Week']].reset_index()\n",
        "q20_avg_te = 0.5 * (pred_b3['q20_d'].astype(float).values + pred_bC['q20_d'].astype(float).values)\n",
        "q80_avg_te = 0.5 * (pred_b3['q80_d'].astype(float).values + pred_bC['q80_d'].astype(float).values)\n",
        "band_te = np.abs(q80_avg_te - q20_avg_te).astype(float)\n",
        "\n",
        "grid = ss.copy()\n",
        "parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid['Patient'] = parts[0]; grid['Weeks'] = parts[1].astype(int)\n",
        "test = pd.read_csv('test.csv')\n",
        "test_base = test[['Patient','Weeks','FVC','Percent']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid = grid.merge(test_base, on='Patient', how='left')\n",
        "dist_te = (grid['Weeks'] - grid['Base_Week']).astype(float).values\n",
        "abs_dist_te = np.abs(dist_te).astype(float)\n",
        "base_fvc_te = grid['Base_FVC'].astype(float).values\n",
        "\n",
        "gs_full = robust_global_slope(compute_patient_slopes(train))\n",
        "fvc_q50_te = base_fvc_te + q50_d_te\n",
        "fvc_anchor_te = base_fvc_te + gs_full * dist_te\n",
        "fvc_te = 0.70 * fvc_q50_te + 0.30 * fvc_anchor_te\n",
        "fvc_te = np.clip(fvc_te, 500, 6000)\n",
        "\n",
        "# Tolerant non-increasing per patient (+25 ml) then pin dist==0\n",
        "df_out = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'FVC': fvc_te})\n",
        "def enforce_non_increasing_tolerant(g, tol=25.0):\n",
        "    g = g.sort_values('Weeks').copy()\n",
        "    f = g['FVC'].values.astype(float)\n",
        "    for i in range(len(f)-2, -1, -1):\n",
        "        f[i] = min(f[i], f[i+1] + tol)\n",
        "    g['FVC'] = f\n",
        "    return g\n",
        "df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(lambda g: enforce_non_increasing_tolerant(g, 25.0))\n",
        "fvc_final = df_out['FVC'].values.astype(float)\n",
        "fvc_final = np.where(abs_dist_te == 0.0, base_fvc_te, fvc_final)\n",
        "\n",
        "# Sigma: averaged bands per-bin with tuned c; banker floor; dist==0->70; optional >=130; per-patient monotone\n",
        "sigma_banker_te = np.maximum(240.0 + 3.0 * abs_dist_te, 70.0)\n",
        "sigma_banker_te = np.where(abs_dist_te > 20.0, np.maximum(sigma_banker_te, 100.0), sigma_banker_te)\n",
        "sigma_from_band = np.zeros_like(abs_dist_te, dtype=float)\n",
        "for (lo,hi), c in best_c.items():\n",
        "    m = (abs_dist_te>lo) & (abs_dist_te<=hi)\n",
        "    if np.any(m): sigma_from_band[m] = band_te[m] / c\n",
        "sigma_qband_te = np.maximum(sigma_from_band, sigma_banker_te)\n",
        "if use_floor130: sigma_qband_te = np.where(abs_dist_te > 30.0, np.maximum(sigma_qband_te, 130.0), sigma_qband_te)\n",
        "sigma_qband_te = np.where(abs_dist_te == 0.0, 70.0, sigma_qband_te)\n",
        "\n",
        "def enforce_sigma_monotone(df):\n",
        "    def _mono(g):\n",
        "        g = g.sort_values('dist').copy()\n",
        "        g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "        return g\n",
        "    return df.groupby('Patient', as_index=False, group_keys=False).apply(_mono)\n",
        "\n",
        "df_sig = pd.DataFrame({'Patient': grid['Patient'].values, 'Weeks': grid['Weeks'].values.astype(int), 'dist': abs_dist_te, 'Sigma': sigma_qband_te.astype(float)})\n",
        "df_sig = enforce_sigma_monotone(df_sig)\n",
        "sigma_final = df_sig['Sigma'].values.astype(float)\n",
        "\n",
        "sub = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_final, 'Confidence': sigma_final})\n",
        "sub.to_csv('submission_v3cat_qband_avgBands.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_v3cat_qband_avgBands.csv and set submission.csv. Elapsed {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[avgBands Diag] OOF pre=1387 post=1229 pats=158\n[avgBands Diag] bin counts: [286, 438, 505]\n[avgBands Sigma] Bin (0.0,5.0] best c=1.50 OOF LL=-6.04091\n[avgBands Sigma] Bin (5.0,15.0] best c=1.40 OOF LL=-6.12936\n[avgBands Sigma] Bin (15.0,1000000000.0] best c=1.60 OOF LL=-6.41398\n[avgBands Sigma] >=130 test: LL_no130=-6.22573 LL_130=-6.22573 adopt=True\n[avgBands OOF] rows=1229 pats=158 LL_qband=-6.22573 | LL_banker=-6.22573\nSaved submission_v3cat_qband_avgBands.csv and set submission.csv. Elapsed 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/181067228.py:173: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(lambda g: enforce_non_increasing_tolerant(g, 25.0))\n/tmp/ipykernel_4688/181067228.py:193: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df.groupby('Patient', as_index=False, group_keys=False).apply(_mono)\n"
          ]
        }
      ]
    },
    {
      "id": "ab27cfe9-468b-42b7-8eff-4e337c04410e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Parametric sigma (a + b*|dist|) floored by banker with progressive floors; hybrid FVC monotonicity on distance-aware blend\n",
        "import numpy as np, pandas as pd, time, warnings, gc\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "def laplace_ll_np(y_true, y_pred, sigma):\n",
        "    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float); sigma = np.asarray(sigma, float)\n",
        "    delta = np.minimum(np.abs(y_true - y_pred), 1000.0)\n",
        "    sigma = np.maximum(sigma, 70.0)\n",
        "    return float(np.mean(-delta / sigma - np.log(sigma)))\n",
        "\n",
        "def prepare_baseline_table(df):\n",
        "    base = (df.sort_values(['Patient','Weeks']).groupby('Patient', as_index=False).first())\n",
        "    base = base[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(\n",
        "        columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "    return base\n",
        "\n",
        "def compute_patient_slopes(df, patient_col='Patient', week_col='Weeks', target_col='FVC'):\n",
        "    slopes = {}\n",
        "    for pid, g in df.groupby(patient_col):\n",
        "        if g.shape[0] >= 2:\n",
        "            x = g[week_col].values.astype(float); y = g[target_col].values.astype(float)\n",
        "            xm = x.mean(); ym = y.mean()\n",
        "            denom = ((x - xm)**2).sum()\n",
        "            slope = ((x - xm) * (y - ym)).sum() / denom if denom > 0 else 0.0\n",
        "            slopes[pid] = slope\n",
        "    return slopes\n",
        "\n",
        "def robust_global_slope(slopes_dict):\n",
        "    if not slopes_dict: return 0.0\n",
        "    return float(np.median(list(slopes_dict.values())))\n",
        "\n",
        "# --- Build OOF sources for distance-aware blend (Slope+Anchor, LME, Quantile q50+per-fold anchor) ---\n",
        "def slope_anchor_oof(train_df, n_splits=5, seed=42):\n",
        "    from sklearn.linear_model import Ridge\n",
        "    from sklearn.neighbors import KNeighborsRegressor\n",
        "    gkf = GroupKFold(n_splits=n_splits)\n",
        "    groups = train_df['Patient'].values\n",
        "    y_list, d_list, fvc_s_list, fvc_a_list, pid_list, wk_list = [], [], [], [], [], []\n",
        "    for fold, (trn_idx, val_idx) in enumerate(gkf.split(train_df, groups=groups), 1):\n",
        "        trn = train_df.iloc[trn_idx].copy(); val = train_df.iloc[val_idx].copy()\n",
        "        base_trn = prepare_baseline_table(trn); base_val = prepare_baseline_table(val)\n",
        "        slopes_tr = compute_patient_slopes(trn)\n",
        "        lab = pd.DataFrame({'Patient': list(slopes_tr.keys()), 's_label': list(slopes_tr.values())})\n",
        "        # Fit slope head on TRAIN baseline\n",
        "        bf_trn, feat_cols, ecdf_bf, ecdf_pc, cats = build_slope_features(base_trn.merge(lab, on='Patient', how='left'), fit=True)\n",
        "        bf_val, _, _, _, _ = build_slope_features(base_val, ecdf_bf, ecdf_pc, cats, fit=False)\n",
        "        sc = StandardScaler(with_mean=True, with_std=True)\n",
        "        X_tr = bf_trn[feat_cols].values.astype(float); y_tr = bf_trn['s_label'].fillna(0.0).values.astype(float)\n",
        "        X_trs = sc.fit_transform(X_tr); X_vs = sc.transform(bf_val[feat_cols].values.astype(float))\n",
        "        ridge = Ridge(alpha=1.0, random_state=seed).fit(X_trs, y_tr)\n",
        "        knn   = KNeighborsRegressor(n_neighbors=9, weights='distance').fit(X_trs, y_tr)\n",
        "        s_r = ridge.predict(X_vs); s_k = knn.predict(X_vs)\n",
        "        q_lo, q_hi = np.percentile(y_tr, [5,95])\n",
        "        s_bl = np.clip(0.80*s_r + 0.20*s_k, q_lo, q_hi)\n",
        "        s_map = dict(zip(base_val['Patient'].values, s_bl))\n",
        "        valm = val.merge(base_val[['Patient','Base_Week','Base_FVC']], on='Patient', how='left')\n",
        "        mask = (valm['Weeks'] >= valm['Base_Week'])\n",
        "        dist = (valm['Weeks'] - valm['Base_Week']).astype(float)\n",
        "        fvc_s = (valm['Base_FVC'].values + valm['Patient'].map(s_map).fillna(0.0).values * dist).astype(float)\n",
        "        gs_fold = robust_global_slope(compute_patient_slopes(trn))\n",
        "        fvc_a = (valm['Base_FVC'].values + gs_fold * dist).astype(float)\n",
        "        y_list.append(valm.loc[mask,'FVC'].values.astype(float)); d_list.append(dist.values[mask].astype(float))\n",
        "        fvc_s_list.append(fvc_s[mask].astype(float)); fvc_a_list.append(fvc_a[mask].astype(float))\n",
        "        pid_list.append(valm.loc[mask,'Patient'].astype(str).values); wk_list.append(valm.loc[mask,'Weeks'].astype(int).values)\n",
        "        del trn, val, base_trn, base_val, bf_trn, bf_val, X_tr, X_trs, X_vs; gc.collect()\n",
        "    return (np.concatenate(y_list), np.concatenate(d_list), np.concatenate(fvc_s_list), np.concatenate(fvc_a_list), np.concatenate(pid_list), np.concatenate(wk_list))\n",
        "\n",
        "def lme_oof(train_df, n_splits=5):\n",
        "    gkf = GroupKFold(n_splits=5); groups = train_df['Patient'].values\n",
        "    y_list, d_list, fvc_list, pid_list, wk_list = [], [], [], [], []\n",
        "    for fold, (trn_idx, val_idx) in enumerate(gkf.split(train_df, groups=groups), 1):\n",
        "        trn = train_df.iloc[trn_idx].copy(); val = train_df.iloc[val_idx].copy()\n",
        "        base_trn = prepare_baseline_table(trn); base_val = prepare_baseline_table(val)\n",
        "        trn_l = trn.drop(columns=['Age','Sex','SmokingStatus'], errors='ignore').merge(base_trn[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "        trn_l['Weeks_Passed'] = (trn_l['Weeks'] - trn_l['Base_Week']).astype(float)/10.0\n",
        "        trn_l = trn_l[trn_l['Weeks_Passed'] >= 0].copy()\n",
        "        age_mean, age_std = trn_l['Age'].mean(), trn_l['Age'].std()+1e-9\n",
        "        pc_mean, pc_std   = trn_l['Percent_at_base'].mean(), trn_l['Percent_at_base'].std()+1e-9\n",
        "        trn_l['Age_std'] = (trn_l['Age'] - age_mean)/age_std; trn_l['Percent_at_base_std'] = (trn_l['Percent_at_base'] - pc_mean)/pc_std\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter('ignore')\n",
        "            md = smf.mixedlm('FVC ~ 1 + Weeks_Passed + I(Weeks_Passed**2) + Age_std + C(Sex) + C(SmokingStatus) + Percent_at_base_std + Age_std:Percent_at_base_std',\n",
        "                              data=trn_l, groups=trn_l['Patient'], re_formula='~Weeks_Passed')\n",
        "            mdf = md.fit(method='lbfgs', reml=True, maxiter=500, disp=False)\n",
        "        val_l = val.drop(columns=['Age','Sex','SmokingStatus'], errors='ignore').merge(base_val[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "        mask = (val_l['Weeks'] >= val_l['Base_Week'])\n",
        "        dist = (val_l['Weeks'] - val_l['Base_Week']).astype(float)\n",
        "        val_l['Weeks_Passed'] = dist/10.0\n",
        "        val_l['Age_std'] = (val_l['Age'] - age_mean)/age_std; val_l['Percent_at_base_std'] = (val_l['Percent_at_base'] - pc_mean)/pc_std\n",
        "        fvc_pred = mdf.predict(val_l).astype(float).values\n",
        "        y_list.append(val_l.loc[mask,'FVC'].values.astype(float)); d_list.append(dist.values[mask].astype(float));\n",
        "        fvc_list.append(fvc_pred[mask].astype(float)); pid_list.append(val_l.loc[mask,'Patient'].astype(str).values); wk_list.append(val_l.loc[mask,'Weeks'].astype(int).values)\n",
        "        del trn, val, base_trn, base_val, trn_l, val_l; gc.collect()\n",
        "    return np.concatenate(y_list), np.concatenate(d_list), np.concatenate(fvc_list), np.concatenate(pid_list), np.concatenate(wk_list)\n",
        "\n",
        "def build_slope_features(base_df, ecdf_basefvc=None, ecdf_percent=None, cats=None, fit=False):\n",
        "    # Light adapter using earlier definitions already in notebook\n",
        "    b = base_df.copy()\n",
        "    b['log_Base_FVC'] = np.log1p(np.maximum(b['Base_FVC'].astype(float), 1.0))\n",
        "    b['BaseFVC_over_Age'] = b['Base_FVC'].astype(float) / np.maximum(b['Age'].astype(float), 1.0)\n",
        "    b['PercentBase_over_Age'] = b['Percent_at_base'].astype(float) / np.maximum(b['Age'].astype(float), 1.0)\n",
        "    if fit:\n",
        "        ecdf_basefvc = np.sort(b['Base_FVC'].values.astype(float))\n",
        "        ecdf_percent = np.sort(b['Percent_at_base'].values.astype(float))\n",
        "    def ecdf_rank_transform(x, xs):\n",
        "        x = np.asarray(x, float); idx = np.searchsorted(xs, x, side='right'); return idx / max(len(xs), 1)\n",
        "    b['BaseFVC_ecdf'] = ecdf_rank_transform(b['Base_FVC'].values, ecdf_basefvc)\n",
        "    b['Percent_ecdf'] = ecdf_rank_transform(b['Percent_at_base'].values, ecdf_percent)\n",
        "    if fit:\n",
        "        cats = {'Sex': sorted(b['Sex'].dropna().astype(str).unique().tolist()), 'SmokingStatus': sorted(b['SmokingStatus'].dropna().astype(str).unique().tolist())}\n",
        "    # one-hot\n",
        "    out = b.copy()\n",
        "    for c, values in cats.items():\n",
        "        col = b[c].astype(str)\n",
        "        for v in values:\n",
        "            out[f'{c}__{v}'] = (col == v).astype(np.int8)\n",
        "    num_cols = ['Age','Base_FVC','log_Base_FVC','Percent_at_base','BaseFVC_over_Age','PercentBase_over_Age','BaseFVC_ecdf','Percent_ecdf']\n",
        "    cat_cols = [c for c in out.columns if c.startswith('Sex__') or c.startswith('SmokingStatus__')]\n",
        "    feat_cols = num_cols + cat_cols\n",
        "    return out, feat_cols, ecdf_basefvc, ecdf_percent, cats\n",
        "\n",
        "# Build OOF arrays\n",
        "y_s, d_s, fvc_s, fvc_a, pid_s, wk_s = slope_anchor_oof(train, 5, 42)\n",
        "y_l, d_l, fvc_l, pid_l, wk_l = lme_oof(train, 5)\n",
        "oof_q = pd.read_csv('oof_quantile_lgbm_v2.csv')\n",
        "train_base = prepare_baseline_table(train)\n",
        "oof_q = oof_q.merge(train_base[['Patient','Base_Week','Base_FVC']], on='Patient', how='left', suffixes=('', '_base'))\n",
        "if 'Base_FVC_base' in oof_q.columns:\n",
        "    if 'Base_FVC' not in oof_q.columns: oof_q['Base_FVC'] = oof_q['Base_FVC_base']\n",
        "    else: oof_q['Base_FVC'] = oof_q['Base_FVC'].fillna(oof_q['Base_FVC_base'])\n",
        "    oof_q.drop(columns=['Base_FVC_base'], inplace=True)\n",
        "if 'Base_Week_base' in oof_q.columns and 'Base_Week' not in oof_q.columns:\n",
        "    oof_q['Base_Week'] = oof_q['Base_Week_base']; oof_q.drop(columns=['Base_Week_base'], inplace=True)\n",
        "oof_q['dist'] = (oof_q['Weeks'] - oof_q['Base_Week']).astype(float)\n",
        "oof_q = oof_q[oof_q['dist'] >= 0].dropna(subset=['q50_delta_oof']).copy()\n",
        "\n",
        "# Per-fold anchors for quantile OOF\n",
        "N_SPLITS = 5\n",
        "gkf = GroupKFold(n_splits=N_SPLITS)\n",
        "groups = train['Patient'].values\n",
        "patient_to_fold, fold_to_gs = {}, {}\n",
        "for fold, (trn_idx, val_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    trn_df = train.iloc[trn_idx]\n",
        "    gs_fold = robust_global_slope(compute_patient_slopes(trn_df))\n",
        "    fold_to_gs[fold] = gs_fold\n",
        "    for p in train.iloc[val_idx]['Patient'].astype(str).unique():\n",
        "        patient_to_fold[p] = fold\n",
        "oof_q['fold'] = oof_q['Patient'].astype(str).map(patient_to_fold).astype(int)\n",
        "oof_q['gs_fold'] = oof_q['fold'].map(fold_to_gs).astype(float)\n",
        "fvc_anchor_q = oof_q['Base_FVC'].astype(float).values + oof_q['gs_fold'].values * oof_q['dist'].astype(float).values\n",
        "fvc_q_point = oof_q['Base_FVC'].astype(float).values + oof_q['q50_delta_oof'].astype(float).values\n",
        "fvc_q = 0.70 * fvc_q_point + 0.30 * fvc_anchor_q\n",
        "\n",
        "# Align OOF by keys\n",
        "df_s = pd.DataFrame({'Patient': pid_s.astype(str), 'Weeks': wk_s.astype(int), 'y_true': y_s.astype(float), 'dist': d_s.astype(float), 'fvc_s': fvc_s.astype(float), 'fvc_a': fvc_a.astype(float)})\n",
        "df_l = pd.DataFrame({'Patient': pid_l.astype(str), 'Weeks': wk_l.astype(int), 'fvc_l': fvc_l.astype(float)})\n",
        "df_q = oof_q[['Patient','Weeks']].astype({'Patient':'str','Weeks':'int'}).copy()\n",
        "df_q['fvc_q'] = fvc_q.astype(float)\n",
        "dfm = df_s.merge(df_l, on=['Patient','Weeks'], how='inner').merge(df_q, on=['Patient','Weeks'], how='inner')\n",
        "\n",
        "y = dfm['y_true'].values.astype(float)\n",
        "dist = dfm['dist'].values.astype(float)\n",
        "s = dfm['fvc_s'].values.astype(float)\n",
        "a = dfm['fvc_a'].values.astype(float)\n",
        "l = dfm['fvc_l'].values.astype(float)\n",
        "q = dfm['fvc_q'].values.astype(float)\n",
        "\n",
        "sigma_banker_oof = np.maximum(240.0 + 3.0 * np.abs(dist), 70.0)\n",
        "sigma_banker_oof = np.where(np.abs(dist) > 20.0, np.maximum(sigma_banker_oof, 100.0), sigma_banker_oof)\n",
        "\n",
        "def grid_best(y, s, l, q, sigma, w_grid=np.arange(0.0, 1.01, 0.05)):\n",
        "    best_ll, best_w = -1e9, (0.05, 0.05, 0.90)\n",
        "    for ws in w_grid:\n",
        "        for wl in w_grid:\n",
        "            wq = 1.0 - ws - wl\n",
        "            if wq < 0 or wq > 1: continue\n",
        "            pred = ws*s + wl*l + wq*q\n",
        "            ll = laplace_ll_np(y, pred, sigma)\n",
        "            if ll > best_ll: best_ll, best_w = ll, (ws, wl, wq)\n",
        "    return best_ll, best_w\n",
        "\n",
        "bins = [(0.0,5.0),(5.0,15.0),(15.0,1e9)]\n",
        "weights = {}\n",
        "for lo,hi in bins:\n",
        "    m = (np.abs(dist)>lo) & (np.abs(dist)<=hi)\n",
        "    if not np.any(m):\n",
        "        weights[(lo,hi)] = (0.05,0.05,0.90)\n",
        "    else:\n",
        "        _, w = grid_best(y[m], s[m], l[m], q[m], sigma_banker_oof[m])\n",
        "        weights[(lo,hi)] = w\n",
        "print('[ParamSigma] OOF weights by bin (S/L/Q):', {k: tuple(round(x,2) for x in v) for k,v in weights.items()})\n",
        "\n",
        "# Build blended OOF FVC using bin weights\n",
        "fvc_blend_oof = np.zeros_like(y)\n",
        "for (lo,hi), (ws, wl, wq) in weights.items():\n",
        "    m = (np.abs(dist)>lo) & (np.abs(dist)<=hi)\n",
        "    if np.any(m): fvc_blend_oof[m] = ws*s[m] + wl*l[m] + wq*q[m]\n",
        "\n",
        "# Parametric sigma grid with progressive floors and banker floor\n",
        "a_grid = [80, 90, 100, 110, 120, 140]\n",
        "b_grid = [1.4, 1.6, 1.8, 2.0]\n",
        "best = (-1e9, None, None)\n",
        "for a0 in a_grid:\n",
        "    for b0 in b_grid:\n",
        "        sig = a0 + b0 * np.abs(dist)\n",
        "        sig = np.maximum(sig, 70.0)\n",
        "        sig = np.where(np.abs(dist) > 20.0, np.maximum(sig, 100.0), sig)\n",
        "        sig = np.where(np.abs(dist) > 30.0, np.maximum(sig, 130.0), sig)\n",
        "        sig = np.where(np.abs(dist) > 40.0, np.maximum(sig, 160.0), sig)\n",
        "        sig = np.maximum(sig, sigma_banker_oof)\n",
        "        ll = laplace_ll_np(y, fvc_blend_oof, sig)\n",
        "        if ll > best[0]: best = (ll, a0, b0)\n",
        "ll_banker_oof = laplace_ll_np(y, fvc_blend_oof, sigma_banker_oof)\n",
        "print(f\"[ParamSigma] Best OOF (a,b)=({best[1]},{best[2]:.2f}) LL={best[0]:.5f} vs BANKER={ll_banker_oof:.5f} \u0394={best[0]-ll_banker_oof:+.5f}\")\n",
        "adopt_sigma = (best[0] >= ll_banker_oof + 0.002)\n",
        "\n",
        "# --- Build TEST: distance-aware blend with learned weights; hybrid FVC monotonicity; parametric sigma ---\n",
        "def load_fvc(path):\n",
        "    return pd.read_csv(path).set_index('Patient_Week').loc[ss['Patient_Week'],'FVC'].astype(float).values\n",
        "\n",
        "fvc_s_test = load_fvc('submission_slope_anchor_banker_wA60.csv') if pd.io.common.file_exists('submission_slope_anchor_banker_wA60.csv') else load_fvc('submission_slope_anchor_banker.csv')\n",
        "try:\n",
        "    fvc_l_test = load_fvc('submission_lme_banker.csv')\n",
        "except Exception:\n",
        "    # build LME banker quickly if missing\n",
        "    base_tr = prepare_baseline_table(train)\n",
        "    trn_l = train.drop(columns=['Age','Sex','SmokingStatus'], errors='ignore').merge(base_tr[['Patient','Base_Week','Base_FVC','Percent_at_base','Age','Sex','SmokingStatus']], on='Patient', how='left')\n",
        "    trn_l['Weeks_Passed'] = (trn_l['Weeks'] - trn_l['Base_Week']).astype(float)/10.0\n",
        "    trn_l = trn_l[trn_l['Weeks_Passed'] >= 0].copy()\n",
        "    age_mean, age_std = trn_l['Age'].mean(), trn_l['Age'].std()+1e-9\n",
        "    pc_mean, pc_std   = trn_l['Percent_at_base'].mean(), trn_l['Percent_at_base'].std()+1e-9\n",
        "    trn_l['Age_std'] = (trn_l['Age'] - age_mean)/age_std; trn_l['Percent_at_base_std'] = (trn_l['Percent_at_base'] - pc_mean)/pc_std\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter('ignore')\n",
        "        md = smf.mixedlm('FVC ~ 1 + Weeks_Passed + I(Weeks_Passed**2) + Age_std + C(Sex) + C(SmokingStatus) + Percent_at_base_std + Age_std:Percent_at_base_std',\n",
        "                          data=trn_l, groups=trn_l['Patient'], re_formula='~Weeks_Passed')\n",
        "        mdf = md.fit(method='lbfgs', reml=True, maxiter=500, disp=False)\n",
        "    grid = ss.copy()\n",
        "    parts = grid['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "    grid['Patient'] = parts[0]; grid['Weeks'] = parts[1].astype(int)\n",
        "    test_bl = test[['Patient','Weeks','FVC','Percent','Age','Sex','SmokingStatus']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "    grid = grid.merge(test_bl, on='Patient', how='left')\n",
        "    grid['Weeks_Passed'] = (grid['Weeks'] - grid['Base_Week']).astype(float)/10.0\n",
        "    grid['Age_std'] = (grid['Age'].astype(float) - age_mean) / (age_std)\n",
        "    grid['Percent_at_base_std'] = (grid['Percent_at_base'].astype(float) - pc_mean) / (pc_std)\n",
        "    fvc_l_test = mdf.predict(grid).astype(float).values\n",
        "\n",
        "# Quantile blended test FVC; rebuild test blend with weights from OOF\n",
        "grid_te = ss.copy()\n",
        "parts = grid_te['Patient_Week'].str.rsplit('_', n=1, expand=True)\n",
        "grid_te['Patient'] = parts[0]; grid_te['Weeks'] = parts[1].astype(int)\n",
        "test_base = test[['Patient','Weeks','FVC','Percent']].rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Percent_at_base'})\n",
        "grid_te = grid_te.merge(test_base, on='Patient', how='left')\n",
        "abs_dist_te = np.abs((grid_te['Weeks'] - grid_te['Base_Week']).astype(float).values)\n",
        "\n",
        "fvc_q_test = pd.read_csv('submission_quantile_lgbm_v2.csv').set_index('Patient_Week').loc[ss['Patient_Week'],'FVC'].astype(float).values\n",
        "\n",
        "fvc_blend_test = np.zeros_like(fvc_q_test)\n",
        "for (lo,hi), (ws, wl, wq) in weights.items():\n",
        "    m = (abs_dist_te > lo) & (abs_dist_te <= hi)\n",
        "    fvc_blend_test[m] = ws*fvc_s_test[m] + wl*fvc_l_test[m] + wq*fvc_q_test[m]\n",
        "\n",
        "# Hybrid FVC monotonicity: tol +25 for |dist|<=10, strict for >10; then pin dist==0; clip\n",
        "dist_te = (grid_te['Weeks'] - grid_te['Base_Week']).astype(float).values\n",
        "base_fvc_te = grid_te['Base_FVC'].astype(float).values\n",
        "fvc_blend_test = np.where(dist_te == 0.0, base_fvc_te, np.clip(fvc_blend_test, 500, 6000))\n",
        "df_out = pd.DataFrame({'Patient': grid_te['Patient'].values, 'Weeks': grid_te['Weeks'].values.astype(int), 'FVC': fvc_blend_test, 'abs_dist': abs_dist_te})\n",
        "def enforce_hybrid_mono(g, tol_short=25.0, thr=10.0):\n",
        "    g = g.sort_values('Weeks').copy()\n",
        "    f = g['FVC'].values.astype(float)\n",
        "    d = g['abs_dist'].values.astype(float)\n",
        "    for i in range(len(f)-2, -1, -1):\n",
        "        tol = tol_short if d[i] <= thr else 0.0\n",
        "        f[i] = min(f[i], f[i+1] + tol)\n",
        "    g['FVC'] = f\n",
        "    return g\n",
        "df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(enforce_hybrid_mono)\n",
        "fvc_final = df_out['FVC'].values.astype(float)\n",
        "fvc_final = np.where(abs_dist_te == 0.0, base_fvc_te, fvc_final)\n",
        "\n",
        "# Parametric sigma on TEST\n",
        "a_best, b_best = best[1], best[2]\n",
        "sigma_param = a_best + b_best * abs_dist_te\n",
        "sigma_param = np.maximum(sigma_param, 70.0)\n",
        "sigma_param = np.where(abs_dist_te > 20.0, np.maximum(sigma_param, 100.0), sigma_param)\n",
        "sigma_param = np.where(abs_dist_te > 30.0, np.maximum(sigma_param, 130.0), sigma_param)\n",
        "sigma_param = np.where(abs_dist_te > 40.0, np.maximum(sigma_param, 160.0), sigma_param)\n",
        "sigma_banker_te = np.maximum(240.0 + 3.0 * abs_dist_te, 70.0)\n",
        "sigma_banker_te = np.where(abs_dist_te > 20.0, np.maximum(sigma_banker_te, 100.0), sigma_banker_te)\n",
        "sigma_te = np.maximum(sigma_param, sigma_banker_te) if adopt_sigma else sigma_banker_te\n",
        "\n",
        "# Per-patient monotone in |dist| for sigma\n",
        "df_sig = pd.DataFrame({'Patient': grid_te['Patient'].values, 'Weeks': grid_te['Weeks'].values.astype(int), 'dist': abs_dist_te, 'Sigma': sigma_te.astype(float)})\n",
        "def enforce_sigma_monotone(g):\n",
        "    g = g.sort_values('dist').copy()\n",
        "    g['Sigma'] = np.maximum.accumulate(g['Sigma'].values)\n",
        "    return g\n",
        "df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n",
        "sigma_final = df_sig['Sigma'].values.astype(float)\n",
        "\n",
        "sub = pd.DataFrame({'Patient_Week': ss['Patient_Week'], 'FVC': fvc_final, 'Confidence': sigma_final})\n",
        "sub.to_csv('submission_distance_blend_paramSigma_hybridMono.csv', index=False)\n",
        "if adopt_sigma:\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print(f\"Saved submission_distance_blend_paramSigma_hybridMono.csv and set submission.csv. Adopt_sigma={adopt_sigma} | OOF gain={best[0]-ll_banker_oof:+.5f} | Elapsed {time.time()-t0:.1f}s\")\n",
        "else:\n",
        "    print(f\"Saved submission_distance_blend_paramSigma_hybridMono.csv. Adopt_sigma={adopt_sigma}; submission.csv unchanged. OOF gain={best[0]-ll_banker_oof:+.5f} | Elapsed {time.time()-t0:.1f}s\")"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ParamSigma] OOF weights by bin (S/L/Q): {(0.0, 5.0): (0.05, 0.0, 0.95), (5.0, 15.0): (0.0, 0.0, 1.0), (15.0, 1000000000.0): (0.05, 0.05, 0.9)}\n[ParamSigma] Best OOF (a,b)=(80,1.40) LL=-6.95557 vs BANKER=-6.95557 \u0394=+0.00000\nSaved submission_distance_blend_paramSigma_hybridMono.csv and set submission.csv. Adopt_sigma=False | OOF gain=+0.00000 | Elapsed 3.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_4688/4177764066.py:287: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_out = df_out.groupby('Patient', as_index=False, group_keys=False).apply(enforce_hybrid_mono)\n/tmp/ipykernel_4688/4177764066.py:308: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df_sig = df_sig.groupby('Patient', as_index=False, group_keys=False).apply(enforce_sigma_monotone)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}