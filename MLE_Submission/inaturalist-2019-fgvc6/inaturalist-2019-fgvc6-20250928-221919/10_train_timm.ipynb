{
  "cells": [
    {
      "id": "11f4fcaa-2bc0-4b4c-ba6f-01a4f43592e6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup: install timm (respect existing torch), load JSONs, build label mapping, stratified split\n",
        "import os, sys, json, math, random, time, shutil, subprocess\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "def run(cmd):\n",
        "    print('>', ' '.join(cmd), flush=True)\n",
        "    subprocess.run(cmd, check=True)\n",
        "\n",
        "def pip_install_pkgs():\n",
        "    # Freeze torch stack; avoid re-installing torch/torchvision\n",
        "    cons = Path('constraints.txt')\n",
        "    if not cons.exists():\n",
        "        cons.write_text('torch==2.4.1\\ntorchvision==0.19.1\\ntorchaudio==2.4.1\\n')\n",
        "    # Install timm without deps to prevent torch reinstallation\n",
        "    run([sys.executable, '-m', 'pip', 'install', 'timm==1.0.9', '--no-deps'])\n",
        "    # Install required deps including HF hub BEFORE importing timm so timm sees them\n",
        "    run([sys.executable, '-m', 'pip', 'install', '-c', 'constraints.txt', 'albumentations', 'opencv-python-headless', 'huggingface_hub', 'safetensors', '--upgrade-strategy', 'only-if-needed'])\n",
        "\n",
        "print('torch:', torch.__version__, 'cuda build:', getattr(torch.version,'cuda', None), 'cuda avail:', torch.cuda.is_available(), flush=True)\n",
        "pip_install_pkgs()\n",
        "import timm\n",
        "\n",
        "cudnn.benchmark = True\n",
        "torch.set_float32_matmul_precision('high')\n",
        "print('GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU', flush=True)\n",
        "\n",
        "# Load JSONs\n",
        "def load_json(fp):\n",
        "    with open(fp, 'r') as f:\n",
        "        return json.load(f)\n",
        "train_js = load_json('train2019.json')\n",
        "test_js = load_json('test2019.json')\n",
        "\n",
        "# Build dataframe from train JSON using JSON-based category_id\n",
        "train_images = {im['id']: im for im in train_js['images']}\n",
        "rows = []\n",
        "for ann in train_js['annotations']:\n",
        "    im = train_images.get(ann['image_id'])\n",
        "    if im is None: continue\n",
        "    rows.append({\n",
        "        'image_id': ann['image_id'],\n",
        "        'file_name': im['file_name'],\n",
        "        'category_id': ann['category_id']\n",
        "    })\n",
        "df = pd.DataFrame(rows)\n",
        "print('Train rows:', len(df), 'unique cats:', df['category_id'].nunique(), flush=True)\n",
        "\n",
        "# Label mapping: map arbitrary category_id -> contiguous [0..C-1]\n",
        "cat_ids = sorted(df['category_id'].unique().tolist())\n",
        "cat_id_to_idx = {cid:i for i,cid in enumerate(cat_ids)}\n",
        "idx_to_cat_id = {i:cid for cid,i in cat_id_to_idx.items()}\n",
        "df['label'] = df['category_id'].map(cat_id_to_idx)\n",
        "num_classes = len(cat_ids)\n",
        "print('num_classes:', num_classes, 'min/max cat_id:', min(cat_ids), max(cat_ids), flush=True)\n",
        "\n",
        "# Stratified split from train (since official val images are not present in extracted files)\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.0115, random_state=42)  # ~3030 holdout to mirror official val size\n",
        "train_idx, val_idx = next(sss.split(df.index.values, df['label'].values))\n",
        "df_train = df.iloc[train_idx].reset_index(drop=True)\n",
        "df_val = df.iloc[val_idx].reset_index(drop=True)\n",
        "print('Split -> train:', len(df_train), 'val:', len(df_val), flush=True)\n",
        "\n",
        "# Save mappings for reuse\n",
        "Path('artifacts').mkdir(exist_ok=True)\n",
        "pd.Series(idx_to_cat_id).to_json('artifacts/idx_to_cat_id.json')\n",
        "pd.Series(cat_id_to_idx).to_json('artifacts/cat_id_to_idx.json')\n",
        "df_train.to_csv('artifacts/train_split.csv', index=False)\n",
        "df_val.to_csv('artifacts/val_split.csv', index=False)\n",
        "print('Prepared splits and mappings.', flush=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.4.1+cu121 cuda build: 12.1 cuda avail: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> /usr/bin/python3.11 -m pip install timm==1.0.9 --no-deps\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm==1.0.9\n  Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.3/2.3 MB 57.2 MB/s eta 0:00:00\nInstalling collected packages: timm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed timm-1.0.9\n> /usr/bin/python3.11 -m pip install -c constraints.txt albumentations opencv-python-headless huggingface_hub safetensors --upgrade-strategy only-if-needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/timm already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/timm-1.0.9.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting albumentations\n  Downloading albumentations-2.0.8-py3-none-any.whl (369 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 369.4/369.4 KB 13.3 MB/s eta 0:00:00\nCollecting opencv-python-headless\n  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 54.0/54.0 MB 113.3 MB/s eta 0:00:00\nCollecting huggingface_hub\n  Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 563.3/563.3 KB 234.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting safetensors\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 485.8/485.8 KB 510.4 MB/s eta 0:00:00\nCollecting PyYAML\n  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 806.6/806.6 KB 508.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy>=1.10.0\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.9/35.9 MB 183.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydantic>=2.9.2\n  Downloading pydantic-2.11.9-py3-none-any.whl (444 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 444.9/444.9 KB 515.6 MB/s eta 0:00:00\nCollecting albucore==0.0.24\n  Downloading albucore-0.0.24-py3-none-any.whl (15 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy>=1.24.4\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 223.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting simsimd>=5.9.2\n  Downloading simsimd-6.5.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.1/1.1 MB 540.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stringzilla>=3.10.4\n  Downloading stringzilla-4.0.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (496 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 496.5/496.5 KB 507.2 MB/s eta 0:00:00\nCollecting opencv-python-headless\n  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 50.0/50.0 MB 221.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting packaging>=20.9\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 423.0 MB/s eta 0:00:00\nCollecting fsspec>=2023.5.0\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 505.7 MB/s eta 0:00:00\nCollecting tqdm>=4.42.1\n  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.5/78.5 KB 472.5 MB/s eta 0:00:00\nCollecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\nCollecting typing-extensions>=3.7.4.3\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 333.7 MB/s eta 0:00:00\nCollecting requests\n  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.7/64.7 KB 428.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hf-xet<2.0.0,>=1.1.3\n  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.2/3.2 MB 479.9 MB/s eta 0:00:00\nCollecting annotated-types>=0.6.0\n  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\nCollecting typing-inspection>=0.4.0\n  Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydantic-core==2.33.2\n  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 533.8 MB/s eta 0:00:00\nCollecting idna<4,>=2.5\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 70.4/70.4 KB 436.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting charset_normalizer<4,>=2\n  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 150.3/150.3 KB 510.0 MB/s eta 0:00:00\nCollecting urllib3<3,>=1.21.1\n  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 129.8/129.8 KB 496.1 MB/s eta 0:00:00\nCollecting certifi>=2017.4.17\n  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 161.2/161.2 KB 455.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: simsimd, urllib3, typing-extensions, tqdm, stringzilla, safetensors, PyYAML, packaging, numpy, idna, hf-xet, fsspec, filelock, charset_normalizer, certifi, annotated-types, typing-inspection, scipy, requests, pydantic-core, opencv-python-headless, pydantic, huggingface_hub, albucore, albumentations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed PyYAML-6.0.3 albucore-0.0.24 albumentations-2.0.8 annotated-types-0.7.0 certifi-2025.8.3 charset_normalizer-3.4.3 filelock-3.19.1 fsspec-2025.9.0 hf-xet-1.1.10 huggingface_hub-0.35.1 idna-3.10 numpy-1.26.4 opencv-python-headless-4.11.0.86 packaging-25.0 pydantic-2.11.9 pydantic-core-2.33.2 requests-2.32.5 safetensors-0.6.2 scipy-1.16.2 simsimd-6.5.3 stringzilla-4.0.14 tqdm-4.67.1 typing-extensions-4.15.0 typing-inspection-0.4.1 urllib3-2.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/albumentations-2.0.8.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/albumentations already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/albucore-0.0.24.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/albucore already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/huggingface_hub-0.35.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/huggingface_hub already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pydantic-2.11.9.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pydantic already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/cv2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/opencv_python_headless-4.11.0.86.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/opencv_python_headless.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pydantic_core already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pydantic_core-2.33.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/requests-2.32.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/requests already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy-1.16.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_inspection-0.4.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_inspection already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/annotated_types-0.7.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/annotated_types already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/certifi-2025.8.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/certifi already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/charset_normalizer-3.4.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/charset_normalizer already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock-3.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec-2025.9.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/hf_xet-1.1.10.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/hf_xet already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/idna-3.10.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/idna already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging-25.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pyyaml-6.0.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/yaml already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/_yaml already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/safetensors already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/safetensors-0.6.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/cli already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/stringzilla-4.0.14.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/stringzilla.cpython-311-x86_64-linux-gnu.so already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tqdm-4.67.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tqdm already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/urllib3-2.5.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/urllib3 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/simsimd-6.5.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/simsimd.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/simsimd already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/simsimd.cpython-311-x86_64-linux-gnu.so already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: NVIDIA A10-24Q\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train rows: 232999 unique cats: 1010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_classes: 1010 min/max cat_id: 0 1009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split -> train: 230319 val: 2680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared splits and mappings.\n"
          ]
        }
      ]
    },
    {
      "id": "6a877d71-fb4e-4237-8c2a-f49122fa93a8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Dataset, transforms, model, and quick smoke-train to validate pipeline (torchvision backbone to avoid HF issues)\n",
        "import math, time, gc, os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as tvm\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class JsonImageDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, i):\n",
        "        row = self.df.iloc[i]\n",
        "        img_path = row['file_name']\n",
        "        with Image.open(img_path) as img:\n",
        "            img = img.convert('RGB')\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        label = int(row['label'])\n",
        "        return img, label\n",
        "\n",
        "# Torchvision ResNet50 (URL-hosted weights) to bypass HF hub\n",
        "weights = tvm.ResNet50_Weights.IMAGENET1K_V2\n",
        "print('Creating model: torchvision resnet50, weights IMAGENET1K_V2', flush=True)\n",
        "base_model = tvm.resnet50(weights=weights)\n",
        "in_feats = base_model.fc.in_features\n",
        "base_model.fc = nn.Linear(in_feats, num_classes)\n",
        "model = base_model.to(device).to(memory_format=torch.channels_last)\n",
        "\n",
        "# Transforms (use standard ImageNet mean/std to avoid weights.meta dependency)\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "img_size = 224\n",
        "train_tfms = T.Compose([\n",
        "    T.RandomResizedCrop(img_size, scale=(0.5, 1.0), interpolation=InterpolationMode.BICUBIC),\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.AutoAugment(policy=T.AutoAugmentPolicy.IMAGENET),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=mean, std=std),\n",
        "])\n",
        "val_tfms = T.Compose([\n",
        "    T.Resize(int(img_size/0.875), interpolation=InterpolationMode.BICUBIC),\n",
        "    T.CenterCrop(img_size),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=mean, std=std),\n",
        "])\n",
        "\n",
        "# Datasets and loaders (small subsets for smoke test)\n",
        "n_train_smoke = 2048\n",
        "n_val_smoke = 512\n",
        "train_ds_full = JsonImageDataset(df_train, transform=train_tfms)\n",
        "val_ds_full = JsonImageDataset(df_val, transform=val_tfms)\n",
        "train_ds = Subset(train_ds_full, np.arange(min(n_train_smoke, len(train_ds_full))))\n",
        "val_ds = Subset(val_ds_full, np.arange(min(n_val_smoke, len(val_ds_full))))\n",
        "\n",
        "bs = 96\n",
        "nw = min(8, os.cpu_count() or 4)\n",
        "train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=nw, pin_memory=True, persistent_workers=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=bs, shuffle=False, num_workers=nw, pin_memory=True, persistent_workers=True)\n",
        "\n",
        "# Optim, loss\n",
        "lr = 1e-3 * (bs / 256.0)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.05, betas=(0.9, 0.999))\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1).to(device)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    t0 = time.time()\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device, non_blocking=True, memory_format=torch.channels_last)\n",
        "            yb = yb.to(device, non_blocking=True)\n",
        "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "                logits = model(xb)\n",
        "            pred = logits.argmax(dim=1)\n",
        "            correct += (pred == yb).sum().item()\n",
        "            total += yb.numel()\n",
        "    acc = correct / max(1, total)\n",
        "    print(f'Eval: acc={acc:.4f}, n={total}, time={time.time()-t0:.1f}s', flush=True)\n",
        "    return acc\n",
        "\n",
        "# One-epoch smoke train\n",
        "epochs = 1\n",
        "print(f'Smoke train: epochs={epochs}, bs={bs}, train_n={len(train_ds)}, val_n={len(val_ds)}', flush=True)\n",
        "best_acc = 0.0\n",
        "t_start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    t0 = time.time()\n",
        "    running_loss = 0.0\n",
        "    seen = 0\n",
        "    for it, (xb, yb) in enumerate(train_loader):\n",
        "        xb = xb.to(device, non_blocking=True, memory_format=torch.channels_last)\n",
        "        yb = yb.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        running_loss += loss.item() * yb.size(0)\n",
        "        seen += yb.size(0)\n",
        "        if (it+1) % 20 == 0:\n",
        "            elapsed = time.time() - t0\n",
        "            print(f'ep {epoch} it {it+1}/{math.ceil(len(train_ds)/bs)} loss {running_loss/seen:.4f} elapsed {elapsed:.1f}s', flush=True)\n",
        "    train_loss = running_loss / max(1, seen)\n",
        "    print(f'Epoch {epoch}: train_loss={train_loss:.4f} epoch_time={time.time()-t0:.1f}s total_elapsed={time.time()-t_start:.1f}s', flush=True)\n",
        "    acc = evaluate(model, val_loader)\n",
        "    best_acc = max(best_acc, acc)\n",
        "\n",
        "print('Smoke training complete. Best val acc:', f'{best_acc:.4f}')\n",
        "torch.cuda.empty_cache(); gc.collect()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model: torchvision resnet50, weights IMAGENET1K_V2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Smoke train: epochs=1, bs=96, train_n=2048, val_n=512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_429/1375595590.py:74: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_429/1375595590.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0 it 20/22 loss 6.8466 elapsed 5.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: train_loss=6.8426 epoch_time=5.8s total_elapsed=5.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_429/1375595590.py:85: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval: acc=0.0098, n=512, time=1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Smoke training complete. Best val acc: 0.0098\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "37501"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "fc9d04fb-86eb-482c-b3b7-9656345a6656",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stage A Extended: ConvNeXt-Tiny @256px, Mixup=0.3, EMA=0.99985, cosine w/ lr floor; resume to 12 epochs; 2-crop TTA inference\n",
        "import math, time, gc, os, json, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as tvm\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from timm.utils import ModelEmaV2\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "assert torch.cuda.is_available(), 'CUDA required for timely training'\n",
        "\n",
        "class JsonImageDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, i):\n",
        "        row = self.df.iloc[i]\n",
        "        with Image.open(row['file_name']) as img:\n",
        "            img = img.convert('RGB')\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, int(row['label'])\n",
        "\n",
        "# Transforms\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "img_size = 256\n",
        "train_tfms = T.Compose([\n",
        "    T.RandomResizedCrop(img_size, scale=(0.5, 1.0), interpolation=InterpolationMode.BICUBIC),\n",
        "    T.RandomHorizontalFlip(0.5),\n",
        "    T.TrivialAugmentWide(interpolation=InterpolationMode.BICUBIC),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean, std),\n",
        "    T.RandomErasing(p=0.25, scale=(0.02, 0.12), ratio=(0.3, 3.3), value='random'),\n",
        "])\n",
        "val_tfms = T.Compose([\n",
        "    T.Resize(int(img_size/0.875), interpolation=InterpolationMode.BICUBIC),\n",
        "    T.CenterCrop(img_size),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "# Datasets & loaders\n",
        "train_ds = JsonImageDataset(df_train, transform=train_tfms)\n",
        "val_ds = JsonImageDataset(df_val, transform=val_tfms)\n",
        "bs = 128\n",
        "nw = min(8, os.cpu_count() or 4)\n",
        "train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=nw, pin_memory=True, persistent_workers=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=bs, shuffle=False, num_workers=nw, pin_memory=True, persistent_workers=True)\n",
        "print(f'train_n={len(train_ds)} val_n={len(val_ds)} bs={bs} workers={nw}', flush=True)\n",
        "\n",
        "# Model: ConvNeXt Tiny\n",
        "weights = tvm.ConvNeXt_Tiny_Weights.IMAGENET1K_V1\n",
        "model = tvm.convnext_tiny(weights=weights)\n",
        "if isinstance(model.classifier, nn.Sequential) and isinstance(model.classifier[-1], nn.Linear):\n",
        "    in_ch = model.classifier[-1].in_features\n",
        "    model.classifier[-1] = nn.Linear(in_ch, num_classes)\n",
        "else:\n",
        "    # Fallback in case of different head structure\n",
        "    for name, m in list(model.named_modules())[::-1]:\n",
        "        if isinstance(m, nn.Linear) and m.in_features > 0:\n",
        "            setattr(model, name.split('.')[-1], nn.Linear(m.in_features, num_classes))\n",
        "            break\n",
        "model = model.to(device).to(memory_format=torch.channels_last)\n",
        "\n",
        "# Optimizer, scheduler, loss, Mixup, EMA\n",
        "base_lr = 1e-3\n",
        "lr_peak = base_lr * (bs / 256.0)\n",
        "lr_min = 1e-6\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr_peak, weight_decay=0.05, betas=(0.9, 0.999))\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1).to(device)\n",
        "scaler = torch.amp.GradScaler('cuda', enabled=True)\n",
        "epochs = 12  # total target epochs\n",
        "warmup_epochs = 1\n",
        "steps_per_epoch = math.ceil(len(train_ds)/bs)\n",
        "total_steps = epochs * steps_per_epoch\n",
        "warmup_steps = warmup_epochs * steps_per_epoch\n",
        "\n",
        "def cosine_lr(step):\n",
        "    # returns multiplicative factor for lr between [lr_min/lr_peak, 1]\n",
        "    if step < warmup_steps:\n",
        "        return (step + 1) / max(1, warmup_steps)\n",
        "    prog = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
        "    cos_val = 0.5 * (1 + math.cos(math.pi * prog))\n",
        "    scaled = (lr_min / lr_peak) + (1 - (lr_min / lr_peak)) * cos_val\n",
        "    return scaled\n",
        "\n",
        "# Mixup utilities\n",
        "mixup_alpha = 0.3\n",
        "def mixup_batch(x, y, alpha=mixup_alpha):\n",
        "    if alpha <= 0:\n",
        "        return x, y, 1.0, None\n",
        "    lam = float(np.random.beta(alpha, alpha)) if alpha > 0 else 1.0\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size, device=x.device)\n",
        "    y_a, y_b = y, y[index]\n",
        "    x = x * lam + x[index] * (1.0 - lam)\n",
        "    return x, (y_a, y_b), lam, index\n",
        "\n",
        "# EMA\n",
        "ema_decay = 0.99985\n",
        "model_ema = ModelEmaV2(model, decay=ema_decay, device=device)\n",
        "\n",
        "# Optional resume from best EMA checkpoint if exists\n",
        "ckpt_path = Path('artifacts/ckpt_convnext_tiny_stageA_fast.pth')\n",
        "best_acc = 0.0\n",
        "if ckpt_path.exists():\n",
        "    try:\n",
        "        ckpt = torch.load(ckpt_path, map_location='cpu')\n",
        "        model.load_state_dict(ckpt['state_dict'], strict=False)\n",
        "        model_ema.module.load_state_dict(ckpt['state_dict'], strict=False)\n",
        "        best_acc = float(ckpt.get('val_acc', 0.0))\n",
        "        print(f'Resumed weights from {ckpt_path} (prev best_acc={best_acc:.4f})', flush=True)\n",
        "    except Exception as e:\n",
        "        print('Resume failed:', e, flush=True)\n",
        "\n",
        "def evaluate(model_eval, loader):\n",
        "    model_eval.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device, non_blocking=True, memory_format=torch.channels_last)\n",
        "            yb = yb.to(device, non_blocking=True)\n",
        "            with torch.amp.autocast('cuda', enabled=True):\n",
        "                logits = model_eval(xb)\n",
        "            pred = logits.argmax(1)\n",
        "            correct += (pred == yb).sum().item()\n",
        "            total += yb.numel()\n",
        "    return correct / max(1, total)\n",
        "\n",
        "# Train loop\n",
        "t0_all = time.time()\n",
        "step = 0\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    seen = 0\n",
        "    t0 = time.time()\n",
        "    for it, (xb, yb) in enumerate(train_loader):\n",
        "        lr_now = lr_peak * cosine_lr(step)\n",
        "        for pg in optimizer.param_groups:\n",
        "            pg['lr'] = lr_now\n",
        "        step += 1\n",
        "        xb = xb.to(device, non_blocking=True, memory_format=torch.channels_last)\n",
        "        yb = yb.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        # Mixup\n",
        "        xb_m, y_pair, lam, _ = mixup_batch(xb, yb, mixup_alpha)\n",
        "        with torch.amp.autocast('cuda', enabled=True):\n",
        "            logits = model(xb_m)\n",
        "            if isinstance(y_pair, tuple):\n",
        "                ya, yb2 = y_pair\n",
        "                # Use criterion for both terms (respects label smoothing) - expert fix\n",
        "                loss = lam * criterion(logits, ya) + (1.0 - lam) * criterion(logits, yb2)\n",
        "            else:\n",
        "                loss = criterion(logits, y_pair)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        # EMA update\n",
        "        model_ema.update(model)\n",
        "        running_loss += loss.item() * yb.size(0)\n",
        "        seen += yb.size(0)\n",
        "        if (it + 1) % 100 == 0:\n",
        "            print(f'ep {epoch} it {it+1}/{steps_per_epoch} loss {running_loss/max(1,seen):.4f} lr {lr_now:.6f} elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "    train_loss = running_loss / max(1, seen)\n",
        "    # Evaluate EMA model\n",
        "    val_acc = evaluate(model_ema.module, val_loader)\n",
        "    print(f'Epoch {epoch}: train_loss={train_loss:.4f} val_acc={val_acc:.4f} epoch_time={time.time()-t0:.1f}s total_elapsed={time.time()-t0_all:.1f}s', flush=True)\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save({'state_dict': model_ema.module.state_dict(), 'val_acc': best_acc}, ckpt_path)\n",
        "        print(f'Saved new best to {ckpt_path} (acc={best_acc:.4f})', flush=True)\n",
        "\n",
        "print(f'Training done. Best val_acc={best_acc:.4f}', flush=True)\n",
        "del train_loader; torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "# Inference on test set with 2-crop TTA (orig + hflip) using EMA weights\n",
        "with open('artifacts/idx_to_cat_id.json','r') as f:\n",
        "    idx_to_cat_id = {int(k): int(v) for k,v in json.load(f).items()}\n",
        "with open('test2019.json','r') as f:\n",
        "    test_js = json.load(f)\n",
        "test_images = test_js['images']\n",
        "test_df = pd.DataFrame(test_images)\n",
        "test_df['path'] = test_df['file_name']\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, df, transform):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.tf = transform\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, i):\n",
        "        row = self.df.iloc[i]\n",
        "        with Image.open(row['path']) as img:\n",
        "            img = img.convert('RGB')\n",
        "        return self.tf(img), int(row['id'])\n",
        "\n",
        "test_ds = TestDataset(test_df, val_tfms)\n",
        "test_loader = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=nw, pin_memory=True, persistent_workers=True)\n",
        "\n",
        "# Load best EMA ckpt for inference\n",
        "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
        "model = tvm.convnext_tiny(weights=None)\n",
        "if isinstance(model.classifier, nn.Sequential) and isinstance(model.classifier[-1], nn.Linear):\n",
        "    in_ch = model.classifier[-1].in_features\n",
        "    model.classifier[-1] = nn.Linear(in_ch, num_classes)\n",
        "else:\n",
        "    for name, m in list(model.named_modules())[::-1]:\n",
        "        if isinstance(m, nn.Linear) and m.in_features > 0:\n",
        "            setattr(model, name.split('.')[-1], nn.Linear(m.in_features, num_classes))\n",
        "            break\n",
        "model.load_state_dict(ckpt['state_dict'], strict=True)\n",
        "model = model.to(device).to(memory_format=torch.channels_last).eval()\n",
        "\n",
        "pred_rows = []\n",
        "t_inf0 = time.time()\n",
        "with torch.no_grad():\n",
        "    for it, (xb, ids) in enumerate(test_loader):\n",
        "        xb = xb.to(device, non_blocking=True, memory_format=torch.channels_last)\n",
        "        with torch.amp.autocast('cuda', enabled=True):\n",
        "            logits1 = model(xb)\n",
        "            logits2 = model(torch.flip(xb, dims=[3]))  # horizontal flip\n",
        "            logits = (logits1 + logits2) * 0.5\n",
        "        preds = logits.argmax(1).detach().cpu().numpy()\n",
        "        for img_id, p in zip(ids.tolist(), preds.tolist()):\n",
        "            pred_rows.append((img_id, idx_to_cat_id[p]))\n",
        "        if (it + 1) % 50 == 0:\n",
        "            print(f'infer it {it+1}/{math.ceil(len(test_ds)/256)} elapsed {time.time()-t_inf0:.1f}s', flush=True)\n",
        "\n",
        "sub = pd.DataFrame(pred_rows, columns=['image_id','category_id'])\n",
        "sub = sub.sort_values('image_id').reset_index(drop=True)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with', len(sub), 'rows', flush=True)\n",
        "\n",
        "torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "# Next: Stage B fine-tune @384px (4-5 epochs, lr~1e-4, mixup=0.1, RE p=0.1), 2-crop TTA."
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_n=230319 val_n=2680 bs=128 workers=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resumed weights from artifacts/ckpt_convnext_tiny_stageA_fast.pth (prev best_acc=0.6698)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_429/3138364984.py:118: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  ckpt = torch.load(ckpt_path, map_location='cpu')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0 it 100/1800 loss 3.3352 lr 0.000028 elapsed 32.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0 it 200/1800 loss 3.2771 lr 0.000056 elapsed 63.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0 it 300/1800 loss 3.2747 lr 0.000083 elapsed 94.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0 it 400/1800 loss 3.2535 lr 0.000111 elapsed 125.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0 it 500/1800 loss 3.2130 lr 0.000139 elapsed 157.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0 it 600/1800 loss 3.1894 lr 0.000167 elapsed 188.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0 it 700/1800 loss 3.1731 lr 0.000194 elapsed 220.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0 it 800/1800 loss 3.1653 lr 0.000222 elapsed 251.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0 it 900/1800 loss 3.1660 lr 0.000250 elapsed 283.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0 it 1000/1800 loss 3.1630 lr 0.000278 elapsed 315.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0 it 1100/1800 loss 3.1599 lr 0.000306 elapsed 347.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0 it 1200/1800 loss 3.1531 lr 0.000333 elapsed 378.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0 it 1300/1800 loss 3.1570 lr 0.000361 elapsed 410.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0 it 1400/1800 loss 3.1607 lr 0.000389 elapsed 442.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0 it 1500/1800 loss 3.1671 lr 0.000417 elapsed 474.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0 it 1600/1800 loss 3.1785 lr 0.000444 elapsed 506.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0 it 1700/1800 loss 3.1843 lr 0.000472 elapsed 538.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0 it 1800/1800 loss 3.1875 lr 0.000500 elapsed 570.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: train_loss=3.1875 val_acc=0.6951 epoch_time=574.6s total_elapsed=574.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved new best to artifacts/ckpt_convnext_tiny_stageA_fast.pth (acc=0.6951)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 1 it 100/1800 loss 3.3506 lr 0.000500 elapsed 33.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 1 it 200/1800 loss 3.3426 lr 0.000500 elapsed 64.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 1 it 300/1800 loss 3.2864 lr 0.000500 elapsed 96.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 1 it 400/1800 loss 3.3231 lr 0.000500 elapsed 128.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 1 it 500/1800 loss 3.3394 lr 0.000499 elapsed 160.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 1 it 600/1800 loss 3.3173 lr 0.000499 elapsed 192.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 1 it 700/1800 loss 3.3178 lr 0.000498 elapsed 224.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 1 it 800/1800 loss 3.3138 lr 0.000498 elapsed 256.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 1 it 900/1800 loss 3.3091 lr 0.000497 elapsed 288.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 1 it 1000/1800 loss 3.3034 lr 0.000497 elapsed 320.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 1 it 1100/1800 loss 3.3054 lr 0.000496 elapsed 352.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 1 it 1200/1800 loss 3.3107 lr 0.000495 elapsed 384.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 1 it 1300/1800 loss 3.2999 lr 0.000495 elapsed 416.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 1 it 1400/1800 loss 3.2982 lr 0.000494 elapsed 447.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 1 it 1500/1800 loss 3.2969 lr 0.000493 elapsed 479.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 1 it 1600/1800 loss 3.3050 lr 0.000492 elapsed 511.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 1 it 1700/1800 loss 3.3095 lr 0.000491 elapsed 543.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 1 it 1800/1800 loss 3.3066 lr 0.000490 elapsed 575.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train_loss=3.3066 val_acc=0.7209 epoch_time=579.5s total_elapsed=1154.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved new best to artifacts/ckpt_convnext_tiny_stageA_fast.pth (acc=0.7209)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 2 it 100/1800 loss 3.3660 lr 0.000489 elapsed 33.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 2 it 200/1800 loss 3.2242 lr 0.000488 elapsed 64.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 2 it 300/1800 loss 3.1940 lr 0.000486 elapsed 96.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 2 it 400/1800 loss 3.1639 lr 0.000485 elapsed 128.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 2 it 500/1800 loss 3.1422 lr 0.000484 elapsed 160.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 2 it 600/1800 loss 3.1159 lr 0.000482 elapsed 192.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 2 it 700/1800 loss 3.1122 lr 0.000481 elapsed 224.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 2 it 800/1800 loss 3.1030 lr 0.000479 elapsed 256.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 2 it 900/1800 loss 3.0938 lr 0.000477 elapsed 288.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 2 it 1000/1800 loss 3.1118 lr 0.000476 elapsed 320.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 2 it 1100/1800 loss 3.1057 lr 0.000474 elapsed 352.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 2 it 1200/1800 loss 3.1155 lr 0.000472 elapsed 384.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 2 it 1300/1800 loss 3.1171 lr 0.000470 elapsed 416.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 2 it 1400/1800 loss 3.1205 lr 0.000469 elapsed 448.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 2 it 1500/1800 loss 3.1170 lr 0.000467 elapsed 480.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 2 it 1600/1800 loss 3.1280 lr 0.000465 elapsed 512.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 2 it 1700/1800 loss 3.1296 lr 0.000463 elapsed 544.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 2 it 1800/1800 loss 3.1389 lr 0.000460 elapsed 576.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: train_loss=3.1389 val_acc=0.7410 epoch_time=579.9s total_elapsed=1734.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved new best to artifacts/ckpt_convnext_tiny_stageA_fast.pth (acc=0.7410)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 3 it 100/1800 loss 2.9167 lr 0.000458 elapsed 33.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 3 it 200/1800 loss 2.9591 lr 0.000456 elapsed 65.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 3 it 300/1800 loss 3.0031 lr 0.000454 elapsed 97.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 3 it 400/1800 loss 3.0138 lr 0.000451 elapsed 128.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 3 it 500/1800 loss 3.0508 lr 0.000449 elapsed 160.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 3 it 600/1800 loss 3.0396 lr 0.000447 elapsed 192.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 3 it 700/1800 loss 3.0393 lr 0.000444 elapsed 224.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 3 it 800/1800 loss 3.0389 lr 0.000442 elapsed 256.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 3 it 900/1800 loss 3.0500 lr 0.000439 elapsed 288.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 3 it 1000/1800 loss 3.0503 lr 0.000436 elapsed 319.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 3 it 1100/1800 loss 3.0478 lr 0.000434 elapsed 351.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 3 it 1200/1800 loss 3.0430 lr 0.000431 elapsed 383.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 3 it 1300/1800 loss 3.0437 lr 0.000428 elapsed 415.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 3 it 1400/1800 loss 3.0399 lr 0.000426 elapsed 447.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 3 it 1500/1800 loss 3.0427 lr 0.000423 elapsed 479.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 3 it 1600/1800 loss 3.0295 lr 0.000420 elapsed 511.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 3 it 1700/1800 loss 3.0295 lr 0.000417 elapsed 543.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 3 it 1800/1800 loss 3.0298 lr 0.000414 elapsed 575.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: train_loss=3.0298 val_acc=0.7556 epoch_time=578.8s total_elapsed=2313.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved new best to artifacts/ckpt_convnext_tiny_stageA_fast.pth (acc=0.7556)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 4 it 100/1800 loss 2.6601 lr 0.000411 elapsed 32.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 4 it 200/1800 loss 2.6969 lr 0.000408 elapsed 64.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 4 it 300/1800 loss 2.7577 lr 0.000405 elapsed 96.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 4 it 400/1800 loss 2.8440 lr 0.000402 elapsed 128.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 4 it 500/1800 loss 2.8778 lr 0.000398 elapsed 160.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 4 it 600/1800 loss 2.8876 lr 0.000395 elapsed 192.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 4 it 700/1800 loss 2.8967 lr 0.000392 elapsed 224.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 4 it 800/1800 loss 2.9092 lr 0.000389 elapsed 256.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 4 it 900/1800 loss 2.9226 lr 0.000385 elapsed 288.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 4 it 1000/1800 loss 2.9314 lr 0.000382 elapsed 319.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 4 it 1100/1800 loss 2.9441 lr 0.000379 elapsed 351.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 4 it 1200/1800 loss 2.9528 lr 0.000375 elapsed 383.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 4 it 1300/1800 loss 2.9581 lr 0.000372 elapsed 415.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 4 it 1400/1800 loss 2.9583 lr 0.000368 elapsed 447.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 4 it 1500/1800 loss 2.9617 lr 0.000365 elapsed 479.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 4 it 1600/1800 loss 2.9651 lr 0.000361 elapsed 511.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 4 it 1700/1800 loss 2.9611 lr 0.000358 elapsed 543.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 4 it 1800/1800 loss 2.9663 lr 0.000354 elapsed 575.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: train_loss=2.9663 val_acc=0.7638 epoch_time=578.9s total_elapsed=2893.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved new best to artifacts/ckpt_convnext_tiny_stageA_fast.pth (acc=0.7638)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 5 it 100/1800 loss 3.0492 lr 0.000351 elapsed 33.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 5 it 200/1800 loss 2.9001 lr 0.000347 elapsed 65.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 5 it 300/1800 loss 2.8430 lr 0.000343 elapsed 96.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 5 it 400/1800 loss 2.8270 lr 0.000340 elapsed 128.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 5 it 500/1800 loss 2.8428 lr 0.000336 elapsed 160.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 5 it 600/1800 loss 2.8206 lr 0.000332 elapsed 192.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 5 it 700/1800 loss 2.8142 lr 0.000328 elapsed 224.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 5 it 800/1800 loss 2.8123 lr 0.000325 elapsed 256.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 5 it 900/1800 loss 2.8005 lr 0.000321 elapsed 288.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 5 it 1000/1800 loss 2.8040 lr 0.000317 elapsed 320.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 5 it 1100/1800 loss 2.8136 lr 0.000313 elapsed 352.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 5 it 1200/1800 loss 2.8288 lr 0.000309 elapsed 384.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 5 it 1300/1800 loss 2.8341 lr 0.000306 elapsed 416.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 5 it 1400/1800 loss 2.8396 lr 0.000302 elapsed 448.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 5 it 1500/1800 loss 2.8250 lr 0.000298 elapsed 480.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 5 it 1600/1800 loss 2.8103 lr 0.000294 elapsed 512.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 5 it 1700/1800 loss 2.8045 lr 0.000290 elapsed 544.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 5 it 1800/1800 loss 2.7986 lr 0.000286 elapsed 576.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: train_loss=2.7986 val_acc=0.7806 epoch_time=580.1s total_elapsed=3473.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved new best to artifacts/ckpt_convnext_tiny_stageA_fast.pth (acc=0.7806)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 6 it 100/1800 loss 2.7335 lr 0.000282 elapsed 33.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 6 it 200/1800 loss 2.7744 lr 0.000278 elapsed 65.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 6 it 300/1800 loss 2.7503 lr 0.000274 elapsed 97.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 6 it 400/1800 loss 2.7660 lr 0.000270 elapsed 129.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 6 it 500/1800 loss 2.7489 lr 0.000266 elapsed 160.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 6 it 600/1800 loss 2.7559 lr 0.000262 elapsed 192.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 6 it 700/1800 loss 2.7313 lr 0.000258 elapsed 224.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 6 it 800/1800 loss 2.7326 lr 0.000254 elapsed 256.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 6 it 900/1800 loss 2.7386 lr 0.000251 elapsed 288.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 6 it 1000/1800 loss 2.7548 lr 0.000247 elapsed 320.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 6 it 1100/1800 loss 2.7482 lr 0.000243 elapsed 352.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 6 it 1200/1800 loss 2.7448 lr 0.000239 elapsed 383.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 6 it 1300/1800 loss 2.7432 lr 0.000235 elapsed 415.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 6 it 1400/1800 loss 2.7379 lr 0.000231 elapsed 447.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 6 it 1500/1800 loss 2.7340 lr 0.000227 elapsed 479.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 6 it 1600/1800 loss 2.7310 lr 0.000223 elapsed 511.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 6 it 1700/1800 loss 2.7260 lr 0.000219 elapsed 543.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 6 it 1800/1800 loss 2.7271 lr 0.000215 elapsed 575.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: train_loss=2.7271 val_acc=0.7918 epoch_time=579.1s total_elapsed=4052.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved new best to artifacts/ckpt_convnext_tiny_stageA_fast.pth (acc=0.7918)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 7 it 100/1800 loss 2.6061 lr 0.000211 elapsed 32.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 7 it 200/1800 loss 2.5467 lr 0.000207 elapsed 64.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 7 it 300/1800 loss 2.5729 lr 0.000203 elapsed 96.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 7 it 400/1800 loss 2.6166 lr 0.000199 elapsed 128.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 7 it 500/1800 loss 2.6388 lr 0.000196 elapsed 160.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 7 it 600/1800 loss 2.6561 lr 0.000192 elapsed 192.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 7 it 700/1800 loss 2.6325 lr 0.000188 elapsed 224.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 7 it 800/1800 loss 2.6117 lr 0.000184 elapsed 256.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 7 it 900/1800 loss 2.6109 lr 0.000180 elapsed 288.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 7 it 1000/1800 loss 2.6027 lr 0.000176 elapsed 320.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 7 it 1100/1800 loss 2.5971 lr 0.000173 elapsed 352.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 7 it 1200/1800 loss 2.5998 lr 0.000169 elapsed 384.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 7 it 1300/1800 loss 2.6062 lr 0.000165 elapsed 416.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 7 it 1400/1800 loss 2.6001 lr 0.000161 elapsed 448.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 7 it 1500/1800 loss 2.5942 lr 0.000158 elapsed 480.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 7 it 1600/1800 loss 2.5976 lr 0.000154 elapsed 512.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 7 it 1700/1800 loss 2.5849 lr 0.000151 elapsed 544.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 7 it 1800/1800 loss 2.5769 lr 0.000147 elapsed 575.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: train_loss=2.5769 val_acc=0.7996 epoch_time=579.6s total_elapsed=4632.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved new best to artifacts/ckpt_convnext_tiny_stageA_fast.pth (acc=0.7996)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 8 it 100/1800 loss 2.5521 lr 0.000143 elapsed 33.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 8 it 200/1800 loss 2.5223 lr 0.000140 elapsed 64.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 8 it 300/1800 loss 2.5041 lr 0.000136 elapsed 96.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 8 it 400/1800 loss 2.4817 lr 0.000133 elapsed 128.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 8 it 500/1800 loss 2.5068 lr 0.000129 elapsed 160.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 8 it 600/1800 loss 2.5249 lr 0.000126 elapsed 192.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 8 it 700/1800 loss 2.5159 lr 0.000122 elapsed 224.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 8 it 800/1800 loss 2.5075 lr 0.000119 elapsed 256.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 8 it 900/1800 loss 2.5078 lr 0.000116 elapsed 288.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 8 it 1000/1800 loss 2.4949 lr 0.000112 elapsed 320.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 8 it 1100/1800 loss 2.4914 lr 0.000109 elapsed 352.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 8 it 1200/1800 loss 2.4882 lr 0.000106 elapsed 384.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 8 it 1300/1800 loss 2.4769 lr 0.000103 elapsed 416.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 8 it 1400/1800 loss 2.4866 lr 0.000099 elapsed 448.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 8 it 1500/1800 loss 2.4776 lr 0.000096 elapsed 480.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 8 it 1600/1800 loss 2.4689 lr 0.000093 elapsed 512.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 8 it 1700/1800 loss 2.4603 lr 0.000090 elapsed 544.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 8 it 1800/1800 loss 2.4642 lr 0.000087 elapsed 576.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: train_loss=2.4642 val_acc=0.8063 epoch_time=580.0s total_elapsed=5213.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved new best to artifacts/ckpt_convnext_tiny_stageA_fast.pth (acc=0.8063)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 9 it 100/1800 loss 2.3584 lr 0.000084 elapsed 33.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 9 it 200/1800 loss 2.3389 lr 0.000081 elapsed 64.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 9 it 300/1800 loss 2.3504 lr 0.000078 elapsed 96.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 9 it 400/1800 loss 2.3961 lr 0.000076 elapsed 128.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 9 it 500/1800 loss 2.4076 lr 0.000073 elapsed 160.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 9 it 600/1800 loss 2.4155 lr 0.000070 elapsed 192.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 9 it 700/1800 loss 2.3963 lr 0.000067 elapsed 223.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 9 it 800/1800 loss 2.4063 lr 0.000065 elapsed 255.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 9 it 900/1800 loss 2.4020 lr 0.000062 elapsed 287.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 9 it 1000/1800 loss 2.3876 lr 0.000059 elapsed 319.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 9 it 1100/1800 loss 2.3769 lr 0.000057 elapsed 351.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 9 it 1200/1800 loss 2.3715 lr 0.000054 elapsed 383.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 9 it 1300/1800 loss 2.3720 lr 0.000052 elapsed 415.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 9 it 1400/1800 loss 2.3850 lr 0.000050 elapsed 447.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 9 it 1500/1800 loss 2.3836 lr 0.000047 elapsed 479.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 9 it 1600/1800 loss 2.3797 lr 0.000045 elapsed 510.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 9 it 1700/1800 loss 2.3749 lr 0.000043 elapsed 542.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 9 it 1800/1800 loss 2.3778 lr 0.000041 elapsed 574.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: train_loss=2.3778 val_acc=0.8160 epoch_time=578.4s total_elapsed=5791.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved new best to artifacts/ckpt_convnext_tiny_stageA_fast.pth (acc=0.8160)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 10 it 100/1800 loss 2.2043 lr 0.000039 elapsed 33.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 10 it 200/1800 loss 2.2677 lr 0.000036 elapsed 65.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 10 it 300/1800 loss 2.3434 lr 0.000034 elapsed 96.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 10 it 400/1800 loss 2.3537 lr 0.000032 elapsed 129.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 10 it 500/1800 loss 2.3733 lr 0.000031 elapsed 160.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 10 it 600/1800 loss 2.3407 lr 0.000029 elapsed 192.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 10 it 700/1800 loss 2.3667 lr 0.000027 elapsed 224.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 10 it 800/1800 loss 2.3697 lr 0.000025 elapsed 256.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 10 it 900/1800 loss 2.3585 lr 0.000024 elapsed 288.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 10 it 1000/1800 loss 2.3401 lr 0.000022 elapsed 320.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 10 it 1100/1800 loss 2.3431 lr 0.000020 elapsed 352.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 10 it 1200/1800 loss 2.3541 lr 0.000019 elapsed 384.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 10 it 1300/1800 loss 2.3520 lr 0.000017 elapsed 416.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 10 it 1400/1800 loss 2.3455 lr 0.000016 elapsed 448.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 10 it 1500/1800 loss 2.3440 lr 0.000015 elapsed 480.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 10 it 1600/1800 loss 2.3452 lr 0.000013 elapsed 512.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 10 it 1700/1800 loss 2.3422 lr 0.000012 elapsed 544.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 10 it 1800/1800 loss 2.3439 lr 0.000011 elapsed 576.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: train_loss=2.3439 val_acc=0.8235 epoch_time=579.8s total_elapsed=6371.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved new best to artifacts/ckpt_convnext_tiny_stageA_fast.pth (acc=0.8235)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 11 it 100/1800 loss 2.1789 lr 0.000010 elapsed 33.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 11 it 200/1800 loss 2.2233 lr 0.000009 elapsed 64.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 11 it 300/1800 loss 2.2568 lr 0.000008 elapsed 96.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 11 it 400/1800 loss 2.2592 lr 0.000007 elapsed 128.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 11 it 500/1800 loss 2.2631 lr 0.000006 elapsed 160.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 11 it 600/1800 loss 2.2876 lr 0.000006 elapsed 192.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 11 it 700/1800 loss 2.2673 lr 0.000005 elapsed 224.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 11 it 800/1800 loss 2.2655 lr 0.000004 elapsed 256.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 11 it 900/1800 loss 2.2785 lr 0.000004 elapsed 288.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 11 it 1000/1800 loss 2.2792 lr 0.000003 elapsed 320.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 11 it 1100/1800 loss 2.2783 lr 0.000003 elapsed 352.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 11 it 1200/1800 loss 2.2959 lr 0.000002 elapsed 384.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 11 it 1300/1800 loss 2.3008 lr 0.000002 elapsed 416.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 11 it 1400/1800 loss 2.2989 lr 0.000002 elapsed 448.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 11 it 1500/1800 loss 2.2892 lr 0.000001 elapsed 480.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 11 it 1600/1800 loss 2.2913 lr 0.000001 elapsed 512.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 11 it 1700/1800 loss 2.2885 lr 0.000001 elapsed 544.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 11 it 1800/1800 loss 2.2858 lr 0.000001 elapsed 576.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: train_loss=2.2858 val_acc=0.8272 epoch_time=579.9s total_elapsed=6952.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved new best to artifacts/ckpt_convnext_tiny_stageA_fast.pth (acc=0.8272)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training done. Best val_acc=0.8272\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_429/3138364984.py:213: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  ckpt = torch.load(ckpt_path, map_location='cpu')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "infer it 50/126 elapsed 23.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "infer it 100/126 elapsed 43.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv with 32214 rows\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "52"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "25c73bc0-8371-46a2-b093-978affcf86ac",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Validate and fix submission.csv header/types for Kaggle format\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "sub_fp = Path('submission.csv')\n",
        "assert sub_fp.exists(), 'submission.csv not found'\n",
        "sub = pd.read_csv(sub_fp)\n",
        "print('Before:', sub.shape, sub.columns.tolist(), sub.dtypes.to_dict())\n",
        "\n",
        "# Kaggle iNat2019 expects columns: id, category_id\n",
        "if 'image_id' in sub.columns and 'id' not in sub.columns:\n",
        "    sub = sub.rename(columns={'image_id': 'id'})\n",
        "\n",
        "# Ensure correct columns and dtypes\n",
        "assert set(sub.columns) == {'id','category_id'}, f'Unexpected columns: {sub.columns.tolist()}'\n",
        "sub['id'] = sub['id'].astype(np.int64)\n",
        "sub['category_id'] = sub['category_id'].astype(np.int64)\n",
        "sub = sub.sort_values('id').reset_index(drop=True)\n",
        "\n",
        "sub.to_csv(sub_fp, index=False)\n",
        "print('After:', sub.shape, sub.columns.tolist(), sub.dtypes.to_dict())\n",
        "print(sub.head().to_string(index=False))\n",
        "print(sub.tail().to_string(index=False))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: (32214, 2) ['image_id', 'category_id'] {'image_id': dtype('int64'), 'category_id': dtype('int64')}\nAfter: (32214, 2) ['id', 'category_id'] {'id': dtype('int64'), 'category_id': dtype('int64')}\n id  category_id\n 20          167\n 86          159\n101          194\n108          471\n112          410\n    id  category_id\n265186          353\n265188          902\n265193          350\n265194          712\n265197          328\n"
          ]
        }
      ]
    },
    {
      "id": "a5a30795-6b8c-4fe5-838d-a9b4d08c2923",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Force submission header to ['image_id','category_id'] (some competitions expect this exact header)\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "sub_fp = Path('submission.csv')\n",
        "assert sub_fp.exists(), 'submission.csv not found'\n",
        "sub = pd.read_csv(sub_fp)\n",
        "print('Current columns:', sub.columns.tolist(), 'shape:', sub.shape, flush=True)\n",
        "if 'id' in sub.columns and 'image_id' not in sub.columns:\n",
        "    sub = sub.rename(columns={'id': 'image_id'})\n",
        "assert set(sub.columns) == {'image_id','category_id'}, f'Unexpected columns: {sub.columns.tolist()}'\n",
        "sub = sub[['image_id','category_id']].copy()\n",
        "sub['image_id'] = sub['image_id'].astype('int64')\n",
        "sub['category_id'] = sub['category_id'].astype('int64')\n",
        "sub = sub.sort_values('image_id').reset_index(drop=True)\n",
        "sub.to_csv(sub_fp, index=False)\n",
        "print('Fixed columns:', sub.columns.tolist(), 'shape:', sub.shape, flush=True)\n",
        "print(sub.head().to_string(index=False))\n",
        "print(sub.tail().to_string(index=False))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current columns: ['id', 'category_id'] shape: (32214, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed columns: ['image_id', 'category_id'] shape: (32214, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " image_id  category_id\n       20          167\n       86          159\n      101          194\n      108          471\n      112          410\n image_id  category_id\n   265186          353\n   265188          902\n   265193          350\n   265194          712\n   265197          328\n"
          ]
        }
      ]
    },
    {
      "id": "faf50015-7d55-49af-afed-5101b963e6d8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Robust submission validator: ensure rows/ids match test2019.json and header ['id','category_id']\n",
        "import json, pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "sub_fp = Path('submission.csv')\n",
        "assert sub_fp.exists(), 'submission.csv not found'\n",
        "sub = pd.read_csv(sub_fp)\n",
        "if 'image_id' in sub.columns and 'id' not in sub.columns:\n",
        "    sub = sub.rename(columns={'image_id':'id'})\n",
        "assert 'id' in sub.columns and 'category_id' in sub.columns, f'Unexpected cols: {sub.columns.tolist()}'\n",
        "\n",
        "# Load test ids from JSON\n",
        "with open('test2019.json','r') as f:\n",
        "    test_js = json.load(f)\n",
        "test_ids = pd.DataFrame({'id': [int(im['id']) for im in test_js['images']]})\n",
        "test_ids = test_ids.sort_values('id').reset_index(drop=True)\n",
        "\n",
        "# Check id coverage\n",
        "sub_ids = set(sub['id'].astype(int).tolist())\n",
        "test_id_set = set(test_ids['id'].tolist())\n",
        "missing = test_id_set - sub_ids\n",
        "extra = sub_ids - test_id_set\n",
        "print(f'missing_in_sub={len(missing)} extra_in_sub={len(extra)} sub_n={len(sub)} test_n={len(test_ids)}', flush=True)\n",
        "\n",
        "# Restrict to test ids, drop extras, align order exactly to test ids\n",
        "sub = sub[['id','category_id']].copy()\n",
        "sub['id'] = sub['id'].astype(np.int64)\n",
        "sub['category_id'] = sub['category_id'].astype(np.int64)\n",
        "sub = test_ids.merge(sub, on='id', how='left')\n",
        "assert len(sub) == len(test_ids), 'Row count mismatch after align'\n",
        "assert sub['category_id'].notna().all(), 'Found NaNs in category_id after align; predictions missing for some ids'\n",
        "\n",
        "sub = sub[['id','category_id']].astype({'id':'int64','category_id':'int64'})\n",
        "sub.to_csv(sub_fp, index=False)\n",
        "print('Final submission:', sub.shape, sub.columns.tolist(), sub.dtypes.to_dict(), flush=True)\n",
        "print(sub.head().to_string(index=False))\n",
        "print(sub.tail().to_string(index=False))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "missing_in_sub=0 extra_in_sub=0 sub_n=32214 test_n=32214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final submission: (32214, 2) ['id', 'category_id'] {'id': dtype('int64'), 'category_id': dtype('int64')}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " id  category_id\n 20          167\n 86          159\n101          194\n108          471\n112          410\n    id  category_id\n265186          353\n265188          902\n265193          350\n265194          712\n265197          328\n"
          ]
        }
      ]
    },
    {
      "id": "ba552f61-2d24-426d-8060-613542fd3450",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Convert submission.csv header to ['id','predicted'] (MLE-Benchmark/Kaggle variant expectation)\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "sub_fp = Path('submission.csv')\n",
        "assert sub_fp.exists(), 'submission.csv not found'\n",
        "sub = pd.read_csv(sub_fp)\n",
        "cols = sub.columns.tolist()\n",
        "if 'image_id' in cols and 'id' not in cols:\n",
        "    sub = sub.rename(columns={'image_id': 'id'})\n",
        "if 'category_id' in sub.columns and 'predicted' not in sub.columns:\n",
        "    sub = sub.rename(columns={'category_id': 'predicted'})\n",
        "assert set(sub.columns) == {'id','predicted'}, f'Unexpected columns: {sub.columns.tolist()}'\n",
        "sub['id'] = sub['id'].astype('int64')\n",
        "sub['predicted'] = sub['predicted'].astype('int64')\n",
        "sub = sub.sort_values('id').reset_index(drop=True)\n",
        "sub.to_csv(sub_fp, index=False)\n",
        "print('submission.csv fixed to columns:', sub.columns.tolist(), 'shape:', sub.shape, flush=True)\n",
        "print(sub.head().to_string(index=False))\n",
        "print(sub.tail().to_string(index=False))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv fixed to columns: ['id', 'predicted'] shape: (32214, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " id  predicted\n 20        167\n 86        159\n101        194\n108        471\n112        410\n    id  predicted\n265186        353\n265188        902\n265193        350\n265194        712\n265197        328\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}