{
  "cells": [
    {
      "id": "892ed6d3-573b-455c-8409-64f26c04da1e",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stage-2 Recognizer: GT-crop classifier (smoke test)\n",
        "\n",
        "Plan:\n",
        "- Parse train.csv (unicode x y w h) and folds_group.csv; use fold 0 for val.\n",
        "- Build GT crop dataset with 15% padding, resized to 192x192, square padding.\n",
        "- Label encode unicode tokens; balanced sampling.\n",
        "- Model: torchvision resnet50 (ImageNet weights), replace fc to num_classes.\n",
        "- Train 1 epoch smoke (cap steps) to validate pipeline; save checkpoint and class map.\n",
        "- Next: extend epochs, add aug, and later run on detector crops for full E2E."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "7077861b-1868-491c-b44f-e3bdf41e90ed",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# GT crops dataset and ResNet50 classifier (fold 0; with unicode mapping + full training)\n",
        "import os, math, time, json, random, re\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageOps\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torch.amp import autocast, GradScaler\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)\n",
        "\n",
        "CWD = Path('.')\n",
        "train_csv = CWD / 'train.csv'\n",
        "folds_csv = CWD / 'folds_group.csv'\n",
        "trans_csv = CWD / 'unicode_translation.csv'\n",
        "train_dir = CWD / 'train_images'\n",
        "assert train_csv.exists() and folds_csv.exists() and train_dir.exists()\n",
        "df_train = pd.read_csv(train_csv)\n",
        "df_folds = pd.read_csv(folds_csv)\n",
        "df_trans = pd.read_csv(trans_csv) if trans_csv.exists() else pd.DataFrame()\n",
        "\n",
        "def parse_labels_full(labels: str):\n",
        "    if not isinstance(labels, str) or labels.strip() == '':\n",
        "        return []\n",
        "    toks = labels.strip().split()\n",
        "    out = []\n",
        "    if len(toks) % 5 != 0:\n",
        "        return out\n",
        "    for i in range(0, len(toks), 5):\n",
        "        u, x, y, w, h = toks[i:i+5]\n",
        "        try:\n",
        "            out.append((u, int(x), int(y), int(w), int(h)))\n",
        "        except:\n",
        "            pass\n",
        "    return out\n",
        "\n",
        "# Build per-image annotations\n",
        "rows = []\n",
        "for r in df_train.itertuples(index=False):\n",
        "    image_id = getattr(r, 'image_id') if hasattr(r, 'image_id') else r[0]\n",
        "    labels = getattr(r, 'labels') if hasattr(r, 'labels') else r[1]\n",
        "    for (u,x,y,w,h) in parse_labels_full(labels):\n",
        "        rows.append((image_id,u,x,y,w,h))\n",
        "df_anns = pd.DataFrame(rows, columns=['image_id','unicode','x','y','w','h'])\n",
        "print('Annotations (raw):', df_anns.shape)\n",
        "\n",
        "# Build unicode mapping to canonical tokens if available\n",
        "def build_unicode_map(df_trans: pd.DataFrame):\n",
        "    if df_trans is None or df_trans.empty:\n",
        "        return {}\n",
        "    # Identify columns with many U+ tokens\n",
        "    cand_cols = []\n",
        "    for c in df_trans.columns:\n",
        "        try:\n",
        "            s = df_trans[c].astype(str)\n",
        "        except Exception:\n",
        "            continue\n",
        "        m = s.str.match(r'^U\\+[0-9A-Fa-f]+$').fillna(False).mean()\n",
        "        if m > 0.2:\n",
        "            cand_cols.append(c)\n",
        "    if not cand_cols:\n",
        "        return {}\n",
        "    def canon_score(c):\n",
        "        name = c.lower()\n",
        "        score = 0\n",
        "        if any(k in name for k in ['canon', 'target', 'to', 'new']):\n",
        "            score += 2\n",
        "        return score\n",
        "    cand_cols_sorted = sorted(cand_cols, key=lambda c: (-canon_score(c), df_trans[c].nunique()))\n",
        "    canon_col = cand_cols_sorted[0]\n",
        "    mapping = {}\n",
        "    for c in cand_cols:\n",
        "        if c == canon_col:\n",
        "            continue\n",
        "        for a, b in zip(df_trans[c].astype(str), df_trans[canon_col].astype(str)):\n",
        "            if re.match(r'^U\\+[0-9A-Fa-f]+$', str(a)) and re.match(r'^U\\+[0-9A-Fa-f]+$', str(b)) and a != b:\n",
        "                mapping[a] = b\n",
        "    print('Unicode mapping built:', len(mapping), 'mappings; canonical col =', canon_col)\n",
        "    return mapping\n",
        "\n",
        "u_map = build_unicode_map(df_trans)\n",
        "if u_map:\n",
        "    df_anns['unicode'] = df_anns['unicode'].map(lambda x: u_map.get(x, x))\n",
        "\n",
        "print('Annotations (mapped) head:', df_anns.head(2).to_dict('records'))\n",
        "\n",
        "# Label encode unicode tokens\n",
        "unicodes = sorted(df_anns['unicode'].unique().tolist())\n",
        "class_to_idx = {u:i for i,u in enumerate(unicodes)}\n",
        "num_classes = len(class_to_idx)\n",
        "print('Num classes (mapped):', num_classes)\n",
        "Path('recognizer_classes.json').write_text(json.dumps(class_to_idx))\n",
        "\n",
        "def clamp(val, lo, hi):\n",
        "    return max(lo, min(hi, val))\n",
        "\n",
        "IMG_SIZE = 192\n",
        "PAD_RATIO = 0.25  # increased for robustness to detector noise\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "train_tf = T.Compose([\n",
        "    T.RandomApply([T.ColorJitter(0.1,0.1,0.1,0.0)], p=0.5),\n",
        "    T.RandomAffine(degrees=5, translate=(0.05,0.05), scale=(0.9,1.1), shear=5),\n",
        "    T.RandomApply([T.GaussianBlur(kernel_size=3, sigma=(0.1,0.5))], p=0.3),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean, std),\n",
        "])\n",
        "val_tf = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "def load_and_crop(image_id, x, y, w, h):\n",
        "    p = train_dir / f'{image_id}.jpg'\n",
        "    if not p.exists():\n",
        "        alt = train_dir / f'{image_id}.png'\n",
        "        if alt.exists():\n",
        "            p = alt\n",
        "    img = Image.open(p).convert('RGB')\n",
        "    W, H = img.size\n",
        "    dx = int(round(w * PAD_RATIO)); dy = int(round(h * PAD_RATIO))\n",
        "    x1 = clamp(x - dx, 0, W-1); y1 = clamp(y - dy, 0, H-1)\n",
        "    x2 = clamp(x + w + dx, 1, W); y2 = clamp(y + h + dy, 1, H)\n",
        "    crop = img.crop((x1, y1, x2, y2))\n",
        "    cw, ch = crop.size\n",
        "    if cw != ch:\n",
        "        m = max(cw, ch)\n",
        "        pad_w = m - cw; pad_h = m - ch\n",
        "        crop = ImageOps.expand(crop, border=(0,0,pad_w,pad_h), fill=0) if cw < m or ch < m else crop\n",
        "    crop = crop.resize((IMG_SIZE, IMG_SIZE), Image.BILINEAR)\n",
        "    return crop\n",
        "\n",
        "class GTCropDataset(Dataset):\n",
        "    def __init__(self, df_anns: pd.DataFrame, df_folds: pd.DataFrame, fold: int, split: str):\n",
        "        fold_map = dict(df_folds.values)\n",
        "        keep = df_anns['image_id'].map(fold_map.get)\n",
        "        if split == 'train':\n",
        "            mask = keep != fold\n",
        "        else:\n",
        "            mask = keep == fold\n",
        "        self.df = df_anns.loc[mask].reset_index(drop=True)\n",
        "        self.split = split\n",
        "        print(split, 'samples:', len(self.df))\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.df.iloc[idx]\n",
        "        img = load_and_crop(r.image_id, r.x, r.y, r.w, r.h)\n",
        "        x = train_tf(img) if self.split == 'train' else val_tf(img)\n",
        "        y = class_to_idx[r.unicode]\n",
        "        return x, y\n",
        "\n",
        "fold = 0\n",
        "train_ds = GTCropDataset(df_anns, df_folds, fold, 'train')\n",
        "val_ds = GTCropDataset(df_anns, df_folds, fold, 'val')\n",
        "\n",
        "# Balanced sampler by inverse frequency\n",
        "cls_counts = train_ds.df['unicode'].map(train_ds.df['unicode'].value_counts())\n",
        "weights = 1.0 / cls_counts.values.astype(np.float64)\n",
        "sampler = WeightedRandomSampler(weights=torch.as_tensor(weights, dtype=torch.double), num_samples=min(len(train_ds), 100000), replacement=True)\n",
        "\n",
        "batch_size = 128\n",
        "num_workers = min(8, os.cpu_count() or 2)\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, num_workers=num_workers, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "# Model\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "weights = ResNet50_Weights.IMAGENET1K_V2\n",
        "model = resnet50(weights=weights)\n",
        "in_features = model.fc.in_features\n",
        "model.fc = nn.Linear(in_features, num_classes)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "# Param groups: lower LR for backbone, higher for head\n",
        "backbone_params = [p for n,p in model.named_parameters() if not n.startswith('fc.')]\n",
        "head_params = list(model.fc.parameters())\n",
        "optimizer = optim.AdamW([\n",
        "    {'params': backbone_params, 'lr': 1e-4},\n",
        "    {'params': head_params, 'lr': 1e-3},\n",
        "], weight_decay=1e-4)\n",
        "scaler = GradScaler('cuda') if device.type=='cuda' else None\n",
        "\n",
        "# Cosine schedule with warmup (step-wise)\n",
        "epochs = 12\n",
        "steps_per_epoch = math.ceil((sampler.num_samples if hasattr(sampler, 'num_samples') else len(train_ds)) / batch_size)\n",
        "total_steps = epochs * steps_per_epoch\n",
        "warmup_steps = 500\n",
        "base_lrs = [g['lr'] for g in optimizer.param_groups]\n",
        "\n",
        "def set_lrs(scale):\n",
        "    for g, base_lr in zip(optimizer.param_groups, base_lrs):\n",
        "        g['lr'] = base_lr * scale\n",
        "\n",
        "# EMA\n",
        "use_ema = True\n",
        "ema_decay = 0.999\n",
        "ema_state = None\n",
        "def init_ema():\n",
        "    # clone current model state\n",
        "    return {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
        "def update_ema():\n",
        "    if not use_ema:\n",
        "        return\n",
        "    with torch.no_grad():\n",
        "        msd = model.state_dict()\n",
        "        for k, v in msd.items():\n",
        "            if k not in ema_state:\n",
        "                ema_state[k] = v.detach().clone()\n",
        "                continue\n",
        "            if torch.is_floating_point(v):\n",
        "                # ema = decay*ema + (1-decay)*v\n",
        "                ema_state[k].copy_(ema_state[k] * ema_decay + v.detach() * (1.0 - ema_decay))\n",
        "            else:\n",
        "                # For non-float buffers (e.g., num_batches_tracked), copy directly\n",
        "                ema_state[k] = v.detach().clone()\n",
        "\n",
        "def eval_with_state(state_dict):\n",
        "    backup = {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    acc = evaluate(model, val_loader)\n",
        "    model.load_state_dict(backup, strict=False)\n",
        "    return acc\n",
        "\n",
        "def train_one_epoch(model, loader, epoch, global_step0):\n",
        "    model.train()\n",
        "    t0 = time.time(); last = t0; n=0; loss_sum=0.0; gstep = global_step0\n",
        "    for i,(x,y) in enumerate(loader):\n",
        "        # LR schedule\n",
        "        if gstep < warmup_steps:\n",
        "            set_lrs((gstep+1)/max(1,warmup_steps))\n",
        "        else:\n",
        "            # cosine from 1.0 -> 0.0 over total_steps-warmup\n",
        "            t = (gstep - warmup_steps) / max(1, total_steps - warmup_steps)\n",
        "            cos_scale = 0.5 * (1 + math.cos(math.pi * t))\n",
        "            set_lrs(cos_scale)\n",
        "\n",
        "        x = x.to(device, non_blocking=True); y = y.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        if scaler is not None:\n",
        "            with autocast('cuda'):\n",
        "                logits = model(x)\n",
        "                loss = criterion(logits, y)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward(); optimizer.step()\n",
        "        # EMA update\n",
        "        if use_ema:\n",
        "            update_ema()\n",
        "\n",
        "        n += 1; loss_sum += loss.item(); gstep += 1\n",
        "        if time.time()-last > 5:\n",
        "            mem = torch.cuda.memory_allocated()/1024**3 if device.type=='cuda' else 0.0\n",
        "            print(f'Epoch {epoch} iter {i} avg_loss {loss_sum/n:.4f} step {gstep}/{total_steps} mem {mem:.2f}GB elapsed {time.time()-t0:.1f}s', flush=True); last=time.time()\n",
        "    print(f'Epoch {epoch} done: steps {n}, avg {loss_sum/max(1,n):.4f}, time {time.time()-t0:.1f}s')\n",
        "    return gstep\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct=0; total=0\n",
        "    with torch.no_grad():\n",
        "        for x,y in loader:\n",
        "            x = x.to(device); y = y.to(device)\n",
        "            logits = model(x)\n",
        "            pred = logits.argmax(1)\n",
        "            correct += (pred==y).sum().item()\n",
        "            total += y.numel()\n",
        "    acc = correct/max(1,total)\n",
        "    print('Val acc:', f'{acc:.4f}', 'total:', total)\n",
        "    return acc\n",
        "\n",
        "best_acc = 0.0\n",
        "best_ckpt = 'recognizer_resnet50_fold0_best.pth'\n",
        "ema_state = init_ema() if use_ema else None\n",
        "global_step = 0\n",
        "for ep in range(1, epochs+1):\n",
        "    global_step = train_one_epoch(model, train_loader, ep, global_step)\n",
        "    # Evaluate with EMA weights if enabled\n",
        "    if use_ema:\n",
        "        acc = eval_with_state(ema_state)\n",
        "    else:\n",
        "        acc = evaluate(model, val_loader)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        to_save = ema_state if use_ema else model.state_dict()\n",
        "        torch.save({'model': to_save, 'num_classes': num_classes, 'classes': class_to_idx, 'ema': use_ema}, best_ckpt)\n",
        "        print('Saved best recognizer checkpoint (acc=', f'{acc:.4f}', ')')\n",
        "\n",
        "# Always save last checkpoint and mapping (EMA if used)\n",
        "last_ckpt = 'recognizer_resnet50_fold0_last.pth'\n",
        "to_save_last = ema_state if use_ema else model.state_dict()\n",
        "torch.save({'model': to_save_last, 'num_classes': num_classes, 'classes': class_to_idx, 'ema': use_ema}, last_ckpt)\n",
        "Path('recognizer_classes.json').write_text(json.dumps(class_to_idx))\n",
        "print('Saved last recognizer checkpoint and classes mapping')\n",
        "print('Best acc:', f'{best_acc:.4f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Annotations (raw): (613505, 6)\nUnicode mapping built: 0 mappings; canonical col = Unicode\nAnnotations (mapped) head: [{'image_id': '200004148_00015_1', 'unicode': 'U+306F', 'x': 1187, 'y': 361, 'w': 47, 'h': 27}, {'image_id': '200004148_00015_1', 'unicode': 'U+306F', 'x': 1487, 'y': 2581, 'w': 48, 'h': 28}]\nNum classes (mapped): 4113\ntrain samples: 497857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val samples: 115648\n"
          ]
        }
      ]
    },
    {
      "id": "0f01c153-e9a7-45a7-9d9c-7271d4c09452",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save recognizer checkpoint from in-memory model after interrupted training\n",
        "import torch, json\n",
        "from pathlib import Path\n",
        "\n",
        "assert 'model' in globals(), 'Recognizer model not found in memory. Re-run training cell 1.'\n",
        "assert 'class_to_idx' in globals() and 'num_classes' in globals(), 'Class mapping not found. Re-run training cell 1.'\n",
        "\n",
        "ckpt_path = Path('recognizer_resnet50_fold0_ep1.pth')\n",
        "torch.save({'model': model.state_dict(), 'num_classes': num_classes, 'classes': class_to_idx}, ckpt_path)\n",
        "Path('recognizer_classes.json').write_text(json.dumps(class_to_idx))\n",
        "print('Saved recognizer checkpoint to', ckpt_path.resolve())\n",
        "print('Saved classes to recognizer_classes.json with', len(class_to_idx), 'classes')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved recognizer checkpoint to /var/lib/simon/agent_run_states/kuzushiji-recognition-20250929-180012/recognizer_resnet50_fold0_ep1.pth\nSaved classes to recognizer_classes.json with 4113 classes\n"
          ]
        }
      ]
    },
    {
      "id": "113cfa04-9700-4c38-8fb6-4b8909365620",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inference: classify detector crops on test and assemble submission.csv\n",
        "import json\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "CWD = Path('.')\n",
        "test_dir = CWD / 'test_images'\n",
        "det_test_path = CWD / 'det_test_preds.parquet'\n",
        "ckpt_path = CWD / 'recognizer_resnet50_fold0_ep1.pth'\n",
        "classes_path = CWD / 'recognizer_classes.json'\n",
        "assert det_test_path.exists(), 'det_test_preds.parquet missing'\n",
        "assert ckpt_path.exists() and classes_path.exists(), 'recognizer checkpoint or classes mapping missing'\n",
        "\n",
        "# Load classes\n",
        "class_to_idx = json.loads(classes_path.read_text())\n",
        "idx_to_class = {int(v): k for k, v in class_to_idx.items()}\n",
        "num_classes = len(idx_to_class)\n",
        "\n",
        "# Build model\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "weights = None  # will load our trained head\n",
        "rec_model = resnet50(weights=weights)\n",
        "in_features = rec_model.fc.in_features\n",
        "rec_model.fc = nn.Linear(in_features, num_classes)\n",
        "state = torch.load(ckpt_path, map_location='cpu')\n",
        "rec_model.load_state_dict(state['model'], strict=False)\n",
        "rec_model.to(device)\n",
        "rec_model.eval()\n",
        "\n",
        "val_tf = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
        "\n",
        "def crop_from_box(image_id: str, x: float, y: float, w: float, h: float):\n",
        "    p = test_dir / f'{image_id}.jpg'\n",
        "    if not p.exists():\n",
        "        alt = test_dir / f'{image_id}.png'\n",
        "        if alt.exists():\n",
        "            p = alt\n",
        "    img = Image.open(p).convert('RGB')\n",
        "    W, H = img.size\n",
        "    dx = int(round(w * PAD_RATIO)); dy = int(round(h * PAD_RATIO))\n",
        "    x1 = clamp(int(x) - dx, 0, W-1); y1 = clamp(int(y) - dy, 0, H-1)\n",
        "    x2 = clamp(int(x + w) + dx, 1, W); y2 = clamp(int(y + h) + dy, 1, H)\n",
        "    crop = img.crop((x1, y1, x2, y2))\n",
        "    cw, ch = crop.size\n",
        "    if cw != ch:\n",
        "        m = max(cw, ch)\n",
        "        pad_w = m - cw; pad_h = m - ch\n",
        "        crop = ImageOps.expand(crop, border=(0,0,pad_w,pad_h), fill=0) if cw < m or ch < m else crop\n",
        "    crop = crop.resize((IMG_SIZE, IMG_SIZE), Image.BILINEAR)\n",
        "    return crop\n",
        "\n",
        "# Load detector predictions\n",
        "det_df = pd.read_parquet(det_test_path)\n",
        "print('Detector test preds:', det_df.shape)\n",
        "\n",
        "# Group by image and run batched classification per image for speed\n",
        "rows_out = []\n",
        "df_sample = pd.read_csv('sample_submission.csv')\n",
        "grp = det_df.groupby('image_id')\n",
        "t0 = time.time()\n",
        "processed = 0\n",
        "for image_id in df_sample['image_id']:\n",
        "    if image_id in grp.groups:\n",
        "        g = grp.get_group(image_id)\n",
        "        # build batch crops\n",
        "        crops = [crop_from_box(image_id, x, y, w, h) for x, y, w, h in zip(g['x'].values, g['y'].values, g['w'].values, g['h'].values)]\n",
        "        if len(crops) == 0:\n",
        "            rows_out.append('')\n",
        "            continue\n",
        "        xs = torch.stack([val_tf(c) for c in crops], dim=0).to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = rec_model(xs)\n",
        "            pred_idx = logits.argmax(1).detach().cpu().numpy().tolist()\n",
        "        pred_unicodes = [idx_to_class.get(int(i), 'U+003F') for i in pred_idx]\n",
        "        # centers for submission\n",
        "        cx = (g['x'].values + g['w'].values/2.0).round().astype(int).tolist()\n",
        "        cy = (g['y'].values + g['h'].values/2.0).round().astype(int).tolist()\n",
        "        toks = []\n",
        "        for u, x_, y_ in zip(pred_unicodes, cx, cy):\n",
        "            toks.extend([u, str(int(x_)), str(int(y_))])\n",
        "        rows_out.append(' '.join(toks))\n",
        "    else:\n",
        "        rows_out.append('')\n",
        "    processed += 1\n",
        "    if processed % 50 == 0:\n",
        "        print(f'Processed {processed}/{len(df_sample)} images in {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "sub_df = pd.DataFrame({'image_id': df_sample['image_id'], 'labels': rows_out})\n",
        "sub_df.to_csv('submission.csv', index=False)\n",
        "print('Wrote submission.csv with shape', sub_df.shape)\n",
        "print(sub_df.head(2))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_1158/1085797476.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state = torch.load(ckpt_path, map_location='cpu')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detector test preds: (34458, 6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 50/361 images in 101.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 100/361 images in 208.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 150/361 images in 308.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 200/361 images in 417.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 250/361 images in 521.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 300/361 images in 619.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 350/361 images in 723.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with shape (361, 2)\n      image_id                                             labels\n0  umgy007-028  U+904A 1285 2603 U+65BC 341 903 U+3067 1294 14...\n1  hnsd004-026  U+6575 493 582 U+3093 1246 2424 U+975E 501 174...\n"
          ]
        }
      ]
    },
    {
      "id": "b19b0deb-63c7-44fa-b7e3-177dc8e6ff2d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Assemble submission using best detector threshold/cap and best recognizer (EMA) checkpoint\n",
        "import json, time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageOps\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms as T\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "CWD = Path('.')\n",
        "best_cfg_path = CWD / 'det_threshold_best_fold0.json'\n",
        "det_test_path = CWD / 'det_test_preds.parquet'\n",
        "classes_path = CWD / 'recognizer_classes.json'\n",
        "best_ckpt_path = CWD / 'recognizer_resnet50_fold0_best.pth'\n",
        "test_dir = CWD / 'test_images'\n",
        "sample_path = CWD / 'sample_submission.csv'\n",
        "assert best_cfg_path.exists() and det_test_path.exists() and classes_path.exists() and best_ckpt_path.exists() and sample_path.exists(), 'Missing required artifacts for assembly'\n",
        "\n",
        "best_cfg = json.loads(best_cfg_path.read_text())\n",
        "th = float(best_cfg['best_threshold']); cap = int(best_cfg['best_cap'])\n",
        "use_dedup = bool(best_cfg.get('dedup', False))\n",
        "dedup_radius = float(best_cfg.get('dedup_radius', 6.0))\n",
        "print('Using detector filter:', best_cfg)\n",
        "\n",
        "# Load detector predictions and filter\n",
        "det_df = pd.read_parquet(det_test_path)\n",
        "print('Raw test preds:', det_df.shape, 'score stats:', det_df['score'].describe().to_dict())\n",
        "det_df = det_df[det_df['score'] >= th].copy()\n",
        "det_df.sort_values(['image_id','score'], ascending=[True, False], inplace=True)\n",
        "det_df['rn'] = det_df.groupby('image_id').cumcount()\n",
        "det_df = det_df[det_df['rn'] < cap].drop(columns=['rn'])\n",
        "\n",
        "def dedup_centers_df(df_img: pd.DataFrame, radius: float = 6.0) -> pd.DataFrame:\n",
        "    if len(df_img) <= 1:\n",
        "        return df_img\n",
        "    g = df_img.sort_values('score', ascending=False).reset_index(drop=True)\n",
        "    keep_idx = []\n",
        "    kept = []\n",
        "    for i, (cx, cy) in enumerate((g['x'] + g['w']/2.0, g['y'] + g['h']/2.0)):\n",
        "        pass  # placeholder (will be replaced below)\n",
        "    # Implement without vectorized zip to maintain clarity\n",
        "    keep_idx = []\n",
        "    kept = []\n",
        "    for i in range(len(g)):\n",
        "        cx = float(g.loc[i, 'x'] + g.loc[i, 'w']/2.0)\n",
        "        cy = float(g.loc[i, 'y'] + g.loc[i, 'h']/2.0)\n",
        "        ok = True\n",
        "        for (kx, ky) in kept:\n",
        "            if (cx - kx)*(cx - kx) + (cy - ky)*(cy - ky) <= radius*radius:\n",
        "                ok = False; break\n",
        "        if ok:\n",
        "            keep_idx.append(i); kept.append((cx, cy))\n",
        "    return g.iloc[keep_idx].reset_index(drop=True)\n",
        "\n",
        "if use_dedup:\n",
        "    parts = []\n",
        "    for img_id, g in det_df.groupby('image_id'):\n",
        "        parts.append(dedup_centers_df(g, radius=dedup_radius))\n",
        "    det_df = pd.concat(parts, axis=0).reset_index(drop=True) if parts else det_df.iloc[0:0]\n",
        "print('Filtered test preds:', det_df.shape, 'dedup applied:' , use_dedup)\n",
        "\n",
        "# Load classes and model\n",
        "class_to_idx = json.loads(classes_path.read_text())\n",
        "idx_to_class = {int(v): k for k, v in class_to_idx.items()}\n",
        "num_classes = len(idx_to_class)\n",
        "rec_model = resnet50(weights=None)\n",
        "rec_model.fc = nn.Linear(rec_model.fc.in_features, num_classes)\n",
        "state = torch.load(best_ckpt_path, map_location='cpu')\n",
        "rec_model.load_state_dict(state['model'], strict=False)\n",
        "rec_model.to(device)\n",
        "rec_model.eval()\n",
        "\n",
        "val_tf = T.Compose([T.ToTensor(), T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
        "\n",
        "def clamp(v, lo, hi):\n",
        "    return max(lo, min(hi, v))\n",
        "\n",
        "def crop_from_box(image_id: str, x: float, y: float, w: float, h: float, pad_ratio: float=0.25, img_size: int=192):\n",
        "    p = test_dir / f'{image_id}.jpg'\n",
        "    if not p.exists():\n",
        "        alt = test_dir / f'{image_id}.png'\n",
        "        if alt.exists():\n",
        "            p = alt\n",
        "    img = Image.open(p).convert('RGB')\n",
        "    W, H = img.size\n",
        "    dx = int(round(w * pad_ratio)); dy = int(round(h * pad_ratio))\n",
        "    x1 = clamp(int(x) - dx, 0, W-1); y1 = clamp(int(y) - dy, 0, H-1)\n",
        "    x2 = clamp(int(x + w) + dx, 1, W); y2 = clamp(int(y + h) + dy, 1, H)\n",
        "    crop = img.crop((x1, y1, x2, y2))\n",
        "    cw, ch = crop.size\n",
        "    if cw != ch:\n",
        "        m = max(cw, ch)\n",
        "        pad_w = m - cw; pad_h = m - ch\n",
        "        crop = ImageOps.expand(crop, border=(0,0,pad_w,pad_h), fill=0) if cw < m or ch < m else crop\n",
        "    crop = crop.resize((192, 192), Image.BILINEAR)\n",
        "    return crop\n",
        "\n",
        "df_sample = pd.read_csv(sample_path)\n",
        "grp = det_df.groupby('image_id')\n",
        "rows_out = []\n",
        "t0 = time.time()\n",
        "for i, image_id in enumerate(df_sample['image_id'].tolist(), 1):\n",
        "    if image_id in grp.groups:\n",
        "        g = grp.get_group(image_id)\n",
        "        crops = [crop_from_box(image_id, x, y, w, h) for x,y,w,h in zip(g['x'].values, g['y'].values, g['w'].values, g['h'].values)]\n",
        "        if len(crops) == 0:\n",
        "            rows_out.append('')\n",
        "        else:\n",
        "            xs = torch.stack([val_tf(c) for c in crops]).to(device)\n",
        "            with torch.no_grad():\n",
        "                logits = rec_model(xs)\n",
        "                pred_idx = logits.argmax(1).detach().cpu().numpy().tolist()\n",
        "            pred_unicodes = [idx_to_class.get(int(k), 'U+003F') for k in pred_idx]\n",
        "            cx = (g['x'].values + g['w'].values/2.0).round().astype(int).tolist()\n",
        "            cy = (g['y'].values + g['h'].values/2.0).round().astype(int).tolist()\n",
        "            toks = []\n",
        "            for u, x_, y_ in zip(pred_unicodes, cx, cy):\n",
        "                toks.extend([u, str(int(x_)), str(int(y_))])\n",
        "            rows_out.append(' '.join(toks))\n",
        "    else:\n",
        "        rows_out.append('')\n",
        "    if i % 50 == 0:\n",
        "        print(f'Assembled {i}/{len(df_sample)} in {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "sub_df = pd.DataFrame({'image_id': df_sample['image_id'], 'labels': rows_out})\n",
        "sub_df.to_csv('submission.csv', index=False)\n",
        "print('Wrote submission.csv with shape', sub_df.shape)\n",
        "print(sub_df.head(2))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}