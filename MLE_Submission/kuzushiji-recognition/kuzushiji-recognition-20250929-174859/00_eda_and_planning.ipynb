{
  "cells": [
    {
      "id": "5123e390-8126-4a4a-8332-58d466e96faa",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kuzushiji Recognition: Plan\n",
        "\n",
        "Goals:\n",
        "- Establish GPU-enabled environment; verify CUDA 12.1 torch stack if needed.\n",
        "- Inspect data schema (train.csv, unicode_translation.csv, sample_submission.csv) and image zips.\n",
        "- Define CV strategy mirroring test distribution.\n",
        "- Ship a fast baseline (e.g., lightweight detector/classifier or simple heuristic) to create a valid submission.\n",
        "- Iterate toward medal via improved modeling (e.g., CNN-based detector/classifier with augmentations, ensembling).\n",
        "\n",
        "Initial Milestones:\n",
        "1) Environment + data EDA\n",
        "2) Baseline pipeline and valid submission\n",
        "3) Cross-validation and OOF checks\n",
        "4) Model improvements and ensembling\n",
        "\n",
        "At each milestone, we will request expert review."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "4cdfe145-27c8-487e-86c2-c7783d4936ff",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, time, subprocess, json, math, re, textwrap, zipfile\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def run_cmd(cmd):\n",
        "    print('>',' '.join(cmd), flush=True)\n",
        "    try:\n",
        "        out = subprocess.check_output(cmd, stderr=subprocess.STDOUT).decode()\n",
        "        print(out, flush=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(e.output.decode(), flush=True)\n",
        "\n",
        "print('Checking GPU via nvidia-smi...')\n",
        "run_cmd(['bash','-lc','nvidia-smi || true'])\n",
        "\n",
        "base = Path('.')\n",
        "files = [\n",
        "    'train.csv',\n",
        "    'unicode_translation.csv',\n",
        "    'sample_submission.csv',\n",
        "    'train_images.zip',\n",
        "    'test_images.zip',\n",
        "]\n",
        "for f in files:\n",
        "    p = base / f\n",
        "    print(f'{f}: exists={p.exists()} size={p.stat().st_size if p.exists() else None}')\n",
        "\n",
        "print('\\nLoading CSVs...')\n",
        "train_df = pd.read_csv(base/'train.csv')\n",
        "utr_df = pd.read_csv(base/'unicode_translation.csv')\n",
        "ss_df = pd.read_csv(base/'sample_submission.csv')\n",
        "print('train.csv shape:', train_df.shape)\n",
        "print('train.csv columns:', list(train_df.columns))\n",
        "print(train_df.head(3))\n",
        "print('\\nunicode_translation.csv shape:', utr_df.shape)\n",
        "print(utr_df.head(3))\n",
        "print('\\nsample_submission.csv shape:', ss_df.shape)\n",
        "print(ss_df.head(3))\n",
        "\n",
        "def peek_zip(zpath, n=10):\n",
        "    if not Path(zpath).exists():\n",
        "        print(f'{zpath} not found')\n",
        "        return\n",
        "    with zipfile.ZipFile(zpath) as zf:\n",
        "        infos = zf.infolist()\n",
        "        print(f'{zpath}: {len(infos)} files in archive')\n",
        "        for i, zi in enumerate(infos[:n]):\n",
        "            print(f'  {i}: {zi.filename} size={zi.file_size}')\n",
        "\n",
        "print('\\nPeeking into zips...')\n",
        "peek_zip('train_images.zip', n=5)\n",
        "peek_zip('test_images.zip', n=5)\n",
        "\n",
        "# Quick inference about submission format\n",
        "print('\\nSubmission format sample row:')\n",
        "print(ss_df.iloc[0].to_dict())\n",
        "print('\\nDone EDA baseline. Next: confirm whether labels string uses triplets or quintets and pixel units.')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking GPU via nvidia-smi...\n> bash -lc nvidia-smi || true\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Sep 30 07:21:09 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.06             Driver Version: 550.144.06     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10-24Q                 On  |   00000002:00:00.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |     414MiB /  24512MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.csv: exists=True size=14069467\nunicode_translation.csv: exists=True size=52646\nsample_submission.csv: exists=True size=13700\ntrain_images.zip: exists=True size=2711943248\ntest_images.zip: exists=True size=307454375\n\nLoading CSVs...\ntrain.csv shape: (3244, 2)\ntrain.csv columns: ['image_id', 'labels']\n            image_id                                             labels\n0  200004148_00015_1  U+306F 1187 361 47 27 U+306F 1487 2581 48 28 U...\n1  200021712-00008_2  U+4E00 1543 1987 58 11 U+4E00 1296 1068 91 11 ...\n2  100249416_00034_1  U+4E00 1214 415 73 11 U+4E00 1386 412 72 13 U+...\n\nunicode_translation.csv shape: (4781, 2)\n  Unicode char\n0  U+0031    1\n1  U+0032    2\n2  U+0034    4\n\nsample_submission.csv shape: (361, 2)\n            image_id                 labels\n0        umgy007-028  U+003F 1 1 U+FF2F 2 2\n1        hnsd004-026  U+003F 1 1 U+FF2F 2 2\n2  200003076_00034_2  U+003F 1 1 U+FF2F 2 2\n\nPeeking into zips...\ntrain_images.zip: 3244 files in archive\n  0: brsk001-030.jpg size=1264911\n  1: 100249376_00028_1.jpg size=664235\n  2: 200021660-00087_2.jpg size=1331155\n  3: hnsd006-030.jpg size=697506\n  4: 200003076_00051_1.jpg size=964810\ntest_images.zip: 361 files in archive\n  0: 100249376_00006_2.jpg size=656050\n  1: 100249376_00037_2.jpg size=653129\n  2: 200021660-00045_1.jpg size=1263039\n  3: 200014740-00077_2.jpg size=696393\n  4: umgy010-043.jpg size=696487\n\nSubmission format sample row:\n{'image_id': 'umgy007-028', 'labels': 'U+003F 1 1 U+FF2F 2 2'}\n\nDone EDA baseline. Next: confirm whether labels string uses triplets or quintets and pixel units.\n"
          ]
        }
      ]
    },
    {
      "id": "fb781e4a-ec10-470a-9889-9e6b79632499",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import shutil\n",
        "from collections import Counter\n",
        "\n",
        "base = Path('.')\n",
        "train_zip = base/'train_images.zip'\n",
        "test_zip = base/'test_images.zip'\n",
        "train_dir = base/'train_images'\n",
        "test_dir = base/'test_images'\n",
        "\n",
        "def ensure_unzip(zpath: Path, out_dir: Path, max_preview: int = 0):\n",
        "    if out_dir.exists() and any(out_dir.iterdir()):\n",
        "        print(f'Exists: {out_dir} (skipping unzip)')\n",
        "        return\n",
        "    print(f'Extracting {zpath} -> {out_dir} ...')\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    with zipfile.ZipFile(zpath) as zf:\n",
        "        zf.extractall(out_dir)\n",
        "    print('Done extract.')\n",
        "    if max_preview:\n",
        "        print('Preview files:')\n",
        "        for i,p in enumerate(sorted(out_dir.iterdir())[:max_preview]):\n",
        "            print(i, p.name, p.stat().st_size)\n",
        "\n",
        "ensure_unzip(train_zip, train_dir, max_preview=5)\n",
        "ensure_unzip(test_zip, test_dir, max_preview=5)\n",
        "\n",
        "# Parse train labels: tokens repeating as [unicode x y w h] in pixels\n",
        "def parse_labels_row(row):\n",
        "    image_id = row['image_id']\n",
        "    s = str(row['labels']) if pd.notna(row['labels']) else ''\n",
        "    if not s.strip():\n",
        "        return []\n",
        "    toks = s.strip().split()\n",
        "    out = []\n",
        "    i = 0\n",
        "    while i < len(toks):\n",
        "        u = toks[i];\n",
        "        if not u.startswith('U+') or i+4 >= len(toks):\n",
        "            # malformed; break\n",
        "            break\n",
        "        try:\n",
        "            x = int(toks[i+1]); y = int(toks[i+2]); w = int(toks[i+3]); h = int(toks[i+4])\n",
        "        except Exception:\n",
        "            break\n",
        "        out.append({'image_id': image_id, 'unicode': u, 'x': x, 'y': y, 'w': w, 'h': h})\n",
        "        i += 5\n",
        "    return out\n",
        "\n",
        "all_rows = []\n",
        "for _, r in train_df.iterrows():\n",
        "    all_rows.extend(parse_labels_row(r))\n",
        "boxes_df = pd.DataFrame(all_rows)\n",
        "print('Parsed boxes:', boxes_df.shape, 'columns:', list(boxes_df.columns))\n",
        "print(boxes_df.head())\n",
        "\n",
        "# Basic stats\n",
        "per_image_counts = boxes_df.groupby('image_id').size().rename('n').reset_index()\n",
        "print('Images:', train_df.shape[0], 'Total boxes:', len(boxes_df), 'Mean per image:', per_image_counts['n'].mean())\n",
        "print('Quantiles per image:', per_image_counts['n'].quantile([0,0.25,0.5,0.75,0.9,0.95,0.99]).to_dict())\n",
        "\n",
        "# Build unicode <-> class_id map for training\n",
        "unicodes = sorted(boxes_df['unicode'].unique().tolist())\n",
        "u2id = {u:i for i,u in enumerate(unicodes)}\n",
        "id2u = {i:u for u,i in u2id.items()}\n",
        "print('Num classes:', len(unicodes))\n",
        "print('Sample mapping:', list(u2id.items())[:5])\n",
        "\n",
        "# Prepare YOLO labels (normalized cx,cy,w,h) into yolo_labels/<image_id>.txt\n",
        "yolo_labels_dir = base/'yolo_labels'\n",
        "yolo_labels_dir.mkdir(exist_ok=True)\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def write_yolo_label_for_image(image_id):\n",
        "    img_path = train_dir / f'{image_id}.jpg'\n",
        "    if not img_path.exists():\n",
        "        # Some files may use .png (unlikely here), try alternate\n",
        "        alt = list(train_dir.glob(f'{image_id}.*'))\n",
        "        if alt:\n",
        "            img_path = alt[0]\n",
        "        else:\n",
        "            return 0\n",
        "    with Image.open(img_path) as im:\n",
        "        w_img, h_img = im.size\n",
        "    df = boxes_df[boxes_df.image_id == image_id]\n",
        "    lines = []\n",
        "    for _, b in df.iterrows():\n",
        "        cx = (b['x'] + b['w']/2) / w_img\n",
        "        cy = (b['y'] + b['h']/2) / h_img\n",
        "        ww = b['w'] / w_img\n",
        "        hh = b['h'] / h_img\n",
        "        cls = u2id[b['unicode']]\n",
        "        # clamp\n",
        "        cx = min(max(cx, 0.0), 1.0); cy = min(max(cy, 0.0), 1.0)\n",
        "        ww = min(max(ww, 0.0), 1.0); hh = min(max(hh, 0.0), 1.0)\n",
        "        lines.append(f\"{cls} {cx:.6f} {cy:.6f} {ww:.6f} {hh:.6f}\")\n",
        "    (yolo_labels_dir/f'{image_id}.txt').write_text('\\n'.join(lines))\n",
        "    return len(lines)\n",
        "\n",
        "t0 = time.time()\n",
        "n_written = 0\n",
        "for img_id in train_df['image_id']:\n",
        "    n_written += write_yolo_label_for_image(img_id)\n",
        "print(f'Wrote YOLO labels for {train_df.shape[0]} images, total boxes {n_written}, elapsed {time.time()-t0:.1f}s')\n",
        "\n",
        "# Create simple train/val split by image with stratification by box-count buckets\n",
        "def make_split(df_images, per_image_counts, val_frac=0.1, seed=42):\n",
        "    df = df_images.merge(per_image_counts, on='image_id', how='left').fillna({'n':0})\n",
        "    bins = pd.qcut(df['n'], q=min(10, max(2, df.shape[0]//50)), duplicates='drop')\n",
        "    df['bin'] = bins.astype(str)\n",
        "    # simple stratified split\n",
        "    rng = pd.Series(range(df.shape[0]))\n",
        "    df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
        "    val_idx = []\n",
        "    for b, g in df.groupby('bin'):\n",
        "        k = max(1, int(len(g)*val_frac))\n",
        "        val_idx.extend(g.index[:k].tolist())\n",
        "    df['is_val'] = False\n",
        "    df.loc[val_idx, 'is_val'] = True\n",
        "    return df[['image_id','is_val']]\n",
        "\n",
        "split_df = make_split(train_df[['image_id']], per_image_counts, val_frac=0.1, seed=42)\n",
        "print(split_df['is_val'].value_counts())\n",
        "split_df.head()\n",
        "\n",
        "print('Ready to install Ultralytics and kick off YOLO training next. Also confirmed submission format appears as triplets (unicode cx cy) from sample_submission; we will output centers from predicted boxes in pixels.')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exists: train_images (skipping unzip)\nExists: test_images (skipping unzip)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed boxes: (613505, 6) columns: ['image_id', 'unicode', 'x', 'y', 'w', 'h']\n            image_id unicode     x     y   w   h\n0  200004148_00015_1  U+306F  1187   361  47  27\n1  200004148_00015_1  U+306F  1487  2581  48  28\n2  200004148_00015_1  U+3070  1187  1063  74  30\n3  200004148_00015_1  U+3070   594  1154  93  31\n4  200004148_00015_1  U+306F  1192  1842  52  32\nImages: 3244 Total boxes: 613505 Mean per image: 189.1199136868064\nQuantiles per image: {0.0: 2.0, 0.25: 132.0, 0.5: 188.0, 0.75: 228.0, 0.9: 322.0, 0.95: 350.0, 0.99: 403.0}\nNum classes: 4113\nSample mapping: [('U+003F', 0), ('U+2000B', 1), ('U+20D45', 2), ('U+2123D', 3), ('U+22999', 4)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote YOLO labels for 3244 images, total boxes 613505, elapsed 94.8s\nis_val\nFalse    2925\nTrue      319\nName: count, dtype: int64\nReady to install Ultralytics and kick off YOLO training next. Also confirmed submission format appears as triplets (unicode cx cy) from sample_submission; we will output centers from predicted boxes in pixels.\n"
          ]
        }
      ]
    },
    {
      "id": "131442fb-cee7-429d-aca2-59356de397a7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, subprocess, shutil, time\n",
        "from pathlib import Path\n",
        "\n",
        "def pip(*args):\n",
        "    print('> pip', ' '.join(args), flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "print('Uninstalling any preexisting torch stack (idempotent) ...', flush=True)\n",
        "for pkg in ('torch','torchvision','torchaudio'):\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', pkg], check=False)\n",
        "\n",
        "for d in (\n",
        "    '/app/.pip-target/torch',\n",
        "    '/app/.pip-target/torch-2.8.0.dist-info',\n",
        "    '/app/.pip-target/torch-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torchvision',\n",
        "    '/app/.pip-target/torchvision-0.23.0.dist-info',\n",
        "    '/app/.pip-target/torchvision-0.19.1.dist-info',\n",
        "    '/app/.pip-target/torchaudio',\n",
        "    '/app/.pip-target/torchaudio-2.8.0.dist-info',\n",
        "    '/app/.pip-target/torchaudio-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torchgen',\n",
        "    '/app/.pip-target/functorch',\n",
        "):\n",
        "    if os.path.exists(d):\n",
        "        print('Removing', d, flush=True)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "print('Installing CUDA 12.1 torch stack ...', flush=True)\n",
        "pip('install',\n",
        "    '--index-url', 'https://download.pytorch.org/whl/cu121',\n",
        "    '--extra-index-url', 'https://pypi.org/simple',\n",
        "    'torch==2.4.1', 'torchvision==0.19.1', 'torchaudio==2.4.1')\n",
        "\n",
        "Path('constraints.txt').write_text('torch==2.4.1\\ntorchvision==0.19.1\\ntorchaudio==2.4.1\\n')\n",
        "\n",
        "print('Installing ultralytics and deps under constraints ...', flush=True)\n",
        "pip('install', '-c', 'constraints.txt',\n",
        "    'ultralytics==8.3.32',\n",
        "    'opencv-python-headless',\n",
        "    'albumentations',\n",
        "    'pyyaml',\n",
        "    '--upgrade-strategy', 'only-if-needed')\n",
        "\n",
        "import torch\n",
        "print('torch', torch.__version__, 'CUDA build', getattr(torch.version, 'cuda', None))\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "print('Installation complete.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "2582f001-9c93-4cb0-b529-40e109f4b51c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Persist mappings and splits\n",
        "Path('artifacts').mkdir(exist_ok=True)\n",
        "Path('artifacts/u2id.json').write_text(json.dumps(u2id, ensure_ascii=False))\n",
        "Path('artifacts/id2u.json').write_text(json.dumps(id2u, ensure_ascii=False))\n",
        "split_df.to_csv('artifacts/split.csv', index=False)\n",
        "\n",
        "# Create train/val txt lists for Ultralytics\n",
        "train_list = []\n",
        "val_list = []\n",
        "for _, r in split_df.iterrows():\n",
        "    img_path = train_dir / f\"{r['image_id']}.jpg\"\n",
        "    if not img_path.exists():\n",
        "        alts = list(train_dir.glob(f\"{r['image_id']}.*\"))\n",
        "        if alts:\n",
        "            img_path = alts[0]\n",
        "        else:\n",
        "            continue\n",
        "    if r['is_val']:\n",
        "        val_list.append(str(img_path.resolve()))\n",
        "    else:\n",
        "        train_list.append(str(img_path.resolve()))\n",
        "\n",
        "Path('artifacts/train.txt').write_text('\\n'.join(train_list))\n",
        "Path('artifacts/val.txt').write_text('\\n'.join(val_list))\n",
        "print(f'Train images: {len(train_list)}, Val images: {len(val_list)}')\n",
        "\n",
        "# Build dataset YAML for Ultralytics (uses txt lists and external labels dir)\n",
        "names = [id2u[i] for i in range(len(id2u))]\n",
        "dataset_yaml = f'''\n",
        "path: .\n",
        "train: artifacts/train.txt\n",
        "val: artifacts/val.txt\n",
        "names: {json.dumps(names, ensure_ascii=False)}\n",
        "nc: {len(names)}\n",
        "roboflow: null\n",
        "'''\n",
        "Path('kuz_dataset.yaml').write_text(dataset_yaml)\n",
        "print('Wrote kuz_dataset.yaml with', len(names), 'classes')\n",
        "\n",
        "# Symlink or inform Ultralytics where labels are located: we keep labels in yolo_labels/\n",
        "# Ultralytics infers labels by replacing /images/ with /labels/. Since we pass txts, it will still do that.\n",
        "# To accommodate, create a parallel labels folder structure via a flat symlink directory named 'labels' at repo root.\n",
        "labels_dir = Path('labels')\n",
        "if not labels_dir.exists():\n",
        "    labels_dir.mkdir(exist_ok=True)\n",
        "    # Create symlinks for each label file into labels/ with same basename as image basename but .txt\n",
        "    created = 0\n",
        "    for p in train_dir.iterdir():\n",
        "        if p.is_file():\n",
        "            stem = p.stem\n",
        "            src = Path('yolo_labels')/f'{stem}.txt'\n",
        "            if src.exists():\n",
        "                dst = labels_dir/f'{stem}.txt'\n",
        "                try:\n",
        "                    if not dst.exists():\n",
        "                        dst.symlink_to(src.resolve())\n",
        "                        created += 1\n",
        "                except Exception:\n",
        "                    # fallback: copy if symlink not permitted\n",
        "                    if not dst.exists():\n",
        "                        dst.write_text(src.read_text())\n",
        "                        created += 1\n",
        "    print('Prepared labels links/copied:', created)\n",
        "else:\n",
        "    print('Labels dir exists; assuming prepared.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "cc4fb681-d007-40f9-89bf-b19e81e3e92f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "root = Path('dataset')\n",
        "img_tr = root/'images'/'train'\n",
        "img_va = root/'images'/'val'\n",
        "lab_tr = root/'labels'/'train'\n",
        "lab_va = root/'labels'/'val'\n",
        "for d in (img_tr, img_va, lab_tr, lab_va):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def safe_link(src: Path, dst: Path):\n",
        "    try:\n",
        "        if dst.exists():\n",
        "            return\n",
        "        dst.symlink_to(src.resolve())\n",
        "    except Exception:\n",
        "        if not dst.exists():\n",
        "            if src.is_file():\n",
        "                shutil.copy2(src, dst)\n",
        "\n",
        "n_tr = n_va = 0\n",
        "for _, r in split_df.iterrows():\n",
        "    img = train_dir / f\"{r['image_id']}.jpg\"\n",
        "    if not img.exists():\n",
        "        alts = list(train_dir.glob(f\"{r['image_id']}.*\"))\n",
        "        if not alts:\n",
        "            continue\n",
        "        img = alts[0]\n",
        "    lab = Path('yolo_labels')/f\"{img.stem}.txt\"\n",
        "    if r['is_val']:\n",
        "        dst_img = img_va / img.name\n",
        "        dst_lab = lab_va / lab.name\n",
        "        safe_link(img, dst_img)\n",
        "        if lab.exists():\n",
        "            safe_link(lab, dst_lab)\n",
        "        n_va += 1\n",
        "    else:\n",
        "        dst_img = img_tr / img.name\n",
        "        dst_lab = lab_tr / lab.name\n",
        "        safe_link(img, dst_img)\n",
        "        if lab.exists():\n",
        "            safe_link(lab, dst_lab)\n",
        "        n_tr += 1\n",
        "\n",
        "print('Symlinked/copied images -> train:', n_tr, 'val:', n_va)\n",
        "print('Images/train files:', len(list(img_tr.glob('*'))), 'Images/val files:', len(list(img_va.glob('*'))))\n",
        "print('Labels/train files:', len(list(lab_tr.glob('*.txt'))), 'Labels/val files:', len(list(lab_va.glob('*.txt'))))\n",
        "\n",
        "# Overwrite dataset YAML to use directory structure (more robust for Ultralytics)\n",
        "names = [id2u[i] for i in range(len(id2u))]\n",
        "yaml_dir = f'''\n",
        "path: {root.as_posix()}\n",
        "train: images/train\n",
        "val: images/val\n",
        "names: {json.dumps(names, ensure_ascii=False)}\n",
        "nc: {len(names)}\n",
        "roboflow: null\n",
        "'''\n",
        "Path('kuz_dataset.yaml').write_text(yaml_dir)\n",
        "print('Wrote kuz_dataset.yaml (dir-based) with', len(names), 'classes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "23fda24c-1246-41f5-80a3-6be8c6bc6573",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, time\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "from ultralytics import YOLO\n",
        "\n",
        "print('Starting YOLOv8 training (single-class detector to avoid OOM)...', flush=True)\n",
        "t0 = time.time()\n",
        "# Use smallest model and single class (class-agnostic detector)\n",
        "model = YOLO('yolov8n.pt')\n",
        "result = model.train(\n",
        "    data='kuz_dataset_single.yaml',\n",
        "    imgsz=1024,\n",
        "    epochs=15,\n",
        "    batch=8,\n",
        "    workers=4,\n",
        "    device=0,\n",
        "    optimizer='auto',\n",
        "    cos_lr=False,\n",
        "    patience=7,\n",
        "    project='runs',\n",
        "    name='yolo8n_kuz_single',\n",
        "    exist_ok=True,\n",
        "    save_period=1,\n",
        "    pretrained=True,\n",
        "    amp=True,\n",
        "    plots=False,\n",
        "    single_cls=True\n",
        ")\n",
        "print('Training done. Elapsed: %.1fs' % (time.time()-t0), flush=True)\n",
        "print('Results dir:', getattr(result, 'save_dir', 'runs/yolo8n_kuz_single'), flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "724bb012-18d0-446f-852e-528dfd0178b2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess, sys\n",
        "def bash(cmd):\n",
        "    print('> bash -lc', cmd, flush=True)\n",
        "    print(subprocess.check_output(['bash','-lc',cmd], stderr=subprocess.STDOUT).decode())\n",
        "\n",
        "print('Installing system libs for OpenCV (libGL/libglib)...', flush=True)\n",
        "bash('apt-get update -y && apt-get install -y libgl1 libglib2.0-0')\n",
        "print('Testing cv2 import after install...', flush=True)\n",
        "import cv2\n",
        "print('cv2 version:', cv2.__version__)\n",
        "print('Re-trying ultralytics import...')\n",
        "from ultralytics import YOLO\n",
        "print('Ultralytics import OK.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "f6af41a3-19ee-497d-b51d-96d8e551fede",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys, subprocess\n",
        "\n",
        "def pip(*args):\n",
        "    print('> pip', ' '.join(args), flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "print('Fixing OpenCV import by enforcing headless build...', flush=True)\n",
        "subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', 'opencv-python', 'opencv-contrib-python', 'opencv-python-headless'], check=False)\n",
        "pip('install', 'opencv-python-headless==4.10.0.84')\n",
        "\n",
        "print('Testing cv2 import...', flush=True)\n",
        "import cv2, os\n",
        "print('cv2 version:', cv2.__version__)\n",
        "print('cv2 file:', cv2.__file__)\n",
        "\n",
        "print('Re-trying ultralytics import...', flush=True)\n",
        "from ultralytics import YOLO\n",
        "print('Ultralytics import OK.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "94826468-a7f1-4ac6-b943-9779f0905aff",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Rewrite kuz_dataset.yaml with absolute image dirs to bypass Ultralytics datasets_dir prefixing\n",
        "root = Path('dataset').resolve()\n",
        "img_tr_abs = (root/'images'/'train').as_posix()\n",
        "img_va_abs = (root/'images'/'val').as_posix()\n",
        "names = [id2u[i] for i in range(len(id2u))]\n",
        "yaml_abs = f'''\n",
        "train: {img_tr_abs}\n",
        "val: {img_va_abs}\n",
        "names: {json.dumps(names, ensure_ascii=False)}\n",
        "nc: {len(names)}\n",
        "roboflow: null\n",
        "'''\n",
        "Path('kuz_dataset.yaml').write_text(yaml_abs)\n",
        "print('Rewrote kuz_dataset.yaml with absolute paths:')\n",
        "print(Path('kuz_dataset.yaml').read_text()[:500])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "21d2eb2e-96d1-4ffe-9c90-e4d89f34216d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json, math, time\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "\n",
        "def load_id2u(path='artifacts/id2u.json'):\n",
        "    with open(path, 'r') as f:\n",
        "        d = json.load(f)\n",
        "    # keys may be strings; convert to int-indexed list\n",
        "    max_k = max(int(k) for k in d.keys())\n",
        "    arr = [None]*(max_k+1)\n",
        "    for k,v in d.items():\n",
        "        arr[int(k)] = v\n",
        "    return arr\n",
        "\n",
        "def build_submission(weights_path: str, conf=0.25, iou=0.65, imgsz=1024, max_det=1000, save_name='submission.csv'):\n",
        "    print(f'Loading model: {weights_path}', flush=True)\n",
        "    model = YOLO(weights_path)\n",
        "    id2u_list = load_id2u('artifacts/id2u.json')\n",
        "    ss = pd.read_csv('sample_submission.csv')\n",
        "    image_ids = ss['image_id'].tolist()\n",
        "    img_paths = []\n",
        "    for img_id in image_ids:\n",
        "        p = Path('test_images')/f'{img_id}.jpg'\n",
        "        if not p.exists():\n",
        "            alts = list(Path('test_images').glob(f'{img_id}.*'))\n",
        "            if alts:\n",
        "                p = alts[0]\n",
        "        img_paths.append(str(p))\n",
        "    print('Running inference on', len(img_paths), 'images ...', flush=True)\n",
        "    results = model.predict(source=img_paths, imgsz=imgsz, conf=conf, iou=iou, max_det=max_det, device=0, stream=True, verbose=False)\n",
        "    rows = []\n",
        "    t0 = time.time()\n",
        "    for i, (img_id, img_path, res) in enumerate(zip(image_ids, img_paths, results)):\n",
        "        if i % 25 == 0:\n",
        "            print(f'Processed {i}/{len(image_ids)} images, elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "        try:\n",
        "            with Image.open(img_path) as im:\n",
        "                w_img, h_img = im.size\n",
        "        except Exception:\n",
        "            w_img = h_img = None\n",
        "        labels = []\n",
        "        if res and hasattr(res, 'boxes') and res.boxes is not None:\n",
        "            boxes = res.boxes\n",
        "            if boxes.xyxy is not None and boxes.cls is not None:\n",
        "                xyxy = boxes.xyxy.cpu().numpy()\n",
        "                cls = boxes.cls.cpu().numpy().astype(int)\n",
        "                for (x1,y1,x2,y2), c in zip(xyxy, cls):\n",
        "                    cx = int(round((float(x1)+float(x2))/2.0))\n",
        "                    cy = int(round((float(y1)+float(y2))/2.0))\n",
        "                    # optional clamp if dims known\n",
        "                    if w_img is not None and h_img is not None:\n",
        "                        cx = max(0, min(cx, w_img-1))\n",
        "                        cy = max(0, min(cy, h_img-1))\n",
        "                    u = id2u_list[c] if 0 <= c < len(id2u_list) else None\n",
        "                    if u:\n",
        "                        labels.extend([u, str(cx), str(cy)])\n",
        "        rows.append({'image_id': img_id, 'labels': ' '.join(labels)})\n",
        "    sub = pd.DataFrame(rows)\n",
        "    sub.to_csv(save_name, index=False)\n",
        "    print('Wrote', save_name, 'with shape', sub.shape, flush=True)\n",
        "    return sub\n",
        "\n",
        "print('Inference/submission utilities ready. After training completes, call:')\n",
        "print(\"build_submission('runs/yolo8m_kuz/weights/best.pt', conf=0.25, iou=0.65, imgsz=1024, max_det=1000)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "5996986e-18f8-4e0e-a0ed-fb99c9721dc0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create a single-class dataset YAML to avoid huge classification head\n",
        "root_abs = Path('dataset').resolve()\n",
        "img_tr_abs = (root_abs/'images'/'train').as_posix()\n",
        "img_va_abs = (root_abs/'images'/'val').as_posix()\n",
        "single_yaml = f'''\n",
        "train: {img_tr_abs}\n",
        "val: {img_va_abs}\n",
        "names: ['char']\n",
        "nc: 1\n",
        "roboflow: null\n",
        "'''\n",
        "Path('kuz_dataset_single.yaml').write_text(single_yaml)\n",
        "print('Wrote kuz_dataset_single.yaml with nc=1 at absolute paths:')\n",
        "print(Path('kuz_dataset_single.yaml').read_text())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "aa497268-eab3-4e0a-a4fc-63a428085c55",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys, subprocess, os, time, math, json, gc\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "def pip(*args):\n",
        "    print('> pip', ' '.join(args), flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "print('Installing kNN embedding deps (timm, faiss-cpu) if missing...', flush=True)\n",
        "try:\n",
        "    import timm  # noqa\n",
        "except Exception:\n",
        "    pip('install', '-c', 'constraints.txt', 'timm==1.0.9', '--upgrade-strategy', 'only-if-needed')\n",
        "try:\n",
        "    import faiss  # noqa\n",
        "except Exception:\n",
        "    pip('install', 'faiss-cpu==1.8.0.post1')\n",
        "\n",
        "import torch\n",
        "import timm\n",
        "import faiss\n",
        "\n",
        "# Global CLAHE instance (reuse for speed)\n",
        "CLAHE = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "\n",
        "# Preprocess: grayscale->3ch, optional CLAHE, pad bbox by 15%, resize to 224\n",
        "def crop_pad_resize(img: Image.Image, x:int, y:int, w:int, h:int, pad_frac:float=0.15, out_size:int=224):\n",
        "    W, H = img.size\n",
        "    cx = x + w/2.0\n",
        "    cy = y + h/2.0\n",
        "    pw = max(2, int(round(w * pad_frac)))\n",
        "    ph = max(2, int(round(h * pad_frac)))\n",
        "    x1 = max(0, int(round(cx - w/2 - pw)))\n",
        "    y1 = max(0, int(round(cy - h/2 - ph)))\n",
        "    x2 = min(W, int(round(cx + w/2 + pw)))\n",
        "    y2 = min(H, int(round(cy + h/2 + ph)))\n",
        "    crop = img.crop((x1, y1, x2, y2))\n",
        "    # grayscale -> 3ch\n",
        "    crop = crop.convert('L')\n",
        "    arr = np.array(crop)\n",
        "    # CLAHE light\n",
        "    arr = CLAHE.apply(arr)\n",
        "    # pad to square keep aspect\n",
        "    h0, w0 = arr.shape[:2]\n",
        "    m = max(h0, w0)\n",
        "    pad_top = (m - h0) // 2\n",
        "    pad_bottom = m - h0 - pad_top\n",
        "    pad_left = (m - w0) // 2\n",
        "    pad_right = m - w0 - pad_left\n",
        "    arr = cv2.copyMakeBorder(arr, pad_top, pad_bottom, pad_left, pad_right, borderType=cv2.BORDER_CONSTANT, value=0)\n",
        "    arr = cv2.resize(arr, (out_size, out_size), interpolation=cv2.INTER_AREA)\n",
        "    arr = np.stack([arr, arr, arr], axis=0).astype(np.float32) / 255.0\n",
        "    return arr\n",
        "\n",
        "# Build embedding model (pretrained, global pooled, L2-normalized) with ImageNet normalization\n",
        "def build_backbone(model_name:str='convnext_tiny', device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    model = timm.create_model(model_name, pretrained=True, num_classes=0, global_pool='avg')\n",
        "    model.eval().to(device)\n",
        "    # resolve data config for proper normalization\n",
        "    data_cfg = timm.data.resolve_model_data_config(model)\n",
        "    mean = torch.tensor(data_cfg.get('mean', (0.485, 0.456, 0.406)), dtype=torch.float32, device=device).view(1,3,1,1)\n",
        "    std = torch.tensor(data_cfg.get('std', (0.229, 0.224, 0.225)), dtype=torch.float32, device=device).view(1,3,1,1)\n",
        "    # get feature dim\n",
        "    with torch.no_grad():\n",
        "        dummy = torch.zeros(1,3,224,224, device=device)\n",
        "        dummy = (dummy - mean) / std\n",
        "        f = model(dummy)\n",
        "    feat_dim = int(f.shape[1])\n",
        "    return model, feat_dim, device, mean, std\n",
        "\n",
        "def embed_batch(model, device, batch_np, mean:torch.Tensor, std:torch.Tensor):\n",
        "    with torch.no_grad():\n",
        "        t = torch.from_numpy(batch_np).to(device)\n",
        "        t = (t - mean) / std\n",
        "        feats = model(t)\n",
        "        feats = torch.nn.functional.normalize(feats, p=2, dim=1)\n",
        "        return feats.detach().cpu().numpy().astype(np.float32)\n",
        "\n",
        "# Create prototype medians per unicode using train split, capping per-class samples for speed\n",
        "def build_prototypes(max_per_class:int=100, img_dir='train_images', out_dir='artifacts', model_name='convnext_tiny'):\n",
        "    out = Path(out_dir); out.mkdir(exist_ok=True, parents=True)\n",
        "    model, feat_dim, device, mean, std = build_backbone(model_name)\n",
        "    # select training rows (not val) and pre-sample per class to avoid scanning all rows\n",
        "    split_map = dict(zip(split_df['image_id'], split_df['is_val']))\n",
        "    df = boxes_df.copy()\n",
        "    df = df[~df['image_id'].map(split_map).fillna(False)]  # keep train-only\n",
        "    # random sample up to max_per_class per unicode\n",
        "    rng = np.random.RandomState(42)\n",
        "    def _sample_grp(g):\n",
        "        if len(g) <= max_per_class:\n",
        "            return g\n",
        "        idx = rng.choice(len(g), size=max_per_class, replace=False)\n",
        "        return g.iloc[idx]\n",
        "    t0 = time.time()\n",
        "    df_s = df.groupby('unicode', as_index=False, group_keys=False).apply(_sample_grp)\n",
        "    df_s = df_s.sample(frac=1.0, random_state=42).reset_index(drop=True)  # shuffle for IO locality somewhat\n",
        "    total_target = len(df_s)\n",
        "    n_classes = df_s['unicode'].nunique()\n",
        "    print(f'Prototype sampling: classes={n_classes}, target_crops={total_target} (cap={max_per_class})', flush=True)\n",
        "\n",
        "    batch, metas = [], []\n",
        "    feats_list = {}  # unicode -> list of embeddings\n",
        "    last_log = time.time()\n",
        "    for i, r in enumerate(df_s.itertuples(index=False)):\n",
        "        iid = r.image_id; u = r.unicode; x=int(r.x); y=int(r.y); w=int(r.w); h=int(r.h)\n",
        "        p = Path(img_dir)/f'{iid}.jpg'\n",
        "        if not p.exists():\n",
        "            alts = list(Path(img_dir).glob(f'{iid}.*'))\n",
        "            if not alts:\n",
        "                continue\n",
        "            p = alts[0]\n",
        "        try:\n",
        "            with Image.open(p) as im:\n",
        "                arr = crop_pad_resize(im, x,y,w,h, pad_frac=0.15, out_size=224)\n",
        "        except Exception:\n",
        "            continue\n",
        "        batch.append(arr); metas.append(u)\n",
        "        if len(batch) == 64 or i == total_target-1:\n",
        "            embs = embed_batch(model, device, np.stack(batch,0), mean, std)\n",
        "            for e,u_ in zip(embs, metas):\n",
        "                if u_ not in feats_list:\n",
        "                    feats_list[u_] = [e.copy()]\n",
        "                else:\n",
        "                    feats_list[u_].append(e)\n",
        "            batch, metas = [], []\n",
        "        if (i+1) % 5000 == 0 or (time.time()-last_log) > 120:\n",
        "            print(f'Embedded {i+1}/{total_target} crops, elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "            last_log = time.time()\n",
        "    unicodes = sorted(feats_list.keys())\n",
        "    # median aggregation per class\n",
        "    protos = []\n",
        "    for u in unicodes:\n",
        "        arr = np.stack(feats_list[u], 0).astype(np.float32)\n",
        "        med = np.median(arr, axis=0)\n",
        "        protos.append(med)\n",
        "    protos = np.stack(protos, 0).astype(np.float32)\n",
        "    # L2 normalize just in case\n",
        "    faiss.normalize_L2(protos)\n",
        "    Path(out/'prototypes.npy').write_bytes(protos.tobytes())\n",
        "    Path(out/'prototypes_unicodes.json').write_text(json.dumps(unicodes, ensure_ascii=False))\n",
        "    print('Saved prototypes:', protos.shape, 'classes:', len(unicodes))\n",
        "    return protos, unicodes, model_name\n",
        "\n",
        "# Evaluate quick top-1 on val GT crops vs prototypes\n",
        "def eval_val_top1(protos:np.ndarray, prot_u:list, model_name='convnext_tiny', img_dir='train_images', max_val_samples:int=20000):\n",
        "    u2idx = {u:i for i,u in enumerate(prot_u)}\n",
        "    # Force CPU to avoid occupying GPU VRAM prior to YOLO detection\n",
        "    model, feat_dim, device, mean, std = build_backbone(model_name, device='cpu')\n",
        "    # gather val rows\n",
        "    split_map = dict(zip(split_df['image_id'], split_df['is_val']))\n",
        "    val_rows = []\n",
        "    for idx, b in boxes_df.iterrows():\n",
        "        iid = b['image_id']\n",
        "        if not split_map.get(iid, False):\n",
        "            continue\n",
        "        if b['unicode'] not in u2idx:\n",
        "            continue\n",
        "        val_rows.append((iid, b['unicode'], int(b['x']), int(b['y']), int(b['w']), int(b['h'])))\n",
        "    if len(val_rows) > max_val_samples:\n",
        "        rng = np.random.RandomState(42)\n",
        "        val_rows = [val_rows[i] for i in rng.choice(len(val_rows), size=max_val_samples, replace=False)]\n",
        "    correct = 0; total = 0\n",
        "    batch, gts = [], []\n",
        "    t0 = time.time()\n",
        "    index = faiss.IndexFlatIP(protos.shape[1])\n",
        "    index.add(protos)\n",
        "    for i, (iid,u,x,y,w,h) in enumerate(val_rows):\n",
        "        p = Path(img_dir)/f'{iid}.jpg'\n",
        "        if not p.exists():\n",
        "            alts = list(Path(img_dir).glob(f'{iid}.*'))\n",
        "            if not alts:\n",
        "                continue\n",
        "            p = alts[0]\n",
        "        try:\n",
        "            with Image.open(p) as im:\n",
        "                arr = crop_pad_resize(im, x,y,w,h, pad_frac=0.15, out_size=224)\n",
        "        except Exception:\n",
        "            continue\n",
        "        batch.append(arr); gts.append(u)\n",
        "        if len(batch) == 128:\n",
        "            embs = embed_batch(model, device, np.stack(batch,0), mean, std)\n",
        "            # cosine via IP on L2-normalized vectors\n",
        "            D,I = index.search(embs, 1)\n",
        "            preds = [prot_u[i0] for i0 in I.flatten().tolist()]\n",
        "            correct += sum(int(a==b) for a,b in zip(preds, gts))\n",
        "            total += len(preds)\n",
        "            batch, gts = [], []\n",
        "        if (i+1) % 5000 == 0:\n",
        "            print(f'Val processed {i+1}/{len(val_rows)}, elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "    if batch:\n",
        "        embs = embed_batch(model, device, np.stack(batch,0), mean, std)\n",
        "        D,I = index.search(embs, 1)\n",
        "        preds = [prot_u[i0] for i0 in I.flatten().tolist()]\n",
        "        correct += sum(int(a==b) for a,b in zip(preds, gts))\n",
        "        total += len(preds)\n",
        "    acc = correct / max(1,total)\n",
        "    print(f'Val top1 accuracy vs prototypes: {acc:.4f} ({correct}/{total})')\n",
        "    return acc\n",
        "\n",
        "print('Two-stage kNN embedding utilities ready.', flush=True)\n",
        "print('Next (after detector finishes or when GPU is free):')\n",
        "print('- Build prototypes: protos, prot_u, model_name = build_prototypes(max_per_class=30)')\n",
        "print('- Eval on holdout: eval_val_top1(protos, prot_u, model_name)')\n",
        "print('Later for submission: detect -> crop -> embed -> nearest prototype -> unicode mapping')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "6258bd8d-1053-4fa0-88e9-a0c1fdb7d086",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json, time, gc, os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import faiss, torch, timm\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "\n",
        "# Minimal duplicate of crop preprocess to be self-contained\n",
        "def crop_pad_resize(img: Image.Image, x:int, y:int, w:int, h:int, pad_frac:float=0.15, out_size:int=224):\n",
        "    W, H = img.size\n",
        "    cx = x + w/2.0; cy = y + h/2.0\n",
        "    pw = max(2, int(round(w * pad_frac))); ph = max(2, int(round(h * pad_frac)))\n",
        "    x1 = max(0, int(round(cx - w/2 - pw))); y1 = max(0, int(round(cy - h/2 - ph)))\n",
        "    x2 = min(W, int(round(cx + w/2 + pw))); y2 = min(H, int(round(cy + h/2 + ph)))\n",
        "    crop = img.crop((x1, y1, x2, y2)).convert('L')\n",
        "    arr = np.array(crop)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    arr = clahe.apply(arr)\n",
        "    h0, w0 = arr.shape[:2]; m = max(h0, w0)\n",
        "    pad_top = (m - h0) // 2; pad_bottom = m - h0 - pad_top\n",
        "    pad_left = (m - w0) // 2; pad_right = m - w0 - pad_left\n",
        "    arr = cv2.copyMakeBorder(arr, pad_top, pad_bottom, pad_left, pad_right, borderType=cv2.BORDER_CONSTANT, value=0)\n",
        "    arr = cv2.resize(arr, (out_size, out_size), interpolation=cv2.INTER_AREA)\n",
        "    arr = np.stack([arr, arr, arr], axis=0).astype(np.float32) / 255.0\n",
        "    return arr\n",
        "\n",
        "def build_backbone(model_name:str='convnext_tiny', device='cpu'):\n",
        "    model = timm.create_model(model_name, pretrained=True, num_classes=0, global_pool='avg')\n",
        "    model.eval().to(device)\n",
        "    # resolve data config for proper normalization\n",
        "    data_cfg = timm.data.resolve_model_data_config(model)\n",
        "    mean = torch.tensor(data_cfg.get('mean', (0.485, 0.456, 0.406)), dtype=torch.float32, device=device).view(1,3,1,1)\n",
        "    std = torch.tensor(data_cfg.get('std', (0.229, 0.224, 0.225)), dtype=torch.float32, device=device).view(1,3,1,1)\n",
        "    with torch.no_grad():\n",
        "        dummy = torch.zeros(1,3,224,224, device=device)\n",
        "        dummy = (dummy - mean) / std\n",
        "        f = model(dummy)\n",
        "    feat_dim = int(f.shape[1])\n",
        "    return model, feat_dim, device, mean, std\n",
        "\n",
        "def embed_batch(model, device, batch_np, mean:torch.Tensor, std:torch.Tensor):\n",
        "    with torch.no_grad():\n",
        "        t = torch.from_numpy(batch_np).to(device)\n",
        "        t = (t - mean) / std\n",
        "        feats = model(t)\n",
        "        feats = torch.nn.functional.normalize(feats, p=2, dim=1)\n",
        "        return feats.detach().cpu().numpy().astype(np.float32)\n",
        "\n",
        "def two_stage_build_submission(det_weights: str,\n",
        "                               prototypes_path: str = 'artifacts/prototypes.npy',\n",
        "                               prot_unicodes_path: str = 'artifacts/prototypes_unicodes.json',\n",
        "                               imgsz_det: int = 768,\n",
        "                               conf: float = 0.08,\n",
        "                               iou: float = 0.65,\n",
        "                               max_det: int = 4000,\n",
        "                               crop_size: int = 224,\n",
        "                               pad_frac: float = 0.15,\n",
        "                               backbone_name: str = 'convnext_tiny',\n",
        "                               min_cosine: float = 0.45,\n",
        "                               det_device: str = 'cpu',\n",
        "                               save_name: str = 'submission.csv',\n",
        "                               predict_half: bool = True,\n",
        "                               dedup_cell: int = 7):\n",
        "    # mitigate fragmentation\n",
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = os.environ.get('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    ss = pd.read_csv('sample_submission.csv')\n",
        "    image_ids = ss['image_id'].tolist()\n",
        "    img_paths = []\n",
        "    for img_id in image_ids:\n",
        "        p = Path('test_images')/f'{img_id}.jpg'\n",
        "        if not p.exists():\n",
        "            alts = list(Path('test_images').glob(f'{img_id}.*'))\n",
        "            if alts:\n",
        "                p = alts[0]\n",
        "        img_paths.append(str(p))\n",
        "\n",
        "    print('Loading detector:', det_weights, flush=True)\n",
        "    det = YOLO(det_weights)\n",
        "    print('Loading prototypes from', prototypes_path, flush=True)\n",
        "    protos = np.fromfile(prototypes_path, dtype=np.float32)\n",
        "    unicodes = json.loads(Path(prot_unicodes_path).read_text())\n",
        "    n_cls = len(unicodes)\n",
        "    feat_dim = protos.size // max(1, n_cls)\n",
        "    protos = protos.reshape(n_cls, feat_dim)\n",
        "    faiss.normalize_L2(protos)\n",
        "    index = faiss.IndexFlatIP(feat_dim)\n",
        "    index.add(protos)\n",
        "    u_list = unicodes\n",
        "\n",
        "    print('Loading backbone on CPU to avoid GPU OOM:', backbone_name, flush=True)\n",
        "    model, feat_dim_backbone, device, mean, std = build_backbone(backbone_name, device='cpu')\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print('Running detection stream on', len(img_paths), 'images ...', flush=True)\n",
        "    results = det.predict(source=img_paths, imgsz=imgsz_det, conf=conf, iou=iou, max_det=max_det, augment=True, device=det_device, stream=True, verbose=False, batch=1, half=predict_half)\n",
        "\n",
        "    rows = []\n",
        "    t0 = time.time()\n",
        "    for i, (img_id, img_path, res) in enumerate(zip(image_ids, img_paths, results)):\n",
        "        if i % 25 == 0:\n",
        "            print(f'Processed {i}/{len(image_ids)} images, elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "        labels_out = []\n",
        "        with Image.open(img_path) as im:\n",
        "            W, H = im.size\n",
        "            if res and hasattr(res, 'boxes') and res.boxes is not None and len(res.boxes) > 0:\n",
        "                b = res.boxes\n",
        "                xyxy = b.xyxy.cpu().numpy()\n",
        "                batch = []\n",
        "                centers = []\n",
        "                for (x1,y1,x2,y2) in xyxy:\n",
        "                    x = int(round(x1)); y = int(round(y1)); w = int(round(x2-x1)); h = int(round(y2-y1))\n",
        "                    # precision filter: skip tiny boxes\n",
        "                    if w < 5 or h < 5:\n",
        "                        continue\n",
        "                    arr = crop_pad_resize(im, x,y,w,h, pad_frac=pad_frac, out_size=crop_size)\n",
        "                    batch.append(arr)\n",
        "                    cx = int(round((x1+x2)/2.0)); cy = int(round((y1+y2)/2.0))\n",
        "                    cx = max(0, min(cx, W-1)); cy = max(0, min(cy, H-1))\n",
        "                    centers.append((cx, cy))\n",
        "                if batch:\n",
        "                    embs = embed_batch(model, device, np.stack(batch,0), mean, std)\n",
        "                    D,I = index.search(embs, 1)\n",
        "                    sims = D.flatten().tolist()\n",
        "                    idxs = I.flatten().tolist()\n",
        "                    # collect predictions\n",
        "                    pred_list = []  # (u, (cx,cy), sim)\n",
        "                    for (cx,cy), j, sim in zip(centers, idxs, sims):\n",
        "                        if sim >= min_cosine:\n",
        "                            u = u_list[j] if 0 <= j < len(u_list) else None\n",
        "                            if u:\n",
        "                                pred_list.append((u, (cx,cy), float(sim)))\n",
        "                    # deduplicate within small center cells per unicode\n",
        "                    if pred_list:\n",
        "                        cells = {}  # key: (u, cx_cell, cy_cell) -> (u,(cx,cy),sim)\n",
        "                        for u, (cx,cy), sim in pred_list:\n",
        "                            key = (u, cx//dedup_cell, cy//dedup_cell)\n",
        "                            if key not in cells or sim > cells[key][2]:\n",
        "                                cells[key] = (u, (cx,cy), sim)\n",
        "                        final_preds = list(cells.values())\n",
        "                        for u, (cx,cy), sim in final_preds:\n",
        "                            labels_out.extend([u, str(cx), str(cy)])\n",
        "        rows.append({'image_id': img_id, 'labels': ' '.join(labels_out)})\n",
        "        del res\n",
        "\n",
        "    sub = pd.DataFrame(rows)\n",
        "    sub.to_csv(save_name, index=False)\n",
        "    print('Saved', save_name, 'shape', sub.shape)\n",
        "    return sub\n",
        "\n",
        "print('Two-stage submission function ready: two_stage_build_submission(det_weights, ...)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "a5d9bd17-18e9-408d-9acf-109ebaef22d0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "import numpy as np, json, pandas as pd\n",
        "\n",
        "# Driver: rebuild prototypes with normalization, then two-stage submission (skip eval for speed)\n",
        "print('Rebuilding prototypes with normalization (max_per_class=30)...', flush=True)\n",
        "protos, prot_u, model_name = build_prototypes(max_per_class=30, img_dir='train_images', out_dir='artifacts', model_name='convnext_tiny')\n",
        "\n",
        "# Skip eval_val_top1 to keep GPU free and save time\n",
        "# acc = eval_val_top1(protos, prot_u, model_name=model_name, img_dir='train_images', max_val_samples=10000)\n",
        "# print('Val top1:', acc)\n",
        "\n",
        "det_weights = 'runs/yolo8n_kuz_single/weights/best.pt'\n",
        "print('Generating two-stage submission using detector:', det_weights, flush=True)\n",
        "two_stage_build_submission(det_weights=det_weights,\n",
        "                           prototypes_path='artifacts/prototypes.npy',\n",
        "                           prot_unicodes_path='artifacts/prototypes_unicodes.json',\n",
        "                           imgsz_det=832,\n",
        "                           conf=0.15,\n",
        "                           iou=0.65,\n",
        "                           max_det=2000,\n",
        "                           crop_size=224,\n",
        "                           pad_frac=0.15,\n",
        "                           backbone_name='convnext_tiny',\n",
        "                           min_cosine=0.45,\n",
        "                           det_device=0,\n",
        "                           save_name='submission.csv')\n",
        "print('Done. Check submission.csv head:')\n",
        "print(pd.read_csv('submission.csv').head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "fd1cbd1f-fc24-44cf-8d06-f593aee59899",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, json, time\n",
        "from pathlib import Path\n",
        "import faiss, torch, timm\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "# Sweep min_cosine on validation crops to choose threshold (F1 proxy)\n",
        "@torch.no_grad()\n",
        "def _build_backbone_cpu(model_name='convnext_tiny'):\n",
        "    model = timm.create_model(model_name, pretrained=True, num_classes=0, global_pool='avg')\n",
        "    model.eval().to('cpu')\n",
        "    data_cfg = timm.data.resolve_model_data_config(model)\n",
        "    mean = torch.tensor(data_cfg.get('mean', (0.485,0.456,0.406)), dtype=torch.float32, device='cpu').view(1,3,1,1)\n",
        "    std  = torch.tensor(data_cfg.get('std',  (0.229,0.224,0.225)), dtype=torch.float32, device='cpu').view(1,3,1,1)\n",
        "    return model, mean, std\n",
        "\n",
        "@torch.no_grad()\n",
        "def _embed_batch_cpu(model, batch_np, mean, std):\n",
        "    t = torch.from_numpy(batch_np).to('cpu')\n",
        "    t = (t - mean) / std\n",
        "    f = model(t)\n",
        "    f = torch.nn.functional.normalize(f, p=2, dim=1)\n",
        "    return f.cpu().numpy().astype(np.float32)\n",
        "\n",
        "def sweep_min_cosine(protos_path='artifacts/prototypes.npy',\n",
        "                     prot_unicodes_path='artifacts/prototypes_unicodes.json',\n",
        "                     model_name='convnext_tiny',\n",
        "                     thresholds=np.arange(0.35, 0.71, 0.05),\n",
        "                     max_val_samples=20000,\n",
        "                     img_dir='train_images'):\n",
        "    protos = np.fromfile(protos_path, dtype=np.float32)\n",
        "    unicodes = json.loads(Path(prot_unicodes_path).read_text())\n",
        "    n_cls = len(unicodes)\n",
        "    d = protos.size // max(1, n_cls)\n",
        "    protos = protos.reshape(n_cls, d).astype(np.float32)\n",
        "    faiss.normalize_L2(protos)\n",
        "    index = faiss.IndexFlatIP(d); index.add(protos)\n",
        "    u2idx = {u:i for i,u in enumerate(unicodes)}\n",
        "    split_map = dict(zip(split_df['image_id'], split_df['is_val']))\n",
        "    rows = [(b.image_id, b.unicode, int(b.x), int(b.y), int(b.w), int(b.h))\n",
        "            for _, b in boxes_df.iterrows()\n",
        "            if split_map.get(b.image_id, False) and (b.unicode in u2idx)]\n",
        "    if len(rows) > max_val_samples:\n",
        "        rng = np.random.RandomState(42)\n",
        "        rows = [rows[i] for i in rng.choice(len(rows), size=max_val_samples, replace=False)]\n",
        "    model, mean, std = _build_backbone_cpu(model_name)\n",
        "    sims = []; corrects = []\n",
        "    batch = []; gts = []\n",
        "    t0 = time.time()\n",
        "    for i, (iid,u,x,y,w,h) in enumerate(rows):\n",
        "        p = Path(img_dir)/f'{iid}.jpg'\n",
        "        if not p.exists():\n",
        "            alts = list(Path(img_dir).glob(f'{iid}.*'))\n",
        "            if not alts:\n",
        "                continue\n",
        "            p = alts[0]\n",
        "        with Image.open(p) as im:\n",
        "            arr = crop_pad_resize(im, x,y,w,h, pad_frac=0.15, out_size=224)\n",
        "        batch.append(arr); gts.append(u)\n",
        "        if len(batch) == 128 or i == len(rows)-1:\n",
        "            embs = _embed_batch_cpu(model, np.stack(batch,0), mean, std)\n",
        "            D,I = index.search(embs, 1)\n",
        "            for d_, i0, u_gt in zip(D.flatten(), I.flatten(), gts):\n",
        "                sims.append(float(d_)); corrects.append(1 if unicodes[i0]==u_gt else 0)\n",
        "            batch = []; gts = []\n",
        "        if (i+1) % 5000 == 0:\n",
        "            print(f'Sweep gather {i+1}/{len(rows)} elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "    sims = np.array(sims); corrects = np.array(corrects)\n",
        "    best_t, best_f1 = 0.50, -1.0\n",
        "    for t in thresholds:\n",
        "        m = sims >= t\n",
        "        if m.sum() == 0:\n",
        "            print(f't={t:.2f} no positives'); continue\n",
        "        prec = corrects[m].mean()\n",
        "        rec = m.mean()\n",
        "        f1 = 2*prec*rec/(prec+rec+1e-9)\n",
        "        print(f't={t:.2f} prec={prec:.4f} rec={rec:.4f} F1p={f1:.4f}')\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_t = f1, float(t)\n",
        "    print('Best min_cosine:', best_t, 'F1proxy:', round(best_f1,4))\n",
        "    return best_t\n",
        "\n",
        "# 1) Sweep for best min_cosine (\u224820\u201330 min on 20k crops); if short on time, comment and set 0.50\n",
        "try:\n",
        "    best_t = sweep_min_cosine()\n",
        "except Exception as e:\n",
        "    print('Sweep failed, defaulting min_cosine=0.50. Error:', e)\n",
        "    best_t = 0.50\n",
        "\n",
        "# 2) Re-run submission with improved detector params\n",
        "print('Building improved submission with imgsz=1024, conf=0.12, min_cosine=', best_t)\n",
        "try:\n",
        "    two_stage_build_submission(det_weights='runs/yolo8n_kuz_single/weights/best.pt',\n",
        "                               prototypes_path='artifacts/prototypes.npy',\n",
        "                               prot_unicodes_path='artifacts/prototypes_unicodes.json',\n",
        "                               imgsz_det=1024,\n",
        "                               conf=0.12,\n",
        "                               iou=0.65,\n",
        "                               max_det=2000,\n",
        "                               crop_size=224,\n",
        "                               pad_frac=0.15,\n",
        "                               backbone_name='convnext_tiny',\n",
        "                               min_cosine=best_t,\n",
        "                               det_device=0,\n",
        "                               save_name='submission_v2.csv')\n",
        "except RuntimeError as e:\n",
        "    # Fallback to 960 if OOM on 1024\n",
        "    print('Got error (likely OOM) at 1024. Falling back to imgsz=960. Error:', e)\n",
        "    two_stage_build_submission(det_weights='runs/yolo8n_kuz_single/weights/best.pt',\n",
        "                               prototypes_path='artifacts/prototypes.npy',\n",
        "                               prot_unicodes_path='artifacts/prototypes_unicodes.json',\n",
        "                               imgsz_det=960,\n",
        "                               conf=0.12,\n",
        "                               iou=0.65,\n",
        "                               max_det=2000,\n",
        "                               crop_size=224,\n",
        "                               pad_frac=0.15,\n",
        "                               backbone_name='convnext_tiny',\n",
        "                               min_cosine=best_t,\n",
        "                               det_device=0,\n",
        "                               save_name='submission_v2.csv')\n",
        "\n",
        "print('submission_v2.csv head:')\n",
        "print(pd.read_csv('submission_v2.csv').head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "172115cc-6f65-4ff4-89b2-62762bce17e3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, time, json, pandas as pd\n",
        "import torch\n",
        "\n",
        "def try_build_submission_chain(sizes=(1024, 960, 896, 864, 832, 768),\n",
        "                               conf=0.12,\n",
        "                               min_cosine=0.7,\n",
        "                               save_name='submission_v2.csv'):\n",
        "    det_weights='runs/yolo8n_kuz_single/weights/best.pt'\n",
        "    print('Starting fallback chain with sizes:', sizes, 'conf=', conf, 'min_cosine=', min_cosine, flush=True)\n",
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = os.environ.get('PYTORCH_CUDA_ALLOC_CONF','expandable_segments:True')\n",
        "    for s in sizes:\n",
        "        try:\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            print(f'Attempting imgsz={s} ...', flush=True)\n",
        "            sub = two_stage_build_submission(det_weights=det_weights,\n",
        "                                            prototypes_path='artifacts/prototypes.npy',\n",
        "                                            prot_unicodes_path='artifacts/prototypes_unicodes.json',\n",
        "                                            imgsz_det=int(s),\n",
        "                                            conf=float(conf),\n",
        "                                            iou=0.65,\n",
        "                                            max_det=2000,\n",
        "                                            crop_size=224,\n",
        "                                            pad_frac=0.15,\n",
        "                                            backbone_name='convnext_tiny',\n",
        "                                            min_cosine=float(min_cosine),\n",
        "                                            det_device=0,\n",
        "                                            save_name=save_name,\n",
        "                                            predict_half=True)\n",
        "            print('Success at imgsz', s, '->', save_name, flush=True)\n",
        "            print(pd.read_csv(save_name).head())\n",
        "            return s\n",
        "        except Exception as e:\n",
        "            print(f'Failed at imgsz={s}: {e}', flush=True)\n",
        "            if torch.cuda.is_available():\n",
        "                try:\n",
        "                    print('CUDA mem allocated/reserved (GB):',\n",
        "                          round(torch.cuda.memory_allocated()/1e9,3),\n",
        "                          round(torch.cuda.memory_reserved()/1e9,3))\n",
        "                except Exception:\n",
        "                    pass\n",
        "            time.sleep(1)\n",
        "    raise RuntimeError('All sizes failed in fallback chain')\n",
        "\n",
        "# Use best threshold from sweep (0.7) and run chain\n",
        "best_t = 0.7\n",
        "chosen_size = try_build_submission_chain(sizes=(1024, 960, 896, 864, 832, 768), conf=0.12, min_cosine=best_t, save_name='submission_v2.csv')\n",
        "print('Chosen imgsz_det:', chosen_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "14c9ee28-0246-4795-8e58-b2d0b2709d66",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import shutil, pandas as pd, os\n",
        "src = 'submission_v2.csv'\n",
        "dst = 'submission.csv'\n",
        "assert os.path.exists(src), f\"{src} not found\"\n",
        "shutil.copy2(src, dst)\n",
        "print('Copied', src, '->', dst)\n",
        "print(pd.read_csv(dst).head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "5551d71a-f5b5-4977-84b8-efa470b9eae3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import shutil, os, pandas as pd, torch\n",
        "print('Rebuilding median prototypes with max_per_class=50 using convnext_small ...', flush=True)\n",
        "protos, prot_u, model_name = build_prototypes(max_per_class=50, img_dir='train_images', out_dir='artifacts', model_name='convnext_small')\n",
        "print('Prototypes ready:', protos.shape, 'classes:', len(prot_u))\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "print('Running two-stage with augment=True, max_det=3000, dedup, imgsz=1024, conf=0.10, min_cosine=0.7, backbone=convnext_small ...', flush=True)\n",
        "two_stage_build_submission(det_weights='runs/yolo8n_kuz_single/weights/best.pt',\n",
        "                           prototypes_path='artifacts/prototypes.npy',\n",
        "                           prot_unicodes_path='artifacts/prototypes_unicodes.json',\n",
        "                           imgsz_det=1024,\n",
        "                           conf=0.10,\n",
        "                           iou=0.65,\n",
        "                           max_det=3000,\n",
        "                           crop_size=224,\n",
        "                           pad_frac=0.15,\n",
        "                           backbone_name='convnext_small',\n",
        "                           min_cosine=0.7,\n",
        "                           det_device=0,\n",
        "                           save_name='submission_v4.csv',\n",
        "                           predict_half=True,\n",
        "                           dedup_cell=5)\n",
        "print('Copying submission_v4.csv -> submission.csv for grading ...', flush=True)\n",
        "shutil.copy2('submission_v4.csv', 'submission.csv')\n",
        "print(pd.read_csv('submission.csv').head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "0d8a0f60-2450-4a01-8d0f-0156de8a06fb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, json, time, pandas as pd, torch\n",
        "from pathlib import Path\n",
        "import faiss\n",
        "from PIL import Image\n",
        "\n",
        "# Quick sweep for min_cosine using convnext_small prototypes\n",
        "@torch.no_grad()\n",
        "def _build_backbone_cpu(model_name='convnext_small'):\n",
        "    import timm, torch\n",
        "    model = timm.create_model(model_name, pretrained=True, num_classes=0, global_pool='avg')\n",
        "    model.eval().to('cpu')\n",
        "    data_cfg = timm.data.resolve_model_data_config(model)\n",
        "    mean = torch.tensor(data_cfg.get('mean', (0.485,0.456,0.406)), dtype=torch.float32, device='cpu').view(1,3,1,1)\n",
        "    std  = torch.tensor(data_cfg.get('std',  (0.229,0.224,0.225)), dtype=torch.float32, device='cpu').view(1,3,1,1)\n",
        "    return model, mean, std\n",
        "\n",
        "@torch.no_grad()\n",
        "def _embed_batch_cpu(model, batch_np, mean, std):\n",
        "    import torch\n",
        "    t = torch.from_numpy(batch_np).to('cpu')\n",
        "    t = (t - mean) / std\n",
        "    f = model(t)\n",
        "    f = torch.nn.functional.normalize(f, p=2, dim=1)\n",
        "    return f.cpu().numpy().astype(np.float32)\n",
        "\n",
        "def sweep_min_cosine_small(protos_path='artifacts/prototypes.npy',\n",
        "                           prot_unicodes_path='artifacts/prototypes_unicodes.json',\n",
        "                           thresholds=np.arange(0.40, 0.76, 0.05),\n",
        "                           max_val_samples=20000,\n",
        "                           img_dir='train_images'):\n",
        "    protos = np.fromfile(protos_path, dtype=np.float32)\n",
        "    unicodes = json.loads(Path(prot_unicodes_path).read_text())\n",
        "    n_cls = len(unicodes)\n",
        "    d = protos.size // max(1, n_cls)\n",
        "    protos = protos.reshape(n_cls, d).astype(np.float32)\n",
        "    faiss.normalize_L2(protos)\n",
        "    index = faiss.IndexFlatIP(d); index.add(protos)\n",
        "    u2idx = {u:i for i,u in enumerate(unicodes)}\n",
        "    split_map = dict(zip(split_df['image_id'], split_df['is_val']))\n",
        "    rows = [(b.image_id, b.unicode, int(b.x), int(b.y), int(b.w), int(b.h))\n",
        "            for _, b in boxes_df.iterrows()\n",
        "            if split_map.get(b.image_id, False) and (b.unicode in u2idx)]\n",
        "    if len(rows) > max_val_samples:\n",
        "        rng = np.random.RandomState(42)\n",
        "        rows = [rows[i] for i in rng.choice(len(rows), size=max_val_samples, replace=False)]\n",
        "    model, mean, std = _build_backbone_cpu('convnext_small')\n",
        "    sims = []; corrects = []\n",
        "    batch = []; gts = []\n",
        "    t0 = time.time()\n",
        "    for i, (iid,u,x,y,w,h) in enumerate(rows):\n",
        "        p = Path(img_dir)/f'{iid}.jpg'\n",
        "        if not p.exists():\n",
        "            alts = list(Path(img_dir).glob(f'{iid}.*'))\n",
        "            if not alts:\n",
        "                continue\n",
        "            p = alts[0]\n",
        "        with Image.open(p) as im:\n",
        "            arr = crop_pad_resize(im, x,y,w,h, pad_frac=0.15, out_size=224)\n",
        "        batch.append(arr); gts.append(u)\n",
        "        if len(batch) == 128 or i == len(rows)-1:\n",
        "            embs = _embed_batch_cpu(model, np.stack(batch,0), mean, std)\n",
        "            D,I = index.search(embs, 1)\n",
        "            for d_, i0, u_gt in zip(D.flatten(), I.flatten(), gts):\n",
        "                sims.append(float(d_)); corrects.append(1 if unicodes[i0]==u_gt else 0)\n",
        "            batch = []; gts = []\n",
        "        if (i+1) % 5000 == 0:\n",
        "            print(f'Sweep gather {i+1}/{len(rows)} elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "    sims = np.array(sims); corrects = np.array(corrects)\n",
        "    best_t, best_f1 = 0.60, -1.0\n",
        "    for t in thresholds:\n",
        "        m = sims >= t\n",
        "        if m.sum() == 0:\n",
        "            print(f't={t:.2f} no positives'); continue\n",
        "        prec = corrects[m].mean()\n",
        "        rec = m.mean()\n",
        "        f1 = 2*prec*rec/(prec+rec+1e-9)\n",
        "        print(f't={t:.2f} prec={prec:.4f} rec={rec:.4f} F1p={f1:.4f}')\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_t = f1, float(t)\n",
        "    print('Best min_cosine:', best_t, 'F1proxy:', round(best_f1,4))\n",
        "    return best_t\n",
        "\n",
        "print('Sweeping min_cosine for convnext_small prototypes ...', flush=True)\n",
        "try:\n",
        "    best_t = sweep_min_cosine_small()\n",
        "except Exception as e:\n",
        "    print('Sweep failed; defaulting to 0.60. Error:', e); best_t = 0.60\n",
        "\n",
        "print('Building v5 submission with conf=0.08, iou=0.65, max_det=4000, dedup_cell=7, min_cosine=', best_t, flush=True)\n",
        "sub = two_stage_build_submission(det_weights='runs/yolo8n_kuz_single/weights/best.pt',\n",
        "                                 prototypes_path='artifacts/prototypes.npy',\n",
        "                                 prot_unicodes_path='artifacts/prototypes_unicodes.json',\n",
        "                                 imgsz_det=1024,\n",
        "                                 conf=0.08,\n",
        "                                 iou=0.65,\n",
        "                                 max_det=4000,\n",
        "                                 crop_size=224,\n",
        "                                 pad_frac=0.15,\n",
        "                                 backbone_name='convnext_small',\n",
        "                                 min_cosine=best_t,\n",
        "                                 det_device=0,\n",
        "                                 save_name='submission_v5.csv',\n",
        "                                 predict_half=True,\n",
        "                                 dedup_cell=7)\n",
        "import shutil\n",
        "shutil.copy2('submission_v5.csv', 'submission.csv')\n",
        "print(pd.read_csv('submission.csv').head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "cf806e6a-1225-4637-8431-07d8bbe24645",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# v6: Exemplar kNN voting + optional two-backbone fusion + adaptive dedup\n",
        "import os, json, time, gc, math\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss, torch, timm, cv2\n",
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "\n",
        "def _crop_pad_resize(img: Image.Image, x:int, y:int, w:int, h:int, pad_frac:float=0.15, out_size:int=224):\n",
        "    W, H = img.size\n",
        "    cx = x + w/2.0; cy = y + h/2.0\n",
        "    pw = max(2, int(round(w * pad_frac))); ph = max(2, int(round(h * pad_frac)))\n",
        "    x1 = max(0, int(round(cx - w/2 - pw))); y1 = max(0, int(round(cy - h/2 - ph)))\n",
        "    x2 = min(W, int(round(cx + w/2 + pw))); y2 = min(H, int(round(cy + h/2 + ph)))\n",
        "    crop = img.crop((x1, y1, x2, y2)).convert('L')\n",
        "    arr = np.array(crop)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    arr = clahe.apply(arr)\n",
        "    h0, w0 = arr.shape[:2]; m = max(h0, w0)\n",
        "    pad_top = (m - h0) // 2; pad_bottom = m - h0 - pad_top\n",
        "    pad_left = (m - w0) // 2; pad_right = m - w0 - pad_left\n",
        "    arr = cv2.copyMakeBorder(arr, pad_top, pad_bottom, pad_left, pad_right, borderType=cv2.BORDER_CONSTANT, value=0)\n",
        "    arr = cv2.resize(arr, (out_size, out_size), interpolation=cv2.INTER_AREA)\n",
        "    arr = np.stack([arr, arr, arr], axis=0).astype(np.float32) / 255.0\n",
        "    return arr\n",
        "\n",
        "@torch.no_grad()\n",
        "def _build_backbone_cpu(model_name:str):\n",
        "    model = timm.create_model(model_name, pretrained=True, num_classes=0, global_pool='avg')\n",
        "    model.eval().to('cpu')\n",
        "    data_cfg = timm.data.resolve_model_data_config(model)\n",
        "    mean = torch.tensor(data_cfg.get('mean', (0.485,0.456,0.406)), dtype=torch.float32, device='cpu').view(1,3,1,1)\n",
        "    std  = torch.tensor(data_cfg.get('std',  (0.229,0.224,0.225)), dtype=torch.float32, device='cpu').view(1,3,1,1)\n",
        "    # warmup\n",
        "    _ = model((torch.zeros(1,3,224,224)-mean)/std)\n",
        "    return model, mean, std\n",
        "\n",
        "@torch.no_grad()\n",
        "def _embed_batch_cpu(model, batch_np, mean, std):\n",
        "    t = torch.from_numpy(batch_np).to('cpu')\n",
        "    t = (t - mean) / std\n",
        "    f = model(t)\n",
        "    f = torch.nn.functional.normalize(f, p=2, dim=1)\n",
        "    return f.cpu().numpy().astype(np.float32)\n",
        "\n",
        "def build_exemplar_bank(max_per_class:int=3,\n",
        "                        img_dir:str='train_images',\n",
        "                        out_dir:str='artifacts',\n",
        "                        model_name:str='convnext_small',\n",
        "                        pad_frac:float=0.15,\n",
        "                        out_size:int=224):\n",
        "    out = Path(out_dir); out.mkdir(parents=True, exist_ok=True)\n",
        "    split_map = dict(zip(split_df['image_id'], split_df['is_val']))\n",
        "    df = boxes_df.copy()\n",
        "    df = df[~df['image_id'].map(split_map).fillna(False)]  # train-only\n",
        "    # cap exemplars per unicode\n",
        "    rng = np.random.RandomState(42)\n",
        "    def _sample_grp(g):\n",
        "        if len(g) <= max_per_class: return g\n",
        "        idx = rng.choice(len(g), size=max_per_class, replace=False)\n",
        "        return g.iloc[idx]\n",
        "    df_s = df.groupby('unicode', as_index=False, group_keys=False).apply(_sample_grp)\n",
        "    df_s = df_s.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "    print(f'Exemplar sampling: classes={df_s.unicode.nunique()}, exemplars={len(df_s)} (<= {max_per_class}/class)', flush=True)\n",
        "    model, mean, std = _build_backbone_cpu(model_name)\n",
        "    batch, metas = [], []\n",
        "    embs = []\n",
        "    t0 = time.time(); last = t0\n",
        "    for i, r in enumerate(df_s.itertuples(index=False)):\n",
        "        iid = r.image_id; u = r.unicode; x=int(r.x); y=int(r.y); w=int(r.w); h=int(r.h)\n",
        "        p = Path(img_dir)/f'{iid}.jpg'\n",
        "        if not p.exists():\n",
        "            alts = list(Path(img_dir).glob(f'{iid}.*'))\n",
        "            if not alts: continue\n",
        "            p = alts[0]\n",
        "        try:\n",
        "            with Image.open(p) as im:\n",
        "                arr = _crop_pad_resize(im, x,y,w,h, pad_frac=pad_frac, out_size=out_size)\n",
        "        except Exception:\n",
        "            continue\n",
        "        batch.append(arr); metas.append(u)\n",
        "        if len(batch)==128 or i==len(df_s)-1:\n",
        "            e = _embed_batch_cpu(model, np.stack(batch,0), mean, std)\n",
        "            embs.append(e); batch=[]\n",
        "        if (i+1)%5000==0 or (time.time()-last)>120:\n",
        "            print(f'Exemplar embed {i+1}/{len(df_s)} elapsed {time.time()-t0:.1f}s', flush=True); last=time.time()\n",
        "    if batch:\n",
        "        e = _embed_batch_cpu(model, np.stack(batch,0), mean, std)\n",
        "        embs.append(e)\n",
        "    embs = np.concatenate(embs, 0).astype(np.float32)\n",
        "    faiss.normalize_L2(embs)\n",
        "    # Save bank\n",
        "    (out/'exemplars.npy').write_bytes(embs.tobytes())\n",
        "    (out/'exemplars_unicodes.json').write_text(json.dumps(metas, ensure_ascii=False))\n",
        "    (out/'exemplars_backbone.txt').write_text(model_name)\n",
        "    print('Saved exemplars:', embs.shape, 'backbone:', model_name, flush=True)\n",
        "    return embs, metas, model_name\n",
        "\n",
        "def _search_k_per_unicode(index: faiss.IndexFlatIP,\n",
        "                          queries: np.ndarray,\n",
        "                          exemplar_unicodes: List[str],\n",
        "                          k:int=5) -> List[Dict[str, float]]:\n",
        "    # Returns for each query: dict unicode -> summed cosine over top-k neighbors of that unicode\n",
        "    D, I = index.search(queries, k)\n",
        "    out = []\n",
        "    for drow, irow in zip(D, I):\n",
        "        acc: Dict[str, float] = {}\n",
        "        for sim, idx in zip(drow.tolist(), irow.tolist()):\n",
        "            if idx == -1: continue\n",
        "            u = exemplar_unicodes[idx]\n",
        "            acc[u] = acc.get(u, 0.0) + float(sim)\n",
        "        out.append(acc)\n",
        "    return out\n",
        "\n",
        "def two_stage_build_submission_exemplars(det_weights: str,\n",
        "                                         imgsz_det:int=1024,\n",
        "                                         conf:float=0.08,\n",
        "                                         iou:float=0.65,\n",
        "                                         max_det:int=4000,\n",
        "                                         pad_frac:float=0.15,\n",
        "                                         crop_size:int=224,\n",
        "                                         backbone_primary:str='convnext_small',\n",
        "                                         min_cosine:float=0.65,\n",
        "                                         k_vote:int=5,\n",
        "                                         save_name:str='submission_v6.csv',\n",
        "                                         predict_half:bool=True,\n",
        "                                         det_device=0,\n",
        "                                         tiny_filter:int=5,\n",
        "                                         exemplars_path:str='artifacts/exemplars.npy',\n",
        "                                         exemplars_unicodes_path:str='artifacts/exemplars_unicodes.json',\n",
        "                                         # Optional second backbone fusion\n",
        "                                         exemplars2_path: Optional[str]=None,\n",
        "                                         exemplars2_unicodes_path: Optional[str]=None,\n",
        "                                         backbone_secondary: Optional[str]=None):\n",
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = os.environ.get('PYTORCH_CUDA_ALLOC_CONF','expandable_segments:True')\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache(); gc.collect()\n",
        "    ss = pd.read_csv('sample_submission.csv')\n",
        "    image_ids = ss['image_id'].tolist()\n",
        "    img_paths = []\n",
        "    for img_id in image_ids:\n",
        "        p = Path('test_images')/f'{img_id}.jpg'\n",
        "        if not p.exists():\n",
        "            alts = list(Path('test_images').glob(f'{img_id}.*'))\n",
        "            if alts: p = alts[0]\n",
        "        img_paths.append(str(p))\n",
        "    print('Loading detector:', det_weights, flush=True)\n",
        "    det = YOLO(det_weights)\n",
        "    # Load exemplar bank(s)\n",
        "    ex1 = np.fromfile(exemplars_path, dtype=np.float32)\n",
        "    metas1: List[str] = json.loads(Path(exemplars_unicodes_path).read_text())\n",
        "    d1 = ex1.size // max(1,len(metas1)); ex1 = ex1.reshape(-1, d1).astype(np.float32)\n",
        "    faiss.normalize_L2(ex1)\n",
        "    idx1 = faiss.IndexFlatIP(d1); idx1.add(ex1)\n",
        "    print('Exemplar bank1:', ex1.shape, 'k=', k_vote, flush=True)\n",
        "    use_fusion = False\n",
        "    if exemplars2_path and exemplars2_unicodes_path and backbone_secondary:\n",
        "        ex2 = np.fromfile(exemplars2_path, dtype=np.float32)\n",
        "        metas2: List[str] = json.loads(Path(exemplars2_unicodes_path).read_text())\n",
        "        d2 = ex2.size // max(1,len(metas2)); ex2 = ex2.reshape(-1, d2).astype(np.float32)\n",
        "        faiss.normalize_L2(ex2)\n",
        "        idx2 = faiss.IndexFlatIP(d2); idx2.add(ex2)\n",
        "        print('Exemplar bank2:', ex2.shape, 'k=', k_vote, flush=True)\n",
        "        use_fusion = True\n",
        "    # Load backbone(s) on CPU\n",
        "    model1, mean1, std1 = _build_backbone_cpu(backbone_primary)\n",
        "    if use_fusion:\n",
        "        model2, mean2, std2 = _build_backbone_cpu(backbone_secondary)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    print('Running detection stream on', len(img_paths), 'images ...', flush=True)\n",
        "    results = det.predict(source=img_paths, imgsz=imgsz_det, conf=conf, iou=iou, max_det=max_det, augment=True, device=det_device, stream=True, verbose=False, batch=1, half=predict_half)\n",
        "    rows = []; t0 = time.time()\n",
        "    for i, (img_id, img_path, res) in enumerate(zip(image_ids, img_paths, results)):\n",
        "        if i % 25 == 0:\n",
        "            print(f'Processed {i}/{len(image_ids)} images, elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "        labels_out = []\n",
        "        with Image.open(img_path) as im:\n",
        "            W, H = im.size\n",
        "            if res and hasattr(res,'boxes') and res.boxes is not None and len(res.boxes)>0:\n",
        "                b = res.boxes\n",
        "                xyxy = b.xyxy.cpu().numpy()\n",
        "                batch1 = []; centers = []; ws = []; hs = []\n",
        "                for (x1,y1,x2,y2) in xyxy:\n",
        "                    x = int(round(x1)); y = int(round(y1)); w = int(round(x2-x1)); h = int(round(y2-y1))\n",
        "                    if w < tiny_filter or h < tiny_filter:\n",
        "                        continue\n",
        "                    arr = _crop_pad_resize(im, x,y,w,h, pad_frac=pad_frac, out_size=crop_size)\n",
        "                    batch1.append(arr);\n",
        "                    cx = int(round((x1+x2)/2.0)); cy = int(round((y1+y2)/2.0))\n",
        "                    cx = max(0, min(cx, W-1)); cy = max(0, min(cy, H-1))\n",
        "                    centers.append((cx,cy)); ws.append(w); hs.append(h)\n",
        "                if batch1:\n",
        "                    q1 = _embed_batch_cpu(model1, np.stack(batch1,0), mean1, std1)\n",
        "                    # vote on bank1\n",
        "                    votes1 = _search_k_per_unicode(idx1, q1, metas1, k=k_vote)\n",
        "                    if use_fusion:\n",
        "                        q2 = _embed_batch_cpu(model2, np.stack(batch1,0), mean2, std2)\n",
        "                        votes2 = _search_k_per_unicode(idx2, q2, metas2, k=k_vote)\n",
        "                    # finalize per-crop prediction\n",
        "                    preds = []  # (u, cx, cy, score, w, h)\n",
        "                    for j in range(len(centers)):\n",
        "                        # primary\n",
        "                        v1 = votes1[j]\n",
        "                        best_u = None; best_score = -1.0\n",
        "                        # candidate set:\n",
        "                        cand_us = set(v1.keys())\n",
        "                        if use_fusion:\n",
        "                            v2 = votes2[j]\n",
        "                            cand_us |= set(v2.keys())\n",
        "                        for u in cand_us:\n",
        "                            s1 = v1.get(u, 0.0)\n",
        "                            if use_fusion:\n",
        "                                s2 = v2.get(u, 0.0)\n",
        "                                s = max(s1, s2)  # fusion by max cosine per unicode\n",
        "                            else:\n",
        "                                s = s1\n",
        "                            if s > best_score:\n",
        "                                best_score = s; best_u = u\n",
        "                        # gate by strongest single neighbor cosine approximation:\n",
        "                        # We approximate by requiring best_score >= min_cosine (since votes sum top-k cosines).\n",
        "                        if best_u is not None and best_score >= float(min_cosine):\n",
        "                            (cx,cy) = centers[j]\n",
        "                            preds.append((best_u, cx, cy, float(best_score), ws[j], hs[j]))\n",
        "                    # adaptive dedup by unicode and size-aware grid\n",
        "                    if preds:\n",
        "                        kept = {}  # key: (u, gx, gy) -> (u,cx,cy,score)\n",
        "                        for u, cx, cy, sc, w, h in preds:\n",
        "                            cell_size = max(7, int(max(1, min(w,h))//4))\n",
        "                            gx = cx // cell_size; gy = cy // cell_size\n",
        "                            key = (u, gx, gy)\n",
        "                            if key not in kept or sc > kept[key][3]:\n",
        "                                kept[key] = (u, cx, cy, sc)\n",
        "                        for u, cx, cy, sc in kept.values():\n",
        "                            labels_out.extend([u, str(cx), str(cy)])\n",
        "        rows.append({'image_id': img_id, 'labels': ' '.join(labels_out)})\n",
        "        del res\n",
        "    sub = pd.DataFrame(rows)\n",
        "    sub.to_csv(save_name, index=False)\n",
        "    print('Saved', save_name, 'shape', sub.shape, flush=True)\n",
        "    return sub\n",
        "\n",
        "print('v6 utilities ready:')\n",
        "print('- build_exemplar_bank(max_per_class=3, model_name=\\'convnext_small\\')')\n",
        "print('- Optional: also build tiny bank for fusion, then call two_stage_build_submission_exemplars(...)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "baa23fe7-5aac-4665-a14b-dbb117577426",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Driver: v6 exemplar kNN voting (single backbone) with adaptive dedup\n",
        "import shutil, pandas as pd\n",
        "print('Building exemplar bank (convnext_small, max_per_class=3) ...', flush=True)\n",
        "embs, metas, bb = build_exemplar_bank(max_per_class=3, img_dir='train_images', out_dir='artifacts', model_name='convnext_small', pad_frac=0.15, out_size=224)\n",
        "print('Exemplars built:', embs.shape, 'backbone:', bb, flush=True)\n",
        "print('Running two_stage_build_submission_exemplars with voting k=5, min_cosine=0.65 ...', flush=True)\n",
        "sub6 = two_stage_build_submission_exemplars(det_weights='runs/yolo8n_kuz_single/weights/best.pt',\n",
        "                                            imgsz_det=1024,\n",
        "                                            conf=0.08,\n",
        "                                            iou=0.65,\n",
        "                                            max_det=4000,\n",
        "                                            pad_frac=0.15,\n",
        "                                            crop_size=224,\n",
        "                                            backbone_primary='convnext_small',\n",
        "                                            min_cosine=0.65,\n",
        "                                            k_vote=5,\n",
        "                                            save_name='submission_v6.csv',\n",
        "                                            predict_half=True,\n",
        "                                            det_device=0,\n",
        "                                            tiny_filter=5,\n",
        "                                            exemplars_path='artifacts/exemplars.npy',\n",
        "                                            exemplars_unicodes_path='artifacts/exemplars_unicodes.json')\n",
        "print('Copying submission_v6.csv -> submission.csv for potential submit ...', flush=True)\n",
        "shutil.copy2('submission_v6.csv', 'submission.csv')\n",
        "print(pd.read_csv('submission.csv').head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "455a2d35-934f-47cf-a6e6-94f90eec21e2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Driver: v7 two-backbone fusion (convnext_small + convnext_tiny) with exemplar voting and adaptive dedup\n",
        "import shutil, os, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "art = Path('artifacts')\n",
        "small_np = art/'exemplars.npy'\n",
        "small_js = art/'exemplars_unicodes.json'\n",
        "small_np_tag = art/'exemplars_small.npy'\n",
        "small_js_tag = art/'exemplars_small_unicodes.json'\n",
        "tiny_np_tag = art/'exemplars_tiny.npy'\n",
        "tiny_js_tag = art/'exemplars_tiny_unicodes.json'\n",
        "\n",
        "# 1) Preserve existing convnext_small exemplars as bank1\n",
        "assert small_np.exists() and small_js.exists(), 'convnext_small exemplars not found; run v6 driver first'\n",
        "shutil.copy2(small_np, small_np_tag)\n",
        "shutil.copy2(small_js, small_js_tag)\n",
        "print('Saved bank1 (small) ->', small_np_tag.name, small_js_tag.name, flush=True)\n",
        "\n",
        "# 2) Build convnext_tiny exemplars as bank2 (will overwrite artifacts/exemplars.* temporarily)\n",
        "print('Building exemplar bank2 (convnext_tiny, max_per_class=3) ...', flush=True)\n",
        "embs2, metas2, bb2 = build_exemplar_bank(max_per_class=3, img_dir='train_images', out_dir='artifacts', model_name='convnext_tiny', pad_frac=0.15, out_size=224)\n",
        "print('Exemplars2 built:', embs2.shape, 'backbone:', bb2, flush=True)\n",
        "shutil.copy2(art/'exemplars.npy', tiny_np_tag)\n",
        "shutil.copy2(art/'exemplars_unicodes.json', tiny_js_tag)\n",
        "print('Saved bank2 (tiny) ->', tiny_np_tag.name, tiny_js_tag.name, flush=True)\n",
        "\n",
        "# 3) Run fused inference: vote per bank (k=5), fuse by max cosine per unicode, threshold 0.65\n",
        "print('Running v7 fused inference (small+tiny) k=5 min_cosine=0.65 ...', flush=True)\n",
        "sub7 = two_stage_build_submission_exemplars(det_weights='runs/yolo8n_kuz_single/weights/best.pt',\n",
        "                                            imgsz_det=1024,\n",
        "                                            conf=0.08,\n",
        "                                            iou=0.65,\n",
        "                                            max_det=4000,\n",
        "                                            pad_frac=0.15,\n",
        "                                            crop_size=224,\n",
        "                                            backbone_primary='convnext_small',\n",
        "                                            min_cosine=0.65,\n",
        "                                            k_vote=5,\n",
        "                                            save_name='submission_v7.csv',\n",
        "                                            predict_half=True,\n",
        "                                            det_device=0,\n",
        "                                            tiny_filter=5,\n",
        "                                            exemplars_path=str(small_np_tag),\n",
        "                                            exemplars_unicodes_path=str(small_js_tag),\n",
        "                                            exemplars2_path=str(tiny_np_tag),\n",
        "                                            exemplars2_unicodes_path=str(tiny_js_tag),\n",
        "                                            backbone_secondary='convnext_tiny')\n",
        "print('Copying submission_v7.csv -> submission.csv ...', flush=True)\n",
        "shutil.copy2('submission_v7.csv', 'submission.csv')\n",
        "print(pd.read_csv('submission.csv').head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "89f0c050-64e1-4dd2-97e7-fe1efe94b5a5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# v7b: Re-run fused inference with stricter threshold (min_cosine=0.67) using existing banks\n",
        "import pandas as pd, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "art = Path('artifacts')\n",
        "small_np_tag = art/'exemplars_small.npy'\n",
        "small_js_tag = art/'exemplars_small_unicodes.json'\n",
        "tiny_np_tag = art/'exemplars_tiny.npy'\n",
        "tiny_js_tag = art/'exemplars_tiny_unicodes.json'\n",
        "assert small_np_tag.exists() and small_js_tag.exists() and tiny_np_tag.exists() and tiny_js_tag.exists(), 'Exemplar banks not found; run v7 first.'\n",
        "print('Running v7b fused inference with min_cosine=0.67 (stricter) ...', flush=True)\n",
        "sub7b = two_stage_build_submission_exemplars(det_weights='runs/yolo8n_kuz_single/weights/best.pt',\n",
        "                                             imgsz_det=1024,\n",
        "                                             conf=0.08,\n",
        "                                             iou=0.65,\n",
        "                                             max_det=4000,\n",
        "                                             pad_frac=0.15,\n",
        "                                             crop_size=224,\n",
        "                                             backbone_primary='convnext_small',\n",
        "                                             min_cosine=0.67,\n",
        "                                             k_vote=5,\n",
        "                                             save_name='submission_v7b.csv',\n",
        "                                             predict_half=True,\n",
        "                                             det_device=0,\n",
        "                                             tiny_filter=5,\n",
        "                                             exemplars_path=str(small_np_tag),\n",
        "                                             exemplars_unicodes_path=str(small_js_tag),\n",
        "                                             exemplars2_path=str(tiny_np_tag),\n",
        "                                             exemplars2_unicodes_path=str(tiny_js_tag),\n",
        "                                             backbone_secondary='convnext_tiny')\n",
        "print('Copying submission_v7b.csv -> submission.csv ...', flush=True)\n",
        "shutil.copy2('submission_v7b.csv', 'submission.csv')\n",
        "print(pd.read_csv('submission.csv').head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "f397cc9d-c2f9-43b4-b3a3-c0fb2208c6ee",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# v8: Multi-scale union (896 + 1024) + two-backbone exemplar voting with adaptive dedup\n",
        "import time, json, gc, os, math\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch, faiss, timm, cv2\n",
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Reuse helpers from v6 cell (crop/emb/backbone/search).\n",
        "def _nms_iou_xyxy(boxes: np.ndarray, scores: np.ndarray, iou_thr: float=0.70, limit: Optional[int]=None) -> List[int]:\n",
        "    # boxes: (N,4) xyxy, scores: (N,)\n",
        "    if boxes.size == 0:\n",
        "        return []\n",
        "    x1 = boxes[:,0]; y1 = boxes[:,1]; x2 = boxes[:,2]; y2 = boxes[:,3]\n",
        "    areas = (np.maximum(0, x2 - x1 + 1) * np.maximum(0, y2 - y1 + 1))\n",
        "    order = scores.argsort()[::-1]\n",
        "    keep = []\n",
        "    while order.size > 0:\n",
        "        i = order[0]\n",
        "        keep.append(int(i))\n",
        "        if limit is not None and len(keep) >= limit:\n",
        "            break\n",
        "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
        "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
        "        inter = w * h\n",
        "        ovr = inter / (areas[i] + areas[order[1:]] - inter + 1e-9)\n",
        "        inds = np.where(ovr <= iou_thr)[0]\n",
        "        order = order[inds + 1]\n",
        "    return keep\n",
        "\n",
        "def two_stage_build_submission_exemplars_multiscale(det_weights: str,\n",
        "                                                    sizes: Tuple[int,int]=(896,1024),\n",
        "                                                    conf: float=0.08,\n",
        "                                                    iou: float=0.65,\n",
        "                                                    max_det: int=4000,\n",
        "                                                    union_iou: float=0.70,\n",
        "                                                    pad_frac: float=0.15,\n",
        "                                                    crop_size: int=224,\n",
        "                                                    backbone_primary: str='convnext_small',\n",
        "                                                    min_cosine: float=0.65,\n",
        "                                                    k_vote: int=5,\n",
        "                                                    save_name: str='submission_v8.csv',\n",
        "                                                    predict_half: bool=True,\n",
        "                                                    det_device=0,\n",
        "                                                    tiny_filter:int=5,\n",
        "                                                    exemplars_path:str='artifacts/exemplars_small.npy',\n",
        "                                                    exemplars_unicodes_path:str='artifacts/exemplars_small_unicodes.json',\n",
        "                                                    exemplars2_path:str='artifacts/exemplars_tiny.npy',\n",
        "                                                    exemplars2_unicodes_path:str='artifacts/exemplars_tiny_unicodes.json',\n",
        "                                                    backbone_secondary: str='convnext_tiny'):\n",
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = os.environ.get('PYTORCH_CUDA_ALLOC_CONF','expandable_segments:True')\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache(); gc.collect()\n",
        "    ss = pd.read_csv('sample_submission.csv')\n",
        "    image_ids = ss['image_id'].tolist()\n",
        "    img_paths = []\n",
        "    for img_id in image_ids:\n",
        "        p = Path('test_images')/f'{img_id}.jpg'\n",
        "        if not p.exists():\n",
        "            alts = list(Path('test_images').glob(f'{img_id}.*'))\n",
        "            if alts: p = alts[0]\n",
        "        img_paths.append(str(p))\n",
        "    print('Loading detector:', det_weights, flush=True)\n",
        "    det = YOLO(det_weights)\n",
        "    # Exemplar banks\n",
        "    ex1 = np.fromfile(exemplars_path, dtype=np.float32)\n",
        "    metas1: List[str] = json.loads(Path(exemplars_unicodes_path).read_text())\n",
        "    d1 = ex1.size // max(1,len(metas1)); ex1 = ex1.reshape(-1, d1).astype(np.float32)\n",
        "    faiss.normalize_L2(ex1); idx1 = faiss.IndexFlatIP(d1); idx1.add(ex1)\n",
        "    ex2 = np.fromfile(exemplars2_path, dtype=np.float32)\n",
        "    metas2: List[str] = json.loads(Path(exemplars2_unicodes_path).read_text())\n",
        "    d2 = ex2.size // max(1,len(metas2)); ex2 = ex2.reshape(-1, d2).astype(np.float32)\n",
        "    faiss.normalize_L2(ex2); idx2 = faiss.IndexFlatIP(d2); idx2.add(ex2)\n",
        "    print('Banks:', ex1.shape, ex2.shape, 'k=', k_vote, flush=True)\n",
        "    # Backbones CPU\n",
        "    model1, mean1, std1 = _build_backbone_cpu(backbone_primary)\n",
        "    model2, mean2, std2 = _build_backbone_cpu(backbone_secondary)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    # 1) Run detection at multiple scales and collect per-image boxes\n",
        "    def detect_at_size(s:int):\n",
        "        return det.predict(source=img_paths, imgsz=s, conf=conf, iou=iou, max_det=max_det, augment=True, device=det_device, stream=True, verbose=False, batch=1, half=predict_half)\n",
        "    print('Running detection size', sizes[0], '...', flush=True)\n",
        "    res_a = list(detect_at_size(int(sizes[0])))\n",
        "    print('Running detection size', sizes[1], '...', flush=True)\n",
        "    res_b = list(detect_at_size(int(sizes[1])))\n",
        "    # 2) Union per image via class-agnostic NMS\n",
        "    rows = []; t0 = time.time()\n",
        "    for i, (img_id, img_path, ra, rb) in enumerate(zip(image_ids, img_paths, res_a, res_b)):\n",
        "        if i % 25 == 0:\n",
        "            print(f'Union {i}/{len(image_ids)} images, elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "        labels_out = []\n",
        "        with Image.open(img_path) as im:\n",
        "            W, H = im.size\n",
        "            all_xyxy = []; all_scores = []\n",
        "            for r in (ra, rb):\n",
        "                if r is not None and hasattr(r,'boxes') and r.boxes is not None and len(r.boxes)>0:\n",
        "                    b = r.boxes\n",
        "                    xyxy = b.xyxy.cpu().numpy();\n",
        "                    confs = b.conf.cpu().numpy() if getattr(b, 'conf', None) is not None else np.ones((xyxy.shape[0],), dtype=np.float32)\n",
        "                    if xyxy.size:\n",
        "                        all_xyxy.append(xyxy); all_scores.append(confs)\n",
        "            if all_xyxy:\n",
        "                xyxy_u = np.concatenate(all_xyxy, 0).astype(np.float32)\n",
        "                scores_u = np.concatenate(all_scores, 0).astype(np.float32)\n",
        "                keep = _nms_iou_xyxy(xyxy_u, scores_u, iou_thr=union_iou, limit=max_det)\n",
        "                xyxy_u = xyxy_u[keep]; scores_u = scores_u[keep]\n",
        "                # 3) Classify merged boxes with exemplar fusion\n",
        "                batch1 = []; centers = []; ws = []; hs = []\n",
        "                for (x1,y1,x2,y2) in xyxy_u:\n",
        "                    x = int(round(x1)); y = int(round(y1)); w = int(round(x2-x1)); h = int(round(y2-y1))\n",
        "                    if w < tiny_filter or h < tiny_filter:\n",
        "                        continue\n",
        "                    arr = _crop_pad_resize(im, x,y,w,h, pad_frac=pad_frac, out_size=crop_size)\n",
        "                    batch1.append(arr)\n",
        "                    cx = int(round((x1+x2)/2.0)); cy = int(round((y1+y2)/2.0))\n",
        "                    cx = max(0, min(cx, W-1)); cy = max(0, min(cy, H-1))\n",
        "                    centers.append((cx,cy)); ws.append(w); hs.append(h)\n",
        "                if batch1:\n",
        "                    q1 = _embed_batch_cpu(model1, np.stack(batch1,0), mean1, std1)\n",
        "                    q2 = _embed_batch_cpu(model2, np.stack(batch1,0), mean2, std2)\n",
        "                    # vote and fuse by max\n",
        "                    votes1 = _search_k_per_unicode(idx1, q1, metas1, k=k_vote)\n",
        "                    votes2 = _search_k_per_unicode(idx2, q2, metas2, k=k_vote)\n",
        "                    preds = []\n",
        "                    for j in range(len(centers)):\n",
        "                        v1 = votes1[j]; v2 = votes2[j]\n",
        "                        cand_us = set(v1.keys()) | set(v2.keys())\n",
        "                        best_u=None; best_s=-1.0\n",
        "                        for u in cand_us:\n",
        "                            s = max(v1.get(u,0.0), v2.get(u,0.0))\n",
        "                            if s > best_s:\n",
        "                                best_s = s; best_u = u\n",
        "                        if best_u is not None and best_s >= float(min_cosine):\n",
        "                            (cx,cy) = centers[j]\n",
        "                            preds.append((best_u, cx, cy, float(best_s), ws[j], hs[j]))\n",
        "                    if preds:\n",
        "                        kept = {}\n",
        "                        for u, cx, cy, sc, w, h in preds:\n",
        "                            cell_size = max(7, int(max(1, min(w,h))//4))\n",
        "                            gx = cx // cell_size; gy = cy // cell_size\n",
        "                            key = (u, gx, gy)\n",
        "                            if key not in kept or sc > kept[key][3]:\n",
        "                                kept[key] = (u, cx, cy, sc)\n",
        "                        for u, cx, cy, sc in kept.values():\n",
        "                            labels_out.extend([u, str(cx), str(cy)])\n",
        "        rows.append({'image_id': img_id, 'labels': ' '.join(labels_out)})\n",
        "        del ra, rb\n",
        "    sub = pd.DataFrame(rows)\n",
        "    sub.to_csv(save_name, index=False)\n",
        "    print('Saved', save_name, 'shape', sub.shape, flush=True)\n",
        "    return sub\n",
        "\n",
        "# Driver for v8\n",
        "import shutil\n",
        "print('Running v8 multi-scale union (896,1024) with fused exemplar voting (small+tiny), min_cosine=0.65 ...', flush=True)\n",
        "sub8 = two_stage_build_submission_exemplars_multiscale(det_weights='runs/yolo8n_kuz_single/weights/best.pt',\n",
        "                                                      sizes=(896,1024),\n",
        "                                                      conf=0.08,\n",
        "                                                      iou=0.65,\n",
        "                                                      max_det=4000,\n",
        "                                                      union_iou=0.70,\n",
        "                                                      pad_frac=0.15,\n",
        "                                                      crop_size=224,\n",
        "                                                      backbone_primary='convnext_small',\n",
        "                                                      min_cosine=0.65,\n",
        "                                                      k_vote=5,\n",
        "                                                      save_name='submission_v8.csv',\n",
        "                                                      predict_half=True,\n",
        "                                                      det_device=0,\n",
        "                                                      tiny_filter=5,\n",
        "                                                      exemplars_path='artifacts/exemplars_small.npy',\n",
        "                                                      exemplars_unicodes_path='artifacts/exemplars_small_unicodes.json',\n",
        "                                                      exemplars2_path='artifacts/exemplars_tiny.npy',\n",
        "                                                      exemplars2_unicodes_path='artifacts/exemplars_tiny_unicodes.json',\n",
        "                                                      backbone_secondary='convnext_tiny')\n",
        "print('Copying submission_v8.csv -> submission.csv ...', flush=True)\n",
        "shutil.copy2('submission_v8.csv', 'submission.csv')\n",
        "print(pd.read_csv('submission.csv').head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "81457037-de2a-49ee-83ea-3dcb7b171196",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# v8b: Multi-scale union with stricter threshold (min_cosine=0.67) using existing banks\n",
        "import shutil, pandas as pd\n",
        "print('Running v8b multi-scale union (896,1024) with fused exemplar voting (small+tiny), min_cosine=0.67 ...', flush=True)\n",
        "sub8b = two_stage_build_submission_exemplars_multiscale(det_weights='runs/yolo8n_kuz_single/weights/best.pt',\n",
        "                                                       sizes=(896,1024),\n",
        "                                                       conf=0.08,\n",
        "                                                       iou=0.65,\n",
        "                                                       max_det=4000,\n",
        "                                                       union_iou=0.70,\n",
        "                                                       pad_frac=0.15,\n",
        "                                                       crop_size=224,\n",
        "                                                       backbone_primary='convnext_small',\n",
        "                                                       min_cosine=0.67,\n",
        "                                                       k_vote=5,\n",
        "                                                       save_name='submission_v8b.csv',\n",
        "                                                       predict_half=True,\n",
        "                                                       det_device=0,\n",
        "                                                       tiny_filter=5,\n",
        "                                                       exemplars_path='artifacts/exemplars_small.npy',\n",
        "                                                       exemplars_unicodes_path='artifacts/exemplars_small_unicodes.json',\n",
        "                                                       exemplars2_path='artifacts/exemplars_tiny.npy',\n",
        "                                                       exemplars2_unicodes_path='artifacts/exemplars_tiny_unicodes.json',\n",
        "                                                       backbone_secondary='convnext_tiny')\n",
        "print('Copying submission_v8b.csv -> submission.csv ...', flush=True)\n",
        "shutil.copy2('submission_v8b.csv', 'submission.csv')\n",
        "print(pd.read_csv('submission.csv').head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "bca5c3f1-61be-493c-991a-2fbc2eb34b59",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# v8c: 3-scale union (832,960,1024) + corrected gating (single-neighbor max) + dynamic per-image threshold\n",
        "import time, json, gc, os, math, numpy as np, pandas as pd, torch, faiss, timm, cv2\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Reuse crop + CPU backbone/embedding from earlier cells\n",
        "def _crop_pad_resize(img: Image.Image, x:int, y:int, w:int, h:int, pad_frac:float=0.15, out_size:int=224):\n",
        "    W, H = img.size\n",
        "    cx = x + w/2.0; cy = y + h/2.0\n",
        "    pw = max(2, int(round(w * pad_frac))); ph = max(2, int(round(h * pad_frac)))\n",
        "    x1 = max(0, int(round(cx - w/2 - pw))); y1 = max(0, int(round(cy - h/2 - ph)))\n",
        "    x2 = min(W, int(round(cx + w/2 + pw))); y2 = min(H, int(round(cy + h/2 + ph)))\n",
        "    crop = img.crop((x1, y1, x2, y2)).convert('L')\n",
        "    arr = np.array(crop)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    arr = clahe.apply(arr)\n",
        "    h0, w0 = arr.shape[:2]; m = max(h0, w0)\n",
        "    pad_top = (m - h0) // 2; pad_bottom = m - h0 - pad_top\n",
        "    pad_left = (m - w0) // 2; pad_right = m - w0 - pad_left\n",
        "    arr = cv2.copyMakeBorder(arr, pad_top, pad_bottom, pad_left, pad_right, borderType=cv2.BORDER_CONSTANT, value=0)\n",
        "    arr = cv2.resize(arr, (out_size, out_size), interpolation=cv2.INTER_AREA)\n",
        "    arr = np.stack([arr, arr, arr], axis=0).astype(np.float32) / 255.0\n",
        "    return arr\n",
        "\n",
        "@torch.no_grad()\n",
        "def _build_backbone_cpu(model_name:str):\n",
        "    model = timm.create_model(model_name, pretrained=True, num_classes=0, global_pool='avg')\n",
        "    model.eval().to('cpu')\n",
        "    data_cfg = timm.data.resolve_model_data_config(model)\n",
        "    mean = torch.tensor(data_cfg.get('mean', (0.485,0.456,0.406)), dtype=torch.float32, device='cpu').view(1,3,1,1)\n",
        "    std  = torch.tensor(data_cfg.get('std',  (0.229,0.224,0.225)), dtype=torch.float32, device='cpu').view(1,3,1,1)\n",
        "    _ = model((torch.zeros(1,3,224,224)-mean)/std)\n",
        "    return model, mean, std\n",
        "\n",
        "@torch.no_grad()\n",
        "def _embed_batch_cpu(model, batch_np, mean, std):\n",
        "    t = torch.from_numpy(batch_np).to('cpu')\n",
        "    t = (t - mean) / std\n",
        "    f = model(t)\n",
        "    f = torch.nn.functional.normalize(f, p=2, dim=1)\n",
        "    return f.cpu().numpy().astype(np.float32)\n",
        "\n",
        "def _nms_iou_xyxy(boxes: np.ndarray, scores: np.ndarray, iou_thr: float=0.75, limit: Optional[int]=None) -> List[int]:\n",
        "    if boxes.size == 0:\n",
        "        return []\n",
        "    x1 = boxes[:,0]; y1 = boxes[:,1]; x2 = boxes[:,2]; y2 = boxes[:,3]\n",
        "    areas = (np.maximum(0, x2 - x1 + 1) * np.maximum(0, y2 - y1 + 1))\n",
        "    order = scores.argsort()[::-1]\n",
        "    keep = []\n",
        "    while order.size > 0:\n",
        "        i = order[0]\n",
        "        keep.append(int(i))\n",
        "        if limit is not None and len(keep) >= limit:\n",
        "            break\n",
        "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
        "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
        "        inter = w * h\n",
        "        ovr = inter / (areas[i] + areas[order[1:]] - inter + 1e-9)\n",
        "        inds = np.where(ovr <= iou_thr)[0]\n",
        "        order = order[inds + 1]\n",
        "    return keep\n",
        "\n",
        "def _search_k_sum_and_max(index: faiss.IndexFlatIP, queries: np.ndarray, exemplar_unicodes: List[str], k:int=5):\n",
        "    # returns two lists, each element is dict: unicode -> sum/max of top-k cosines for that unicode\n",
        "    D, I = index.search(queries, k)\n",
        "    sum_list = []; max_list = []\n",
        "    for drow, irow in zip(D, I):\n",
        "        acc_sum: Dict[str, float] = {}; acc_max: Dict[str, float] = {}\n",
        "        for sim, idx in zip(drow.tolist(), irow.tolist()):\n",
        "            if idx == -1: continue\n",
        "            u = exemplar_unicodes[idx]\n",
        "            acc_sum[u] = acc_sum.get(u, 0.0) + float(sim)\n",
        "            acc_max[u] = max(acc_max.get(u, -1e9), float(sim))\n",
        "        sum_list.append(acc_sum); max_list.append(acc_max)\n",
        "    return sum_list, max_list\n",
        "\n",
        "def two_stage_build_submission_exemplars_multiscale_v8c(det_weights: str,\n",
        "                                                        sizes: Tuple[int,int,int]=(832,960,1024),\n",
        "                                                        conf: float=0.08,\n",
        "                                                        iou: float=0.65,\n",
        "                                                        max_det: int=4000,\n",
        "                                                        union_iou: float=0.75,\n",
        "                                                        pad_frac: float=0.15,\n",
        "                                                        crop_size: int=224,\n",
        "                                                        backbone_primary: str='convnext_small',\n",
        "                                                        min_cosine_floor: float=0.65,\n",
        "                                                        k_vote: int=5,\n",
        "                                                        save_name: str='submission_v8c.csv',\n",
        "                                                        predict_half: bool=True,\n",
        "                                                        det_device=0,\n",
        "                                                        tiny_filter:int=5,\n",
        "                                                        exemplars_path:str='artifacts/exemplars_small.npy',\n",
        "                                                        exemplars_unicodes_path:str='artifacts/exemplars_small_unicodes.json',\n",
        "                                                        exemplars2_path:str='artifacts/exemplars_tiny.npy',\n",
        "                                                        exemplars2_unicodes_path:str='artifacts/exemplars_tiny_unicodes.json',\n",
        "                                                        backbone_secondary: str='convnext_tiny',\n",
        "                                                        use_dynamic_thresh: bool=True):\n",
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = os.environ.get('PYTORCH_CUDA_ALLOC_CONF','expandable_segments:True')\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache(); gc.collect()\n",
        "    ss = pd.read_csv('sample_submission.csv')\n",
        "    image_ids = ss['image_id'].tolist()\n",
        "    img_paths = []\n",
        "    for img_id in image_ids:\n",
        "        p = Path('test_images')/f'{img_id}.jpg'\n",
        "        if not p.exists():\n",
        "            alts = list(Path('test_images').glob(f'{img_id}.*'))\n",
        "            if alts: p = alts[0]\n",
        "        img_paths.append(str(p))\n",
        "    print('Loading detector:', det_weights, flush=True)\n",
        "    det = YOLO(det_weights)\n",
        "    # Exemplar banks\n",
        "    ex1 = np.fromfile(exemplars_path, dtype=np.float32)\n",
        "    metas1: List[str] = json.loads(Path(exemplars_unicodes_path).read_text())\n",
        "    d1 = ex1.size // max(1,len(metas1)); ex1 = ex1.reshape(-1, d1).astype(np.float32)\n",
        "    faiss.normalize_L2(ex1); idx1 = faiss.IndexFlatIP(d1); idx1.add(ex1)\n",
        "    ex2 = np.fromfile(exemplars2_path, dtype=np.float32)\n",
        "    metas2: List[str] = json.loads(Path(exemplars2_unicodes_path).read_text())\n",
        "    d2 = ex2.size // max(1,len(metas2)); ex2 = ex2.reshape(-1, d2).astype(np.float32)\n",
        "    faiss.normalize_L2(ex2); idx2 = faiss.IndexFlatIP(d2); idx2.add(ex2)\n",
        "    print('Banks:', ex1.shape, ex2.shape, 'k=', k_vote, flush=True)\n",
        "    # Backbones CPU\n",
        "    model1, mean1, std1 = _build_backbone_cpu(backbone_primary)\n",
        "    model2, mean2, std2 = _build_backbone_cpu(backbone_secondary)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    def detect_at_size(s:int):\n",
        "        return det.predict(source=img_paths, imgsz=s, conf=conf, iou=iou, max_det=max_det, augment=True, device=det_device, stream=True, verbose=False, batch=1, half=predict_half)\n",
        "    print('Running detection sizes', sizes, '...', flush=True)\n",
        "    res_all = [list(detect_at_size(int(s))) for s in sizes]\n",
        "    rows = []; t0 = time.time()\n",
        "    for i, items in enumerate(zip(image_ids, img_paths, *res_all)):\n",
        "        img_id = items[0]; img_path = items[1]; res_list = items[2:]\n",
        "        if i % 25 == 0:\n",
        "            print(f'Union {i}/{len(image_ids)} images, elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "        labels_out = []\n",
        "        with Image.open(img_path) as im:\n",
        "            W, H = im.size\n",
        "            all_xyxy = []; all_scores = []\n",
        "            for r in res_list:\n",
        "                if r is not None and hasattr(r,'boxes') and r.boxes is not None and len(r.boxes)>0:\n",
        "                    b = r.boxes\n",
        "                    xyxy = b.xyxy.cpu().numpy();\n",
        "                    confs = b.conf.cpu().numpy() if getattr(b, 'conf', None) is not None else np.ones((xyxy.shape[0],), dtype=np.float32)\n",
        "                    if xyxy.size:\n",
        "                        all_xyxy.append(xyxy); all_scores.append(confs)\n",
        "            if all_xyxy:\n",
        "                xyxy_u = np.concatenate(all_xyxy, 0).astype(np.float32)\n",
        "                scores_u = np.concatenate(all_scores, 0).astype(np.float32)\n",
        "                keep = _nms_iou_xyxy(xyxy_u, scores_u, iou_thr=union_iou, limit=max_det)\n",
        "                xyxy_u = xyxy_u[keep]; scores_u = scores_u[keep]\n",
        "                batch1 = []; centers = []; ws = []; hs = []\n",
        "                for (x1,y1,x2,y2) in xyxy_u:\n",
        "                    x = int(round(x1)); y = int(round(y1)); w = int(round(x2-x1)); h = int(round(y2-y1))\n",
        "                    if w < tiny_filter or h < tiny_filter:\n",
        "                        continue\n",
        "                    arr = _crop_pad_resize(im, x,y,w,h, pad_frac=pad_frac, out_size=crop_size)\n",
        "                    batch1.append(arr)\n",
        "                    cx = int(round((x1+x2)/2.0)); cy = int(round((y1+y2)/2.0))\n",
        "                    cx = max(0, min(cx, W-1)); cy = max(0, min(cy, H-1))\n",
        "                    centers.append((cx,cy)); ws.append(w); hs.append(h)\n",
        "                preds_all = []  # store (u, cx, cy, s_gate, best_sum, second_sum, w, h)\n",
        "                if batch1:\n",
        "                    q1 = _embed_batch_cpu(model1, np.stack(batch1,0), mean1, std1)\n",
        "                    q2 = _embed_batch_cpu(model2, np.stack(batch1,0), mean2, std2)\n",
        "                    v1_sum, v1_max = _search_k_sum_and_max(idx1, q1, metas1, k=k_vote)\n",
        "                    v2_sum, v2_max = _search_k_sum_and_max(idx2, q2, metas2, k=k_vote)\n",
        "                    for j in range(len(centers)):\n",
        "                        sdict1 = v1_sum[j]; mdict1 = v1_max[j]\n",
        "                        sdict2 = v2_sum[j]; mdict2 = v2_max[j]\n",
        "                        cand_us = set(sdict1.keys()) | set(sdict2.keys())\n",
        "                        # choose by fused sum (0.6 small + 0.4 tiny)\n",
        "                        fused_scores = []\n",
        "                        for u in cand_us:\n",
        "                            fs = 0.6*sdict1.get(u,0.0) + 0.4*sdict2.get(u,0.0)\n",
        "                            fused_scores.append((u, fs))\n",
        "                        if not fused_scores:\n",
        "                            continue\n",
        "                        fused_scores.sort(key=lambda t: t[1], reverse=True)\n",
        "                        best_u, best_sum = fused_scores[0]\n",
        "                        second_sum = fused_scores[1][1] if len(fused_scores) > 1 else -1.0\n",
        "                        s_gate = max(mdict1.get(best_u, -1.0), mdict2.get(best_u, -1.0))\n",
        "                        (cx,cy) = centers[j]\n",
        "                        preds_all.append((best_u, cx, cy, float(s_gate), float(best_sum), float(second_sum), ws[j], hs[j]))\n",
        "                # dynamic per-image threshold from s_gate list\n",
        "                if preds_all:\n",
        "                    s_list = np.array([p[3] for p in preds_all], dtype=np.float32)\n",
        "                    if use_dynamic_thresh and s_list.size > 0:\n",
        "                        t_img = float(np.percentile(s_list, 85))\n",
        "                        t_img = float(np.clip(t_img, 0.63, 0.72))\n",
        "                        thr = max(t_img, float(min_cosine_floor))\n",
        "                    else:\n",
        "                        thr = float(min_cosine_floor)\n",
        "                    kept = {}  # (u,gx,gy) -> (u,cx,cy,s_gate)\n",
        "                    for (u, cx, cy, s_gate, best_sum, second_sum, w, h) in preds_all:\n",
        "                        local_thr = thr\n",
        "                        if (best_sum - second_sum) < 0.02:\n",
        "                            local_thr = thr + 0.02\n",
        "                        if s_gate >= local_thr:\n",
        "                            cell_size = max(7, int(max(1, min(w,h))//4))\n",
        "                            gx = cx // cell_size; gy = cy // cell_size\n",
        "                            key = (u, gx, gy)\n",
        "                            if key not in kept or s_gate > kept[key][3]:\n",
        "                                kept[key] = (u, cx, cy, s_gate)\n",
        "                    for u, cx, cy, s_gate in kept.values():\n",
        "                        labels_out.extend([u, str(cx), str(cy)])\n",
        "        rows.append({'image_id': img_id, 'labels': ' '.join(labels_out)})\n",
        "    sub = pd.DataFrame(rows)\n",
        "    sub.to_csv(save_name, index=False)\n",
        "    print('Saved', save_name, 'shape', sub.shape, flush=True)\n",
        "    return sub\n",
        "\n",
        "# Driver v8c\n",
        "import shutil\n",
        "print('Running v8c: 3-scale union (832,960,1024), union_iou=0.75, fused-sum choice (0.6/0.4), gate by single-neighbor max with dynamic per-image threshold ...', flush=True)\n",
        "sub8c = two_stage_build_submission_exemplars_multiscale_v8c(det_weights='runs/yolo8n_kuz_single/weights/best.pt',\n",
        "                                                           sizes=(832,960,1024),\n",
        "                                                           conf=0.08,\n",
        "                                                           iou=0.65,\n",
        "                                                           max_det=4000,\n",
        "                                                           union_iou=0.75,\n",
        "                                                           pad_frac=0.15,\n",
        "                                                           crop_size=224,\n",
        "                                                           backbone_primary='convnext_small',\n",
        "                                                           min_cosine_floor=0.65,\n",
        "                                                           k_vote=5,\n",
        "                                                           save_name='submission_v8c.csv',\n",
        "                                                           predict_half=True,\n",
        "                                                           det_device=0,\n",
        "                                                           tiny_filter=5,\n",
        "                                                           exemplars_path='artifacts/exemplars_small.npy',\n",
        "                                                           exemplars_unicodes_path='artifacts/exemplars_small_unicodes.json',\n",
        "                                                           exemplars2_path='artifacts/exemplars_tiny.npy',\n",
        "                                                           exemplars2_unicodes_path='artifacts/exemplars_tiny_unicodes.json',\n",
        "                                                           backbone_secondary='convnext_tiny',\n",
        "                                                           use_dynamic_thresh=True)\n",
        "print('Copying submission_v8c.csv -> submission.csv ...', flush=True)\n",
        "shutil.copy2('submission_v8c.csv', 'submission.csv')\n",
        "print(pd.read_csv('submission.csv').head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "90702b2a-7dc7-4d76-b039-87057cf25ed4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# v9: Supervised crop classifier (convnext_small) training + inference hooks\n",
        "import os, math, time, json, gc, random\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torchvision.transforms as T\n",
        "import timm\n",
        "\n",
        "# Globals from earlier cells: boxes_df, split_df, train_dir, id2u, u2id\n",
        "u2id = json.loads(Path('artifacts/u2id.json').read_text())\n",
        "id2u = json.loads(Path('artifacts/id2u.json').read_text())\n",
        "num_classes = len(id2u)\n",
        "\n",
        "def clamp_int(v, lo, hi):\n",
        "    return int(max(lo, min(hi, v)))\n",
        "\n",
        "class CropDataset(Dataset):\n",
        "    def __init__(self, rows: List[Tuple[str,str,int,int,int,int]], img_dir='train_images', train=True):\n",
        "        self.rows = rows\n",
        "        self.img_dir = Path(img_dir)\n",
        "        self.train = train\n",
        "        # Augs as per expert advice (light, glyph-safe)\n",
        "        self.jitter = T.ColorJitter(0.2,0.2,0.2) if train else nn.Identity()\n",
        "        self.affine = T.RandomAffine(degrees=5, shear=5) if train else nn.Identity()\n",
        "        self.blur = T.RandomApply([T.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0))], p=0.15) if train else nn.Identity()\n",
        "        self.erase = T.RandomErasing(p=0.1) if train else nn.Identity()\n",
        "        self.to_tensor = T.ToTensor()\n",
        "        self.norm_mean = torch.tensor([0.485,0.456,0.406]).view(3,1,1)\n",
        "        self.norm_std = torch.tensor([0.229,0.224,0.225]).view(3,1,1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        iid, u, x, y, w, h = self.rows[idx]\n",
        "        p = self.img_dir/f\"{iid}.jpg\"\n",
        "        if not p.exists():\n",
        "            alts = list(self.img_dir.glob(f\"{iid}.*\"))\n",
        "            if alts:\n",
        "                p = alts[0]\n",
        "        with Image.open(p) as im:\n",
        "            W,H = im.size\n",
        "            # pad_frac in [0.25,0.35] during train; fixed 0.30 during val\n",
        "            pad_frac = random.uniform(0.25,0.35) if self.train else 0.30\n",
        "            cx = x + w/2.0; cy = y + h/2.0\n",
        "            pw = max(2, int(round(w*pad_frac))); ph = max(2, int(round(h*pad_frac)))\n",
        "            x1 = clamp_int(round(cx - w/2 - pw), 0, W-1);\n",
        "            y1 = clamp_int(round(cy - h/2 - ph), 0, H-1);\n",
        "            x2 = clamp_int(round(cx + w/2 + pw), 0, W-1);\n",
        "            y2 = clamp_int(round(cy + h/2 + ph), 0, H-1);\n",
        "            crop = im.crop((x1,y1,x2,y2)).convert('L')\n",
        "            # square pad to 224\n",
        "            arr = np.array(crop)\n",
        "            h0,w0 = arr.shape[:2]; m = max(h0,w0)\n",
        "            pad_top = (m - h0)//2; pad_bottom = m - h0 - pad_top\n",
        "            pad_left = (m - w0)//2; pad_right = m - w0 - pad_left\n",
        "            arr = np.pad(arr, ((pad_top,pad_bottom),(pad_left,pad_right)), mode='constant', constant_values=0)\n",
        "            crop = Image.fromarray(arr).resize((224,224), resample=Image.BILINEAR)\n",
        "            # grayscale->3ch\n",
        "            img3 = Image.merge('RGB', (crop,crop,crop))\n",
        "        # torchvision augs\n",
        "        if self.train:\n",
        "            img3 = self.jitter(img3)\n",
        "            img3 = self.affine(img3)\n",
        "            img3 = self.blur(img3)\n",
        "        t = self.to_tensor(img3)  # [0,1]\n",
        "        # RandomErasing requires Tensor\n",
        "        if self.train:\n",
        "            t = self.erase(t)\n",
        "        t = (t - self.norm_mean) / self.norm_std\n",
        "        y_lbl = u2id.get(u, -1)\n",
        "        return t, y_lbl\n",
        "\n",
        "def make_train_val_rows():\n",
        "    split_map = dict(zip(split_df['image_id'], split_df['is_val']))\n",
        "    rows_tr = []; rows_va = []\n",
        "    for r in boxes_df.itertuples(index=False):\n",
        "        tup = (r.image_id, r.unicode, int(r.x), int(r.y), int(r.w), int(r.h))\n",
        "        if split_map.get(r.image_id, False):\n",
        "            rows_va.append(tup)\n",
        "        else:\n",
        "            rows_tr.append(tup)\n",
        "    return rows_tr, rows_va\n",
        "\n",
        "def build_samplers(rows_tr: List[Tuple[str,str,int,int,int,int]], num_samples_per_epoch=160_000):\n",
        "    # compute class frequencies\n",
        "    from collections import Counter\n",
        "    freq = Counter([u for (_,u,_,_,_,_) in rows_tr])\n",
        "    # weight per sample = 1/sqrt(freq[label])\n",
        "    weights = []\n",
        "    for (_,u,_,_,_,_) in rows_tr:\n",
        "        f = max(1, freq[u])\n",
        "        weights.append(1.0/math.sqrt(f))\n",
        "    weights = torch.tensor(weights, dtype=torch.float32)\n",
        "    sampler = WeightedRandomSampler(weights=weights, num_samples=num_samples_per_epoch, replacement=True)\n",
        "    return sampler\n",
        "\n",
        "def create_model_convnext_small(nc: int):\n",
        "    model = timm.create_model('convnext_small', pretrained=True, num_classes=nc)\n",
        "    return model\n",
        "\n",
        "def train_classifier(epochs_head=2, epochs_full=8, batch_size=256, num_workers=8, lr_head=3e-3, lr_all=5e-4, wd=0.05, label_smoothing=0.1, steps_warmup=200, samples_per_epoch=160_000, out_dir='artifacts/cls_convnext_small'):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    rows_tr, rows_va = make_train_val_rows()\n",
        "    ds_tr = CropDataset(rows_tr, train=True)\n",
        "    ds_va = CropDataset(rows_va, train=False)\n",
        "    sampler = build_samplers(rows_tr, num_samples_per_epoch=samples_per_epoch)\n",
        "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, sampler=sampler, num_workers=num_workers, pin_memory=True, persistent_workers=(num_workers>0))\n",
        "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, persistent_workers=(num_workers>0))\n",
        "    model = create_model_convnext_small(num_classes).to(device)\n",
        "    # EMA\n",
        "    ema = timm.utils.ModelEmaV2(model, decay=0.999, device=device)\n",
        "    # Phase 1: strict freeze backbone, train head only (ConvNeXt: model.head.fc is final Linear)\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "    assert hasattr(model, 'head') and hasattr(model.head, 'fc'), 'Expected convnext_small with model.head.fc'\n",
        "    for p in model.head.fc.parameters():\n",
        "        p.requires_grad = True\n",
        "    head_params = [p for p in model.parameters() if p.requires_grad]\n",
        "    opt = torch.optim.AdamW(head_params, lr=lr_head, weight_decay=wd)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(device=='cuda'))\n",
        "    loss_fn = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
        "    best_acc = 0.0\n",
        "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def run_val():\n",
        "        model_to_eval = ema.module if ema is not None else model\n",
        "        model_to_eval.eval()\n",
        "        correct=0; total=0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in dl_va:\n",
        "                xb = xb.to(device, non_blocking=True); yb = yb.to(device, non_blocking=True)\n",
        "                with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
        "                    logits = model_to_eval(xb)\n",
        "                pred = logits.argmax(1)\n",
        "                correct += (pred==yb).sum().item()\n",
        "                total += yb.numel()\n",
        "        acc = correct/max(1,total)\n",
        "        return acc\n",
        "\n",
        "    step=0\n",
        "    print('Phase 1: training head for', epochs_head, 'epochs', flush=True)\n",
        "    for ep in range(epochs_head):\n",
        "        model.train()\n",
        "        t0=time.time()\n",
        "        for it,(xb,yb) in enumerate(dl_tr):\n",
        "            xb = xb.to(device, non_blocking=True); yb = yb.to(device, non_blocking=True)\n",
        "            with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
        "                logits = model(xb)\n",
        "                loss = loss_fn(logits, yb)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            scaler.scale(loss).backward()\n",
        "            # grad clipping\n",
        "            scaler.unscale_(opt)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            ema.update(model)\n",
        "            step+=1\n",
        "            if it%200==0:\n",
        "                print(f'Head ep{ep} it{it} loss={loss.item():.4f}', flush=True)\n",
        "        acc = run_val()\n",
        "        print(f'Head epoch {ep} val_acc={acc:.4f} elapsed={time.time()-t0:.1f}s', flush=True)\n",
        "        if acc>best_acc:\n",
        "            best_acc=acc\n",
        "            torch.save({'model': ema.module.state_dict(), 'acc': acc, 'ep': ep}, Path(out_dir)/'best_head.pt')\n",
        "\n",
        "    # Phase 2: unfreeze all\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = True\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr_all, weight_decay=wd)\n",
        "    # cosine decay to 1e-5\n",
        "    total_steps = epochs_full * math.ceil(samples_per_epoch/batch_size)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=total_steps, eta_min=1e-5)\n",
        "    print('Phase 2: training full model for', epochs_full, 'epochs', flush=True)\n",
        "    warmup_steps = int(steps_warmup)\n",
        "    for ep in range(epochs_full):\n",
        "        model.train()\n",
        "        t0=time.time()\n",
        "        iters = math.ceil(samples_per_epoch/batch_size)\n",
        "        it = 0\n",
        "        for xb,yb in dl_tr:\n",
        "            xb = xb.to(device, non_blocking=True); yb = yb.to(device, non_blocking=True)\n",
        "            # warmup lr\n",
        "            if step <= warmup_steps:\n",
        "                lr_now = 1e-6 + (lr_all - 1e-6) * (step / max(1,warmup_steps))\n",
        "                for pg in opt.param_groups: pg['lr'] = lr_now\n",
        "            with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
        "                logits = model(xb)\n",
        "                loss = loss_fn(logits, yb)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(opt)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            ema.update(model)\n",
        "            if step > warmup_steps:\n",
        "                sched.step()\n",
        "            step+=1\n",
        "            it+=1\n",
        "            if it>=iters:\n",
        "                break\n",
        "            if it%200==0:\n",
        "                lr_disp = opt.param_groups[0]['lr']\n",
        "                print(f'Full ep{ep} it{it}/{iters} loss={loss.item():.4f} lr={lr_disp:.2e}', flush=True)\n",
        "        acc = run_val()\n",
        "        print(f'Full epoch {ep} val_acc={acc:.4f} elapsed={time.time()-t0:.1f}s', flush=True)\n",
        "        if acc>best_acc:\n",
        "            best_acc=acc\n",
        "            torch.save({'model': ema.module.state_dict(), 'acc': acc, 'ep': (ep)}, Path(out_dir)/'best.pt')\n",
        "    # save final\n",
        "    torch.save({'model': ema.module.state_dict(), 'acc': best_acc}, Path(out_dir)/'last.pt')\n",
        "    with open(Path(out_dir)/'id2u.json','w') as f:\n",
        "        json.dump(id2u, f, ensure_ascii=False)\n",
        "    print('Training done. Best val_acc=', best_acc, 'artifacts in', out_dir, flush=True)\n",
        "    return str(Path(out_dir)/'best.pt')\n",
        "\n",
        "print('v9 classifier training utilities ready:')\n",
        "print('- Run: best_ckpt = train_classifier(epochs_head=2, epochs_full=8, batch_size=256)')\n",
        "print('- Then integrate into detection pipeline: crop -> model logits -> softmax; gate by prob>=0.45 and dedup as before.')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v9 classifier training utilities ready:\n- Run: best_ckpt = train_classifier(epochs_head=2, epochs_full=8, batch_size=256)\n- Then integrate into detection pipeline: crop -> model logits -> softmax; gate by prob>=0.45 and dedup as before.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ]
    },
    {
      "id": "7632b292-5a8e-46e3-b33b-30be98e8e116",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# v9 driver: launch supervised crop classifier training with auto batch fallback\n",
        "import torch, time\n",
        "print('Starting supervised classifier training (convnext_small) ...', flush=True)\n",
        "# Enable TF32 for speed/stability per expert advice\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision('high')\n",
        "torch.backends.cudnn.benchmark = True\n",
        "# Start with safer batches to avoid wasting time on OOM at large sizes\n",
        "bs_list = [128, 96, 64]\n",
        "best_ckpt = None\n",
        "ooms = []\n",
        "for bs in bs_list:\n",
        "    try:\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        print(f'Trying batch_size={bs} ...', flush=True)\n",
        "        best_ckpt = train_classifier(epochs_head=2, epochs_full=8, batch_size=bs, num_workers=8, samples_per_epoch=160_000, out_dir='artifacts/cls_convnext_small')\n",
        "        print('Training finished with batch_size=', bs, '->', best_ckpt, flush=True)\n",
        "        break\n",
        "    except RuntimeError as e:\n",
        "        msg = str(e)\n",
        "        if 'CUDA out of memory' in msg or 'CUDAMemoryError' in msg:\n",
        "            print(f'OOM at batch_size={bs}. Reducing batch...', flush=True)\n",
        "            ooms.append((bs, msg[:200]))\n",
        "            continue\n",
        "        else:\n",
        "            raise\n",
        "if best_ckpt is None:\n",
        "    raise RuntimeError(f'All batch sizes failed. OOMs: {ooms[:3]} ...')\n",
        "print('Best checkpoint:', best_ckpt, flush=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting supervised classifier training (convnext_small) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying batch_size=128 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase 1: training head for 2 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_7021/2091364392.py:132: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(device=='cuda'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_7021/2091364392.py:159: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head ep0 it0 loss=8.5947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head ep0 it200 loss=8.3531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head ep0 it400 loss=9.0206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head ep0 it600 loss=8.5483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head ep0 it800 loss=7.4343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head ep0 it1000 loss=7.9846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head ep0 it1200 loss=7.2566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_7021/2091364392.py:144: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head epoch 0 val_acc=0.4116 elapsed=614.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head ep1 it0 loss=8.1001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head ep1 it200 loss=7.3557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head ep1 it400 loss=7.6340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head ep1 it600 loss=7.8747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head ep1 it800 loss=7.3639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head ep1 it1000 loss=7.5143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head ep1 it1200 loss=7.7730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head epoch 1 val_acc=0.4702 elapsed=610.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase 2: training full model for 8 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_7021/2091364392.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full ep0 it200/1250 loss=7.3283 lr=5.00e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full ep0 it400/1250 loss=6.8937 lr=4.98e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full ep0 it600/1250 loss=4.6939 lr=4.96e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full ep0 it800/1250 loss=3.4583 lr=4.92e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full ep0 it1000/1250 loss=2.7400 lr=4.88e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full ep0 it1200/1250 loss=2.5390 lr=4.83e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full epoch 0 val_acc=0.3902 elapsed=661.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full ep1 it200/1250 loss=2.2542 lr=4.75e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full ep1 it400/1250 loss=2.0767 lr=4.68e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full ep1 it600/1250 loss=1.8703 lr=4.60e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full ep1 it800/1250 loss=1.7180 lr=4.51e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full ep1 it1000/1250 loss=1.6815 lr=4.41e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full ep1 it1200/1250 loss=1.7369 lr=4.31e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full epoch 1 val_acc=0.9284 elapsed=651.0s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m     torch.cuda.empty_cache()\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTrying batch_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m'\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m best_ckpt = \u001b[43mtrain_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs_head\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs_full\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m160_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43martifacts/cls_convnext_small\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mTraining finished with batch_size=\u001b[39m\u001b[33m'\u001b[39m, bs, \u001b[33m'\u001b[39m\u001b[33m->\u001b[39m\u001b[33m'\u001b[39m, best_ckpt, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 203\u001b[39m, in \u001b[36mtrain_classifier\u001b[39m\u001b[34m(epochs_head, epochs_full, batch_size, num_workers, lr_head, lr_all, wd, label_smoothing, steps_warmup, samples_per_epoch, out_dir)\u001b[39m\n\u001b[32m    201\u001b[39m     loss = loss_fn(logits, yb)\n\u001b[32m    202\u001b[39m opt.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m scaler.unscale_(opt)\n\u001b[32m    205\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/_tensor.py:521\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    513\u001b[39m         Tensor.backward,\n\u001b[32m    514\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    519\u001b[39m         inputs=inputs,\n\u001b[32m    520\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/autograd/__init__.py:289\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    284\u001b[39m     retain_graph = create_graph\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/autograd/graph.py:769\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    767\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    768\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    770\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "e035a3b1-d127-405c-ac49-1a4393a7ba1c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# v9 inference: YOLO single-class detector + supervised convnext_small classifier\n",
        "import os, time, json, math, gc\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision.transforms as T\n",
        "import timm\n",
        "from ultralytics import YOLO\n",
        "\n",
        "@torch.no_grad()\n",
        "def load_cls_model(ckpt_path='artifacts/cls_convnext_small/best.pt', nc=None, device='cuda'):\n",
        "    d = torch.load(ckpt_path, map_location='cpu')\n",
        "    if nc is None:\n",
        "        id2u_local = json.loads(Path('artifacts/id2u.json').read_text())\n",
        "        nc = len(id2u_local)\n",
        "    model = timm.create_model('convnext_small', pretrained=False, num_classes=nc)\n",
        "    model.load_state_dict(d['model'], strict=True)\n",
        "    model.eval().to(device)\n",
        "    return model\n",
        "\n",
        "def preprocess_crop_for_cls(im: Image.Image, x:int, y:int, w:int, h:int, pad_frac:float=0.30, out_size:int=224):\n",
        "    W,H = im.size\n",
        "    cx = x + w/2.0; cy = y + h/2.0\n",
        "    pw = max(2, int(round(w*pad_frac))); ph = max(2, int(round(h*pad_frac)))\n",
        "    x1 = max(0, int(round(cx - w/2 - pw))); y1 = max(0, int(round(cy - h/2 - ph)))\n",
        "    x2 = min(W-1, int(round(cx + w/2 + pw))); y2 = min(H-1, int(round(cy + h/2 + ph)))\n",
        "    crop = im.crop((x1, y1, x2, y2)).convert('L')\n",
        "    arr = np.array(crop)\n",
        "    h0,w0 = arr.shape[:2]; m = max(h0,w0)\n",
        "    pad_top = (m-h0)//2; pad_bottom = m-h0-pad_top\n",
        "    pad_left = (m-w0)//2; pad_right = m-w0-pad_left\n",
        "    arr = np.pad(arr, ((pad_top,pad_bottom),(pad_left,pad_right)), mode='constant', constant_values=0)\n",
        "    crop = Image.fromarray(arr).resize((out_size,out_size), resample=Image.BILINEAR)\n",
        "    img3 = Image.merge('RGB', (crop,crop,crop))\n",
        "    return img3\n",
        "\n",
        "class ImgNetNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        mean = torch.tensor([0.485,0.456,0.406]).view(1,3,1,1)\n",
        "        std = torch.tensor([0.229,0.224,0.225]).view(1,3,1,1)\n",
        "        self.register_buffer('mean', mean, persistent=False)\n",
        "        self.register_buffer('std', std, persistent=False)\n",
        "    def forward(self, x):\n",
        "        return (x - self.mean) / self.std\n",
        "\n",
        "@torch.no_grad()\n",
        "def classify_crops_batch(model, crops: List[Image.Image], device='cuda', batch_size=512):\n",
        "    tfm = T.ToTensor()\n",
        "    norm = ImgNetNorm().to(device)\n",
        "    probs_all = []\n",
        "    for i in range(0, len(crops), batch_size):\n",
        "        batch_imgs = crops[i:i+batch_size]\n",
        "        if not batch_imgs:\n",
        "            continue\n",
        "        t = torch.stack([tfm(img) for img in batch_imgs], 0).to(device, non_blocking=True)\n",
        "        t = norm(t)\n",
        "        with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
        "            logits = model(t)\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "        probs_all.append(probs.detach().cpu())\n",
        "    if probs_all:\n",
        "        return torch.cat(probs_all, 0).numpy()\n",
        "    return np.zeros((0,1), dtype=np.float32)\n",
        "\n",
        "def _nms_iou_xyxy(boxes: np.ndarray, scores: np.ndarray, iou_thr: float=0.75, limit: Optional[int]=None) -> List[int]:\n",
        "    if boxes.size == 0:\n",
        "        return []\n",
        "    x1 = boxes[:,0]; y1 = boxes[:,1]; x2 = boxes[:,2]; y2 = boxes[:,3]\n",
        "    areas = (np.maximum(0, x2 - x1 + 1) * np.maximum(0, y2 - y1 + 1))\n",
        "    order = scores.argsort()[::-1]\n",
        "    keep = []\n",
        "    while order.size > 0:\n",
        "        i = order[0]\n",
        "        keep.append(int(i))\n",
        "        if limit is not None and len(keep) >= limit:\n",
        "            break\n",
        "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
        "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
        "        inter = w * h\n",
        "        ovr = inter / (areas[i] + areas[order[1:]] - inter + 1e-9)\n",
        "        inds = np.where(ovr <= iou_thr)[0]\n",
        "        order = order[inds + 1]\n",
        "    return keep\n",
        "\n",
        "@torch.no_grad()\n",
        "def two_stage_supervised_submission(det_weights='runs/yolo8n_kuz_single/weights/best.pt',\n",
        "                                   cls_ckpt='artifacts/cls_convnext_small/best.pt',\n",
        "                                   sizes=(832,960,1024),\n",
        "                                   conf=0.08,\n",
        "                                   iou=0.65,\n",
        "                                   union_iou=0.75,\n",
        "                                   max_det=4000,\n",
        "                                   pad_frac=0.30,\n",
        "                                   crop_size=224,\n",
        "                                   prob_floor=0.45,\n",
        "                                   dyn_pct=85,\n",
        "                                   dyn_clamp=(0.46, 0.58),\n",
        "                                   ambiguity_delta=0.02,\n",
        "                                   tiny_filter=5,\n",
        "                                   save_name='submission_v9.csv',\n",
        "                                   device_cls='cuda',\n",
        "                                   predict_half=True,\n",
        "                                   det_device=0):\n",
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = os.environ.get('PYTORCH_CUDA_ALLOC_CONF','expandable_segments:True')\n",
        "    id2u_local = json.loads(Path('artifacts/id2u.json').read_text())\n",
        "    model = load_cls_model(cls_ckpt, nc=len(id2u_local), device=device_cls)\n",
        "    det = YOLO(det_weights)\n",
        "    ss = pd.read_csv('sample_submission.csv')\n",
        "    image_ids = ss['image_id'].tolist()\n",
        "    img_paths = []\n",
        "    for img_id in image_ids:\n",
        "        p = Path('test_images')/f'{img_id}.jpg'\n",
        "        if not p.exists():\n",
        "            alts = list(Path('test_images').glob(f'{img_id}.*'))\n",
        "            if alts: p = alts[0]\n",
        "        img_paths.append(str(p))\n",
        "    # run detection at multiple sizes\n",
        "    def detect_at_size(s:int):\n",
        "        return det.predict(source=img_paths, imgsz=int(s), conf=float(conf), iou=float(iou), max_det=int(max_det), augment=True, device=det_device, stream=True, verbose=False, batch=1, half=predict_half)\n",
        "    print('Running detector at sizes:', sizes, flush=True)\n",
        "    res_all = [list(detect_at_size(s)) for s in sizes]\n",
        "    rows = []; t0=time.time()\n",
        "    for i, items in enumerate(zip(image_ids, img_paths, *res_all)):\n",
        "        img_id = items[0]; img_path = items[1]; rlist = items[2:]\n",
        "        if i % 25 == 0:\n",
        "            print(f'Infer {i}/{len(image_ids)} elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "        labels_out = []\n",
        "        with Image.open(img_path) as im:\n",
        "            W,H = im.size\n",
        "            # union NMS\n",
        "            all_xyxy = []; all_scores = []\n",
        "            for r in rlist:\n",
        "                if r is not None and hasattr(r,'boxes') and r.boxes is not None and len(r.boxes)>0:\n",
        "                    b = r.boxes\n",
        "                    xyxy = b.xyxy.cpu().numpy()\n",
        "                    confs = b.conf.cpu().numpy() if getattr(b,'conf',None) is not None else np.ones((xyxy.shape[0],), dtype=np.float32)\n",
        "                    if xyxy.size:\n",
        "                        all_xyxy.append(xyxy); all_scores.append(confs)\n",
        "            if all_xyxy:\n",
        "                xyxy_u = np.concatenate(all_xyxy, 0).astype(np.float32)\n",
        "                scores_u = np.concatenate(all_scores, 0).astype(np.float32)\n",
        "                keep = _nms_iou_xyxy(xyxy_u, scores_u, iou_thr=float(union_iou), limit=int(max_det))\n",
        "                xyxy_u = xyxy_u[keep]; scores_u = scores_u[keep]\n",
        "                # build crops\n",
        "                crops = []; centers = []; ws = []; hs = []\n",
        "                for (x1,y1,x2,y2) in xyxy_u:\n",
        "                    x = int(round(x1)); y = int(round(y1)); w = int(round(x2-x1)); h = int(round(y2-y1))\n",
        "                    if w < tiny_filter or h < tiny_filter:\n",
        "                        continue\n",
        "                    img3 = preprocess_crop_for_cls(im, x,y,w,h, pad_frac=pad_frac, out_size=crop_size)\n",
        "                    crops.append(img3)\n",
        "                    cx = int(round((x1+x2)/2.0)); cy = int(round((y1+y2)/2.0))\n",
        "                    cx = max(0, min(cx, W-1)); cy = max(0, min(cy, H-1))\n",
        "                    centers.append((cx,cy)); ws.append(w); hs.append(h)\n",
        "                if crops:\n",
        "                    probs = classify_crops_batch(model, crops, device=device_cls, batch_size=512)  # (N, C)\n",
        "                    top2 = np.partition(-probs, 2, axis=1)[:, :2]  # negative for descending\n",
        "                    top1_prob = -top2[:,0]\n",
        "                    # dynamic threshold\n",
        "                    if len(top1_prob) > 0:\n",
        "                        t_img = float(np.percentile(top1_prob, dyn_pct))\n",
        "                        t_img = float(np.clip(t_img, dyn_clamp[0], dyn_clamp[1]))\n",
        "                        thr = max(prob_floor, t_img)\n",
        "                    else:\n",
        "                        thr = prob_floor\n",
        "                    preds = []  # (u, cx, cy, p, w, h, gap)\n",
        "                    for j in range(probs.shape[0]):\n",
        "                        p = probs[j]\n",
        "                        k1 = int(p.argmax()); v1 = float(p[k1])\n",
        "                        # compute top2\n",
        "                        k2 = int(np.argpartition(p, -2)[-2]) if p.shape[0] >= 2 else k1\n",
        "                        v2 = float(p[k2]) if p.shape[0] >= 2 else 0.0\n",
        "                        gap = v1 - v2\n",
        "                        local_thr = thr + (ambiguity_delta if gap < 0.04 else 0.0)\n",
        "                        if v1 >= local_thr:\n",
        "                            u = id2u_local[str(k1)] if isinstance(id2u_local, dict) else id2u_local[k1]\n",
        "                            cx,cy = centers[j]\n",
        "                            preds.append((u, cx, cy, v1, ws[j], hs[j], gap))\n",
        "                    # unicode-aware size/grid dedup\n",
        "                    if preds:\n",
        "                        kept = {}  # key: (u, gx, gy) -> best (by prob)\n",
        "                        for (u, cx, cy, p1, w, h, gap) in preds:\n",
        "                            cell_size = max(7, int(max(1, min(w,h))//4))\n",
        "                            gx = cx // cell_size; gy = cy // cell_size\n",
        "                            key = (u, gx, gy)\n",
        "                            if key not in kept or p1 > kept[key][3]:\n",
        "                                kept[key] = (u, cx, cy, p1)\n",
        "                        for u, cx, cy, p1 in kept.values():\n",
        "                            labels_out.extend([u, str(cx), str(cy)])\n",
        "        rows.append({'image_id': img_id, 'labels': ' '.join(labels_out)})\n",
        "    sub = pd.DataFrame(rows)\n",
        "    sub.to_csv(save_name, index=False)\n",
        "    print('Saved', save_name, 'shape', sub.shape, flush=True)\n",
        "    return sub\n",
        "\n",
        "print('v9 supervised inference utilities ready: two_stage_supervised_submission(...)')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v9 supervised inference utilities ready: two_stage_supervised_submission(...)\n"
          ]
        }
      ]
    },
    {
      "id": "0e409c50-bc9d-4756-b814-79f3a88c86bc",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Driver: v9 supervised two-stage inference to produce submission_v9.csv and submission.csv\n",
        "import shutil, pandas as pd\n",
        "print('Running v9 two-stage supervised inference ...', flush=True)\n",
        "sub = two_stage_supervised_submission(det_weights='runs/yolo8n_kuz_single/weights/best.pt',\n",
        "                                      cls_ckpt='artifacts/cls_convnext_small/best.pt',\n",
        "                                      sizes=(832,960,1024),\n",
        "                                      conf=0.08,\n",
        "                                      iou=0.65,\n",
        "                                      union_iou=0.75,\n",
        "                                      max_det=4000,\n",
        "                                      pad_frac=0.30,\n",
        "                                      crop_size=224,\n",
        "                                      prob_floor=0.45,\n",
        "                                      dyn_pct=85,\n",
        "                                      dyn_clamp=(0.46,0.58),\n",
        "                                      ambiguity_delta=0.02,\n",
        "                                      tiny_filter=5,\n",
        "                                      save_name='submission_v9.csv',\n",
        "                                      device_cls='cuda',\n",
        "                                      predict_half=True,\n",
        "                                      det_device=0)\n",
        "print('Copying submission_v9.csv -> submission.csv for scoring ...', flush=True)\n",
        "shutil.copy2('submission_v9.csv', 'submission.csv')\n",
        "print(pd.read_csv('submission.csv').head())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running v9 two-stage supervised inference ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running detector at sizes: (832, 960, 1024)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infer 0/361 elapsed 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_7021/3263789133.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infer 25/361 elapsed 101.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infer 50/361 elapsed 193.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infer 75/361 elapsed 277.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infer 100/361 elapsed 363.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infer 125/361 elapsed 466.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infer 150/361 elapsed 534.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infer 175/361 elapsed 612.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infer 200/361 elapsed 666.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infer 225/361 elapsed 745.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infer 250/361 elapsed 804.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infer 275/361 elapsed 861.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infer 300/361 elapsed 893.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infer 325/361 elapsed 945.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infer 350/361 elapsed 999.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_v9.csv shape (361, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying submission_v9.csv -> submission.csv for scoring ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            image_id                                             labels\n0        umgy007-028  U+69D8 910 1691 U+904A 528 2362 U+4F4D 715 214...\n1        hnsd004-026  U+6253 687 2506 U+79CB 892 1301 U+679C 309 475...\n2  200003076_00034_2  U+91D1 1937 1577 U+6E05 1929 2335 U+6210 1943 ...\n3        brsk001-014  U+64AD 1990 816 U+25CB 955 1016 U+76E7 602 289...\n4  200014685-00003_2  U+5AC1 936 2303 U+4E39 1182 1482 U+4ED6 702 19...\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}