{
  "cells": [
    {
      "id": "30bf5db5-c0a4-49df-aeb8-6b9c6ac11b7c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Config & imports for stable, deterministic training\n",
        "import os, sys, gc, time, math, json, random, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Global config\n",
        "SEEDS = [7, 42, 2025]\n",
        "PRIMARY_SEED = 42\n",
        "N_FOLDS = 8\n",
        "N_THREADS = 6  # cap threads for stability\n",
        "HEARTBEAT_PATH = 'heartbeat.log'\n",
        "\n",
        "def seed_everything(seed: int = PRIMARY_SEED):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    try:\n",
        "        import torch\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def heartbeat(msg: str):\n",
        "    try:\n",
        "        with open(HEARTBEAT_PATH, 'a') as f:\n",
        "            f.write(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {msg}\\n\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Silence warnings/logs\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
        "os.environ['TQDM_DISABLE'] = '1'\n",
        "os.environ['PIP_DISABLE_PIP_VERSION_CHECK'] = '1'\n",
        "os.environ['OMP_NUM_THREADS'] = str(N_THREADS)\n",
        "os.environ['OPENBLAS_NUM_THREADS'] = str(N_THREADS)\n",
        "os.environ['MKL_NUM_THREADS'] = str(N_THREADS)\n",
        "os.environ['VECLIB_MAXIMUM_THREADS'] = str(N_THREADS)\n",
        "os.environ['NUMEXPR_NUM_THREADS'] = str(N_THREADS)\n",
        "\n",
        "# Pandas display\n",
        "pd.set_option('display.max_columns', 200)\n",
        "pd.set_option('display.width', 200)\n",
        "\n",
        "seed_everything(PRIMARY_SEED)\n",
        "print('Config ready: SEEDS', SEEDS, '| N_FOLDS', N_FOLDS, '| N_THREADS', N_THREADS)\n",
        "heartbeat('CONFIG INITIALIZED')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config ready: SEEDS [7, 42, 2025] | N_FOLDS 8 | N_THREADS 6\n"
          ]
        }
      ]
    },
    {
      "id": "4d714910-9adc-4bc0-83db-a3fd14d0b8bc",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Utils: feature engineering, grouping, mm-lite features, encoders\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "def cell_volume(a, b, c, alpha_deg, beta_deg, gamma_deg):\n",
        "    alpha = np.deg2rad(alpha_deg); beta = np.deg2rad(beta_deg); gamma = np.deg2rad(gamma_deg)\n",
        "    ca, cb, cg = np.cos(alpha), np.cos(beta), np.cos(gamma)\n",
        "    term = 1 + 2*ca*cb*cg - ca**2 - cb**2 - cg**2\n",
        "    term = np.clip(term, 0, None)\n",
        "    return a * b * c * np.sqrt(term)\n",
        "\n",
        "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    a, b, c = df['lattice_vector_1_ang'], df['lattice_vector_2_ang'], df['lattice_vector_3_ang']\n",
        "    alpha, beta, gamma = df['lattice_angle_alpha_degree'], df['lattice_angle_beta_degree'], df['lattice_angle_gamma_degree']\n",
        "    vol = cell_volume(a, b, c, alpha, beta, gamma)\n",
        "    df['cell_volume'] = vol\n",
        "    df['volume_per_atom'] = vol / df['number_of_total_atoms']\n",
        "    df['atoms_per_volume'] = df['number_of_total_atoms'] / vol.replace(0, np.nan)\n",
        "    # angles info\n",
        "    for ang, s in [('alpha', alpha), ('beta', beta), ('gamma', gamma)]:\n",
        "        df[f'cos_{ang}'] = np.cos(np.deg2rad(s))\n",
        "        df[f'abs_{ang}_dev90'] = np.abs(s - 90.0)\n",
        "    df['orthorhombicity'] = df[['abs_alpha_dev90','abs_beta_dev90','abs_gamma_dev90']].sum(axis=1)\n",
        "    # fractions\n",
        "    for el in ['al','ga','in']:\n",
        "        df[f'percent_atom_{el}'] = df[f'percent_atom_{el}'].astype(float)\n",
        "        df[f'frac_{el}'] = df[f'percent_atom_{el}'] / 100.0\n",
        "    df['percent_atom_o'] = 100.0 - (df['percent_atom_al'] + df['percent_atom_ga'] + df['percent_atom_in'])\n",
        "    df['frac_o'] = df['percent_atom_o'] / 100.0\n",
        "    # cation weights\n",
        "    frac_cat = (df['frac_al'] + df['frac_ga'] + df['frac_in']).replace(0, np.nan)\n",
        "    df['w_al'] = (df['frac_al']/frac_cat).fillna(0.0)\n",
        "    df['w_ga'] = (df['frac_ga']/frac_cat).fillna(0.0)\n",
        "    df['w_in'] = (df['frac_in']/frac_cat).fillna(0.0)\n",
        "    # Vegard baseline and bowing\n",
        "    df['vegard_bg'] = 8.8*df['w_al'] + 4.8*df['w_ga'] + 2.9*df['w_in']\n",
        "    df['bow_in'] = df['w_in']*(1.0 - df['w_in'])\n",
        "    df['bow_ga'] = df['w_ga']*(1.0 - df['w_ga'])\n",
        "    # logs\n",
        "    df['log_vpa'] = np.log1p(df['volume_per_atom'].clip(lower=0))\n",
        "    df['log_apv'] = np.log1p(df['atoms_per_volume'].clip(lower=0))\n",
        "    df['log_oc'] = np.log1p((df['frac_o']/(frac_cat+1e-9)).clip(lower=0))\n",
        "    df['log_in_over_al'] = np.log1p(((df['frac_in']+1e-6)/(df['frac_al']+1e-6)).clip(lower=0))\n",
        "    # reduced lattice\n",
        "    l = df['cell_volume'].replace(0, np.nan).pow(1/3)\n",
        "    df['a_red'] = df['lattice_vector_1_ang']/l\n",
        "    df['b_red'] = df['lattice_vector_2_ang']/l\n",
        "    df['c_red'] = df['lattice_vector_3_ang']/l\n",
        "    df.replace([np.inf,-np.inf], np.nan, inplace=True)\n",
        "    return df\n",
        "\n",
        "def compute_stoich_groups(df: pd.DataFrame):\n",
        "    N = np.rint(df['number_of_total_atoms']/5.0).astype(int)\n",
        "    n_cat = 2 * N\n",
        "    frac_al = df['percent_atom_al']/100.0\n",
        "    frac_ga = df['percent_atom_ga']/100.0\n",
        "    frac_in = df['percent_atom_in']/100.0\n",
        "    frac_cations_total = (frac_al + frac_ga + frac_in).replace(0, np.nan)\n",
        "    w_al = (frac_al / frac_cations_total).clip(0,1).fillna(0)\n",
        "    w_ga = (frac_ga / frac_cations_total).clip(0,1).fillna(0)\n",
        "    w_in = (1.0 - w_al - w_ga).clip(0,1)\n",
        "    n_al = np.rint(n_cat * w_al).astype(int)\n",
        "    n_ga = np.rint(n_cat * w_ga).astype(int)\n",
        "    n_in = (n_cat - n_al - n_ga).astype(int)\n",
        "    n_o = 3 * N\n",
        "    key = pd.Series(list(zip(N, n_al, n_ga, n_in))).astype(str)\n",
        "    return key, N, n_al, n_ga, n_in, n_o\n",
        "\n",
        "# mm-lite and extra low-cost features\n",
        "def add_mm_lite_and_extras(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    # mm-lite stoichiometry norms\n",
        "    fa, fg, fi, fo = df['frac_al'], df['frac_ga'], df['frac_in'], df['frac_o']\n",
        "    arr = np.stack([fa, fg, fi, fo], axis=1)\n",
        "    df['sto_s2'] = np.sqrt((arr**2).sum(axis=1))\n",
        "    df['sto_s3'] = np.cbrt(np.clip((arr**3).sum(axis=1), a_min=0, a_max=None))\n",
        "    df['sto_s5'] = np.clip((arr**5).sum(axis=1), a_min=0, a_max=None)**(1/5)\n",
        "    df['frac_max'] = arr.max(axis=1); df['frac_min'] = arr.min(axis=1); df['frac_range'] = df['frac_max'] - df['frac_min']\n",
        "    # cation mix stats\n",
        "    w = np.stack([df['w_al'], df['w_ga'], df['w_in']], axis=1)\n",
        "    df['w_max'] = w.max(axis=1); df['w_min'] = w.min(axis=1); df['w_range'] = df['w_max'] - df['w_min']\n",
        "    df['hhi_cation2'] = (w**2).sum(axis=1)\n",
        "    # valence-orbital proxies\n",
        "    s_map = {'Al':2,'Ga':2,'In':2,'O':2}; p_map = {'Al':1,'Ga':1,'In':1,'O':4}\n",
        "    s_cat = df['w_al']*s_map['Al'] + df['w_ga']*s_map['Ga'] + df['w_in']*s_map['In']\n",
        "    p_cat = df['w_al']*p_map['Al'] + df['w_ga']*p_map['Ga'] + df['w_in']*p_map['In']\n",
        "    df['vo_cat_s_mean'] = s_cat; df['vo_cat_p_mean'] = p_cat\n",
        "    df['vo_cat_p_frac'] = p_cat / (s_cat + p_cat + 1e-9); df['vo_cat_p_minus_s'] = p_cat - s_cat\n",
        "    s_tot = fa*s_map['Al'] + fg*s_map['Ga'] + fi*s_map['In'] + fo*s_map['O']\n",
        "    p_tot = fa*p_map['Al'] + fg*p_map['Ga'] + fi*p_map['In'] + fo*p_map['O']\n",
        "    df['vo_tot_s_mean'] = s_tot; df['vo_tot_p_mean'] = p_tot\n",
        "    df['vo_tot_p_frac'] = p_tot / (s_tot + p_tot + 1e-9); df['vo_tot_p_minus_s'] = p_tot - s_tot\n",
        "    # physics-driven contrasts\n",
        "    props = {\n",
        "        'chi_pauling': {'Al':1.61,'Ga':1.81,'In':1.78,'O':3.44},\n",
        "        'ionic_radius': {'Al':0.535,'Ga':0.62,'In':0.80,'O':1.38},\n",
        "        'Z': {'Al':13,'Ga':31,'In':49,'O':8},\n",
        "        'period': {'Al':3,'Ga':4,'In':5,'O':2},\n",
        "        'group': {'Al':13,'Ga':13,'In':13,'O':16},\n",
        "        'first_ionization_energy': {'Al':5.986,'Ga':5.999,'In':5.786,'O':13.618}\n",
        "    }\n",
        "    for name, tbl in props.items():\n",
        "        ca, cg, ci, co = tbl['Al'], tbl['Ga'], tbl['In'], tbl['O']\n",
        "        wmean = df['w_al']*ca + df['w_ga']*cg + df['w_in']*ci\n",
        "        df[f'catw_{name}_mean'] = wmean\n",
        "        df[f'catw_{name}_var'] = (df['w_al']*(ca-wmean)**2 + df['w_ga']*(cg-wmean)**2 + df['w_in']*(ci-wmean)**2)\n",
        "    df['o_minus_catw_chi_pauling'] = 3.44 - df['catw_chi_pauling_mean']\n",
        "    df['o_minus_catw_ionic_radius'] = 1.38 - df['catw_ionic_radius_mean']\n",
        "    # low-cost extras\n",
        "    df['inv_vpa'] = 1.0 / (df['volume_per_atom'] + 1e-6)\n",
        "    df['vegard_bg_sq'] = df['vegard_bg']**2\n",
        "    df['sqrt_vegard'] = np.sqrt(np.clip(df['vegard_bg'], a_min=0, a_max=None))\n",
        "    df['log1p_vegard'] = np.log1p(np.clip(df['vegard_bg'], a_min=0, a_max=None))\n",
        "    # pairwise cation-property ratios\n",
        "    df['ratio_Z_over_ir'] = df['catw_Z_mean'] / (df['catw_ionic_radius_mean'] + 1e-9)\n",
        "    df['ratio_chi_over_ir'] = df['catw_chi_pauling_mean'] / (df['catw_ionic_radius_mean'] + 1e-9)\n",
        "    df['ratio_fie_over_chi'] = df['catw_first_ionization_energy_mean'] / (df['catw_chi_pauling_mean'] + 1e-9)\n",
        "    # N interactions (assumes N exists later; safe fill if missing)\n",
        "    if 'N' in df.columns:\n",
        "        for col in ['vegard_bg','w_in','catw_chi_pauling_mean','catw_ionic_radius_mean']:\n",
        "            df[f'N_x_{col}'] = df['N'] * df[col]\n",
        "    # cation weight ratios\n",
        "    eps = 1e-6\n",
        "    df['w_al_over_in'] = (df['w_al']+eps)/(df['w_in']+eps)\n",
        "    df['w_ga_over_in'] = (df['w_ga']+eps)/(df['w_in']+eps)\n",
        "    df['w_al_over_ga'] = (df['w_al']+eps)/(df['w_ga']+eps)\n",
        "    df['log1p_w_al_over_in'] = np.log1p(df['w_al_over_in'])\n",
        "    df['log1p_w_ga_over_in'] = np.log1p(df['w_ga_over_in'])\n",
        "    df['log1p_w_al_over_ga'] = np.log1p(df['w_al_over_ga'])\n",
        "    # diffs\n",
        "    df['diff_Z_minus_period'] = df['catw_Z_mean'] - df['catw_period_mean']\n",
        "\n",
        "    # === High-signal physics features (added) ===\n",
        "    # Cation entropy and effective cation count\n",
        "    eps = 1e-12\n",
        "    w_al = df['w_al'].clip(0,1); w_ga = df['w_ga'].clip(0,1); w_in = df['w_in'].clip(0,1)\n",
        "    H_cation = -(w_al*np.log(w_al+eps) + w_ga*np.log(w_ga+eps) + w_in*np.log(w_in+eps))\n",
        "    df['H_cation'] = H_cation\n",
        "    df['eff_cations'] = np.exp(H_cation)\n",
        "    # Simplex geometry: distances to corners and center\n",
        "    center = np.array([1/3, 1/3, 1/3])\n",
        "    W = np.stack([w_al, w_ga, w_in], axis=1)\n",
        "    df['dist_l2_center'] = np.linalg.norm(W - center, axis=1)\n",
        "    df['dist_l1_center'] = np.abs(W - center).sum(axis=1)\n",
        "    # distances to Al/Ga/In corners\n",
        "    corners = {'al': np.array([1,0,0]), 'ga': np.array([0,1,0]), 'in': np.array([0,0,1])}\n",
        "    for k, v in corners.items():\n",
        "        diff = W - v\n",
        "        df[f'dist_l2_{k}_corner'] = np.linalg.norm(diff, axis=1)\n",
        "        df[f'dist_l1_{k}_corner'] = np.abs(diff).sum(axis=1)\n",
        "    # weight polys/interactions\n",
        "    df['w_al_sq'] = w_al**2; df['w_ga_sq'] = w_ga**2; df['w_in_sq'] = w_in**2\n",
        "    df['w_al_ga'] = w_al*w_ga; df['w_al_in'] = w_al*w_in; df['w_ga_in'] = w_ga*w_in\n",
        "    df['sum_w_cu'] = w_al**3 + w_ga**3 + w_in**3\n",
        "    # Ionic radius / tolerance proxies\n",
        "    r_al, r_ga, r_in, r_o = 0.535, 0.62, 0.80, 1.38\n",
        "    rM = w_al*r_al + w_ga*r_ga + w_in*r_in\n",
        "    df['rM'] = rM\n",
        "    df['rM_var'] = (w_al*(r_al - rM)**2 + w_ga*(r_ga - rM)**2 + w_in*(r_in - rM)**2)\n",
        "    df['t_ratio'] = rM / r_o\n",
        "    df['t_dev'] = np.abs(df['t_ratio'] - 1.0)\n",
        "    # Charge density proxies\n",
        "    if 'N' in df.columns:\n",
        "        vol = (df['cell_volume'].replace(0, np.nan)).astype(float)\n",
        "        df['charge_density_6N'] = (6.0 * df['N']) / (vol + 1e-9)\n",
        "        df['charge_density_3N'] = (3.0 * df['N']) / (vol + 1e-9)\n",
        "    else:\n",
        "        df['charge_density_6N'] = 0.0; df['charge_density_3N'] = 0.0\n",
        "    # Vegard interactions and contrasts\n",
        "    df['veg_w_al'] = df['vegard_bg'] * w_al\n",
        "    df['veg_w_ga'] = df['vegard_bg'] * w_ga\n",
        "    df['veg_w_in'] = df['vegard_bg'] * w_in\n",
        "    df['veg_minus_catw_chi'] = df['vegard_bg'] - df['catw_chi_pauling_mean']\n",
        "\n",
        "    df.replace([np.inf,-np.inf], np.nan, inplace=True)\n",
        "    return df\n",
        "\n",
        "def lattice_system_from_sgnum(sgnum: int) -> int:\n",
        "    n = int(sgnum)\n",
        "    if n <= 2: return 1\n",
        "    if n <= 15: return 2\n",
        "    if n <= 74: return 3\n",
        "    if n <= 142: return 4\n",
        "    if n <= 167: return 5\n",
        "    if n <= 194: return 6\n",
        "    return 7\n",
        "\n",
        "def build_stratified_group_folds(train_df: pd.DataFrame, gkey: pd.Series, y: pd.Series, n_splits: int = 8, seed: int = 42) -> np.ndarray:\n",
        "    gmean = y.groupby(gkey).mean()\n",
        "    gbin = pd.qcut(gmean, q=10, labels=False, duplicates='drop')\n",
        "    uniq = pd.DataFrame({'g': gmean.index, 'bin': gbin.values}).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "    group_to_fold = {}\n",
        "    for k, (_, val_idx) in enumerate(skf.split(uniq['g'], uniq['bin'])):\n",
        "        for g in uniq['g'].iloc[val_idx]:\n",
        "            group_to_fold[g] = k\n",
        "    fold_ids = gkey.map(group_to_fold).astype(int).values\n",
        "    return fold_ids\n",
        "\n",
        "def oof_m_estimate_te(series_cat: pd.Series, y_log: pd.Series, fold_ids: np.ndarray, m: float, return_full_map: bool = False):\n",
        "    te = np.zeros(len(series_cat), dtype=float)\n",
        "    global_mean = float(y_log.mean())\n",
        "    s_cat = series_cat.astype(str)\n",
        "    for k in np.unique(fold_ids):\n",
        "        tr = np.where(fold_ids != k)[0]; va = np.where(fold_ids == k)[0]\n",
        "        s_tr = s_cat.iloc[tr]\n",
        "        counts = s_tr.groupby(s_tr).size()\n",
        "        sums = pd.Series(y_log.iloc[tr].values, index=s_tr.index).groupby(s_tr).sum()\n",
        "        enc = (sums + m*global_mean) / (counts + m)\n",
        "        te[va] = s_cat.iloc[va].map(enc).fillna(global_mean).values\n",
        "    if return_full_map:\n",
        "        counts_all = s_cat.groupby(s_cat).size()\n",
        "        sums_all = pd.Series(y_log.values, index=s_cat.index).groupby(s_cat).sum()\n",
        "        enc_all = (sums_all + m*global_mean) / (counts_all + m)\n",
        "        return te, enc_all.to_dict(), global_mean\n",
        "    return te\n",
        "\n",
        "print('Utils ready.')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utils ready.\n"
          ]
        }
      ]
    },
    {
      "id": "7c4bf5f2-0b04-4040-80af-388dcacab416",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Centralized Ordered Target Encoding (OOF LOO) + OOF Frequency Encodings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def _safe_series(x):\n",
        "    if isinstance(x, pd.Series):\n",
        "        return x\n",
        "    return pd.Series(x)\n",
        "\n",
        "def oof_freq_encoding(train_cat: pd.Series, test_cat: pd.Series, fold_ids: np.ndarray):\n",
        "    train_cat = train_cat.astype(str)\n",
        "    test_cat = test_cat.astype(str)\n",
        "    n = len(train_cat)\n",
        "    fe_tr = np.zeros(n, dtype=float)\n",
        "    uniq_folds = np.unique(fold_ids)\n",
        "    for k in uniq_folds:\n",
        "        tr_idx = np.where(fold_ids != k)[0]\n",
        "        va_idx = np.where(fold_ids == k)[0]\n",
        "        counts = train_cat.iloc[tr_idx].value_counts(normalize=True)\n",
        "        fe_tr[va_idx] = train_cat.iloc[va_idx].map(counts).fillna(0.0).values\n",
        "    full_counts = train_cat.value_counts(normalize=True)\n",
        "    fe_te = test_cat.map(full_counts).fillna(0.0).values\n",
        "    return fe_tr, fe_te\n",
        "\n",
        "def ordered_te_oof(train_cat: pd.Series, test_cat: pd.Series, y_log: np.ndarray, fold_ids: np.ndarray, m: float, noise_std: float, min_support: int = 5, rng: np.random.RandomState | None = None):\n",
        "    # STRICT OOF per-fold encodings for train; add noise only to training rows; keep validation OOF clean; full-train map for test.\n",
        "    if rng is None:\n",
        "        rng = np.random.RandomState(42)\n",
        "    s_tr = train_cat.astype(str)\n",
        "    s_te = test_cat.astype(str)\n",
        "    y_log = pd.Series(y_log)\n",
        "    n = len(s_tr)\n",
        "    enc_tr = np.zeros(n, dtype=float)\n",
        "    uniq = np.unique(fold_ids)\n",
        "    for k in uniq:\n",
        "        tr_idx = np.where(fold_ids != k)[0]\n",
        "        va_idx = np.where(fold_ids == k)[0]\n",
        "        s_tr_k = s_tr.iloc[tr_idx]\n",
        "        y_k = y_log.iloc[tr_idx]\n",
        "        mu = float(y_k.mean())\n",
        "        stats = pd.DataFrame({'cat': s_tr_k, 'y': y_k}).groupby('cat')['y'].agg(['sum','count'])\n",
        "        enc_map = (stats['sum'] + m * mu) / (stats['count'] + m)\n",
        "        if min_support is not None and min_support > 0:\n",
        "            rare = stats['count'] < min_support\n",
        "            if rare.any():\n",
        "                enc_map.loc[rare] = mu\n",
        "        # Clean, noise-free OOF encodings for validation indices\n",
        "        enc_tr[va_idx] = s_tr.iloc[va_idx].map(enc_map).fillna(mu).values\n",
        "        # Apply noise ONLY to training rows within this fold; do not overwrite already-set validation entries\n",
        "        enc_vals = s_tr.iloc[tr_idx].map(enc_map).fillna(mu).values\n",
        "        if noise_std and noise_std > 0:\n",
        "            enc_vals = enc_vals + rng.normal(0.0, noise_std, size=enc_vals.shape)\n",
        "        # assign only where not yet set by validation (enc_tr initialized to 0.0); validation rows are non-zero now\n",
        "        mask_not_set = (enc_tr[tr_idx] == 0.0)\n",
        "        if mask_not_set.any():\n",
        "            tmp = enc_tr.copy()\n",
        "            tmp_tr = tmp[tr_idx]\n",
        "            tmp_tr[mask_not_set] = enc_vals[mask_not_set]\n",
        "            enc_tr[tr_idx] = tmp_tr\n",
        "    # Test encodings from full map (no noise)\n",
        "    mu_full = float(y_log.mean())\n",
        "    stats_full = pd.DataFrame({'cat': s_tr, 'y': y_log}).groupby('cat')['y'].agg(['sum','count'])\n",
        "    enc_map_full = (stats_full['sum'] + m * mu_full) / (stats_full['count'] + m)\n",
        "    enc_te = s_te.map(enc_map_full).fillna(mu_full).values.astype(float)\n",
        "    return enc_tr.astype(float), enc_te\n",
        "\n",
        "def add_encoded_features(X_tr: pd.DataFrame, X_te: pd.DataFrame, tr_df: pd.DataFrame, te_df: pd.DataFrame, y_log: np.ndarray, fold_ids: np.ndarray, seed: int = 42):\n",
        "    rng = np.random.RandomState(int(seed))\n",
        "    # Categories\n",
        "    sg_tr = tr_df['spacegroup'].astype(str)\n",
        "    sg_te = te_df['spacegroup'].astype(str)\n",
        "    ls_tr = tr_df['lattice_system'].astype(int).astype(str)  # treat as categorical\n",
        "    ls_te = te_df['lattice_system'].astype(int).astype(str)\n",
        "    g_tr = tr_df[['N','n_al','n_ga','n_in']].astype(int).astype(str).agg('_'.join, axis=1)\n",
        "    g_te = te_df[['N','n_al','n_ga','n_in']].astype(int).astype(str).agg('_'.join, axis=1)\n",
        "    # Nb buckets (use q=8 as per guidance)\n",
        "    Nb_tr = pd.qcut(tr_df['N'].astype(float), q=8, labels=False, duplicates='drop')\n",
        "    try:\n",
        "        _, bins = pd.qcut(tr_df['N'].astype(float), q=8, duplicates='drop', retbins=True)\n",
        "        bins = np.unique(bins)\n",
        "        Nb_te_raw = np.digitize(te_df['N'].astype(float).values, bins[1:-1], right=True)\n",
        "        Nb_te = pd.Series(Nb_te_raw, index=te_df.index)\n",
        "    except Exception:\n",
        "        Nb_te = pd.qcut(te_df['N'].astype(float), q=8, labels=False, duplicates='drop')\n",
        "    Nb_tr = Nb_tr.astype('Int64').astype(str).fillna('-1')\n",
        "    Nb_te = Nb_te.astype('Int64').astype(str).fillna('-1')\n",
        "    # Encodings per spec\n",
        "    # Use zero noise for low-cardinality features like lattice_system and Nb\n",
        "    enc_cfg = [\n",
        "        ('sg', sg_tr, sg_te, 30.0, 0.006),\n",
        "        ('group', g_tr, g_te, 14.0, 0.004),\n",
        "        ('ls', ls_tr, ls_te, 10.0, 0.0),\n",
        "        ('Nb', Nb_tr, Nb_te, 10.0, 0.0),\n",
        "    ]\n",
        "    Xtr = X_tr.copy()\n",
        "    Xte = X_te.copy()\n",
        "    meta_oof = {}  # return for stacking if needed\n",
        "    for name, cat_tr, cat_te, m, sigma in enc_cfg:\n",
        "        te_tr, te_te = ordered_te_oof(cat_tr, cat_te, y_log, fold_ids, m=m, noise_std=sigma, min_support=5, rng=rng)\n",
        "        fe_tr, fe_te = oof_freq_encoding(cat_tr, cat_te, fold_ids)\n",
        "        # For base models: DROP group encodings (they are fold-constant due to group-disjoint CV)\n",
        "        if name != 'group':\n",
        "            Xtr[f'te_{name}'] = te_tr\n",
        "            Xte[f'te_{name}'] = te_te\n",
        "            Xtr[f'fe_{name}'] = fe_tr\n",
        "            Xte[f'fe_{name}'] = fe_te\n",
        "        # Always return in meta_oof for stacking diagnostics\n",
        "        meta_oof[f'te_{name}'] = te_tr\n",
        "        meta_oof[f'fe_{name}'] = fe_tr\n",
        "    # Ensure numeric-only matrices and consistent fills\n",
        "    med = Xtr.median(numeric_only=True)\n",
        "    Xtr = Xtr.fillna(med)\n",
        "    Xte = Xte.fillna(med)\n",
        "    num_cols = list(Xtr.select_dtypes(include=[np.number]).columns)\n",
        "    Xtr = Xtr[num_cols]\n",
        "    Xte = Xte[num_cols]\n",
        "    return Xtr, Xte, meta_oof"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "id": "125f309f-36fc-494c-99fd-145f1d3b6b5b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cache cleanup to prevent drift before rebuilding\n",
        "import os, glob\n",
        "from pathlib import Path\n",
        "\n",
        "patterns = [\n",
        "    'fold_ids.npy',\n",
        "    'y.npy',\n",
        "    'X.parquet',\n",
        "    'X_test.parquet',\n",
        "    'train_fe.parquet',\n",
        "    'test_fe.parquet',\n",
        "    'features.json',\n",
        "    'stoich_groups.csv',\n",
        "    'oof_*.npy',\n",
        "    'pred_*.npy',\n",
        "    'oof_*.npz',\n",
        "    'pred_*.npz'\n",
        "]\n",
        "removed = []\n",
        "for pat in patterns:\n",
        "    for fp in glob.glob(pat):\n",
        "        try:\n",
        "            Path(fp).unlink(missing_ok=True)\n",
        "            removed.append(fp)\n",
        "        except Exception as e:\n",
        "            print('Could not remove', fp, '|', e)\n",
        "print('Removed files:', len(removed))\n",
        "print(sorted(removed)[:25], '...')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed files: 18\n['X.parquet', 'X_test.parquet', 'features.json', 'fold_ids.npy', 'oof_fe_Nb.npy', 'oof_fe_group.npy', 'oof_fe_ls.npy', 'oof_fe_sg.npy', 'oof_lgbm.npy', 'oof_te_Nb.npy', 'oof_te_group.npy', 'oof_te_ls.npy', 'oof_te_sg.npy', 'pred_lgbm_test.npy', 'stoich_groups.csv', 'test_fe.parquet', 'train_fe.parquet', 'y.npy'] ...\n"
          ]
        }
      ]
    },
    {
      "id": "d7b55276-0a2e-4ebc-8cc4-1bfbf973dad5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Folds + feature build/cache: engineer, mm-lite, groups, lattice_system; persist fold_ids and X/y\n",
        "import numpy as np, pandas as pd, json, gc, time\n",
        "from pathlib import Path\n",
        "\n",
        "t0 = time.time()\n",
        "heartbeat('BUILD START')\n",
        "\n",
        "# Paths\n",
        "FOLD_PATH = Path('fold_ids.npy')\n",
        "Y_PATH = Path('y.npy')\n",
        "X_TR_PATH = Path('X.parquet')\n",
        "X_TE_PATH = Path('X_test.parquet')\n",
        "TRAIN_FE_PATH = Path('train_fe.parquet')\n",
        "TEST_FE_PATH = Path('test_fe.parquet')\n",
        "FEATS_JSON = Path('features.json')\n",
        "GROUPS_CSV = Path('stoich_groups.csv')\n",
        "\n",
        "# Load\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "# Base features\n",
        "tr = engineer_features(train)\n",
        "te = engineer_features(test)\n",
        "\n",
        "# Stoichiometry counts and group key\n",
        "gkey_tr, N_tr, al_tr, ga_tr, in_tr, o_tr = compute_stoich_groups(train)\n",
        "gkey_te, N_te, al_te, ga_te, in_te, o_te = compute_stoich_groups(test)\n",
        "tr['N'] = N_tr; tr['n_al'] = al_tr; tr['n_ga'] = ga_tr; tr['n_in'] = in_tr; tr['n_o'] = o_tr\n",
        "te['N'] = N_te; te['n_al'] = al_te; te['n_ga'] = ga_te; te['n_in'] = in_te; te['n_o'] = o_te\n",
        "\n",
        "# Lattice system from spacegroup\n",
        "tr['sg_number'] = pd.to_numeric(tr['spacegroup'], errors='coerce').fillna(-1).astype(int)\n",
        "te['sg_number'] = pd.to_numeric(te['spacegroup'], errors='coerce').fillna(-1).astype(int)\n",
        "tr['lattice_system'] = tr['sg_number'].apply(lattice_system_from_sgnum).astype(int)\n",
        "te['lattice_system'] = te['sg_number'].apply(lattice_system_from_sgnum).astype(int)\n",
        "\n",
        "# mm-lite and extra features\n",
        "tr = add_mm_lite_and_extras(tr)\n",
        "te = add_mm_lite_and_extras(te)\n",
        "\n",
        "# Debug: verify presence of newly added physics features\n",
        "check_cols = ['H_cation','eff_cations','t_ratio','t_dev','rM_var','charge_density_6N','dist_l2_center','veg_w_al','veg_w_ga','veg_w_in']\n",
        "present = [c for c in check_cols if c in tr.columns]\n",
        "missing = [c for c in check_cols if c not in tr.columns]\n",
        "print('Physics features present:', present)\n",
        "print('Physics features missing:', missing)\n",
        "\n",
        "# Persist engineered frames (for reuse by modeling cell)\n",
        "tr.to_parquet(TRAIN_FE_PATH, index=False)\n",
        "te.to_parquet(TEST_FE_PATH, index=False)\n",
        "\n",
        "# Build 8-fold stratified group-disjoint folds (stratify by group mean target)\n",
        "y = train['bandgap_energy_ev'].astype(float)\n",
        "fold_ids = build_stratified_group_folds(tr, gkey_tr.astype(str), y, n_splits=N_FOLDS, seed=PRIMARY_SEED)\n",
        "np.save(FOLD_PATH, fold_ids)\n",
        "np.save(Y_PATH, np.log1p(y.clip(lower=0)).values)\n",
        "\n",
        "# Save group mapping (for diagnostics/reuse)\n",
        "pd.DataFrame({'id': train['id'], 'stoich_group': gkey_tr.astype(str)}).to_csv(GROUPS_CSV, index=False)\n",
        "\n",
        "# Build numeric-only X caches (no encodings yet; encodings will be added in modeling using frozen folds)\n",
        "drop_cols = ['id','bandgap_energy_ev']\n",
        "common_cols = [c for c in tr.columns if c in te.columns]\n",
        "feat_cols = [c for c in common_cols if c not in drop_cols]\n",
        "X_tr = tr[feat_cols].copy()\n",
        "X_te = te[feat_cols].copy()\n",
        "med = X_tr.median(numeric_only=True)\n",
        "X_tr = X_tr.fillna(med)\n",
        "X_te = X_te.fillna(med)\n",
        "num_cols = list(X_tr.select_dtypes(include=[np.number]).columns)\n",
        "X_tr = X_tr[num_cols]\n",
        "X_te = X_te[num_cols]\n",
        "X_tr.to_parquet(X_TR_PATH, index=False)\n",
        "X_te.to_parquet(X_TE_PATH, index=False)\n",
        "with open(FEATS_JSON, 'w') as f:\n",
        "    json.dump({'features': num_cols}, f)\n",
        "\n",
        "print('Built & cached:',\n",
        "      'fold_ids.npy', FOLD_PATH.exists(),\n",
        "      '| X.parquet', X_TR_PATH.exists(),\n",
        "      '| X_test.parquet', X_TE_PATH.exists(),\n",
        "      '| y.npy', Y_PATH.exists(),\n",
        "      '| feats', len(num_cols),\n",
        "      '| elapsed', f'{time.time()-t0:.1f}s')\n",
        "heartbeat('BUILD DONE')\n",
        "gc.collect();"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Physics features present: ['H_cation', 'eff_cations', 't_ratio', 't_dev', 'rM_var', 'charge_density_6N', 'dist_l2_center', 'veg_w_al', 'veg_w_ga', 'veg_w_in']\nPhysics features missing: []\nBuilt & cached: fold_ids.npy True | X.parquet True | X_test.parquet True | y.npy True | feats 123 | elapsed 0.3s\n"
          ]
        }
      ]
    },
    {
      "id": "15ce7dd4-f4de-445f-b662-e8c2c023cd98",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Models: load caches, add centralized OOF Ordered TEs using frozen folds, train multi-seed LightGBM, save OOF/preds\n",
        "import numpy as np, pandas as pd, time, gc, json, os\n",
        "from pathlib import Path\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "heartbeat('MODELS START')\n",
        "t0_all = time.time()\n",
        "\n",
        "# Paths\n",
        "FOLD_PATH = Path('fold_ids.npy')\n",
        "Y_PATH = Path('y.npy')\n",
        "X_TR_PATH = Path('X.parquet')\n",
        "X_TE_PATH = Path('X_test.parquet')\n",
        "TRAIN_FE_PATH = Path('train_fe.parquet')\n",
        "TEST_FE_PATH = Path('test_fe.parquet')\n",
        "FEATS_JSON = Path('features.json')\n",
        "\n",
        "# Load caches\n",
        "fold_ids = np.load(FOLD_PATH)\n",
        "y_log = np.load(Y_PATH)\n",
        "X_tr = pd.read_parquet(X_TR_PATH)\n",
        "X_te = pd.read_parquet(X_TE_PATH)\n",
        "tr = pd.read_parquet(TRAIN_FE_PATH)\n",
        "te = pd.read_parquet(TEST_FE_PATH)\n",
        "with open(FEATS_JSON) as f: meta = json.load(f)\n",
        "\n",
        "# Centralized encodings (OOF Ordered TE + OOF Frequency) using frozen folds and y_log\n",
        "X_tr_enc, X_te_enc, meta_oof = add_encoded_features(X_tr, X_te, tr, te, y_log, fold_ids, seed=PRIMARY_SEED)\n",
        "print('Feature matrix (centralized encodings):', X_tr_enc.shape, X_te_enc.shape)\n",
        "\n",
        "# For LightGBM stability: drop all te_* columns (use physics + fe_* only)\n",
        "drop_te_cols = [c for c in X_tr_enc.columns if c.startswith('te_')]\n",
        "X_tr_lgb = X_tr_enc.drop(columns=drop_te_cols, errors='ignore')\n",
        "X_te_lgb = X_te_enc.drop(columns=drop_te_cols, errors='ignore')\n",
        "enc_cols_kept = [c for c in X_tr_lgb.columns if c.startswith('fe_')]\n",
        "\n",
        "# Auto-drop zero-variance columns\n",
        "std = X_tr_lgb.std(numeric_only=True)\n",
        "const_cols = list(std[std == 0].index)\n",
        "if const_cols:\n",
        "    X_tr_lgb = X_tr_lgb.drop(columns=const_cols, errors='ignore')\n",
        "    X_te_lgb = X_te_lgb.drop(columns=const_cols, errors='ignore')\n",
        "\n",
        "print('LGB matrices (no te_*, const-dropped):', X_tr_lgb.shape, X_te_lgb.shape, '| kept fe_ cols:', len(enc_cols_kept))\n",
        "\n",
        "# Quick diagnostics\n",
        "try:\n",
        "    base_n = len(meta.get('features', []))\n",
        "    enc_cols_all = [c for c in X_tr_enc.columns if c.startswith('te_') or c.startswith('fe_')]\n",
        "    low_uniq = {c: X_tr_lgb[c].nunique() for c in X_tr_lgb.columns if c.startswith('fe_')}\n",
        "    print('Base feat count:', base_n, '| Enc cols added (all):', len(enc_cols_all), '| fe_* kept:', len(enc_cols_kept))\n",
        "    print('Const cols dropped:', const_cols)\n",
        "    print('fe_ nunique:', {k: int(v) for k, v in low_uniq.items()})\n",
        "except Exception as e:\n",
        "    print('Diagnostics warning:', e)\n",
        "\n",
        "# Persist meta OOF encodings for later stacking\n",
        "for k, v in meta_oof.items():\n",
        "    np.save(f'oof_{k}.npy', np.asarray(v, dtype=float))\n",
        "\n",
        "# LightGBM params (Variant B from expert guidance, with extra_trees for variance dampening)\n",
        "base_params = {\n",
        "    'objective': 'regression', 'metric': 'rmse',\n",
        "    'learning_rate': 0.020, 'num_leaves': 32, 'max_depth': -1,\n",
        "    'min_data_in_leaf': 240, 'feature_fraction': 0.58,\n",
        "    'bagging_fraction': 0.75, 'bagging_freq': 1,\n",
        "    'lambda_l2': 18.0, 'lambda_l1': 0.0,\n",
        "    'extra_trees': True, 'extra_tree_threshold': 0.5,\n",
        "    'verbosity': -1, 'num_threads': N_THREADS,\n",
        "    'deterministic': True, 'force_col_wise': True\n",
        "}\n",
        "\n",
        "seeds = SEEDS\n",
        "n_splits = len(np.unique(fold_ids))\n",
        "oof_seeds = []; pred_seeds = []\n",
        "\n",
        "for si, SEED in enumerate(seeds):\n",
        "    params = dict(base_params)\n",
        "    params['seed'] = int(SEED)\n",
        "    oof = np.zeros(len(X_tr_lgb), dtype=float)\n",
        "    pred = np.zeros(len(X_te_lgb), dtype=float)\n",
        "    t0 = time.time()\n",
        "    for k in range(n_splits):\n",
        "        tr_idx = np.where(fold_ids != k)[0]; va_idx = np.where(fold_ids == k)[0]\n",
        "        dtr = lgb.Dataset(X_tr_lgb.iloc[tr_idx], label=y_log[tr_idx], free_raw_data=False)\n",
        "        dva = lgb.Dataset(X_tr_lgb.iloc[va_idx], label=y_log[va_idx], free_raw_data=False)\n",
        "        model = lgb.train(params, dtr, num_boost_round=11000, valid_sets=[dtr, dva], valid_names=['train','valid'],\n",
        "                          callbacks=[lgb.early_stopping(600), lgb.log_evaluation(0)])\n",
        "        oof[va_idx] = model.predict(X_tr_lgb.iloc[va_idx], num_iteration=model.best_iteration)\n",
        "        pred += model.predict(X_te_lgb, num_iteration=model.best_iteration) / n_splits\n",
        "        print(f'LGB SEED {SEED} | fold {k} done | best_iter {model.best_iteration} | elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "        del model, dtr, dva; gc.collect()\n",
        "    rmse = float(mean_squared_error(y_log, oof) ** 0.5)\n",
        "    print(f'SEED {SEED}: OOF RMSLE {rmse:.6f} | elapsed {time.time()-t0:.1f}s')\n",
        "    oof_seeds.append(oof); pred_seeds.append(pred)\n",
        "\n",
        "# Average across seeds\n",
        "oof_avg = np.mean(np.vstack(oof_seeds), axis=0)\n",
        "pred_avg = np.mean(np.vstack(pred_seeds), axis=0)\n",
        "cv = float(mean_squared_error(y_log, oof_avg) ** 0.5)\n",
        "print(f'Blended seeds CV RMSLE: {cv:.6f} | total elapsed {time.time()-t0_all:.1f}s')\n",
        "\n",
        "# Persist OOF/test preds and a quick submission (for sanity); final blend + calibration handled later\n",
        "np.save('oof_lgbm.npy', oof_avg)\n",
        "np.save('pred_lgbm_test.npy', pred_avg)\n",
        "sub = pd.DataFrame({'id': pd.read_csv('test.csv')['id'], 'bandgap_energy_ev': np.expm1(pred_avg).clip(0, 6.5)})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv', sub.shape)\n",
        "heartbeat('MODELS DONE')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature matrix (centralized encodings): (2160, 129) (240, 129)\nLGB matrices (no te_*, const-dropped): (2160, 125) (240, 125) | kept fe_ cols: 3\nBase feat count: 123 | Enc cols added (all): 6 | fe_* kept: 3\nConst cols dropped: ['vo_tot_s_mean']\nfe_ nunique: {'fe_sg': 48, 'fe_ls': 40, 'fe_Nb': 24}\nTraining until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[8110]\ttrain's rmse: 0.0751505\tvalid's rmse: 0.0740106\nLGB SEED 7 | fold 0 done | best_iter 8110 | elapsed 2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[8215]\ttrain's rmse: 0.0720042\tvalid's rmse: 0.0956388\nLGB SEED 7 | fold 1 done | best_iter 8215 | elapsed 5.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[7699]\ttrain's rmse: 0.0738746\tvalid's rmse: 0.0859531\nLGB SEED 7 | fold 2 done | best_iter 7699 | elapsed 7.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[4843]\ttrain's rmse: 0.0745762\tvalid's rmse: 0.104252\nLGB SEED 7 | fold 3 done | best_iter 4843 | elapsed 9.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[6067]\ttrain's rmse: 0.0762362\tvalid's rmse: 0.0792978\nLGB SEED 7 | fold 4 done | best_iter 6067 | elapsed 11.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[4776]\ttrain's rmse: 0.0785192\tvalid's rmse: 0.0712584\nLGB SEED 7 | fold 5 done | best_iter 4776 | elapsed 12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[6415]\ttrain's rmse: 0.0729317\tvalid's rmse: 0.104816\nLGB SEED 7 | fold 6 done | best_iter 6415 | elapsed 15.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did not meet early stopping. Best iteration is:\n[10768]\ttrain's rmse: 0.0734387\tvalid's rmse: 0.0718828\nLGB SEED 7 | fold 7 done | best_iter 10768 | elapsed 18.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEED 7: OOF RMSLE 0.086908 | elapsed 18.5s\nTraining until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[6565]\ttrain's rmse: 0.0765407\tvalid's rmse: 0.0745378\nLGB SEED 42 | fold 0 done | best_iter 6565 | elapsed 2.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[7564]\ttrain's rmse: 0.0725704\tvalid's rmse: 0.0958433\nLGB SEED 42 | fold 1 done | best_iter 7564 | elapsed 4.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[7121]\ttrain's rmse: 0.0745311\tvalid's rmse: 0.0860765\nLGB SEED 42 | fold 2 done | best_iter 7121 | elapsed 6.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[6113]\ttrain's rmse: 0.0729129\tvalid's rmse: 0.103931\nLGB SEED 42 | fold 3 done | best_iter 6113 | elapsed 8.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[5641]\ttrain's rmse: 0.0767063\tvalid's rmse: 0.0790215\nLGB SEED 42 | fold 4 done | best_iter 5641 | elapsed 10.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[5030]\ttrain's rmse: 0.0786236\tvalid's rmse: 0.071913\nLGB SEED 42 | fold 5 done | best_iter 5030 | elapsed 12.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[7260]\ttrain's rmse: 0.0719473\tvalid's rmse: 0.104021\nLGB SEED 42 | fold 6 done | best_iter 7260 | elapsed 14.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[8857]\ttrain's rmse: 0.0746273\tvalid's rmse: 0.0716686\nLGB SEED 42 | fold 7 done | best_iter 8857 | elapsed 17.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEED 42: OOF RMSLE 0.086875 | elapsed 17.9s\nTraining until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[7271]\ttrain's rmse: 0.0756971\tvalid's rmse: 0.0746322\nLGB SEED 2025 | fold 0 done | best_iter 7271 | elapsed 2.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[6072]\ttrain's rmse: 0.074075\tvalid's rmse: 0.0954372\nLGB SEED 2025 | fold 1 done | best_iter 6072 | elapsed 4.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[6194]\ttrain's rmse: 0.0754323\tvalid's rmse: 0.0856745\nLGB SEED 2025 | fold 2 done | best_iter 6194 | elapsed 6.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[5132]\ttrain's rmse: 0.0740051\tvalid's rmse: 0.103977\nLGB SEED 2025 | fold 3 done | best_iter 5132 | elapsed 8.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[6230]\ttrain's rmse: 0.0759505\tvalid's rmse: 0.079044\nLGB SEED 2025 | fold 4 done | best_iter 6230 | elapsed 10.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[5628]\ttrain's rmse: 0.0774817\tvalid's rmse: 0.0714186\nLGB SEED 2025 | fold 5 done | best_iter 5628 | elapsed 12.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[7632]\ttrain's rmse: 0.0718986\tvalid's rmse: 0.104137\nLGB SEED 2025 | fold 6 done | best_iter 7632 | elapsed 14.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 600 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[7162]\ttrain's rmse: 0.0761482\tvalid's rmse: 0.0733414\nLGB SEED 2025 | fold 7 done | best_iter 7162 | elapsed 16.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEED 2025: OOF RMSLE 0.086913 | elapsed 17.0s\nBlended seeds CV RMSLE: 0.086744 | total elapsed 53.6s\nSaved submission.csv (240, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "e422ed07-83cb-4839-a682-86687299b183",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost OOF with centralized encodings + stronger regularization; save OOF/test; optional NNLS blend\n",
        "import numpy as np, pandas as pd, json, time, gc, os, sys, subprocess\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "t0_all = time.time()\n",
        "print('CatBoost (centralized encodings) start')\n",
        "\n",
        "# Ensure CatBoost is available\n",
        "try:\n",
        "    from catboost import CatBoostRegressor, Pool\n",
        "except Exception:\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--prefer-binary', '-q', 'catboost'])\n",
        "    from catboost import CatBoostRegressor, Pool\n",
        "\n",
        "# Paths\n",
        "FOLD_PATH = Path('fold_ids.npy')\n",
        "Y_PATH = Path('y.npy')\n",
        "X_TR_PATH = Path('X.parquet')\n",
        "X_TE_PATH = Path('X_test.parquet')\n",
        "TRAIN_FE_PATH = Path('train_fe.parquet')\n",
        "TEST_FE_PATH = Path('test_fe.parquet')\n",
        "\n",
        "# Load caches\n",
        "fold_ids = np.load(FOLD_PATH)\n",
        "y_log = np.load(Y_PATH)\n",
        "X_tr = pd.read_parquet(X_TR_PATH)\n",
        "X_te = pd.read_parquet(X_TE_PATH)\n",
        "tr = pd.read_parquet(TRAIN_FE_PATH)\n",
        "te = pd.read_parquet(TEST_FE_PATH)\n",
        "\n",
        "# Centralized encodings (strict OOF Ordered TE + OOF Frequency); drop te_* for base CB per guidance\n",
        "X_tr_enc, X_te_enc, meta_oof = add_encoded_features(X_tr, X_te, tr, te, y_log, fold_ids, seed=PRIMARY_SEED)\n",
        "drop_te_cols = [c for c in X_tr_enc.columns if c.startswith('te_')]\n",
        "X_tr_cb = X_tr_enc.drop(columns=drop_te_cols, errors='ignore').copy()\n",
        "X_te_cb = X_te_enc.drop(columns=drop_te_cols, errors='ignore').copy()\n",
        "print('CB base matrices (no te_*):', X_tr_cb.shape, X_te_cb.shape)\n",
        "\n",
        "# Add raw categoricals\n",
        "X_tr_cb['spacegroup'] = tr['spacegroup'].astype(str).values\n",
        "X_te_cb['spacegroup'] = te['spacegroup'].astype(str).values\n",
        "X_tr_cb['lattice_system'] = tr['lattice_system'].astype(int).astype(str).values\n",
        "X_te_cb['lattice_system'] = te['lattice_system'].astype(int).astype(str).values\n",
        "# Nb categorical (qcut on train, digitize test by train bins) using q=8 as in encoders\n",
        "try:\n",
        "    _, bins = pd.qcut(tr['N'].astype(float), q=8, duplicates='drop', retbins=True)\n",
        "    bins = np.unique(bins)\n",
        "    Nb_tr_lab = pd.qcut(tr['N'].astype(float), q=8, labels=False, duplicates='drop').astype('Int64')\n",
        "    Nb_te_raw = np.digitize(te['N'].astype(float).values, bins[1:-1], right=True)\n",
        "    Nb_te_lab = pd.Series(Nb_te_raw, index=te.index).astype('Int64')\n",
        "except Exception:\n",
        "    Nb_tr_lab = pd.qcut(tr['N'].astype(float), q=8, labels=False, duplicates='drop').astype('Int64')\n",
        "    Nb_te_lab = pd.qcut(te['N'].astype(float), q=8, labels=False, duplicates='drop').astype('Int64')\n",
        "X_tr_cb['Nb_cat'] = Nb_tr_lab.astype(str).fillna('-1').values\n",
        "X_te_cb['Nb_cat'] = Nb_te_lab.astype(str).fillna('-1').values\n",
        "\n",
        "# Cat features indices\n",
        "cat_cols = ['spacegroup','lattice_system','Nb_cat']\n",
        "cat_idx = [X_tr_cb.columns.get_loc(c) for c in cat_cols]\n",
        "\n",
        "# Fill NaNs for numeric columns only; leave categoricals as-is\n",
        "num_cols = X_tr_cb.columns.difference(cat_cols)\n",
        "med = X_tr_cb[num_cols].median(numeric_only=True)\n",
        "X_tr_cb[num_cols] = X_tr_cb[num_cols].fillna(med)\n",
        "X_te_cb[num_cols] = X_te_cb[num_cols].fillna(med)\n",
        "\n",
        "seeds = SEEDS\n",
        "n_splits = len(np.unique(fold_ids))\n",
        "oof_cb_seeds = []; pred_cb_seeds = []\n",
        "\n",
        "for SEED in seeds:\n",
        "    params = dict(\n",
        "        loss_function='RMSE', iterations=8000, learning_rate=0.028, depth=7,\n",
        "        l2_leaf_reg=15.0, subsample=0.8, rsm=0.78, od_type='Iter', od_wait=400,\n",
        "        random_seed=int(SEED), verbose=0, allow_writing_files=False, thread_count=N_THREADS\n",
        "    )\n",
        "    oof = np.zeros(len(X_tr_cb), dtype=float)\n",
        "    pred = np.zeros(len(X_te_cb), dtype=float)\n",
        "    t0 = time.time()\n",
        "    for k in range(n_splits):\n",
        "        tr_idx = np.where(fold_ids != k)[0]; va_idx = np.where(fold_ids == k)[0]\n",
        "        pool_tr = Pool(X_tr_cb.iloc[tr_idx], y_log[tr_idx], cat_features=cat_idx)\n",
        "        pool_va = Pool(X_tr_cb.iloc[va_idx], y_log[va_idx], cat_features=cat_idx)\n",
        "        model = CatBoostRegressor(**params)\n",
        "        model.fit(pool_tr, eval_set=pool_va, use_best_model=True)\n",
        "        oof[va_idx] = model.predict(pool_va)\n",
        "        pred += model.predict(Pool(X_te_cb, cat_features=cat_idx)) / n_splits\n",
        "        del model, pool_tr, pool_va; gc.collect()\n",
        "    rmse = float(mean_squared_error(y_log, oof) ** 0.5)\n",
        "    print(f'CatBoost SEED {SEED}: OOF RMSLE {rmse:.6f} | elapsed {time.time()-t0:.1f}s')\n",
        "    oof_cb_seeds.append(oof); pred_cb_seeds.append(pred)\n",
        "\n",
        "# Average across seeds for CatBoost\n",
        "oof_cb = np.mean(np.vstack(oof_cb_seeds), axis=0)\n",
        "pred_cb = np.mean(np.vstack(pred_cb_seeds), axis=0)\n",
        "cv_cb = float(mean_squared_error(y_log, oof_cb) ** 0.5)\n",
        "print(f'CatBoost averaged CV RMSLE: {cv_cb:.6f}')\n",
        "np.save('oof_catboost.npy', oof_cb)\n",
        "np.save('pred_catboost_test.npy', pred_cb)\n",
        "\n",
        "# Optional: blend with existing LGB OOF if available (for quick check); calibration handled in separate cell\n",
        "if Path('oof_lgbm.npy').exists() and Path('pred_lgbm_test.npy').exists():\n",
        "    oof_lgb = np.load('oof_lgbm.npy')\n",
        "    pred_lgb = np.load('pred_lgbm_test.npy')\n",
        "    P = np.vstack([oof_lgb, oof_cb]).T\n",
        "    w, _ = nnls(P, y_log); w = w/(w.sum() if w.sum()>0 else 1.0)\n",
        "    oof_blend = P @ w; cv_blend = float(mean_squared_error(y_log, oof_blend) ** 0.5)\n",
        "    print('NNLS weights (LGB, CB):', w, '| Blended CV RMSLE:', f'{cv_blend:.6f}')\n",
        "    Ptest = np.vstack([pred_lgb, pred_cb]).T\n",
        "    pred_blend = Ptest @ w\n",
        "    sub = pd.DataFrame({'id': pd.read_csv('test.csv')['id'], 'bandgap_energy_ev': np.expm1(pred_blend).clip(0, 6.5)})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (blend preview):', sub.shape)\n",
        "else:\n",
        "    sub = pd.DataFrame({'id': pd.read_csv('test.csv')['id'], 'bandgap_energy_ev': np.expm1(pred_cb).clip(0, 6.5)})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (CatBoost only):', sub.shape)\n",
        "\n",
        "print('CatBoost (centralized encodings) done | total elapsed', f'{time.time()-t0_all:.1f}s')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost (centralized encodings) start\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CB base matrices (no te_*): (2160, 126) (240, 126)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost SEED 7: OOF RMSLE 0.086514 | elapsed 93.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost SEED 42: OOF RMSLE 0.086495 | elapsed 93.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost SEED 2025: OOF RMSLE 0.086509 | elapsed 94.0s\nCatBoost averaged CV RMSLE: 0.086165\nNNLS weights (LGB, CB): [0.43589716 0.56410284] | Blended CV RMSLE: 0.085477\nSaved submission.csv (blend preview): (240, 2)\nCatBoost (centralized encodings) done | total elapsed 281.1s\n"
          ]
        }
      ]
    },
    {
      "id": "123e3ca3-a943-4403-9555-f6226ec268ad",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Post-processing: NNLS re-blend + Per-fold/Global Isotonic calibration (choose best on OOF)\n",
        "import numpy as np, pandas as pd, time\n",
        "from pathlib import Path\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "t0 = time.time()\n",
        "print('Calibration start (per-fold vs global)')\n",
        "\n",
        "# Load OOF/logits and targets\n",
        "y_log = np.load('y.npy')\n",
        "fold_ids = np.load('fold_ids.npy')\n",
        "oof_lgb = np.load('oof_lgbm.npy')\n",
        "pred_lgb = np.load('pred_lgbm_test.npy')\n",
        "oof_cb = np.load('oof_catboost.npy')\n",
        "pred_cb = np.load('pred_catboost_test.npy')\n",
        "\n",
        "# NNLS weights on OOF (log space)\n",
        "P = np.vstack([oof_lgb, oof_cb]).T\n",
        "w, _ = nnls(P, y_log)\n",
        "w = w / (w.sum() if w.sum() > 0 else 1.0)\n",
        "print('NNLS weights (LGB, CB):', w)\n",
        "oof_blend = P @ w\n",
        "cv_blend = float(mean_squared_error(y_log, oof_blend) ** 0.5)\n",
        "print(f'Pre-calibration blended CV RMSLE: {cv_blend:.6f}')\n",
        "Ptest = np.vstack([pred_lgb, pred_cb]).T\n",
        "pred_blend = Ptest @ w\n",
        "\n",
        "# Per-fold isotonic calibration\n",
        "n_splits = len(np.unique(fold_ids))\n",
        "oof_cal_fold = np.zeros_like(oof_blend)\n",
        "pred_cal_fold_parts = []\n",
        "for k in range(n_splits):\n",
        "    tr_idx = np.where(fold_ids != k)[0]\n",
        "    va_idx = np.where(fold_ids == k)[0]\n",
        "    iso_k = IsotonicRegression(out_of_bounds='clip')\n",
        "    iso_k.fit(oof_blend[tr_idx], y_log[tr_idx])\n",
        "    oof_cal_fold[va_idx] = iso_k.transform(oof_blend[va_idx])\n",
        "    pred_cal_fold_parts.append(iso_k.transform(pred_blend))\n",
        "cv_fold = float(mean_squared_error(y_log, oof_cal_fold) ** 0.5)\n",
        "pred_cal_fold = np.mean(np.stack(pred_cal_fold_parts, axis=0), axis=0)\n",
        "print(f'Per-fold isotonic blended CV RMSLE: {cv_fold:.6f}')\n",
        "\n",
        "# Global isotonic calibration\n",
        "iso_full = IsotonicRegression(out_of_bounds='clip')\n",
        "iso_full.fit(oof_blend, y_log)\n",
        "oof_cal_full = iso_full.transform(oof_blend)\n",
        "cv_full = float(mean_squared_error(y_log, oof_cal_full) ** 0.5)\n",
        "pred_cal_full = iso_full.transform(pred_blend)\n",
        "print(f'Global isotonic blended CV RMSLE: {cv_full:.6f}')\n",
        "\n",
        "# Choose best calibration based on OOF CV\n",
        "use_full = cv_full <= cv_fold\n",
        "pred_cal = pred_cal_full if use_full else pred_cal_fold\n",
        "chosen = 'global' if use_full else 'per-fold'\n",
        "print(f'Chosen calibration: {chosen}')\n",
        "\n",
        "sub = pd.DataFrame({'id': pd.read_csv('test.csv')['id'], 'bandgap_energy_ev': np.expm1(pred_cal).clip(0, 6.5)})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Calibrated submission.csv saved:', sub.shape, '| elapsed', f'{time.time()-t0:.1f}s')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calibration start (per-fold vs global)\nNNLS weights (LGB, CB): [0.50270604 0.49729396]\nPre-calibration blended CV RMSLE: 0.085059\nPer-fold isotonic blended CV RMSLE: 0.088344\nGlobal isotonic blended CV RMSLE: 0.082182\nChosen calibration: global\nCalibrated submission.csv saved: (240, 2) | elapsed 0.0s\n"
          ]
        }
      ]
    },
    {
      "id": "31c5b5fa-f695-44f3-964d-37f4b6eccd94",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Residual corrector: small LGBM on compact features; add scaled residuals, recalibrate, save submission\n",
        "import numpy as np, pandas as pd, json, time, gc\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from scipy.optimize import nnls\n",
        "import lightgbm as lgb\n",
        "\n",
        "t0 = time.time()\n",
        "print('Residual corrector start')\n",
        "\n",
        "# Load cached frames and matrices\n",
        "y_log = np.load('y.npy')\n",
        "fold_ids = np.load('fold_ids.npy')\n",
        "tr = pd.read_parquet('train_fe.parquet')\n",
        "te = pd.read_parquet('test_fe.parquet')\n",
        "X_tr = pd.read_parquet('X.parquet')\n",
        "X_te = pd.read_parquet('X_test.parquet')\n",
        "\n",
        "# Rebuild encodings used in base models (spacegroup TE+FE, lattice_system FE)\n",
        "m_smooth = 18.0\n",
        "global_mean = float(y_log.mean())\n",
        "sg_tr = tr['spacegroup'].astype(str)\n",
        "sg_te = te['spacegroup'].astype(str)\n",
        "ls_tr = tr['lattice_system'].astype(int)\n",
        "ls_te = te['lattice_system'].astype(int)\n",
        "te_sg = np.zeros(len(tr), dtype=float); fe_sg = np.zeros(len(tr), dtype=float)\n",
        "for k in np.unique(fold_ids):\n",
        "    tr_idx = np.where(fold_ids != k)[0]; va_idx = np.where(fold_ids == k)[0]\n",
        "    s_tr = sg_tr.iloc[tr_idx]\n",
        "    counts = s_tr.groupby(s_tr).size()\n",
        "    sums = pd.Series(y_log[tr_idx], index=s_tr.index).groupby(s_tr).sum()\n",
        "    enc = (sums + m_smooth*global_mean) / (counts + m_smooth)\n",
        "    te_sg[va_idx] = sg_tr.iloc[va_idx].map(enc).fillna(global_mean).values\n",
        "    fe = counts / counts.sum()\n",
        "    fe_sg[va_idx] = sg_tr.iloc[va_idx].map(fe).fillna(0.0).values\n",
        "counts_all = sg_tr.groupby(sg_tr).size()\n",
        "sums_all = pd.Series(y_log, index=sg_tr.index).groupby(sg_tr).sum()\n",
        "enc_all = (sums_all + m_smooth*global_mean) / (counts_all + m_smooth)\n",
        "fe_all = counts_all / counts_all.sum()\n",
        "te_sg_test = sg_te.map(enc_all).fillna(global_mean).values\n",
        "fe_sg_test = sg_te.map(fe_all).fillna(0.0).values\n",
        "fe_ls = np.zeros(len(tr), dtype=float)\n",
        "for k in np.unique(fold_ids):\n",
        "    tr_idx = np.where(fold_ids != k)[0]; va_idx = np.where(fold_ids == k)[0]\n",
        "    ls_counts = ls_tr.iloc[tr_idx].value_counts(normalize=True)\n",
        "    fe_ls[va_idx] = ls_tr.iloc[va_idx].map(ls_counts).fillna(0.0).values\n",
        "fe_ls_test = ls_te.map(ls_tr.value_counts(normalize=True)).fillna(0.0).values\n",
        "\n",
        "# Assemble compact feature subset for residual corrector\n",
        "feat_names = []\n",
        "def add_feat(col):\n",
        "    if col in tr.columns: feat_names.append(col)\n",
        "\n",
        "for col in ['vegard_bg','bow_in','bow_ga','w_al','w_ga','w_in','w_al_sq','w_ga_sq','w_in_sq',\n",
        "            'w_al_ga','w_al_in','w_ga_in','N','inv_vpa','log_vpa','o_minus_catw_chi_pauling',\n",
        "            'catw_chi_pauling_mean','catw_ionic_radius_mean','vo_cat_p_frac','vo_tot_p_frac']:\n",
        "    add_feat(col)\n",
        "\n",
        "Xr_tr = tr[feat_names].copy()\n",
        "Xr_te = te[feat_names].copy()\n",
        "Xr_tr['te_sg'] = te_sg; Xr_tr['fe_sg'] = fe_sg; Xr_tr['fe_ls'] = fe_ls\n",
        "Xr_te['te_sg'] = te_sg_test; Xr_te['fe_sg'] = fe_sg_test; Xr_te['fe_ls'] = fe_ls_test\n",
        "med = Xr_tr.median(numeric_only=True)\n",
        "Xr_tr = Xr_tr.fillna(med); Xr_te = Xr_te.fillna(med)\n",
        "\n",
        "# Build base blend logits from saved OOF/test arrays\n",
        "oof_lgb = np.load('oof_lgbm.npy')\n",
        "oof_cb = np.load('oof_catboost.npy')\n",
        "pred_lgb = np.load('pred_lgbm_test.npy')\n",
        "pred_cb = np.load('pred_catboost_test.npy')\n",
        "P = np.vstack([oof_lgb, oof_cb]).T\n",
        "w, _ = nnls(P, y_log); w = w / (w.sum() if w.sum() > 0 else 1.0)\n",
        "oof_blend = P @ w\n",
        "Ptest = np.vstack([pred_lgb, pred_cb]).T\n",
        "pred_blend = Ptest @ w\n",
        "cv_blend = float(mean_squared_error(y_log, oof_blend) ** 0.5)\n",
        "print(f'Base blended CV RMSLE: {cv_blend:.6f} | NNLS weights {w}')\n",
        "\n",
        "# Residuals\n",
        "residual = y_log - oof_blend\n",
        "\n",
        "# Train small LGBM on residuals (frozen folds, strong regularization)\n",
        "params = {\n",
        "  'objective':'regression','metric':'rmse','learning_rate':0.05,\n",
        "  'num_leaves':31,'min_data_in_leaf':600,'feature_fraction':0.7,\n",
        "  'bagging_fraction':0.8,'bagging_freq':1,'lambda_l2':20.0,'lambda_l1':0.0,\n",
        "  'verbosity':-1,'num_threads': 6, 'deterministic': True, 'force_col_wise': True\n",
        "}\n",
        "oof_res = np.zeros(len(Xr_tr)); pred_res = np.zeros(len(Xr_te))\n",
        "n_splits = len(np.unique(fold_ids))\n",
        "for k in range(n_splits):\n",
        "    tr_idx = np.where(fold_ids != k)[0]; va_idx = np.where(fold_ids == k)[0]\n",
        "    dtr = lgb.Dataset(Xr_tr.iloc[tr_idx], label=residual[tr_idx], free_raw_data=False)\n",
        "    dva = lgb.Dataset(Xr_tr.iloc[va_idx], label=residual[va_idx], free_raw_data=False)\n",
        "    m = lgb.train(params, dtr, num_boost_round=1200, valid_sets=[dtr,dva], valid_names=['train','valid'], callbacks=[lgb.early_stopping(200), lgb.log_evaluation(0)])\n",
        "    oof_res[va_idx] = m.predict(Xr_tr.iloc[va_idx], num_iteration=m.best_iteration)\n",
        "    pred_res += m.predict(Xr_te, num_iteration=m.best_iteration) / n_splits\n",
        "    del m, dtr, dva; gc.collect()\n",
        "cv_res = float(mean_squared_error(residual, oof_res) ** 0.5)\n",
        "print(f'Residual model CV RMSE (log space): {cv_res:.6f}')\n",
        "\n",
        "# Line search for alpha scaling on residuals\n",
        "alphas = np.linspace(0.1, 0.35, 6)\n",
        "best_alpha, best_cv = 0.0, 1e9\n",
        "for a in alphas:\n",
        "    oof_adj = oof_blend + a * oof_res\n",
        "    cv_a = float(mean_squared_error(y_log, oof_adj) ** 0.5)\n",
        "    if cv_a < best_cv: best_cv, best_alpha = cv_a, float(a)\n",
        "print(f'Best alpha: {best_alpha:.3f} | CV RMSLE: {best_cv:.6f}')\n",
        "\n",
        "# Apply to test logits and calibrate isotonic again\n",
        "oof_final = oof_blend + best_alpha * oof_res\n",
        "pred_final = pred_blend + best_alpha * pred_res\n",
        "iso = IsotonicRegression(out_of_bounds='clip')\n",
        "iso.fit(oof_final, y_log)\n",
        "oof_cal = iso.transform(oof_final)\n",
        "cv_cal = float(mean_squared_error(y_log, oof_cal) ** 0.5)\n",
        "print(f'Post-residual isotonic CV RMSLE: {cv_cal:.6f}')\n",
        "pred_cal = iso.transform(pred_final)\n",
        "\n",
        "# Save final submission\n",
        "sub = pd.DataFrame({'id': pd.read_csv('test.csv')['id'], 'bandgap_energy_ev': np.expm1(pred_cal).clip(0, 6.5)})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Final submission.csv saved:', sub.shape, '| elapsed', f'{time.time()-t0:.1f}s')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Residual corrector start\nBase blended CV RMSLE: 0.085059 | NNLS weights [0.50270604 0.49729396]\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[41]\ttrain's rmse: 0.0866941\tvalid's rmse: 0.0727166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[80]\ttrain's rmse: 0.0829254\tvalid's rmse: 0.0953313\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[58]\ttrain's rmse: 0.0852044\tvalid's rmse: 0.0815654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[63]\ttrain's rmse: 0.082142\tvalid's rmse: 0.101062\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1]\ttrain's rmse: 0.0857059\tvalid's rmse: 0.0798164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[52]\ttrain's rmse: 0.0866116\tvalid's rmse: 0.0702811\nTraining until validation scores don't improve for 200 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[361]\ttrain's rmse: 0.0816651\tvalid's rmse: 0.102641\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[1]\ttrain's rmse: 0.0870461\tvalid's rmse: 0.0678113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Residual model CV RMSE (log space): 0.084967\nBest alpha: 0.350 | CV RMSLE: 0.085008\nPost-residual isotonic CV RMSLE: 0.082152\nFinal submission.csv saved: (240, 2) | elapsed 1.2s\n"
          ]
        }
      ]
    },
    {
      "id": "537f7a95-585f-42da-83bd-23d37c67f9cd",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Add stoichiometry-group OOF target mean + freq; retrain LGBM+CatBoost; NNLS blend; isotonic\n",
        "import numpy as np, pandas as pd, time, gc, json, os, sys, subprocess\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.optimize import nnls\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "print('Group OOF TE + retrain start')\n",
        "\n",
        "# Ensure CatBoost\n",
        "try:\n",
        "    from catboost import CatBoostRegressor, Pool\n",
        "except Exception:\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--prefer-binary', '-q', 'catboost'])\n",
        "    from catboost import CatBoostRegressor, Pool\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load caches\n",
        "fold_ids = np.load('fold_ids.npy')\n",
        "y_log = np.load('y.npy')\n",
        "tr = pd.read_parquet('train_fe.parquet')\n",
        "te = pd.read_parquet('test_fe.parquet')\n",
        "X_tr = pd.read_parquet('X.parquet')\n",
        "X_te = pd.read_parquet('X_test.parquet')\n",
        "\n",
        "# Re-create SG and LS encodings (as before) to append consistently\n",
        "m_smooth_sg = 18.0\n",
        "global_mean = float(y_log.mean())\n",
        "sg_tr = tr['spacegroup'].astype(str)\n",
        "sg_te = te['spacegroup'].astype(str)\n",
        "ls_tr = tr['lattice_system'].astype(int)\n",
        "ls_te = te['lattice_system'].astype(int)\n",
        "te_sg = np.zeros(len(tr), dtype=float); fe_sg = np.zeros(len(tr), dtype=float); fe_ls = np.zeros(len(tr), dtype=float)\n",
        "for k in np.unique(fold_ids):\n",
        "    tr_idx = np.where(fold_ids != k)[0]; va_idx = np.where(fold_ids == k)[0]\n",
        "    s_tr = sg_tr.iloc[tr_idx]\n",
        "    counts = s_tr.groupby(s_tr).size()\n",
        "    sums = pd.Series(y_log[tr_idx], index=s_tr.index).groupby(s_tr).sum()\n",
        "    enc = (sums + m_smooth_sg*global_mean) / (counts + m_smooth_sg)\n",
        "    te_sg[va_idx] = sg_tr.iloc[va_idx].map(enc).fillna(global_mean).values\n",
        "    fe = counts / counts.sum()\n",
        "    fe_sg[va_idx] = sg_tr.iloc[va_idx].map(fe).fillna(0.0).values\n",
        "    ls_counts = ls_tr.iloc[tr_idx].value_counts(normalize=True)\n",
        "    fe_ls[va_idx] = ls_tr.iloc[va_idx].map(ls_counts).fillna(0.0).values\n",
        "counts_all = sg_tr.groupby(sg_tr).size()\n",
        "sums_all = pd.Series(y_log, index=sg_tr.index).groupby(sg_tr).sum()\n",
        "enc_all = (sums_all + m_smooth_sg*global_mean) / (counts_all + m_smooth_sg)\n",
        "fe_all = counts_all / counts_all.sum()\n",
        "te_sg_test = sg_te.map(enc_all).fillna(global_mean).values\n",
        "fe_sg_test = sg_te.map(fe_all).fillna(0.0).values\n",
        "fe_ls_test = ls_te.map(ls_tr.value_counts(normalize=True)).fillna(0.0).values\n",
        "\n",
        "# Build stoichiometry group key from counts saved in engineered frames\n",
        "g_tr = tr[['N','n_al','n_ga','n_in']].astype(int).astype(str).agg('_'.join, axis=1)\n",
        "g_te = te[['N','n_al','n_ga','n_in']].astype(int).astype(str).agg('_'.join, axis=1)\n",
        "\n",
        "# OOF target encoding for stoich group (log space mean) + group frequency\n",
        "m_smooth_g = 20.0\n",
        "te_group = np.zeros(len(tr), dtype=float)\n",
        "fe_group = np.zeros(len(tr), dtype=float)\n",
        "for k in np.unique(fold_ids):\n",
        "    tr_idx = np.where(fold_ids != k)[0]; va_idx = np.where(fold_ids == k)[0]\n",
        "    g_tr_k = g_tr.iloc[tr_idx]\n",
        "    counts = g_tr_k.groupby(g_tr_k).size()\n",
        "    sums = pd.Series(y_log[tr_idx], index=g_tr_k.index).groupby(g_tr_k).sum()\n",
        "    enc = (sums + m_smooth_g*global_mean) / (counts + m_smooth_g)\n",
        "    te_group[va_idx] = g_tr.iloc[va_idx].map(enc).fillna(global_mean).values\n",
        "    fe = counts / counts.sum()\n",
        "    fe_group[va_idx] = g_tr.iloc[va_idx].map(fe).fillna(0.0).values\n",
        "# Full-map for test\n",
        "counts_all_g = g_tr.groupby(g_tr).size()\n",
        "sums_all_g = pd.Series(y_log, index=g_tr.index).groupby(g_tr).sum()\n",
        "enc_all_g = (sums_all_g + m_smooth_g*global_mean) / (counts_all_g + m_smooth_g)\n",
        "fe_all_g = counts_all_g / counts_all_g.sum()\n",
        "te_group_test = g_te.map(enc_all_g).fillna(global_mean).values\n",
        "fe_group_test = g_te.map(fe_all_g).fillna(0.0).values\n",
        "\n",
        "# Assemble modeling matrices by appending encodings\n",
        "X_tr_enc = X_tr.copy(); X_te_enc = X_te.copy()\n",
        "for name, arr_tr, arr_te in [('te_sg', te_sg, te_sg_test), ('fe_sg', fe_sg, fe_sg_test), ('fe_ls', fe_ls, fe_ls_test), ('te_group', te_group, te_group_test), ('fe_group', fe_group, fe_group_test)]:\n",
        "    X_tr_enc[name] = arr_tr; X_te_enc[name] = arr_te\n",
        "med = X_tr_enc.median(numeric_only=True)\n",
        "X_tr_enc = X_tr_enc.fillna(med); X_te_enc = X_te_enc.fillna(med)\n",
        "num_cols = list(X_tr_enc.select_dtypes(include=[np.number]).columns)\n",
        "X_tr_enc = X_tr_enc[num_cols]; X_te_enc = X_te_enc[num_cols]\n",
        "print('Matrices with group encodings:', X_tr_enc.shape, X_te_enc.shape)\n",
        "\n",
        "# Train LGBM (3 seeds, 8 folds)\n",
        "base_params = {'objective':'regression','metric':'rmse','learning_rate':0.03,'num_leaves':96,'max_depth':-1,'min_data_in_leaf':420,'feature_fraction':0.78,'bagging_fraction':0.8,'bagging_freq':1,'lambda_l2':10.0,'lambda_l1':0.0,'verbosity':-1,'num_threads': N_THREADS, 'deterministic': True, 'force_col_wise': True}\n",
        "seeds = SEEDS\n",
        "n_splits = len(np.unique(fold_ids))\n",
        "oof_lgb_seeds, pred_lgb_seeds = [], []\n",
        "for SEED in seeds:\n",
        "    params = dict(base_params); params['seed'] = int(SEED)\n",
        "    oof = np.zeros(len(X_tr_enc), dtype=float); pred = np.zeros(len(X_te_enc), dtype=float)\n",
        "    for k in range(n_splits):\n",
        "        tr_idx = np.where(fold_ids != k)[0]; va_idx = np.where(fold_ids == k)[0]\n",
        "        dtr = lgb.Dataset(X_tr_enc.iloc[tr_idx], label=y_log[tr_idx], free_raw_data=False)\n",
        "        dva = lgb.Dataset(X_tr_enc.iloc[va_idx], label=y_log[va_idx], free_raw_data=False)\n",
        "        m = lgb.train(params, dtr, num_boost_round=7000, valid_sets=[dtr,dva], valid_names=['train','valid'], callbacks=[lgb.early_stopping(400), lgb.log_evaluation(0)])\n",
        "        oof[va_idx] = m.predict(X_tr_enc.iloc[va_idx], num_iteration=m.best_iteration)\n",
        "        pred += m.predict(X_te_enc, num_iteration=m.best_iteration)/n_splits\n",
        "        del m, dtr, dva; gc.collect()\n",
        "    print('LGB seed', SEED, 'OOF', float(mean_squared_error(y_log, oof) ** 0.5))\n",
        "    oof_lgb_seeds.append(oof); pred_lgb_seeds.append(pred)\n",
        "oof_lgb = np.mean(np.vstack(oof_lgb_seeds), axis=0)\n",
        "pred_lgb = np.mean(np.vstack(pred_lgb_seeds), axis=0)\n",
        "print('LGB avg CV:', float(mean_squared_error(y_log, oof_lgb) ** 0.5))\n",
        "np.save('oof_lgbm_grp.npy', oof_lgb); np.save('pred_lgbm_grp_test.npy', pred_lgb)\n",
        "\n",
        "# Train CatBoost (3 seeds, 8 folds) with raw categoricals + appended encodings\n",
        "X_tr_cb = X_tr_enc.copy(); X_te_cb = X_te_enc.copy()\n",
        "X_tr_cb['spacegroup'] = sg_tr.values; X_te_cb['spacegroup'] = sg_te.values\n",
        "X_tr_cb['lattice_system'] = ls_tr.values; X_te_cb['lattice_system'] = ls_te.values\n",
        "cat_cols = ['spacegroup','lattice_system']\n",
        "cat_idx = [X_tr_cb.columns.get_loc(c) for c in cat_cols]\n",
        "num_only = X_tr_cb.columns.difference(cat_cols)\n",
        "med_cb = X_tr_cb[num_only].median(numeric_only=True)\n",
        "X_tr_cb[num_only] = X_tr_cb[num_only].fillna(med_cb); X_te_cb[num_only] = X_te_cb[num_only].fillna(med_cb)\n",
        "oof_cb_seeds, pred_cb_seeds = [], []\n",
        "for SEED in seeds:\n",
        "    params_cb = dict(loss_function='RMSE', iterations=6000, learning_rate=0.028, depth=8, l2_leaf_reg=10.0, subsample=0.8, rsm=0.75, od_type='Iter', od_wait=350, random_seed=int(SEED), verbose=0, allow_writing_files=False, thread_count=N_THREADS)\n",
        "    oof = np.zeros(len(X_tr_cb), dtype=float); pred = np.zeros(len(X_te_cb), dtype=float)\n",
        "    for k in range(n_splits):\n",
        "        tr_idx = np.where(fold_ids != k)[0]; va_idx = np.where(fold_ids == k)[0]\n",
        "        pool_tr = Pool(X_tr_cb.iloc[tr_idx], y_log[tr_idx], cat_features=cat_idx); pool_va = Pool(X_tr_cb.iloc[va_idx], y_log[va_idx], cat_features=cat_idx)\n",
        "        m = CatBoostRegressor(**params_cb); m.fit(pool_tr, eval_set=pool_va, use_best_model=True)\n",
        "        oof[va_idx] = m.predict(pool_va); pred += m.predict(Pool(X_te_cb, cat_features=cat_idx))/n_splits\n",
        "        del m, pool_tr, pool_va; gc.collect()\n",
        "    print('CB seed', SEED, 'OOF', float(mean_squared_error(y_log, oof) ** 0.5))\n",
        "    oof_cb_seeds.append(oof); pred_cb_seeds.append(pred)\n",
        "oof_cb = np.mean(np.vstack(oof_cb_seeds), axis=0)\n",
        "pred_cb = np.mean(np.vstack(pred_cb_seeds), axis=0)\n",
        "print('CB avg CV:', float(mean_squared_error(y_log, oof_cb) ** 0.5))\n",
        "np.save('oof_catboost_grp.npy', oof_cb); np.save('pred_catboost_grp_test.npy', pred_cb)\n",
        "\n",
        "# NNLS blend on new OOF logits\n",
        "P = np.vstack([oof_lgb, oof_cb]).T\n",
        "w, _ = nnls(P, y_log); w = w / (w.sum() if w.sum() > 0 else 1.0)\n",
        "oof_blend = P @ w; cv_blend = float(mean_squared_error(y_log, oof_blend) ** 0.5)\n",
        "print('NNLS w (LGB,CB):', w, '| Blended CV:', cv_blend)\n",
        "Ptest = np.vstack([pred_lgb, pred_cb]).T\n",
        "pred_blend = Ptest @ w\n",
        "\n",
        "# Isotonic calibration\n",
        "iso = IsotonicRegression(out_of_bounds='clip')\n",
        "iso.fit(oof_blend, y_log)\n",
        "oof_cal = iso.transform(oof_blend)\n",
        "cv_cal = float(mean_squared_error(y_log, oof_cal) ** 0.5)\n",
        "print('Post-calibration CV:', cv_cal)\n",
        "pred_cal = iso.transform(pred_blend)\n",
        "\n",
        "# Save submission\n",
        "sub = pd.DataFrame({'id': pd.read_csv('test.csv')['id'], 'bandgap_energy_ev': np.expm1(pred_cal).clip(0, 6.5)})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('submission.csv saved (group TE run):', sub.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "930ba6a7-6c70-4723-8193-5d7c5101d9f6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# KNN prior on stoichiometry groups (OOF, log space) + quick LGBM retrain\n",
        "import numpy as np, pandas as pd, time, gc, json\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import lightgbm as lgb\n",
        "\n",
        "print('KNN prior build + LGBM retrain start')\n",
        "\n",
        "# Load caches\n",
        "fold_ids = np.load('fold_ids.npy')\n",
        "y_log = np.load('y.npy')\n",
        "tr = pd.read_parquet('train_fe.parquet')\n",
        "te = pd.read_parquet('test_fe.parquet')\n",
        "X_tr = pd.read_parquet('X.parquet')\n",
        "X_te = pd.read_parquet('X_test.parquet')\n",
        "\n",
        "# Build stoichiometry vector per sample: (N, n_al, n_ga, n_in) as ints\n",
        "G_tr = tr[['N','n_al','n_ga','n_in']].astype(int).values\n",
        "G_te = te[['N','n_al','n_ga','n_in']].astype(int).values\n",
        "\n",
        "# Helper: KNN prior per fold (no leakage). Distance = L1 on counts. m-smooth to global mean.\n",
        "def knn_prior_oof(G, y_log, fold_ids, K=7, m=20.0):\n",
        "    n = len(G)\n",
        "    out = np.zeros(n, dtype=float)\n",
        "    gmean = float(y_log.mean())\n",
        "    for k in np.unique(fold_ids):\n",
        "        tr_idx = np.where(fold_ids != k)[0]; va_idx = np.where(fold_ids == k)[0]\n",
        "        Gtr, ytr = G[tr_idx], y_log[tr_idx]\n",
        "        # unique groups in train fold\n",
        "        df_tr = pd.DataFrame(Gtr, columns=['N','al','ga','in'])\n",
        "        grp = df_tr.groupby(['N','al','ga','in']).agg(idx=('N','size')).reset_index()\n",
        "        # build map: group key -> (mean, count)\n",
        "        keys = list(map(tuple, grp[['N','al','ga','in']].values))\n",
        "        # compute mean and count per unique group\n",
        "        df_stats = pd.DataFrame(Gtr, columns=['N','al','ga','in'])\n",
        "        df_stats['y'] = ytr\n",
        "        stats = df_stats.groupby(['N','al','ga','in']).agg(mean=('y','mean'), cnt=('y','size')).reset_index()\n",
        "        U = stats[['N','al','ga','in']].values\n",
        "        mu = stats['mean'].values; cnt = stats['cnt'].values\n",
        "        # for each valid sample, compute L1 distance to all U and take KNN weighted mean (count-weighted), then m-smooth\n",
        "        Gva = G[va_idx]\n",
        "        # compute distances\n",
        "        for i, g in enumerate(Gva):\n",
        "            d = np.sum(np.abs(U - g), axis=1)\n",
        "            # take K nearest\n",
        "            idx = np.argpartition(d, K)[:K]\n",
        "            w = cnt[idx].astype(float) + 1e-6\n",
        "            m_knn = np.sum(mu[idx] * w) / np.sum(w)\n",
        "            # m-smoothing towards global mean using total counts\n",
        "            c_tot = float(np.sum(cnt[idx]))\n",
        "            prior = (m_knn * c_tot + m * gmean) / (c_tot + m)\n",
        "            out[va_idx[i]] = prior\n",
        "    return out\n",
        "\n",
        "t0 = time.time()\n",
        "knn_oof = knn_prior_oof(G_tr, y_log, fold_ids, K=7, m=20.0)\n",
        "print('KNN OOF built in', f'{time.time()-t0:.1f}s')\n",
        "\n",
        "# Train-fold full-map for test using all train data\n",
        "def knn_prior_infer(G_all, y_all, G_query, K=7, m=20.0):\n",
        "    gmean = float(y_all.mean())\n",
        "    df = pd.DataFrame(G_all, columns=['N','al','ga','in'])\n",
        "    df['y'] = y_all\n",
        "    stats = df.groupby(['N','al','ga','in']).agg(mean=('y','mean'), cnt=('y','size')).reset_index()\n",
        "    U = stats[['N','al','ga','in']].values\n",
        "    mu = stats['mean'].values; cnt = stats['cnt'].values\n",
        "    out = np.zeros(len(G_query), dtype=float)\n",
        "    for i, g in enumerate(G_query):\n",
        "        d = np.sum(np.abs(U - g), axis=1)\n",
        "        idx = np.argpartition(d, K)[:K]\n",
        "        w = cnt[idx].astype(float) + 1e-6\n",
        "        m_knn = np.sum(mu[idx] * w) / np.sum(w)\n",
        "        c_tot = float(np.sum(cnt[idx]))\n",
        "        out[i] = (m_knn * c_tot + m * gmean) / (c_tot + m)\n",
        "    return out\n",
        "\n",
        "t1 = time.time()\n",
        "knn_te = knn_prior_infer(G_tr, y_log, G_te, K=7, m=20.0)\n",
        "print('KNN test built in', f'{time.time()-t1:.1f}s')\n",
        "\n",
        "# Append to matrices and retrain LGBM quickly (3 seeds)\n",
        "X_tr_knn = X_tr.copy(); X_te_knn = X_te.copy()\n",
        "X_tr_knn['knn_group_prior'] = knn_oof\n",
        "X_te_knn['knn_group_prior'] = knn_te\n",
        "med = X_tr_knn.median(numeric_only=True)\n",
        "X_tr_knn = X_tr_knn.fillna(med); X_te_knn = X_te_knn.fillna(med)\n",
        "num_cols = list(X_tr_knn.select_dtypes(include=[np.number]).columns)\n",
        "X_tr_knn = X_tr_knn[num_cols]; X_te_knn = X_te_knn[num_cols]\n",
        "\n",
        "params = {'objective':'regression','metric':'rmse','learning_rate':0.03,'num_leaves':96,'max_depth':-1,'min_data_in_leaf':500,'feature_fraction':0.75,'bagging_fraction':0.8,'bagging_freq':1,'lambda_l2':12.0,'lambda_l1':0.0,'verbosity':-1,'num_threads': N_THREADS,'deterministic': True,'force_col_wise': True}\n",
        "seeds = SEEDS\n",
        "n_splits = len(np.unique(fold_ids))\n",
        "oof_seeds = []; pred_seeds = []\n",
        "for SEED in seeds:\n",
        "    p = dict(params); p['seed'] = int(SEED)\n",
        "    oof = np.zeros(len(X_tr_knn)); pred = np.zeros(len(X_te_knn))\n",
        "    for k in range(n_splits):\n",
        "        tr_idx = np.where(fold_ids != k)[0]; va_idx = np.where(fold_ids == k)[0]\n",
        "        dtr = lgb.Dataset(X_tr_knn.iloc[tr_idx], label=y_log[tr_idx], free_raw_data=False)\n",
        "        dva = lgb.Dataset(X_tr_knn.iloc[va_idx], label=y_log[va_idx], free_raw_data=False)\n",
        "        m = lgb.train(p, dtr, num_boost_round=7000, valid_sets=[dtr,dva], valid_names=['train','valid'], callbacks=[lgb.early_stopping(400), lgb.log_evaluation(0)])\n",
        "        oof[va_idx] = m.predict(X_tr_knn.iloc[va_idx], num_iteration=m.best_iteration)\n",
        "        pred += m.predict(X_te_knn, num_iteration=m.best_iteration)/n_splits\n",
        "        del m, dtr, dva; gc.collect()\n",
        "    rmse = float(mean_squared_error(y_log, oof) ** 0.5)\n",
        "    print(f'KNN-LGB SEED {SEED}: OOF {rmse:.6f}')\n",
        "    oof_seeds.append(oof); pred_seeds.append(pred)\n",
        "oof_avg = np.mean(np.vstack(oof_seeds), axis=0)\n",
        "pred_avg = np.mean(np.vstack(pred_seeds), axis=0)\n",
        "cv = float(mean_squared_error(y_log, oof_avg) ** 0.5)\n",
        "print(f'KNN-LGB blended seeds CV: {cv:.6f}')\n",
        "np.save('oof_lgbm_knn.npy', oof_avg); np.save('pred_lgbm_knn_test.npy', pred_avg)\n",
        "\n",
        "# Optional: blend with previous CB OOF if available and recalibrate\n",
        "if Path('oof_catboost.npy').exists() and Path('pred_catboost_test.npy').exists():\n",
        "    o_cb = np.load('oof_catboost.npy'); p_cb = np.load('pred_catboost_test.npy')\n",
        "    P = np.vstack([oof_avg, o_cb]).T\n",
        "    from scipy.optimize import nnls\n",
        "    w, _ = nnls(P, y_log); w = w/(w.sum() if w.sum()>0 else 1.0)\n",
        "    oof_blend = P @ w; cv_blend = float(mean_squared_error(y_log, oof_blend) ** 0.5)\n",
        "    print('KNN-LGB + CB NNLS w:', w, '| CV:', cv_blend)\n",
        "    Ptest = np.vstack([pred_avg, p_cb]).T\n",
        "    pred_blend = Ptest @ w\n",
        "    # Isotonic calibration\n",
        "    from sklearn.isotonic import IsotonicRegression\n",
        "    iso = IsotonicRegression(out_of_bounds='clip')\n",
        "    iso.fit(oof_blend, y_log)\n",
        "    oof_cal = iso.transform(oof_blend)\n",
        "    cv_cal = float(mean_squared_error(y_log, oof_cal) ** 0.5)\n",
        "    print('Post-calibration CV:', cv_cal)\n",
        "    sub = pd.DataFrame({'id': pd.read_csv('test.csv')['id'], 'bandgap_energy_ev': np.expm1(iso.transform(pred_blend)).clip(0,6.5)})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('submission.csv saved (KNN prior run):', sub.shape)\n",
        "print('Done.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "398a4c93-e618-4f62-b896-46c7ac0686b5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ridge meta-stacker with OOF encodings + physics features + per-fold/global isotonic (choose best)\n",
        "import numpy as np, pandas as pd, time, gc, json\n",
        "from pathlib import Path\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "print('Ridge meta-stacker start')\n",
        "\n",
        "# Load base OOF/test\n",
        "y_log = np.load('y.npy')\n",
        "fold_ids = np.load('fold_ids.npy')\n",
        "oof_lgb = np.load('oof_lgbm.npy')\n",
        "pred_lgb = np.load('pred_lgbm_test.npy')\n",
        "oof_cb = np.load('oof_catboost.npy')\n",
        "pred_cb = np.load('pred_catboost_test.npy')\n",
        "\n",
        "# Load engineered frames for physics features\n",
        "tr = pd.read_parquet('train_fe.parquet')\n",
        "te = pd.read_parquet('test_fe.parquet')\n",
        "\n",
        "# Recompute OOF ordered TEs to get train (OOF) + test encodings for meta features\n",
        "X_tr_base = pd.read_parquet('X.parquet')\n",
        "X_te_base = pd.read_parquet('X_test.parquet')\n",
        "X_tr_enc_tmp, X_te_enc_tmp, meta_oof = add_encoded_features(X_tr_base, X_te_base, tr, te, y_log, fold_ids, seed=PRIMARY_SEED)\n",
        "\n",
        "# Extract meta encoding columns (train OOF and test mapped)\n",
        "def get_enc_pair(name):\n",
        "    tr_col = f'te_{name}'; fe_tr_col = f'fe_{name}'\n",
        "    te_col = f'te_{name}'; fe_te_col = f'fe_{name}'\n",
        "    return (X_tr_enc_tmp[tr_col].values, X_te_enc_tmp[te_col].values,\n",
        "            X_tr_enc_tmp[fe_tr_col].values, X_te_enc_tmp[fe_te_col].values)\n",
        "\n",
        "te_sg_tr, te_sg_te, fe_sg_tr, fe_sg_te = get_enc_pair('sg')\n",
        "te_ls_tr, te_ls_te, fe_ls_tr, fe_ls_te = get_enc_pair('ls')\n",
        "te_Nb_tr, te_Nb_te, fe_Nb_tr, fe_Nb_te = get_enc_pair('Nb')\n",
        "\n",
        "# Physics features for meta (compact, high-signal set) with availability guard\n",
        "phys_cols = ['vegard_bg','H_cation','eff_cations','t_ratio','t_dev','rM_var','charge_density_6N',\n",
        "             'dist_l2_center','w_al','w_ga','w_in','veg_w_al','veg_w_ga','veg_w_in']\n",
        "phys_cols_avail = [c for c in phys_cols if c in tr.columns]\n",
        "missing = [c for c in phys_cols if c not in phys_cols_avail]\n",
        "if missing:\n",
        "    print('Missing physics cols (skipped):', missing)\n",
        "phys_tr = tr[phys_cols_avail].copy() if phys_cols_avail else pd.DataFrame(index=tr.index)\n",
        "phys_te = te[phys_cols_avail].copy() if phys_cols_avail else pd.DataFrame(index=te.index)\n",
        "if not phys_tr.empty:\n",
        "    med = phys_tr.median(numeric_only=True)\n",
        "    phys_tr = phys_tr.fillna(med)\n",
        "    phys_te = phys_te.fillna(med)\n",
        "\n",
        "# Build meta matrices\n",
        "M_tr_list = [oof_lgb, oof_cb, te_sg_tr, te_ls_tr, te_Nb_tr, fe_sg_tr, fe_ls_tr, fe_Nb_tr]\n",
        "M_te_list = [pred_lgb, pred_cb, te_sg_te, te_ls_te, te_Nb_te, fe_sg_te, fe_ls_te, fe_Nb_te]\n",
        "if not phys_tr.empty:\n",
        "    M_tr = np.column_stack(M_tr_list + [phys_tr.values])\n",
        "    M_te = np.column_stack(M_te_list + [phys_te.values])\n",
        "else:\n",
        "    M_tr = np.column_stack(M_tr_list)\n",
        "    M_te = np.column_stack(M_te_list)\n",
        "print('Meta matrices:', M_tr.shape, M_te.shape)\n",
        "\n",
        "# Per-fold standardization + RidgeCV\n",
        "alphas = [0.1, 1.0, 10.0]\n",
        "n = len(y_log); n_splits = len(np.unique(fold_ids))\n",
        "oof_meta = np.zeros(n, dtype=float)\n",
        "pred_meta = np.zeros(len(M_te), dtype=float)\n",
        "for k in range(n_splits):\n",
        "    tr_idx = np.where(fold_ids != k)[0]; va_idx = np.where(fold_ids == k)[0]\n",
        "    sc = StandardScaler(with_mean=True, with_std=True)\n",
        "    Mtr = sc.fit_transform(M_tr[tr_idx]); Mva = sc.transform(M_tr[va_idx]); Mte_sc = sc.transform(M_te)\n",
        "    ridge = RidgeCV(alphas=alphas, fit_intercept=True, cv=None, scoring=None)\n",
        "    ridge.fit(Mtr, y_log[tr_idx])\n",
        "    oof_meta[va_idx] = ridge.predict(Mva)\n",
        "    pred_meta += ridge.predict(Mte_sc) / n_splits\n",
        "    print(f'Fold {k} Ridge alpha={ridge.alpha_:.3f}')\n",
        "cv_meta = float(mean_squared_error(y_log, oof_meta) ** 0.5)\n",
        "print(f'Ridge meta OOF CV RMSLE: {cv_meta:.6f}')\n",
        "\n",
        "# Per-fold isotonic calibration on meta\n",
        "oof_cal_fold = np.zeros_like(oof_meta)\n",
        "pred_cal_fold_parts = []\n",
        "for k in range(n_splits):\n",
        "    tr_idx = np.where(fold_ids != k)[0]; va_idx = np.where(fold_ids == k)[0]\n",
        "    iso_k = IsotonicRegression(out_of_bounds='clip')\n",
        "    iso_k.fit(oof_meta[tr_idx], y_log[tr_idx])\n",
        "    oof_cal_fold[va_idx] = iso_k.transform(oof_meta[va_idx])\n",
        "    pred_cal_fold_parts.append(iso_k.transform(pred_meta))\n",
        "cv_fold = float(mean_squared_error(y_log, oof_cal_fold) ** 0.5)\n",
        "pred_cal_fold = np.mean(np.stack(pred_cal_fold_parts, axis=0), axis=0)\n",
        "print(f'Per-fold isotonic-calibrated meta CV RMSLE: {cv_fold:.6f}')\n",
        "\n",
        "# Global isotonic calibration on meta\n",
        "iso_full = IsotonicRegression(out_of_bounds='clip')\n",
        "iso_full.fit(oof_meta, y_log)\n",
        "oof_cal_full = iso_full.transform(oof_meta)\n",
        "cv_full = float(mean_squared_error(y_log, oof_cal_full) ** 0.5)\n",
        "pred_cal_full = iso_full.transform(pred_meta)\n",
        "print(f'Global isotonic meta CV RMSLE: {cv_full:.6f}')\n",
        "\n",
        "# Choose best\n",
        "use_full = cv_full <= cv_fold\n",
        "pred_cal = pred_cal_full if use_full else pred_cal_fold\n",
        "chosen = 'global' if use_full else 'per-fold'\n",
        "print('Chosen calibration for meta:', chosen)\n",
        "\n",
        "# Save calibrated submission\n",
        "sub = pd.DataFrame({'id': pd.read_csv('test.csv')['id'], 'bandgap_energy_ev': np.expm1(pred_cal).clip(0, 6.5)})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('submission.csv saved (ridge meta, best iso):', sub.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge meta-stacker start\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta matrices: (2160, 22) (240, 22)\nFold 0 Ridge alpha=10.000\nFold 1 Ridge alpha=10.000\nFold 2 Ridge alpha=10.000\nFold 3 Ridge alpha=10.000\nFold 4 Ridge alpha=10.000\nFold 5 Ridge alpha=10.000\nFold 6 Ridge alpha=10.000\nFold 7 Ridge alpha=10.000\nRidge meta OOF CV RMSLE: 0.085054\nPer-fold isotonic-calibrated meta CV RMSLE: 0.086810\nGlobal isotonic meta CV RMSLE: 0.081867\nChosen calibration for meta: global\nsubmission.csv saved (ridge meta, best iso): (240, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "e23287ff-c36c-4a1e-87dc-d7c3039ac8db",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Diagnostics: inspect engineered feature columns\n",
        "import pandas as pd\n",
        "tr = pd.read_parquet('train_fe.parquet')\n",
        "te = pd.read_parquet('test_fe.parquet')\n",
        "cols = set(tr.columns.tolist())\n",
        "check = ['vegard_bg','H_cation','eff_cations','t_ratio','t_dev','rM_var','charge_density_6N','dist_l2_center','w_al','w_ga','w_in','veg_w_al','veg_w_ga','veg_w_in']\n",
        "missing = [c for c in check if c not in cols]\n",
        "present = [c for c in check if c in cols]\n",
        "print('Present:', present)\n",
        "print('Missing:', missing)\n",
        "print('Total columns in train_fe:', len(tr.columns))\n",
        "print('Sample columns:', tr.columns[:30].tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "349b9813-c355-4df7-8593-2fc710639a4a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Dual-CV base models: build second fold split (seed=777), retrain LGBM (Variant A) and CatBoost,\n",
        "# average OOF/test across splits, and overwrite base OOF/preds for meta-stacking\n",
        "import numpy as np, pandas as pd, time, gc, json, sys, subprocess, os\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Ensure CatBoost is available\n",
        "try:\n",
        "    from catboost import CatBoostRegressor, Pool\n",
        "except Exception:\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--prefer-binary', '-q', 'catboost'])\n",
        "    from catboost import CatBoostRegressor, Pool\n",
        "\n",
        "print('Dual-CV base models start')\n",
        "t0_all = time.time()\n",
        "\n",
        "# Load caches and raw train for group key\n",
        "y_log = np.load('y.npy')\n",
        "tr = pd.read_parquet('train_fe.parquet')\n",
        "te = pd.read_parquet('test_fe.parquet')\n",
        "X_tr = pd.read_parquet('X.parquet')\n",
        "X_te = pd.read_parquet('X_test.parquet')\n",
        "train_csv = pd.read_csv('train.csv')\n",
        "\n",
        "# Build stoichiometry group key from raw train (for fold construction)\n",
        "gkey_tr, _, _, _, _, _ = compute_stoich_groups(train_csv)\n",
        "y = train_csv['bandgap_energy_ev'].astype(float)\n",
        "\n",
        "split_seeds = [PRIMARY_SEED, 777]\n",
        "oofs_lgb_splits, preds_lgb_splits = [], []\n",
        "oofs_cb_splits, preds_cb_splits = [], []\n",
        "\n",
        "for s in split_seeds:\n",
        "    print(f'--- Split seed {s} ---')\n",
        "    fold_ids_split = build_stratified_group_folds(tr, gkey_tr.astype(str), y, n_splits=N_FOLDS, seed=int(s))\n",
        "    # Encoded features (uses strict OOF within this split)\n",
        "    X_tr_enc, X_te_enc, _ = add_encoded_features(X_tr, X_te, tr, te, y_log, fold_ids_split, seed=int(s))\n",
        "    # LGBM Variant A, dropping te_* (physics + fe_ only), auto-drop const cols\n",
        "    drop_te = [c for c in X_tr_enc.columns if c.startswith('te_')]\n",
        "    X_tr_lgb = X_tr_enc.drop(columns=drop_te, errors='ignore').copy()\n",
        "    X_te_lgb = X_te_enc.drop(columns=drop_te, errors='ignore').copy()\n",
        "    std = X_tr_lgb.std(numeric_only=True); const_cols = list(std[std == 0].index)\n",
        "    if const_cols:\n",
        "        X_tr_lgb = X_tr_lgb.drop(columns=const_cols, errors='ignore'); X_te_lgb = X_te_lgb.drop(columns=const_cols, errors='ignore')\n",
        "    params_lgb = {\n",
        "        'objective':'regression','metric':'rmse','learning_rate':0.023,'num_leaves':48,'max_depth':-1,\n",
        "        'min_data_in_leaf':160,'feature_fraction':0.62,'bagging_fraction':0.80,'bagging_freq':1,\n",
        "        'lambda_l2':15.0,'lambda_l1':0.0,'verbosity':-1,'num_threads': N_THREADS,'deterministic': True,'force_col_wise': True\n",
        "    }\n",
        "    seeds = SEEDS\n",
        "    n_splits = len(np.unique(fold_ids_split))\n",
        "    oof_lgb_seeds, pred_lgb_seeds = [], []\n",
        "    for SEED in seeds:\n",
        "        p = dict(params_lgb); p['seed'] = int(SEED)\n",
        "        oof = np.zeros(len(X_tr_lgb)); pred = np.zeros(len(X_te_lgb))\n",
        "        t0 = time.time()\n",
        "        for k in range(n_splits):\n",
        "            tr_idx = np.where(fold_ids_split != k)[0]; va_idx = np.where(fold_ids_split == k)[0]\n",
        "            dtr = lgb.Dataset(X_tr_lgb.iloc[tr_idx], label=y_log[tr_idx], free_raw_data=False)\n",
        "            dva = lgb.Dataset(X_tr_lgb.iloc[va_idx], label=y_log[va_idx], free_raw_data=False)\n",
        "            m = lgb.train(p, dtr, num_boost_round=9000, valid_sets=[dtr,dva], valid_names=['train','valid'], callbacks=[lgb.early_stopping(500), lgb.log_evaluation(0)])\n",
        "            oof[va_idx] = m.predict(X_tr_lgb.iloc[va_idx], num_iteration=m.best_iteration)\n",
        "            pred += m.predict(X_te_lgb, num_iteration=m.best_iteration) / n_splits\n",
        "            del m, dtr, dva; gc.collect()\n",
        "        print(f'LGB split {s} seed {SEED} OOF:', float(mean_squared_error(y_log, oof)**0.5))\n",
        "        oof_lgb_seeds.append(oof); pred_lgb_seeds.append(pred)\n",
        "    oof_lgb_avg = np.mean(np.vstack(oof_lgb_seeds), axis=0)\n",
        "    pred_lgb_avg = np.mean(np.vstack(pred_lgb_seeds), axis=0)\n",
        "    print(f'LGB split {s} blended seeds CV:', float(mean_squared_error(y_log, oof_lgb_avg)**0.5))\n",
        "    oofs_lgb_splits.append(oof_lgb_avg); preds_lgb_splits.append(pred_lgb_avg)\n",
        "\n",
        "    # CatBoost: drop te_* and add raw categoricals\n",
        "    X_tr_cb = X_tr_enc.drop(columns=drop_te, errors='ignore').copy()\n",
        "    X_te_cb = X_te_enc.drop(columns=drop_te, errors='ignore').copy()\n",
        "    X_tr_cb['spacegroup'] = tr['spacegroup'].astype(str).values\n",
        "    X_te_cb['spacegroup'] = te['spacegroup'].astype(str).values\n",
        "    X_tr_cb['lattice_system'] = tr['lattice_system'].astype(int).astype(str).values\n",
        "    X_te_cb['lattice_system'] = te['lattice_system'].astype(int).astype(str).values\n",
        "    # Nb categorical with q=8 based on train bins\n",
        "    try:\n",
        "        _, bins = pd.qcut(tr['N'].astype(float), q=8, duplicates='drop', retbins=True)\n",
        "        bins = np.unique(bins)\n",
        "        Nb_tr_lab = pd.qcut(tr['N'].astype(float), q=8, labels=False, duplicates='drop').astype('Int64')\n",
        "        Nb_te_raw = np.digitize(te['N'].astype(float).values, bins[1:-1], right=True)\n",
        "        Nb_te_lab = pd.Series(Nb_te_raw, index=te.index).astype('Int64')\n",
        "    except Exception:\n",
        "        Nb_tr_lab = pd.qcut(tr['N'].astype(float), q=8, labels=False, duplicates='drop').astype('Int64')\n",
        "        Nb_te_lab = pd.qcut(te['N'].astype(float), q=8, labels=False, duplicates='drop').astype('Int64')\n",
        "    X_tr_cb['Nb_cat'] = Nb_tr_lab.astype(str).fillna('-1').values\n",
        "    X_te_cb['Nb_cat'] = Nb_te_lab.astype(str).fillna('-1').values\n",
        "    cat_cols = ['spacegroup','lattice_system','Nb_cat']\n",
        "    cat_idx = [X_tr_cb.columns.get_loc(c) for c in cat_cols]\n",
        "    num_cols = X_tr_cb.columns.difference(cat_cols)\n",
        "    med = X_tr_cb[num_cols].median(numeric_only=True)\n",
        "    X_tr_cb[num_cols] = X_tr_cb[num_cols].fillna(med)\n",
        "    X_te_cb[num_cols] = X_te_cb[num_cols].fillna(med)\n",
        "    oof_cb_seeds, pred_cb_seeds = [], []\n",
        "    for SEED in seeds:\n",
        "        params_cb = dict(loss_function='RMSE', iterations=8000, learning_rate=0.028, depth=7, l2_leaf_reg=15.0, subsample=0.8, rsm=0.78, od_type='Iter', od_wait=400, random_seed=int(SEED), verbose=0, allow_writing_files=False, thread_count=N_THREADS)\n",
        "        oof = np.zeros(len(X_tr_cb)); pred = np.zeros(len(X_te_cb))\n",
        "        for k in range(n_splits):\n",
        "            tr_idx = np.where(fold_ids_split != k)[0]; va_idx = np.where(fold_ids_split == k)[0]\n",
        "            pool_tr = Pool(X_tr_cb.iloc[tr_idx], y_log[tr_idx], cat_features=cat_idx)\n",
        "            pool_va = Pool(X_tr_cb.iloc[va_idx], y_log[va_idx], cat_features=cat_idx)\n",
        "            m = CatBoostRegressor(**params_cb); m.fit(pool_tr, eval_set=pool_va, use_best_model=True)\n",
        "            oof[va_idx] = m.predict(pool_va)\n",
        "            pred += m.predict(Pool(X_te_cb, cat_features=cat_idx)) / n_splits\n",
        "            del m, pool_tr, pool_va; gc.collect()\n",
        "        print(f'CB split {s} seed {SEED} OOF:', float(mean_squared_error(y_log, oof)**0.5))\n",
        "        oof_cb_seeds.append(oof); pred_cb_seeds.append(pred)\n",
        "    oof_cb_avg = np.mean(np.vstack(oof_cb_seeds), axis=0)\n",
        "    pred_cb_avg = np.mean(np.vstack(pred_cb_seeds), axis=0)\n",
        "    print(f'CB split {s} blended seeds CV:', float(mean_squared_error(y_log, oof_cb_avg)**0.5))\n",
        "    oofs_cb_splits.append(oof_cb_avg); preds_cb_splits.append(pred_cb_avg)\n",
        "\n",
        "# Average across splits (elementwise) and overwrite base arrays for meta\n",
        "oof_lgb_dual = np.mean(np.vstack(oofs_lgb_splits), axis=0)\n",
        "pred_lgb_dual = np.mean(np.vstack(preds_lgb_splits), axis=0)\n",
        "oof_cb_dual = np.mean(np.vstack(oofs_cb_splits), axis=0)\n",
        "pred_cb_dual = np.mean(np.vstack(preds_cb_splits), axis=0)\n",
        "cv_lgb_dual = float(mean_squared_error(y_log, oof_lgb_dual) ** 0.5)\n",
        "cv_cb_dual = float(mean_squared_error(y_log, oof_cb_dual) ** 0.5)\n",
        "print(f'Dual-split LGB CV: {cv_lgb_dual:.6f} | Dual-split CB CV: {cv_cb_dual:.6f}')\n",
        "np.save('oof_lgbm.npy', oof_lgb_dual); np.save('pred_lgbm_test.npy', pred_lgb_dual)\n",
        "np.save('oof_catboost.npy', oof_cb_dual); np.save('pred_catboost_test.npy', pred_cb_dual)\n",
        "print('Saved averaged base OOF/test arrays for meta. | elapsed', f'{time.time()-t0_all:.1f}s')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dual-CV base models start\n--- Split seed 42 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1375]\ttrain's rmse: 0.0700237\tvalid's rmse: 0.0736036\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1337]\ttrain's rmse: 0.0674385\tvalid's rmse: 0.094339\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1330]\ttrain's rmse: 0.0682411\tvalid's rmse: 0.0828005\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[2700]\ttrain's rmse: 0.060081\tvalid's rmse: 0.10534\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1035]\ttrain's rmse: 0.0714191\tvalid's rmse: 0.0849327\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1937]\ttrain's rmse: 0.0664754\tvalid's rmse: 0.0715234\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[2003]\ttrain's rmse: 0.0623148\tvalid's rmse: 0.102896\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[2128]\ttrain's rmse: 0.0657641\tvalid's rmse: 0.0689785\nLGB split 42 seed 7 OOF: 0.08655077648822491\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1842]\ttrain's rmse: 0.0671059\tvalid's rmse: 0.0738396\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1120]\ttrain's rmse: 0.0692965\tvalid's rmse: 0.0947926\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1329]\ttrain's rmse: 0.0683401\tvalid's rmse: 0.083658\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[3367]\ttrain's rmse: 0.0579914\tvalid's rmse: 0.105053\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[684]\ttrain's rmse: 0.0757667\tvalid's rmse: 0.0855322\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1737]\ttrain's rmse: 0.0675309\tvalid's rmse: 0.0718839\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1555]\ttrain's rmse: 0.0648864\tvalid's rmse: 0.103393\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[5163]\ttrain's rmse: 0.0565444\tvalid's rmse: 0.0687686\nLGB split 42 seed 42 OOF: 0.08685890890565168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1291]\ttrain's rmse: 0.0705062\tvalid's rmse: 0.0742929\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1322]\ttrain's rmse: 0.0676976\tvalid's rmse: 0.0948128\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1130]\ttrain's rmse: 0.0699983\tvalid's rmse: 0.0832855\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[2791]\ttrain's rmse: 0.0597308\tvalid's rmse: 0.104716\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[893]\ttrain's rmse: 0.0728169\tvalid's rmse: 0.0849865\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1601]\ttrain's rmse: 0.0684979\tvalid's rmse: 0.0728776\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1958]\ttrain's rmse: 0.0625253\tvalid's rmse: 0.102719\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[2447]\ttrain's rmse: 0.0645121\tvalid's rmse: 0.0690991\nLGB split 42 seed 2025 OOF: 0.08679915604398557\nLGB split 42 blended seeds CV: 0.08662247995012161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CB split 42 seed 7 OOF: 0.08651405928813617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CB split 42 seed 42 OOF: 0.08649545911911077\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CB split 42 seed 2025 OOF: 0.08650920878616801\nCB split 42 blended seeds CV: 0.0861648606238306\n--- Split seed 777 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[2051]\ttrain's rmse: 0.0645006\tvalid's rmse: 0.0859913\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[631]\ttrain's rmse: 0.0773221\tvalid's rmse: 0.0737914\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[2456]\ttrain's rmse: 0.0636422\tvalid's rmse: 0.0823387\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1745]\ttrain's rmse: 0.0671703\tvalid's rmse: 0.0798162\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[2066]\ttrain's rmse: 0.0633397\tvalid's rmse: 0.104263\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1287]\ttrain's rmse: 0.0682879\tvalid's rmse: 0.0911895\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[678]\ttrain's rmse: 0.0766524\tvalid's rmse: 0.0717539\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1940]\ttrain's rmse: 0.063647\tvalid's rmse: 0.0991081\nLGB split 777 seed 7 OOF: 0.08637382185707773\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[2257]\ttrain's rmse: 0.0636461\tvalid's rmse: 0.0855932\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[586]\ttrain's rmse: 0.0781826\tvalid's rmse: 0.0733462\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[2165]\ttrain's rmse: 0.0649358\tvalid's rmse: 0.08171\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1907]\ttrain's rmse: 0.0665785\tvalid's rmse: 0.0802221\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[2273]\ttrain's rmse: 0.0624058\tvalid's rmse: 0.103934\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1370]\ttrain's rmse: 0.0676342\tvalid's rmse: 0.0909848\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[650]\ttrain's rmse: 0.0772517\tvalid's rmse: 0.0715847\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[2232]\ttrain's rmse: 0.0622368\tvalid's rmse: 0.099482\nLGB split 777 seed 42 OOF: 0.08621274051102557\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[2290]\ttrain's rmse: 0.0634761\tvalid's rmse: 0.0858605\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[701]\ttrain's rmse: 0.0763349\tvalid's rmse: 0.073789\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[2194]\ttrain's rmse: 0.0646934\tvalid's rmse: 0.0815118\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[2183]\ttrain's rmse: 0.0652629\tvalid's rmse: 0.0802758\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1558]\ttrain's rmse: 0.0659906\tvalid's rmse: 0.103709\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1501]\ttrain's rmse: 0.0666867\tvalid's rmse: 0.0907639\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[596]\ttrain's rmse: 0.0782789\tvalid's rmse: 0.0713732\nTraining until validation scores don't improve for 500 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[2023]\ttrain's rmse: 0.0632113\tvalid's rmse: 0.0994276\nLGB split 777 seed 2025 OOF: 0.08618616533255861\nLGB split 777 blended seeds CV: 0.08615981693803354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CB split 777 seed 7 OOF: 0.0864775044391502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CB split 777 seed 42 OOF: 0.0866485247088922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CB split 777 seed 2025 OOF: 0.08732978920289618\nCB split 777 blended seeds CV: 0.0863866721831346\nDual-split LGB CV: 0.085840 | Dual-split CB CV: 0.085882\nSaved averaged base OOF/test arrays for meta. | elapsed 611.1s\n"
          ]
        }
      ]
    },
    {
      "id": "87daa5f8-4c7e-4cd1-bd07-9db6f061a4fc",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Validate and standardize submission format\n",
        "import pandas as pd, numpy as np\n",
        "test_ids = pd.read_csv('test.csv')['id']\n",
        "sub = pd.read_csv('submission.csv')\n",
        "print('Submission head:', sub.head())\n",
        "assert list(sub.columns) == ['id','bandgap_energy_ev'], f'Bad columns: {sub.columns.tolist()}'\n",
        "assert len(sub) == len(test_ids), f'Row count mismatch: {len(sub)} vs {len(test_ids)}'\n",
        "assert set(sub['id']) == set(test_ids), 'ID set mismatch with test.csv'\n",
        "assert sub['bandgap_energy_ev'].notna().all(), 'Found NaNs in predictions'\n",
        "assert np.isfinite(sub['bandgap_energy_ev']).all(), 'Found non-finite values in predictions'\n",
        "# enforce types and order by id to be safe\n",
        "sub = sub[['id','bandgap_energy_ev']].copy()\n",
        "sub['id'] = sub['id'].astype(int)\n",
        "sub['bandgap_energy_ev'] = sub['bandgap_energy_ev'].astype(float)\n",
        "sub = sub.merge(test_ids.to_frame('id'), on='id', how='right')\n",
        "sub = sub[['id','bandgap_energy_ev']].sort_values('id')\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('submission.csv validated and saved:', sub.shape, sub.dtypes.to_dict())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission head:    id  bandgap_energy_ev\n0   1           1.884447\n1   2           1.701527\n2   3           4.335909\n3   4           2.973716\n4   5           1.222728\nsubmission.csv validated and saved: (240, 2) {'id': dtype('int64'), 'bandgap_energy_ev': dtype('float64')}\n"
          ]
        }
      ]
    },
    {
      "id": "6c55393c-bd8b-4453-ae1a-c496327cd3e8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Diagnose expected submission columns and target availability\n",
        "import pandas as pd, numpy as np\n",
        "train = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "print('Train columns:', train.columns.tolist())\n",
        "print('Test columns:', test_df.columns.tolist())\n",
        "for col in ['bandgap_energy_ev', 'formation_energy_ev_natom']:\n",
        "    if col in train.columns:\n",
        "        mn, mx = train[col].min(), train[col].max()\n",
        "        print(f\"{col}: min={mn}, max={mx}\")\n",
        "    else:\n",
        "        print(col, 'NOT FOUND in train.csv')\n",
        "print('submission.csv preview:')\n",
        "print(pd.read_csv('submission.csv').head())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train columns: ['id', 'spacegroup', 'number_of_total_atoms', 'percent_atom_al', 'percent_atom_ga', 'percent_atom_in', 'lattice_vector_1_ang', 'lattice_vector_2_ang', 'lattice_vector_3_ang', 'lattice_angle_alpha_degree', 'lattice_angle_beta_degree', 'lattice_angle_gamma_degree', 'formation_energy_ev_natom', 'bandgap_energy_ev']\nTest columns: ['id', 'spacegroup', 'number_of_total_atoms', 'percent_atom_al', 'percent_atom_ga', 'percent_atom_in', 'lattice_vector_1_ang', 'lattice_vector_2_ang', 'lattice_vector_3_ang', 'lattice_angle_alpha_degree', 'lattice_angle_beta_degree', 'lattice_angle_gamma_degree']\nbandgap_energy_ev: min=0.0001, max=5.2861\nformation_energy_ev_natom: min=0.0, max=0.6572\nsubmission.csv preview:\n   id  bandgap_energy_ev\n0   1           1.884447\n1   2           1.701527\n2   3           4.335909\n3   4           2.973716\n4   5           1.222728\n"
          ]
        }
      ]
    },
    {
      "id": "6495ef75-a87f-463a-8471-85ab7e5bb30f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train formation_energy_ev_natom model (LGBM) and build multi-target submission\n",
        "import numpy as np, pandas as pd, time, gc, json\n",
        "from pathlib import Path\n",
        "import lightgbm as lgb\n",
        "\n",
        "print('Formation energy model start')\n",
        "t0 = time.time()\n",
        "\n",
        "# Load caches and folds\n",
        "fold_ids = np.load('fold_ids.npy')\n",
        "X_tr = pd.read_parquet('X.parquet')\n",
        "X_te = pd.read_parquet('X_test.parquet')\n",
        "tr = pd.read_parquet('train_fe.parquet')\n",
        "te = pd.read_parquet('test_fe.parquet')\n",
        "train_csv = pd.read_csv('train.csv')\n",
        "\n",
        "# Target (log1p)\n",
        "y_form = train_csv['formation_energy_ev_natom'].astype(float).values\n",
        "y_form_log = np.log1p(np.clip(y_form, 0, None))\n",
        "\n",
        "# Centralized encodings using same folds and target y_form_log\n",
        "X_tr_enc, X_te_enc, _ = add_encoded_features(X_tr, X_te, tr, te, y_form_log, fold_ids, seed=PRIMARY_SEED)\n",
        "\n",
        "# For LGBM stability: drop te_* (use physics + fe_ only), drop const\n",
        "drop_te_cols = [c for c in X_tr_enc.columns if c.startswith('te_')]\n",
        "X_tr_lgb = X_tr_enc.drop(columns=drop_te_cols, errors='ignore').copy()\n",
        "X_te_lgb = X_te_enc.drop(columns=drop_te_cols, errors='ignore').copy()\n",
        "std = X_tr_lgb.std(numeric_only=True)\n",
        "const_cols = list(std[std == 0].index)\n",
        "if const_cols:\n",
        "    X_tr_lgb = X_tr_lgb.drop(columns=const_cols, errors='ignore')\n",
        "    X_te_lgb = X_te_lgb.drop(columns=const_cols, errors='ignore')\n",
        "print('Form LGB matrices:', X_tr_lgb.shape, X_te_lgb.shape)\n",
        "\n",
        "# LGBM params (mirrored, slightly stronger leaf regularization for smoother target)\n",
        "params = {\n",
        "    'objective': 'regression', 'metric': 'rmse',\n",
        "    'learning_rate': 0.023, 'num_leaves': 48, 'max_depth': -1,\n",
        "    'min_data_in_leaf': 200, 'feature_fraction': 0.62,\n",
        "    'bagging_fraction': 0.80, 'bagging_freq': 1,\n",
        "    'lambda_l2': 15.0, 'lambda_l1': 0.0,\n",
        "    'verbosity': -1, 'num_threads': N_THREADS,\n",
        "    'deterministic': True, 'force_col_wise': True\n",
        "}\n",
        "\n",
        "seeds = SEEDS\n",
        "n_splits = len(np.unique(fold_ids))\n",
        "oof_seeds, pred_seeds = [], []\n",
        "for SEED in seeds:\n",
        "    p = dict(params); p['seed'] = int(SEED)\n",
        "    oof = np.zeros(len(X_tr_lgb), dtype=float)\n",
        "    pred = np.zeros(len(X_te_lgb), dtype=float)\n",
        "    t0s = time.time()\n",
        "    for k in range(n_splits):\n",
        "        tr_idx = np.where(fold_ids != k)[0]; va_idx = np.where(fold_ids == k)[0]\n",
        "        dtr = lgb.Dataset(X_tr_lgb.iloc[tr_idx], label=y_form_log[tr_idx], free_raw_data=False)\n",
        "        dva = lgb.Dataset(X_tr_lgb.iloc[va_idx], label=y_form_log[va_idx], free_raw_data=False)\n",
        "        m = lgb.train(p, dtr, num_boost_round=6000, valid_sets=[dtr, dva], valid_names=['train','valid'], callbacks=[lgb.early_stopping(400), lgb.log_evaluation(0)])\n",
        "        oof[va_idx] = m.predict(X_tr_lgb.iloc[va_idx], num_iteration=m.best_iteration)\n",
        "        pred += m.predict(X_te_lgb, num_iteration=m.best_iteration) / n_splits\n",
        "        del m, dtr, dva; gc.collect()\n",
        "    oof_seeds.append(oof); pred_seeds.append(pred)\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    rmse = float(mean_squared_error(y_form_log, oof) ** 0.5)\n",
        "    print(f'Form LGB SEED {SEED}: OOF RMSLE {rmse:.6f}')\n",
        "\n",
        "oof_avg = np.mean(np.vstack(oof_seeds), axis=0)\n",
        "pred_avg = np.mean(np.vstack(pred_seeds), axis=0)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "cv = float(mean_squared_error(y_form_log, oof_avg) ** 0.5)\n",
        "print(f'Form LGB blended seeds CV RMSLE: {cv:.6f}')\n",
        "np.save('oof_form_lgbm.npy', oof_avg)\n",
        "np.save('pred_form_lgbm_test.npy', pred_avg)\n",
        "\n",
        "# Build multi-target submission by merging formation predictions with current bandgap submission\n",
        "sub_bg = pd.read_csv('submission.csv')  # contains id, bandgap_energy_ev\n",
        "form_pred = np.expm1(pred_avg).clip(0, None)\n",
        "sub = pd.DataFrame({'id': pd.read_csv('test.csv')['id']})\n",
        "sub['formation_energy_ev_natom'] = form_pred\n",
        "sub = sub.merge(sub_bg, on='id', how='left')\n",
        "assert sub['bandgap_energy_ev'].notna().all(), 'Missing bandgap predictions when merging'\n",
        "sub = sub[['id', 'formation_energy_ev_natom', 'bandgap_energy_ev']]\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Multi-target submission.csv saved:', sub.shape, '| elapsed', f'{time.time()-t0:.1f}s')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formation energy model start\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Form LGB matrices: (2160, 125) (240, 125)\nTraining until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[4280]\ttrain's rmse: 0.0246047\tvalid's rmse: 0.0313949\nTraining until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1174]\ttrain's rmse: 0.0281075\tvalid's rmse: 0.0352266\nTraining until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[3231]\ttrain's rmse: 0.0255598\tvalid's rmse: 0.0299978\nTraining until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[2419]\ttrain's rmse: 0.0255088\tvalid's rmse: 0.0355533\nTraining until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1170]\ttrain's rmse: 0.0285994\tvalid's rmse: 0.0290569\nTraining until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1333]\ttrain's rmse: 0.0266241\tvalid's rmse: 0.0388658\nTraining until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[3260]\ttrain's rmse: 0.0252959\tvalid's rmse: 0.0310031\nForm LGB SEED 7: OOF RMSLE 0.033014\nTraining until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[4736]\ttrain's rmse: 0.0243078\tvalid's rmse: 0.0312392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1142]\ttrain's rmse: 0.0281651\tvalid's rmse: 0.0352259\nTraining until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[2602]\ttrain's rmse: 0.0261938\tvalid's rmse: 0.0301218\nTraining until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1462]\ttrain's rmse: 0.0275537\tvalid's rmse: 0.0326305\nTraining until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[3219]\ttrain's rmse: 0.0245709\tvalid's rmse: 0.035575\nTraining until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1252]\ttrain's rmse: 0.0283729\tvalid's rmse: 0.0287606\nTraining until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1642]\ttrain's rmse: 0.0260012\tvalid's rmse: 0.0390344\nTraining until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[3647]\ttrain's rmse: 0.0249991\tvalid's rmse: 0.0310648\nForm LGB SEED 42: OOF RMSLE 0.033064\nTraining until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[4789]\ttrain's rmse: 0.0242708\tvalid's rmse: 0.0314578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1056]\ttrain's rmse: 0.0283244\tvalid's rmse: 0.0353031\nTraining until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[2570]\ttrain's rmse: 0.0262369\tvalid's rmse: 0.0303921\nTraining until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[2184]\ttrain's rmse: 0.0264441\tvalid's rmse: 0.0324046\nTraining until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[3144]\ttrain's rmse: 0.0246561\tvalid's rmse: 0.0355834\nTraining until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1322]\ttrain's rmse: 0.0281652\tvalid's rmse: 0.0289095\nTraining until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1372]\ttrain's rmse: 0.0265239\tvalid's rmse: 0.039015\nTraining until validation scores don't improve for 400 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[4177]\ttrain's rmse: 0.0245691\tvalid's rmse: 0.0308337\nForm LGB SEED 2025: OOF RMSLE 0.033096\nForm LGB blended seeds CV RMSLE: 0.033022\nMulti-target submission.csv saved: (240, 3) | elapsed 31.6s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}