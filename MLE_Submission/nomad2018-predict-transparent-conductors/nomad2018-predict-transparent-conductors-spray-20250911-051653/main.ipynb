{
  "cells": [
    {
      "id": "2a0a2c6a-a83d-4312-b678-9cefff2d1f52",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan to WIN A MEDAL: NOMAD2018 Predicting Transparent Conductors\n",
        "\n",
        "Objective: Predict bandgap_energy_ev for test structures; optimize RMSLE (single-column case of mean-column-wise-rmsle).\n",
        "\n",
        "High-level strategy:\n",
        "- Data audit: Locate train.csv/test.csv or alternative labels file; map structure folders (ids) to targets.\n",
        "- Feature engineering:\n",
        "  - Composition-only features from each geometry.xyz: element counts and fractions (Al, Ga, In, O), O ratio, total atoms (N_total), predicted N from stoichiometry.\n",
        "  - Matminer composition featurizers: Stoichiometry, ElementProperty (Magpie), ValenceOrbital, AtomicOrbitals, IonProperty.\n",
        "  - Optional structural proxies from XYZ (no lattice):\n",
        "    - Centered pairwise-distance statistics (mean/std/min/max, RDF histogram), nearest-neighbor stats by element pairs.\n",
        "    - Coordination counts via distance thresholds per element pair (heuristic radii).\n",
        "- Modeling:\n",
        "  - Target transform: y_log = log1p(y); optimize RMSLE naturally; predictions = expm1(y_pred), clip >= 0.\n",
        "  - Strong baselines: LightGBM, CatBoost, XGBoost. Start with LGBM; add CatBoost; blend/stack.\n",
        "  - 5-fold KFold with shuffle (seed) for quick iteration; consider GroupKFold by composition signature if leakage suspected.\n",
        "  - Early stopping, robust seeds; feature importance to iterate.\n",
        "- Tuning:\n",
        "  - Quick grid for LGBM (num_leaves, max_depth, min_data_in_leaf, feature_fraction, bagging_fraction, lambda_l1/l2).\n",
        "  - Consider Optuna if time permits.\n",
        "- Inference & submission:\n",
        "  - Build test features with identical pipeline.\n",
        "  - Save submission.csv with columns: id, bandgap_energy_ev.\n",
        "\n",
        "Risk & unknowns to resolve immediately:\n",
        "- Repository currently shows train/ and test/ with per-id geometry.xyz files; train.csv not visible. Need to discover label file (possibly at root or a metadata CSV).\n",
        "- If only structures provided, we will parse labels from a CSV like train.csv or targets.csv; else, abort and search docs.\n",
        "\n",
        "First steps (next cells):\n",
        "1) Probe filesystem for any CSVs (train.csv, test.csv, targets.csv, metadata.csv).\n",
        "2) If found, load and inspect target distribution and id alignment with folders.\n",
        "3) Implement robust parser:\n",
        "   - Read XYZ -> element list and cartesian coords; compute composition; build composition features.\n",
        "   - Parallelize with joblib; cache to parquet for fast iteration.\n",
        "4) Train LGBM baseline with CV; log fold times and scores; create feature importance.\n",
        "5) If CV RMSLE > 0.07, iterate features (RDF/NN stats) and/or swap model to CatBoost/XGB and blend.\n",
        "\n",
        "Time management:\n",
        "- Always log progress; parallel feature build to utilize 36 vCPUs; avoid heavy structure features that need periodic boundary conditions (not available).\n",
        "- While long runs execute, request expert review for guidance/interrupts."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "a1f0ee94-9281-425b-974e-26229591ee0a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Data audit: list files, load CSVs, inspect, and verify folder alignment\n",
        "import os, sys, json, math, textwrap, time, gc\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "print('CWD:', os.getcwd())\n",
        "print('Top-level files/folders:', os.listdir('.'))\n",
        "\n",
        "# Load CSVs\n",
        "train_csv = 'train.csv'\n",
        "test_csv = 'test.csv'\n",
        "assert os.path.exists(train_csv), f\"Missing {train_csv} in CWD\"\n",
        "assert os.path.exists(test_csv), f\"Missing {test_csv} in CWD\"\n",
        "train = pd.read_csv(train_csv)\n",
        "test = pd.read_csv(test_csv)\n",
        "print('train shape:', train.shape)\n",
        "print('test shape:', test.shape)\n",
        "print('train columns:', train.columns.tolist())\n",
        "print('test columns:', test.columns.tolist())\n",
        "\n",
        "# Basic info\n",
        "print('\\ntrain.info():')\n",
        "print(train.info())\n",
        "print('\\ntest.info():')\n",
        "print(test.info())\n",
        "\n",
        "# Target exploration\n",
        "target_col = 'bandgap_energy_ev'\n",
        "assert target_col in train.columns, f\"Target column {target_col} not in train.csv columns\"\n",
        "print('\\nTarget describe:')\n",
        "print(train[target_col].describe())\n",
        "print('Num zeros in target:', int((train[target_col] == 0).sum()))\n",
        "print('Num NaNs in target:', int(train[target_col].isna().sum()))\n",
        "\n",
        "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
        "ax.hist(train[target_col].dropna(), bins=50, color='steelblue', edgecolor='k', alpha=0.8)\n",
        "ax.set_title('bandgap_energy_ev histogram')\n",
        "ax.set_xlabel('bandgap (eV)')\n",
        "ax.set_ylabel('count')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Verify alignment with geometry.xyz folders\n",
        "def verify_paths(df, split_name):\n",
        "    base = Path(split_name)\n",
        "    assert base.exists(), f\"Missing folder: {base}\"\n",
        "    assert 'id' in df.columns, \"id column missing in CSV\"\n",
        "    ids = df['id'].astype(str).values\n",
        "    missing = []\n",
        "    for i, sid in enumerate(ids):\n",
        "        path = base / sid / 'geometry.xyz'\n",
        "        if not path.exists():\n",
        "            missing.append(str(path))\n",
        "        if (i+1) % 2000 == 0:\n",
        "            print(f'Checked {i+1}/{len(ids)} {split_name} ids...')\n",
        "    print(f\"{split_name}: total ids={len(ids)}, missing geometries={len(missing)}\")\n",
        "    if missing:\n",
        "        print('Examples of missing:', missing[:5])\n",
        "    assert len(missing) == 0, f\"Found {len(missing)} missing geometry.xyz files in {split_name}\"\n",
        "\n",
        "verify_paths(train, 'train')\n",
        "verify_paths(test, 'test')\n",
        "print('Data audit completed successfully.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "ad4a7f67-330a-4e44-8847-fe1e20689247",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Baseline features, GroupKFold CV, LightGBM model, submission\n",
        "import os, time, math, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def cell_volume(a, b, c, alpha_deg, beta_deg, gamma_deg):\n",
        "    # Volume for triclinic cell from a,b,c and angles\n",
        "    alpha = np.deg2rad(alpha_deg); beta = np.deg2rad(beta_deg); gamma = np.deg2rad(gamma_deg)\n",
        "    cos_alpha, cos_beta, cos_gamma = np.cos(alpha), np.cos(beta), np.cos(gamma)\n",
        "    term = 1 + 2*cos_alpha*cos_beta*cos_gamma - cos_alpha**2 - cos_beta**2 - cos_gamma**2\n",
        "    term = np.clip(term, 0, None)\n",
        "    return a * b * c * np.sqrt(term)\n",
        "\n",
        "def add_engineered(df):\n",
        "    df = df.copy()\n",
        "    # Volume and density-like features\n",
        "    df['cell_volume'] = cell_volume(df['lattice_vector_1_ang'], df['lattice_vector_2_ang'], df['lattice_vector_3_ang'],\n",
        "                                     df['lattice_angle_alpha_degree'], df['lattice_angle_beta_degree'], df['lattice_angle_gamma_degree'])\n",
        "    df['atoms_per_volume'] = df['number_of_total_atoms'] / (df['cell_volume'].replace(0, np.nan))\n",
        "    # Composition counts from percent and total atoms\n",
        "    for el, col in [('al','percent_atom_al'), ('ga','percent_atom_ga'), ('in','percent_atom_in')]:\n",
        "        df[f'n_{el}'] = np.rint(df['number_of_total_atoms'] * df[col] / 100.0).astype(int)\n",
        "    df['percent_atom_o'] = 100.0 - (df['percent_atom_al'] + df['percent_atom_ga'] + df['percent_atom_in'])\n",
        "    df['n_o'] = (df['number_of_total_atoms'] - (df['n_al'] + df['n_ga'] + df['n_in'])).astype(int)\n",
        "    # Fractions (0-1) for tree models; keep percents too\n",
        "    for el in ['al','ga','in','o']:\n",
        "        pcol = f'percent_atom_{el}'\n",
        "        if pcol in df.columns:\n",
        "            df[f'frac_{el}'] = df[pcol] / 100.0\n",
        "    # Cation ratios and stats\n",
        "    df['frac_cation'] = df[['frac_al','frac_ga','frac_in']].sum(axis=1)\n",
        "    df['frac_o_to_cation'] = df['frac_o'] / (df['frac_cation'] + 1e-9)\n",
        "    df['mix_entropy_cation'] = -np.sum(np.where(df[['frac_al','frac_ga','frac_in']]>0, df[['frac_al','frac_ga','frac_in']] * np.log(df[['frac_al','frac_ga','frac_in']]+1e-12), 0), axis=1)\n",
        "    df['hhi_cation'] = np.sum(df[['frac_al','frac_ga','frac_in']]**2, axis=1)\n",
        "    # Angles trigonometric\n",
        "    for ang in ['alpha','beta','gamma']:\n",
        "        col = f'lattice_angle_{ang}_degree'\n",
        "        df[f'cos_{ang}'] = np.cos(np.deg2rad(df[col]))\n",
        "        df[f'sin_{ang}'] = np.sin(np.deg2rad(df[col]))\n",
        "    # Safe fill for infinities\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    return df\n",
        "\n",
        "# Reload CSVs to ensure clean state\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "train_fe = add_engineered(train)\n",
        "test_fe = add_engineered(test)\n",
        "\n",
        "# Group key to avoid leakage: integer counts per element\n",
        "group_cols = ['n_al','n_ga','n_in','n_o']\n",
        "groups = train_fe[group_cols].astype(int).astype(str).agg('_'.join, axis=1)\n",
        "\n",
        "# Features selection\n",
        "drop_cols = ['id','bandgap_energy_ev','formation_energy_ev_natom']\n",
        "features = [c for c in train_fe.columns if c not in drop_cols]\n",
        "cat_cols = ['spacegroup']\n",
        "\n",
        "X = train_fe[features]\n",
        "X_test = test_fe[features]\n",
        "y = train_fe['bandgap_energy_ev'].astype(float)\n",
        "\n",
        "# LightGBM setup\n",
        "import importlib\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except Exception as e:\n",
        "    import sys, subprocess\n",
        "    print('Installing lightgbm...'); subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "# Log-transform for RMSLE\n",
        "y_log = np.log1p(y.clip(lower=0))\n",
        "\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'learning_rate': 0.04,\n",
        "    'num_leaves': 192,\n",
        "    'max_depth': -1,\n",
        "    'min_data_in_leaf': 80,\n",
        "    'feature_fraction': 0.85,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 1,\n",
        "    'lambda_l2': 0.2,\n",
        "    'verbosity': -1,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "n_splits = 5\n",
        "gkf = GroupKFold(n_splits=n_splits)\n",
        "oof_pred = np.zeros(len(X))\n",
        "test_pred = np.zeros(len(X_test))\n",
        "\n",
        "fold_times = []\n",
        "for fold, (trn_idx, val_idx) in enumerate(gkf.split(X, y_log, groups=groups), 1):\n",
        "    t0 = time.time()\n",
        "    X_tr, X_va = X.iloc[trn_idx], X.iloc[val_idx]\n",
        "    y_tr, y_va = y_log.iloc[trn_idx], y_log.iloc[val_idx]\n",
        "    lgb_train = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_cols, free_raw_data=False)\n",
        "    lgb_valid = lgb.Dataset(X_va, label=y_va, categorical_feature=cat_cols, free_raw_data=False)\n",
        "    model = lgb.train(params, lgb_train, num_boost_round=5000, valid_sets=[lgb_train, lgb_valid],\n",
        "                      valid_names=['train','valid'],\n",
        "                      callbacks=[lgb.early_stopping(200), lgb.log_evaluation(200)])\n",
        "    oof_pred[val_idx] = model.predict(X_va, num_iteration=model.best_iteration)\n",
        "    test_pred += model.predict(X_test, num_iteration=model.best_iteration) / n_splits\n",
        "    elapsed = time.time() - t0\n",
        "    fold_times.append(elapsed)\n",
        "    rmse = mean_squared_error(y_va, oof_pred[val_idx], squared=False)\n",
        "    print(f'Fold {fold}/{n_splits} RMSE(log1p): {rmse:.6f} | elapsed: {elapsed:.1f}s | best_iter: {model.best_iteration}')\n",
        "    del model, lgb_train, lgb_valid; gc.collect()\n",
        "\n",
        "# CV score in RMSLE space (since we trained on log1p)\n",
        "cv_rmse_log = mean_squared_error(y_log, oof_pred, squared=False)\n",
        "print(f'CV RMSLE: {cv_rmse_log:.6f}  | mean fold time: {np.mean(fold_times):.1f}s')\n",
        "\n",
        "# Train final model on full data (optional, we already averaged test preds across folds).\n",
        "pred_bandgap = np.expm1(test_pred).clip(min=0)\n",
        "\n",
        "# Save submission\n",
        "sub = pd.DataFrame({'id': test['id'], 'bandgap_energy_ev': pred_bandgap})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with shape:', sub.shape)\n",
        "sub.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "b9919ffa-5e04-4ff6-a522-44efa2030ec0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fix grouping, add diagnostics, enrich features, retrain LGBM\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def cell_volume(a, b, c, alpha_deg, beta_deg, gamma_deg):\n",
        "    alpha = np.deg2rad(alpha_deg); beta = np.deg2rad(beta_deg); gamma = np.deg2rad(gamma_deg)\n",
        "    ca, cb, cg = np.cos(alpha), np.cos(beta), np.cos(gamma)\n",
        "    term = 1 + 2*ca*cb*cg - ca**2 - cb**2 - cg**2\n",
        "    term = np.clip(term, 0, None)\n",
        "    return a * b * c * np.sqrt(term)\n",
        "\n",
        "def engineer_features(df):\n",
        "    df = df.copy()\n",
        "    # Geometry-derived\n",
        "    a, b, c = df['lattice_vector_1_ang'], df['lattice_vector_2_ang'], df['lattice_vector_3_ang']\n",
        "    alpha, beta, gamma = df['lattice_angle_alpha_degree'], df['lattice_angle_beta_degree'], df['lattice_angle_gamma_degree']\n",
        "    vol = cell_volume(a, b, c, alpha, beta, gamma)\n",
        "    df['cell_volume'] = vol\n",
        "    df['volume_per_atom'] = vol / df['number_of_total_atoms']\n",
        "    df['a_over_b'] = a / b\n",
        "    df['b_over_c'] = b / c\n",
        "    df['c_over_a'] = c / a\n",
        "    df['abc_mean'] = (a + b + c) / 3.0\n",
        "    df['abc_max'] = np.max(np.stack([a,b,c], axis=1), axis=1)\n",
        "    df['abc_min'] = np.min(np.stack([a,b,c], axis=1), axis=1)\n",
        "    df['abc_anisotropy'] = (df['abc_max'] - df['abc_min']) / (df['abc_mean'] + 1e-9)\n",
        "    for ang_name, series in [('alpha',alpha),('beta',beta),('gamma',gamma)]:\n",
        "        df[f'cos_{ang_name}'] = np.cos(np.deg2rad(series))\n",
        "        df[f'abs_{ang_name}_dev90'] = np.abs(series - 90.0)\n",
        "    df['orthorhombicity'] = df[['abs_alpha_dev90','abs_beta_dev90','abs_gamma_dev90']].sum(axis=1)\n",
        "    df['atoms_per_volume'] = df['number_of_total_atoms'] / (vol.replace(0, np.nan))\n",
        "\n",
        "    # Fractions\n",
        "    for el in ['al','ga','in']:\n",
        "        df[f'frac_{el}'] = df[f'percent_atom_{el}'] / 100.0\n",
        "    df['percent_atom_o'] = 100.0 - (df['percent_atom_al'] + df['percent_atom_ga'] + df['percent_atom_in'])\n",
        "    df['frac_o'] = df['percent_atom_o'] / 100.0\n",
        "    df['frac_cation'] = df[['frac_al','frac_ga','frac_in']].sum(axis=1)\n",
        "    # Mix stats\n",
        "    cat_fracs = df[['frac_al','frac_ga','frac_in']].clip(lower=0, upper=1)\n",
        "    df['mix_entropy_cation'] = -np.sum(np.where(cat_fracs>0, cat_fracs*np.log(cat_fracs+1e-12), 0), axis=1)\n",
        "    df['hhi_cation'] = np.sum(cat_fracs**2, axis=1)\n",
        "    # Pairwise interactions\n",
        "    df['al_x_ga'] = df['frac_al']*df['frac_ga']\n",
        "    df['al_x_in'] = df['frac_al']*df['frac_in']\n",
        "    df['ga_x_in'] = df['frac_ga']*df['frac_in']\n",
        "    df['al_minus_ga'] = df['frac_al']-df['frac_ga']\n",
        "    df['al_minus_in'] = df['frac_al']-df['frac_in']\n",
        "    df['ga_minus_in'] = df['frac_ga']-df['frac_in']\n",
        "    eps = 1e-6\n",
        "    df['al_over_ga'] = (df['frac_al']+eps)/(df['frac_ga']+eps)\n",
        "    df['al_over_in'] = (df['frac_al']+eps)/(df['frac_in']+eps)\n",
        "    df['ga_over_in'] = (df['frac_ga']+eps)/(df['frac_in']+eps)\n",
        "    # Categorical preparation\n",
        "    df['spacegroup'] = df['spacegroup'].astype('category')\n",
        "    df.replace([np.inf,-np.inf], np.nan, inplace=True)\n",
        "    return df\n",
        "\n",
        "def compute_stoich_groups(df):\n",
        "    # Compute integer counts using cation stoichiometry consistency\n",
        "    # For all sesquioxides: total atoms = 5N, cations = 2N, oxygens = 3N\n",
        "    N = np.rint(df['number_of_total_atoms']/5.0).astype(int)\n",
        "    n_cat = 2 * N\n",
        "    # Fractions provided are per total atoms; sum(frac_al, frac_ga, frac_in) ~ 0.4\n",
        "    frac_al = df['percent_atom_al']/100.0\n",
        "    frac_ga = df['percent_atom_ga']/100.0\n",
        "    frac_in = df['percent_atom_in']/100.0\n",
        "    frac_cations_total = (frac_al + frac_ga + frac_in).replace(0, np.nan)\n",
        "    # Convert to fractions among cations\n",
        "    w_al = (frac_al / frac_cations_total).clip(0, 1).fillna(0)\n",
        "    w_ga = (frac_ga / frac_cations_total).clip(0, 1).fillna(0)\n",
        "    # ensure sums to 1\n",
        "    w_in = (1.0 - w_al - w_ga).clip(0, 1)\n",
        "    n_al = np.rint(n_cat * w_al).astype(int)\n",
        "    n_ga = np.rint(n_cat * w_ga).astype(int)\n",
        "    n_in = (n_cat - n_al - n_ga).astype(int)\n",
        "    n_o = 3 * N\n",
        "    key = pd.Series(list(zip(N, n_al, n_ga, n_in))).astype(str)\n",
        "    return key, N, n_al, n_ga, n_in, n_o\n",
        "\n",
        "# Load fresh\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "train_fe = engineer_features(train)\n",
        "test_fe = engineer_features(test)\n",
        "\n",
        "# Build groups\n",
        "groups, N, n_al, n_ga, n_in, n_o = compute_stoich_groups(train)\n",
        "train_fe['N'] = N; train_fe['n_al'] = n_al; train_fe['n_ga'] = n_ga; train_fe['n_in'] = n_in; train_fe['n_o'] = n_o\n",
        "test_groups, N_te, al_te, ga_te, in_te, o_te = compute_stoich_groups(test)\n",
        "test_fe['N'] = N_te; test_fe['n_al'] = al_te; test_fe['n_ga'] = ga_te; test_fe['n_in'] = in_te; test_fe['n_o'] = o_te\n",
        "\n",
        "# Create balanced shuffled fold assignment at group level\n",
        "n_splits = 5\n",
        "uniq_groups = groups.drop_duplicates().sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "chunks = np.array_split(uniq_groups.values, n_splits)\n",
        "group_to_fold = {}\n",
        "for k, arr in enumerate(chunks):\n",
        "    for g in arr:\n",
        "        group_to_fold[g] = k\n",
        "fold_ids = groups.map(group_to_fold).astype(int).values\n",
        "\n",
        "# Diagnostics\n",
        "y = train_fe['bandgap_energy_ev'].astype(float)\n",
        "print('Overall target describe:\\n', y.describe())\n",
        "print('Unique groups:', len(uniq_groups))\n",
        "print('Sample groups:', uniq_groups.head().tolist())\n",
        "for k in range(n_splits):\n",
        "    val_idx = np.where(fold_ids==k)[0]\n",
        "    trn_idx = np.where(fold_ids!=k)[0]\n",
        "    print(f'Fold {k}: n={len(val_idx)}, uniq_groups={pd.Series(groups.iloc[val_idx]).nunique()}')\n",
        "    print(pd.Series(groups.iloc[val_idx]).value_counts().head())\n",
        "    print('Fold target describe:\\n', y.iloc[val_idx].describe())\n",
        "    inter = set(groups.iloc[val_idx]).intersection(set(groups.iloc[trn_idx]))\n",
        "    assert len(inter)==0, 'Group leakage detected!'\n",
        "\n",
        "# Feature list (ensure train/test alignment) and drop target\n",
        "drop_cols = ['id','bandgap_energy_ev','formation_energy_ev_natom']\n",
        "common_cols = [c for c in train_fe.columns if c in test_fe.columns]\n",
        "features = [c for c in common_cols if c not in drop_cols]\n",
        "X = train_fe[features].copy()\n",
        "X_test = test_fe[features].copy()\n",
        "y_log = np.log1p(y.clip(lower=0))\n",
        "\n",
        "# LightGBM with stronger regularization\n",
        "import lightgbm as lgb\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'learning_rate': 0.03,\n",
        "    'num_leaves': 96,\n",
        "    'max_depth': -1,\n",
        "    'min_data_in_leaf': 200,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 1,\n",
        "    'lambda_l2': 1.0,\n",
        "    'lambda_l1': 0.2,\n",
        "    'verbosity': -1,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "oof = np.zeros(len(X)); test_pred = np.zeros(len(X_test))\n",
        "fold_times = []\n",
        "for k in range(n_splits):\n",
        "    t0 = time.time()\n",
        "    val_idx = np.where(fold_ids==k)[0]\n",
        "    trn_idx = np.where(fold_ids!=k)[0]\n",
        "    dtrain = lgb.Dataset(X.iloc[trn_idx], label=y_log.iloc[trn_idx], categorical_feature=['spacegroup'], free_raw_data=False)\n",
        "    dvalid = lgb.Dataset(X.iloc[val_idx], label=y_log.iloc[val_idx], categorical_feature=['spacegroup'], free_raw_data=False)\n",
        "    model = lgb.train(params, dtrain, num_boost_round=5000, valid_sets=[dtrain,dvalid], valid_names=['train','valid'], callbacks=[lgb.early_stopping(300), lgb.log_evaluation(200)])\n",
        "    oof[val_idx] = model.predict(X.iloc[val_idx], num_iteration=model.best_iteration)\n",
        "    test_pred += model.predict(X_test, num_iteration=model.best_iteration) / n_splits\n",
        "    rmse = float(mean_squared_error(y_log.iloc[val_idx], oof[val_idx]) ** 0.5)\n",
        "    fold_times.append(time.time()-t0)\n",
        "    print(f'Fold {k} RMSE(log1p): {rmse:.6f} | best_iter: {model.best_iteration} | elapsed: {fold_times[-1]:.1f}s')\n",
        "    del model, dtrain, dvalid; gc.collect()\n",
        "\n",
        "cv = float(mean_squared_error(y_log, oof) ** 0.5)\n",
        "print(f'New CV RMSLE: {cv:.6f} | mean fold time: {np.mean(fold_times):.1f}s')\n",
        "\n",
        "# Save new submission\n",
        "pred_bandgap = np.expm1(test_pred).clip(min=0)\n",
        "sub = pd.DataFrame({'id': test['id'], 'bandgap_energy_ev': pred_bandgap})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('submission.csv saved:', sub.shape)\n",
        "sub.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall target describe:\n count    2160.000000\nmean        2.075512\nstd         1.005867\nmin         0.000100\n25%         1.275050\n50%         1.901650\n75%         2.761150\nmax         5.286100\nName: bandgap_energy_ev, dtype: float64\nUnique groups: 692\nSample groups: ['(6, 0, 2, 10)', '(16, 26, 4, 2)', '(6, 2, 2, 8)', '(16, 21, 3, 8)', '(8, 8, 0, 8)']\nFold 0: n=468, uniq_groups=139\n(6, 8, 4, 0)     16\n(8, 13, 0, 3)    15\n(8, 14, 0, 2)    13\n(8, 9, 2, 5)     12\n(8, 0, 3, 13)    11\nName: count, dtype: int64\nFold target describe:\n count    468.000000\nmean       1.950675\nstd        1.035782\nmin        0.000100\n25%        1.118925\n50%        1.808550\n75%        2.543750\nmax        5.286100\nName: bandgap_energy_ev, dtype: float64\nFold 1: n=461, uniq_groups=139\n(6, 6, 6, 0)       19\n(6, 4, 8, 0)       14\n(8, 10, 1, 5)      13\n(16, 1, 1, 30)     13\n(16, 0, 16, 16)    13\nName: count, dtype: int64\nFold target describe:\n count    461.000000\nmean       2.185851\nstd        0.997888\nmin        0.233100\n25%        1.342500\n50%        1.935200\n75%        3.006000\nmax        4.894600\nName: bandgap_energy_ev, dtype: float64\nFold 2: n=385, uniq_groups=138\n(8, 12, 0, 4)      11\n(8, 1, 10, 5)      10\n(4, 3, 5, 0)       10\n(16, 0, 14, 18)     9\n(8, 11, 5, 0)       9\nName: count, dtype: int64\nFold target describe:\n count    385.000000\nmean       1.981778\nstd        0.924152\nmin        0.202200\n25%        1.202300\n50%        1.845400\n75%        2.658200\nmax        4.670500\nName: bandgap_energy_ev, dtype: float64\nFold 3: n=381, uniq_groups=138\n(16, 18, 14, 0)    15\n(6, 7, 5, 0)       13\n(6, 9, 1, 2)       11\n(8, 13, 3, 0)      11\n(16, 13, 19, 0)    10\nName: count, dtype: int64\nFold target describe:\n count    381.000000\nmean       2.112460\nstd        1.032963\nmin        0.005700\n25%        1.318500\n50%        1.886300\n75%        2.844700\nmax        5.245700\nName: bandgap_energy_ev, dtype: float64\nFold 4: n=465, uniq_groups=138\n(6, 5, 7, 0)       22\n(16, 16, 16, 0)    15\n(16, 15, 17, 0)    13\n(16, 19, 13, 0)    12\n(16, 15, 11, 6)    11\nName: count, dtype: int64\nFold target describe:\n count    465.000000\nmean       2.139097\nstd        1.009885\nmin        0.004900\n25%        1.307300\n50%        2.039800\n75%        2.848500\nmax        5.211400\nName: bandgap_energy_ev, dtype: float64\nTraining until validation scores don't improve for 300 rounds\n[200]\ttrain's rmse: 0.0893365\tvalid's rmse: 0.11413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain's rmse: 0.0800665\tvalid's rmse: 0.105045\n[600]\ttrain's rmse: 0.0753728\tvalid's rmse: 0.100822\n[800]\ttrain's rmse: 0.0723948\tvalid's rmse: 0.0983915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain's rmse: 0.0703115\tvalid's rmse: 0.0973343\n[1200]\ttrain's rmse: 0.0686176\tvalid's rmse: 0.0969536\n[1400]\ttrain's rmse: 0.0673267\tvalid's rmse: 0.0966142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\ttrain's rmse: 0.0661675\tvalid's rmse: 0.096612\n[1800]\ttrain's rmse: 0.0651302\tvalid's rmse: 0.0966671\nEarly stopping, best iteration is:\n[1502]\ttrain's rmse: 0.0666871\tvalid's rmse: 0.096564\nFold 0 RMSE(log1p): 0.096564 | best_iter: 1502 | elapsed: 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 300 rounds\n[200]\ttrain's rmse: 0.0923241\tvalid's rmse: 0.0930194\n[400]\ttrain's rmse: 0.0822953\tvalid's rmse: 0.0884679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0777468\tvalid's rmse: 0.0865252\n[800]\ttrain's rmse: 0.0748294\tvalid's rmse: 0.085685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain's rmse: 0.0728146\tvalid's rmse: 0.0852458\n[1200]\ttrain's rmse: 0.0711822\tvalid's rmse: 0.0850852\n[1400]\ttrain's rmse: 0.0698328\tvalid's rmse: 0.0848514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\ttrain's rmse: 0.0687103\tvalid's rmse: 0.0846592\n[1800]\ttrain's rmse: 0.0677057\tvalid's rmse: 0.0847021\n[2000]\ttrain's rmse: 0.066815\tvalid's rmse: 0.0845745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2200]\ttrain's rmse: 0.0659828\tvalid's rmse: 0.0845675\n[2400]\ttrain's rmse: 0.06523\tvalid's rmse: 0.0844758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2600]\ttrain's rmse: 0.0645406\tvalid's rmse: 0.0846742\nEarly stopping, best iteration is:\n[2354]\ttrain's rmse: 0.0653934\tvalid's rmse: 0.0844583\nFold 1 RMSE(log1p): 0.084458 | best_iter: 2354 | elapsed: 1.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 300 rounds\n[200]\ttrain's rmse: 0.0907615\tvalid's rmse: 0.0954211\n[400]\ttrain's rmse: 0.0806605\tvalid's rmse: 0.0922907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0760448\tvalid's rmse: 0.0907279\n[800]\ttrain's rmse: 0.0732371\tvalid's rmse: 0.0902232\n[1000]\ttrain's rmse: 0.0712328\tvalid's rmse: 0.0900915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0696421\tvalid's rmse: 0.08997\n[1400]\ttrain's rmse: 0.068331\tvalid's rmse: 0.0900507\nEarly stopping, best iteration is:\n[1254]\ttrain's rmse: 0.0692788\tvalid's rmse: 0.0899133\nFold 2 RMSE(log1p): 0.089913 | best_iter: 1254 | elapsed: 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 300 rounds\n[200]\ttrain's rmse: 0.0918792\tvalid's rmse: 0.0963871\n[400]\ttrain's rmse: 0.0826031\tvalid's rmse: 0.0888624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0780258\tvalid's rmse: 0.0857932\n[800]\ttrain's rmse: 0.0754138\tvalid's rmse: 0.0843728\n[1000]\ttrain's rmse: 0.0735065\tvalid's rmse: 0.0836148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0720295\tvalid's rmse: 0.0830419\n[1400]\ttrain's rmse: 0.0708707\tvalid's rmse: 0.0825756\n[1600]\ttrain's rmse: 0.0697821\tvalid's rmse: 0.0822702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\ttrain's rmse: 0.0688293\tvalid's rmse: 0.0819862\n[2000]\ttrain's rmse: 0.0679862\tvalid's rmse: 0.0818472\n[2200]\ttrain's rmse: 0.0671905\tvalid's rmse: 0.0816003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\ttrain's rmse: 0.0664088\tvalid's rmse: 0.0815923\n[2600]\ttrain's rmse: 0.0656912\tvalid's rmse: 0.0815257\nEarly stopping, best iteration is:\n[2316]\ttrain's rmse: 0.0667402\tvalid's rmse: 0.0814752\nFold 3 RMSE(log1p): 0.081475 | best_iter: 2316 | elapsed: 1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 300 rounds\n[200]\ttrain's rmse: 0.0934315\tvalid's rmse: 0.0936426\n[400]\ttrain's rmse: 0.0840476\tvalid's rmse: 0.0861089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.079551\tvalid's rmse: 0.0829195\n[800]\ttrain's rmse: 0.0767522\tvalid's rmse: 0.0810449\n[1000]\ttrain's rmse: 0.0747718\tvalid's rmse: 0.0801754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0732264\tvalid's rmse: 0.0793587\n[1400]\ttrain's rmse: 0.0718615\tvalid's rmse: 0.0789466\n[1600]\ttrain's rmse: 0.070686\tvalid's rmse: 0.0786586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\ttrain's rmse: 0.0696695\tvalid's rmse: 0.0785104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\ttrain's rmse: 0.0687292\tvalid's rmse: 0.0784933\n[2200]\ttrain's rmse: 0.0678703\tvalid's rmse: 0.0784836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\ttrain's rmse: 0.0670541\tvalid's rmse: 0.0784728\nEarly stopping, best iteration is:\n[2289]\ttrain's rmse: 0.0674853\tvalid's rmse: 0.0783439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 RMSE(log1p): 0.078344 | best_iter: 2289 | elapsed: 1.6s\nNew CV RMSLE: 0.086464 | mean fold time: 1.2s\nsubmission.csv saved: (240, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "   id  bandgap_energy_ev\n0   1           1.918390\n1   2           1.699429\n2   3           4.327457\n3   4           2.898343\n4   5           1.143205",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>bandgap_energy_ev</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1.918390</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1.699429</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>4.327457</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2.898343</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1.143205</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "28d41042-462c-43a6-b76a-44bb867cd97a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1) Stabilize CV: Stratify stoichiometry groups by target mean into folds\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Ensure prerequisites from Cell 3\n",
        "assert 'train_fe' in globals(), 'train_fe missing; run feature engineering cell.'\n",
        "assert 'compute_stoich_groups' in globals(), 'compute_stoich_groups missing; run grouping cell.'\n",
        "\n",
        "# y target\n",
        "y = train_fe['bandgap_energy_ev'].astype(float)\n",
        "\n",
        "# Ensure group key exists\n",
        "if 'groups' not in globals():\n",
        "    _gkey, _N, _al, _ga, _in, _o = compute_stoich_groups(pd.read_csv('train.csv'))\n",
        "    groups = _gkey.astype(str)\n",
        "\n",
        "# Build stratified folds by group mean target\n",
        "gkey = groups.astype(str)\n",
        "gmean = y.groupby(gkey).mean()\n",
        "gbin = pd.qcut(gmean, q=10, labels=False, duplicates='drop')\n",
        "uniq = pd.DataFrame({'g': gmean.index, 'bin': gbin.values}).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "group_to_fold = {}\n",
        "for k, (_, val_idx) in enumerate(skf.split(uniq['g'], uniq['bin'])):\n",
        "    for g in uniq['g'].iloc[val_idx]:\n",
        "        group_to_fold[g] = k\n",
        "fold_ids = gkey.map(group_to_fold).astype(int).values\n",
        "\n",
        "# Assert no leakage\n",
        "for k in range(5):\n",
        "    vi = np.where(fold_ids==k)[0]; ti = np.where(fold_ids!=k)[0]\n",
        "    assert set(gkey.iloc[vi]).isdisjoint(set(gkey.iloc[ti])), f'Group leakage detected in fold {k}'\n",
        "\n",
        "# Diagnostics\n",
        "print('Stratified GroupKFold created. Fold sizes:', pd.Series(fold_ids).value_counts().sort_index().to_dict())\n",
        "print('Group bins distribution:', uniq['bin'].value_counts().sort_index().to_dict())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "04f6c807-e6da-4af1-adaf-3d971afe69c8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Formation-energy OOF meta-feature + retrain bandgap\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Ensure we have engineered frames and fold_ids from previous cell\n",
        "assert 'train_fe' in globals() and 'test_fe' in globals() and 'fold_ids' in globals(), 'Run previous cell first.'\n",
        "\n",
        "# Features for formation model (must exist in both train/test, exclude targets)\n",
        "drop_cols = ['id','bandgap_energy_ev','formation_energy_ev_natom']\n",
        "common_cols = [c for c in train_fe.columns if c in test_fe.columns]\n",
        "feat_fe = [c for c in common_cols if c not in drop_cols]\n",
        "Xf = train_fe[feat_fe].copy()\n",
        "Xf_test = test_fe[feat_fe].copy()\n",
        "\n",
        "# Target with shift for log1p\n",
        "y_fe = train_fe['formation_energy_ev_natom'].astype(float)\n",
        "c_shift = float(max(0.0, -y_fe.min() + 1e-6))\n",
        "y_fe_log = np.log1p(y_fe + c_shift)\n",
        "\n",
        "params_fe = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'learning_rate': 0.03,\n",
        "    'num_leaves': 96,\n",
        "    'max_depth': -1,\n",
        "    'min_data_in_leaf': 200,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 1,\n",
        "    'lambda_l2': 1.0,\n",
        "    'lambda_l1': 0.2,\n",
        "    'verbosity': -1,\n",
        "    'seed': 2025\n",
        "}\n",
        "\n",
        "n_splits = len(np.unique(fold_ids))\n",
        "oof_fe_log = np.zeros(len(Xf)); test_fe_log = np.zeros(len(Xf_test))\n",
        "for k in range(n_splits):\n",
        "    trn_idx = np.where(fold_ids!=k)[0]\n",
        "    val_idx = np.where(fold_ids==k)[0]\n",
        "    dtr = lgb.Dataset(Xf.iloc[trn_idx], label=y_fe_log.iloc[trn_idx], categorical_feature=['spacegroup'], free_raw_data=False)\n",
        "    dva = lgb.Dataset(Xf.iloc[val_idx], label=y_fe_log.iloc[val_idx], categorical_feature=['spacegroup'], free_raw_data=False)\n",
        "    m = lgb.train(params_fe, dtr, num_boost_round=5000, valid_sets=[dtr,dva], valid_names=['train','valid'], callbacks=[lgb.early_stopping(300), lgb.log_evaluation(300)])\n",
        "    oof_fe_log[val_idx] = m.predict(Xf.iloc[val_idx], num_iteration=m.best_iteration)\n",
        "    test_fe_log += m.predict(Xf_test, num_iteration=m.best_iteration) / n_splits\n",
        "    del m, dtr, dva; gc.collect()\n",
        "cv_fe = mean_squared_error(y_fe_log, oof_fe_log, squared=False)\n",
        "print(f'Formation-energy CV (log space RMSE): {cv_fe:.6f}')\n",
        "\n",
        "# Back-transform predictions\n",
        "oof_fe = np.expm1(oof_fe_log) - c_shift\n",
        "pred_fe_test = np.expm1(test_fe_log) - c_shift\n",
        "\n",
        "# Attach meta-feature\n",
        "train_fe['pred_fe_meta'] = oof_fe\n",
        "test_fe['pred_fe_meta'] = pred_fe_test\n",
        "\n",
        "# Rebuild features with meta-feature included\n",
        "drop_cols_bg = ['id','bandgap_energy_ev']\n",
        "common_cols_bg = [c for c in train_fe.columns if c in test_fe.columns]\n",
        "features_bg = [c for c in common_cols_bg if c not in drop_cols_bg]\n",
        "X = train_fe[features_bg].copy()\n",
        "X_test = test_fe[features_bg].copy()\n",
        "y_bg = train_fe['bandgap_energy_ev'].astype(float)\n",
        "y_bg_log = np.log1p(y_bg.clip(lower=0))\n",
        "\n",
        "params_bg = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'learning_rate': 0.03,\n",
        "    'num_leaves': 96,\n",
        "    'max_depth': -1,\n",
        "    'min_data_in_leaf': 200,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 1,\n",
        "    'lambda_l2': 1.0,\n",
        "    'lambda_l1': 0.2,\n",
        "    'verbosity': -1,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "oof = np.zeros(len(X)); test_pred = np.zeros(len(X_test))\n",
        "for k in range(n_splits):\n",
        "    trn_idx = np.where(fold_ids!=k)[0]\n",
        "    val_idx = np.where(fold_ids==k)[0]\n",
        "    dtr = lgb.Dataset(X.iloc[trn_idx], label=y_bg_log.iloc[trn_idx], categorical_feature=['spacegroup'], free_raw_data=False)\n",
        "    dva = lgb.Dataset(X.iloc[val_idx], label=y_bg_log.iloc[val_idx], categorical_feature=['spacegroup'], free_raw_data=False)\n",
        "    m = lgb.train(params_bg, dtr, num_boost_round=5000, valid_sets=[dtr,dva], valid_names=['train','valid'], callbacks=[lgb.early_stopping(300), lgb.log_evaluation(300)])\n",
        "    oof[val_idx] = m.predict(X.iloc[val_idx], num_iteration=m.best_iteration)\n",
        "    test_pred += m.predict(X_test, num_iteration=m.best_iteration) / n_splits\n",
        "    del m, dtr, dva; gc.collect()\n",
        "cv_bg = mean_squared_error(y_bg_log, oof, squared=False)\n",
        "print(f'Bandgap CV RMSLE with FE meta-feature: {cv_bg:.6f}')\n",
        "\n",
        "# Write submission\n",
        "pred_bg = np.expm1(test_pred).clip(min=0)\n",
        "sub = pd.DataFrame({'id': test_fe['id'], 'bandgap_energy_ev': pred_bg})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('submission.csv saved:', sub.shape)\n",
        "sub.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "f0719fef-7fb9-4738-8ace-d0ba8bb19c82",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Lightweight XYZ structural features (pairwise stats + RDF) and retrain\n",
        "import os, time, gc, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Dependencies\n",
        "try:\n",
        "    from ase.io import read as ase_read\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'ase'])\n",
        "    from ase.io import read as ase_read\n",
        "try:\n",
        "    from joblib import Parallel, delayed\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'joblib'])\n",
        "    from joblib import Parallel, delayed\n",
        "\n",
        "BIN_MAX = 8.0\n",
        "BIN_SIZE = 0.2\n",
        "BINS = np.arange(0.0, BIN_MAX + BIN_SIZE, BIN_SIZE)\n",
        "\n",
        "CATIONS = {'Al', 'Ga', 'In'}\n",
        "\n",
        "def pairwise_dists(positions):\n",
        "    n = positions.shape[0]\n",
        "    if n <= 1:\n",
        "        return np.array([])\n",
        "    diffs = positions[:, None, :] - positions[None, :, :]\n",
        "    D = np.sqrt(np.sum(diffs**2, axis=2) + 1e-12)\n",
        "    iu = np.triu_indices(n, k=1)\n",
        "    return D[iu]\n",
        "\n",
        "def stats_from_array(x):\n",
        "    if x.size == 0:\n",
        "        return dict(min=np.nan, p5=np.nan, p25=np.nan, p50=np.nan, p75=np.nan, p95=np.nan, mean=np.nan, std=np.nan, max=np.nan)\n",
        "    q = np.percentile(x, [5,25,50,75,95])\n",
        "    return dict(min=float(np.min(x)), p5=float(q[0]), p25=float(q[1]), p50=float(q[2]), p75=float(q[3]), p95=float(q[4]), mean=float(np.mean(x)), std=float(np.std(x)), max=float(np.max(x)))\n",
        "\n",
        "def rdf_hist(distances):\n",
        "    if distances.size == 0:\n",
        "        return np.zeros(len(BINS)-1, dtype=float)\n",
        "    h, _ = np.histogram(distances, bins=BINS)\n",
        "    denom = max(1, distances.size)\n",
        "    return (h / denom).astype(float)\n",
        "\n",
        "def manual_xyz_parse(p):\n",
        "    try:\n",
        "        with open(p, 'r') as f:\n",
        "            lines = f.read().strip().splitlines()\n",
        "        if len(lines) < 3:\n",
        "            return None, None\n",
        "        try:\n",
        "            n = int(lines[0].strip().split()[0])\n",
        "        except Exception:\n",
        "            n = None\n",
        "        data_lines = lines[2:]\n",
        "        symbols = []\n",
        "        coords = []\n",
        "        for ln in data_lines:\n",
        "            parts = ln.strip().split()\n",
        "            if len(parts) < 4:\n",
        "                continue\n",
        "            sym = parts[0]\n",
        "            try:\n",
        "                x, y, z = float(parts[1]), float(parts[2]), float(parts[3])\n",
        "            except Exception:\n",
        "                continue\n",
        "            symbols.append(sym)\n",
        "            coords.append([x, y, z])\n",
        "        if len(coords) == 0:\n",
        "            return None, None\n",
        "        return np.array(symbols), np.array(coords, dtype=float)\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "def read_xyz_features(split, sid):\n",
        "    p = Path(split) / str(int(sid)) / 'geometry.xyz'\n",
        "    symbols = None; pos = None\n",
        "    # Try ASE single-frame\n",
        "    try:\n",
        "        atoms = ase_read(str(p), index=0, format='xyz')\n",
        "        pos = atoms.get_positions()\n",
        "        symbols = np.array(atoms.get_chemical_symbols())\n",
        "    except Exception as e1:\n",
        "        # Try multi-frame and pick first valid\n",
        "        try:\n",
        "            frames = ase_read(str(p), index=':', format='xyz')\n",
        "            if isinstance(frames, list) and len(frames) > 0:\n",
        "                atoms = frames[0]\n",
        "                pos = atoms.get_positions()\n",
        "                symbols = np.array(atoms.get_chemical_symbols())\n",
        "        except Exception as e2:\n",
        "            # Fallback to manual parser\n",
        "            symbols, pos = manual_xyz_parse(str(p))\n",
        "            if symbols is None or pos is None:\n",
        "                print(f'Failed to parse {p}: {e1} | {e2}')\n",
        "                return {'id': int(sid)}\n",
        "    if pos is None or symbols is None or len(pos) == 0:\n",
        "        print(f'Empty positions for {p}')\n",
        "        return {'id': int(sid)}\n",
        "\n",
        "    # center to remove translation\n",
        "    pos = pos - pos.mean(axis=0, keepdims=True)\n",
        "    is_o = symbols == 'O'\n",
        "    is_cat = np.isin(symbols, list(CATIONS))\n",
        "    d_all = pairwise_dists(pos)\n",
        "    idx_o = np.where(is_o)[0]\n",
        "    idx_c = np.where(is_cat)[0]\n",
        "\n",
        "    def subset_dists(idxs_a, idxs_b, same):\n",
        "        if len(idxs_a)==0 or len(idxs_b)==0:\n",
        "            return np.array([])\n",
        "        A = pos[idxs_a]; B = pos[idxs_b]\n",
        "        if same:\n",
        "            n = A.shape[0]\n",
        "            if n <= 1:\n",
        "                return np.array([])\n",
        "            diffs = A[:, None, :] - A[None, :, :]\n",
        "            D = np.sqrt(np.sum(diffs**2, axis=2) + 1e-12)\n",
        "            iu = np.triu_indices(n, k=1)\n",
        "            return D[iu]\n",
        "        else:\n",
        "            diffs = A[:, None, :] - B[None, :, :]\n",
        "            D = np.sqrt(np.sum(diffs**2, axis=2) + 1e-12)\n",
        "            return D.reshape(-1)\n",
        "\n",
        "    d_cc = subset_dists(idx_c, idx_c, same=True)\n",
        "    d_oo = subset_dists(idx_o, idx_o, same=True)\n",
        "    d_co = subset_dists(idx_c, idx_o, same=False)\n",
        "    feat = {'id': int(sid)}\n",
        "    for name, arr in [('all', d_all), ('cc', d_cc), ('oo', d_oo), ('co', d_co)]:\n",
        "        st = stats_from_array(arr)\n",
        "        for k, v in st.items():\n",
        "            feat[f'd_{name}_{k}'] = v\n",
        "        rh = rdf_hist(arr)\n",
        "        for i, v in enumerate(rh):\n",
        "            feat[f'rdf_{name}_{i}'] = float(v)\n",
        "\n",
        "    def nearest_cross(A_idx, B_idx):\n",
        "        if len(A_idx)==0 or len(B_idx)==0:\n",
        "            return dict(min=np.nan, mean=np.nan, max=np.nan)\n",
        "        A = pos[A_idx]; B = pos[B_idx]\n",
        "        diffs = A[:, None, :] - B[None, :, :]\n",
        "        D = np.sqrt(np.sum(diffs**2, axis=2) + 1e-12)\n",
        "        nn = D.min(axis=1)\n",
        "        return dict(min=float(nn.min()), mean=float(nn.mean()), max=float(nn.max()))\n",
        "\n",
        "    nn_co = nearest_cross(idx_c, idx_o)\n",
        "    nn_oc = nearest_cross(idx_o, idx_c)\n",
        "    for k,v in nn_co.items(): feat[f'nn_c_to_o_{k}'] = v\n",
        "    for k,v in nn_oc.items(): feat[f'nn_o_to_c_{k}'] = v\n",
        "    return feat\n",
        "\n",
        "def build_xyz_df(split, ids, n_jobs=8):\n",
        "    t0 = time.time()\n",
        "    feats = Parallel(n_jobs=n_jobs, backend='loky')(delayed(read_xyz_features)(split, sid) for sid in ids)\n",
        "    df = pd.DataFrame(feats).sort_values('id').reset_index(drop=True)\n",
        "    print(f'{split}: built XYZ features for {len(ids)} ids in {time.time()-t0:.1f}s with shape {df.shape}')\n",
        "    return df\n",
        "\n",
        "# Ensure prerequisite frames and folds exist\n",
        "assert 'train_fe' in globals() and 'test_fe' in globals() and 'fold_ids' in globals(), 'Run earlier cells first.'\n",
        "\n",
        "train_ids = train_fe['id'].values if 'id' in train_fe.columns else pd.read_csv('train.csv')['id'].values\n",
        "test_ids = test_fe['id'].values if 'id' in test_fe.columns else pd.read_csv('test.csv')['id'].values\n",
        "\n",
        "# Build or load cached XYZ features (force rebuild after parser fix)\n",
        "cache_tr = Path('xyz_train.parquet')\n",
        "cache_te = Path('xyz_test.parquet')\n",
        "if cache_tr.exists(): cache_tr.unlink()\n",
        "if cache_te.exists(): cache_te.unlink()\n",
        "xyz_tr = build_xyz_df('train', train_ids, n_jobs=16)\n",
        "xyz_te = build_xyz_df('test', test_ids, n_jobs=16)\n",
        "xyz_tr.to_parquet(cache_tr, index=False); xyz_te.to_parquet(cache_te, index=False)\n",
        "print('Cached XYZ features to parquet.')\n",
        "\n",
        "# Sanity check that we actually created many feature columns\n",
        "print('XYZ feature columns:', xyz_tr.columns.tolist()[:10], '... total:', len(xyz_tr.columns))\n",
        "\n",
        "# Merge into engineered frames\n",
        "train_fe = train_fe.merge(xyz_tr, on='id', how='left')\n",
        "test_fe = test_fe.merge(xyz_te, on='id', how='left')\n",
        "\n",
        "# Retrain LGBM with added XYZ features using same folds\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "drop_cols = ['id','bandgap_energy_ev']\n",
        "common_cols = [c for c in train_fe.columns if c in test_fe.columns]\n",
        "features = [c for c in common_cols if c not in drop_cols]\n",
        "X = train_fe[features].copy()\n",
        "X_test = test_fe[features].copy()\n",
        "y = train_fe['bandgap_energy_ev'].astype(float)\n",
        "y_log = np.log1p(y.clip(lower=0))\n",
        "\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'learning_rate': 0.03,\n",
        "    'num_leaves': 96,\n",
        "    'max_depth': -1,\n",
        "    'min_data_in_leaf': 200,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 1,\n",
        "    'lambda_l2': 3.0,\n",
        "    'lambda_l1': 0.2,\n",
        "    'verbosity': -1,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "n_splits = len(np.unique(fold_ids))\n",
        "oof = np.zeros(len(X)); test_pred = np.zeros(len(X_test))\n",
        "for k in range(n_splits):\n",
        "    trn_idx = np.where(fold_ids!=k)[0]\n",
        "    val_idx = np.where(fold_ids==k)[0]\n",
        "    dtr = lgb.Dataset(X.iloc[trn_idx], label=y_log.iloc[trn_idx], categorical_feature=['spacegroup'], free_raw_data=False)\n",
        "    dva = lgb.Dataset(X.iloc[val_idx], label=y_log.iloc[val_idx], categorical_feature=['spacegroup'], free_raw_data=False)\n",
        "    m = lgb.train(params, dtr, num_boost_round=5000, valid_sets=[dtr,dva], valid_names=['train','valid'], callbacks=[lgb.early_stopping(300), lgb.log_evaluation(300)])\n",
        "    oof[val_idx] = m.predict(X.iloc[val_idx], num_iteration=m.best_iteration)\n",
        "    test_pred += m.predict(X_test, num_iteration=m.best_iteration) / n_splits\n",
        "    del m, dtr, dva; gc.collect()\n",
        "cv = mean_squared_error(y_log, oof, squared=False)\n",
        "print(f'CV RMSLE with XYZ features: {cv:.6f}')\n",
        "\n",
        "# Save submission\n",
        "pred_bg = np.expm1(test_pred).clip(min=0)\n",
        "sub = pd.DataFrame({'id': test_fe['id'], 'bandgap_energy_ev': pred_bg})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('submission.csv saved:', sub.shape)\n",
        "sub.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: built XYZ features for 2160 ids in 1.8s with shape (2160, 203)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test: built XYZ features for 240 ids in 0.3s with shape (240, 203)\nCached XYZ features to parquet.\nXYZ feature columns: ['id', 'd_all_min', 'd_all_p5', 'd_all_p25', 'd_all_p50', 'd_all_p75', 'd_all_p95', 'd_all_mean', 'd_all_std', 'd_all_max'] ... total: 203\nTraining until validation scores don't improve for 300 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain's rmse: 0.0820224\tvalid's rmse: 0.108077\n[600]\ttrain's rmse: 0.0732857\tvalid's rmse: 0.101438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain's rmse: 0.068578\tvalid's rmse: 0.0980759\n[1200]\ttrain's rmse: 0.0654777\tvalid's rmse: 0.0967751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\ttrain's rmse: 0.0629212\tvalid's rmse: 0.0961109\n[1800]\ttrain's rmse: 0.0609254\tvalid's rmse: 0.09596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\ttrain's rmse: 0.0591926\tvalid's rmse: 0.0958148\nEarly stopping, best iteration is:\n[2093]\ttrain's rmse: 0.0592325\tvalid's rmse: 0.0957706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 300 rounds\n[300]\ttrain's rmse: 0.0848733\tvalid's rmse: 0.0892039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0754857\tvalid's rmse: 0.0858933\n[900]\ttrain's rmse: 0.0706754\tvalid's rmse: 0.0848009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0673629\tvalid's rmse: 0.0844294\n[1500]\ttrain's rmse: 0.064823\tvalid's rmse: 0.0842012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\ttrain's rmse: 0.0626647\tvalid's rmse: 0.0838746\n[2100]\ttrain's rmse: 0.0608356\tvalid's rmse: 0.0838457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\ttrain's rmse: 0.0592077\tvalid's rmse: 0.0837712\nEarly stopping, best iteration is:\n[2349]\ttrain's rmse: 0.0594585\tvalid's rmse: 0.0837093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 300 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain's rmse: 0.0832171\tvalid's rmse: 0.093395\n[600]\ttrain's rmse: 0.0739555\tvalid's rmse: 0.0906044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain's rmse: 0.0692626\tvalid's rmse: 0.0900064\n[1200]\ttrain's rmse: 0.066015\tvalid's rmse: 0.0900276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1147]\ttrain's rmse: 0.0665018\tvalid's rmse: 0.089901\nTraining until validation scores don't improve for 300 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain's rmse: 0.0845764\tvalid's rmse: 0.0904315\n[600]\ttrain's rmse: 0.0756515\tvalid's rmse: 0.0842928\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain's rmse: 0.0710796\tvalid's rmse: 0.0820401\n[1200]\ttrain's rmse: 0.0678913\tvalid's rmse: 0.0812824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\ttrain's rmse: 0.0655195\tvalid's rmse: 0.0804976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\ttrain's rmse: 0.0635658\tvalid's rmse: 0.0803347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\ttrain's rmse: 0.0617522\tvalid's rmse: 0.0803118\n[2400]\ttrain's rmse: 0.0602301\tvalid's rmse: 0.0801906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[2275]\ttrain's rmse: 0.0608401\tvalid's rmse: 0.0801157\nTraining until validation scores don't improve for 300 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain's rmse: 0.0861201\tvalid's rmse: 0.0876482\n[600]\ttrain's rmse: 0.076906\tvalid's rmse: 0.0824866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain's rmse: 0.0720975\tvalid's rmse: 0.0809133\n[1200]\ttrain's rmse: 0.0689646\tvalid's rmse: 0.0802659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\ttrain's rmse: 0.0663805\tvalid's rmse: 0.0799949\n[1800]\ttrain's rmse: 0.0641817\tvalid's rmse: 0.079945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\ttrain's rmse: 0.0622199\tvalid's rmse: 0.0800118\nEarly stopping, best iteration is:\n[2022]\ttrain's rmse: 0.0627079\tvalid's rmse: 0.0798619\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "got an unexpected keyword argument 'squared'",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 226\u001b[39m\n\u001b[32m    224\u001b[39m     test_pred += m.predict(X_test, num_iteration=m.best_iteration) / n_splits\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m m, dtr, dva; gc.collect()\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m cv = \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_log\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moof\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCV RMSLE with XYZ features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    229\u001b[39m \u001b[38;5;66;03m# Save submission\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/utils/_param_validation.py:196\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m func_sig = signature(func)\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m params = \u001b[43mfunc_sig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m params.apply_defaults()\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# ignore self/cls and positional/keyword markers\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/inspect.py:3204\u001b[39m, in \u001b[36mSignature.bind\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3199\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args, **kwargs):\n\u001b[32m   3200\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[32m   3201\u001b[39m \u001b[33;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[32m   3202\u001b[39m \u001b[33;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[32m   3203\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/inspect.py:3193\u001b[39m, in \u001b[36mSignature._bind\u001b[39m\u001b[34m(self, args, kwargs, partial)\u001b[39m\n\u001b[32m   3191\u001b[39m         arguments[kwargs_param.name] = kwargs\n\u001b[32m   3192\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3193\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   3194\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mgot an unexpected keyword argument \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m   3195\u001b[39m                 arg=\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))))\n\u001b[32m   3197\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_arguments_cls(\u001b[38;5;28mself\u001b[39m, arguments)\n",
            "\u001b[31mTypeError\u001b[39m: got an unexpected keyword argument 'squared'"
          ]
        }
      ]
    },
    {
      "id": "e270d78c-a8b9-40f9-88c5-dad67735b004",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2) Add missing low-dimension matminer composition features (Stoichiometry, ValenceOrbital, IonProperty)\n",
        "import numpy as np, pandas as pd, sys, subprocess\n",
        "\n",
        "assert 'train_fe' in globals() and 'test_fe' in globals(), 'Run earlier feature engineering cells first.'\n",
        "\n",
        "try:\n",
        "    from matminer.featurizers.composition import Stoichiometry, ValenceOrbital, IonProperty\n",
        "    from pymatgen.core.composition import Composition\n",
        "except Exception:\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'matminer', 'pymatgen'])\n",
        "    from matminer.featurizers.composition import Stoichiometry, ValenceOrbital, IonProperty\n",
        "    from pymatgen.core.composition import Composition\n",
        "\n",
        "def add_mm_lowdim(df):\n",
        "    tmp = df[['composition']].copy()\n",
        "    tmp['composition'] = tmp['composition'].apply(Composition)\n",
        "    out = tmp.copy()\n",
        "    for fz in [Stoichiometry(), ValenceOrbital(props=['avg','frac']), IonProperty(fast=True)]:\n",
        "        out = fz.featurize_dataframe(out, col_id='composition', ignore_errors=True)\n",
        "    out = out.drop(columns=['composition'])\n",
        "    out.columns = [f'mm2_{c}' for c in out.columns]\n",
        "    return out\n",
        "\n",
        "# Build and merge\n",
        "mm2_tr = add_mm_lowdim(train_fe)\n",
        "mm2_te = add_mm_lowdim(test_fe)\n",
        "print('Low-dim matminer built:', mm2_tr.shape, mm2_te.shape)\n",
        "train_fe = pd.concat([train_fe.reset_index(drop=True), mm2_tr.reset_index(drop=True)], axis=1)\n",
        "test_fe  = pd.concat([test_fe.reset_index(drop=True),  mm2_te.reset_index(drop=True)], axis=1)\n",
        "print('After merge shapes:', train_fe.shape, test_fe.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "90a0058f-8622-45ab-99a1-5f9c0acbd9a0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3) Prune XYZ RDF features; 4) Add interaction features, bowing/logs, cation-weight contrasts; OOF TE for spacegroup; rebuild folds; quick importance pruning\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "\n",
        "assert 'train_fe' in globals() and 'test_fe' in globals(), 'Run earlier feature engineering cells first.'\n",
        "t0_all = time.time()\n",
        "\n",
        "# ------------------ Prune all rdf_* bins from XYZ features ------------------\n",
        "t0 = time.time()\n",
        "xyz_drop = [c for c in train_fe.columns if c.startswith('rdf_')]\n",
        "train_fe.drop(columns=xyz_drop, inplace=True, errors='ignore')\n",
        "test_fe.drop(columns=xyz_drop, inplace=True, errors='ignore')\n",
        "print('Dropped RDF bins:', len(xyz_drop), '| elapsed:', f'{time.time()-t0:.2f}s', flush=True)\n",
        "\n",
        "# ------------------ Ensure base composition fractions and Vegard weights ------------------\n",
        "t0 = time.time()\n",
        "for df in (train_fe, test_fe):\n",
        "    if 'frac_al' not in df.columns: df['frac_al'] = df['percent_atom_al'] / 100.0\n",
        "    if 'frac_ga' not in df.columns: df['frac_ga'] = df['percent_atom_ga'] / 100.0\n",
        "    if 'frac_in' not in df.columns: df['frac_in'] = df['percent_atom_in'] / 100.0\n",
        "    if 'percent_atom_o' not in df.columns:\n",
        "        df['percent_atom_o'] = 100.0 - (df['percent_atom_al'] + df['percent_atom_ga'] + df['percent_atom_in'])\n",
        "    if 'frac_o' not in df.columns:\n",
        "        df['frac_o'] = df['percent_atom_o']/100.0\n",
        "    frac_cat = (df['frac_al'] + df['frac_ga'] + df['frac_in']).replace(0, np.nan)\n",
        "    if 'w_al' not in df.columns: df['w_al'] = (df['frac_al']/frac_cat).fillna(0)\n",
        "    if 'w_ga' not in df.columns: df['w_ga'] = (df['frac_ga']/frac_cat).fillna(0)\n",
        "    if 'w_in' not in df.columns: df['w_in'] = (df['frac_in']/frac_cat).fillna(0)\n",
        "    if 'vegard_bg' not in df.columns:\n",
        "        df['vegard_bg'] = 8.8*df['w_al'] + 4.8*df['w_ga'] + 2.9*df['w_in']\n",
        "print('Ensured base composition + Vegard | elapsed:', f'{time.time()-t0:.2f}s', flush=True)\n",
        "\n",
        "# ------------------ Add bowing and log features ------------------\n",
        "t0 = time.time()\n",
        "def add_bowing_logs(df):\n",
        "    df['bow_in'] = df['w_in']*(1.0 - df['w_in'])\n",
        "    df['bow_ga'] = df['w_ga']*(1.0 - df['w_ga'])\n",
        "    if 'volume_per_atom' in df.columns:\n",
        "        df['log_vpa'] = np.log1p(df['volume_per_atom'].clip(lower=0))\n",
        "    if 'atoms_per_volume' in df.columns:\n",
        "        df['log_apv'] = np.log1p(df['atoms_per_volume'].clip(lower=0))\n",
        "    df['log_oc'] = np.log1p((df['frac_o']/(df['frac_al']+df['frac_ga']+df['frac_in']+1e-9)).clip(lower=0))\n",
        "    df['log_in_over_al'] = np.log1p(((df['frac_in']+1e-6)/(df['frac_al']+1e-6)).clip(lower=0))\n",
        "    return df\n",
        "train_fe = add_bowing_logs(train_fe)\n",
        "test_fe = add_bowing_logs(test_fe)\n",
        "print('Added bowing/log features | elapsed:', f'{time.time()-t0:.2f}s', flush=True)\n",
        "\n",
        "# ------------------ Add interaction features (keep existing if present) ------------------\n",
        "t0 = time.time()\n",
        "def add_interactions(df):\n",
        "    fa, fg, fi = df['frac_al'], df['frac_ga'], df['frac_in']\n",
        "    if 'al_in_diff_sq' not in df.columns: df['al_in_diff_sq'] = (fa - fi) ** 2\n",
        "    if 'ga_in_diff_sq' not in df.columns: df['ga_in_diff_sq'] = (fg - fi) ** 2\n",
        "    if 'frac_al_cu' not in df.columns: df['frac_al_cu'] = fa ** 3\n",
        "    if 'frac_ga_cu' not in df.columns: df['frac_ga_cu'] = fg ** 3\n",
        "    if 'frac_in_cu' not in df.columns: df['frac_in_cu'] = fi ** 3\n",
        "    if 'w_al_x_veg' not in df.columns: df['w_al_x_veg'] = df['w_al'] * df['vegard_bg']\n",
        "    if 'w_in_x_veg' not in df.columns: df['w_in_x_veg'] = df['w_in'] * df['vegard_bg']\n",
        "    for wname in ['al','ga','in']:\n",
        "        if f'w_{wname}_sq' not in df.columns: df[f'w_{wname}_sq'] = df[f'w_{wname}']**2\n",
        "    if 'w_al_ga' not in df.columns: df['w_al_ga'] = df['w_al']*df['w_ga']\n",
        "    if 'w_al_in' not in df.columns: df['w_al_in'] = df['w_al']*df['w_in']\n",
        "    if 'w_ga_in' not in df.columns: df['w_ga_in'] = df['w_ga']*df['w_in']\n",
        "    return df\n",
        "train_fe = add_interactions(train_fe)\n",
        "test_fe = add_interactions(test_fe)\n",
        "print('After interactions/bowing/logs:', train_fe.shape, test_fe.shape, '| elapsed:', f'{time.time()-t0:.2f}s', flush=True)\n",
        "\n",
        "# ------------------ Cation-weighted contrasts (high-signal) ------------------\n",
        "t0 = time.time()\n",
        "props = {\n",
        "    'chi_pauling': {'Al':1.61,'Ga':1.81,'In':1.78,'O':3.44},\n",
        "    'ionic_radius': {'Al':0.535,'Ga':0.62,'In':0.80,'O':1.38},  # Shannon approx (\u00c5)\n",
        "    'Z': {'Al':13,'Ga':31,'In':49,'O':8},\n",
        "    'period': {'Al':3,'Ga':4,'In':5,'O':2},\n",
        "    'group': {'Al':13,'Ga':13,'In':13,'O':16},\n",
        "    'covalent_radius': {'Al':1.21,'Ga':1.22,'In':1.42,'O':0.66},\n",
        "    'first_ionization_energy': {'Al':5.986,'Ga':5.999,'In':5.786,'O':13.618},\n",
        "    'electron_affinity': {'Al':0.441,'Ga':0.30,'In':0.30,'O':1.461}\n",
        "}\n",
        "\n",
        "def add_cation_weighted(df):\n",
        "    wa, wg, wi = df['w_al'], df['w_ga'], df['w_in']\n",
        "    for name, table in props.items():\n",
        "        ca = table['Al']; cg = table['Ga']; ci = table['In']; co = table['O']\n",
        "        wmean = wa*ca + wg*cg + wi*ci\n",
        "        df[f'catw_{name}_mean'] = wmean\n",
        "        df[f'catw_{name}_var'] = (wa*(ca - wmean)**2 + wg*(cg - wmean)**2 + wi*(ci - wmean)**2)\n",
        "        if name in ['chi_pauling','ionic_radius']:\n",
        "            df[f'o_minus_catw_{name}'] = co - wmean\n",
        "    return df\n",
        "train_fe = add_cation_weighted(train_fe)\n",
        "test_fe = add_cation_weighted(test_fe)\n",
        "print('Added cation-weighted contrasts | elapsed:', f'{time.time()-t0:.2f}s', flush=True)\n",
        "\n",
        "# ------------------ Rebuild stratified group-disjoint folds with more splits (n_splits=8) ------------------\n",
        "t0 = time.time()\n",
        "assert 'compute_stoich_groups' in globals(), 'compute_stoich_groups missing; run grouping cell.'\n",
        "y = train_fe['bandgap_energy_ev'].astype(float)\n",
        "# Ensure necessary count columns exist (vectorized, avoid row-wise apply)\n",
        "need_cols = ['N','n_al','n_ga','n_in','n_o']\n",
        "missing = [c for c in need_cols if c not in train_fe.columns]\n",
        "if missing:\n",
        "    tr_csv = pd.read_csv('train.csv')\n",
        "    key_tr, N_tr, al_tr, ga_tr, in_tr, o_tr = compute_stoich_groups(tr_csv)\n",
        "    train_fe['N'] = N_tr; train_fe['n_al'] = al_tr; train_fe['n_ga'] = ga_tr; train_fe['n_in'] = in_tr; train_fe['n_o'] = o_tr\n",
        "    te_csv = pd.read_csv('test.csv')\n",
        "    key_te, N_te, al_te, ga_te, in_te, o_te = compute_stoich_groups(te_csv)\n",
        "    test_fe['N'] = N_te; test_fe['n_al'] = al_te; test_fe['n_ga'] = ga_te; test_fe['n_in'] = in_te; test_fe['n_o'] = o_te\n",
        "# Vectorized group key\n",
        "gkey = train_fe[['N','n_al','n_ga','n_in']].astype(int).astype(str).agg('_'.join, axis=1)\n",
        "gmean = y.groupby(gkey).mean()\n",
        "gbin = pd.qcut(gmean, q=10, labels=False, duplicates='drop')\n",
        "uniq = pd.DataFrame({'g': gmean.index, 'bin': gbin.values}).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "n_splits_new = 8\n",
        "skf = StratifiedKFold(n_splits=n_splits_new, shuffle=True, random_state=42)\n",
        "group_to_fold = {}\n",
        "for k, (_, val_idx) in enumerate(skf.split(uniq['g'], uniq['bin'])):\n",
        "    for g in uniq['g'].iloc[val_idx]: group_to_fold[g] = k\n",
        "fold_ids = gkey.map(group_to_fold).astype(int).values\n",
        "print('Rebuilt fold_ids with n_splits=', n_splits_new, 'Fold sizes:', pd.Series(fold_ids).value_counts().sort_index().to_dict(), '| elapsed:', f'{time.time()-t0:.2f}s', flush=True)\n",
        "\n",
        "# ------------------ OOF target encoding for spacegroup (group-disjoint) ------------------\n",
        "t0 = time.time()\n",
        "train_fe['te_sg'] = 0.0\n",
        "y_log = np.log1p(y.clip(lower=0))\n",
        "global_mean = float(y_log.mean())\n",
        "for k in range(n_splits_new):\n",
        "    trn_idx = np.where(fold_ids!=k)[0]\n",
        "    val_idx = np.where(fold_ids==k)[0]\n",
        "    m = train_fe.iloc[trn_idx].groupby('spacegroup')['bandgap_energy_ev'].apply(lambda s: np.log1p(s.clip(lower=0)).mean())\n",
        "    te_map = m.to_dict()\n",
        "    train_fe.loc[train_fe.index[val_idx], 'te_sg'] = train_fe.iloc[val_idx]['spacegroup'].map(te_map).fillna(global_mean).values\n",
        "sg_map_full = train_fe.groupby('spacegroup')['bandgap_energy_ev'].apply(lambda s: np.log1p(s.clip(lower=0)).mean()).to_dict()\n",
        "test_fe['te_sg'] = test_fe['spacegroup'].map(sg_map_full).fillna(global_mean)\n",
        "print('Added OOF TE for spacegroup | elapsed:', f'{time.time()-t0:.2f}s', flush=True)\n",
        "\n",
        "# ------------------ Quick LightGBM importance-based pruning ------------------\n",
        "t0 = time.time()\n",
        "import lightgbm as lgb\n",
        "drop_cols = ['id','bandgap_energy_ev','composition']\n",
        "common_cols = [c for c in train_fe.columns if c in test_fe.columns]\n",
        "feat_all = [c for c in common_cols if c not in drop_cols]\n",
        "# Numeric only, keep spacegroup if present\n",
        "num_cols = list(train_fe[feat_all].select_dtypes(include=[np.number]).columns)\n",
        "if 'spacegroup' in feat_all and 'spacegroup' not in num_cols:\n",
        "    num_cols.append('spacegroup')\n",
        "train_X = train_fe[num_cols].copy()\n",
        "test_X = test_fe[num_cols].copy()\n",
        "med = train_X.median(numeric_only=True)\n",
        "train_X = train_X.fillna(med)\n",
        "test_X = test_X.fillna(med)\n",
        "\n",
        "lgb_quick = lgb.LGBMRegressor(n_estimators=300, learning_rate=0.1, random_state=42)\n",
        "lgb_quick.fit(train_X, y_log)\n",
        "fi = pd.DataFrame({'feat': train_X.columns, 'imp': lgb_quick.feature_importances_})\n",
        "fi = fi.sort_values('imp', ascending=True).reset_index(drop=True)\n",
        "keep_ratio = 0.65\n",
        "k = int(np.ceil(len(fi)*keep_ratio))\n",
        "keep_feats = fi.sort_values('imp', ascending=False).head(k)['feat'].tolist()\n",
        "drop_feats = [f for f in train_X.columns if f not in keep_feats]\n",
        "print(f'Quick LGBM importance pruning: keeping {len(keep_feats)} / {len(train_X.columns)} features (drop {len(drop_feats)}). Took {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "# Apply pruning to frames (safe drop only for features present in both train/test and numeric)\n",
        "train_drop_cols = [c for c in drop_feats if c in train_fe.columns]\n",
        "test_drop_cols = [c for c in drop_feats if c in test_fe.columns]\n",
        "# Do not drop essential columns if they slipped in\n",
        "essentials = set(['id','bandgap_energy_ev','composition'])\n",
        "train_drop_cols = [c for c in train_drop_cols if c not in essentials]\n",
        "test_drop_cols = [c for c in test_drop_cols if c not in essentials]\n",
        "train_fe.drop(columns=train_drop_cols, inplace=True, errors='ignore')\n",
        "test_fe.drop(columns=test_drop_cols, inplace=True, errors='ignore')\n",
        "print('Applied pruning to train/test frames. New shapes:', train_fe.shape, test_fe.shape, '| total elapsed:', f'{time.time()-t0_all:.1f}s', flush=True)\n",
        "\n",
        "gc.collect();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "794900f6-db25-44b8-b638-e3e75d51ab10",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Matminer Magpie features + Vegard predictor + LGBM/CatBoost/XGBoost + NNLS blend\n",
        "import numpy as np, pandas as pd, gc, time, os, sys, subprocess\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "assert 'train_fe' in globals() and 'test_fe' in globals() and 'fold_ids' in globals(), 'Run previous cells first.'\n",
        "\n",
        "# Install deps if missing\n",
        "try:\n",
        "    from matminer.featurizers.composition import ElementProperty\n",
        "    from pymatgen.core.composition import Composition\n",
        "except Exception:\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'matminer', 'pymatgen'])\n",
        "    from matminer.featurizers.composition import ElementProperty\n",
        "    from pymatgen.core.composition import Composition\n",
        "try:\n",
        "    from catboost import CatBoostRegressor, Pool\n",
        "except Exception:\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'catboost'])\n",
        "    from catboost import CatBoostRegressor, Pool\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'xgboost'])\n",
        "    import xgboost as xgb\n",
        "try:\n",
        "    from scipy.optimize import nnls\n",
        "except Exception:\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'scipy'])\n",
        "    from scipy.optimize import nnls\n",
        "\n",
        "# Ensure stoichiometric counts exist (from cell 3) and create composition strings\n",
        "def ensure_counts(df):\n",
        "    need = ['N','n_al','n_ga','n_in','n_o']\n",
        "    for c in need:\n",
        "        assert c in df.columns, f'Missing {c} in engineered frame; rerun grouping cell.'\n",
        "    return df\n",
        "train_fe = ensure_counts(train_fe)\n",
        "test_fe = ensure_counts(test_fe)\n",
        "\n",
        "def comp_str(row):\n",
        "    return f\"Al{int(row['n_al'])} Ga{int(row['n_ga'])} In{int(row['n_in'])} O{int(row['n_o'])}\"\n",
        "train_fe['composition'] = train_fe.apply(comp_str, axis=1)\n",
        "test_fe['composition'] = test_fe.apply(comp_str, axis=1)\n",
        "\n",
        "# Ensure spacegroup categorical dtype\n",
        "for df in (train_fe, test_fe):\n",
        "    df['spacegroup'] = df['spacegroup'].astype('category')\n",
        "\n",
        "# Magpie features\n",
        "def build_magpie(df):\n",
        "    tmp = df[['composition']].copy()\n",
        "    tmp['composition'] = tmp['composition'].apply(Composition)\n",
        "    ep = ElementProperty.from_preset('magpie')\n",
        "    out = ep.featurize_dataframe(tmp, col_id='composition', ignore_errors=True)\n",
        "    out.columns = ['composition'] + [f'mm_{c}' for c in out.columns[1:]]\n",
        "    return out.drop(columns=['composition'])\n",
        "\n",
        "t0 = time.time()\n",
        "mm_tr = build_magpie(train_fe)\n",
        "mm_te = build_magpie(test_fe)\n",
        "print(f'Magpie built: train {mm_tr.shape}, test {mm_te.shape} in {time.time()-t0:.1f}s')\n",
        "\n",
        "# Merge Magpie features (other features like mm2_ from prior cell are already in train_fe/test_fe)\n",
        "train_fe = pd.concat([train_fe.reset_index(drop=True), mm_tr.reset_index(drop=True)], axis=1)\n",
        "test_fe = pd.concat([test_fe.reset_index(drop=True), mm_te.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Vegard-like predictor + interactions (if missing)\n",
        "def add_vegard(df):\n",
        "    frac_al = df['percent_atom_al']/100.0\n",
        "    frac_ga = df['percent_atom_ga']/100.0\n",
        "    frac_in = df['percent_atom_in']/100.0\n",
        "    frac_cat = (frac_al + frac_ga + frac_in).replace(0, np.nan)\n",
        "    w_al = (frac_al/frac_cat).fillna(0)\n",
        "    w_ga = (frac_ga/frac_cat).fillna(0)\n",
        "    w_in = (frac_in/frac_cat).fillna(0)\n",
        "    df['vegard_bg'] = df.get('vegard_bg', 8.8*w_al + 4.8*w_ga + 2.9*w_in)\n",
        "    for wname, w in [('al',w_al),('ga',w_ga),('in',w_in)]:\n",
        "        if f'w_{wname}' not in df.columns: df[f'w_{wname}'] = w\n",
        "        if f'w_{wname}_sq' not in df.columns: df[f'w_{wname}_sq'] = w*w\n",
        "    if 'w_al_ga' not in df.columns: df['w_al_ga'] = df['w_al']*df['w_ga']\n",
        "    if 'w_al_in' not in df.columns: df['w_al_in'] = df['w_al']*df['w_in']\n",
        "    if 'w_ga_in' not in df.columns: df['w_ga_in'] = df['w_ga']*df['w_in']\n",
        "    # small interactions\n",
        "    for name, expr in [('al_in_diff_sq',(frac_al-frac_in)**2), ('ga_in_diff_sq',(frac_ga-frac_in)**2),\n",
        "                       ('frac_al_cu', frac_al**3), ('frac_ga_cu', frac_ga**3), ('frac_in_cu', frac_in**3),\n",
        "                       ('w_al_x_veg', w_al*df['vegard_bg']), ('w_in_x_veg', w_in*df['vegard_bg'])]:\n",
        "        if name not in df.columns: df[name] = expr\n",
        "    return df\n",
        "train_fe = add_vegard(train_fe)\n",
        "test_fe = add_vegard(test_fe)\n",
        "\n",
        "# Normalize lattice lengths by volume^(1/3) (if not present)\n",
        "for df in (train_fe, test_fe):\n",
        "    if 'a_red' not in df.columns or 'b_red' not in df.columns or 'c_red' not in df.columns:\n",
        "        vol = df['cell_volume'].replace(0, np.nan)\n",
        "        l = vol.pow(1/3)\n",
        "        df['a_red'] = df['lattice_vector_1_ang']/l\n",
        "        df['b_red'] = df['lattice_vector_2_ang']/l\n",
        "        df['c_red'] = df['lattice_vector_3_ang']/l\n",
        "\n",
        "# Build feature matrices\n",
        "drop_cols = ['id','bandgap_energy_ev','composition']\n",
        "common_cols = [c for c in train_fe.columns if c in test_fe.columns]\n",
        "features = [c for c in common_cols if c not in drop_cols]\n",
        "features = pd.Index(features).drop_duplicates().tolist()\n",
        "\n",
        "med = train_fe[features].median(numeric_only=True)\n",
        "train_X = train_fe[features].copy().fillna(med)\n",
        "test_X = test_fe[features].copy().fillna(med)\n",
        "\n",
        "# Align and dtype guard\n",
        "train_X = train_X.loc[:, ~train_X.columns.duplicated()]\n",
        "test_X = test_X.loc[:, ~test_X.columns.duplicated()]\n",
        "common_aligned = [c for c in train_X.columns if c in test_X.columns]\n",
        "train_X = train_X[common_aligned]\n",
        "test_X = test_X[common_aligned]\n",
        "bad = list(train_X.select_dtypes(include=['object']).columns)\n",
        "if bad:\n",
        "    print('Dropping object cols:', bad)\n",
        "    train_X.drop(columns=bad, inplace=True)\n",
        "    test_X.drop(columns=bad, inplace=True)\n",
        "num_cols = list(train_X.select_dtypes(include=[np.number]).columns)\n",
        "if 'spacegroup' in train_X.columns:\n",
        "    num_cols = list(dict.fromkeys(num_cols + ['spacegroup']))\n",
        "train_X = train_X[num_cols]\n",
        "test_X = test_X[num_cols]\n",
        "assert train_X.select_dtypes(include='object').empty and test_X.select_dtypes(include='object').empty\n",
        "\n",
        "y = train_fe['bandgap_energy_ev'].astype(float)\n",
        "y_log = np.log1p(y.clip(lower=0))\n",
        "n_splits = len(np.unique(fold_ids))\n",
        "\n",
        "# LightGBM OOF\n",
        "import lightgbm as lgb\n",
        "params_lgb = {\n",
        "    'objective': 'regression', 'metric': 'rmse', 'learning_rate': 0.03,\n",
        "    'num_leaves': 128, 'max_depth': -1, 'min_data_in_leaf': 150,\n",
        "    'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1,\n",
        "    'lambda_l2': 2.0, 'lambda_l1': 0.0, 'verbosity': -1, 'seed': 42\n",
        "}\n",
        "oof_lgb = np.zeros(len(train_X)); pred_lgb = np.zeros(len(test_X))\n",
        "for k in range(n_splits):\n",
        "    trn = np.where(fold_ids!=k)[0]; val = np.where(fold_ids==k)[0]\n",
        "    dtr = lgb.Dataset(train_X.iloc[trn], label=y_log.iloc[trn], categorical_feature=['spacegroup'], free_raw_data=False)\n",
        "    dva = lgb.Dataset(train_X.iloc[val], label=y_log.iloc[val], categorical_feature=['spacegroup'], free_raw_data=False)\n",
        "    m = lgb.train(params_lgb, dtr, num_boost_round=7000, valid_sets=[dtr,dva], valid_names=['train','valid'], callbacks=[lgb.early_stopping(450), lgb.log_evaluation(300)])\n",
        "    oof_lgb[val] = m.predict(train_X.iloc[val], num_iteration=m.best_iteration)\n",
        "    pred_lgb += m.predict(test_X, num_iteration=m.best_iteration)/n_splits\n",
        "    del m, dtr, dva; gc.collect()\n",
        "cv_lgb = mean_squared_error(y_log, oof_lgb, squared=False)\n",
        "print(f'LGBM CV RMSLE: {cv_lgb:.6f}')\n",
        "\n",
        "# CatBoost OOF\n",
        "cat_params = dict(loss_function='RMSE', eval_metric='RMSE', iterations=5000, learning_rate=0.03, depth=7,\n",
        "                   l2_leaf_reg=5.0, subsample=0.8, rsm=0.8, random_seed=42, od_type='Iter', od_wait=300, verbose=300)\n",
        "oof_cb = np.zeros(len(train_X)); pred_cb = np.zeros(len(test_X))\n",
        "cat_idx = [train_X.columns.get_loc('spacegroup')] if 'spacegroup' in train_X.columns else []\n",
        "for k in range(n_splits):\n",
        "    trn = np.where(fold_ids!=k)[0]; val = np.where(fold_ids==k)[0]\n",
        "    pool_tr = Pool(train_X.iloc[trn], y_log.iloc[trn], cat_features=cat_idx)\n",
        "    pool_va = Pool(train_X.iloc[val], y_log.iloc[val], cat_features=cat_idx)\n",
        "    model_cb = CatBoostRegressor(**cat_params)\n",
        "    model_cb.fit(pool_tr, eval_set=pool_va, use_best_model=True)\n",
        "    oof_cb[val] = model_cb.predict(pool_va)\n",
        "    pred_cb += model_cb.predict(Pool(test_X, cat_features=cat_idx))/n_splits\n",
        "    del model_cb, pool_tr, pool_va; gc.collect()\n",
        "cv_cb = mean_squared_error(y_log, oof_cb, squared=False)\n",
        "print(f'CatBoost CV RMSLE: {cv_cb:.6f}')\n",
        "\n",
        "# XGBoost OOF\n",
        "xgb_params = dict(objective='reg:squarederror', eval_metric='rmse', tree_method='hist',\n",
        "                  max_depth=6, eta=0.03, subsample=0.8, colsample_bytree=0.8,\n",
        "                  min_child_weight=5, reg_lambda=3.0, reg_alpha=0.0, random_state=42)\n",
        "oof_xgb = np.zeros(len(train_X)); pred_xgb = np.zeros(len(test_X))\n",
        "for k in range(n_splits):\n",
        "    trn = np.where(fold_ids!=k)[0]; val = np.where(fold_ids==k)[0]\n",
        "    dtr = xgb.DMatrix(train_X.iloc[trn], label=y_log.iloc[trn])\n",
        "    dva = xgb.DMatrix(train_X.iloc[val], label=y_log.iloc[val])\n",
        "    dte = xgb.DMatrix(test_X)\n",
        "    model = xgb.train(xgb_params, dtr, num_boost_round=8000, evals=[(dva,'valid')], early_stopping_rounds=400, verbose_eval=300)\n",
        "    oof_xgb[val] = model.predict(xgb.DMatrix(train_X.iloc[val]), iteration_range=(0, model.best_ntree_limit))\n",
        "    pred_xgb += model.predict(dte, iteration_range=(0, model.best_ntree_limit))/n_splits\n",
        "    del model, dtr, dva, dte; gc.collect()\n",
        "cv_xgb = mean_squared_error(y_log, oof_xgb, squared=False)\n",
        "print(f'XGBoost CV RMSLE: {cv_xgb:.6f}')\n",
        "\n",
        "# NNLS blend in log space\n",
        "P = np.vstack([oof_lgb, oof_cb, oof_xgb]).T\n",
        "w, _ = nnls(P, y_log.values)\n",
        "w = w / (w.sum() if w.sum() > 0 else 1)\n",
        "print('NNLS weights (LGB, CB, XGB):', w)\n",
        "oof_blend = P @ w\n",
        "cv_blend = mean_squared_error(y_log, oof_blend, squared=False)\n",
        "print(f'Blended (NNLS) CV RMSLE: {cv_blend:.6f}')\n",
        "Ptest = np.vstack([pred_lgb, pred_cb, pred_xgb]).T\n",
        "pred_blend = Ptest @ w\n",
        "\n",
        "# Save submission (clip after expm1)\n",
        "pred_bg = np.expm1(pred_blend).clip(0, 6.5)\n",
        "sub = pd.DataFrame({'id': test_fe['id'], 'bandgap_energy_ev': pred_bg})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('submission.csv saved:', sub.shape)\n",
        "sub.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "c70e1a75-134d-495c-bc01-3242b8abc8f1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 8A) Fast feature pass: drop rdf_*, add key features, rebuild 8-folds, add OOF TE (no importance pruning)\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "\n",
        "assert 'train_fe' in globals() and 'test_fe' in globals(), 'Run earlier feature engineering cells first.'\n",
        "t0_all = time.time()\n",
        "\n",
        "# Drop rdf_* to reduce noise\n",
        "t0 = time.time()\n",
        "rdf_cols_tr = [c for c in train_fe.columns if c.startswith('rdf_')]\n",
        "rdf_cols_te = [c for c in test_fe.columns if c.startswith('rdf_')]\n",
        "train_fe.drop(columns=rdf_cols_tr, inplace=True, errors='ignore')\n",
        "test_fe.drop(columns=rdf_cols_te, inplace=True, errors='ignore')\n",
        "print('Dropped RDF bins:', len(rdf_cols_tr), '| elapsed:', f'{time.time()-t0:.2f}s', flush=True)\n",
        "\n",
        "# Ensure base composition fractions, cation weights, Vegard\n",
        "t0 = time.time()\n",
        "for df in (train_fe, test_fe):\n",
        "    if 'frac_al' not in df.columns: df['frac_al'] = df['percent_atom_al'] / 100.0\n",
        "    if 'frac_ga' not in df.columns: df['frac_ga'] = df['percent_atom_ga'] / 100.0\n",
        "    if 'frac_in' not in df.columns: df['frac_in'] = df['percent_atom_in'] / 100.0\n",
        "    if 'percent_atom_o' not in df.columns:\n",
        "        df['percent_atom_o'] = 100.0 - (df['percent_atom_al'] + df['percent_atom_ga'] + df['percent_atom_in'])\n",
        "    if 'frac_o' not in df.columns: df['frac_o'] = df['percent_atom_o'] / 100.0\n",
        "    frac_cat = (df['frac_al'] + df['frac_ga'] + df['frac_in']).replace(0, np.nan)\n",
        "    if 'w_al' not in df.columns: df['w_al'] = (df['frac_al']/frac_cat).fillna(0)\n",
        "    if 'w_ga' not in df.columns: df['w_ga'] = (df['frac_ga']/frac_cat).fillna(0)\n",
        "    if 'w_in' not in df.columns: df['w_in'] = (df['frac_in']/frac_cat).fillna(0)\n",
        "    if 'vegard_bg' not in df.columns: df['vegard_bg'] = 8.8*df['w_al'] + 4.8*df['w_ga'] + 2.9*df['w_in']\n",
        "print('Ensured base composition + Vegard | elapsed:', f'{time.time()-t0:.2f}s', flush=True)\n",
        "\n",
        "# Bowing/log features\n",
        "t0 = time.time()\n",
        "def add_bowing_logs(df):\n",
        "    df['bow_in'] = df['w_in']*(1.0 - df['w_in'])\n",
        "    df['bow_ga'] = df['w_ga']*(1.0 - df['w_ga'])\n",
        "    if 'volume_per_atom' in df.columns: df['log_vpa'] = np.log1p(df['volume_per_atom'].clip(lower=0))\n",
        "    if 'atoms_per_volume' in df.columns: df['log_apv'] = np.log1p(df['atoms_per_volume'].clip(lower=0))\n",
        "    df['log_oc'] = np.log1p((df['frac_o']/(df['frac_al']+df['frac_ga']+df['frac_in']+1e-9)).clip(lower=0))\n",
        "    df['log_in_over_al'] = np.log1p(((df['frac_in']+1e-6)/(df['frac_al']+1e-6)).clip(lower=0))\n",
        "    return df\n",
        "train_fe = add_bowing_logs(train_fe); test_fe = add_bowing_logs(test_fe)\n",
        "print('Added bowing/log features | elapsed:', f'{time.time()-t0:.2f}s', flush=True)\n",
        "\n",
        "# Interactions (lightweight)\n",
        "t0 = time.time()\n",
        "def add_interactions(df):\n",
        "    fa, fg, fi = df['frac_al'], df['frac_ga'], df['frac_in']\n",
        "    if 'al_in_diff_sq' not in df.columns: df['al_in_diff_sq'] = (fa - fi) ** 2\n",
        "    if 'ga_in_diff_sq' not in df.columns: df['ga_in_diff_sq'] = (fg - fi) ** 2\n",
        "    if 'frac_al_cu' not in df.columns: df['frac_al_cu'] = fa ** 3\n",
        "    if 'frac_ga_cu' not in df.columns: df['frac_ga_cu'] = fg ** 3\n",
        "    if 'frac_in_cu' not in df.columns: df['frac_in_cu'] = fi ** 3\n",
        "    if 'w_al_x_veg' not in df.columns: df['w_al_x_veg'] = df['w_al'] * df['vegard_bg']\n",
        "    if 'w_in_x_veg' not in df.columns: df['w_in_x_veg'] = df['w_in'] * df['vegard_bg']\n",
        "    for wname in ['al','ga','in']:\n",
        "        if f'w_{wname}_sq' not in df.columns: df[f'w_{wname}_sq'] = df[f'w_{wname}']**2\n",
        "    if 'w_al_ga' not in df.columns: df['w_al_ga'] = df['w_al']*df['w_ga']\n",
        "    if 'w_al_in' not in df.columns: df['w_al_in'] = df['w_al']*df['w_in']\n",
        "    if 'w_ga_in' not in df.columns: df['w_ga_in'] = df['w_ga']*df['w_in']\n",
        "    return df\n",
        "train_fe = add_interactions(train_fe); test_fe = add_interactions(test_fe)\n",
        "print('After interactions:', train_fe.shape, test_fe.shape, '| elapsed:', f'{time.time()-t0:.2f}s', flush=True)\n",
        "\n",
        "# Cation-weighted contrasts\n",
        "t0 = time.time()\n",
        "props = {\n",
        "    'chi_pauling': {'Al':1.61,'Ga':1.81,'In':1.78,'O':3.44},\n",
        "    'ionic_radius': {'Al':0.535,'Ga':0.62,'In':0.80,'O':1.38},\n",
        "    'Z': {'Al':13,'Ga':31,'In':49,'O':8},\n",
        "    'period': {'Al':3,'Ga':4,'In':5,'O':2},\n",
        "    'group': {'Al':13,'Ga':13,'In':13,'O':16},\n",
        "    'covalent_radius': {'Al':1.21,'Ga':1.22,'In':1.42,'O':0.66},\n",
        "    'first_ionization_energy': {'Al':5.986,'Ga':5.999,'In':5.786,'O':13.618},\n",
        "    'electron_affinity': {'Al':0.441,'Ga':0.30,'In':0.30,'O':1.461}\n",
        "}\n",
        "def add_cation_weighted(df):\n",
        "    wa, wg, wi = df['w_al'], df['w_ga'], df['w_in']\n",
        "    for name, table in props.items():\n",
        "        ca, cg, ci, co = table['Al'], table['Ga'], table['In'], table['O']\n",
        "        wmean = wa*ca + wg*cg + wi*ci\n",
        "        df[f'catw_{name}_mean'] = wmean\n",
        "        df[f'catw_{name}_var'] = (wa*(ca-wmean)**2 + wg*(cg-wmean)**2 + wi*(ci-wmean)**2)\n",
        "        if name in ['chi_pauling','ionic_radius']:\n",
        "            df[f'o_minus_catw_{name}'] = co - wmean\n",
        "    return df\n",
        "train_fe = add_cation_weighted(train_fe); test_fe = add_cation_weighted(test_fe)\n",
        "print('Added cation-weighted contrasts | elapsed:', f'{time.time()-t0:.2f}s', flush=True)\n",
        "\n",
        "# Rebuild 8-fold stratified group-disjoint folds (vectorized)\n",
        "t0 = time.time()\n",
        "assert 'compute_stoich_groups' in globals(), 'compute_stoich_groups missing; run grouping cell.'\n",
        "y = train_fe['bandgap_energy_ev'].astype(float)\n",
        "need_cols = ['N','n_al','n_ga','n_in','n_o']\n",
        "missing = [c for c in need_cols if c not in train_fe.columns]\n",
        "if missing:\n",
        "    tr_csv = pd.read_csv('train.csv'); te_csv = pd.read_csv('test.csv')\n",
        "    _, N_tr, al_tr, ga_tr, in_tr, o_tr = compute_stoich_groups(tr_csv)\n",
        "    train_fe['N'] = N_tr; train_fe['n_al'] = al_tr; train_fe['n_ga'] = ga_tr; train_fe['n_in'] = in_tr; train_fe['n_o'] = o_tr\n",
        "    _, N_te, al_te, ga_te, in_te, o_te = compute_stoich_groups(te_csv)\n",
        "    test_fe['N'] = N_te; test_fe['n_al'] = al_te; test_fe['n_ga'] = ga_te; test_fe['n_in'] = in_te; test_fe['n_o'] = o_te\n",
        "gkey = train_fe[['N','n_al','n_ga','n_in']].astype(int).astype(str).agg('_'.join, axis=1)\n",
        "gmean = y.groupby(gkey).mean()\n",
        "gbin = pd.qcut(gmean, q=10, labels=False, duplicates='drop')\n",
        "uniq = pd.DataFrame({'g': gmean.index, 'bin': gbin.values}).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "n_splits_new = 8\n",
        "skf = StratifiedKFold(n_splits=n_splits_new, shuffle=True, random_state=42)\n",
        "group_to_fold = {}\n",
        "for k, (_, val_idx) in enumerate(skf.split(uniq['g'], uniq['bin'])):\n",
        "    for g in uniq['g'].iloc[val_idx]: group_to_fold[g] = k\n",
        "fold_ids = gkey.map(group_to_fold).astype(int).values\n",
        "print('Fold sizes:', pd.Series(fold_ids).value_counts().sort_index().to_dict(), '| elapsed:', f'{time.time()-t0:.2f}s', flush=True)\n",
        "\n",
        "# OOF target encoding for spacegroup (in log space)\n",
        "t0 = time.time()\n",
        "train_fe['te_sg'] = 0.0\n",
        "y_log = np.log1p(y.clip(lower=0))\n",
        "global_mean = float(y_log.mean())\n",
        "for k in range(n_splits_new):\n",
        "    trn_idx = np.where(fold_ids!=k)[0]; val_idx = np.where(fold_ids==k)[0]\n",
        "    m = train_fe.iloc[trn_idx].groupby('spacegroup')['bandgap_energy_ev'].apply(lambda s: np.log1p(s.clip(lower=0)).mean())\n",
        "    te_map = m.to_dict()\n",
        "    train_fe.loc[train_fe.index[val_idx], 'te_sg'] = train_fe.iloc[val_idx]['spacegroup'].map(te_map).fillna(global_mean).values\n",
        "sg_map_full = train_fe.groupby('spacegroup')['bandgap_energy_ev'].apply(lambda s: np.log1p(s.clip(lower=0)).mean()).to_dict()\n",
        "test_fe['te_sg'] = test_fe['spacegroup'].map(sg_map_full).fillna(global_mean)\n",
        "print('Added OOF TE for spacegroup | elapsed:', f'{time.time()-t0:.2f}s', flush=True)\n",
        "\n",
        "print('Fast pass done. Shapes:', train_fe.shape, test_fe.shape, '| total elapsed:', f'{time.time()-t0_all:.1f}s', flush=True)\n",
        "gc.collect();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "c5549886-974c-4986-af6d-5c629b948167",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Rebuild 8-fold stratified group-disjoint folds quickly (vectorized, minimal I/O)\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "assert 'train_fe' in globals() and 'compute_stoich_groups' in globals(), 'Prerequisites missing.'\n",
        "y = train_fe['bandgap_energy_ev'].astype(float)\n",
        "if 'groups' not in globals():\n",
        "    _gkey, *_ = compute_stoich_groups(pd.read_csv('train.csv'))\n",
        "    groups = _gkey.astype(str)\n",
        "gkey = groups.astype(str)\n",
        "gmean = y.groupby(gkey).mean()\n",
        "gbin = pd.qcut(gmean, q=10, labels=False, duplicates='drop')\n",
        "uniq = pd.DataFrame({'g': gmean.index, 'bin': gbin.values}).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "skf = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)\n",
        "group_to_fold = {}\n",
        "for k, (_, val_idx) in enumerate(skf.split(uniq['g'], uniq['bin'])):\n",
        "    for g in uniq['g'].iloc[val_idx]:\n",
        "        group_to_fold[g] = k\n",
        "fold_ids = gkey.map(group_to_fold).astype(int).values\n",
        "print('8-fold fold_ids built. Fold sizes:', pd.Series(fold_ids).value_counts().sort_index().to_dict())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "8a86de78-8d90-456c-8375-1b34a77b4a4f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Clean end-to-end pipeline (no Magpie): build compact features, 8-fold CV, OOF TE, 3-seed LGBM+XGB, NNLS blend\n",
        "import numpy as np, pandas as pd, time, gc, os\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "t0_all = time.time()\n",
        "print('Start clean pipeline...')\n",
        "\n",
        "# ------------------ Load base CSVs ------------------\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "y = train['bandgap_energy_ev'].astype(float)\n",
        "\n",
        "# ------------------ Engineer base features (reuse functions if available) ------------------\n",
        "assert 'engineer_features' in globals(), 'Run Cell 3 to define engineer_features()'\n",
        "train_fe = engineer_features(train)\n",
        "test_fe = engineer_features(test)\n",
        "\n",
        "# ------------------ Stoichiometric counts and group key ------------------\n",
        "assert 'compute_stoich_groups' in globals(), 'Run Cell 3 to define compute_stoich_groups()'\n",
        "groups, N, n_al, n_ga, n_in, n_o = compute_stoich_groups(train)\n",
        "train_fe['N'] = N; train_fe['n_al'] = n_al; train_fe['n_ga'] = n_ga; train_fe['n_in'] = n_in; train_fe['n_o'] = n_o\n",
        "gkey = groups.astype(str)\n",
        "groups_te, N_te, al_te, ga_te, in_te, o_te = compute_stoich_groups(test)\n",
        "test_fe['N'] = N_te; test_fe['n_al'] = al_te; test_fe['n_ga'] = ga_te; test_fe['n_in'] = in_te; test_fe['n_o'] = o_te\n",
        "\n",
        "# ------------------ Composition weights and Vegard + bowing/logs + interactions ------------------\n",
        "for df in (train_fe, test_fe):\n",
        "    df['frac_al'] = df['percent_atom_al']/100.0\n",
        "    df['frac_ga'] = df['percent_atom_ga']/100.0\n",
        "    df['frac_in'] = df['percent_atom_in']/100.0\n",
        "    df['percent_atom_o'] = 100.0 - (df['percent_atom_al'] + df['percent_atom_ga'] + df['percent_atom_in'])\n",
        "    df['frac_o'] = df['percent_atom_o']/100.0\n",
        "    frac_cat = (df['frac_al'] + df['frac_ga'] + df['frac_in']).replace(0, np.nan)\n",
        "    df['w_al'] = (df['frac_al']/frac_cat).fillna(0)\n",
        "    df['w_ga'] = (df['frac_ga']/frac_cat).fillna(0)\n",
        "    df['w_in'] = (df['frac_in']/frac_cat).fillna(0)\n",
        "    df['vegard_bg'] = 8.8*df['w_al'] + 4.8*df['w_ga'] + 2.9*df['w_in']\n",
        "    df['bow_in'] = df['w_in']*(1.0 - df['w_in'])\n",
        "    df['bow_ga'] = df['w_ga']*(1.0 - df['w_ga'])\n",
        "    if 'volume_per_atom' in df.columns: df['log_vpa'] = np.log1p(df['volume_per_atom'].clip(lower=0))\n",
        "    if 'atoms_per_volume' in df.columns: df['log_apv'] = np.log1p(df['atoms_per_volume'].clip(lower=0))\n",
        "    df['log_oc'] = np.log1p((df['frac_o']/(df['frac_al']+df['frac_ga']+df['frac_in']+1e-9)).clip(lower=0))\n",
        "    df['log_in_over_al'] = np.log1p(((df['frac_in']+1e-6)/(df['frac_al']+1e-6)).clip(lower=0))\n",
        "    # interactions\n",
        "    df['w_al_sq'] = df['w_al']**2; df['w_ga_sq'] = df['w_ga']**2; df['w_in_sq'] = df['w_in']**2\n",
        "    df['w_al_ga'] = df['w_al']*df['w_ga']; df['w_al_in'] = df['w_al']*df['w_in']; df['w_ga_in'] = df['w_ga']*df['w_in']\n",
        "    df['w_al_x_veg'] = df['w_al']*df['vegard_bg']; df['w_in_x_veg'] = df['w_in']*df['vegard_bg']\n",
        "    df['al_in_diff_sq'] = (df['frac_al']-df['frac_in'])**2; df['ga_in_diff_sq'] = (df['frac_ga']-df['frac_in'])**2\n",
        "    df['frac_al_cu'] = df['frac_al']**3; df['frac_ga_cu'] = df['frac_ga']**3; df['frac_in_cu'] = df['frac_in']**3\n",
        "    # a_red/b_red/c_red\n",
        "    vol = df['cell_volume'].replace(0, np.nan); l = vol.pow(1/3)\n",
        "    df['a_red'] = df['lattice_vector_1_ang']/l; df['b_red'] = df['lattice_vector_2_ang']/l; df['c_red'] = df['lattice_vector_3_ang']/l\n",
        "\n",
        "# mix metrics (if not already) were added by engineer_features\n",
        "\n",
        "# ------------------ Cation-weighted contrasts (EN, ionic radius) ------------------\n",
        "props = {\n",
        "    'chi_pauling': {'Al':1.61,'Ga':1.81,'In':1.78,'O':3.44},\n",
        "    'ionic_radius': {'Al':0.535,'Ga':0.62,'In':0.80,'O':1.38}\n",
        "}\n",
        "def add_cation_weighted(df):\n",
        "    wa, wg, wi = df['w_al'], df['w_ga'], df['w_in']\n",
        "    for name, tbl in props.items():\n",
        "        ca, cg, ci, co = tbl['Al'], tbl['Ga'], tbl['In'], tbl['O']\n",
        "        wmean = wa*ca + wg*cg + wi*ci\n",
        "        df[f'catw_{name}_mean'] = wmean\n",
        "        df[f'catw_{name}_var'] = (wa*(ca-wmean)**2 + wg*(cg-wmean)**2 + wi*(ci-wmean)**2)\n",
        "        df[f'o_minus_catw_{name}'] = co - wmean\n",
        "    return df\n",
        "train_fe = add_cation_weighted(train_fe); test_fe = add_cation_weighted(test_fe)\n",
        "\n",
        "# ------------------ Minimal XYZ features (load cache or build, then prune) ------------------\n",
        "cache_tr = Path('xyz_train.parquet'); cache_te = Path('xyz_test.parquet')\n",
        "if cache_tr.exists() and cache_te.exists():\n",
        "    xyz_tr = pd.read_parquet(cache_tr); xyz_te = pd.read_parquet(cache_te)\n",
        "else:\n",
        "    assert 'build_xyz_df' in globals(), 'Run Cell 6 to define build_xyz_df/read_xyz_features'\n",
        "    xyz_tr = pd.read_parquet(cache_tr) if cache_tr.exists() else build_xyz_df('train', train['id'].values, n_jobs=16)\n",
        "    xyz_te = pd.read_parquet(cache_te) if cache_te.exists() else build_xyz_df('test', test['id'].values, n_jobs=16)\n",
        "    xyz_tr.to_parquet(cache_tr, index=False); xyz_te.to_parquet(cache_te, index=False)\n",
        "\n",
        "# prune: drop all rdf_* and mid-quantiles p5/p25/p50/p75/p95; keep only min/mean/std/max of d_* for all,cc,co,oo and nn_* min/mean/max\n",
        "def prune_xyz(df):\n",
        "    keep = ['id']\n",
        "    for base in ['all','cc','co','oo']:\n",
        "        for stat in ['min','mean','std','max']:\n",
        "            keep.append(f'd_{base}_{stat}')\n",
        "    for dirn in ['c_to_o','o_to_c']:\n",
        "        for stat in ['min','mean','max']:\n",
        "            keep.append(f'nn_{dirn}_{stat}')\n",
        "    cols = [c for c in df.columns if c in keep]\n",
        "    return df[cols].copy()\n",
        "xyz_tr_p = prune_xyz(xyz_tr)\n",
        "xyz_te_p = prune_xyz(xyz_te)\n",
        "train_fe = train_fe.merge(xyz_tr_p, on='id', how='left')\n",
        "test_fe = test_fe.merge(xyz_te_p, on='id', how='left')\n",
        "print('Merged minimal XYZ:', train_fe.shape, test_fe.shape)\n",
        "\n",
        "# ------------------ Build 8-fold stratified group-disjoint folds ------------------\n",
        "y = train_fe['bandgap_energy_ev'].astype(float)\n",
        "gmean = y.groupby(gkey).mean()\n",
        "gbin = pd.qcut(gmean, q=10, labels=False, duplicates='drop')\n",
        "uniq = pd.DataFrame({'g': gmean.index, 'bin': gbin.values}).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "skf = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)\n",
        "group_to_fold = {}\n",
        "for k, (_, val_idx) in enumerate(skf.split(uniq['g'], uniq['bin'])):\n",
        "    for g in uniq['g'].iloc[val_idx]: group_to_fold[g] = k\n",
        "fold_ids = gkey.map(group_to_fold).astype(int).values\n",
        "print('Fold sizes:', pd.Series(fold_ids).value_counts().sort_index().to_dict())\n",
        "\n",
        "# ------------------ OOF target encoding for spacegroup in log space ------------------\n",
        "train_fe['te_sg'] = 0.0\n",
        "y_log = np.log1p(y.clip(lower=0))\n",
        "global_mean = float(y_log.mean())\n",
        "for k in range(8):\n",
        "    trn_idx = np.where(fold_ids!=k)[0]; val_idx = np.where(fold_ids==k)[0]\n",
        "    m = train_fe.iloc[trn_idx].groupby('spacegroup')['bandgap_energy_ev'].apply(lambda s: np.log1p(s.clip(lower=0)).mean())\n",
        "    te_map = m.to_dict()\n",
        "    sg_series = train_fe.iloc[val_idx]['spacegroup'].astype(str)\n",
        "    mapped = sg_series.map(te_map).astype(float).fillna(global_mean)\n",
        "    train_fe.loc[train_fe.index[val_idx], 'te_sg'] = mapped.values\n",
        "sg_map_full = train_fe.groupby('spacegroup')['bandgap_energy_ev'].apply(lambda s: np.log1p(s.clip(lower=0)).mean()).to_dict()\n",
        "test_fe['te_sg'] = test_fe['spacegroup'].astype(str).map(sg_map_full).astype(float).fillna(global_mean)\n",
        "\n",
        "# ------------------ Build final feature matrices ------------------\n",
        "drop_cols = ['id','bandgap_energy_ev']\n",
        "common_cols = [c for c in train_fe.columns if c in test_fe.columns]\n",
        "features = [c for c in common_cols if c not in drop_cols]\n",
        "# Drop any rdf_* remnants just in case\n",
        "features = [c for c in features if not c.startswith('rdf_')]\n",
        "# Ensure numeric except allow spacegroup\n",
        "num_cols = list(train_fe[features].select_dtypes(include=[np.number]).columns)\n",
        "if 'spacegroup' in features and 'spacegroup' not in num_cols: num_cols.append('spacegroup')\n",
        "train_X = train_fe[num_cols].copy(); test_X = test_fe[num_cols].copy()\n",
        "med = train_X.median(numeric_only=True); train_X = train_X.fillna(med); test_X = test_X.fillna(med)\n",
        "print('Feature matrix shapes:', train_X.shape, test_X.shape)\n",
        "\n",
        "# ------------------ Models: 3 seeds x (LGBM, XGB) ------------------\n",
        "import lightgbm as lgb, xgboost as xgb\n",
        "seeds = [7, 42, 2025]\n",
        "n_splits = 8\n",
        "oof_lgb_seeds = []; pred_lgb_seeds = []\n",
        "oof_xgb_seeds = []; pred_xgb_seeds = []\n",
        "\n",
        "for SEED in seeds:\n",
        "    print(f'-- LGBM seed {SEED} --'); t0 = time.time()\n",
        "    params_lgb = {\n",
        "        'objective':'regression','metric':'rmse','learning_rate':0.03,\n",
        "        'num_leaves':128,'max_depth':-1,'min_data_in_leaf':200,\n",
        "        'feature_fraction':0.8,'bagging_fraction':0.8,'bagging_freq':1,\n",
        "        'lambda_l2':3.0,'lambda_l1':0.0,'verbosity':-1,'seed':SEED\n",
        "    }\n",
        "    oof_lgb = np.zeros(len(train_X)); pred_lgb = np.zeros(len(test_X))\n",
        "    for k in range(n_splits):\n",
        "        trn = np.where(fold_ids!=k)[0]; val = np.where(fold_ids==k)[0]\n",
        "        dtr = lgb.Dataset(train_X.iloc[trn], label=y_log.iloc[trn], categorical_feature=['spacegroup'] if 'spacegroup' in train_X.columns else None, free_raw_data=False)\n",
        "        dva = lgb.Dataset(train_X.iloc[val], label=y_log.iloc[val], categorical_feature=['spacegroup'] if 'spacegroup' in train_X.columns else None, free_raw_data=False)\n",
        "        m = lgb.train(params_lgb, dtr, num_boost_round=7000, valid_sets=[dtr,dva], valid_names=['train','valid'], callbacks=[lgb.early_stopping(450), lgb.log_evaluation(300)])\n",
        "        oof_lgb[val] = m.predict(train_X.iloc[val], num_iteration=m.best_iteration)\n",
        "        pred_lgb += m.predict(test_X, num_iteration=m.best_iteration)/n_splits\n",
        "        del m, dtr, dva; gc.collect()\n",
        "    rmse = float(mean_squared_error(y_log, oof_lgb) ** 0.5); print(f'LGBM seed {SEED} OOF RMSLE: {rmse:.6f} | {time.time()-t0:.1f}s')\n",
        "    oof_lgb_seeds.append(oof_lgb); pred_lgb_seeds.append(pred_lgb)\n",
        "\n",
        "    print(f'-- XGB seed {SEED} --'); t0 = time.time()\n",
        "    xgb_params = dict(objective='reg:squarederror', eval_metric='rmse', tree_method='hist',\n",
        "                      max_depth=6, eta=0.03, subsample=0.8, colsample_bytree=0.8,\n",
        "                      min_child_weight=5, reg_lambda=3.0, reg_alpha=0.0, random_state=SEED)\n",
        "    oof_xgb = np.zeros(len(train_X)); pred_xgb = np.zeros(len(test_X))\n",
        "    for k in range(n_splits):\n",
        "        trn = np.where(fold_ids!=k)[0]; val = np.where(fold_ids==k)[0]\n",
        "        dtr = xgb.DMatrix(train_X.iloc[trn], label=y_log.iloc[trn], enable_categorical=True); dva = xgb.DMatrix(train_X.iloc[val], label=y_log.iloc[val], enable_categorical=True); dte = xgb.DMatrix(test_X, enable_categorical=True)\n",
        "        model = xgb.train(xgb_params, dtr, num_boost_round=8000, evals=[(dva,'valid')], early_stopping_rounds=400, verbose_eval=False)\n",
        "        oof_xgb[val] = model.predict(dva)\n",
        "        pred_xgb += model.predict(dte)/n_splits\n",
        "        del model, dtr, dva, dte; gc.collect()\n",
        "    rmse = float(mean_squared_error(y_log, oof_xgb) ** 0.5); print(f'XGB seed {SEED} OOF RMSLE: {rmse:.6f} | {time.time()-t0:.1f}s')\n",
        "    oof_xgb_seeds.append(oof_xgb); pred_xgb_seeds.append(pred_xgb)\n",
        "\n",
        "# Average across seeds\n",
        "oof_lgb_avg = np.mean(np.vstack(oof_lgb_seeds), axis=0)\n",
        "pred_lgb_avg = np.mean(np.vstack(pred_lgb_seeds), axis=0)\n",
        "oof_xgb_avg = np.mean(np.vstack(oof_xgb_seeds), axis=0)\n",
        "pred_xgb_avg = np.mean(np.vstack(pred_xgb_seeds), axis=0)\n",
        "cv_lgb = float(mean_squared_error(y_log, oof_lgb_avg) ** 0.5)\n",
        "cv_xgb = float(mean_squared_error(y_log, oof_xgb_avg) ** 0.5)\n",
        "print(f'Averaged LGBM CV RMSLE: {cv_lgb:.6f} | Averaged XGB CV RMSLE: {cv_xgb:.6f}')\n",
        "\n",
        "# ------------------ NNLS blend on seed-averaged OOF ------------------\n",
        "from scipy.optimize import nnls\n",
        "P = np.vstack([oof_lgb_avg, oof_xgb_avg]).T\n",
        "w, _ = nnls(P, y_log.values)\n",
        "w = w / (w.sum() if w.sum() > 0 else 1.0)\n",
        "print('NNLS weights (LGB, XGB):', w)\n",
        "oof_blend = P @ w\n",
        "cv_blend = float(mean_squared_error(y_log, oof_blend) ** 0.5)\n",
        "print(f'Blended CV RMSLE: {cv_blend:.6f}')\n",
        "Ptest = np.vstack([pred_lgb_avg, pred_xgb_avg]).T\n",
        "pred_blend = Ptest @ w\n",
        "\n",
        "# ------------------ Save submission ------------------\n",
        "pred_bandgap = np.expm1(pred_blend).clip(0, 6.5)\n",
        "sub = pd.DataFrame({'id': test['id'], 'bandgap_energy_ev': pred_bandgap})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv', sub.shape, '| total elapsed:', f'{time.time()-t0_all:.1f}s')\n",
        "sub.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start clean pipeline...\nMerged minimal XYZ: (2160, 107) (240, 105)\nFold sizes: {0: 309, 1: 300, 2: 264, 3: 273, 4: 248, 5: 265, 6: 252, 7: 249}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_16606/921288145.py:118: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  m = train_fe.iloc[trn_idx].groupby('spacegroup')['bandgap_energy_ev'].apply(lambda s: np.log1p(s.clip(lower=0)).mean())\n/tmp/ipykernel_16606/921288145.py:118: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  m = train_fe.iloc[trn_idx].groupby('spacegroup')['bandgap_energy_ev'].apply(lambda s: np.log1p(s.clip(lower=0)).mean())\n/tmp/ipykernel_16606/921288145.py:118: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  m = train_fe.iloc[trn_idx].groupby('spacegroup')['bandgap_energy_ev'].apply(lambda s: np.log1p(s.clip(lower=0)).mean())\n/tmp/ipykernel_16606/921288145.py:118: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  m = train_fe.iloc[trn_idx].groupby('spacegroup')['bandgap_energy_ev'].apply(lambda s: np.log1p(s.clip(lower=0)).mean())\n/tmp/ipykernel_16606/921288145.py:118: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  m = train_fe.iloc[trn_idx].groupby('spacegroup')['bandgap_energy_ev'].apply(lambda s: np.log1p(s.clip(lower=0)).mean())\n/tmp/ipykernel_16606/921288145.py:118: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  m = train_fe.iloc[trn_idx].groupby('spacegroup')['bandgap_energy_ev'].apply(lambda s: np.log1p(s.clip(lower=0)).mean())\n/tmp/ipykernel_16606/921288145.py:118: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  m = train_fe.iloc[trn_idx].groupby('spacegroup')['bandgap_energy_ev'].apply(lambda s: np.log1p(s.clip(lower=0)).mean())\n/tmp/ipykernel_16606/921288145.py:118: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  m = train_fe.iloc[trn_idx].groupby('spacegroup')['bandgap_energy_ev'].apply(lambda s: np.log1p(s.clip(lower=0)).mean())\n/tmp/ipykernel_16606/921288145.py:123: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  sg_map_full = train_fe.groupby('spacegroup')['bandgap_energy_ev'].apply(lambda s: np.log1p(s.clip(lower=0)).mean()).to_dict()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature matrix shapes: (2160, 105) (240, 105)\n-- LGBM seed 7 --\nTraining until validation scores don't improve for 450 rounds\n[300]\ttrain's rmse: 0.0856496\tvalid's rmse: 0.0774812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0772158\tvalid's rmse: 0.0745231\n[900]\ttrain's rmse: 0.073193\tvalid's rmse: 0.0738798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0704691\tvalid's rmse: 0.0739593\n[1500]\ttrain's rmse: 0.068259\tvalid's rmse: 0.0742076\nEarly stopping, best iteration is:\n[1067]\ttrain's rmse: 0.0715769\tvalid's rmse: 0.0736709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 450 rounds\n[300]\ttrain's rmse: 0.0815879\tvalid's rmse: 0.100798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0733341\tvalid's rmse: 0.0967418\n[900]\ttrain's rmse: 0.0694476\tvalid's rmse: 0.0965866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[653]\ttrain's rmse: 0.0725173\tvalid's rmse: 0.0963557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 450 rounds\n[300]\ttrain's rmse: 0.0836861\tvalid's rmse: 0.0940116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0749884\tvalid's rmse: 0.0864025\n[900]\ttrain's rmse: 0.0708483\tvalid's rmse: 0.0846562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0681095\tvalid's rmse: 0.084239\n[1500]\ttrain's rmse: 0.0658102\tvalid's rmse: 0.0843299\nEarly stopping, best iteration is:\n[1117]\ttrain's rmse: 0.0687921\tvalid's rmse: 0.0840047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 450 rounds\n[300]\ttrain's rmse: 0.080151\tvalid's rmse: 0.111537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0723708\tvalid's rmse: 0.107929\n[900]\ttrain's rmse: 0.068648\tvalid's rmse: 0.106719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0661379\tvalid's rmse: 0.10621\n[1500]\ttrain's rmse: 0.064165\tvalid's rmse: 0.106117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\ttrain's rmse: 0.0625183\tvalid's rmse: 0.106034\n[2100]\ttrain's rmse: 0.0610491\tvalid's rmse: 0.106228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1905]\ttrain's rmse: 0.0619777\tvalid's rmse: 0.105949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 450 rounds\n[300]\ttrain's rmse: 0.0830686\tvalid's rmse: 0.0862842\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0751174\tvalid's rmse: 0.084659\n[900]\ttrain's rmse: 0.0713326\tvalid's rmse: 0.0847306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0686077\tvalid's rmse: 0.0849417\nEarly stopping, best iteration is:\n[854]\ttrain's rmse: 0.071823\tvalid's rmse: 0.0845651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 450 rounds\n[300]\ttrain's rmse: 0.0843786\tvalid's rmse: 0.0782782\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0762588\tvalid's rmse: 0.0738623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain's rmse: 0.0724736\tvalid's rmse: 0.07284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0698496\tvalid's rmse: 0.0722456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\ttrain's rmse: 0.0677388\tvalid's rmse: 0.072013\n[1800]\ttrain's rmse: 0.065988\tvalid's rmse: 0.0721585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\ttrain's rmse: 0.0643984\tvalid's rmse: 0.0722795\nEarly stopping, best iteration is:\n[1652]\ttrain's rmse: 0.0668233\tvalid's rmse: 0.0718587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 450 rounds\n[300]\ttrain's rmse: 0.0812928\tvalid's rmse: 0.109006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0729543\tvalid's rmse: 0.103708\n[900]\ttrain's rmse: 0.0691171\tvalid's rmse: 0.102344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.066445\tvalid's rmse: 0.101943\n[1500]\ttrain's rmse: 0.0643745\tvalid's rmse: 0.101888\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\ttrain's rmse: 0.0625484\tvalid's rmse: 0.101979\nEarly stopping, best iteration is:\n[1439]\ttrain's rmse: 0.0647881\tvalid's rmse: 0.101662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 450 rounds\n[300]\ttrain's rmse: 0.0841944\tvalid's rmse: 0.0762753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0764886\tvalid's rmse: 0.0723259\n[900]\ttrain's rmse: 0.0729529\tvalid's rmse: 0.0705437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0703476\tvalid's rmse: 0.0694227\n[1500]\ttrain's rmse: 0.0681918\tvalid's rmse: 0.0689946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\ttrain's rmse: 0.0664515\tvalid's rmse: 0.06855\n[2100]\ttrain's rmse: 0.0648673\tvalid's rmse: 0.0681788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\ttrain's rmse: 0.0634607\tvalid's rmse: 0.0678188\n[2700]\ttrain's rmse: 0.0622122\tvalid's rmse: 0.0674495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\ttrain's rmse: 0.0609934\tvalid's rmse: 0.0672938\n[3300]\ttrain's rmse: 0.0598725\tvalid's rmse: 0.0672952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3600]\ttrain's rmse: 0.0588747\tvalid's rmse: 0.0670349\n[3900]\ttrain's rmse: 0.0578846\tvalid's rmse: 0.0669583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4200]\ttrain's rmse: 0.0569952\tvalid's rmse: 0.066674\n[4500]\ttrain's rmse: 0.0561555\tvalid's rmse: 0.0669064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[4207]\ttrain's rmse: 0.0569705\tvalid's rmse: 0.066625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGBM seed 7 OOF RMSLE: 0.086713 | 10.3s\n-- XGB seed 7 --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB seed 7 OOF RMSLE: 0.092530 | 14.9s\n-- LGBM seed 42 --\nTraining until validation scores don't improve for 450 rounds\n[300]\ttrain's rmse: 0.085508\tvalid's rmse: 0.0781661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0771791\tvalid's rmse: 0.0747851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain's rmse: 0.0731036\tvalid's rmse: 0.0739759\n[1200]\ttrain's rmse: 0.0703407\tvalid's rmse: 0.0741083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[996]\ttrain's rmse: 0.0721374\tvalid's rmse: 0.0737348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 450 rounds\n[300]\ttrain's rmse: 0.0815807\tvalid's rmse: 0.0999958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0734396\tvalid's rmse: 0.0956292\n[900]\ttrain's rmse: 0.0695839\tvalid's rmse: 0.0950461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0669634\tvalid's rmse: 0.0954804\nEarly stopping, best iteration is:\n[860]\ttrain's rmse: 0.070012\tvalid's rmse: 0.0949792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 450 rounds\n[300]\ttrain's rmse: 0.0837445\tvalid's rmse: 0.0938564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0751999\tvalid's rmse: 0.0862385\n[900]\ttrain's rmse: 0.0709888\tvalid's rmse: 0.0844289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0682034\tvalid's rmse: 0.0839498\n[1500]\ttrain's rmse: 0.0659152\tvalid's rmse: 0.0842596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1177]\ttrain's rmse: 0.068366\tvalid's rmse: 0.0838876\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 450 rounds\n[300]\ttrain's rmse: 0.0798898\tvalid's rmse: 0.111709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0721518\tvalid's rmse: 0.10808\n[900]\ttrain's rmse: 0.0685581\tvalid's rmse: 0.107172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0660951\tvalid's rmse: 0.106718\n[1500]\ttrain's rmse: 0.0640944\tvalid's rmse: 0.106442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\ttrain's rmse: 0.0623874\tvalid's rmse: 0.10635\n[2100]\ttrain's rmse: 0.0609154\tvalid's rmse: 0.106664\nEarly stopping, best iteration is:\n[1651]\ttrain's rmse: 0.0632026\tvalid's rmse: 0.106149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 450 rounds\n[300]\ttrain's rmse: 0.0831767\tvalid's rmse: 0.0856575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0751093\tvalid's rmse: 0.0837509\n[900]\ttrain's rmse: 0.0712341\tvalid's rmse: 0.083749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0684839\tvalid's rmse: 0.0838816\nEarly stopping, best iteration is:\n[841]\ttrain's rmse: 0.0718286\tvalid's rmse: 0.0834152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 450 rounds\n[300]\ttrain's rmse: 0.0846432\tvalid's rmse: 0.0786524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0763644\tvalid's rmse: 0.0744153\n[900]\ttrain's rmse: 0.0725899\tvalid's rmse: 0.073078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0699753\tvalid's rmse: 0.072375\n[1500]\ttrain's rmse: 0.0678473\tvalid's rmse: 0.0720426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\ttrain's rmse: 0.0660848\tvalid's rmse: 0.0720875\n[2100]\ttrain's rmse: 0.0644905\tvalid's rmse: 0.0718777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\ttrain's rmse: 0.0630992\tvalid's rmse: 0.0720452\nEarly stopping, best iteration is:\n[2200]\ttrain's rmse: 0.0640098\tvalid's rmse: 0.0718165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 450 rounds\n[300]\ttrain's rmse: 0.0813722\tvalid's rmse: 0.109564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0731332\tvalid's rmse: 0.103896\n[900]\ttrain's rmse: 0.0692716\tvalid's rmse: 0.102814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0666337\tvalid's rmse: 0.102272\n[1500]\ttrain's rmse: 0.0645641\tvalid's rmse: 0.10213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\ttrain's rmse: 0.0627842\tvalid's rmse: 0.102127\n[2100]\ttrain's rmse: 0.0611821\tvalid's rmse: 0.102656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1699]\ttrain's rmse: 0.0633593\tvalid's rmse: 0.101965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 450 rounds\n[300]\ttrain's rmse: 0.0843094\tvalid's rmse: 0.0751923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0766112\tvalid's rmse: 0.0716441\n[900]\ttrain's rmse: 0.0729071\tvalid's rmse: 0.0703212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0702609\tvalid's rmse: 0.069114\n[1500]\ttrain's rmse: 0.0682411\tvalid's rmse: 0.0687042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\ttrain's rmse: 0.0664917\tvalid's rmse: 0.0681041\n[2100]\ttrain's rmse: 0.0648839\tvalid's rmse: 0.0678158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\ttrain's rmse: 0.0635033\tvalid's rmse: 0.0673557\n[2700]\ttrain's rmse: 0.0622023\tvalid's rmse: 0.0675497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\ttrain's rmse: 0.0609959\tvalid's rmse: 0.0671894\n[3300]\ttrain's rmse: 0.0599078\tvalid's rmse: 0.0670289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3600]\ttrain's rmse: 0.0588721\tvalid's rmse: 0.0671246\n[3900]\ttrain's rmse: 0.057941\tvalid's rmse: 0.067161\nEarly stopping, best iteration is:\n[3554]\ttrain's rmse: 0.059023\tvalid's rmse: 0.0669614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGBM seed 42 OOF RMSLE: 0.086466 | 13.7s\n-- XGB seed 42 --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB seed 42 OOF RMSLE: 0.092677 | 13.6s\n-- LGBM seed 2025 --\nTraining until validation scores don't improve for 450 rounds\n[300]\ttrain's rmse: 0.0859364\tvalid's rmse: 0.0771393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0771758\tvalid's rmse: 0.074141\n[900]\ttrain's rmse: 0.0730051\tvalid's rmse: 0.0731567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0702805\tvalid's rmse: 0.0730463\nEarly stopping, best iteration is:\n[1004]\ttrain's rmse: 0.0719747\tvalid's rmse: 0.0728362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 450 rounds\n[300]\ttrain's rmse: 0.0814154\tvalid's rmse: 0.0999094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0732328\tvalid's rmse: 0.0959326\n[900]\ttrain's rmse: 0.0694414\tvalid's rmse: 0.0953001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0667236\tvalid's rmse: 0.0958198\nEarly stopping, best iteration is:\n[854]\ttrain's rmse: 0.0699752\tvalid's rmse: 0.0952198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 450 rounds\n[300]\ttrain's rmse: 0.0837879\tvalid's rmse: 0.0938396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0753796\tvalid's rmse: 0.0859205\n[900]\ttrain's rmse: 0.071236\tvalid's rmse: 0.0842573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0683923\tvalid's rmse: 0.0838426\n[1500]\ttrain's rmse: 0.0661048\tvalid's rmse: 0.0840176\nEarly stopping, best iteration is:\n[1129]\ttrain's rmse: 0.0690391\tvalid's rmse: 0.0837008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 450 rounds\n[300]\ttrain's rmse: 0.0800815\tvalid's rmse: 0.111737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0723075\tvalid's rmse: 0.108137\n[900]\ttrain's rmse: 0.0686146\tvalid's rmse: 0.106972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0661008\tvalid's rmse: 0.106454\n[1500]\ttrain's rmse: 0.0640729\tvalid's rmse: 0.106422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\ttrain's rmse: 0.0624185\tvalid's rmse: 0.106393\nEarly stopping, best iteration is:\n[1350]\ttrain's rmse: 0.0650625\tvalid's rmse: 0.106209\nTraining until validation scores don't improve for 450 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain's rmse: 0.0833347\tvalid's rmse: 0.0859517\n[600]\ttrain's rmse: 0.0752093\tvalid's rmse: 0.0846921\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain's rmse: 0.0713428\tvalid's rmse: 0.084483\nEarly stopping, best iteration is:\n[640]\ttrain's rmse: 0.0745108\tvalid's rmse: 0.0843916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 450 rounds\n[300]\ttrain's rmse: 0.0845529\tvalid's rmse: 0.0783966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0763483\tvalid's rmse: 0.0736694\n[900]\ttrain's rmse: 0.0725471\tvalid's rmse: 0.072082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0699088\tvalid's rmse: 0.071604\n[1500]\ttrain's rmse: 0.0679742\tvalid's rmse: 0.0715702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\ttrain's rmse: 0.0661533\tvalid's rmse: 0.0716301\nEarly stopping, best iteration is:\n[1459]\ttrain's rmse: 0.0682271\tvalid's rmse: 0.0713699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 450 rounds\n[300]\ttrain's rmse: 0.0815259\tvalid's rmse: 0.110147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's rmse: 0.0731359\tvalid's rmse: 0.10455\n[900]\ttrain's rmse: 0.0692384\tvalid's rmse: 0.103231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's rmse: 0.0666076\tvalid's rmse: 0.103181\n[1500]\ttrain's rmse: 0.0644938\tvalid's rmse: 0.10303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\ttrain's rmse: 0.0627103\tvalid's rmse: 0.102991\n[2100]\ttrain's rmse: 0.0610989\tvalid's rmse: 0.102816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\ttrain's rmse: 0.0596901\tvalid's rmse: 0.10271\nEarly stopping, best iteration is:\n[2207]\ttrain's rmse: 0.06058\tvalid's rmse: 0.102592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 450 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain's rmse: 0.0843953\tvalid's rmse: 0.0756015\n[600]\ttrain's rmse: 0.0766445\tvalid's rmse: 0.0713352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain's rmse: 0.0728799\tvalid's rmse: 0.0698603\n[1200]\ttrain's rmse: 0.0703448\tvalid's rmse: 0.0691909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\ttrain's rmse: 0.0683241\tvalid's rmse: 0.0684698\n[1800]\ttrain's rmse: 0.0664805\tvalid's rmse: 0.0683905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\ttrain's rmse: 0.0649096\tvalid's rmse: 0.0680098\n[2400]\ttrain's rmse: 0.0635109\tvalid's rmse: 0.0677584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2700]\ttrain's rmse: 0.0622104\tvalid's rmse: 0.0677844\n[3000]\ttrain's rmse: 0.0610164\tvalid's rmse: 0.0676602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3300]\ttrain's rmse: 0.0598999\tvalid's rmse: 0.0673782\n[3600]\ttrain's rmse: 0.0588645\tvalid's rmse: 0.0673075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3900]\ttrain's rmse: 0.0579341\tvalid's rmse: 0.0672961\n[4200]\ttrain's rmse: 0.0570096\tvalid's rmse: 0.06733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4500]\ttrain's rmse: 0.0561554\tvalid's rmse: 0.0672897\n[4800]\ttrain's rmse: 0.0553514\tvalid's rmse: 0.0673912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[4524]\ttrain's rmse: 0.0560889\tvalid's rmse: 0.0671464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGBM seed 2025 OOF RMSLE: 0.086548 | 10.0s\n-- XGB seed 2025 --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB seed 2025 OOF RMSLE: 0.092398 | 13.5s\nAveraged LGBM CV RMSLE: 0.086440 | Averaged XGB CV RMSLE: 0.092294\nNNLS weights (LGB, XGB): [0.94364721 0.05635279]\nBlended CV RMSLE: 0.086418\nSaved submission.csv (240, 2) | total elapsed: 76.3s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "   id  bandgap_energy_ev\n0   1           1.921003\n1   2           1.697664\n2   3           4.329125\n3   4           2.918891\n4   5           1.117051",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>bandgap_energy_ev</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1.921003</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1.697664</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>4.329125</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2.918891</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1.117051</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "02e10968-dfd9-4d5d-a5e5-28a293fae0ac",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DIAG: quick stdout and state check\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "print('[DIAG] start')\n",
        "print('[DIAG] globals:', {k: True for k in ['train_fe','test_fe','fold_ids','engineer_features','compute_stoich_groups'] if k in globals()})\n",
        "if 'train_fe' in globals():\n",
        "    print('[DIAG] train_fe shape:', train_fe.shape)\n",
        "    rdfc = sum(1 for c in train_fe.columns if isinstance(c, str) and c.startswith('rdf_'))\n",
        "    print('[DIAG] rdf_* columns in train_fe:', rdfc)\n",
        "    print('[DIAG] sample cols:', list(train_fe.columns[:8]))\n",
        "if 'fold_ids' in globals():\n",
        "    try:\n",
        "        import numpy as _np\n",
        "        uniq, counts = _np.unique(fold_ids, return_counts=True)\n",
        "        print('[DIAG] fold_ids unique:', dict(zip(uniq.tolist(), counts.tolist())))\n",
        "    except Exception as e:\n",
        "        print('[DIAG] fold_ids check error:', e)\n",
        "print('[DIAG] done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "55fa97b2-7033-49a9-9ffb-48087b79d652",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install matminer/pymatgen explicitly with logs, then sanity-import\n",
        "import sys, subprocess, time, os\n",
        "t0 = time.time()\n",
        "print('[SETUP] Installing dependencies: pymatgen, matminer (prefer binary wheels)')\n",
        "os.environ['PIP_DISABLE_PIP_VERSION_CHECK'] = '1'\n",
        "os.environ['PIP_NO_INPUT'] = '1'\n",
        "cmd = [sys.executable, '-m', 'pip', 'install', '--prefer-binary', '--upgrade', 'pymatgen', 'matminer']\n",
        "print('[SETUP] Running:', ' '.join(cmd))\n",
        "subprocess.check_call(cmd)\n",
        "print('[SETUP] Install finished in', f'{time.time()-t0:.1f}s')\n",
        "print('[SETUP] Importing modules to warm cache...')\n",
        "import importlib\n",
        "mm = importlib.import_module('matminer')\n",
        "pmg = importlib.import_module('pymatgen')\n",
        "from matminer.featurizers.composition import Stoichiometry, ValenceOrbital, IonProperty\n",
        "from pymatgen.core.composition import Composition\n",
        "print('[SETUP] Versions -> matminer:', getattr(mm, '__version__', 'unknown'), '| pymatgen:', getattr(pmg, '__version__', 'unknown'))\n",
        "print('[SETUP] Ready.')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "id": "3716549b-7fee-46b9-b15c-e0303d022318",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Precompute cached low-dim matminer features (deduplicated compositions only)\n",
        "import numpy as np, pandas as pd, time, os, sys, subprocess, warnings\n",
        "from pathlib import Path\n",
        "\n",
        "t0_all = time.time()\n",
        "print('[MM] Start precompute low-dim matminer features...')\n",
        "os.environ['TQDM_DISABLE'] = '1'\n",
        "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Ensure grouping util exists\n",
        "assert 'compute_stoich_groups' in globals(), 'Run Cell 3 to define compute_stoich_groups()'\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "_, N_tr, al_tr, ga_tr, in_tr, o_tr = compute_stoich_groups(train)\n",
        "_, N_te, al_te, ga_te, in_te, o_te = compute_stoich_groups(test)\n",
        "comp_tr = pd.DataFrame({'n_al': al_tr, 'n_ga': ga_tr, 'n_in': in_tr, 'n_o': o_tr})\n",
        "comp_te = pd.DataFrame({'n_al': al_te, 'n_ga': ga_te, 'n_in': in_te, 'n_o': o_te})\n",
        "def comp_str_df(df):\n",
        "    return 'Al' + df['n_al'].astype(int).astype(str) + ' Ga' + df['n_ga'].astype(int).astype(str) + ' In' + df['n_in'].astype(int).astype(str) + ' O' + df['n_o'].astype(int).astype(str)\n",
        "comp_tr['composition'] = comp_str_df(comp_tr)\n",
        "comp_te['composition'] = comp_str_df(comp_te)\n",
        "\n",
        "def build_mm_lowdim_from_comp(comp_series, cache_path):\n",
        "    cache_p = Path(cache_path)\n",
        "    if cache_p.exists():\n",
        "        try:\n",
        "            cached = pd.read_parquet(cache_p)\n",
        "            if len(cached) == len(comp_series):\n",
        "                print(f'[MM] Loaded cache: {cache_path} shape={cached.shape}')\n",
        "                return cached.reset_index(drop=True)\n",
        "        except Exception:\n",
        "            pass\n",
        "    t0 = time.time()\n",
        "    try:\n",
        "        from matminer.featurizers.composition import Stoichiometry, ValenceOrbital, IonProperty\n",
        "        from pymatgen.core.composition import Composition\n",
        "    except Exception:\n",
        "        print('[MM] Installing matminer/pymatgen...'); subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'matminer', 'pymatgen'])\n",
        "        from matminer.featurizers.composition import Stoichiometry, ValenceOrbital, IonProperty\n",
        "        from pymatgen.core.composition import Composition\n",
        "    uniq = pd.Series(comp_series.astype(str).unique())\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter('ignore')\n",
        "        comp_objs = uniq.apply(lambda s: Composition(s))\n",
        "    df_u = pd.DataFrame({'composition': uniq.values, 'comp_obj': comp_objs.values})\n",
        "    fz_list = [Stoichiometry(), ValenceOrbital(props=['avg','frac'], impute_nan=True), IonProperty(fast=True, impute_nan=True)]\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter('ignore')\n",
        "        out_u = df_u[['comp_obj']].rename(columns={'comp_obj':'composition'}).copy()\n",
        "        for fz in fz_list:\n",
        "            out_u = fz.featurize_dataframe(out_u, col_id='composition', ignore_errors=True, pbar=False)\n",
        "        feats_u = out_u.drop(columns=['composition'])\n",
        "    feats_u.columns = [f'mm2_{c}' for c in feats_u.columns]\n",
        "    map_df = pd.concat([df_u[['composition']], feats_u], axis=1)\n",
        "    all_map = pd.DataFrame({'composition': comp_series.values})\n",
        "    out = all_map.merge(map_df, on='composition', how='left').drop(columns=['composition'])\n",
        "    try:\n",
        "        out.to_parquet(cache_p, index=False)\n",
        "        print(f'[MM] Cached -> {cache_path} shape={out.shape} | uniq={len(uniq)} | {time.time()-t0:.1f}s')\n",
        "    except Exception:\n",
        "        print(f'[MM] Built (no cache write) shape={out.shape} | uniq={len(uniq)} | {time.time()-t0:.1f}s')\n",
        "    return out.reset_index(drop=True)\n",
        "\n",
        "mm_tr = build_mm_lowdim_from_comp(comp_tr['composition'], 'mm2_train.parquet')\n",
        "mm_te = build_mm_lowdim_from_comp(comp_te['composition'], 'mm2_test.parquet')\n",
        "print('[MM] Done. train/test shapes:', mm_tr.shape, mm_te.shape, '| total elapsed:', f'{time.time()-t0_all:.1f}s')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "id": "03e3c8d6-2985-44fb-81f1-5a5fdd985856",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Composition-only upgraded pipeline: drop XYZ and matminer, add mm-lite features, expanded cation contrasts, smoothed TE, LGBM-only (multi-seed) with optional monotone constraints\n",
        "import numpy as np, pandas as pd, time, gc, os, warnings\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "t0_all = time.time()\n",
        "print('Start composition-only pipeline (mm-lite, LGBM-only, no matminer/xyz)...', flush=True)\n",
        "\n",
        "# Silence noisy warnings and progress bars\n",
        "os.environ['TQDM_DISABLE'] = '1'\n",
        "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
        "warnings.filterwarnings('ignore')\n",
        "try:\n",
        "    from tqdm import auto as _tqdm_auto\n",
        "    _tqdm_auto.tqdm_disable = True\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ------------------ Load base CSVs ------------------\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "y = train['bandgap_energy_ev'].astype(float)\n",
        "\n",
        "# ------------------ Engineer base features (reuse functions) ------------------\n",
        "assert 'engineer_features' in globals(), 'Run Cell 3 to define engineer_features()'\n",
        "assert 'compute_stoich_groups' in globals(), 'Run Cell 3 to define compute_stoich_groups()'\n",
        "train_fe = engineer_features(train).copy()\n",
        "test_fe = engineer_features(test).copy()\n",
        "\n",
        "# Stoichiometric counts and group key\n",
        "g_tr, N, n_al, n_ga, n_in, n_o = compute_stoich_groups(train)\n",
        "train_fe['N'] = N; train_fe['n_al'] = n_al; train_fe['n_ga'] = n_ga; train_fe['n_in'] = n_in; train_fe['n_o'] = n_o\n",
        "g_te, N_te, al_te, ga_te, in_te, o_te = compute_stoich_groups(test)\n",
        "test_fe['N'] = N_te; test_fe['n_al'] = al_te; test_fe['n_ga'] = ga_te; test_fe['n_in'] = in_te; test_fe['n_o'] = o_te\n",
        "gkey = g_tr.astype(str)\n",
        "\n",
        "# ------------------ Composition weights, Vegard, bowing/logs, interactions, reduced lattice ------------------\n",
        "for df in (train_fe, test_fe):\n",
        "    df['frac_al'] = df['percent_atom_al']/100.0\n",
        "    df['frac_ga'] = df['percent_atom_ga']/100.0\n",
        "    df['frac_in'] = df['percent_atom_in']/100.0\n",
        "    df['percent_atom_o'] = 100.0 - (df['percent_atom_al'] + df['percent_atom_ga'] + df['percent_atom_in'])\n",
        "    df['frac_o'] = df['percent_atom_o']/100.0\n",
        "    frac_cat = (df['frac_al'] + df['frac_ga'] + df['frac_in']).replace(0, np.nan)\n",
        "    df['w_al'] = (df['frac_al']/frac_cat).fillna(0)\n",
        "    df['w_ga'] = (df['frac_ga']/frac_cat).fillna(0)\n",
        "    df['w_in'] = (df['frac_in']/frac_cat).fillna(0)\n",
        "    df['vegard_bg'] = 8.8*df['w_al'] + 4.8*df['w_ga'] + 2.9*df['w_in']\n",
        "    df['bow_in'] = df['w_in']*(1.0 - df['w_in'])\n",
        "    df['bow_ga'] = df['w_ga']*(1.0 - df['w_ga'])\n",
        "    if 'volume_per_atom' in df.columns: df['log_vpa'] = np.log1p(df['volume_per_atom'].clip(lower=0))\n",
        "    if 'atoms_per_volume' in df.columns: df['log_apv'] = np.log1p(df['atoms_per_volume'].clip(lower=0))\n",
        "    df['log_oc'] = np.log1p((df['frac_o']/(df['frac_al']+df['frac_ga']+df['frac_in']+1e-9)).clip(lower=0))\n",
        "    df['log_in_over_al'] = np.log1p(((df['frac_in']+1e-6)/(df['frac_al']+1e-6)).clip(lower=0))\n",
        "    # interactions\n",
        "    df['w_al_sq'] = df['w_al']**2; df['w_ga_sq'] = df['w_ga']**2; df['w_in_sq'] = df['w_in']**2\n",
        "    df['w_al_ga'] = df['w_al']*df['w_ga']; df['w_al_in'] = df['w_al']*df['w_in']; df['w_ga_in'] = df['w_ga']*df['w_in']\n",
        "    df['w_al_x_veg'] = df['w_al']*df['vegard_bg']; df['w_in_x_veg'] = df['w_in']*df['vegard_bg']\n",
        "    df['al_in_diff_sq'] = (df['frac_al']-df['frac_in'])**2; df['ga_in_diff_sq'] = (df['frac_ga']-df['frac_in'])**2\n",
        "    df['frac_al_cu'] = df['frac_al']**3; df['frac_ga_cu'] = df['frac_ga']**3; df['frac_in_cu'] = df['frac_in']**3\n",
        "    # reduced lattice\n",
        "    vol = df['cell_volume'].replace(0, np.nan); l = vol.pow(1/3)\n",
        "    df['a_red'] = df['lattice_vector_1_ang']/l; df['b_red'] = df['lattice_vector_2_ang']/l; df['c_red'] = df['lattice_vector_3_ang']/l\n",
        "\n",
        "# ------------------ Expanded cation-weighted contrasts ------------------\n",
        "props = {\n",
        "    'chi_pauling': {'Al':1.61,'Ga':1.81,'In':1.78,'O':3.44},\n",
        "    'ionic_radius': {'Al':0.535,'Ga':0.62,'In':0.80,'O':1.38},\n",
        "    'Z': {'Al':13,'Ga':31,'In':49,'O':8},\n",
        "    'period': {'Al':3,'Ga':4,'In':5,'O':2},\n",
        "    'group': {'Al':13,'Ga':13,'In':13,'O':16},\n",
        "    'covalent_radius': {'Al':1.21,'Ga':1.22,'In':1.42,'O':0.66},\n",
        "    'first_ionization_energy': {'Al':5.986,'Ga':5.999,'In':5.786,'O':13.618},\n",
        "    'electron_affinity': {'Al':0.441,'Ga':0.30,'In':0.30,'O':1.461}\n",
        "}\n",
        "def add_cation_weighted(df):\n",
        "    wa, wg, wi = df['w_al'], df['w_ga'], df['w_in']\n",
        "    for name, tbl in props.items():\n",
        "        ca, cg, ci, co = tbl['Al'], tbl['Ga'], tbl['In'], tbl['O']\n",
        "        wmean = wa*ca + wg*cg + wi*ci\n",
        "        df[f'catw_{name}_mean'] = wmean\n",
        "        df[f'catw_{name}_var'] = (wa*(ca-wmean)**2 + wg*(cg-wmean)**2 + wi*(ci-wmean)**2)\n",
        "    # O-minus-cation deltas for key props\n",
        "    df['o_minus_catw_chi_pauling'] = props['chi_pauling']['O'] - df['catw_chi_pauling_mean']\n",
        "    df['o_minus_catw_ionic_radius'] = props['ionic_radius']['O'] - df['catw_ionic_radius_mean']\n",
        "    return df\n",
        "train_fe = add_cation_weighted(train_fe); test_fe = add_cation_weighted(test_fe)\n",
        "\n",
        "# ------------------ mm-lite features (no matminer) ------------------\n",
        "def add_mm_lite(df):\n",
        "    # Stoichiometry norms from fracs\n",
        "    fa, fg, fi, fo = df['frac_al'], df['frac_ga'], df['frac_in'], df['frac_o']\n",
        "    arr = np.stack([fa, fg, fi, fo], axis=1)\n",
        "    df['sto_s2'] = np.sqrt((arr**2).sum(axis=1))\n",
        "    df['sto_s3'] = np.cbrt((arr**3).sum(axis=1).clip(lower=0))\n",
        "    df['sto_s5'] = (arr**5).sum(axis=1).clip(lower=0) ** (1/5)\n",
        "    df['frac_max'] = arr.max(axis=1); df['frac_min'] = arr.min(axis=1); df['frac_range'] = df['frac_max'] - df['frac_min']\n",
        "    # mix stats on cations\n",
        "    w = np.stack([df['w_al'], df['w_ga'], df['w_in']], axis=1)\n",
        "    df['w_max'] = w.max(axis=1); df['w_min'] = w.min(axis=1); df['w_range'] = df['w_max'] - df['w_min']\n",
        "    df['hhi_cation2'] = (w**2).sum(axis=1)\n",
        "    # Valence-orbital proxies (hardcoded) s/p counts\n",
        "    s_map = {'Al':2,'Ga':2,'In':2,'O':2}; p_map = {'Al':1,'Ga':1,'In':1,'O':4}\n",
        "    # cation-weighted\n",
        "    s_cat = df['w_al']*s_map['Al'] + df['w_ga']*s_map['Ga'] + df['w_in']*s_map['In']\n",
        "    p_cat = df['w_al']*p_map['Al'] + df['w_ga']*p_map['Ga'] + df['w_in']*p_map['In']\n",
        "    df['vo_cat_s_mean'] = s_cat; df['vo_cat_p_mean'] = p_cat\n",
        "    df['vo_cat_p_frac'] = p_cat / (s_cat + p_cat + 1e-9); df['vo_cat_p_minus_s'] = p_cat - s_cat\n",
        "    # total-weighted\n",
        "    s_tot = fa*s_map['Al'] + fg*s_map['Ga'] + fi*s_map['In'] + fo*s_map['O']\n",
        "    p_tot = fa*p_map['Al'] + fg*p_map['Ga'] + fi*p_map['In'] + fo*p_map['O']\n",
        "    df['vo_tot_s_mean'] = s_tot; df['vo_tot_p_mean'] = p_tot\n",
        "    df['vo_tot_p_frac'] = p_tot / (s_tot + p_tot + 1e-9); df['vo_tot_p_minus_s'] = p_tot - s_tot\n",
        "    # Oxidation consistency (Al3+, Ga3+, In3+, O2-)\n",
        "    cation_charge = 3.0*(df['n_al'] + df['n_ga'] + df['n_in'])\n",
        "    oxygen_charge = -2.0*df['n_o']\n",
        "    charge_imb = cation_charge + oxygen_charge\n",
        "    df['charge_imbalance'] = charge_imb\n",
        "    denom = (5.0*df['N']).replace(0, np.nan)\n",
        "    df['abs_imbalance_per_5N'] = np.abs(charge_imb) / denom\n",
        "    return df\n",
        "train_fe = add_mm_lite(train_fe); test_fe = add_mm_lite(test_fe)\n",
        "\n",
        "# ------------------ Spacegroup expansions ------------------\n",
        "def lattice_system_from_sgnum(sgnum):\n",
        "    n = int(sgnum)\n",
        "    if n<=2: return 1\n",
        "    if n<=15: return 2\n",
        "    if n<=74: return 3\n",
        "    if n<=142: return 4\n",
        "    if n<=167: return 5\n",
        "    if n<=194: return 6\n",
        "    return 7\n",
        "for df in (train_fe, test_fe):\n",
        "    df['sg_number'] = pd.to_numeric(df['spacegroup'], errors='coerce').fillna(-1).astype(int)\n",
        "    df['lattice_system'] = df['sg_number'].apply(lattice_system_from_sgnum).astype(int)\n",
        "\n",
        "# ------------------ Build 8-fold stratified group-disjoint folds ------------------\n",
        "y = train_fe['bandgap_energy_ev'].astype(float)\n",
        "gmean = y.groupby(gkey).mean()\n",
        "gbin = pd.qcut(gmean, q=10, labels=False, duplicates='drop')\n",
        "uniq = pd.DataFrame({'g': gmean.index, 'bin': gbin.values}).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "skf = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)\n",
        "group_to_fold = {}\n",
        "for k, (_, val_idx) in enumerate(skf.split(uniq['g'], uniq['bin'])):\n",
        "    for g in uniq['g'].iloc[val_idx]: group_to_fold[g] = k\n",
        "fold_ids = gkey.map(group_to_fold).astype(int).values\n",
        "print('Fold sizes:', pd.Series(fold_ids).value_counts().sort_index().to_dict(), flush=True)\n",
        "\n",
        "# ------------------ Target encodings (m-estimate smoothing) ------------------\n",
        "y_log = np.log1p(y.clip(lower=0))\n",
        "global_mean = float(y_log.mean())\n",
        "m_smooth = 12.0\n",
        "train_fe['te_sg'] = 0.0\n",
        "train_fe['fe_sg'] = 0.0  # frequency encoding\n",
        "for k in range(8):\n",
        "    trn_idx = np.where(fold_ids!=k)[0]; val_idx = np.where(fold_ids==k)[0]\n",
        "    df_tr = train_fe.iloc[trn_idx].copy()\n",
        "    s_tr = df_tr['spacegroup'].astype(str)\n",
        "    grp = s_tr.groupby(s_tr)\n",
        "    counts = grp.size()\n",
        "    sums = df_tr.groupby(s_tr)['bandgap_energy_ev'].apply(lambda s: np.log1p(s.clip(lower=0)).sum())\n",
        "    te = (sums + m_smooth*global_mean) / (counts + m_smooth)\n",
        "    fe = counts / counts.sum()\n",
        "    sg_val = train_fe.iloc[val_idx]['spacegroup'].astype(str)\n",
        "    train_fe.loc[train_fe.index[val_idx], 'te_sg'] = sg_val.map(te).fillna(global_mean).values\n",
        "    train_fe.loc[train_fe.index[val_idx], 'fe_sg'] = sg_val.map(fe).fillna(0.0).values\n",
        "# full-map for test\n",
        "s_all = train_fe['spacegroup'].astype(str)\n",
        "counts_all = s_all.groupby(s_all).size()\n",
        "sums_all = train_fe.groupby(s_all)['bandgap_energy_ev'].apply(lambda s: np.log1p(s.clip(lower=0)).sum())\n",
        "te_all = (sums_all + m_smooth*global_mean) / (counts_all + m_smooth)\n",
        "fe_all = counts_all / counts_all.sum()\n",
        "test_fe['te_sg'] = test_fe['spacegroup'].astype(str).map(te_all).fillna(global_mean)\n",
        "test_fe['fe_sg'] = test_fe['spacegroup'].astype(str).map(fe_all).fillna(0.0)\n",
        "\n",
        "# lattice_system frequency encoding\n",
        "for k in range(8):\n",
        "    trn_idx = np.where(fold_ids!=k)[0]; val_idx = np.where(fold_ids==k)[0]\n",
        "    ls_counts = train_fe.iloc[trn_idx]['lattice_system'].value_counts(normalize=True)\n",
        "    ls_val = train_fe.iloc[val_idx]['lattice_system']\n",
        "    train_fe.loc[train_fe.index[val_idx], 'fe_ls'] = ls_val.map(ls_counts).fillna(0.0).values\n",
        "ls_counts_all = train_fe['lattice_system'].value_counts(normalize=True)\n",
        "test_fe['fe_ls'] = test_fe['lattice_system'].map(ls_counts_all).fillna(0.0)\n",
        "\n",
        "# ------------------ Build final feature matrices (composition-only; no XYZ/matminer) ------------------\n",
        "drop_cols = ['id','bandgap_energy_ev']\n",
        "common_cols = [c for c in train_fe.columns if c in test_fe.columns]\n",
        "features = [c for c in common_cols if c not in drop_cols]\n",
        "# Ensure numeric matrix for LGBM\n",
        "train_X = train_fe[features].copy()\n",
        "test_X = test_fe[features].copy()\n",
        "med = train_X.median(numeric_only=True)\n",
        "train_X = train_X.fillna(med)\n",
        "test_X = test_X.fillna(med)\n",
        "num_cols = list(train_X.select_dtypes(include=[np.number]).columns)\n",
        "train_X = train_X[num_cols]\n",
        "test_X = test_X[num_cols]\n",
        "print('Feature matrix shapes (LGB numeric):', train_X.shape, test_X.shape, flush=True)\n",
        "\n",
        "# ------------------ LightGBM only: 1 seed x 8 folds (quick run), average ------------------\n",
        "import lightgbm as lgb\n",
        "seeds = [42]\n",
        "n_splits = 8\n",
        "oof_lgb_seeds = []; pred_lgb_seeds = []\n",
        "\n",
        "# Disable monotone constraints for robustness in quick run\n",
        "# mono_map = {'vegard_bg': +1, 'w_in': -1, 'catw_chi_pauling_mean': +1}\n",
        "# mono_list = [mono_map.get(c, 0) for c in train_X.columns]\n",
        "\n",
        "for SEED in seeds:\n",
        "    print(f'-- LGBM seed {SEED} --', flush=True); t0 = time.time()\n",
        "    params_lgb = {\n",
        "        'objective':'regression','metric':'rmse','learning_rate':0.03,\n",
        "        'num_leaves':96,'max_depth':-1,'min_data_in_leaf':450,\n",
        "        'feature_fraction':0.78,'bagging_fraction':0.8,'bagging_freq':1,\n",
        "        'lambda_l2':10.0,'lambda_l1':0.0,'verbosity':-1,'seed':SEED\n",
        "    }\n",
        "    oof_lgb = np.zeros(len(train_X)); pred_lgb = np.zeros(len(test_X))\n",
        "    for k in range(n_splits):\n",
        "        trn = np.where(fold_ids!=k)[0]; val = np.where(fold_ids==k)[0]\n",
        "        print(f'   Fold {k} trn={len(trn)} val={len(val)}', flush=True)\n",
        "        dtr = lgb.Dataset(train_X.iloc[trn], label=y_log.iloc[trn], free_raw_data=False)\n",
        "        dva = lgb.Dataset(train_X.iloc[val], label=y_log.iloc[val], free_raw_data=False)\n",
        "        m = lgb.train(params_lgb, dtr, num_boost_round=5000, valid_sets=[dtr,dva], valid_names=['train','valid'], callbacks=[lgb.early_stopping(400), lgb.log_evaluation(300)])\n",
        "        oof_lgb[val] = m.predict(train_X.iloc[val], num_iteration=m.best_iteration)\n",
        "        pred_lgb += m.predict(test_X, num_iteration=m.best_iteration)/n_splits\n",
        "        del m, dtr, dva; gc.collect()\n",
        "    rmse = float(mean_squared_error(y_log, oof_lgb) ** 0.5); print(f'LGBM seed {SEED} OOF RMSLE: {rmse:.6f} | {time.time()-t0:.1f}s', flush=True)\n",
        "    oof_lgb_seeds.append(oof_lgb); pred_lgb_seeds.append(pred_lgb)\n",
        "\n",
        "# Average across seeds\n",
        "oof_avg = np.mean(np.vstack(oof_lgb_seeds), axis=0)\n",
        "pred_avg = np.mean(np.vstack(pred_lgb_seeds), axis=0)\n",
        "cv_lgb = float(mean_squared_error(y_log, oof_avg) ** 0.5)\n",
        "print(f'Averaged LGBM CV RMSLE: {cv_lgb:.6f}', flush=True)\n",
        "\n",
        "# ------------------ Save submission ------------------\n",
        "pred_bandgap = np.expm1(pred_avg).clip(0, 6.5)\n",
        "sub = pd.DataFrame({'id': test['id'], 'bandgap_energy_ev': pred_bandgap})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv', sub.shape, '| total elapsed:', f'{time.time()-t0_all:.1f}s', flush=True)\n",
        "sub.head()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "id": "ad2b684e-c472-4a55-9d9c-0ae54ab1b158",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install CatBoost explicitly with logs, then sanity-import\n",
        "import sys, subprocess, time, os, importlib\n",
        "t0 = time.time()\n",
        "print('[SETUP] Installing CatBoost (prefer binary wheels)')\n",
        "os.environ['PIP_DISABLE_PIP_VERSION_CHECK'] = '1'\n",
        "os.environ['PIP_NO_INPUT'] = '1'\n",
        "cmd = [sys.executable, '-m', 'pip', 'install', '--prefer-binary', '--upgrade', 'catboost']\n",
        "print('[SETUP] Running:', ' '.join(cmd))\n",
        "subprocess.check_call(cmd)\n",
        "print('[SETUP] Install finished in', f'{time.time()-t0:.1f}s')\n",
        "print('[SETUP] Importing catboost to warm cache...')\n",
        "cb = importlib.import_module('catboost')\n",
        "print('[SETUP] CatBoost version:', getattr(cb, '__version__', 'unknown'))\n",
        "print('[SETUP] Ready.')"
      ],
      "execution_count": 14,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}