{
  "cells": [
    {
      "id": "d48cf1fa-a5cf-45ef-849a-b5f84739fb3f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, subprocess, json, time, pandas as pd, glob\n",
        "from pathlib import Path\n",
        "\n",
        "def run(cmd):\n",
        "    p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    return p.stdout\n",
        "\n",
        "print('=== GPU CHECK (nvidia-smi) ===', flush=True)\n",
        "print(run(['bash','-lc','nvidia-smi || true']))\n",
        "\n",
        "print('Python:', sys.version)\n",
        "print('CWD:', os.getcwd())\n",
        "\n",
        "root = Path('.')\n",
        "print('\\n=== Repo listing (top-level) ===')\n",
        "for p in sorted(root.glob('*')):\n",
        "    try:\n",
        "        mt = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(p.stat().st_mtime))\n",
        "        print(f\"{p}  [mtime {mt}]\")\n",
        "    except Exception as e:\n",
        "        print(p, e)\n",
        "\n",
        "train_csv = Path('train.csv')\n",
        "sample_sub = Path('sample_submission.csv')\n",
        "print('\\nFiles exist:', train_csv.exists(), sample_sub.exists())\n",
        "df = pd.read_csv(train_csv)\n",
        "print('train.csv shape:', df.shape)\n",
        "print('train.csv head:\\n', df.head())\n",
        "print('Columns:', list(df.columns))\n",
        "\n",
        "train_img_dir = Path('train_images')\n",
        "test_img_dir = Path('test_images')\n",
        "train_subdirs = sorted([p for p in train_img_dir.glob('*') if p.is_dir()])\n",
        "test_images = sorted([p for p in test_img_dir.glob('*.jpg')])\n",
        "print(f\"train_images dirs: {len(train_subdirs)} | example: {train_subdirs[:3]}\")\n",
        "print(f\"test_images count: {len(test_images)} | example: {test_images[:3]}\")\n",
        "\n",
        "print('\\nLabel distribution (top 10):')\n",
        "label_col = 'hotel_id' if 'hotel_id' in df.columns else df.columns[-1]\n",
        "print(df[label_col].value_counts().head(10))\n",
        "\n",
        "print('\\nBasic checks done.')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== GPU CHECK (nvidia-smi) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep 27 05:40:12 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.06             Driver Version: 550.144.06     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10-24Q                 On  |   00000002:00:00.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |     182MiB /  24512MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\nPython: 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]\nCWD: /var/lib/simon/agent_run_states/hotel-id-2021-fgvc8-20250927-050441\n\n=== Repo listing (top-level) ===\n.00_eda_and_planning_kernel_state.json  [mtime 2025-09-27 05:40:11]\n00_eda_and_planning.ipynb  [mtime 2025-09-27 05:40:01]\nagent_metadata  [mtime 2025-09-27 05:40:01]\ndescription.md  [mtime 2025-09-27 05:37:19]\ndocker_run.log  [mtime 2025-09-27 05:40:12]\nrequirements.txt  [mtime 2025-09-27 05:40:10]\nsample_submission.csv  [mtime 2025-09-27 05:36:52]\nsubmission.csv  [mtime 2025-09-27 05:38:11]\ntask.txt  [mtime 2025-09-27 05:29:40]\ntest_images  [mtime 2025-09-27 05:37:18]\ntrain.csv  [mtime 2025-09-27 05:36:52]\ntrain_images  [mtime 2025-09-27 05:36:55]\n\nFiles exist: True True\ntrain.csv shape: (87798, 4)\ntrain.csv head:\n                   image  chain  hotel_id            timestamp\n0  d29287f52c2a871f.jpg      5     22408  2018-04-16 17:01:49\n1  e9d067c249e4c2f9.jpg     70      2324  2016-07-08 22:26:21\n2  cc9877a40a63ed93.jpg      4     47514  2017-04-14 02:28:56\n3  a4963bc1d337d0c9.jpg      0     45612  2018-11-19 05:04:41\n4  c0ef87b07b40cdb2.jpg      6     16341  2017-09-27 19:43:51\nColumns: ['image', 'chain', 'hotel_id', 'timestamp']\ntrain_images dirs: 88 | example: [PosixPath('train_images/0'), PosixPath('train_images/1'), PosixPath('train_images/10')]\ntest_images count: 9756 | example: [PosixPath('test_images/80196e6999ce63cf.jpg'), PosixPath('test_images/80296afd55d516ea.jpg'), PosixPath('test_images/802aab95d62b7daa.jpg')]\n\nLabel distribution (top 10):\nhotel_id\n36363    86\n18807    84\n60181    81\n53586    80\n64314    77\n4869     76\n40933    76\n58739    76\n6831     76\n18661    76\nName: count, dtype: int64\n\nBasic checks done.\n"
          ]
        }
      ]
    },
    {
      "id": "6feda80f-9f67-4a43-ac89-aa9e72a29ff7",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan to Medal: Hotel-ID 2021 (FGVC8)\n",
        "\n",
        "Objectives:\n",
        "- Build a fast, strong baseline quickly; iterate to medal.\n",
        "- Use GPU-accelerated image embeddings and retrieval; add training if needed.\n",
        "\n",
        "Data Understanding:\n",
        "- train.csv: 87,798 rows with columns [image, chain, hotel_id, timestamp].\n",
        "- Images: train_images has 88 subdirs; test_images has 9,756 jpgs.\n",
        "- Target: hotel_id; metric: MAP@5.\n",
        "\n",
        "Validation Strategy:\n",
        "- Start with quick holdout split stratified by hotel_id to validate pipeline.\n",
        "- Upgrade to group-aware CV by hotel_id with temporal awareness: train < test-time split to mimic domain drift (avoid leakage via timestamp).\n",
        "- Cache OOF predictions and embeddings.\n",
        "\n",
        "Baseline v1 (No training, retrieval):\n",
        "- Install Torch cu121 + timm/transformers/faiss-cpu.\n",
        "- Use CLIP ViT-B/32 or ViT-L/14 pretrained encoder to extract embeddings for all train images.\n",
        "- Build FAISS IndexFlatIP over L2-normalized train embeddings.\n",
        "- For each test image, retrieve top-K nearest train images; aggregate hotel_id votes/similarities to produce top-5 hotels.\n",
        "- Expect strong MAP@5 quickly; iterate on encoder choice and resolution.\n",
        "\n",
        "Improvements v2:\n",
        "- Try multiple backbones: CLIP ViT-L/14, ConvNeXt-T, EVA02-CLIP, NFNet. Blend embeddings (weighted).\n",
        "- TTA: horizontal flip and multi-crop averaging of embeddings.\n",
        "- Fine-tune with ArcFace/CosFace on hotel_id if time permits (timm + partial freeze).\n",
        "- Use chain/time as priors or re-ranking (e.g., boost same-chain hotels).\n",
        "\n",
        "Efficiency:\n",
        "- Cache embeddings to .npy and metadata to .parquet.\n",
        "- Log progress and elapsed time per batch.\n",
        "- Subsample smoke test on 2k images before full run.\n",
        "\n",
        "Submission:\n",
        "- Create submission.csv with columns [image, hotel_id] where hotel_id is space-separated top-5 predictions.\n",
        "- Verify format against sample_submission.csv head and length.\n",
        "\n",
        "Next Steps (immediate):\n",
        "1) Confirm image path resolution for train_images (map image name -> subdir).\n",
        "2) Install Torch cu121 stack and required libs.\n",
        "3) Implement embedding extraction script and cache train/test embeddings.\n",
        "4) Implement FAISS retrieval, CV check, and produce a first submission.\n",
        "5) Request expert review after baseline OOF and before full-scale compute/ensembles."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "6b7bf420-db63-4e16-a116-e24785723d00",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, subprocess, shutil, time\n",
        "from pathlib import Path\n",
        "\n",
        "def pip(*args):\n",
        "    print('>', *args, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "# 0) Uninstall any preexisting torch stack to avoid mismatches\n",
        "for pkg in ('torch','torchvision','torchaudio'):\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', pkg], check=False)\n",
        "\n",
        "# Clean stray site dirs that can shadow correct wheels (idempotent)\n",
        "for d in (\n",
        "    '/app/.pip-target/torch',\n",
        "    '/app/.pip-target/torchvision',\n",
        "    '/app/.pip-target/torchaudio',\n",
        "    '/app/.pip-target/torch-2.8.0.dist-info',\n",
        "    '/app/.pip-target/torch-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torchvision-0.23.0.dist-info',\n",
        "    '/app/.pip-target/torchvision-0.19.1.dist-info',\n",
        "    '/app/.pip-target/torchaudio-2.8.0.dist-info',\n",
        "    '/app/.pip-target/torchaudio-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torchgen',\n",
        "    '/app/.pip-target/functorch',\n",
        "):\n",
        "    if os.path.exists(d):\n",
        "        print('Removing', d, flush=True)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "# 1) Install the EXACT cu121 torch stack FIRST\n",
        "pip('install',\n",
        "    '--index-url', 'https://download.pytorch.org/whl/cu121',\n",
        "    '--extra-index-url', 'https://pypi.org/simple',\n",
        "    'torch==2.4.1', 'torchvision==0.19.1', 'torchaudio==2.4.1')\n",
        "\n",
        "# 2) Freeze torch versions\n",
        "Path('constraints.txt').write_text('torch==2.4.1\\ntorchvision==0.19.1\\ntorchaudio==2.4.1\\n')\n",
        "\n",
        "# 3) Install non-torch deps honoring constraints\n",
        "pip('install', '-c', 'constraints.txt',\n",
        "    'timm==1.0.9',\n",
        "    'open_clip_torch==2.26.1',\n",
        "    'faiss-cpu==1.7.4',\n",
        "    'opencv-python-headless',\n",
        "    'pillow',\n",
        "    'pandas',\n",
        "    'numpy',\n",
        "    'scikit-learn',\n",
        "    'tqdm',\n",
        "    '--upgrade-strategy', 'only-if-needed')\n",
        "\n",
        "# 4) Sanity check GPU\n",
        "import torch\n",
        "print('torch:', torch.__version__, 'built CUDA:', getattr(torch.version, 'cuda', None))\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "assert str(getattr(torch.version,'cuda','')).startswith('12.1'), f'Wrong CUDA build: {torch.version.cuda}'\n",
        "assert torch.cuda.is_available(), 'CUDA not available'\n",
        "print('GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "print('Environment ready.')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torch as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torchvision as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> install --index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://pypi.org/simple torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torchaudio as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121, https://pypi.org/simple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (799.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 MB 385.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.19.1\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.1/7.1 MB 368.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.4/3.4 MB 505.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 226.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 449.8 MB/s eta 0:00:00\nCollecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 190.3 MB/s eta 0:00:00\nCollecting fsspec\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 510.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 235.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 212.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 227.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 355.2 MB/s eta 0:00:00\nCollecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 427.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing-extensions>=4.8.0\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 383.6 MB/s eta 0:00:00\nCollecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 425.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 478.7 MB/s eta 0:00:00\nCollecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 162.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 262.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 222.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 221.1 MB/s eta 0:00:00\nCollecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 251.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 258.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow!=8.3.*,>=5.3.0\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 267.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 236.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\nCollecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 512.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.2 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pillow-11.3.0 sympy-1.14.0 torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121 triton-3.0.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> install -c constraints.txt timm==1.0.9 open_clip_torch==2.26.1 faiss-cpu==1.7.4 opencv-python-headless pillow pandas numpy scikit-learn tqdm --upgrade-strategy only-if-needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm==1.0.9\n  Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.3/2.3 MB 49.2 MB/s eta 0:00:00\nCollecting open_clip_torch==2.26.1\n  Downloading open_clip_torch-2.26.1-py3-none-any.whl (1.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.5/1.5 MB 209.3 MB/s eta 0:00:00\nCollecting faiss-cpu==1.7.4\n  Downloading faiss_cpu-1.7.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 17.6/17.6 MB 107.7 MB/s eta 0:00:00\nCollecting opencv-python-headless\n  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 54.0/54.0 MB 40.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 196.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas\n  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.4/12.4 MB 256.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 193.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.7/9.7 MB 306.2 MB/s eta 0:00:00\nCollecting tqdm\n  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.5/78.5 KB 411.6 MB/s eta 0:00:00\nCollecting huggingface_hub\n  Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 563.3/563.3 KB 504.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch\n  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 797.1/797.1 MB 238.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision\n  Downloading torchvision-0.19.1-cp311-cp311-manylinux1_x86_64.whl (7.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.0/7.0 MB 295.7 MB/s eta 0:00:00\nCollecting pyyaml\n  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 806.6/806.6 KB 541.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting safetensors\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 485.8/485.8 KB 526.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting regex\n  Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 KB 493.1 MB/s eta 0:00:00\nCollecting ftfy\n  Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.8/44.8 KB 305.5 MB/s eta 0:00:00\nCollecting opencv-python-headless\n  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 50.0/50.0 MB 390.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tzdata>=2022.7\n  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 347.8/347.8 KB 489.4 MB/s eta 0:00:00\nCollecting pytz>=2020.1\n  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 509.2/509.2 KB 524.6 MB/s eta 0:00:00\nCollecting python-dateutil>=2.8.2\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 229.9/229.9 KB 62.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy>=1.8.0\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.9/35.9 MB 270.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting threadpoolctl>=3.1.0\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nCollecting joblib>=1.2.0\n  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 308.4/308.4 KB 510.3 MB/s eta 0:00:00\nCollecting six>=1.5\n  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 205.2 MB/s eta 0:00:00\nCollecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 96.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 393.3 MB/s eta 0:00:00\nCollecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 224.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 481.0 MB/s eta 0:00:00\nCollecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 294.2 MB/s eta 0:00:00\nCollecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 547.1 MB/s eta 0:00:00\nCollecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 272.6 MB/s eta 0:00:00\nCollecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 277.3 MB/s eta 0:00:00\nCollecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 241.4 MB/s eta 0:00:00\nCollecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 200.4 MB/s eta 0:00:00\nCollecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 60.0 MB/s eta 0:00:00\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 233.3 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 231.3 MB/s eta 0:00:00\nCollecting fsspec\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 516.6 MB/s eta 0:00:00\nCollecting typing-extensions>=4.8.0\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 334.4 MB/s eta 0:00:00\nCollecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 491.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 267.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wcwidth\n  Downloading wcwidth-0.2.14-py2.py3-none-any.whl (37 kB)\nCollecting hf-xet<2.0.0,>=1.1.3\n  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.2/3.2 MB 566.6 MB/s eta 0:00:00\nCollecting packaging>=20.9\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 428.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting requests\n  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.7/64.7 KB 409.4 MB/s eta 0:00:00\nCollecting MarkupSafe>=2.0\n  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting idna<4,>=2.5\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 70.4/70.4 KB 421.6 MB/s eta 0:00:00\nCollecting urllib3<3,>=1.21.1\n  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 129.8/129.8 KB 508.4 MB/s eta 0:00:00\nCollecting certifi>=2017.4.17\n  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 161.2/161.2 KB 468.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting charset_normalizer<4,>=2\n  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 150.3/150.3 KB 507.3 MB/s eta 0:00:00\nCollecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 547.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: pytz, mpmath, faiss-cpu, wcwidth, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, sympy, six, safetensors, regex, pyyaml, pillow, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, joblib, idna, hf-xet, fsspec, filelock, charset_normalizer, certifi, triton, scipy, requests, python-dateutil, opencv-python-headless, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, ftfy, scikit-learn, pandas, nvidia-cusolver-cu12, huggingface_hub, torch, torchvision, timm, open_clip_torch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.2 certifi-2025.8.3 charset_normalizer-3.4.3 faiss-cpu-1.7.4 filelock-3.19.1 fsspec-2025.9.0 ftfy-6.3.1 hf-xet-1.1.10 huggingface_hub-0.35.1 idna-3.10 jinja2-3.1.6 joblib-1.5.2 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 open_clip_torch-2.26.1 opencv-python-headless-4.11.0.86 packaging-25.0 pandas-2.3.2 pillow-11.3.0 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.3 regex-2025.9.18 requests-2.32.5 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 six-1.17.0 sympy-1.14.0 threadpoolctl-3.6.0 timm-1.0.9 torch-2.4.1 torchvision-0.19.1 tqdm-4.67.1 triton-3.0.0 typing-extensions-4.15.0 tzdata-2025.2 urllib3-2.5.0 wcwidth-0.2.14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/torchvision already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchvision.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/functorch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchgen already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusolver_cu12-11.4.5.107.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/jinja2-3.1.6.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cudnn_cu12-9.1.0.70.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusparse_cu12-12.1.0.106.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/triton-3.0.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock-3.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec-2025.9.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/MarkupSafe-3.0.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx-3.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cublas_cu12-12.1.3.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_cupti_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_nvrtc_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_runtime_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cufft_cu12-11.0.2.54.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_curand_cu12-10.3.2.106.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nccl_cu12-2.20.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvjitlink_cu12-12.9.86.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvtx_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow-11.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/PIL already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy-1.14.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/isympy.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath-1.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/share already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.4.1+cu121 built CUDA: 12.1\nCUDA available: True\nGPU: NVIDIA A10-24Q\nEnvironment ready.\n"
          ]
        }
      ]
    },
    {
      "id": "44dfb02f-6cf4-47e4-a640-2d14db6084e7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import os, time\n",
        "\n",
        "print('=== Build path mapping (train_images/{chain}/{image}) ===', flush=True)\n",
        "train_csv = Path('train.csv')\n",
        "df = pd.read_csv(train_csv)\n",
        "df['chain'] = df['chain'].astype(str)\n",
        "df['path'] = 'train_images/' + df['chain'] + '/' + df['image']\n",
        "\n",
        "exists = df['path'].map(lambda p: Path(p).exists())\n",
        "missing = (~exists).sum()\n",
        "print(f'Total rows: {len(df)} | Missing files: {missing}')\n",
        "if missing > 0:\n",
        "    print('Example missing paths:', df.loc[~exists, 'path'].head(5).tolist())\n",
        "\n",
        "# Save metadata cache\n",
        "df[['image','chain','hotel_id','timestamp','path']].to_parquet('train_meta.parquet', index=False)\n",
        "print('Saved train_meta.parquet')\n",
        "\n",
        "print('\\n=== Build test listing ===')\n",
        "test_dir = Path('test_images')\n",
        "test_files = sorted([p.name for p in test_dir.glob('*.jpg')])\n",
        "test_df = pd.DataFrame({'image': test_files})\n",
        "test_df['path'] = test_df['image'].map(lambda x: str(test_dir / x))\n",
        "print('Test count:', len(test_df), '| head:', test_df.head().to_dict('records'))\n",
        "test_df.to_parquet('test_meta.parquet', index=False)\n",
        "print('Saved test_meta.parquet')\n",
        "\n",
        "print('Path mapping complete.')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Build path mapping (train_images/{chain}/{image}) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows: 87798 | Missing files: 0\nSaved train_meta.parquet\n\n=== Build test listing ===\nTest count: 9756 | head: [{'image': '80196e6999ce63cf.jpg', 'path': 'test_images/80196e6999ce63cf.jpg'}, {'image': '80296afd55d516ea.jpg', 'path': 'test_images/80296afd55d516ea.jpg'}, {'image': '802aab95d62b7daa.jpg', 'path': 'test_images/802aab95d62b7daa.jpg'}, {'image': '802af4d04faf14df.jpg', 'path': 'test_images/802af4d04faf14df.jpg'}, {'image': '802b5ed622fd3587.jpg', 'path': 'test_images/802b5ed622fd3587.jpg'}]\nSaved test_meta.parquet\nPath mapping complete.\n"
          ]
        }
      ]
    },
    {
      "id": "6c55849d-68f7-4157-bb1e-2a137bc6e0a3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, time, math, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageOps\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import open_clip\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def load_model():\n",
        "    print('Loading OpenCLIP ViT-L/14-336 (openai)...', flush=True)\n",
        "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "        'ViT-L-14-336', pretrained='openai'\n",
        "    )\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    return model, preprocess\n",
        "\n",
        "@torch.no_grad()\n",
        "def embed_paths(paths, model, preprocess, batch_size=40, tta_hflip=False):\n",
        "    embs = []\n",
        "    n = len(paths)\n",
        "    t0 = time.time()\n",
        "    for i in range(0, n, batch_size):\n",
        "        j = min(i + batch_size, n)\n",
        "        batch_paths = paths[i:j]\n",
        "        ims = []\n",
        "        ims_flip = []\n",
        "        for p in batch_paths:\n",
        "            img = Image.open(p).convert('RGB')\n",
        "            ims.append(preprocess(img))\n",
        "            if tta_hflip:\n",
        "                ims_flip.append(preprocess(ImageOps.mirror(img)))\n",
        "        x = torch.stack(ims).to(device, non_blocking=True)\n",
        "        with torch.amp.autocast('cuda', enabled=True):\n",
        "            feat = model.encode_image(x)\n",
        "        feat = F.normalize(feat.float(), dim=1)\n",
        "        if tta_hflip:\n",
        "            xf = torch.stack(ims_flip).to(device, non_blocking=True)\n",
        "            with torch.amp.autocast('cuda', enabled=True):\n",
        "                feat_f = model.encode_image(xf)\n",
        "            feat_f = F.normalize(feat_f.float(), dim=1)\n",
        "            feat = F.normalize((feat + feat_f) / 2, dim=1)\n",
        "        embs.append(feat.cpu())\n",
        "        if ((i // batch_size) % 20) == 0:\n",
        "            elapsed = time.time() - t0\n",
        "            print(f'  Embedded {j}/{n} images | elapsed {elapsed:.1f}s', flush=True)\n",
        "        del x\n",
        "        if tta_hflip:\n",
        "            del xf\n",
        "        torch.cuda.synchronize()\n",
        "    embs = torch.cat(embs, dim=0).numpy().astype('float32')\n",
        "    return embs\n",
        "\n",
        "def run_embedding_pipeline(limit_train=None, limit_test=None, out_prefix='openclip_vitl14_336_openai_noTTA'):\n",
        "    model, preprocess = load_model()\n",
        "    tr = pd.read_parquet('train_meta.parquet')\n",
        "    te = pd.read_parquet('test_meta.parquet')\n",
        "    if limit_train is not None:\n",
        "        tr = tr.sample(n=min(limit_train, len(tr)), random_state=42).reset_index(drop=True)\n",
        "    if limit_test is not None:\n",
        "        te = te.sample(n=min(limit_test, len(te)), random_state=42).reset_index(drop=True)\n",
        "    print(f'Train images: {len(tr)} | Test images: {len(te)}')\n",
        "\n",
        "    # Train embeddings\n",
        "    t0 = time.time()\n",
        "    tr_embs = embed_paths(tr['path'].tolist(), model, preprocess, batch_size=40, tta_hflip=False)\n",
        "    np.save(f'{out_prefix}_train.npy', tr_embs)\n",
        "    tr[['image','chain','hotel_id','path']].to_parquet(f'{out_prefix}_train_meta.parquet', index=False)\n",
        "    print(f'Saved train embs: {tr_embs.shape} in {time.time()-t0:.1f}s')\n",
        "    del tr_embs; gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    # Test embeddings\n",
        "    t1 = time.time()\n",
        "    te_embs = embed_paths(te['path'].tolist(), model, preprocess, batch_size=40, tta_hflip=False)\n",
        "    np.save(f'{out_prefix}_test.npy', te_embs)\n",
        "    te[['image','path']].to_parquet(f'{out_prefix}_test_meta.parquet', index=False)\n",
        "    print(f'Saved test embs: {te_embs.shape} in {time.time()-t1:.1f}s')\n",
        "\n",
        "    print('All embeddings saved with prefix:', out_prefix)\n",
        "\n",
        "# Control auto-run in this cell via SMOKE flag\n",
        "SMOKE = False\n",
        "if SMOKE:\n",
        "    run_embedding_pipeline(limit_train=2000, limit_test=1000, out_prefix='openclip_vitl14_336_openai_noTTA_smoke')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "id": "61dbee65-a3e6-407b-86cd-e1db431b8381",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import faiss\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "def build_faiss_ip_index(embs: np.ndarray):\n",
        "    assert embs.ndim == 2\n",
        "    try:\n",
        "        faiss.omp_set_num_threads(min(16, os.cpu_count() or 16))\n",
        "    except Exception:\n",
        "        pass\n",
        "    index = faiss.IndexFlatIP(embs.shape[1])\n",
        "    index.add(embs.astype('float32'))\n",
        "    return index\n",
        "\n",
        "def aggregate_sum_per_hotel(train_meta: pd.DataFrame, neighbor_idxs: np.ndarray, neighbor_sims: np.ndarray, topk: int = 500):\n",
        "    scores = defaultdict(float)\n",
        "    k = min(topk, len(neighbor_idxs))\n",
        "    for ii, sim in zip(neighbor_idxs[:k], neighbor_sims[:k]):\n",
        "        hid = int(train_meta.iloc[ii].hotel_id)\n",
        "        s = max(float(sim), 0.0) ** 1.3\n",
        "        scores[hid] += s\n",
        "    return scores\n",
        "\n",
        "def per_hotel_centroid_scores(q_vec: np.ndarray, train_meta: pd.DataFrame, train_embs: np.ndarray, neighbor_idxs: np.ndarray, topk: int = 500, cap_per_hotel: int = 60):\n",
        "    # Build centroids from top-K neighbors per candidate hotel\n",
        "    k = min(topk, len(neighbor_idxs))\n",
        "    by_hotel = defaultdict(list)\n",
        "    for ii in neighbor_idxs[:k]:\n",
        "        hid = int(train_meta.iloc[ii].hotel_id)\n",
        "        if len(by_hotel[hid]) < cap_per_hotel:\n",
        "            by_hotel[hid].append(train_embs[ii])\n",
        "    c_scores = {}\n",
        "    for hid, vecs in by_hotel.items():\n",
        "        V = np.stack(vecs).astype('float32')\n",
        "        V /= (np.linalg.norm(V, axis=1, keepdims=True) + 1e-8)\n",
        "        c = V.mean(axis=0)\n",
        "        c /= (np.linalg.norm(c) + 1e-8)\n",
        "        c_scores[hid] = float(np.dot(q_vec, c))\n",
        "    return c_scores\n",
        "\n",
        "def top5_from_scores(score_dict: dict):\n",
        "    items = sorted(score_dict.items(), key=lambda x: -x[1])\n",
        "    top = []\n",
        "    seen = set()\n",
        "    for hid, _ in items:\n",
        "        s = str(hid)\n",
        "        if s not in seen:\n",
        "            top.append(s); seen.add(s)\n",
        "        if len(top) == 5:\n",
        "            break\n",
        "    if not top:\n",
        "        top = ['0']\n",
        "    while len(top) < 5:\n",
        "        top.append(top[-1])\n",
        "    return ' '.join(top[:5])\n",
        "\n",
        "def retrieve_and_submit(prefix: str, out_csv: str, K: int = 500, qe_m: int = 10, centroid_blend: float = 0.20):\n",
        "    # Load embeddings and meta\n",
        "    tr_emb = np.load(f'{prefix}_train.npy')\n",
        "    te_emb = np.load(f'{prefix}_test.npy')\n",
        "    tr_meta = pd.read_parquet(f'{prefix}_train_meta.parquet')\n",
        "    te_meta = pd.read_parquet(f'{prefix}_test_meta.parquet')\n",
        "    # Normalize\n",
        "    tr_norm = tr_emb / (np.linalg.norm(tr_emb, axis=1, keepdims=True) + 1e-8)\n",
        "    te_norm = te_emb / (np.linalg.norm(te_emb, axis=1, keepdims=True) + 1e-8)\n",
        "    # Build FAISS\n",
        "    print('Building FAISS IndexFlatIP with', tr_norm.shape, flush=True)\n",
        "    index = build_faiss_ip_index(tr_norm.astype('float32'))\n",
        "    K = min(K, tr_norm.shape[0])\n",
        "    print('Searching...', flush=True)\n",
        "    sims, idxs = index.search(te_norm.astype('float32'), K)\n",
        "    # Aggregate per hotel with optional QE and centroid re-score\n",
        "    preds = []\n",
        "    for i in range(te_norm.shape[0]):\n",
        "        q = te_norm[i]\n",
        "        inds = idxs[i]\n",
        "        s = sims[i]\n",
        "        # Query expansion\n",
        "        m = min(qe_m, len(inds))\n",
        "        if m > 0:\n",
        "            nn = tr_norm[inds[:m]]\n",
        "            q2 = q + nn.mean(axis=0)\n",
        "            q2 /= (np.linalg.norm(q2) + 1e-8)\n",
        "        else:\n",
        "            q2 = q\n",
        "        # Sum of positives per hotel (ReLU^1.3)\n",
        "        sum_scores = aggregate_sum_per_hotel(tr_meta, inds, s, topk=K)\n",
        "        # Centroid per hotel from top-K and blend (cap=60)\n",
        "        cent_scores = per_hotel_centroid_scores(q2, tr_meta, tr_norm, inds, topk=K, cap_per_hotel=60)\n",
        "        # Merge\n",
        "        final_scores = defaultdict(float)\n",
        "        for hid, v in sum_scores.items():\n",
        "            final_scores[hid] += (1.0 - centroid_blend) * v\n",
        "        for hid, v in cent_scores.items():\n",
        "            final_scores[hid] += centroid_blend * max(v, 0.0)\n",
        "        preds.append(top5_from_scores(final_scores))\n",
        "        if (i+1) % 200 == 0:\n",
        "            print(f'  Scored {i+1}/{te_norm.shape[0]} queries', flush=True)\n",
        "    # Align to required submission order\n",
        "    sample = pd.read_csv('sample_submission.csv')\n",
        "    sub = pd.DataFrame({'image': te_meta['image'].values, 'hotel_id': preds})\n",
        "    sub = sample[['image']].merge(sub, on='image', how='left')\n",
        "    # Fallback if needed\n",
        "    if sub['hotel_id'].isna().any():\n",
        "        freq = tr_meta['hotel_id'].value_counts().index.astype(str).tolist()[:5]\n",
        "        fallback = ' '.join(freq + [freq[-1]]*(5-len(freq)))\n",
        "        sub['hotel_id'] = sub['hotel_id'].fillna(fallback)\n",
        "    assert sub['hotel_id'].str.split().map(len).eq(5).all(), 'Each row must have 5 predictions'\n",
        "    sub.to_csv(out_csv, index=False)\n",
        "    print('Saved submission to', out_csv)\n",
        "\n",
        "print('Retrieval functions ready. Use retrieve_and_submit with prefix from cell 4 output.')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval functions ready. Use retrieve_and_submit with prefix from cell 4 output.\n"
          ]
        }
      ]
    },
    {
      "id": "fbb02b3e-b195-407c-8601-797e5c9fe262",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Full extraction (ViT-L/14-336 openai, no TTA) + retrieval to submission.csv\n",
        "import time, os\n",
        "t0 = time.time()\n",
        "print('=== Starting FULL embedding extraction (ViT-L/14-336 openai, no TTA) ===', flush=True)\n",
        "FULL_PREFIX = 'openclip_vitl14_336_openai_noTTA'\n",
        "try:\n",
        "    # Extract embeddings for all train/test\n",
        "    run_embedding_pipeline(limit_train=None, limit_test=None, out_prefix=FULL_PREFIX)\n",
        "    print(f'Embedding extraction done in {time.time()-t0:.1f}s, starting retrieval...', flush=True)\n",
        "    # Retrieval with similarity-sum + QE + centroid re-score\n",
        "    retrieve_and_submit(prefix=FULL_PREFIX, out_csv='submission.csv', K=200, qe_m=5, centroid_blend=0.3)\n",
        "    print('submission.csv created. Verify head and length match sample_submission.csv')\n",
        "except NameError as e:\n",
        "    print('Functions not found in current kernel. Please re-run cells 4 and 5 first.', e)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Starting FULL embedding extraction (ViT-L/14-336 openai, no TTA) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading OpenCLIP ViT-L/14-336 (openai)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|                                               | 0.00/934M [00:00<?, ?iB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|                                       | 164k/934M [00:00<10:10, 1.53MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|                                       | 360k/934M [00:00<09:12, 1.69MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|                                       | 557k/934M [00:00<09:08, 1.70MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|                                      | 1.13M/934M [00:00<04:55, 3.16MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|\u258e                                     | 8.55M/934M [00:00<00:33, 27.5MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|\u258b                                     | 17.0M/934M [00:00<00:19, 46.2MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|\u2589                                     | 21.8M/934M [00:00<00:29, 30.9MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|\u2588                                     | 25.6M/934M [00:01<00:37, 24.2MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|\u2588\u258e                                    | 32.9M/934M [00:01<00:26, 33.8MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|\u2588\u258b                                    | 42.0M/934M [00:01<00:19, 46.0MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|\u2588\u2589                                    | 47.7M/934M [00:01<00:18, 47.7MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|\u2588\u2588\u258e                                   | 56.9M/934M [00:01<00:14, 58.8MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|\u2588\u2588\u258c                                   | 63.7M/934M [00:01<00:14, 59.5MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|\u2588\u2588\u2589                                   | 71.6M/934M [00:01<00:13, 64.8MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|\u2588\u2588\u2588\u258f                                  | 78.5M/934M [00:02<00:27, 30.9MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|\u2588\u2588\u2588\u258d                                  | 83.9M/934M [00:02<00:26, 31.5MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|\u2588\u2588\u2588\u258c                                  | 88.6M/934M [00:02<00:25, 32.6MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|\u2588\u2588\u2588\u2588\u258f                                  | 100M/934M [00:02<00:17, 48.0MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|\u2588\u2588\u2588\u2588\u258d                                  | 107M/934M [00:02<00:15, 52.6MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|\u2588\u2588\u2588\u2588\u258b                                  | 114M/934M [00:02<00:14, 54.9MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|\u2588\u2588\u2588\u2588\u2588                                  | 120M/934M [00:03<00:18, 43.4MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|\u2588\u2588\u2588\u2588\u2588\u258d                                 | 129M/934M [00:03<00:15, 52.4MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|\u2588\u2588\u2588\u2588\u2588\u258b                                 | 136M/934M [00:03<00:13, 57.2MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                | 148M/934M [00:03<00:11, 71.3MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                | 156M/934M [00:04<00:33, 23.3MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                | 162M/934M [00:04<00:29, 25.7MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                               | 173M/934M [00:04<00:21, 35.5MiB/s]"
          ]
        }
      ]
    },
    {
      "id": "b7e90a57-4831-4919-9ef8-b4636e9555c1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install FAISS-CPU (fallback) and sanity check\n",
        "import sys, subprocess\n",
        "\n",
        "def pip(*args):\n",
        "    print('>', *args, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "print('Installing faiss-cpu (GPU wheels unavailable in this environment)...')\n",
        "pip('install', 'faiss-cpu==1.7.4')\n",
        "\n",
        "import faiss, numpy as np\n",
        "print('FAISS version:', faiss.__version__)\n",
        "d = 64\n",
        "index = faiss.IndexFlatIP(d)\n",
        "xb = np.random.randn(1000, d).astype('float32')\n",
        "xb /= np.linalg.norm(xb, axis=1, keepdims=True) + 1e-8\n",
        "index.add(xb)\n",
        "xq = np.random.randn(10, d).astype('float32')\n",
        "xq /= np.linalg.norm(xq, axis=1, keepdims=True) + 1e-8\n",
        "D, I = index.search(xq, 5)\n",
        "print('FAISS-CPU sanity OK. Top5 shape:', I.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing faiss-cpu (GPU wheels unavailable in this environment)...\n> install faiss-cpu==1.7.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu==1.7.4\n  Downloading faiss_cpu-1.7.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 17.6/17.6 MB 167.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: faiss-cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed faiss-cpu-1.7.4\nFAISS version: 1.7.4\nFAISS-CPU sanity OK. Top5 shape: (10, 5)\n"
          ]
        }
      ]
    },
    {
      "id": "1e74e1c5-5099-4613-a43f-9530fcd2b984",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Temporal per-hotel CV (holdout newest 20%) using precomputed train embeddings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "def parse_ts(s):\n",
        "    return pd.to_datetime(s)\n",
        "\n",
        "def build_temporal_holdout(df: pd.DataFrame, val_frac: float = 0.2, min_keep_train: int = 3):\n",
        "    df = df.copy()\n",
        "    df['ts'] = parse_ts(df['timestamp'])\n",
        "    tr_idx, val_idx = [], []\n",
        "    for hid, g in df.groupby('hotel_id'):\n",
        "        g = g.sort_values('ts')\n",
        "        n = len(g)\n",
        "        if n < min_keep_train:\n",
        "            tr_idx.extend(g.index.tolist())\n",
        "            continue\n",
        "        cut = max(1, int(round(n * (1 - val_frac))))\n",
        "        tr_idx.extend(g.index[:cut].tolist())\n",
        "        val_idx.extend(g.index[cut:].tolist())\n",
        "    return np.array(tr_idx, dtype=int), np.array(val_idx, dtype=int)\n",
        "\n",
        "def l2norm(x):\n",
        "    return x / (np.linalg.norm(x, axis=1, keepdims=True) + 1e-8)\n",
        "\n",
        "def map_at_5(truth: np.ndarray, preds: list):\n",
        "    score = 0.0\n",
        "    for t, p in zip(truth, preds):\n",
        "        parts = p.split()[:5]\n",
        "        for k, h in enumerate(parts, 1):\n",
        "            if str(t) == h:\n",
        "                score += 1.0 / k\n",
        "                break\n",
        "    return score / len(truth)\n",
        "\n",
        "def cv_evaluate(prefix: str, K: int = 200, qe_m: int = 5, centroid_blend: float = 0.3, val_frac: float = 0.2):\n",
        "    # Load full train embeddings and meta\n",
        "    tr_emb = np.load(f'{prefix}_train.npy')\n",
        "    meta = pd.read_parquet(f'{prefix}_train_meta.parquet')\n",
        "    tr_emb = tr_emb.astype('float32')\n",
        "    tr_emb = l2norm(tr_emb)\n",
        "    tr_idx_all, val_idx_all = build_temporal_holdout(meta, val_frac=val_frac, min_keep_train=3)\n",
        "    print(f'Train split size: {len(tr_idx_all)} | Val split size: {len(val_idx_all)}', flush=True)\n",
        "    # Build FAISS on train-only\n",
        "    dim = tr_emb.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(tr_emb[tr_idx_all])\n",
        "    # Query = val embeddings\n",
        "    q = tr_emb[val_idx_all]\n",
        "    Kq = min(K, len(tr_idx_all))\n",
        "    print('Searching val queries...', flush=True)\n",
        "    sims, idxs = index.search(q, Kq)\n",
        "    # Map back to global train indices\n",
        "    neigh_global = tr_idx_all[idxs]\n",
        "    # Remove self-matches (if any same image in gallery)\n",
        "    preds = []\n",
        "    y_true = meta.iloc[val_idx_all]['hotel_id'].values\n",
        "    # Reuse retrieval helpers from cell 5\n",
        "    for i in range(q.shape[0]):\n",
        "        inds = neigh_global[i]\n",
        "        sims_i = sims[i]\n",
        "        # Filter exact self index if present\n",
        "        mask = inds != val_idx_all[i]\n",
        "        inds = inds[mask]\n",
        "        sims_f = sims_i[mask]\n",
        "        # QE\n",
        "        m = min(qe_m, len(inds))\n",
        "        if m > 0:\n",
        "            nn = tr_emb[inds[:m]]\n",
        "            q2 = q[i] + nn.mean(axis=0)\n",
        "            q2 /= (np.linalg.norm(q2) + 1e-8)\n",
        "        else:\n",
        "            q2 = q[i]\n",
        "        # Sum positives\n",
        "        sum_scores = aggregate_sum_per_hotel(meta, inds, sims_f, topk=Kq)\n",
        "        # Centroid blend\n",
        "        cent_scores = per_hotel_centroid_scores(q2, meta, tr_emb, inds, topk=Kq, cap_per_hotel=50)\n",
        "        final_scores = defaultdict(float)\n",
        "        for hid, v in sum_scores.items():\n",
        "            final_scores[hid] += (1.0 - centroid_blend) * v\n",
        "        for hid, v in cent_scores.items():\n",
        "            final_scores[hid] += centroid_blend * max(v, 0.0)\n",
        "        preds.append(top5_from_scores(final_scores))\n",
        "        if (i+1) % 500 == 0:\n",
        "            print(f'  Val scored {i+1}/{q.shape[0]}', flush=True)\n",
        "    m5 = map_at_5(y_true, preds)\n",
        "    print(f'CV MAP@5 (val newest {int(val_frac*100)}%): {m5:.5f}')\n",
        "    return m5\n",
        "\n",
        "print('Temporal CV utilities ready. After embeddings complete, run cv_evaluate(\"openclip_vitl14_336_openai_noTTA\") to validate and tune K/centroid_blend.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "aa4bdcdf-48f7-485e-a949-a7af69d0535d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Regenerate submission with tuned retrieval params after embeddings finish\n",
        "print('Rebuilding submission with K=500, QE m=10, centroid_blend=0.20, cap_per_hotel=60, ReLU^1.3', flush=True)\n",
        "prefix = FULL_PREFIX if 'FULL_PREFIX' in globals() else 'openclip_vitl14_336_openai_noTTA'\n",
        "retrieve_and_submit(prefix=prefix, out_csv='submission.csv', K=500, qe_m=10, centroid_blend=0.20)\n",
        "import pandas as pd, os, time\n",
        "sub = pd.read_csv('submission.csv')\n",
        "print('submission.csv shape:', sub.shape, '| head:\\n', sub.head())\n",
        "print('mtime:', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(os.path.getmtime('submission.csv'))), flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}