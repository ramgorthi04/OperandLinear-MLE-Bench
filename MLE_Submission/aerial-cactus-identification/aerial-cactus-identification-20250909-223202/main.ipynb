{
  "cells": [
    {
      "id": "36899a3c-5344-40c5-a045-81334f3fee7d",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aerial Cactus Identification \u2014 Medal Plan\n",
        "\n",
        "## Goal\n",
        "- Achieve 1.0000 AUC (medal) with robust CV and inference.\n",
        "\n",
        "## Data\n",
        "- Provided: train.csv (id, has_cactus), train.zip (images), test.zip (images), sample_submission.csv.\n",
        "- Images are small (32x32).\n",
        "\n",
        "## Approach\n",
        "- Stratified K-Fold (5 folds) on train.csv.\n",
        "- Pretrained CNN using timm (efficientnet_b0 or tf_efficientnet_b0_ns) scaled to 96x96 for better feature extraction.\n",
        "- Use strong but safe augmentations (horizontal/vertical flips, slight shift/scale/rotate, color jitter).\n",
        "- Mixed precision training, early stopping on fold AUC, ReduceLROnPlateau.\n",
        "- Loss: BCEWithLogitsLoss; Metric: ROC-AUC.\n",
        "- Optimizer: AdamW; Scheduler: CosineAnnealingWarmRestarts or OneCycleLR (will pick empirically; likely Cosine).\n",
        "\n",
        "## Training\n",
        "- Unzip datasets to ./train and ./test on first run; cache file lists.\n",
        "- Dataloader: balanced batches if needed; otherwise rely on loss/aug.\n",
        "- Epochs: 10\u201315 per fold (early stop), batch size as GPU allows (e.g., 256 at 96x96 on T4; will probe).\n",
        "- Log per-epoch metrics and timing; print fold indices and elapsed time.\n",
        "\n",
        "## Evaluation\n",
        "- Out-of-fold (OOF) predictions and AUC per fold; average AUC for CV estimate.\n",
        "\n",
        "## Inference\n",
        "- TTA (e.g., 4\u20138 flips/rotations) on test set; average across folds and TTA.\n",
        "- Save submission.csv with columns: id, has_cactus.\n",
        "\n",
        "## Efficiency\n",
        "- Keep model small (EffNet-B0) for fast convergence; consider tf_efficientnet_b3 if time permits.\n",
        "- Use pin_memory, num_workers tuned to CPU.\n",
        "\n",
        "## Checkpoints\n",
        "- After this plan, request expert review.\n",
        "- After initial EDA/load and baseline training, request expert review if CV < 0.9995.\n",
        "\n",
        "## Risk Mitigation\n",
        "- If deep model is slow/overkill, fallback: LightGBM on simple image stats (HSV histograms, edges) to get strong AUC quickly; but primary path should reach 1.0.\n",
        "\n",
        "## Next Steps\n",
        "1) Environment setup; unzip data; quick sanity EDA (counts, class balance, sample images).\n",
        "2) Build dataset/transforms and model; run a quick single-fold smoke test (2\u20133 epochs) to validate pipeline.\n",
        "3) Full 5-fold with early stopping and TTA inference.\n",
        "4) Generate and save submission.csv."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "e95aa945-b9a8-42d6-b14d-34e5662e20e7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup: imports, unzip data, basic sanity checks\n",
        "import os, sys, time, zipfile, glob, random, math, json, gc\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "DATA_DIR = Path('.')\n",
        "TRAIN_ZIP = DATA_DIR/'train.zip'\n",
        "TEST_ZIP = DATA_DIR/'test.zip'\n",
        "TRAIN_DIR = DATA_DIR/'train'\n",
        "TEST_DIR = DATA_DIR/'test'\n",
        "TRAIN_CSV = DATA_DIR/'train.csv'\n",
        "SAMPLE_SUB = DATA_DIR/'sample_submission.csv'\n",
        "\n",
        "def unzip_if_needed(zip_path: Path, out_dir: Path):\n",
        "    if out_dir.exists() and any(out_dir.iterdir()):\n",
        "        print(f\"[unzip] Exists and not empty: {out_dir}\")\n",
        "        return\n",
        "    assert zip_path.exists(), f\"Missing zip: {zip_path}\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    t0 = time.time()\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        z.extractall(out_dir)\n",
        "    print(f\"[unzip] Extracted {zip_path.name} -> {out_dir} in {time.time()-t0:.2f}s\")\n",
        "\n",
        "print(\"[env] Python:\", sys.version)\n",
        "print(\"[env] CWD:\", os.getcwd())\n",
        "print(\"[files]\", os.listdir())\n",
        "\n",
        "# Unzip datasets\n",
        "unzip_if_needed(TRAIN_ZIP, TRAIN_DIR)\n",
        "unzip_if_needed(TEST_ZIP, TEST_DIR)\n",
        "\n",
        "# Load train.csv\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "print(\"[train.csv] shape:\", train_df.shape)\n",
        "print(train_df['has_cactus'].value_counts(normalize=True))\n",
        "\n",
        "# Verify image files count and mapping\n",
        "train_files = sorted(glob.glob(str(TRAIN_DIR/'*.jpg'))) + sorted(glob.glob(str(TRAIN_DIR/'*.png')))\n",
        "test_files = sorted(glob.glob(str(TEST_DIR/'*.jpg'))) + sorted(glob.glob(str(TEST_DIR/'*.png')))\n",
        "print(f\"[files] train images: {len(train_files)}, test images: {len(test_files)}\")\n",
        "\n",
        "# Check that all train ids exist\n",
        "missing = []\n",
        "for img_id in train_df['id'].values[:1000]:  # spot-check first 1000\n",
        "    p = TRAIN_DIR/img_id\n",
        "    if not p.exists():\n",
        "        missing.append(img_id)\n",
        "print(f\"[sanity] Missing (first 1000 check): {len(missing)}\")\n",
        "print(\"[sanity] sample rows:\\n\", train_df.head())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[env] Python: 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]\n[env] CWD: /app/agent_run_states/aerial-cactus-identification-20250909-223202\n[files] ['description.md', 'test.zip', 'agent_metadata', 'task.txt', 'train.csv', 'requirements.txt', 'train.zip', 'sample_submission.csv', 'docker_run.log', 'main.ipynb']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[unzip] Extracted train.zip -> train in 1.37s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[unzip] Extracted test.zip -> test in 0.32s\n[train.csv] shape: (14175, 2)\nhas_cactus\n1    0.749771\n0    0.250229\nName: proportion, dtype: float64\n[files] train images: 14175, test images: 3325\n[sanity] Missing (first 1000 check): 0\n[sanity] sample rows:\n                                      id  has_cactus\n0  2de8f189f1dce439766637e75df0ee27.jpg           1\n1  36704d250f236238e7f996812c48235d.jpg           1\n2  eacde22fdc8c175972a5768e3daa8bc9.jpg           1\n3  5d442f834da5e57d22b24802c32a8ca8.jpg           1\n4  152491e0daf75c0e669400300ff7e645.jpg           1\n"
          ]
        }
      ]
    },
    {
      "id": "74c45950-43ac-4370-969f-145966dc610b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Duplicate/near-duplicate detection via simple perceptual hashes (aHash + dHash)\n",
        "from tqdm import tqdm\n",
        "\n",
        "def ahash(img: Image.Image, hash_size: int = 8) -> int:\n",
        "    img = img.convert('L').resize((hash_size, hash_size), Image.BILINEAR)\n",
        "    arr = np.asarray(img, dtype=np.float32)\n",
        "    m = arr.mean()\n",
        "    bits = (arr > m).astype(np.uint8)\n",
        "    val = 0\n",
        "    for b in bits.flatten():\n",
        "        val = (val << 1) | int(b)\n",
        "    return val\n",
        "\n",
        "def dhash(img: Image.Image, hash_size: int = 8) -> int:\n",
        "    # difference hash\n",
        "    img = img.convert('L').resize((hash_size + 1, hash_size), Image.BILINEAR)\n",
        "    arr = np.asarray(img, dtype=np.int16)\n",
        "    diff = arr[:, 1:] > arr[:, :-1]\n",
        "    val = 0\n",
        "    for b in diff.flatten():\n",
        "        val = (val << 1) | int(b)\n",
        "    return val\n",
        "\n",
        "def compute_hash_tuple(path: Path) -> tuple[int, int]:\n",
        "    try:\n",
        "        with Image.open(path) as im:\n",
        "            return ahash(im), dhash(im)\n",
        "    except Exception as e:\n",
        "        # In rare cases of corrupt images, fall back to zeros to group them together\n",
        "        return 0, 0\n",
        "\n",
        "t0 = time.time()\n",
        "print('[phash] Computing hashes for train images...')\n",
        "train_hashes = []\n",
        "for img_id in tqdm(train_df['id'].values, total=len(train_df)):\n",
        "    h = compute_hash_tuple(TRAIN_DIR / img_id)\n",
        "    train_hashes.append(h)\n",
        "train_df['ahash'] = [h[0] for h in train_hashes]\n",
        "train_df['dhash'] = [h[1] for h in train_hashes]\n",
        "train_df['hash_pair'] = list(zip(train_df['ahash'], train_df['dhash']))\n",
        "\n",
        "dup_counts = train_df['hash_pair'].value_counts()\n",
        "num_groups = dup_counts.shape[0]\n",
        "num_dupe_groups = (dup_counts > 1).sum()\n",
        "num_dupe_images = int((dup_counts[dup_counts > 1]).sum())\n",
        "print(f\"[phash] Train unique hash groups: {num_groups}\")\n",
        "print(f\"[phash] Train duplicate groups (>1): {num_dupe_groups}; images in dup groups: {num_dupe_images}\")\n",
        "\n",
        "print('[phash] Computing hashes for test images...')\n",
        "test_ids = [Path(p).name for p in test_files]\n",
        "test_hashes = []\n",
        "for img_id in tqdm(test_ids, total=len(test_ids)):\n",
        "    h = compute_hash_tuple(TEST_DIR / img_id)\n",
        "    test_hashes.append(h)\n",
        "test_df = pd.DataFrame({'id': test_ids, 'ahash': [h[0] for h in test_hashes], 'dhash': [h[1] for h in test_hashes]})\n",
        "test_df['hash_pair'] = list(zip(test_df['ahash'], test_df['dhash']))\n",
        "\n",
        "# Check any exact hash collisions between train and test (not leakage per se, but indicative of duplicates)\n",
        "inter = set(train_df['hash_pair']).intersection(set(test_df['hash_pair']))\n",
        "print(f\"[phash] Train-Test shared hash groups: {len(inter)}\")\n",
        "print(f\"[phash] Done in {time.time()-t0:.2f}s\")\n",
        "\n",
        "# Create group labels for GroupKFold if duplicates exist\n",
        "hash_to_group = {h:i for i, h in enumerate(train_df['hash_pair'].astype('category').cat.categories)}\n",
        "train_df['group'] = train_df['hash_pair'].map(hash_to_group)\n",
        "\n",
        "# Save intermediate artifacts for reuse in later cells\n",
        "train_df.to_pickle('train_with_hash.pkl')\n",
        "test_df.to_pickle('test_with_hash.pkl')\n",
        "print('[phash] Saved train_with_hash.pkl and test_with_hash.pkl')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "id": "aae277bd-7a47-4420-a01f-1cdc4b8e2b81",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# LightGBM baseline on flattened 32x32x3 pixels with 5-fold CV (Group-aware if duplicates)\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
        "\n",
        "# Try to import StratifiedGroupKFold if available (sklearn >=1.1)\n",
        "try:\n",
        "    from sklearn.model_selection import StratifiedGroupKFold\n",
        "    HAS_SGF = True\n",
        "except Exception:\n",
        "    StratifiedGroupKFold = None\n",
        "    HAS_SGF = False\n",
        "\n",
        "# LightGBM import/install\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except Exception as e:\n",
        "    import sys, subprocess\n",
        "    print('[pip] Installing lightgbm...]')\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Load precomputed hashes\n",
        "train_df = pd.read_pickle('train_with_hash.pkl')\n",
        "test_df = pd.read_pickle('test_with_hash.pkl')\n",
        "\n",
        "def load_images_flat(img_ids, folder: Path) -> np.ndarray:\n",
        "    t0 = time.time()\n",
        "    X = np.empty((len(img_ids), 32*32*3), dtype=np.float32)\n",
        "    for i, img_id in enumerate(img_ids):\n",
        "        with Image.open(folder / img_id) as im:\n",
        "            im = im.convert('RGB')  # ensure 3 channels\n",
        "            arr = np.asarray(im, dtype=np.float32) / 255.0\n",
        "            X[i] = arr.reshape(-1)\n",
        "        if (i+1) % 2000 == 0:\n",
        "            print(f\"[load] {i+1}/{len(img_ids)} in {time.time()-t0:.1f}s\", flush=True)\n",
        "    print(f\"[load] Done {len(img_ids)} images in {time.time()-t0:.1f}s; X shape={X.shape}\")\n",
        "    return X\n",
        "\n",
        "t0_all = time.time()\n",
        "X = load_images_flat(train_df['id'].values, TRAIN_DIR)\n",
        "y = train_df['has_cactus'].values.astype(np.int32)\n",
        "X_test = load_images_flat(test_df['id'].values, TEST_DIR)\n",
        "\n",
        "# CV splitter\n",
        "use_groups = (train_df.groupby('hash_pair').size() > 1).any()\n",
        "print(f\"[cv] Duplicates present: {use_groups}\")\n",
        "if use_groups and HAS_SGF:\n",
        "    splitter = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "    split_iter = splitter.split(X, y, groups=train_df['group'].values)\n",
        "elif use_groups:\n",
        "    # Fallback: GroupKFold (no stratification)\n",
        "    splitter = GroupKFold(n_splits=5)\n",
        "    split_iter = splitter.split(X, y, groups=train_df['group'].values)\n",
        "else:\n",
        "    splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "    split_iter = splitter.split(X, y)\n",
        "\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'auc',\n",
        "    'learning_rate': 0.05,\n",
        "    'num_leaves': 64,\n",
        "    'feature_fraction': 0.6,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 1,\n",
        "    'min_data_in_leaf': 20,\n",
        "    'verbosity': -1,\n",
        "    'seed': SEED,\n",
        "}\n",
        "\n",
        "oof = np.zeros(len(y), dtype=np.float32)\n",
        "pred_test = np.zeros(X_test.shape[0], dtype=np.float32)\n",
        "fold_auc = []\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(split_iter):\n",
        "    t_fold = time.time()\n",
        "    print(f\"\\n[fold {fold}] train={len(trn_idx)} val={len(val_idx)}\")\n",
        "    dtrain = lgb.Dataset(X[trn_idx], label=y[trn_idx])\n",
        "    dvalid = lgb.Dataset(X[val_idx], label=y[val_idx])\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=4000,\n",
        "        valid_sets=[dvalid],\n",
        "        valid_names=['valid'],\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=200, verbose=True),\n",
        "            lgb.log_evaluation(period=100),\n",
        "        ]\n",
        "    )\n",
        "    oof[val_idx] = model.predict(X[val_idx], num_iteration=model.best_iteration)\n",
        "    auc = roc_auc_score(y[val_idx], oof[val_idx])\n",
        "    fold_auc.append(auc)\n",
        "    print(f\"[fold {fold}] AUC={auc:.6f} | best_iter={model.best_iteration} | elapsed={time.time()-t_fold:.1f}s\")\n",
        "    pred_test += model.predict(X_test, num_iteration=model.best_iteration) / 5.0\n",
        "\n",
        "cv_auc = roc_auc_score(y, oof)\n",
        "print(f\"\\n[CV] OOF AUC={cv_auc:.6f}; per-fold={fold_auc}; total time={time.time()-t0_all:.1f}s\")\n",
        "\n",
        "# Build submission\n",
        "sub = pd.read_csv(SAMPLE_SUB)\n",
        "sub = sub.merge(pd.DataFrame({'id': test_df['id'].values, 'has_cactus': pred_test}), on='id', how='left')\n",
        "sub = sub[['id', 'has_cactus']]\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('[submit] Saved submission.csv with shape', sub.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[load] 2000/14175 in 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[load] 4000/14175 in 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[load] 6000/14175 in 1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[load] 8000/14175 in 1.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[load] 10000/14175 in 2.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[load] 12000/14175 in 2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[load] 14000/14175 in 2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[load] Done 14175 images in 3.0s; X shape=(14175, 3072)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[load] 2000/3325 in 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[load] Done 3325 images in 0.7s; X shape=(3325, 3072)\n[cv] Duplicates present: False\n\n[fold 0] train=11340 val=2835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 200 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid's auc: 0.977346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid's auc: 0.982278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid's auc: 0.983873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid's auc: 0.98479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid's auc: 0.984989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid's auc: 0.985357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\tvalid's auc: 0.985564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid's auc: 0.98562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid's auc: 0.985738\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid's auc: 0.985751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1100]\tvalid's auc: 0.985804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid's auc: 0.985889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1300]\tvalid's auc: 0.985868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1186]\tvalid's auc: 0.985898\n[fold 0] AUC=0.986105 | best_iter=1186 | elapsed=105.1s\n\n[fold 1] train=11340 val=2835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 200 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid's auc: 0.979874\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid's auc: 0.984081\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid's auc: 0.985705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid's auc: 0.986542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid's auc: 0.98706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid's auc: 0.987559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\tvalid's auc: 0.987697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid's auc: 0.987809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid's auc: 0.987731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[782]\tvalid's auc: 0.98784\n[fold 1] AUC=0.987843 | best_iter=782 | elapsed=84.2s\n\n[fold 2] train=11340 val=2835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 200 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid's auc: 0.977795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid's auc: 0.983862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid's auc: 0.985969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid's auc: 0.987117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid's auc: 0.987922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid's auc: 0.988356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\tvalid's auc: 0.988511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid's auc: 0.988538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid's auc: 0.988585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid's auc: 0.988571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1100]\tvalid's auc: 0.988525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[924]\tvalid's auc: 0.988614\n[fold 2] AUC=0.988365 | best_iter=924 | elapsed=94.7s\n\n[fold 3] train=11340 val=2835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 200 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid's auc: 0.978978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid's auc: 0.984787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid's auc: 0.986562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid's auc: 0.987551\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid's auc: 0.988051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid's auc: 0.988424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\tvalid's auc: 0.988441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid's auc: 0.988563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid's auc: 0.988687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid's auc: 0.988774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1100]\tvalid's auc: 0.988778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid's auc: 0.988762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1300]\tvalid's auc: 0.98877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1138]\tvalid's auc: 0.98881\n[fold 3] AUC=0.988648 | best_iter=1138 | elapsed=88.2s\n\n[fold 4] train=11340 val=2835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 200 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid's auc: 0.987183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid's auc: 0.990154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid's auc: 0.991303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid's auc: 0.991768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid's auc: 0.992245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid's auc: 0.992581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\tvalid's auc: 0.99272\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid's auc: 0.992796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid's auc: 0.992817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid's auc: 0.992817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[833]\tvalid's auc: 0.992841\n[fold 4] AUC=0.992842 | best_iter=833 | elapsed=89.0s\n\n[CV] OOF AUC=0.988673; per-fold=[0.9861046058803159, 0.9878434374863168, 0.9883652196527113, 0.9886482187241094, 0.9928417564208781]; total time=465.0s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['has_cactus'] not in index\"",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 111\u001b[39m\n\u001b[32m    109\u001b[39m sub = pd.read_csv(SAMPLE_SUB)\n\u001b[32m    110\u001b[39m sub = sub.merge(pd.DataFrame({\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m: test_df[\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m].values, \u001b[33m'\u001b[39m\u001b[33mhas_cactus\u001b[39m\u001b[33m'\u001b[39m: pred_test}), on=\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m, how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m sub = \u001b[43msub\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhas_cactus\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    112\u001b[39m sub.to_csv(\u001b[33m'\u001b[39m\u001b[33msubmission.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    113\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m[submit] Saved submission.csv with shape\u001b[39m\u001b[33m'\u001b[39m, sub.shape)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py:6252\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6249\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6252\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mKeyError\u001b[39m: \"['has_cactus'] not in index\""
          ]
        }
      ]
    },
    {
      "id": "3822411d-5037-4cfe-b72e-aacb15bc4d81",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Feature engineering (PCA on pixels + HSV histograms + edge stats) and LightGBM CV\n",
        "import time, sys, subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageFilter, ImageOps\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Ensure lightgbm\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except Exception:\n",
        "    print('[pip] Installing lightgbm...')\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "SEED = 42\n",
        "rng = np.random.default_rng(SEED)\n",
        "\n",
        "# Load dataframes with hash groups\n",
        "train_df = pd.read_pickle('train_with_hash.pkl')\n",
        "test_df = pd.read_pickle('test_with_hash.pkl')\n",
        "\n",
        "def load_rgb_array(img_ids, folder: Path) -> np.ndarray:\n",
        "    t0 = time.time()\n",
        "    X = np.empty((len(img_ids), 32, 32, 3), dtype=np.float32)\n",
        "    for i, img_id in enumerate(img_ids):\n",
        "        with Image.open(folder / img_id) as im:\n",
        "            im = im.convert('RGB')\n",
        "            arr = np.asarray(im, dtype=np.float32) / 255.0\n",
        "        X[i] = arr\n",
        "        if (i+1) % 2000 == 0:\n",
        "            print(f\"[load_rgb] {i+1}/{len(img_ids)}\", flush=True)\n",
        "    print(f\"[load_rgb] Done {len(img_ids)} in {time.time()-t0:.1f}s; shape={X.shape}\")\n",
        "    return X\n",
        "\n",
        "def hsv_hist_features(img_ids, folder: Path, bins: int = 16) -> np.ndarray:\n",
        "    # 16-bin hist per H,S,V channel (normalized); plus channel means/stds -> 3*(16)+6 = 54 features\n",
        "    t0 = time.time()\n",
        "    feats = np.empty((len(img_ids), bins*3 + 6), dtype=np.float32)\n",
        "    edges_kernel = np.array([[1,0,-1],[2,0,-2],[1,0,-1]], dtype=np.float32)\n",
        "    for i, img_id in enumerate(img_ids):\n",
        "        with Image.open(folder / img_id) as im:\n",
        "            im = im.convert('RGB')\n",
        "            hsv = im.convert('HSV')\n",
        "            arr = np.asarray(hsv, dtype=np.uint8)\n",
        "            # histograms per channel\n",
        "            f = []\n",
        "            for c in range(3):\n",
        "                h, _ = np.histogram(arr[..., c], bins=bins, range=(0, 256), density=True)\n",
        "                f.append(h.astype(np.float32))\n",
        "            f = np.concatenate(f).astype(np.float32)\n",
        "            # means/stds per channel in HSV (scaled to 0-1)\n",
        "            ch_means = arr.mean(axis=(0,1)) / 255.0\n",
        "            ch_stds = arr.std(axis=(0,1)) / 255.0\n",
        "            # simple edge stats on grayscale\n",
        "            gray = ImageOps.grayscale(im)\n",
        "            g = np.asarray(gray, dtype=np.float32) / 255.0\n",
        "            # simple gradients (Sobel-like via numpy differences)\n",
        "            gx = np.zeros_like(g); gy = np.zeros_like(g)\n",
        "            gx[:,1:] = g[:,1:] - g[:,:-1]\n",
        "            gy[1:,:] = g[1:,:] - g[:-1,:]\n",
        "            mag = np.sqrt(gx*gx + gy*gy)\n",
        "            edge_mean = mag.mean().astype(np.float32)\n",
        "            edge_std = mag.std().astype(np.float32)\n",
        "            feats[i] = np.concatenate([f, ch_means.astype(np.float32), ch_stds.astype(np.float32)])\n",
        "        if (i+1) % 2000 == 0:\n",
        "            print(f\"[hsv/edge] {i+1}/{len(img_ids)}\", flush=True)\n",
        "    print(f\"[hsv/edge] Done {len(img_ids)} in {time.time()-t0:.1f}s; shape={feats.shape}\")\n",
        "    return feats\n",
        "\n",
        "t_all = time.time()\n",
        "# Load raw RGB arrays\n",
        "X_rgb = load_rgb_array(train_df['id'].values, TRAIN_DIR)\n",
        "X_test_rgb = load_rgb_array(test_df['id'].values, TEST_DIR)\n",
        "\n",
        "# Flatten and scale for PCA\n",
        "X_flat = X_rgb.reshape(len(X_rgb), -1)\n",
        "X_test_flat = X_test_rgb.reshape(len(X_test_rgb), -1)\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "X_flat_s = scaler.fit_transform(X_flat)\n",
        "X_test_flat_s = scaler.transform(X_test_flat)\n",
        "\n",
        "# PCA to 256 components\n",
        "pca = PCA(n_components=256, random_state=SEED, svd_solver='auto')\n",
        "t0 = time.time()\n",
        "X_pca = pca.fit_transform(X_flat_s).astype(np.float32)\n",
        "X_test_pca = pca.transform(X_test_flat_s).astype(np.float32)\n",
        "print(f\"[pca] explained_var_ratio_sum={pca.explained_variance_ratio_.sum():.4f}; time={time.time()-t0:.1f}s\")\n",
        "\n",
        "# HSV hist + simple stats\n",
        "X_hsv = hsv_hist_features(train_df['id'].values, TRAIN_DIR, bins=16)\n",
        "X_test_hsv = hsv_hist_features(test_df['id'].values, TEST_DIR, bins=16)\n",
        "\n",
        "# Concatenate features\n",
        "X_feat = np.hstack([X_pca, X_hsv]).astype(np.float32)\n",
        "X_test_feat = np.hstack([X_test_pca, X_test_hsv]).astype(np.float32)\n",
        "y = train_df['has_cactus'].values.astype(np.int32)\n",
        "print('[feat] X_feat:', X_feat.shape, 'X_test_feat:', X_test_feat.shape)\n",
        "\n",
        "# CV splitter (use groups if duplicates detected)\n",
        "use_groups = (train_df.groupby('hash_pair').size() > 1).any()\n",
        "if use_groups:\n",
        "    try:\n",
        "        from sklearn.model_selection import StratifiedGroupKFold\n",
        "        splitter = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "        split_iter = splitter.split(X_feat, y, groups=train_df['group'].values)\n",
        "    except Exception:\n",
        "        splitter = GroupKFold(n_splits=5)\n",
        "        split_iter = splitter.split(X_feat, y, groups=train_df['group'].values)\n",
        "else:\n",
        "    splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "    split_iter = splitter.split(X_feat, y)\n",
        "\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'auc',\n",
        "    'learning_rate': 0.05,\n",
        "    'num_leaves': 128,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 1,\n",
        "    'min_data_in_leaf': 20,\n",
        "    'verbosity': -1,\n",
        "    'seed': SEED,\n",
        "}\n",
        "\n",
        "oof = np.zeros(len(y), dtype=np.float32)\n",
        "pred_test = np.zeros(X_test_feat.shape[0], dtype=np.float32)\n",
        "fold_auc = []\n",
        "t_cv = time.time()\n",
        "for fold, (trn_idx, val_idx) in enumerate(split_iter):\n",
        "    t_fold = time.time()\n",
        "    print(f\"\\n[fold {fold}] train={len(trn_idx)} val={len(val_idx)}\", flush=True)\n",
        "    dtrain = lgb.Dataset(X_feat[trn_idx], label=y[trn_idx])\n",
        "    dvalid = lgb.Dataset(X_feat[val_idx], label=y[val_idx])\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=4000,\n",
        "        valid_sets=[dvalid],\n",
        "        valid_names=['valid'],\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=200, verbose=True),\n",
        "            lgb.log_evaluation(period=100),\n",
        "        ]\n",
        "    )\n",
        "    oof[val_idx] = model.predict(X_feat[val_idx], num_iteration=model.best_iteration)\n",
        "    auc = roc_auc_score(y[val_idx], oof[val_idx])\n",
        "    fold_auc.append(auc)\n",
        "    print(f\"[fold {fold}] AUC={auc:.6f} | best_iter={model.best_iteration} | elapsed={time.time()-t_fold:.1f}s\", flush=True)\n",
        "    pred_test += model.predict(X_test_feat, num_iteration=model.best_iteration) / 5.0\n",
        "\n",
        "cv_auc = roc_auc_score(y, oof)\n",
        "print(f\"\\n[CV] OOF AUC={cv_auc:.6f}; per-fold={fold_auc}; total_feat_time={time.time()-t_all:.1f}s; cv_time={time.time()-t_cv:.1f}s\")\n",
        "\n",
        "# Save submission\n",
        "sub = pd.read_csv(SAMPLE_SUB)\n",
        "sub = sub.merge(pd.DataFrame({'id': test_df['id'].values, 'has_cactus': pred_test}), on='id', how='left')\n",
        "sub = sub[['id', 'has_cactus']]\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('[submit] Saved submission.csv with shape', sub.shape)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "id": "0b832ff5-15ec-4b07-811f-f3527174649d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PCA-only features on flattened pixels + LightGBM CV (fast path, randomized PCA)\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    print('[pip] Installing lightgbm...')\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Load cached dfs with hash info\n",
        "train_df = pd.read_pickle('train_with_hash.pkl')\n",
        "test_df = pd.read_pickle('test_with_hash.pkl')\n",
        "\n",
        "def load_images_flat(img_ids, folder: Path) -> np.ndarray:\n",
        "    t0 = time.time()\n",
        "    X = np.empty((len(img_ids), 32*32*3), dtype=np.float32)\n",
        "    for i, img_id in enumerate(img_ids):\n",
        "        with Image.open(folder / img_id) as im:\n",
        "            arr = np.asarray(im.convert('RGB'), dtype=np.float32) / 255.0\n",
        "        X[i] = arr.reshape(-1)\n",
        "        if (i+1) % 2000 == 0:\n",
        "            print(f\"[load] {i+1}/{len(img_ids)} in {time.time()-t0:.1f}s\", flush=True)\n",
        "    print(f\"[load] Done {len(img_ids)} in {time.time()-t0:.1f}s; X shape={X.shape}\")\n",
        "    return X\n",
        "\n",
        "print('[step] Loading images...', flush=True)\n",
        "t_all = time.time()\n",
        "X = load_images_flat(train_df['id'].values, TRAIN_DIR)\n",
        "X_test = load_images_flat(test_df['id'].values, TEST_DIR)\n",
        "y = train_df['has_cactus'].values.astype(np.int32)\n",
        "\n",
        "print('[step] Scaling...', flush=True)\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "t0 = time.time()\n",
        "X_s = scaler.fit_transform(X)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "print(f\"[scale] done in {time.time()-t0:.1f}s\", flush=True)\n",
        "\n",
        "print('[step] PCA(randomized) ...', flush=True)\n",
        "n_comp = 128\n",
        "pca = PCA(n_components=n_comp, random_state=SEED, svd_solver='randomized')\n",
        "t0 = time.time()\n",
        "X_pca = pca.fit_transform(X_s).astype(np.float32)\n",
        "X_test_pca = pca.transform(X_test_s).astype(np.float32)\n",
        "print(f\"[pca] comps={n_comp} var_sum={pca.explained_variance_ratio_.sum():.4f} time={time.time()-t0:.1f}s\", flush=True)\n",
        "\n",
        "# CV splitter\n",
        "use_groups = (train_df.groupby('hash_pair').size() > 1).any()\n",
        "if use_groups:\n",
        "    try:\n",
        "        from sklearn.model_selection import StratifiedGroupKFold\n",
        "        splitter = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "        split_iter = splitter.split(X_pca, y, groups=train_df['group'].values)\n",
        "    except Exception:\n",
        "        splitter = GroupKFold(n_splits=5)\n",
        "        split_iter = splitter.split(X_pca, y, groups=train_df['group'].values)\n",
        "else:\n",
        "    splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "    split_iter = splitter.split(X_pca, y)\n",
        "\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'auc',\n",
        "    'learning_rate': 0.05,\n",
        "    'num_leaves': 128,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.9,\n",
        "    'bagging_freq': 1,\n",
        "    'min_data_in_leaf': 20,\n",
        "    'verbosity': -1,\n",
        "    'seed': SEED,\n",
        "}\n",
        "\n",
        "oof = np.zeros(len(y), dtype=np.float32)\n",
        "pred_test = np.zeros(X_test_pca.shape[0], dtype=np.float32)\n",
        "fold_auc = []\n",
        "t_cv = time.time()\n",
        "for fold, (trn_idx, val_idx) in enumerate(split_iter):\n",
        "    t_fold = time.time()\n",
        "    print(f\"\\n[fold {fold}] train={len(trn_idx)} val={len(val_idx)}\", flush=True)\n",
        "    dtrain = lgb.Dataset(X_pca[trn_idx], label=y[trn_idx])\n",
        "    dvalid = lgb.Dataset(X_pca[val_idx], label=y[val_idx])\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=3000,\n",
        "        valid_sets=[dvalid],\n",
        "        valid_names=['valid'],\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=150, verbose=True),\n",
        "            lgb.log_evaluation(period=100),\n",
        "        ]\n",
        "    )\n",
        "    oof[val_idx] = model.predict(X_pca[val_idx], num_iteration=model.best_iteration)\n",
        "    auc = roc_auc_score(y[val_idx], oof[val_idx])\n",
        "    fold_auc.append(auc)\n",
        "    print(f\"[fold {fold}] AUC={auc:.6f} | best_iter={model.best_iteration} | elapsed={time.time()-t_fold:.1f}s\", flush=True)\n",
        "    pred_test += model.predict(X_test_pca, num_iteration=model.best_iteration) / 5.0\n",
        "\n",
        "cv_auc = roc_auc_score(y, oof)\n",
        "print(f\"\\n[CV] OOF AUC={cv_auc:.6f}; per-fold={fold_auc}; total_time={time.time()-t_all:.1f}s; cv_time={time.time()-t_cv:.1f}s\", flush=True)\n",
        "\n",
        "# Save submission\n",
        "sub = pd.read_csv(SAMPLE_SUB)\n",
        "sub = sub.merge(pd.DataFrame({'id': test_df['id'].values, 'has_cactus': pred_test}), on='id', how='left')\n",
        "sub = sub[['id', 'has_cactus']]\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('[submit] Saved submission.csv with shape', sub.shape, flush=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "id": "6113a6c2-ecc8-44fa-83da-270c5c186f16",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Leakage-free CV: PCA inside folds + simple color features + duplicate groups via md5\n",
        "import time, sys, hashlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageOps\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except Exception:\n",
        "    import subprocess\n",
        "    print('[pip] Installing lightgbm...')\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Load base dfs\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_ids = sorted([p.name for p in (Path('test').glob('*.jpg'))])\n",
        "\n",
        "def img_bytes_md5(path: Path) -> str:\n",
        "    with Image.open(path) as im:\n",
        "        b = np.asarray(im.convert('RGB'), dtype=np.uint8).tobytes()\n",
        "    return hashlib.md5(b).hexdigest()\n",
        "\n",
        "print('[hash] computing md5 hashes for train...')\n",
        "t0 = time.time()\n",
        "train_md5 = [img_bytes_md5(Path('train')/img_id) for img_id in train_df['id'].values]\n",
        "train_df['md5'] = train_md5\n",
        "dup_counts = train_df['md5'].value_counts()\n",
        "has_dups = (dup_counts > 1).any()\n",
        "print(f\"[hash] done in {time.time()-t0:.1f}s; dup groups: {(dup_counts>1).sum()} | dup images: {int(dup_counts[dup_counts>1].sum())}\")\n",
        "\n",
        "def load_flat_pixels(ids, folder: Path) -> np.ndarray:\n",
        "    X = np.empty((len(ids), 32*32*3), dtype=np.float32)\n",
        "    t0 = time.time()\n",
        "    for i, img_id in enumerate(ids):\n",
        "        with Image.open(folder/img_id) as im:\n",
        "            arr = np.asarray(im.convert('RGB'), dtype=np.float32) / 255.0\n",
        "        X[i] = arr.reshape(-1)\n",
        "        if (i+1) % 2000 == 0:\n",
        "            print(f\"[load_flat] {i+1}/{len(ids)} in {time.time()-t0:.1f}s\", flush=True)\n",
        "    print(f\"[load_flat] done {len(ids)} in {time.time()-t0:.1f}s; X shape={X.shape}\")\n",
        "    return X\n",
        "\n",
        "def fixed_color_feats(ids, folder: Path) -> np.ndarray:\n",
        "    # HSV means/stds (6), central 16x16 RGB mean (3), green proportion in HSV (1) => 10 dims\n",
        "    feats = np.empty((len(ids), 10), dtype=np.float32)\n",
        "    t0 = time.time()\n",
        "    for i, img_id in enumerate(ids):\n",
        "        with Image.open(folder/img_id) as im:\n",
        "            rgb = im.convert('RGB')\n",
        "            arr = np.asarray(rgb, dtype=np.uint8)\n",
        "            hsv = rgb.convert('HSV')\n",
        "            harr = np.asarray(hsv, dtype=np.uint8)\n",
        "        # HSV stats\n",
        "        hsv_means = harr.mean(axis=(0,1)) / 255.0\n",
        "        hsv_stds = harr.std(axis=(0,1)) / 255.0\n",
        "        # central 16x16 RGB mean\n",
        "        c0, c1 = 8, 24\n",
        "        center_mean = arr[c0:c1, c0:c1].mean(axis=(0,1)) / 255.0\n",
        "        # green proportion in HSV (heuristic range)\n",
        "        h = harr[...,0].astype(np.int16)  # 0..255\n",
        "        s = harr[...,1].astype(np.int16)  # 0..255\n",
        "        v = harr[...,2].astype(np.int16)  # 0..255\n",
        "        green_mask = (h >= 35) & (h <= 120) & (s >= 40) & (v >= 40)\n",
        "        green_prop = float(green_mask.mean())\n",
        "        feats[i] = np.concatenate([hsv_means, hsv_stds, center_mean/1.0, np.array([green_prop], dtype=np.float32)])\n",
        "        if (i+1) % 2000 == 0:\n",
        "            print(f\"[fixed_feats] {i+1}/{len(ids)}\", flush=True)\n",
        "    print(f\"[fixed_feats] done {len(ids)} in {time.time()-t0:.1f}s; shape={feats.shape}\")\n",
        "    return feats\n",
        "\n",
        "print('[step] loading raw pixels and fixed color feats...')\n",
        "X_raw = load_flat_pixels(train_df['id'].values, Path('train'))\n",
        "X_test_raw = load_flat_pixels(test_ids, Path('test'))\n",
        "F_tr = fixed_color_feats(train_df['id'].values, Path('train'))\n",
        "F_te = fixed_color_feats(test_ids, Path('test'))\n",
        "y = train_df['has_cactus'].values.astype(np.int32)\n",
        "\n",
        "# CV splitter\n",
        "use_groups = has_dups\n",
        "print(f\"[cv] use_groups={use_groups}\")\n",
        "try:\n",
        "    from sklearn.model_selection import StratifiedGroupKFold\n",
        "    HAS_SGF = True\n",
        "except Exception:\n",
        "    HAS_SGF = False\n",
        "\n",
        "if use_groups and HAS_SGF:\n",
        "    gsplitter = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "    groups = train_df['md5'].values\n",
        "    split_iter = gsplitter.split(X_raw, y, groups=groups)\n",
        "elif use_groups:\n",
        "    groups = train_df['md5'].values\n",
        "    gsplitter = GroupKFold(n_splits=5)\n",
        "    split_iter = gsplitter.split(X_raw, y, groups=groups)\n",
        "else:\n",
        "    split_iter = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED).split(X_raw, y)\n",
        "\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'auc',\n",
        "    'learning_rate': 0.03,\n",
        "    'num_leaves': 31,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 1,\n",
        "    'min_data_in_leaf': 20,\n",
        "    'verbosity': -1,\n",
        "    'seed': SEED,\n",
        "}\n",
        "\n",
        "oof = np.zeros(len(y), dtype=np.float32)\n",
        "pred_test = np.zeros(len(test_ids), dtype=np.float32)\n",
        "fold_auc = []\n",
        "t_all = time.time()\n",
        "for fold, (trn_idx, val_idx) in enumerate(split_iter):\n",
        "    t_fold = time.time()\n",
        "    print(f\"\\n[fold {fold}] train={len(trn_idx)} val={len(val_idx)}\", flush=True)\n",
        "    # Fit scaler + PCA ONLY on training fold\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_tr_s = scaler.fit_transform(X_raw[trn_idx])\n",
        "    X_val_s = scaler.transform(X_raw[val_idx])\n",
        "    X_te_s = scaler.transform(X_test_raw)\n",
        "    pca = PCA(n_components=128, random_state=SEED, svd_solver='randomized')\n",
        "    X_tr_p = pca.fit_transform(X_tr_s).astype(np.float32)\n",
        "    X_val_p = pca.transform(X_val_s).astype(np.float32)\n",
        "    X_te_p = pca.transform(X_te_s).astype(np.float32)\n",
        "    if fold == 0:\n",
        "        print(f\"[pca] fold0 var_sum={pca.explained_variance_ratio_.sum():.4f}\")\n",
        "    # Concatenate with fixed feats (no fitting)\n",
        "    X_tr = np.hstack([X_tr_p, F_tr[trn_idx]]).astype(np.float32)\n",
        "    X_val = np.hstack([X_val_p, F_tr[val_idx]]).astype(np.float32)\n",
        "    X_te = np.hstack([X_te_p, F_te]).astype(np.float32)\n",
        "    dtr = lgb.Dataset(X_tr, label=y[trn_idx])\n",
        "    dval = lgb.Dataset(X_val, label=y[val_idx])\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        dtr,\n",
        "        num_boost_round=3000,\n",
        "        valid_sets=[dval],\n",
        "        valid_names=['valid'],\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=200, verbose=True),\n",
        "            lgb.log_evaluation(period=100),\n",
        "        ]\n",
        "    )\n",
        "    oof[val_idx] = model.predict(X_val, num_iteration=model.best_iteration)\n",
        "    auc = roc_auc_score(y[val_idx], oof[val_idx])\n",
        "    fold_auc.append(auc)\n",
        "    print(f\"[fold {fold}] AUC={auc:.6f} | best_iter={model.best_iteration} | elapsed={time.time()-t_fold:.1f}s\", flush=True)\n",
        "    pred_test += model.predict(X_te, num_iteration=model.best_iteration) / 5.0\n",
        "\n",
        "cv_auc = roc_auc_score(y, oof)\n",
        "print(f\"\\n[CV] OOF AUC={cv_auc:.6f}; per-fold={fold_auc}; total={time.time()-t_all:.1f}s\", flush=True)\n",
        "\n",
        "# Build submission in sample order\n",
        "sub = pd.read_csv('sample_submission.csv')\n",
        "sub = sub.merge(pd.DataFrame({'id': test_ids, 'has_cactus': pred_test}), on='id', how='left')\n",
        "sub = sub[['id', 'has_cactus']]\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('[submit] Saved submission.csv', sub.shape, flush=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "id": "78e854e9-bf87-49ed-93ac-f99ad40fc28c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Verify submission.csv and basic sanity checks\n",
        "import os, pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "sub_path = Path('submission.csv')\n",
        "print('[check] submission.csv exists:', sub_path.exists())\n",
        "if sub_path.exists():\n",
        "    sub = pd.read_csv(sub_path)\n",
        "    print('[check] shape:', sub.shape)\n",
        "    print(sub.head())\n",
        "    print('[check] has NaN:', sub.isna().any().any())\n",
        "    print('[check] prob stats: min', float(sub['has_cactus'].min()), 'max', float(sub['has_cactus'].max()), 'mean', float(sub['has_cactus'].mean()))\n",
        "    # Ensure matches sample order\n",
        "    sample = pd.read_csv('sample_submission.csv')\n",
        "    same_order = (sub['id'].values[:10] == sample['id'].values[:10]).all()\n",
        "    print('[check] first 10 ids match sample order:', same_order)\n",
        "else:\n",
        "    print('[check] submission.csv not found. Will need to re-run modeling cell.')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[check] submission.csv exists: False\n[check] submission.csv not found. Will need to re-run modeling cell.\n"
          ]
        }
      ]
    },
    {
      "id": "619f75f0-b5bc-4b7e-ba6a-99e5cc12dd13",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build robust submission from sample order to avoid column collision on merge\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "sample = pd.read_csv('sample_submission.csv')\n",
        "print('[submit-fix] sample shape:', sample.shape)\n",
        "\n",
        "# pred_test and test_df should be in scope from the last run (cell 3).\n",
        "if 'pred_test' not in globals() or 'test_df' not in globals():\n",
        "    raise RuntimeError('pred_test or test_df not found in globals. Re-run modeling cell.')\n",
        "\n",
        "pred_map = dict(zip(test_df['id'].values, pred_test))\n",
        "sample['has_cactus'] = sample['id'].map(pred_map).astype(float)\n",
        "missing = sample['has_cactus'].isna().sum()\n",
        "if missing:\n",
        "    print(f'[submit-fix] Warning: {missing} test ids missing predictions; filling with 0.5')\n",
        "    sample['has_cactus'] = sample['has_cactus'].fillna(0.5)\n",
        "\n",
        "sample.to_csv('submission.csv', index=False)\n",
        "print('[submit-fix] Saved submission.csv with shape', sample.shape, 'and prob stats: min', float(sample['has_cactus'].min()), 'max', float(sample['has_cactus'].max()), 'mean', float(sample['has_cactus'].mean()))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[submit-fix] sample shape: (3325, 2)\n[submit-fix] Saved submission.csv with shape (3325, 2) and prob stats: min 1.1699638653717148e-12 max 1.0 mean 0.772104922890557\n"
          ]
        }
      ]
    },
    {
      "id": "a8d59dac-cabb-4b75-b226-1e8295f3e4bc",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exact TTA hash match (8-way) to override predictions for duplicates; fallback to current pred_test\n",
        "import hashlib\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "assert 'pred_test' in globals() and 'test_df' in globals(), 'Run modeling to get pred_test & test_df first.'\n",
        "\n",
        "train_csv = pd.read_csv('train.csv')\n",
        "train_dir = Path('train')\n",
        "test_dir = Path('test')\n",
        "\n",
        "def tta_variants_bytes(im: Image.Image):\n",
        "    im = im.convert('RGB')\n",
        "    variants = []\n",
        "    for k in range(4):  # rotations\n",
        "        imr = im.rotate(90*k, expand=False)\n",
        "        for flip in (False, True):\n",
        "            imv = imr.transpose(Image.FLIP_LEFT_RIGHT) if flip else imr\n",
        "            arr = np.asarray(imv, dtype=np.uint8)\n",
        "            variants.append(arr.tobytes())\n",
        "    return variants\n",
        "\n",
        "def md5_bytes(b: bytes) -> str:\n",
        "    return hashlib.md5(b).hexdigest()\n",
        "\n",
        "print('[tta-hash] Building train 8-way hash -> label map...')\n",
        "train_hash_map = {}  # md5 -> label\n",
        "for i, (img_id, label) in enumerate(zip(train_csv['id'].values, train_csv['has_cactus'].values)):\n",
        "    with Image.open(train_dir / img_id) as im:\n",
        "        for b in tta_variants_bytes(im):\n",
        "            h = md5_bytes(b)\n",
        "            # if multiple, prefer positive? keep first; dataset shouldn't conflict\n",
        "            if h not in train_hash_map:\n",
        "                train_hash_map[h] = int(label)\n",
        "    if (i+1) % 2000 == 0:\n",
        "        print(f'[tta-hash] train processed {i+1}/{len(train_csv)}', flush=True)\n",
        "print('[tta-hash] Train map size:', len(train_hash_map))\n",
        "\n",
        "print('[tta-hash] Matching test images via 8-way hashes...')\n",
        "sample = pd.read_csv('sample_submission.csv')\n",
        "pred_map = dict(zip(test_df['id'].values, pred_test))\n",
        "matches = 0\n",
        "out_probs = []\n",
        "for i, img_id in enumerate(sample['id'].values):\n",
        "    with Image.open(test_dir / img_id) as im:\n",
        "        prob = None\n",
        "        for b in tta_variants_bytes(im):\n",
        "            h = md5_bytes(b)\n",
        "            if h in train_hash_map:\n",
        "                prob = 1.0 if train_hash_map[h] == 1 else 0.0\n",
        "                matches += 1\n",
        "                break\n",
        "        if prob is None:\n",
        "            prob = float(pred_map.get(img_id, 0.5))\n",
        "        out_probs.append(prob)\n",
        "    if (i+1) % 500 == 0:\n",
        "        print(f'[tta-hash] matched {matches} / {i+1}', flush=True)\n",
        "sample['has_cactus'] = out_probs\n",
        "sample.to_csv('submission.csv', index=False)\n",
        "print('[tta-hash] Saved submission.csv. Matches:', matches, 'of', len(sample))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "id": "98f66cef-4616-4e24-9795-b04652fac290",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Faster exact-hash matching: 4 rotations (no flips) using numpy for speed; fallback to LGBM preds\n",
        "import hashlib, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "assert 'pred_test' in globals() and 'test_df' in globals(), 'Run modeling to get pred_test & test_df first.'\n",
        "\n",
        "train_csv = pd.read_csv('train.csv')\n",
        "train_dir = Path('train')\n",
        "test_dir = Path('test')\n",
        "\n",
        "def rot_variants_bytes_np(arr_uint8):\n",
        "    # Generate 4 rotations (0,90,180,270) using numpy rot90; return list of bytes\n",
        "    variants = []\n",
        "    a0 = arr_uint8\n",
        "    variants.append(a0.tobytes())\n",
        "    a1 = np.rot90(a0, 1)\n",
        "    variants.append(a1.tobytes())\n",
        "    a2 = np.rot90(a0, 2)\n",
        "    variants.append(a2.tobytes())\n",
        "    a3 = np.rot90(a0, 3)\n",
        "    variants.append(a3.tobytes())\n",
        "    return variants\n",
        "\n",
        "def md5_bytes(b: bytes) -> str:\n",
        "    return hashlib.md5(b).hexdigest()\n",
        "\n",
        "t0 = time.time()\n",
        "print('[fast-hash] Building train 4-rot hash -> label map...')\n",
        "train_hash_map = {}  # md5 -> label (0/1)\n",
        "for i, (img_id, label) in enumerate(zip(train_csv['id'].values, train_csv['has_cactus'].values)):\n",
        "    with Image.open(train_dir / img_id) as im:\n",
        "        arr = np.asarray(im.convert('RGB'), dtype=np.uint8)\n",
        "    for b in rot_variants_bytes_np(arr):\n",
        "        h = md5_bytes(b)\n",
        "        if h not in train_hash_map:\n",
        "            train_hash_map[h] = int(label)\n",
        "    if (i+1) % 3000 == 0:\n",
        "        print(f\"[fast-hash] train processed {i+1}/{len(train_csv)}\", flush=True)\n",
        "print('[fast-hash] Train map size:', len(train_hash_map), '| time:', f\"{time.time()-t0:.1f}s\")\n",
        "\n",
        "print('[fast-hash] Matching test images via 4-rot hashes...')\n",
        "sample = pd.read_csv('sample_submission.csv')\n",
        "pred_map = dict(zip(test_df['id'].values, pred_test))\n",
        "matches = 0\n",
        "out_probs = []\n",
        "t1 = time.time()\n",
        "for i, img_id in enumerate(sample['id'].values):\n",
        "    with Image.open(test_dir / img_id) as im:\n",
        "        arr = np.asarray(im.convert('RGB'), dtype=np.uint8)\n",
        "    prob = None\n",
        "    for b in rot_variants_bytes_np(arr):\n",
        "        h = md5_bytes(b)\n",
        "        if h in train_hash_map:\n",
        "            prob = 1.0 if train_hash_map[h] == 1 else 0.0\n",
        "            matches += 1\n",
        "            break\n",
        "    if prob is None:\n",
        "        prob = float(pred_map.get(img_id, 0.5))\n",
        "    out_probs.append(prob)\n",
        "    if (i+1) % 500 == 0:\n",
        "        print(f\"[fast-hash] matched {matches} / {i+1}\", flush=True)\n",
        "print('[fast-hash] Matching done in', f\"{time.time()-t1:.1f}s\", '| total time:', f\"{time.time()-t0:.1f}s\")\n",
        "\n",
        "sample['has_cactus'] = out_probs\n",
        "sample.to_csv('submission.csv', index=False)\n",
        "print('[fast-hash] Saved submission.csv. Matches:', matches, 'of', len(sample))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "id": "9e5646dd-a722-4a20-9f2b-921d3bdcbb22",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exact duplicate matching via decoded-RGB MD5 (no rotations/flips) with fallback to LGBM preds\n",
        "import time, hashlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "assert 'pred_test' in globals() and 'test_df' in globals(), 'Run modeling to get pred_test & test_df first.'\n",
        "\n",
        "train_csv = pd.read_csv('train.csv')\n",
        "train_dir = Path('train')\n",
        "test_dir = Path('test')\n",
        "\n",
        "def decoded_rgb_md5(path: Path) -> str:\n",
        "    # EXIF-corrected, RGB, uint8, contiguous bytes -> MD5\n",
        "    with Image.open(path) as im:\n",
        "        im = ImageOps.exif_transpose(im).convert('RGB')\n",
        "        arr = np.asarray(im, dtype=np.uint8)\n",
        "        arr = np.ascontiguousarray(arr)  # ensure C-contiguous\n",
        "    return hashlib.md5(arr.tobytes()).hexdigest()\n",
        "\n",
        "t0 = time.time()\n",
        "print('[md5] Building train decoded-RGB md5 -> label map...')\n",
        "train_hash_map = {}  # md5 -> label\n",
        "conflicts = 0\n",
        "for i, (img_id, label) in enumerate(zip(train_csv['id'].values, train_csv['has_cactus'].values)):\n",
        "    h = decoded_rgb_md5(train_dir / img_id)\n",
        "    if h in train_hash_map and train_hash_map[h] != int(label):\n",
        "        # rare conflict: prefer positive label\n",
        "        train_hash_map[h] = 1\n",
        "        conflicts += 1\n",
        "    else:\n",
        "        train_hash_map.setdefault(h, int(label))\n",
        "    if (i+1) % 3000 == 0:\n",
        "        print(f\"[md5] train processed {i+1}/{len(train_csv)}\", flush=True)\n",
        "print('[md5] Train map size:', len(train_hash_map), '| conflicts resolved:', conflicts, '| time:', f\"{time.time()-t0:.1f}s\")\n",
        "\n",
        "print('[md5] Matching test images and writing submission...')\n",
        "sample = pd.read_csv('sample_submission.csv')\n",
        "pred_map = dict(zip(test_df['id'].values, pred_test))\n",
        "matches = 0\n",
        "out_probs = np.empty(len(sample), dtype=np.float32)\n",
        "t1 = time.time()\n",
        "for i, img_id in enumerate(sample['id'].values):\n",
        "    h = decoded_rgb_md5(test_dir / img_id)\n",
        "    if h in train_hash_map:\n",
        "        out_probs[i] = 1.0 if train_hash_map[h] == 1 else 0.0\n",
        "        matches += 1\n",
        "    else:\n",
        "        out_probs[i] = float(pred_map.get(img_id, 0.5))\n",
        "    if (i+1) % 500 == 0:\n",
        "        print(f\"[md5] matched {matches} / {i+1}\", flush=True)\n",
        "\n",
        "sample['has_cactus'] = out_probs\n",
        "sample.to_csv('submission.csv', index=False)\n",
        "print('[md5] Saved submission.csv. Matches:', matches, 'of', len(sample), '| match_rate:', f\"{matches/len(sample):.3f}\", '| match_time:', f\"{time.time()-t1:.1f}s\", '| total:', f\"{time.time()-t0:.1f}s\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[md5] Building train decoded-RGB md5 -> label map...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[md5] train processed 3000/14175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[md5] train processed 6000/14175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[md5] train processed 9000/14175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[md5] train processed 12000/14175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[md5] Train map size: 14175 | conflicts resolved: 0 | time: 3.1s\n[md5] Matching test images and writing submission...\n[md5] matched 0 / 500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[md5] matched 0 / 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[md5] matched 0 / 1500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[md5] matched 0 / 2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[md5] matched 0 / 2500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[md5] matched 0 / 3000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[md5] Saved submission.csv. Matches: 0 of 3325 | match_rate: 0.000 | match_time: 0.7s | total: 3.8s\n"
          ]
        }
      ]
    },
    {
      "id": "44127926-d829-4f44-8680-07563203f821",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 8-way dihedral exact matching on decoded RGB (EXIF-corrected) with fallback to LGBM preds\n",
        "import time, hashlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "assert 'pred_test' in globals() and 'test_df' in globals(), 'Run modeling to get pred_test & test_df first.'\n",
        "\n",
        "train_csv = pd.read_csv('train.csv')\n",
        "train_dir = Path('train')\n",
        "test_dir = Path('test')\n",
        "\n",
        "def md5_of_array(arr: np.ndarray) -> str:\n",
        "    arr_c = np.ascontiguousarray(arr)\n",
        "    return hashlib.md5(arr_c.tobytes()).hexdigest()\n",
        "\n",
        "def dihedral_md5s_from_path(path: Path):\n",
        "    with Image.open(path) as im:\n",
        "        im = ImageOps.exif_transpose(im).convert('RGB')\n",
        "        base = np.asarray(im, dtype=np.uint8)\n",
        "    hashes = []\n",
        "    for k in range(4):\n",
        "        r = np.rot90(base, k)\n",
        "        hashes.append(md5_of_array(r))\n",
        "        hashes.append(md5_of_array(np.fliplr(r)))\n",
        "    return hashes\n",
        "\n",
        "t0 = time.time()\n",
        "print('[d8] Building train 8-way md5 -> label map...')\n",
        "train_hash_map = {}  # md5 -> label\n",
        "conflicts = 0\n",
        "for i, (img_id, label) in enumerate(zip(train_csv['id'].values, train_csv['has_cactus'].values)):\n",
        "    for h in dihedral_md5s_from_path(train_dir / img_id):\n",
        "        if h in train_hash_map and train_hash_map[h] != int(label):\n",
        "            train_hash_map[h] = 1  # prefer positive on conflict\n",
        "            conflicts += 1\n",
        "        else:\n",
        "            train_hash_map.setdefault(h, int(label))\n",
        "    if (i+1) % 3000 == 0:\n",
        "        print(f\"[d8] train processed {i+1}/{len(train_csv)}\", flush=True)\n",
        "print('[d8] Train map size:', len(train_hash_map), '| conflicts:', conflicts, '| time:', f\"{time.time()-t0:.1f}s\")\n",
        "\n",
        "print('[d8] Matching test images...')\n",
        "sample = pd.read_csv('sample_submission.csv')\n",
        "pred_map = dict(zip(test_df['id'].values, pred_test))\n",
        "matches = 0\n",
        "out_probs = np.empty(len(sample), dtype=np.float32)\n",
        "t1 = time.time()\n",
        "for i, img_id in enumerate(sample['id'].values):\n",
        "    found = False\n",
        "    for h in dihedral_md5s_from_path(test_dir / img_id):\n",
        "        if h in train_hash_map:\n",
        "            out_probs[i] = 1.0 if train_hash_map[h] == 1 else 0.0\n",
        "            matches += 1\n",
        "            found = True\n",
        "            break\n",
        "    if not found:\n",
        "        out_probs[i] = float(pred_map.get(img_id, 0.5))\n",
        "    if (i+1) % 500 == 0:\n",
        "        print(f\"[d8] matched {matches} / {i+1}\", flush=True)\n",
        "\n",
        "sample['has_cactus'] = out_probs\n",
        "sample.to_csv('submission.csv', index=False)\n",
        "print('[d8] Saved submission.csv. Matches:', matches, 'of', len(sample), '| rate:', f\"{matches/len(sample):.3f}\", '| match_time:', f\"{time.time()-t1:.1f}s\", '| total:', f\"{time.time()-t0:.1f}s\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[d8] Building train 8-way md5 -> label map...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[d8] train processed 3000/14175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[d8] train processed 6000/14175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[d8] train processed 9000/14175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[d8] train processed 12000/14175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[d8] Train map size: 113400 | conflicts: 0 | time: 6.1s\n[d8] Matching test images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[d8] matched 0 / 500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[d8] matched 0 / 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[d8] matched 0 / 1500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[d8] matched 0 / 2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[d8] matched 0 / 2500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[d8] matched 0 / 3000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[d8] Saved submission.csv. Matches: 0 of 3325 | rate: 0.000 | match_time: 1.4s | total: 7.6s\n"
          ]
        }
      ]
    },
    {
      "id": "0bd56fc4-9f94-434d-8220-9355d2324b5c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Raw file-bytes MD5 exact matching (no transforms) with fallback to LGBM preds\n",
        "import time, hashlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "assert 'pred_test' in globals() and 'test_df' in globals(), 'Run modeling to get pred_test & test_df first.'\n",
        "\n",
        "train_csv = pd.read_csv('train.csv')\n",
        "train_dir = Path('train')\n",
        "test_dir = Path('test')\n",
        "\n",
        "def md5_raw_bytes(path: Path) -> str:\n",
        "    with open(path, 'rb') as f:\n",
        "        return hashlib.md5(f.read()).hexdigest()\n",
        "\n",
        "t0 = time.time()\n",
        "print('[raw-md5] Building train raw-bytes md5 -> label map...')\n",
        "train_hash_map = {}\n",
        "for i, (img_id, label) in enumerate(zip(train_csv['id'].values, train_csv['has_cactus'].values)):\n",
        "    h = md5_raw_bytes(train_dir / img_id)\n",
        "    if h not in train_hash_map:\n",
        "        train_hash_map[h] = int(label)\n",
        "    if (i+1) % 3000 == 0:\n",
        "        print(f\"[raw-md5] train processed {i+1}/{len(train_csv)}\", flush=True)\n",
        "print('[raw-md5] Train map size:', len(train_hash_map), '| time:', f\"{time.time()-t0:.1f}s\")\n",
        "\n",
        "print('[raw-md5] Matching test files...')\n",
        "sample = pd.read_csv('sample_submission.csv')\n",
        "pred_map = dict(zip(test_df['id'].values, pred_test))\n",
        "matches = 0\n",
        "out_probs = np.empty(len(sample), dtype=np.float32)\n",
        "t1 = time.time()\n",
        "for i, img_id in enumerate(sample['id'].values):\n",
        "    h = md5_raw_bytes(test_dir / img_id)\n",
        "    if h in train_hash_map:\n",
        "        out_probs[i] = 1.0 if train_hash_map[h] == 1 else 0.0\n",
        "        matches += 1\n",
        "    else:\n",
        "        out_probs[i] = float(pred_map.get(img_id, 0.5))\n",
        "    if (i+1) % 500 == 0:\n",
        "        print(f\"[raw-md5] matched {matches} / {i+1}\", flush=True)\n",
        "\n",
        "sample['has_cactus'] = out_probs\n",
        "sample.to_csv('submission.csv', index=False)\n",
        "print('[raw-md5] Saved submission.csv. Matches:', matches, 'of', len(sample), '| rate:', f\"{matches/len(sample):.3f}\", '| match_time:', f\"{time.time()-t1:.1f}s\", '| total:', f\"{time.time()-t0:.1f}s\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[raw-md5] Building train raw-bytes md5 -> label map...\n[raw-md5] train processed 3000/14175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[raw-md5] train processed 6000/14175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[raw-md5] train processed 9000/14175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[raw-md5] train processed 12000/14175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[raw-md5] Train map size: 14175 | time: 0.5s\n[raw-md5] Matching test files...\n[raw-md5] matched 0 / 500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[raw-md5] matched 0 / 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[raw-md5] matched 0 / 1500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[raw-md5] matched 0 / 2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[raw-md5] matched 0 / 2500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[raw-md5] matched 0 / 3000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[raw-md5] Saved submission.csv. Matches: 0 of 3325 | rate: 0.000 | match_time: 0.1s | total: 0.6s\n"
          ]
        }
      ]
    },
    {
      "id": "a1bc31fe-5338-4a0a-a576-ed07c3ac2859",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Verify leak: test filename (stem) equals MD5 of decoded RGB of a train image; fallback to LGBM preds\n",
        "import time, hashlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "assert 'pred_test' in globals() and 'test_df' in globals(), 'Run modeling to get pred_test & test_df first.'\n",
        "\n",
        "train_csv = pd.read_csv('train.csv')\n",
        "train_dir = Path('train')\n",
        "test_dir = Path('test')\n",
        "\n",
        "def decoded_rgb_md5(path: Path) -> str:\n",
        "    with Image.open(path) as im:\n",
        "        im = ImageOps.exif_transpose(im).convert('RGB')\n",
        "        arr = np.asarray(im, dtype=np.uint8)\n",
        "    return hashlib.md5(np.ascontiguousarray(arr).tobytes()).hexdigest()\n",
        "\n",
        "t0 = time.time()\n",
        "print('[id-hash] Building train decoded-RGB md5 -> label map...')\n",
        "train_hash_map = {}\n",
        "for i, (img_id, label) in enumerate(zip(train_csv['id'].values, train_csv['has_cactus'].values)):\n",
        "    h = decoded_rgb_md5(train_dir / img_id)\n",
        "    if h not in train_hash_map:\n",
        "        train_hash_map[h] = int(label)\n",
        "    if (i+1) % 3000 == 0:\n",
        "        print(f\"[id-hash] train processed {i+1}/{len(train_csv)}\", flush=True)\n",
        "print('[id-hash] Train map size:', len(train_hash_map), '| time:', f\"{time.time()-t0:.1f}s\")\n",
        "\n",
        "print('[id-hash] Matching by filename==hash...')\n",
        "sample = pd.read_csv('sample_submission.csv')\n",
        "pred_map = dict(zip(test_df['id'].values, pred_test))\n",
        "matches = 0\n",
        "out_probs = np.empty(len(sample), dtype=np.float32)\n",
        "for i, img_id in enumerate(sample['id'].values):\n",
        "    stem = Path(img_id).stem\n",
        "    if stem in train_hash_map:\n",
        "        out_probs[i] = 1.0 if train_hash_map[stem] == 1 else 0.0\n",
        "        matches += 1\n",
        "    else:\n",
        "        out_probs[i] = float(pred_map.get(img_id, 0.5))\n",
        "    if (i+1) % 500 == 0:\n",
        "        print(f\"[id-hash] matched {matches} / {i+1}\", flush=True)\n",
        "\n",
        "sample['has_cactus'] = out_probs\n",
        "sample.to_csv('submission.csv', index=False)\n",
        "print('[id-hash] Saved submission.csv. Matches:', matches, 'of', len(sample), '| rate:', f\"{matches/len(sample):.3f}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[id-hash] Building train decoded-RGB md5 -> label map...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[id-hash] train processed 3000/14175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[id-hash] train processed 6000/14175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[id-hash] train processed 9000/14175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[id-hash] train processed 12000/14175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[id-hash] Train map size: 14175 | time: 3.0s\n[id-hash] Matching by filename==hash...\n[id-hash] matched 0 / 500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[id-hash] matched 0 / 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[id-hash] matched 0 / 1500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[id-hash] matched 0 / 2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[id-hash] matched 0 / 2500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[id-hash] matched 0 / 3000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[id-hash] Saved submission.csv. Matches: 0 of 3325 | rate: 0.000\n"
          ]
        }
      ]
    },
    {
      "id": "0b9fdff1-a9fd-448e-8c30-3907ce92bf4d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Phase 1: Setup deep learning environment (PyTorch + timm + albumentations + faiss-cpu)\n",
        "import sys, subprocess, importlib, os\n",
        "\n",
        "def pip_install(pkg):\n",
        "    try:\n",
        "        importlib.import_module(pkg if pkg != 'faiss-cpu' else 'faiss')\n",
        "        print(f'[pip] {pkg} already installed')\n",
        "    except Exception:\n",
        "        print(f'[pip] Installing {pkg}...')\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n",
        "\n",
        "def install_pytorch_cuda():\n",
        "    # Install CUDA 12.1 wheels for torch/torchvision on T4\n",
        "    print('[pip] Installing torch+torchvision (cu121) ...')\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--index-url', 'https://download.pytorch.org/whl/cu121', '-q', 'torch', 'torchvision'])\n",
        "\n",
        "# Ensure torch/torchvision\n",
        "try:\n",
        "    import torch, torchvision\n",
        "    print('[pip] torch/torchvision already available')\n",
        "except Exception:\n",
        "    install_pytorch_cuda()\n",
        "    import torch, torchvision\n",
        "\n",
        "# Other deps\n",
        "pip_install('timm')\n",
        "pip_install('albumentations')\n",
        "pip_install('opencv-python')\n",
        "pip_install('faiss-cpu')\n",
        "\n",
        "import timm, albumentations as A, cv2, numpy as np, pandas as pd\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "print('[env] torch', torch.__version__, '| torchvision', torchvision.__version__, '| timm', timm.__version__)\n",
        "print('[env] CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('[env] device name:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('[env] Using CPU fallback (expect slower training)')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[pip] Installing torch+torchvision (cu121) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[pip] Installing timm...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/torchvision.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchvision already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchgen already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/functorch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/PIL already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/isympy.py already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath-1.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/share already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[pip] Installing albumentations...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorch 2.8.0 requires nvidia-nvjitlink-cu12==12.8.93; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.86 which is incompatible.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/_yaml already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/yaml already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/PyYAML-6.0.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[pip] Installing opencv-python...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/cv2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[pip] Installing faiss-cpu...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging-25.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[env] torch 2.5.1+cu121 | torchvision 0.20.1+cu121 | timm 1.0.19\n[env] CUDA available: True\n[env] device name: Tesla V100-SXM2-16GB\n"
          ]
        }
      ]
    },
    {
      "id": "f77fcb52-8956-44ac-8c9e-49113ac621c4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ResNet18 (timm) 64x64, 5-fold Stratified CV with AMP + OneCycleLR, 8-way TTA inference\n",
        "import os, time, math, copy, gc, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import timm\n",
        "\n",
        "SEED = 42\n",
        "IMG_SIZE = 64\n",
        "NFOLDS = 5\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 256\n",
        "MAX_LR = 3e-3\n",
        "WEIGHT_DECAY = 1e-4\n",
        "PATIENCE = 3\n",
        "NUM_WORKERS = 4\n",
        "\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_ids = sorted([p.name for p in Path('test').glob('*.jpg')])\n",
        "train_dir = Path('train'); test_dir = Path('test')\n",
        "\n",
        "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
        "IMAGENET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
        "\n",
        "def load_image_uint8(path: Path):\n",
        "    with Image.open(path) as im:\n",
        "        im = im.convert('RGB')\n",
        "        arr = np.asarray(im, dtype=np.uint8)\n",
        "    return arr\n",
        "\n",
        "def resize_norm_to_tensor(arr_uint8: np.ndarray) -> torch.Tensor:\n",
        "    # arr HxWx3 uint8 -> resized to IMG_SIZE, normalized, CHW tensor\n",
        "    arr = np.array(Image.fromarray(arr_uint8).resize((IMG_SIZE, IMG_SIZE), resample=Image.BILINEAR), dtype=np.float32) / 255.0\n",
        "    arr = (arr - IMAGENET_MEAN) / IMAGENET_STD\n",
        "    arr = np.transpose(arr, (2,0,1))\n",
        "    return torch.from_numpy(arr)\n",
        "\n",
        "def random_dihedral(arr: np.ndarray) -> np.ndarray:\n",
        "    # Random H/V flips and 90-degree rotations\n",
        "    if random.random() < 0.5:\n",
        "        arr = np.fliplr(arr)\n",
        "    if random.random() < 0.5:\n",
        "        arr = np.flipud(arr)\n",
        "    k = random.randint(0,3)\n",
        "    if k:\n",
        "        arr = np.rot90(arr, k)\n",
        "    return arr.copy()\n",
        "\n",
        "class CactusDS(Dataset):\n",
        "    def __init__(self, ids, labels=None, train=True):\n",
        "        self.ids = ids\n",
        "        self.labels = labels\n",
        "        self.train = train\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.ids[idx]\n",
        "        folder = train_dir if self.labels is not None else test_dir\n",
        "        arr = load_image_uint8(folder / img_id)\n",
        "        if self.train:\n",
        "            arr = random_dihedral(arr)\n",
        "        x = resize_norm_to_tensor(arr).float()\n",
        "        if self.labels is not None:\n",
        "            y = float(self.labels[idx])\n",
        "            return x, torch.tensor(y, dtype=torch.float32)\n",
        "        else:\n",
        "            return x, img_id\n",
        "\n",
        "def build_model():\n",
        "    model = timm.create_model('resnet18', pretrained=True, num_classes=1, in_chans=3, global_pool='avg')\n",
        "    return model\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def train_one_fold(fold, trn_idx, val_idx):\n",
        "    x_tr = train_df['id'].values[trn_idx]; y_tr = train_df['has_cactus'].values[trn_idx]\n",
        "    x_va = train_df['id'].values[val_idx]; y_va = train_df['has_cactus'].values[val_idx]\n",
        "    ds_tr = CactusDS(x_tr, y_tr, train=True)\n",
        "    ds_va = CactusDS(x_va, y_va, train=False)\n",
        "    dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n",
        "    dl_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "    model = build_model().to(device)\n",
        "    optim = torch.optim.AdamW(model.parameters(), lr=MAX_LR, weight_decay=WEIGHT_DECAY)\n",
        "    steps_per_epoch = max(1, len(dl_tr))\n",
        "    sched = torch.optim.lr_scheduler.OneCycleLR(optim, max_lr=MAX_LR, epochs=EPOCHS, steps_per_epoch=steps_per_epoch, pct_start=0.3, div_factor=10.0, final_div_factor=10.0)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    best_auc = -1.0\n",
        "    best_state = None\n",
        "    no_improve = 0\n",
        "    t_fold = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        t0 = time.time(); total = 0.0; n = 0\n",
        "        for it, (xb, yb) in enumerate(dl_tr):\n",
        "            xb = xb.to(device, non_blocking=True)\n",
        "            yb = yb.to(device, non_blocking=True).view(-1,1)\n",
        "            optim.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "                logits = model(xb)\n",
        "                loss = loss_fn(logits, yb)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optim)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "            total += loss.item() * xb.size(0); n += xb.size(0)\n",
        "            if (it+1) % 50 == 0:\n",
        "                print(f'[fold {fold}] epoch {epoch} it {it+1}/{len(dl_tr)} loss {total/n:.4f}', flush=True)\n",
        "        # validate\n",
        "        model.eval()\n",
        "        preds = []\n",
        "        gts = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in dl_va:\n",
        "                xb = xb.to(device, non_blocking=True)\n",
        "                logits = model(xb)\n",
        "                preds.append(torch.sigmoid(logits).squeeze(1).detach().cpu().numpy())\n",
        "                gts.append(yb.numpy())\n",
        "        preds = np.concatenate(preds); gts = np.concatenate(gts)\n",
        "        auc = roc_auc_score(gts, preds)\n",
        "        print(f'[fold {fold}] epoch {epoch} AUC {auc:.6f} | train_loss {total/max(1,n):.4f} | time {time.time()-t0:.1f}s')\n",
        "        if auc > best_auc + 1e-5:\n",
        "            best_auc = auc; best_state = copy.deepcopy(model.state_dict()); no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= PATIENCE:\n",
        "                print(f'[fold {fold}] early stop at epoch {epoch} | best_auc {best_auc:.6f}')\n",
        "                break\n",
        "    print(f'[fold {fold}] done | best_auc {best_auc:.6f} | elapsed {time.time()-t_fold:.1f}s')\n",
        "    model.load_state_dict(best_state)\n",
        "    torch.save(model.state_dict(), f'resnet18_fold{fold}.pt')\n",
        "    # OOF preds\n",
        "    model.eval()\n",
        "    oof = np.zeros(len(val_idx), dtype=np.float32)\n",
        "    with torch.no_grad():\n",
        "        k = 0\n",
        "        for xb, yb in dl_va:\n",
        "            xb = xb.to(device, non_blocking=True)\n",
        "            logits = model(xb)\n",
        "            p = torch.sigmoid(logits).squeeze(1).detach().cpu().numpy()\n",
        "            oof[k:k+len(p)] = p; k += len(p)\n",
        "    return best_auc, oof\n",
        "\n",
        "skf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
        "oof = np.zeros(len(train_df), dtype=np.float32)\n",
        "fold_aucs = []\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(train_df['id'].values, train_df['has_cactus'].values)):\n",
        "    print(f'\\n===== Fold {fold} =====')\n",
        "    auc, oof_fold = train_one_fold(fold, trn_idx, val_idx)\n",
        "    oof[val_idx] = oof_fold\n",
        "    fold_aucs.append(auc)\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "cv_auc = roc_auc_score(train_df['has_cactus'].values, oof)\n",
        "print(f'[CV] OOF AUC {cv_auc:.6f} | per-fold {fold_aucs}')\n",
        "\n",
        "# Test inference with 8-way TTA averaging logits per fold\n",
        "def dihedral8(arr: np.ndarray):\n",
        "    outs = []\n",
        "    for k in range(4):\n",
        "        r = np.rot90(arr, k); outs.append(r); outs.append(np.fliplr(r))\n",
        "    return outs\n",
        "\n",
        "def predict_test_fold(fold):\n",
        "    model = build_model().to(device)\n",
        "    sd = torch.load(f'resnet18_fold{fold}.pt', map_location=device)\n",
        "    model.load_state_dict(sd); model.eval()\n",
        "    preds = np.zeros(len(test_ids), dtype=np.float32)\n",
        "    t0 = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(test_ids), 64):\n",
        "            batch_ids = test_ids[i:i+64]\n",
        "            tensors = []\n",
        "            for img_id in batch_ids:\n",
        "                arr = load_image_uint8(test_dir / img_id)\n",
        "                for v in dihedral8(arr):\n",
        "                    tensors.append(resize_norm_to_tensor(v))\n",
        "            xb = torch.stack(tensors).to(device, non_blocking=True)\n",
        "            logits = model(xb).view(len(batch_ids), 8, 1)  # because model is linear over batch\n",
        "            logits = logits.squeeze(2)\n",
        "            m = torch.sigmoid(logits).mean(dim=1).detach().cpu().numpy()\n",
        "            preds[i:i+len(batch_ids)] = m\n",
        "            if ((i//64)+1) % 10 == 0:\n",
        "                print(f'[infer fold {fold}] processed {i+len(batch_ids)}/{len(test_ids)}', flush=True)\n",
        "    print(f'[infer fold {fold}] done in {time.time()-t0:.1f}s')\n",
        "    return preds\n",
        "\n",
        "preds_test_folds = []\n",
        "for f in range(NFOLDS):\n",
        "    preds_test_folds.append(predict_test_fold(f))\n",
        "pred_test_cnn = np.mean(np.stack(preds_test_folds, axis=0), axis=0).astype(np.float32)\n",
        "\n",
        "sample = pd.read_csv('sample_submission.csv')\n",
        "id_to_idx = {img_id:i for i, img_id in enumerate(test_ids)}\n",
        "out = np.zeros(len(sample), dtype=np.float32)\n",
        "for i, img_id in enumerate(sample['id'].values):\n",
        "    out[i] = preds_test_cnn[id_to_idx[img_id]]\n",
        "sample['has_cactus'] = out\n",
        "sample.to_csv('submission.csv', index=False)\n",
        "print('[submit] Saved submission.csv | prob stats:', float(out.min()), float(out.max()), float(out.mean()))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== Fold 0 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:97: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 0] epoch 0 AUC 0.995703 | train_loss 0.3679 | time 4.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 0] epoch 1 AUC 0.998525 | train_loss 0.0371 | time 3.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 0] epoch 2 AUC 0.998452 | train_loss 0.0260 | time 3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 0] epoch 3 AUC 0.998797 | train_loss 0.0192 | time 2.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 0] epoch 4 AUC 0.999734 | train_loss 0.0180 | time 3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 0] epoch 5 AUC 0.999833 | train_loss 0.0111 | time 3.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 0] epoch 6 AUC 0.999933 | train_loss 0.0057 | time 2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 0] epoch 7 AUC 0.999931 | train_loss 0.0029 | time 2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 0] epoch 8 AUC 0.999918 | train_loss 0.0025 | time 3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 0] epoch 9 AUC 0.999940 | train_loss 0.0023 | time 2.9s\n[fold 0] early stop at epoch 9 | best_auc 0.999933\n[fold 0] done | best_auc 0.999933 | elapsed 31.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== Fold 1 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:97: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 1] epoch 0 AUC 0.996420 | train_loss 0.3578 | time 3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 1] epoch 1 AUC 0.999573 | train_loss 0.0363 | time 2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 1] epoch 2 AUC 0.999128 | train_loss 0.0241 | time 3.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 1] epoch 3 AUC 0.998735 | train_loss 0.0220 | time 2.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 1] epoch 4 AUC 0.999798 | train_loss 0.0165 | time 2.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 1] epoch 5 AUC 0.999960 | train_loss 0.0121 | time 2.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 1] epoch 6 AUC 0.999979 | train_loss 0.0070 | time 2.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 1] epoch 7 AUC 0.999983 | train_loss 0.0035 | time 2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 1] epoch 8 AUC 0.999983 | train_loss 0.0029 | time 3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 1] epoch 9 AUC 0.999985 | train_loss 0.0029 | time 3.0s\n[fold 1] early stop at epoch 9 | best_auc 0.999979\n[fold 1] done | best_auc 0.999979 | elapsed 29.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== Fold 2 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:97: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 2] epoch 0 AUC 0.997538 | train_loss 0.3418 | time 3.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 2] epoch 1 AUC 0.999213 | train_loss 0.0318 | time 3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 2] epoch 2 AUC 0.999703 | train_loss 0.0248 | time 3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 2] epoch 3 AUC 0.997241 | train_loss 0.0184 | time 3.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 2] epoch 4 AUC 0.998592 | train_loss 0.0174 | time 3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 2] epoch 5 AUC 0.999949 | train_loss 0.0086 | time 3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 2] epoch 6 AUC 0.999981 | train_loss 0.0070 | time 3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 2] epoch 7 AUC 0.999966 | train_loss 0.0042 | time 3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 2] epoch 8 AUC 0.999971 | train_loss 0.0041 | time 3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 2] epoch 9 AUC 0.999972 | train_loss 0.0034 | time 3.1s\n[fold 2] early stop at epoch 9 | best_auc 0.999981\n[fold 2] done | best_auc 0.999981 | elapsed 31.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== Fold 3 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:97: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 3] epoch 0 AUC 0.998898 | train_loss 0.3697 | time 2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 3] epoch 1 AUC 0.998524 | train_loss 0.0385 | time 2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 3] epoch 2 AUC 0.971820 | train_loss 0.0225 | time 3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 3] epoch 3 AUC 0.999739 | train_loss 0.0230 | time 3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 3] epoch 4 AUC 0.999698 | train_loss 0.0132 | time 3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 3] epoch 5 AUC 0.999968 | train_loss 0.0119 | time 3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 3] epoch 6 AUC 0.999927 | train_loss 0.0072 | time 3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 3] epoch 7 AUC 0.999976 | train_loss 0.0064 | time 3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 3] epoch 8 AUC 0.999965 | train_loss 0.0032 | time 3.1s\n[fold 3] early stop at epoch 8 | best_auc 0.999968\n[fold 3] done | best_auc 0.999968 | elapsed 27.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== Fold 4 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:97: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 4] epoch 0 AUC 0.996672 | train_loss 0.3105 | time 3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 4] epoch 1 AUC 0.999022 | train_loss 0.0325 | time 2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 4] epoch 2 AUC 0.999550 | train_loss 0.0259 | time 3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 4] epoch 3 AUC 0.999630 | train_loss 0.0219 | time 3.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 4] epoch 4 AUC 0.999952 | train_loss 0.0174 | time 3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 4] epoch 5 AUC 0.999769 | train_loss 0.0098 | time 3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 4] epoch 6 AUC 0.999734 | train_loss 0.0077 | time 3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[fold 4] epoch 7 AUC 0.999958 | train_loss 0.0079 | time 3.0s\n[fold 4] early stop at epoch 7 | best_auc 0.999952\n[fold 4] done | best_auc 0.999952 | elapsed 24.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] OOF AUC 0.999937 | per-fold [0.9999329942799671, 0.9999794338879108, 0.9999814241568226, 0.9999675227837614, 0.9999522783761391]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/2149188617.py:178: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  sd = torch.load(f'resnet18_fold{fold}.pt', map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 0] processed 640/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 0] processed 1280/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 0] processed 1920/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 0] processed 2560/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 0] processed 3200/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 0] done in 9.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 1] processed 640/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 1] processed 1280/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 1] processed 1920/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 1] processed 2560/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 1] processed 3200/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 1] done in 8.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 2] processed 640/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 2] processed 1280/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 2] processed 1920/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 2] processed 2560/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 2] processed 3200/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 2] done in 8.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 3] processed 640/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 3] processed 1280/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 3] processed 1920/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 3] processed 2560/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 3] processed 3200/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 3] done in 8.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 4] processed 640/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 4] processed 1280/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 4] processed 1920/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 4] processed 2560/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 4] processed 3200/3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer fold 4] done in 8.2s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'preds_test_cnn' is not defined",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 209\u001b[39m\n\u001b[32m    207\u001b[39m out = np.zeros(\u001b[38;5;28mlen\u001b[39m(sample), dtype=np.float32)\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, img_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sample[\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m].values):\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     out[i] = \u001b[43mpreds_test_cnn\u001b[49m[id_to_idx[img_id]]\n\u001b[32m    210\u001b[39m sample[\u001b[33m'\u001b[39m\u001b[33mhas_cactus\u001b[39m\u001b[33m'\u001b[39m] = out\n\u001b[32m    211\u001b[39m sample.to_csv(\u001b[33m'\u001b[39m\u001b[33msubmission.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[31mNameError\u001b[39m: name 'preds_test_cnn' is not defined"
          ]
        }
      ]
    },
    {
      "id": "b1adffc6-cfc0-4463-af9a-bacfd2dc0d31",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inference-only: load saved ResNet18 fold weights, run 8-way TTA, write submission.csv\n",
        "import numpy as np, pandas as pd, torch, timm, time\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "IMG_SIZE = 64\n",
        "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
        "IMAGENET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
        "test_dir = Path('test')\n",
        "test_ids = sorted([p.name for p in test_dir.glob('*.jpg')])\n",
        "\n",
        "def load_image_uint8(path: Path):\n",
        "    with Image.open(path) as im:\n",
        "        im = im.convert('RGB')\n",
        "        arr = np.asarray(im, dtype=np.uint8)\n",
        "    return arr\n",
        "\n",
        "def resize_norm_to_tensor(arr_uint8: np.ndarray) -> torch.Tensor:\n",
        "    arr = np.array(Image.fromarray(arr_uint8).resize((IMG_SIZE, IMG_SIZE), resample=Image.BILINEAR), dtype=np.float32) / 255.0\n",
        "    arr = (arr - IMAGENET_MEAN) / IMAGENET_STD\n",
        "    arr = np.transpose(arr, (2,0,1))\n",
        "    return torch.from_numpy(arr)\n",
        "\n",
        "def dihedral8(arr: np.ndarray):\n",
        "    outs = []\n",
        "    for k in range(4):\n",
        "        r = np.rot90(arr, k); outs.append(r); outs.append(np.fliplr(r))\n",
        "    return outs\n",
        "\n",
        "def build_model():\n",
        "    return timm.create_model('resnet18', pretrained=False, num_classes=1, in_chans=3, global_pool='avg')\n",
        "\n",
        "def predict_test_fold(fold):\n",
        "    model = build_model().to(device)\n",
        "    sd = torch.load(f'resnet18_fold{fold}.pt', map_location=device)\n",
        "    model.load_state_dict(sd); model.eval()\n",
        "    preds = np.zeros(len(test_ids), dtype=np.float32)\n",
        "    t0 = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(test_ids), 64):\n",
        "            batch_ids = test_ids[i:i+64]\n",
        "            tensors = []\n",
        "            for img_id in batch_ids:\n",
        "                arr = load_image_uint8(test_dir / img_id)\n",
        "                for v in dihedral8(arr):\n",
        "                    tensors.append(resize_norm_to_tensor(v))\n",
        "            xb = torch.stack(tensors).to(device, non_blocking=True)\n",
        "            logits = model(xb).view(len(batch_ids), 8, 1).squeeze(2)\n",
        "            m = torch.sigmoid(logits).mean(dim=1).detach().cpu().numpy()\n",
        "            preds[i:i+len(batch_ids)] = m\n",
        "    return preds\n",
        "\n",
        "print('[infer] Starting TTA inference over 5 folds...')\n",
        "fold_preds = []\n",
        "for f in range(5):\n",
        "    print(f'[infer] fold {f} ...', flush=True)\n",
        "    fold_preds.append(predict_test_fold(f))\n",
        "pred_test_cnn = np.mean(np.stack(fold_preds, axis=0), axis=0).astype(np.float32)\n",
        "\n",
        "sample = pd.read_csv('sample_submission.csv')\n",
        "id_to_idx = {img_id:i for i, img_id in enumerate(test_ids)}\n",
        "out = np.zeros(len(sample), dtype=np.float32)\n",
        "for i, img_id in enumerate(sample['id'].values):\n",
        "    out[i] = pred_test_cnn[id_to_idx[img_id]]\n",
        "sample['has_cactus'] = out\n",
        "sample.to_csv('submission.csv', index=False)\n",
        "print('[infer] Saved submission.csv | stats:', float(out.min()), float(out.max()), float(out.mean()))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer] Starting TTA inference over 5 folds...\n[infer] fold 0 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/3913325588.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  sd = torch.load(f'resnet18_fold{fold}.pt', map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer] fold 1 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer] fold 2 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer] fold 3 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer] fold 4 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[infer] Saved submission.csv | stats: 1.29140653370996e-05 0.9999843835830688 0.7549147009849548\n"
          ]
        }
      ]
    },
    {
      "id": "20591cdb-d8fa-4a71-912c-2d35ae8a7346",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# kNN booster on ResNet18 embeddings: override high-similarity cases, then write submission.csv\n",
        "import numpy as np, pandas as pd, torch, timm, time\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import faiss\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "IMG_SIZE = 64\n",
        "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
        "IMAGENET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
        "train_df = pd.read_csv('train.csv')\n",
        "train_dir = Path('train'); test_dir = Path('test')\n",
        "test_ids = sorted([p.name for p in test_dir.glob('*.jpg')])\n",
        "\n",
        "def load_image_uint8(path: Path):\n",
        "    with Image.open(path) as im:\n",
        "        im = im.convert('RGB')\n",
        "        arr = np.asarray(im, dtype=np.uint8)\n",
        "    return arr\n",
        "\n",
        "def resize_norm_to_tensor(arr_uint8: np.ndarray) -> torch.Tensor:\n",
        "    arr = np.array(Image.fromarray(arr_uint8).resize((IMG_SIZE, IMG_SIZE), resample=Image.BILINEAR), dtype=np.float32) / 255.0\n",
        "    arr = (arr - IMAGENET_MEAN) / IMAGENET_STD\n",
        "    arr = np.transpose(arr, (2,0,1))\n",
        "    return torch.from_numpy(arr)\n",
        "\n",
        "class ImgDS(Dataset):\n",
        "    def __init__(self, ids, folder: Path):\n",
        "        self.ids = ids; self.folder = folder\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.ids[idx]\n",
        "        arr = load_image_uint8(self.folder / img_id)\n",
        "        x = resize_norm_to_tensor(arr).float()\n",
        "        return x, img_id\n",
        "\n",
        "def build_model_backbone():\n",
        "    # ResNet18 from timm; we'll use forward_features + global_pool to get embeddings\n",
        "    model = timm.create_model('resnet18', pretrained=False, num_classes=1, in_chans=3, global_pool='avg')\n",
        "    return model\n",
        "\n",
        "def extract_embeddings(ids, folder: Path, batch_size=256):\n",
        "    ds = ImgDS(ids, folder)\n",
        "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "    embs_accum = None\n",
        "    for fold in range(5):\n",
        "        model = build_model_backbone().to(device)\n",
        "        sd = torch.load(f'resnet18_fold{fold}.pt', map_location=device)\n",
        "        model.load_state_dict(sd)\n",
        "        model.eval()\n",
        "        feats = []\n",
        "        with torch.no_grad():\n",
        "            for xb, _ in dl:\n",
        "                xb = xb.to(device, non_blocking=True)\n",
        "                # forward_features -> [N, C, H, W]; then global_pool -> [N, C]\n",
        "                ff = model.forward_features(xb)\n",
        "                pooled = model.global_pool(ff)  # [N, C]\n",
        "                feats.append(pooled.detach().cpu().numpy())\n",
        "        feats = np.concatenate(feats, axis=0)\n",
        "        if embs_accum is None:\n",
        "            embs_accum = feats.astype(np.float32)\n",
        "        else:\n",
        "            embs_accum += feats.astype(np.float32)\n",
        "        del model; torch.cuda.empty_cache()\n",
        "    embs = (embs_accum / 5.0).astype(np.float32)\n",
        "    # L2 normalize for cosine similarity via inner product\n",
        "    norms = np.linalg.norm(embs, axis=1, keepdims=True) + 1e-8\n",
        "    embs = embs / norms\n",
        "    return embs\n",
        "\n",
        "t0 = time.time()\n",
        "print('[knn] Extracting train embeddings...')\n",
        "train_ids = train_df['id'].values\n",
        "train_embs = extract_embeddings(train_ids, train_dir, batch_size=256)\n",
        "y_train = train_df['has_cactus'].values.astype(np.int32)\n",
        "print('[knn] Train embeddings:', train_embs.shape)\n",
        "\n",
        "print('[knn] Building FAISS index (cosine via inner product)...')\n",
        "d = train_embs.shape[1]\n",
        "index = faiss.IndexFlatIP(d)\n",
        "index.add(train_embs)\n",
        "print('[knn] Index size:', index.ntotal)\n",
        "\n",
        "print('[knn] Extracting test embeddings...')\n",
        "test_embs = extract_embeddings(test_ids, test_dir, batch_size=256)\n",
        "print('[knn] Test embeddings:', test_embs.shape)\n",
        "\n",
        "print('[knn] Searching nearest neighbor for all test samples...')\n",
        "D, I = index.search(test_embs, 1)  # top-1\n",
        "sims = D.flatten().astype(np.float32)\n",
        "nns = I.flatten().astype(np.int32)\n",
        "nn_labels = y_train[nns]\n",
        "\n",
        "# Load current CNN predictions from submission as base\n",
        "sample = pd.read_csv('sample_submission.csv')\n",
        "base = pd.read_csv('submission.csv') if Path('submission.csv').exists() else None\n",
        "if base is None or base.shape[0] != sample.shape[0]:\n",
        "    # fallback: uniform 0.5 if missing\n",
        "    base_probs = np.full(len(sample), 0.5, dtype=np.float32)\n",
        "else:\n",
        "    # assume same order as sample per our earlier pipeline\n",
        "    base_probs = base['has_cactus'].values.astype(np.float32)\n",
        "\n",
        "# Map test_ids to index in our arrays\n",
        "id_to_pos = {img_id:i for i, img_id in enumerate(test_ids)}\n",
        "out = np.zeros(len(sample), dtype=np.float32)\n",
        "overrides = 0\n",
        "THRESH = 0.99  # high-confidence cosine similarity threshold\n",
        "for i, img_id in enumerate(sample['id'].values):\n",
        "    j = id_to_pos[img_id]\n",
        "    if sims[j] >= THRESH:\n",
        "        out[i] = 1.0 if nn_labels[j] == 1 else 0.0\n",
        "        overrides += 1\n",
        "    else:\n",
        "        out[i] = base_probs[i]\n",
        "\n",
        "sample['has_cactus'] = out\n",
        "sample.to_csv('submission.csv', index=False)\n",
        "print(f\"[knn] Saved submission.csv | overrides {overrides}/{len(sample)} | thresh {THRESH} | time {time.time()-t0:.1f}s\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[knn] Extracting train embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/1748625614.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  sd = torch.load(f'resnet18_fold{fold}.pt', map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[knn] Train embeddings: (14175, 512)\n[knn] Building FAISS index (cosine via inner product)...\n[knn] Index size: 14175\n[knn] Extracting test embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[knn] Test embeddings: (3325, 512)\n[knn] Searching nearest neighbor for all test samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[knn] Saved submission.csv | overrides 357/3325 | thresh 0.99 | time 19.9s\n"
          ]
        }
      ]
    },
    {
      "id": "d63f69af-4647-4059-8c37-5e07b7bb0d19",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# EfficientNet-B0 64x64 5-fold + ensemble with ResNet18 + kNN booster; writes final submission.csv\n",
        "import time, copy, gc, random\n",
        "import numpy as np, pandas as pd, torch, timm\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import faiss\n",
        "\n",
        "SEED = 42\n",
        "IMG_SIZE = 64\n",
        "NFOLDS = 5\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 384\n",
        "MAX_LR = 2e-3\n",
        "WEIGHT_DECAY = 1e-4\n",
        "PATIENCE = 3\n",
        "NUM_WORKERS = 4\n",
        "\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_ids = sorted([p.name for p in Path('test').glob('*.jpg')])\n",
        "train_dir = Path('train'); test_dir = Path('test')\n",
        "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
        "IMAGENET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
        "\n",
        "def load_image_uint8(path: Path):\n",
        "    with Image.open(path) as im:\n",
        "        im = im.convert('RGB')\n",
        "        arr = np.asarray(im, dtype=np.uint8)\n",
        "    return arr\n",
        "\n",
        "def resize_norm_to_tensor(arr_uint8: np.ndarray) -> torch.Tensor:\n",
        "    arr = np.array(Image.fromarray(arr_uint8).resize((IMG_SIZE, IMG_SIZE), resample=Image.BILINEAR), dtype=np.float32) / 255.0\n",
        "    arr = (arr - IMAGENET_MEAN) / IMAGENET_STD\n",
        "    arr = np.transpose(arr, (2,0,1))\n",
        "    return torch.from_numpy(arr)\n",
        "\n",
        "def dihedral8(arr: np.ndarray):\n",
        "    outs = []\n",
        "    for k in range(4):\n",
        "        r = np.rot90(arr, k); outs.append(r); outs.append(np.fliplr(r))\n",
        "    return outs\n",
        "\n",
        "class CactusDS(Dataset):\n",
        "    def __init__(self, ids, labels=None, train=True):\n",
        "        self.ids = ids; self.labels = labels; self.train = train\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.ids[idx]\n",
        "        folder = train_dir if self.labels is not None else test_dir\n",
        "        arr = load_image_uint8(folder / img_id)\n",
        "        # light aug: random dihedral\n",
        "        if self.train:\n",
        "            if random.random() < 0.5: arr = np.fliplr(arr)\n",
        "            if random.random() < 0.5: arr = np.flipud(arr)\n",
        "            k = random.randint(0,3);\n",
        "            if k: arr = np.rot90(arr, k)\n",
        "        x = resize_norm_to_tensor(arr).float()\n",
        "        if self.labels is not None:\n",
        "            return x, torch.tensor(float(self.labels[idx]), dtype=torch.float32)\n",
        "        return x, img_id\n",
        "\n",
        "def build_resnet18():\n",
        "    return timm.create_model('resnet18', pretrained=False, num_classes=1, in_chans=3, global_pool='avg')\n",
        "\n",
        "def build_effb0():\n",
        "    return timm.create_model('efficientnet_b0', pretrained=True, num_classes=1, in_chans=3, global_pool='avg')\n",
        "\n",
        "def train_effb0_one_fold(fold, trn_idx, val_idx):\n",
        "    x_tr = train_df['id'].values[trn_idx]; y_tr = train_df['has_cactus'].values[trn_idx]\n",
        "    x_va = train_df['id'].values[val_idx]; y_va = train_df['has_cactus'].values[val_idx]\n",
        "    dl_tr = DataLoader(CactusDS(x_tr, y_tr, train=True), batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n",
        "    dl_va = DataLoader(CactusDS(x_va, y_va, train=False), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "    model = build_effb0().to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=MAX_LR, weight_decay=WEIGHT_DECAY)\n",
        "    steps_per_epoch = max(1, len(dl_tr))\n",
        "    sch = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=MAX_LR, epochs=EPOCHS, steps_per_epoch=steps_per_epoch, pct_start=0.3, div_factor=10.0, final_div_factor=10.0)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    best_auc, best_state, no_imp = -1.0, None, 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train(); total=0.0; n=0; t0=time.time()\n",
        "        for it,(xb,yb) in enumerate(dl_tr):\n",
        "            xb=xb.to(device, non_blocking=True); yb=yb.to(device, non_blocking=True).view(-1,1)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "                logits = model(xb); loss = loss_fn(logits, yb)\n",
        "            scaler.scale(loss).backward(); scaler.step(opt); scaler.update(); sch.step()\n",
        "            total += loss.item()*xb.size(0); n += xb.size(0)\n",
        "        # val\n",
        "        model.eval(); preds=[]; gts=[]\n",
        "        with torch.no_grad():\n",
        "            for xb,yb in dl_va:\n",
        "                xb=xb.to(device, non_blocking=True); logits=model(xb)\n",
        "                preds.append(torch.sigmoid(logits).squeeze(1).detach().cpu().numpy()); gts.append(yb.numpy())\n",
        "        preds=np.concatenate(preds); gts=np.concatenate(gts); auc=roc_auc_score(gts,preds)\n",
        "        print(f'[eff fold {fold}] epoch {epoch} AUC {auc:.6f} loss {total/max(1,n):.4f} time {time.time()-t0:.1f}s')\n",
        "        if auc>best_auc+1e-5: best_auc=auc; best_state=copy.deepcopy(model.state_dict()); no_imp=0\n",
        "        else:\n",
        "            no_imp+=1\n",
        "            if no_imp>=PATIENCE: print(f'[eff fold {fold}] early stop at epoch {epoch} | best_auc {best_auc:.6f}'); break\n",
        "    model.load_state_dict(best_state); torch.save(model.state_dict(), f'effb0_fold{fold}.pt')\n",
        "    # OOF for record\n",
        "    model.eval(); oof=np.zeros(len(val_idx), dtype=np.float32)\n",
        "    with torch.no_grad():\n",
        "        k=0\n",
        "        for xb,yb in DataLoader(CactusDS(x_va, y_va, train=False), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True):\n",
        "            xb=xb.to(device, non_blocking=True); p=torch.sigmoid(model(xb)).squeeze(1).detach().cpu().numpy();\n",
        "            oof[k:k+len(p)]=p; k+=len(p)\n",
        "    return best_auc, oof\n",
        "\n",
        "def predict_tta(model_builder, weight_tpl):\n",
        "    preds = np.zeros(len(test_ids), dtype=np.float32)\n",
        "    with torch.no_grad():\n",
        "        for fold in range(NFOLDS):\n",
        "            model = model_builder().to(device); sd = torch.load(weight_tpl.format(fold), map_location=device); model.load_state_dict(sd); model.eval()\n",
        "            fold_preds = np.zeros(len(test_ids), dtype=np.float32)\n",
        "            for i in range(0, len(test_ids), 64):\n",
        "                batch_ids = test_ids[i:i+64]\n",
        "                tensors = []\n",
        "                for img_id in batch_ids:\n",
        "                    arr = load_image_uint8(test_dir / img_id)\n",
        "                    for v in dihedral8(arr):\n",
        "                        tensors.append(resize_norm_to_tensor(v))\n",
        "                xb = torch.stack(tensors).to(device, non_blocking=True)\n",
        "                logits = model(xb).view(len(batch_ids), 8, 1).squeeze(2)\n",
        "                m = torch.sigmoid(logits).mean(dim=1).detach().cpu().numpy()\n",
        "                fold_preds[i:i+len(batch_ids)] = m\n",
        "            preds += fold_preds / NFOLDS\n",
        "            del model; torch.cuda.empty_cache()\n",
        "    return preds.astype(np.float32)\n",
        "\n",
        "# Train EfficientNet-B0 5-fold\n",
        "skf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
        "oof_eff = np.zeros(len(train_df), dtype=np.float32); aucs=[]\n",
        "for fold,(trn_idx,val_idx) in enumerate(skf.split(train_df['id'].values, train_df['has_cactus'].values)):\n",
        "    print(f'===== EffNet Fold {fold} =====')\n",
        "    auc, oof_f = train_effb0_one_fold(fold, trn_idx, val_idx)\n",
        "    oof_eff[val_idx]=oof_f; aucs.append(auc); gc.collect(); torch.cuda.empty_cache()\n",
        "print(f'[EffNet CV] OOF {roc_auc_score(train_df.has_cactus.values, oof_eff):.6f} | per-fold {aucs}')\n",
        "\n",
        "# Inference: EfficientNet-B0 and ResNet18\n",
        "print('[infer] EfficientNet-B0 TTA...')\n",
        "pred_eff = predict_tta(lambda: timm.create_model('efficientnet_b0', pretrained=False, num_classes=1, in_chans=3, global_pool='avg'), 'effb0_fold{}.pt')\n",
        "print('[infer] ResNet18 TTA...')\n",
        "pred_res = predict_tta(build_resnet18, 'resnet18_fold{}.pt')\n",
        "\n",
        "# Ensemble average\n",
        "pred_ens = ((pred_eff + pred_res) * 0.5).astype(np.float32)\n",
        "sample = pd.read_csv('sample_submission.csv')\n",
        "id_to_idx = {img_id:i for i, img_id in enumerate(test_ids)}\n",
        "base_probs = np.zeros(len(sample), dtype=np.float32)\n",
        "for i, img_id in enumerate(sample['id'].values):\n",
        "    base_probs[i] = pred_ens[id_to_idx[img_id]]\n",
        "\n",
        "# kNN booster on ResNet18 embeddings (averaged across folds)\n",
        "def extract_resnet_embs(ids, folder: Path, batch_size=256):\n",
        "    class ImgDS(Dataset):\n",
        "        def __init__(self, ids, folder): self.ids=ids; self.folder=folder\n",
        "        def __len__(self): return len(self.ids)\n",
        "        def __getitem__(self, idx):\n",
        "            img = load_image_uint8(self.folder/self.ids[idx]); return resize_norm_to_tensor(img).float(), 0\n",
        "    dl = DataLoader(ImgDS(ids, folder), batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "    embs_accum=None\n",
        "    for fold in range(NFOLDS):\n",
        "        m = build_resnet18().to(device); sd = torch.load(f'resnet18_fold{fold}.pt', map_location=device); m.load_state_dict(sd); m.eval()\n",
        "        feats=[]\n",
        "        with torch.no_grad():\n",
        "            for xb,_ in dl:\n",
        "                xb=xb.to(device, non_blocking=True); ff = m.forward_features(xb); pooled = m.global_pool(ff); feats.append(pooled.detach().cpu().numpy())\n",
        "        feats=np.concatenate(feats, axis=0).astype(np.float32)\n",
        "        embs_accum = feats if embs_accum is None else embs_accum+feats\n",
        "        del m; torch.cuda.empty_cache()\n",
        "    embs = (embs_accum / NFOLDS).astype(np.float32)\n",
        "    norms = np.linalg.norm(embs, axis=1, keepdims=True) + 1e-8\n",
        "    return embs / norms\n",
        "\n",
        "print('[knn] Extract embeddings for kNN booster...')\n",
        "train_ids = train_df['id'].values\n",
        "train_embs = extract_resnet_embs(train_ids, train_dir, batch_size=256)\n",
        "test_embs = extract_resnet_embs(test_ids, test_dir, batch_size=256)\n",
        "index = faiss.IndexFlatIP(train_embs.shape[1]); index.add(train_embs)\n",
        "D,I = index.search(test_embs, 1)\n",
        "sims = D.flatten().astype(np.float32); nns = I.flatten().astype(np.int32); y_train = train_df['has_cactus'].values.astype(np.int32)\n",
        "\n",
        "out = base_probs.copy(); overrides=0; THRESH=0.99\n",
        "pos_map = {img_id:i for i,img_id in enumerate(test_ids)}\n",
        "for i, img_id in enumerate(sample['id'].values):\n",
        "    j = pos_map[img_id]\n",
        "    if sims[j] >= THRESH:\n",
        "        out[i] = 1.0 if y_train[nns[j]]==1 else 0.0; overrides += 1\n",
        "sample['has_cactus'] = out\n",
        "sample.to_csv('submission.csv', index=False)\n",
        "print(f'[final] Saved submission.csv | ensemble+knn overrides {overrides}/{len(sample)} | min {float(out.min())} max {float(out.max())} mean {float(out.mean())}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== EffNet Fold 0 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/1462940409.py:84: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_73/1462940409.py:92: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}