{
  "cells": [
    {
      "id": "b773dc9b-4dbc-4366-aaca-4105c88f2734",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan: TensorFlow Speech Recognition Challenge\n",
        "\n",
        "Objectives:\n",
        "- Build a robust, GPU-accelerated audio pipeline (log-mel spectrograms) with strong CV.\n",
        "- Ship a fast baseline ASAP; iterate to medal via FE+augment+ensembling if time allows.\n",
        "\n",
        "Data understanding:\n",
        "- Repo contains train/ and test/ directories; sample_submission.csv provides ID/label format.\n",
        "- Labels will be exactly the 12 classes: 10 target words + unknown + silence. Map all non-target folders to unknown; generate silence from _background_noise_.\n",
        "\n",
        "Validation:\n",
        "- Use StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42) with groups=speaker_id (filename prefix before \"_nohash_\") and y=label.\n",
        "- Deterministic seed; cache fold splits and features.\n",
        "- Fit normalization on train-fold only (no global stats).\n",
        "\n",
        "Baseline v1 (fast & strong):\n",
        "- Precompute log-mel spectrograms (SR=16k mono): n_fft=512, win=400 (25ms), hop=160 (10ms), n_mels=64, fmin\u224820, fmax=8000; log(mel+1e-6).\n",
        "- Normalization: per-utterance z-norm or train-fold CMVN (per-frequency).\n",
        "- Backbone: ResNet18 2D (1 input channel), global avg pool, dropout 0.2, label smoothing 0.05.\n",
        "- Optimizer: AdamW (lr=3e-3, wd=1e-4), cosine LR 30 epochs, warmup 1\u20132, early stopping on val acc.\n",
        "- Augmentations: time shift \u00b1120ms, background noise mixing (SNR 5\u201320 dB), SpecAugment (2\u00d7T=24, 2\u00d7F=10), mixup \u03b1=0.3 p=0.5; optional speed 0.9\u20131.1.\n",
        "\n",
        "Improvements:\n",
        "- Tune mel params (64\u2192128 mels), second config diversity; TTA with time shifts.\n",
        "- Add EfficientNet-B0 or second seed; average logits across folds/models.\n",
        "\n",
        "Pipeline steps:\n",
        "1) Environment check: GPU availability and versions.\n",
        "2) Data inventory: list classes, derive label mapping (targets, unknown, silence), extract speaker_id.\n",
        "3) Feature builder: precompute/cached log-mels to .npy/.pt (float16) for train/test.\n",
        "4) Grouped stratified 5-fold training with progress logging; save OOF + per-fold test logits.\n",
        "5) Inference: fold-average logits; optional TTA; create submission.csv with exact label order.\n",
        "6) Error analysis on OOF; iterate.\n",
        "\n",
        "Risks & mitigations:\n",
        "- CV/LB mismatch: enforce speaker-grouped stratification.\n",
        "- Label traps (unknown/silence): strict mapping and consistent proportions.\n",
        "- Leakage: fit normalization per train-fold only.\n",
        "- Slow I/O: cache spectrograms; DataLoader tuning.\n",
        "\n",
        "Next actions:\n",
        "- Run environment check and data inventory (including sample_submission labels).\n",
        "- Implement fast spectrogram extractor and ResNet18 baseline.\n",
        "- Establish and save folds; smoke test on 1\u20132 folds, then full 5-fold.\n",
        "- After baseline, request expert review for targeted improvements."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "74a21930-32b7-4fa5-b89c-6d3a1c21f163",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment & Data Inventory\n",
        "import os, sys, subprocess, time, json, math, random, glob, re\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\")\n",
        "\n",
        "# Torch install/check\n",
        "log(\"Checking PyTorch and GPU...\")\n",
        "try:\n",
        "    import torch\n",
        "except Exception as e:\n",
        "    log(f\"PyTorch not found, installing... ({e})\")\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'torch==2.4.0', 'torchvision==0.19.0', 'torchaudio==2.4.0', '--index-url', 'https://download.pytorch.org/whl/cu121'], check=True)\n",
        "    import torch\n",
        "\n",
        "gpu_available = torch.cuda.is_available()\n",
        "log(f\"GPU Available: {gpu_available}\")\n",
        "if gpu_available:\n",
        "    log(f\"GPU Count: {torch.cuda.device_count()}\")\n",
        "    log(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    log(f\"GPU Memory: {props.total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "ROOT = Path('.')\n",
        "TRAIN_DIR = ROOT / 'train'\n",
        "TEST_DIR = ROOT / 'test'\n",
        "SAMPLE_SUB = ROOT / 'sample_submission.csv'\n",
        "\n",
        "log(\"Listing directories...\")\n",
        "log(f\"CWD: {ROOT.resolve()}\")\n",
        "log(f\"Train exists: {TRAIN_DIR.exists()} | Test exists: {TEST_DIR.exists()} | sample_submission exists: {SAMPLE_SUB.exists()}\")\n",
        "\n",
        "# Read sample_submission (to confirm format/labels)\n",
        "if SAMPLE_SUB.exists():\n",
        "    df_ss = pd.read_csv(SAMPLE_SUB)\n",
        "    log(f\"sample_submission shape: {df_ss.shape}\")\n",
        "    log(\"sample_submission head:\")\n",
        "    print(df_ss.head(3))\n",
        "else:\n",
        "    df_ss = None\n",
        "\n",
        "# Inspect train structure\n",
        "log(\"Scanning train folders...\")\n",
        "train_classes = []\n",
        "if TRAIN_DIR.exists():\n",
        "    for p in sorted(TRAIN_DIR.iterdir()):\n",
        "        if p.is_dir():\n",
        "            train_classes.append(p.name)\n",
        "\n",
        "TARGET_WORDS = ['yes','no','up','down','left','right','on','off','stop','go']\n",
        "log(f\"Found {len(train_classes)} train folders: {train_classes}\")\n",
        "bg_folder = '_background_noise_'\n",
        "has_bg = bg_folder in train_classes\n",
        "log(f\"Background noise folder present: {has_bg}\")\n",
        "\n",
        "# Count wav files per folder\n",
        "folder_counts = {}\n",
        "total_files = 0\n",
        "for cls in train_classes:\n",
        "    wavs = glob.glob(str(TRAIN_DIR / cls / '*.wav'))\n",
        "    folder_counts[cls] = len(wavs)\n",
        "    total_files += len(wavs)\n",
        "log(f\"Total train wav files: {total_files}\")\n",
        "top_counts = sorted(folder_counts.items(), key=lambda x: -x[1])[:10]\n",
        "log(f\"Top folders by count: {top_counts}\")\n",
        "\n",
        "# Speaker id helper\n",
        "speaker_re = re.compile(r'^(?P<spk>[^_]+)_nohash_')\n",
        "def get_speaker_id(fname: str) -> str:\n",
        "    m = speaker_re.match(Path(fname).stem)\n",
        "    return m.group('spk') if m else Path(fname).stem.split('_')[0]\n",
        "\n",
        "# Sample a few files and extract speaker ids\n",
        "sample_files = []\n",
        "for cls in train_classes:\n",
        "    sample_files.extend(glob.glob(str(TRAIN_DIR / cls / '*.wav'))[:3])\n",
        "log(f\"Sampling {len(sample_files)} files for speaker_id parsing...\")\n",
        "spk_samples = [(Path(f).name, get_speaker_id(f)) for f in sample_files[:10]]\n",
        "print(spk_samples)\n",
        "\n",
        "# Test dir inventory\n",
        "log(\"Scanning test set...\")\n",
        "test_files = glob.glob(str(TEST_DIR / '**' / '*.wav')) if TEST_DIR.exists() else []\n",
        "log(f\"Total test wav files: {len(test_files)}\")\n",
        "print(test_files[:5])\n",
        "\n",
        "# Summarize mapping plan\n",
        "non_target = sorted([c for c in train_classes if c not in TARGET_WORDS and c != bg_folder])\n",
        "log(f\"Non-target folders (to be mapped to 'unknown'): count={len(non_target)}\")\n",
        "print(non_target[:20])\n",
        "\n",
        "log(\"Environment & data inventory complete.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05:06:48] Checking PyTorch and GPU...\n[05:06:48] PyTorch not found, installing... (No module named 'torch')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05:07:43] GPU Available: False\n[05:07:43] Listing directories...\n[05:07:43] CWD: /var/lib/simon/agent_run_states/tensorflow-speech-recognition-challenge-20250922-045634\n[05:07:43] Train exists: True | Test exists: True | sample_submission exists: True\n[05:07:43] sample_submission shape: (6473, 2)\n[05:07:43] sample_submission head:\n               fname    label\n0  clip_00000000.wav  silence\n1  clip_00000001.wav  silence\n2  clip_00000002.wav  silence\n[05:07:43] Scanning train folders...\n[05:07:43] Found 1 train folders: ['audio']\n[05:07:43] Background noise folder present: False\n[05:07:43] Total train wav files: 0\n[05:07:43] Top folders by count: [('audio', 0)]\n[05:07:43] Sampling 0 files for speaker_id parsing...\n[]\n[05:07:43] Scanning test set...\n[05:07:43] Total test wav files: 6473\n['test/audio/clip_00002578.wav', 'test/audio/clip_00001415.wav', 'test/audio/clip_00004142.wav', 'test/audio/clip_00002943.wav', 'test/audio/clip_00000977.wav']\n[05:07:43] Non-target folders (to be mapped to 'unknown'): count=1\n['audio']\n[05:07:43] Environment & data inventory complete.\n"
          ]
        }
      ]
    },
    {
      "id": "04ebd835-21e3-4fc2-8a7a-d9f1810b47e2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Deep inventory: list files under root/train/test and look for metadata\n",
        "import os\n",
        "from pathlib import Path\n",
        "import itertools\n",
        "\n",
        "def list_dir(path, max_entries=50):\n",
        "    p = Path(path)\n",
        "    entries = []\n",
        "    if not p.exists():\n",
        "        return entries\n",
        "    for i, fp in enumerate(p.rglob('*')):\n",
        "        if i >= max_entries:\n",
        "            break\n",
        "        entries.append(str(fp))\n",
        "    return entries\n",
        "\n",
        "print('Root files:')\n",
        "for f in sorted(Path('.').glob('*')):\n",
        "    print(' -', f)\n",
        "\n",
        "print('\\nAny CSV/NPY/NPZ/JSON in root:')\n",
        "for ext in ['*.csv','*.npy','*.npz','*.json','*.parquet']:\n",
        "    for f in sorted(Path('.').glob(ext)):\n",
        "        print(' -', f)\n",
        "\n",
        "print('\\nTop files under train/:')\n",
        "for fp in list_dir('train', max_entries=200):\n",
        "    print(' -', fp)\n",
        "\n",
        "print('\\nTop files under test/:')\n",
        "for fp in list_dir('test', max_entries=50):\n",
        "    print(' -', fp)\n",
        "\n",
        "print('\\nLook for metadata files anywhere:')\n",
        "meta_patterns = ['*train*.csv','*labels*.csv','*meta*.csv','*.parquet','*train*.json']\n",
        "for pat in meta_patterns:\n",
        "    for f in sorted(Path('.').rglob(pat)):\n",
        "        print(' -', f)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root files:\n - .00_eda_and_planning_kernel_state.json\n - 00_eda_and_planning.ipynb\n - agent_metadata\n - description.md\n - docker_run.log\n - requirements.txt\n - sample_submission.csv\n - submission.csv\n - task.txt\n - test\n - train\n\nAny CSV/NPY/NPZ/JSON in root:\n - sample_submission.csv\n - submission.csv\n - .00_eda_and_planning_kernel_state.json\n\nTop files under train/:\n - train/audio\n - train/audio/wow\n - train/audio/zero\n - train/audio/up\n - train/audio/seven\n - train/audio/_background_noise_\n - train/audio/no\n - train/audio/five\n - train/audio/dog\n - train/audio/eight\n - train/audio/sheila\n - train/audio/bed\n - train/audio/two\n - train/audio/six\n - train/audio/marvin\n - train/audio/left\n - train/audio/down\n - train/audio/stop\n - train/audio/happy\n - train/audio/on\n - train/audio/off\n - train/audio/house\n - train/audio/right\n - train/audio/four\n - train/audio/yes\n - train/audio/go\n - train/audio/nine\n - train/audio/cat\n - train/audio/bird\n - train/audio/tree\n - train/audio/three\n - train/audio/one\n - train/audio/wow/d107dc42_nohash_0.wav\n - train/audio/wow/12529547_nohash_0.wav\n - train/audio/wow/e4a2cf79_nohash_0.wav\n - train/audio/wow/7846fd85_nohash_0.wav\n - train/audio/wow/e6db3894_nohash_0.wav\n - train/audio/wow/5e3b7a84_nohash_1.wav\n - train/audio/wow/3df9a3d4_nohash_0.wav\n - train/audio/wow/0cd323ec_nohash_1.wav\n - train/audio/wow/5a3712c9_nohash_0.wav\n - train/audio/wow/84bf12ff_nohash_0.wav\n - train/audio/wow/6c968bd9_nohash_0.wav\n - train/audio/wow/30f31e42_nohash_0.wav\n - train/audio/wow/b544d4fd_nohash_1.wav\n - train/audio/wow/1c1060b1_nohash_0.wav\n - train/audio/wow/40115b19_nohash_0.wav\n - train/audio/wow/105e72bb_nohash_0.wav\n - train/audio/wow/016e2c6d_nohash_1.wav\n - train/audio/wow/3dfd6c23_nohash_0.wav\n - train/audio/wow/59c3a7f2_nohash_0.wav\n - train/audio/wow/ecbd8d66_nohash_0.wav\n - train/audio/wow/7c1d8533_nohash_0.wav\n - train/audio/wow/8ff44869_nohash_1.wav\n - train/audio/wow/a7dd45cf_nohash_0.wav\n - train/audio/wow/5184ed3e_nohash_0.wav\n - train/audio/wow/64da5281_nohash_0.wav\n - train/audio/wow/fb9d6d23_nohash_0.wav\n - train/audio/wow/43f57297_nohash_1.wav\n - train/audio/wow/d3badc9a_nohash_0.wav\n - train/audio/wow/6a203e0e_nohash_0.wav\n - train/audio/wow/541120c7_nohash_0.wav\n - train/audio/wow/2b3f509b_nohash_0.wav\n - train/audio/wow/14775481_nohash_1.wav\n - train/audio/wow/72d75d96_nohash_1.wav\n - train/audio/wow/85b877b5_nohash_0.wav\n - train/audio/wow/3824c00e_nohash_0.wav\n - train/audio/wow/4d9e07cf_nohash_0.wav\n - train/audio/wow/d91a159e_nohash_0.wav\n - train/audio/wow/d1453a87_nohash_0.wav\n - train/audio/wow/39c13eed_nohash_3.wav\n - train/audio/wow/0a7c2a8d_nohash_3.wav\n - train/audio/wow/f292725f_nohash_1.wav\n - train/audio/wow/21307344_nohash_0.wav\n - train/audio/wow/f5733968_nohash_0.wav\n - train/audio/wow/44bc77f7_nohash_1.wav\n - train/audio/wow/09ddc105_nohash_0.wav\n - train/audio/wow/3367cff6_nohash_0.wav\n - train/audio/wow/6e74c582_nohash_0.wav\n - train/audio/wow/facd97c0_nohash_2.wav\n - train/audio/wow/da4ef063_nohash_0.wav\n - train/audio/wow/e41e41f7_nohash_1.wav\n - train/audio/wow/f192e6b4_nohash_0.wav\n - train/audio/wow/9f22307d_nohash_0.wav\n - train/audio/wow/a0f93943_nohash_1.wav\n - train/audio/wow/9229bff9_nohash_0.wav\n - train/audio/wow/0a7c2a8d_nohash_2.wav\n - train/audio/wow/6a861f21_nohash_1.wav\n - train/audio/wow/129c7d8d_nohash_0.wav\n - train/audio/wow/7eee5973_nohash_1.wav\n - train/audio/wow/f9af823e_nohash_0.wav\n - train/audio/wow/d94eb94f_nohash_1.wav\n - train/audio/wow/2aec99ec_nohash_1.wav\n - train/audio/wow/9785931e_nohash_0.wav\n - train/audio/wow/763188c4_nohash_0.wav\n - train/audio/wow/179a61b7_nohash_0.wav\n - train/audio/wow/7be5a0f3_nohash_0.wav\n - train/audio/wow/c518d1b1_nohash_0.wav\n - train/audio/wow/afb9e62e_nohash_0.wav\n - train/audio/wow/fad7a69a_nohash_0.wav\n - train/audio/wow/9a7c1f83_nohash_1.wav\n - train/audio/wow/5628d7b7_nohash_4.wav\n - train/audio/wow/34805883_nohash_2.wav\n - train/audio/wow/8625475c_nohash_0.wav\n - train/audio/wow/f3d06008_nohash_0.wav\n - train/audio/wow/881583a6_nohash_0.wav\n - train/audio/wow/3aa6f4e2_nohash_1.wav\n - train/audio/wow/e9a76b2f_nohash_0.wav\n - train/audio/wow/2510c044_nohash_1.wav\n - train/audio/wow/364f979f_nohash_0.wav\n - train/audio/wow/095847e4_nohash_0.wav\n - train/audio/wow/facd97c0_nohash_1.wav\n - train/audio/wow/b25b6065_nohash_0.wav\n - train/audio/wow/2151b09a_nohash_0.wav\n - train/audio/wow/dabf67d9_nohash_1.wav\n - train/audio/wow/c8771f88_nohash_0.wav\n - train/audio/wow/988e2f9a_nohash_0.wav\n - train/audio/wow/ac899eb7_nohash_2.wav\n - train/audio/wow/fd395b74_nohash_0.wav\n - train/audio/wow/f4386675_nohash_0.wav\n - train/audio/wow/833a0279_nohash_0.wav\n - train/audio/wow/c44d2a58_nohash_0.wav\n - train/audio/wow/9712cce0_nohash_0.wav\n - train/audio/wow/b46e8153_nohash_1.wav\n - train/audio/wow/dfb6450b_nohash_0.wav\n - train/audio/wow/f9f9751d_nohash_0.wav\n - train/audio/wow/b487da60_nohash_0.wav\n - train/audio/wow/5e1b34a6_nohash_2.wav\n - train/audio/wow/af30314d_nohash_1.wav\n - train/audio/wow/ec5ab5d5_nohash_0.wav\n - train/audio/wow/e269bac0_nohash_0.wav\n - train/audio/wow/8c3c4715_nohash_1.wav\n - train/audio/wow/6f5eea74_nohash_0.wav\n - train/audio/wow/88f8a99c_nohash_0.wav\n - train/audio/wow/6e916de8_nohash_2.wav\n - train/audio/wow/05cf43ef_nohash_0.wav\n - train/audio/wow/1338a799_nohash_0.wav\n - train/audio/wow/6aafb34f_nohash_0.wav\n - train/audio/wow/7318280c_nohash_0.wav\n - train/audio/wow/3ae5c04f_nohash_0.wav\n - train/audio/wow/d103dd6e_nohash_0.wav\n - train/audio/wow/99b05bcf_nohash_0.wav\n - train/audio/wow/590750e8_nohash_0.wav\n - train/audio/wow/bb31b82b_nohash_1.wav\n - train/audio/wow/fc94edb0_nohash_0.wav\n - train/audio/wow/529eda42_nohash_0.wav\n - train/audio/wow/f9bdf10e_nohash_0.wav\n - train/audio/wow/6f7724f5_nohash_0.wav\n - train/audio/wow/6c0f6493_nohash_0.wav\n - train/audio/wow/bfdb9801_nohash_0.wav\n - train/audio/wow/0137b3f4_nohash_0.wav\n - train/audio/wow/f864cd4a_nohash_1.wav\n - train/audio/wow/d9ae8983_nohash_1.wav\n - train/audio/wow/e0a7c5a0_nohash_1.wav\n - train/audio/wow/d53e25ba_nohash_0.wav\n - train/audio/wow/591d32f3_nohash_1.wav\n - train/audio/wow/cb5d2c6e_nohash_2.wav\n - train/audio/wow/71aa5b54_nohash_0.wav\n - train/audio/wow/e6db3894_nohash_1.wav\n - train/audio/wow/937b433e_nohash_0.wav\n - train/audio/wow/cc71bada_nohash_1.wav\n - train/audio/wow/53458368_nohash_0.wav\n - train/audio/wow/caa4779f_nohash_0.wav\n - train/audio/wow/e57abea3_nohash_2.wav\n - train/audio/wow/01bcfc0c_nohash_0.wav\n - train/audio/wow/7d86b703_nohash_1.wav\n - train/audio/wow/d2eae23d_nohash_1.wav\n - train/audio/wow/0ac15fe9_nohash_2.wav\n - train/audio/wow/e5dadd24_nohash_1.wav\n - train/audio/wow/257e17e0_nohash_2.wav\n - train/audio/wow/93f30cc4_nohash_3.wav\n - train/audio/wow/dca2797e_nohash_0.wav\n - train/audio/wow/df280250_nohash_0.wav\n - train/audio/wow/ab353673_nohash_2.wav\n - train/audio/wow/c120e80e_nohash_1.wav\n - train/audio/wow/8c3c4715_nohash_0.wav\n - train/audio/wow/20d3f11f_nohash_1.wav\n - train/audio/wow/5b09db89_nohash_0.wav\n - train/audio/wow/a3fc7884_nohash_0.wav\n - train/audio/wow/0d53e045_nohash_0.wav\n - train/audio/wow/1e31353f_nohash_1.wav\n - train/audio/wow/aa62fdad_nohash_2.wav\n - train/audio/wow/c7124b73_nohash_0.wav\n - train/audio/wow/1993db46_nohash_1.wav\n - train/audio/wow/58df33b5_nohash_0.wav\n - train/audio/wow/8f0d3c27_nohash_0.wav\n - train/audio/wow/f00180d0_nohash_0.wav\n - train/audio/wow/9f63152b_nohash_0.wav\n - train/audio/wow/695c2127_nohash_0.wav\n - train/audio/wow/1851e33b_nohash_1.wav\n - train/audio/wow/b26343e9_nohash_1.wav\n - train/audio/wow/173e6bbf_nohash_2.wav\n - train/audio/wow/bfaf2000_nohash_0.wav\n - train/audio/wow/f5c3de1b_nohash_0.wav\n - train/audio/wow/b87bdb22_nohash_0.wav\n - train/audio/wow/c948d727_nohash_0.wav\n - train/audio/wow/070b49af_nohash_0.wav\n - train/audio/wow/89f680f3_nohash_0.wav\n - train/audio/wow/44f68a83_nohash_1.wav\n - train/audio/wow/a6d586b7_nohash_0.wav\n\nTop files under test/:\n - test/audio\n - test/audio/clip_00002578.wav\n - test/audio/clip_00001415.wav\n - test/audio/clip_00004142.wav\n - test/audio/clip_00002943.wav\n - test/audio/clip_00000977.wav\n - test/audio/clip_00001161.wav\n - test/audio/clip_00004080.wav\n - test/audio/clip_00004247.wav\n - test/audio/clip_00003085.wav\n - test/audio/clip_00005679.wav\n - test/audio/clip_00002611.wav\n - test/audio/clip_00001754.wav\n - test/audio/clip_00005199.wav\n - test/audio/clip_00001095.wav\n - test/audio/clip_00001689.wav\n - test/audio/clip_00006189.wav\n - test/audio/clip_00005505.wav\n - test/audio/clip_00004018.wav\n - test/audio/clip_00000811.wav\n - test/audio/clip_00003155.wav\n - test/audio/clip_00001517.wav\n - test/audio/clip_00001706.wav\n - test/audio/clip_00001878.wav\n - test/audio/clip_00004172.wav\n - test/audio/clip_00005055.wav\n - test/audio/clip_00001475.wav\n - test/audio/clip_00004646.wav\n - test/audio/clip_00000303.wav\n - test/audio/clip_00002847.wav\n - test/audio/clip_00000410.wav\n - test/audio/clip_00000985.wav\n - test/audio/clip_00005045.wav\n - test/audio/clip_00001827.wav\n - test/audio/clip_00002831.wav\n - test/audio/clip_00002732.wav\n - test/audio/clip_00005532.wav\n - test/audio/clip_00000178.wav\n - test/audio/clip_00003012.wav\n - test/audio/clip_00005573.wav\n - test/audio/clip_00001857.wav\n - test/audio/clip_00001496.wav\n - test/audio/clip_00003642.wav\n - test/audio/clip_00005334.wav\n - test/audio/clip_00005071.wav\n - test/audio/clip_00000748.wav\n - test/audio/clip_00002327.wav\n - test/audio/clip_00001604.wav\n - test/audio/clip_00003701.wav\n - test/audio/clip_00000254.wav\n\nLook for metadata files anywhere:\n"
          ]
        }
      ]
    },
    {
      "id": "04ba4389-d37f-4335-934f-55ba46f43c13",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build train dataframe with 12-class mapping (targets, unknown, silence)\n",
        "import glob, math, random, re\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "ROOT = Path('.')\n",
        "AUDIO_DIR = ROOT / 'train' / 'audio'\n",
        "TEST_DIR = ROOT / 'test' / 'audio'\n",
        "TARGET_WORDS = ['yes','no','up','down','left','right','on','off','stop','go']\n",
        "BG_DIR = AUDIO_DIR / '_background_noise_'\n",
        "\n",
        "def get_speaker_id_from_path(fp: Path) -> str:\n",
        "    m = re.match(r'^(?P<spk>[^_]+)_nohash_', fp.stem)\n",
        "    return m.group('spk') if m else fp.stem.split('_')[0]\n",
        "\n",
        "# Enumerate class folders\n",
        "class_dirs = [p for p in AUDIO_DIR.iterdir() if p.is_dir()]\n",
        "class_names = sorted([p.name for p in class_dirs])\n",
        "print('Class folders (train/audio):', class_names)\n",
        "\n",
        "# Split into targets, background, and non-target (unknown source)\n",
        "bg_present = BG_DIR.exists()\n",
        "non_target_classes = [c for c in class_names if c not in TARGET_WORDS and c != '_background_noise_']\n",
        "print(f'Background present: {bg_present}; non-target classes (unknown sources): {len(non_target_classes)}')\n",
        "\n",
        "# Gather file paths\n",
        "def gather_wavs(dirpath: Path):\n",
        "    return sorted([Path(p) for p in glob.glob(str(dirpath / '*.wav'))])\n",
        "\n",
        "rows = []\n",
        "for cls in TARGET_WORDS:\n",
        "    cls_dir = AUDIO_DIR / cls\n",
        "    for fp in gather_wavs(cls_dir):\n",
        "        rows.append({'path': str(fp), 'label': cls, 'speaker': get_speaker_id_from_path(fp), 'kind': 'target'})\n",
        "\n",
        "for cls in non_target_classes:\n",
        "    cls_dir = AUDIO_DIR / cls\n",
        "    for fp in gather_wavs(cls_dir):\n",
        "        rows.append({'path': str(fp), 'label': 'unknown', 'speaker': get_speaker_id_from_path(fp), 'kind': 'unknown'})\n",
        "\n",
        "df_train = pd.DataFrame(rows)\n",
        "print('Train rows (target+unknown):', df_train.shape)\n",
        "print(df_train.groupby('label').size().sort_values(ascending=False).head(15))\n",
        "\n",
        "# Build silence examples from background noise: create ~10% of train size\n",
        "silence_rows = []\n",
        "if bg_present:\n",
        "    bg_files = gather_wavs(BG_DIR)\n",
        "    # Some files in _background_noise_ are long; we'll sample random 1s offsets during feature extraction.\n",
        "    n_silence = max(1, int(0.10 * len(df_train)))\n",
        "    rng = np.random.default_rng(42)\n",
        "    for i in range(n_silence):\n",
        "        fp = bg_files[i % len(bg_files)]\n",
        "        silence_rows.append({'path': str(fp), 'label': 'silence', 'speaker': f'silence_{i}', 'kind': 'silence', 'bg_index': i})\n",
        "    print(f'Generated planned silence entries: {len(silence_rows)}')\n",
        "else:\n",
        "    print('Warning: _background_noise_ not found; no explicit silence examples will be created.')\n",
        "\n",
        "if len(silence_rows) > 0:\n",
        "    df_sil = pd.DataFrame(silence_rows)\n",
        "    df_train = pd.concat([df_train, df_sil], ignore_index=True)\n",
        "\n",
        "print('Final train rows (incl. silence if any):', df_train.shape)\n",
        "print(df_train['label'].value_counts().head(15))\n",
        "\n",
        "# Test dataframe\n",
        "test_files = sorted([Path(p) for p in glob.glob(str(TEST_DIR / '*.wav'))])\n",
        "df_test = pd.DataFrame({'fname': [p.name for p in test_files], 'path': [str(p) for p in test_files]})\n",
        "print('Test rows:', df_test.shape)\n",
        "print(df_test.head())\n",
        "\n",
        "# Save metadata for reuse\n",
        "df_train.to_csv('train_meta.csv', index=False)\n",
        "df_test.to_csv('test_meta.csv', index=False)\n",
        "print('Saved train_meta.csv and test_meta.csv.')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class folders (train/audio): ['_background_noise_', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'four', 'go', 'happy', 'house', 'left', 'marvin', 'nine', 'no', 'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three', 'tree', 'two', 'up', 'wow', 'yes', 'zero']\nBackground present: True; non-target classes (unknown sources): 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train rows (target+unknown): (58249, 4)\nlabel\nunknown    36932\nstop        2157\nno          2146\nup          2146\non          2142\ndown        2141\nyes         2138\ngo          2133\noff         2124\nright       2114\nleft        2076\ndtype: int64\nGenerated planned silence entries: 5824\nFinal train rows (incl. silence if any): (64073, 5)\nlabel\nunknown    36932\nsilence     5824\nstop        2157\nno          2146\nup          2146\non          2142\ndown        2141\nyes         2138\ngo          2133\noff         2124\nright       2114\nleft        2076\nName: count, dtype: int64\nTest rows: (6473, 2)\n               fname                          path\n0  clip_00000000.wav  test/audio/clip_00000000.wav\n1  clip_00000001.wav  test/audio/clip_00000001.wav\n2  clip_00000002.wav  test/audio/clip_00000002.wav\n3  clip_00000003.wav  test/audio/clip_00000003.wav\n4  clip_00000004.wav  test/audio/clip_00000004.wav\nSaved train_meta.csv and test_meta.csv.\n"
          ]
        }
      ]
    },
    {
      "id": "28e0c47b-6a49-4619-b766-b61a51f2f650",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Feature extraction: log-mel spectrograms (per-utterance z-norm), cache to .npy\n",
        "import sys, subprocess, time, math, json, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Install deps for audio feature extraction\n",
        "try:\n",
        "    import librosa, soundfile as sf\n",
        "except Exception as e:\n",
        "    print('Installing librosa & soundfile...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'librosa==0.10.1', 'soundfile==0.12.1', 'numba==0.59.1'], check=True)\n",
        "    import librosa, soundfile as sf\n",
        "\n",
        "SR = 16000\n",
        "N_MELS = 64\n",
        "N_FFT = 512\n",
        "WIN_LENGTH = 400\n",
        "HOP_LENGTH = 160\n",
        "FMIN, FMAX = 20, 8000\n",
        "EPS = 1e-6\n",
        "FIX_DURATION = 1.0\n",
        "\n",
        "def load_audio_1s(path: str, rng: np.random.Generator | None = None) -> np.ndarray:\n",
        "    y, sr = librosa.load(path, sr=SR, mono=True)\n",
        "    target_len = int(FIX_DURATION * SR)\n",
        "    if len(y) < target_len:\n",
        "        y = np.pad(y, (0, target_len - len(y)))\n",
        "    elif len(y) > target_len:\n",
        "        # random crop if rng provided, else center crop\n",
        "        if rng is not None:\n",
        "            start = rng.integers(0, len(y) - target_len + 1)\n",
        "        else:\n",
        "            start = (len(y) - target_len) // 2\n",
        "        y = y[start:start+target_len]\n",
        "    return y\n",
        "\n",
        "def compute_logmel(y: np.ndarray) -> np.ndarray:\n",
        "    mel = librosa.feature.melspectrogram(y=y, sr=SR, n_fft=N_FFT, hop_length=HOP_LENGTH, win_length=WIN_LENGTH, window='hann',\n",
        "                                         n_mels=N_MELS, fmin=FMIN, fmax=FMAX, power=2.0, center=True)\n",
        "    logmel = np.log(mel + EPS)\n",
        "    # per-utterance z-norm\n",
        "    m = logmel.mean(axis=1, keepdims=True)\n",
        "    s = logmel.std(axis=1, keepdims=True) + 1e-8\n",
        "    logmel = (logmel - m) / s\n",
        "    return logmel.astype(np.float32)  # [n_mels, T]\n",
        "\n",
        "def extract_features(df_train_csv='train_meta.csv', df_test_csv='test_meta.csv', seed=42, max_train=4000, max_test=1000):\n",
        "    t0 = time.time()\n",
        "    df_tr = pd.read_csv(df_train_csv)\n",
        "    df_te = pd.read_csv(df_test_csv)\n",
        "    if isinstance(max_train, int) and max_train > 0:\n",
        "        df_tr = df_tr.sample(n=min(max_train, len(df_tr)), random_state=seed).reset_index(drop=True)\n",
        "    if isinstance(max_test, int) and max_test > 0:\n",
        "        df_te = df_te.head(max_test).reset_index(drop=True)\n",
        "    # Label mapping (12 classes)\n",
        "    classes = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "    cls2idx = {c:i for i,c in enumerate(classes)}\n",
        "\n",
        "    # Pre-allocate lists\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "    groups = []\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    print(f\"[FE] Train rows: {len(df_tr)} | Test rows: {len(df_te)}\")\n",
        "    # Train\n",
        "    for i, row in df_tr.iterrows():\n",
        "        if i % 1000 == 0:\n",
        "            print(f\"[FE] Train {i}/{len(df_tr)} elapsed {time.time()-t0:.1f}s\", flush=True)\n",
        "        path = row['path']\n",
        "        label = row['label']\n",
        "        spk = row['speaker']\n",
        "        # For background noise (silence rows), random crop inside file\n",
        "        rgen = rng if label == 'silence' else None\n",
        "        y = load_audio_1s(path, rng=rgen)\n",
        "        feat = compute_logmel(y)  # [M, T]\n",
        "        X_list.append(feat.flatten())\n",
        "        y_list.append(cls2idx[label])\n",
        "        groups.append(str(spk))\n",
        "\n",
        "    X_train = np.stack(X_list, axis=0)\n",
        "    y_train = np.array(y_list, dtype=np.int64)\n",
        "    groups = np.array(groups)\n",
        "    np.save('X_train_logmel.npy', X_train)\n",
        "    np.save('y_train.npy', y_train)\n",
        "    np.save('groups.npy', groups)\n",
        "    print(f\"[FE] Saved X_train_logmel.npy {X_train.shape}, y_train {y_train.shape}\")\n",
        "\n",
        "    # Test\n",
        "    X_list = []\n",
        "    fnames = []\n",
        "    for i, row in df_te.iterrows():\n",
        "        if i % 500 == 0:\n",
        "            print(f\"[FE] Test {i}/{len(df_te)} elapsed {time.time()-t0:.1f}s\", flush=True)\n",
        "        path = row['path']\n",
        "        y = load_audio_1s(path, rng=None)\n",
        "        feat = compute_logmel(y)\n",
        "        X_list.append(feat.flatten())\n",
        "        fnames.append(row['fname'])\n",
        "    X_test = np.stack(X_list, axis=0)\n",
        "    np.save('X_test_logmel.npy', X_test)\n",
        "    pd.Series(fnames).to_csv('test_fnames.csv', index=False, header=False)\n",
        "    print(f\"[FE] Saved X_test_logmel.npy {X_test.shape}\")\n",
        "    print(f\"[FE] Total time: {time.time()-t0:.1f}s\")\n",
        "\n",
        "# Smoke extract on subset for fast iteration\n",
        "extract_features(max_train=4000, max_test=1000)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing librosa & soundfile... No module named 'librosa'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FE] Train rows: 4000 | Test rows: 1000\n[FE] Train 0/4000 elapsed 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FE] Train 1000/4000 elapsed 8.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FE] Train 2000/4000 elapsed 10.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FE] Train 3000/4000 elapsed 11.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FE] Saved X_train_logmel.npy (4000, 6464), y_train (4000,)\n[FE] Test 0/1000 elapsed 13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FE] Test 500/1000 elapsed 13.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FE] Saved X_test_logmel.npy (1000, 6464)\n[FE] Total time: 14.3s\n"
          ]
        }
      ]
    },
    {
      "id": "91960f7e-e7fe-49fd-be48-820656c801ab",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Smoke-test model: XGBoost on subset log-mels with StratifiedGroupKFold\n",
        "import sys, subprocess, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    print('Installing xgboost...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "\n",
        "CLASSES = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "num_class = len(CLASSES)\n",
        "\n",
        "X = np.load('X_train_logmel.npy')  # subset (4000, 6464)\n",
        "y = np.load('y_train.npy')\n",
        "groups = np.load('groups.npy')\n",
        "\n",
        "X_test = np.load('X_test_logmel.npy')  # subset test (1000, 6464)\n",
        "test_fnames = pd.read_csv('test_fnames.csv', header=None)[0].values\n",
        "\n",
        "print('Shapes:', X.shape, y.shape, groups.shape, X_test.shape)\n",
        "\n",
        "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_pred = np.zeros((len(y), num_class), dtype=np.float32)\n",
        "test_pred = np.zeros((len(X_test), num_class), dtype=np.float32)\n",
        "\n",
        "params = dict(\n",
        "    objective='multi:softprob',\n",
        "    num_class=num_class,\n",
        "    tree_method='hist',\n",
        "    max_bin=256,\n",
        "    max_depth=8,\n",
        "    eta=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    min_child_weight=1,\n",
        "    reg_alpha=0.0,\n",
        "    reg_lambda=1.5,\n",
        "    eval_metric='mlogloss',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "for fold, (tr_idx, va_idx) in enumerate(cv.split(X, y, groups)):\n",
        "    t0 = time.time()\n",
        "    print(f'Fold {fold} | train {len(tr_idx)} val {len(va_idx)}')\n",
        "    dtr = xgb.DMatrix(X[tr_idx], label=y[tr_idx])\n",
        "    dva = xgb.DMatrix(X[va_idx], label=y[va_idx])\n",
        "    watch = [(dtr, 'train'), (dva, 'valid')]\n",
        "    model = xgb.train(params, dtr, num_boost_round=2000, evals=watch, early_stopping_rounds=100, verbose_eval=100)\n",
        "    oof_pred[va_idx] = model.predict(dva, iteration_range=(0, model.best_ntree_limit))\n",
        "    dte = xgb.DMatrix(X_test)\n",
        "    test_pred += model.predict(dte, iteration_range=(0, model.best_ntree_limit)) / cv.n_splits\n",
        "    va_acc = accuracy_score(y[va_idx], oof_pred[va_idx].argmax(1))\n",
        "    print(f'Fold {fold} acc: {va_acc:.4f} | elapsed {time.time()-t0:.1f}s')\n",
        "\n",
        "oof_acc = accuracy_score(y, oof_pred.argmax(1))\n",
        "print(f'OOF accuracy: {oof_acc:.4f} | total {time.time()-start:.1f}s')\n",
        "\n",
        "# Save smoke artifacts\n",
        "np.save('oof_pred_subset.npy', oof_pred)\n",
        "np.save('test_pred_subset.npy', test_pred)\n",
        "\n",
        "# Note: test_pred covers only first 1000 test files (subset FE). Full submission will be built after full FE.\n",
        "print('Smoke test complete.')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes: (4000, 6464) (4000,) (4000,) (1000, 6464)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0 | train 3224 val 776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-mlogloss:2.33515\tvalid-mlogloss:2.36803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain-mlogloss:0.12298\tvalid-mlogloss:1.15637\n"
          ]
        }
      ]
    },
    {
      "id": "ac2a2a58-ad50-4785-8d5f-4b4c1e22f0b1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CPU-friendly pooled feature extraction (MFCC+\u0394+\u0394\u0394 stats, log-mel pooled stats, spectral descriptors) with parallelism\n",
        "import sys, subprocess, os, time, math, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from joblib import Parallel, delayed\n",
        "import multiprocessing as mp\n",
        "\n",
        "try:\n",
        "    import librosa, soundfile as sf\n",
        "except Exception as e:\n",
        "    print('Installing librosa & soundfile...', e)\n",
        "    import sys\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'librosa==0.10.1', 'soundfile==0.12.1', 'numba==0.59.1'], check=True)\n",
        "    import librosa, soundfile as sf\n",
        "\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "SR = 16000\n",
        "N_MELS = 64\n",
        "N_FFT = 512\n",
        "WIN_LENGTH = 400\n",
        "HOP_LENGTH = 160\n",
        "FMIN, FMAX = 20, 8000\n",
        "EPS = 1e-6\n",
        "FIX_DURATION = 1.0\n",
        "N_MFCC = 20\n",
        "\n",
        "def load_audio_fixed(path: str, shift_samples: int = 0, rng: np.random.Generator | None = None) -> np.ndarray:\n",
        "    y, sr = librosa.load(path, sr=SR, mono=True)\n",
        "    target_len = int(FIX_DURATION * SR)\n",
        "    # Apply shift (positive -> right, negative -> left) by padding and slicing\n",
        "    if shift_samples != 0:\n",
        "        if shift_samples > 0:\n",
        "            y = np.pad(y, (shift_samples, 0))\n",
        "        else:\n",
        "            y = np.pad(y, (0, -shift_samples))\n",
        "        start = max(0, 0)  # we'll crop below\n",
        "    # Pad/crop to 1s\n",
        "    if len(y) < target_len:\n",
        "        y = np.pad(y, (0, target_len - len(y)))\n",
        "    elif len(y) > target_len:\n",
        "        if rng is not None:\n",
        "            start = rng.integers(0, len(y) - target_len + 1)\n",
        "        else:\n",
        "            start = (len(y) - target_len) // 2\n",
        "        y = y[start:start+target_len]\n",
        "    return y.astype(np.float32)\n",
        "\n",
        "def pooled_stats(x: np.ndarray, axis: int = -1, percentiles=(25, 75)) -> np.ndarray:\n",
        "    # x shape [features, time] or [time]\n",
        "    if x.ndim == 1:\n",
        "        x = x[None, :]\n",
        "    mean = np.mean(x, axis=axis)\n",
        "    std = np.std(x, axis=axis) + 1e-8\n",
        "    p25 = np.percentile(x, percentiles[0], axis=axis)\n",
        "    p75 = np.percentile(x, percentiles[1], axis=axis)\n",
        "    return np.concatenate([mean, std, p25, p75], axis=0)\n",
        "\n",
        "def mfcc_extra_stats(feat_2d: np.ndarray) -> np.ndarray:\n",
        "    # feat_2d shape [n_coeff, time]\n",
        "    mn = np.min(feat_2d, axis=1)\n",
        "    mx = np.max(feat_2d, axis=1)\n",
        "    sk = skew(feat_2d, axis=1, bias=False, nan_policy='omit')\n",
        "    ku = kurtosis(feat_2d, axis=1, fisher=True, bias=False, nan_policy='omit')\n",
        "    return np.concatenate([mn, mx, sk, ku], axis=0)\n",
        "\n",
        "def extract_feature_vector(path: str, label: str | None, speaker: str | None, seed: int = 42, is_silence: bool = False, shift_samples: int = 0) -> tuple:\n",
        "    rng = np.random.default_rng(seed) if is_silence else None\n",
        "    y = load_audio_fixed(path, shift_samples=shift_samples, rng=rng)\n",
        "    # Log-mel for pooled stats\n",
        "    mel = librosa.feature.melspectrogram(y=y, sr=SR, n_fft=N_FFT, hop_length=HOP_LENGTH, win_length=WIN_LENGTH,\n",
        "                                         window='hann', n_mels=N_MELS, fmin=FMIN, fmax=FMAX, power=2.0, center=True)\n",
        "    logmel = np.log(mel + EPS)\n",
        "    logmel_stats = pooled_stats(logmel, axis=1)  # shape 64*4 = 256\n",
        "    # MFCC + deltas + delta-delta\n",
        "    mfcc = librosa.feature.mfcc(S=librosa.power_to_db(mel, ref=np.max), n_mfcc=N_MFCC)\n",
        "    mfcc_d = librosa.feature.delta(mfcc, order=1)\n",
        "    mfcc_dd = librosa.feature.delta(mfcc, order=2)\n",
        "    mfcc_stats = np.concatenate([\n",
        "        np.mean(mfcc, axis=1), np.std(mfcc, axis=1),\n",
        "        np.mean(mfcc_d, axis=1), np.std(mfcc_d, axis=1),\n",
        "        np.mean(mfcc_dd, axis=1), np.std(mfcc_dd, axis=1)\n",
        "    ])  # 20*6 = 120\n",
        "    # MFCC extras per coach: per-coef min/max/skew/kurt for mfcc, mfcc_d, mfcc_dd\n",
        "    mfcc_extras = np.concatenate([\n",
        "        mfcc_extra_stats(mfcc),\n",
        "        mfcc_extra_stats(mfcc_d),\n",
        "        mfcc_extra_stats(mfcc_dd)\n",
        "    ])  # 20*4*3 = 240\n",
        "    # Spectral descriptors\n",
        "    sc = librosa.feature.spectral_centroid(y=y, sr=SR)\n",
        "    sbw = librosa.feature.spectral_bandwidth(y=y, sr=SR)\n",
        "    srf = librosa.feature.spectral_rolloff(y=y, sr=SR, roll_percent=0.95)\n",
        "    zcr = librosa.feature.zero_crossing_rate(y)\n",
        "    rms = librosa.feature.rms(y=y)\n",
        "    spec_contrast = librosa.feature.spectral_contrast(y=y, sr=SR, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
        "    # New: spectral flatness, RMS percentiles, energy slope, chroma\n",
        "    sflat = librosa.feature.spectral_flatness(y=y)  # (1, T)\n",
        "    sflat_mean = float(np.mean(sflat))\n",
        "    sflat_std = float(np.std(sflat) + 1e-8)\n",
        "    # RMS percentiles on time axis\n",
        "    rms_vec = rms[0]\n",
        "    rms_p10 = float(np.percentile(rms_vec, 10))\n",
        "    rms_p90 = float(np.percentile(rms_vec, 90))\n",
        "    # Energy slope: mean(last 20%) - mean(first 20%)\n",
        "    T = rms_vec.shape[0]\n",
        "    k = max(1, int(0.2 * T))\n",
        "    e_slope = float(np.mean(rms_vec[-k:]) - np.mean(rms_vec[:k]))\n",
        "    # Optional: chroma_stft mean/std (12x2)\n",
        "    chroma = librosa.feature.chroma_stft(y=y, sr=SR, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
        "    chroma_mean = np.mean(chroma, axis=1)\n",
        "    chroma_std = np.std(chroma, axis=1) + 1e-8\n",
        "\n",
        "    spec_desc = np.array([\n",
        "        sc.mean(), sc.std(),\n",
        "        sbw.mean(), sbw.std(),\n",
        "        srf.mean(), srf.std(),\n",
        "        zcr.mean(), zcr.std(),\n",
        "        rms.mean(), rms.std(),\n",
        "        sflat_mean, sflat_std,\n",
        "        rms_p10, rms_p90,\n",
        "        e_slope\n",
        "    ], dtype=np.float32)\n",
        "    spec_contrast_mean = spec_contrast.mean(axis=1)  # 7 dims\n",
        "    feats = np.concatenate([\n",
        "        logmel_stats,            # 256\n",
        "        mfcc_stats,              # 120\n",
        "        mfcc_extras,             # 240\n",
        "        spec_desc,               # 15\n",
        "        spec_contrast_mean,      # 7\n",
        "        chroma_mean, chroma_std  # 24\n",
        "    ]).astype(np.float32)\n",
        "    return feats, label, speaker\n",
        "\n",
        "def run_pooled_feature_extraction(train_meta='train_meta.csv', test_meta='test_meta.csv',\n",
        "                                  out_prefix='pooled', max_train=None, max_test=None,\n",
        "                                  n_jobs=None, seed=42, tta_shifts_ms=None):\n",
        "    t0 = time.time()\n",
        "    df_tr = pd.read_csv(train_meta)\n",
        "    df_te = pd.read_csv(test_meta)\n",
        "    if max_train is not None:\n",
        "        df_tr = df_tr.sample(n=min(max_train, len(df_tr)), random_state=seed).reset_index(drop=True)\n",
        "    if max_test is not None:\n",
        "        df_te = df_te.head(max_test).reset_index(drop=True)\n",
        "    print(f\"[POOL-FE] Train rows: {len(df_tr)} | Test rows: {len(df_te)}\")\n",
        "    # Prepare TTA shifts in samples\n",
        "    if tta_shifts_ms is None:\n",
        "        tta_shifts_ms = [ -200, -100, 0, 100, 200 ]\n",
        "    shifts = [int(ms/1000.0 * SR) for ms in tta_shifts_ms]\n",
        "    # Parallel settings\n",
        "    if n_jobs is None:\n",
        "        n_jobs = max(1, mp.cpu_count() - 2)\n",
        "    print(f\"[POOL-FE] Using n_jobs={n_jobs} | shifts(ms)={tta_shifts_ms}\")\n",
        "\n",
        "    # Train features (single shift: we do not augment here; shifts used only for test TTA)\n",
        "    def _proc_train(row):\n",
        "        path = row['path']\n",
        "        label = row['label']\n",
        "        speaker = row['speaker']\n",
        "        is_sil = (label == 'silence')\n",
        "        feats, label_out, spk = extract_feature_vector(path, label, speaker, seed=seed, is_silence=is_sil, shift_samples=0)\n",
        "        return feats, label_out, spk\n",
        "\n",
        "    tr_results = Parallel(n_jobs=n_jobs, backend='loky')(delayed(_proc_train)(row) for _, row in df_tr.iterrows())\n",
        "    X_train = np.stack([r[0] for r in tr_results])\n",
        "    y_labels = [r[1] for r in tr_results]\n",
        "    groups = np.array([r[2] for r in tr_results])\n",
        "    classes = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "    cls2idx = {c:i for i,c in enumerate(classes)}\n",
        "    y_train = np.array([cls2idx[l] for l in y_labels], dtype=np.int64)\n",
        "    np.save(f'X_train_{out_prefix}.npy', X_train)\n",
        "    np.save(f'y_train_{out_prefix}.npy', y_train)\n",
        "    np.save(f'groups_{out_prefix}.npy', groups)\n",
        "    print(f\"[POOL-FE] Saved train: {X_train.shape} | time {time.time()-t0:.1f}s\")\n",
        "\n",
        "    # Test features with TTA shifts; average stored separately or at inference\n",
        "    def _proc_test(row, shift_samples):\n",
        "        path = row['path']\n",
        "        feats, _, _ = extract_feature_vector(path, None, None, seed=seed, is_silence=False, shift_samples=shift_samples)\n",
        "        return feats\n",
        "\n",
        "    fnames = df_te['fname'].tolist()\n",
        "    X_tta = []\n",
        "    for s in shifts:\n",
        "        tt0 = time.time()\n",
        "        feats_list = Parallel(n_jobs=n_jobs, backend='loky')(delayed(_proc_test)(row, s) for _, row in df_te.iterrows())\n",
        "        X_t = np.stack(feats_list)\n",
        "        X_tta.append(X_t)\n",
        "        print(f\"[POOL-FE] Test shift {s} samples -> {X_t.shape} | elapsed {time.time()-tt0:.1f}s\")\n",
        "    X_test = np.stack(X_tta, axis=0)  # [n_shifts, N, D]\n",
        "    np.save(f'X_test_{out_prefix}_tta.npy', X_test)\n",
        "    pd.Series(fnames).to_csv(f'test_fnames_{out_prefix}.csv', index=False, header=False)\n",
        "    print(f\"[POOL-FE] Saved test TTA: {X_test.shape} | total {time.time()-t0:.1f}s\")\n",
        "\n",
        "# Run pooled FE on full data with 5 TTA shifts for test (updated to [-200,-100,0,100,200] ms)\n",
        "run_pooled_feature_extraction(out_prefix='pooled', max_train=None, max_test=None, n_jobs=None, seed=42, tta_shifts_ms=[-200, -100, 0, 100, 200])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[POOL-FE] Train rows: 64073 | Test rows: 6473\n[POOL-FE] Using n_jobs=34 | shifts(ms)=[-200, -100, 0, 100, 200]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[POOL-FE] Saved train: (64073, 662) | time 45.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[POOL-FE] Test shift -3200 samples -> (6473, 662) | elapsed 4.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[POOL-FE] Test shift -1600 samples -> (6473, 662) | elapsed 5.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[POOL-FE] Test shift 0 samples -> (6473, 662) | elapsed 4.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[POOL-FE] Test shift 1600 samples -> (6473, 662) | elapsed 5.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[POOL-FE] Test shift 3200 samples -> (6473, 662) | elapsed 4.7s\n[POOL-FE] Saved test TTA: (5, 6473, 662) | total 70.3s\n"
          ]
        }
      ]
    },
    {
      "id": "056a1fd4-52cc-48ba-961c-62d5f5579c60",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train XGBoost on pooled CPU-friendly features with SGKF and TTA; produce submission\n",
        "import os, time, numpy as np, pandas as pd, sys, subprocess\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    print('Installing xgboost...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "\n",
        "CLASSES = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "num_class = len(CLASSES)\n",
        "\n",
        "train_feat = 'X_train_pooled.npy'\n",
        "train_y = 'y_train_pooled.npy'\n",
        "train_groups = 'groups_pooled.npy'\n",
        "# Use small-shift TTA features for test\n",
        "test_feat_tta = 'X_test_pooled_tta_small.npy'\n",
        "test_fnames_csv = 'test_fnames_pooled_small.csv'\n",
        "\n",
        "if not (os.path.exists(train_feat) and os.path.exists(train_y) and os.path.exists(train_groups) and os.path.exists(test_feat_tta) and os.path.exists(test_fnames_csv)):\n",
        "    print('Pooled features not found. Run cell 6 to generate pooled features first.')\n",
        "else:\n",
        "    X = np.load(train_feat)\n",
        "    y = np.load(train_y)\n",
        "    groups = np.load(train_groups)\n",
        "    X_test_tta = np.load(test_feat_tta)  # [n_shifts, N, D]\n",
        "    test_fnames = pd.read_csv(test_fnames_csv, header=None)[0].values\n",
        "    n_shifts, n_test, D = X_test_tta.shape\n",
        "    print('Shapes:', X.shape, y.shape, groups.shape, X_test_tta.shape)\n",
        "\n",
        "    params = dict(\n",
        "        objective='multi:softprob',\n",
        "        num_class=num_class,\n",
        "        tree_method='hist',\n",
        "        max_bin=256,\n",
        "        max_depth=7,\n",
        "        eta=0.05,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        min_child_weight=3,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=1.0,\n",
        "        eval_metric='mlogloss',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    oof = np.zeros((len(y), num_class), dtype=np.float32)\n",
        "    test_pred = np.zeros((n_test, num_class), dtype=np.float32)\n",
        "    start = time.time()\n",
        "\n",
        "    for fold, (tr_idx, va_idx) in enumerate(cv.split(X, y, groups)):\n",
        "        t0 = time.time()\n",
        "        print(f'Fold {fold} | train {len(tr_idx)} val {len(va_idx)}')\n",
        "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "        X_tr = scaler.fit_transform(X[tr_idx])\n",
        "        X_va = scaler.transform(X[va_idx])\n",
        "        # Clip outliers\n",
        "        X_tr = np.clip(X_tr, -5, 5)\n",
        "        X_va = np.clip(X_va, -5, 5)\n",
        "        # Balanced sample weights for heavy class imbalance\n",
        "        tr_weights = compute_sample_weight('balanced', y=y[tr_idx])\n",
        "        dtr = xgb.DMatrix(X_tr, label=y[tr_idx], weight=tr_weights)\n",
        "        dva = xgb.DMatrix(X_va, label=y[va_idx])\n",
        "        model = xgb.train(params, dtr, num_boost_round=2000, evals=[(dtr,'train'),(dva,'valid')], early_stopping_rounds=100, verbose_eval=100)\n",
        "        # Determine best iteration for prediction in XGBoost >=2.x\n",
        "        best_iter = getattr(model, 'best_iteration', None)\n",
        "        if best_iter is None:\n",
        "            try:\n",
        "                best_iter = model.num_boosted_rounds() - 1\n",
        "            except Exception:\n",
        "                best_iter = None\n",
        "        if best_iter is not None:\n",
        "            oof[va_idx] = model.predict(dva, iteration_range=(0, best_iter + 1))\n",
        "        else:\n",
        "            oof[va_idx] = model.predict(dva)\n",
        "        va_acc = accuracy_score(y[va_idx], oof[va_idx].argmax(1))\n",
        "        print(f'Fold {fold} acc: {va_acc:.4f} | elapsed {time.time()-t0:.1f}s')\n",
        "        # Test TTA\n",
        "        fold_test = np.zeros((n_test, num_class), dtype=np.float32)\n",
        "        for s in range(n_shifts):\n",
        "            X_te_s = scaler.transform(X_test_tta[s])\n",
        "            X_te_s = np.clip(X_te_s, -5, 5)\n",
        "            dte = xgb.DMatrix(X_te_s)\n",
        "            if best_iter is not None:\n",
        "                fold_test += model.predict(dte, iteration_range=(0, best_iter + 1)) / n_shifts\n",
        "            else:\n",
        "                fold_test += model.predict(dte) / n_shifts\n",
        "        test_pred += fold_test / cv.n_splits\n",
        "\n",
        "    oof_acc = accuracy_score(y, oof.argmax(1))\n",
        "    print(f'OOF accuracy: {oof_acc:.4f} | total {time.time()-start:.1f}s')\n",
        "    np.save('oof_pooled.npy', oof)\n",
        "    # Save test preds with small-TTA suffix\n",
        "    np.save('test_pred_pooled_tta50.npy', test_pred)\n",
        "    print('Saved XGB preds (small TTA) to test_pred_pooled_tta50.npy.')\n",
        "\n",
        "    # Build submission (optional, for quick check)\n",
        "    pred_idx = test_pred.argmax(1)\n",
        "    labels = [CLASSES[i] for i in pred_idx]\n",
        "    sub = pd.DataFrame({'fname': test_fnames, 'label': labels})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv with shape:', sub.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes: (64073, 662) (64073,) (64073,) (5, 6473, 662)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0 | train 52005 val 12068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-mlogloss:2.33250\tvalid-mlogloss:2.38331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain-mlogloss:0.24060\tvalid-mlogloss:0.74707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mlogloss:0.08020\tvalid-mlogloss:0.47907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain-mlogloss:0.03884\tvalid-mlogloss:0.38749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mlogloss:0.02276\tvalid-mlogloss:0.34849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\ttrain-mlogloss:0.01511\tvalid-mlogloss:0.33076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain-mlogloss:0.01096\tvalid-mlogloss:0.32142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\ttrain-mlogloss:0.00846\tvalid-mlogloss:0.31587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain-mlogloss:0.00685\tvalid-mlogloss:0.31224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain-mlogloss:0.00573\tvalid-mlogloss:0.31005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain-mlogloss:0.00493\tvalid-mlogloss:0.30868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1100]\ttrain-mlogloss:0.00432\tvalid-mlogloss:0.30782\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain-mlogloss:0.00386\tvalid-mlogloss:0.30732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1300]\ttrain-mlogloss:0.00350\tvalid-mlogloss:0.30717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\ttrain-mlogloss:0.00320\tvalid-mlogloss:0.30701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\ttrain-mlogloss:0.00295\tvalid-mlogloss:0.30707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1556]\ttrain-mlogloss:0.00283\tvalid-mlogloss:0.30699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0 acc: 0.9018 | elapsed 500.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 | train 51074 val 12999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-mlogloss:2.33007\tvalid-mlogloss:2.38589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain-mlogloss:0.23088\tvalid-mlogloss:0.77783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mlogloss:0.07527\tvalid-mlogloss:0.52077\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain-mlogloss:0.03620\tvalid-mlogloss:0.43762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mlogloss:0.02132\tvalid-mlogloss:0.40697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\ttrain-mlogloss:0.01421\tvalid-mlogloss:0.39249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain-mlogloss:0.01033\tvalid-mlogloss:0.38603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\ttrain-mlogloss:0.00799\tvalid-mlogloss:0.38280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain-mlogloss:0.00649\tvalid-mlogloss:0.38147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain-mlogloss:0.00546\tvalid-mlogloss:0.38113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain-mlogloss:0.00470\tvalid-mlogloss:0.38101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1089]\ttrain-mlogloss:0.00419\tvalid-mlogloss:0.38137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 acc: 0.8832 | elapsed 426.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 | train 51792 val 12281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-mlogloss:2.33221\tvalid-mlogloss:2.38324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain-mlogloss:0.23833\tvalid-mlogloss:0.75401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mlogloss:0.07860\tvalid-mlogloss:0.48901\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain-mlogloss:0.03779\tvalid-mlogloss:0.39934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mlogloss:0.02219\tvalid-mlogloss:0.36339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\ttrain-mlogloss:0.01478\tvalid-mlogloss:0.34656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain-mlogloss:0.01075\tvalid-mlogloss:0.33777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\ttrain-mlogloss:0.00830\tvalid-mlogloss:0.33246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain-mlogloss:0.00673\tvalid-mlogloss:0.32974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain-mlogloss:0.00563\tvalid-mlogloss:0.32777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain-mlogloss:0.00486\tvalid-mlogloss:0.32701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1100]\ttrain-mlogloss:0.00427\tvalid-mlogloss:0.32660\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain-mlogloss:0.00382\tvalid-mlogloss:0.32629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1268]\ttrain-mlogloss:0.00356\tvalid-mlogloss:0.32664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 acc: 0.8941 | elapsed 426.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 | train 50819 val 13254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-mlogloss:2.33087\tvalid-mlogloss:2.38537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain-mlogloss:0.23165\tvalid-mlogloss:0.75722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mlogloss:0.07549\tvalid-mlogloss:0.50170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain-mlogloss:0.03629\tvalid-mlogloss:0.41785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mlogloss:0.02134\tvalid-mlogloss:0.38470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\ttrain-mlogloss:0.01418\tvalid-mlogloss:0.37027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain-mlogloss:0.01034\tvalid-mlogloss:0.36354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\ttrain-mlogloss:0.00805\tvalid-mlogloss:0.35972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain-mlogloss:0.00653\tvalid-mlogloss:0.35769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain-mlogloss:0.00549\tvalid-mlogloss:0.35686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain-mlogloss:0.00473\tvalid-mlogloss:0.35677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1046]\ttrain-mlogloss:0.00446\tvalid-mlogloss:0.35688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 acc: 0.8874 | elapsed 359.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 | train 50602 val 13471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-mlogloss:2.33100\tvalid-mlogloss:2.38609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain-mlogloss:0.23282\tvalid-mlogloss:0.77955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mlogloss:0.07577\tvalid-mlogloss:0.51648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain-mlogloss:0.03636\tvalid-mlogloss:0.42886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mlogloss:0.02141\tvalid-mlogloss:0.39478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\ttrain-mlogloss:0.01425\tvalid-mlogloss:0.37887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain-mlogloss:0.01038\tvalid-mlogloss:0.37135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\ttrain-mlogloss:0.00808\tvalid-mlogloss:0.36755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain-mlogloss:0.00657\tvalid-mlogloss:0.36544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain-mlogloss:0.00552\tvalid-mlogloss:0.36459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain-mlogloss:0.00476\tvalid-mlogloss:0.36421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1100]\ttrain-mlogloss:0.00419\tvalid-mlogloss:0.36425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1158]\ttrain-mlogloss:0.00392\tvalid-mlogloss:0.36415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 acc: 0.8863 | elapsed 385.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF accuracy: 0.8903 | total 2104.5s\nSaved XGB preds (small TTA) to test_pred_pooled_tta50.npy.\nSaved submission.csv with shape: (6473, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "ddc09182-771c-4ff0-b728-103c5e7b87e5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost on pooled features + blend with XGBoost; SGKF, weights, clipping, 5-shift TTA\n",
        "import os, time, numpy as np, pandas as pd, sys, subprocess\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostClassifier, Pool\n",
        "except Exception as e:\n",
        "    print('Installing catboost...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'catboost==1.2.5'], check=True)\n",
        "    from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "CLASSES = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "num_class = len(CLASSES)\n",
        "\n",
        "train_feat = 'X_train_pooled.npy'\n",
        "train_y = 'y_train_pooled.npy'\n",
        "train_groups = 'groups_pooled.npy'\n",
        "# Use small-shift TTA features for test\n",
        "test_feat_tta = 'X_test_pooled_tta_small.npy'\n",
        "test_fnames_csv = 'test_fnames_pooled_small.csv'\n",
        "\n",
        "assert os.path.exists(train_feat) and os.path.exists(train_y) and os.path.exists(train_groups), 'Missing train pooled features'\n",
        "assert os.path.exists(test_feat_tta) and os.path.exists(test_fnames_csv), 'Missing test pooled TTA features'\n",
        "\n",
        "X = np.load(train_feat)\n",
        "y = np.load(train_y)\n",
        "groups = np.load(train_groups)\n",
        "X_test_tta = np.load(test_feat_tta)  # [n_shifts, N, D]\n",
        "test_fnames = pd.read_csv(test_fnames_csv, header=None)[0].values\n",
        "n_shifts, n_test, D = X_test_tta.shape\n",
        "print('Shapes:', X.shape, y.shape, groups.shape, X_test_tta.shape)\n",
        "\n",
        "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_cb = np.zeros((len(y), num_class), dtype=np.float32)\n",
        "test_cb = np.zeros((n_test, num_class), dtype=np.float32)\n",
        "start = time.time()\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(cv.split(X, y, groups)):\n",
        "    t0 = time.time()\n",
        "    print(f'CB Fold {fold} | train {len(tr_idx)} val {len(va_idx)}')\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_tr = scaler.fit_transform(X[tr_idx])\n",
        "    X_va = scaler.transform(X[va_idx])\n",
        "    # Clip outliers\n",
        "    X_tr = np.clip(X_tr, -5, 5)\n",
        "    X_va = np.clip(X_va, -5, 5)\n",
        "    # Weights\n",
        "    tr_weights = compute_sample_weight('balanced', y=y[tr_idx])\n",
        "    train_pool = Pool(X_tr, label=y[tr_idx], weight=tr_weights)\n",
        "    valid_pool = Pool(X_va, label=y[va_idx])\n",
        "    model = CatBoostClassifier(\n",
        "        loss_function='MultiClass',\n",
        "        eval_metric='MultiClass',\n",
        "        depth=8,\n",
        "        learning_rate=0.05,\n",
        "        l2_leaf_reg=1.0,\n",
        "        iterations=4000,\n",
        "        od_type='Iter',\n",
        "        od_wait=275,\n",
        "        border_count=128,\n",
        "        bootstrap_type='Bernoulli',\n",
        "        subsample=0.8,\n",
        "        rsm=0.8,\n",
        "        random_strength=0.1,\n",
        "        random_seed=42,\n",
        "        thread_count=-1,\n",
        "        verbose=100\n",
        "    )\n",
        "    model.fit(train_pool, eval_set=valid_pool, use_best_model=True, verbose=100)\n",
        "    oof_cb[va_idx] = model.predict_proba(valid_pool)\n",
        "    va_acc = accuracy_score(y[va_idx], oof_cb[va_idx].argmax(1))\n",
        "    print(f'CB Fold {fold} acc: {va_acc:.4f} | elapsed {time.time()-t0:.1f}s')\n",
        "    # Test TTA\n",
        "    fold_test = np.zeros((n_test, num_class), dtype=np.float32)\n",
        "    for s in range(n_shifts):\n",
        "        X_te_s = scaler.transform(X_test_tta[s])\n",
        "        X_te_s = np.clip(X_te_s, -5, 5)\n",
        "        test_pool = Pool(X_te_s)\n",
        "        fold_test += model.predict_proba(test_pool) / n_shifts\n",
        "    test_cb += fold_test / cv.n_splits\n",
        "\n",
        "oof_acc_cb = accuracy_score(y, oof_cb.argmax(1))\n",
        "print(f'CatBoost OOF accuracy: {oof_acc_cb:.4f} | total {time.time()-start:.1f}s')\n",
        "np.save('oof_pooled_cat.npy', oof_cb)\n",
        "# Save test preds with small-TTA suffix\n",
        "np.save('test_pred_pooled_cat_tta50.npy', test_cb)\n",
        "print('Saved CatBoost preds (small TTA) to test_pred_pooled_cat_tta50.npy.')\n",
        "\n",
        "# Optional quick blended submission with existing XGB (if small-TTA available) for sanity\n",
        "if os.path.exists('test_pred_pooled_tta50.npy'):\n",
        "    oof_xgb = np.load('oof_pooled.npy') if os.path.exists('oof_pooled.npy') else None\n",
        "    test_xgb = np.load('test_pred_pooled_tta50.npy')\n",
        "    if oof_xgb is not None:\n",
        "        alpha = 0.5\n",
        "        oof_blend = (1 - alpha) * oof_xgb + alpha * oof_cb\n",
        "        acc = accuracy_score(y, oof_blend.argmax(1))\n",
        "        print(f'Quick XGB+CB alpha=0.5 OOF acc: {acc:.5f}')\n",
        "    pred_idx = ((test_xgb + test_cb) * 0.5).argmax(1)\n",
        "    labels = [CLASSES[i] for i in pred_idx]\n",
        "    sub = pd.DataFrame({'fname': test_fnames, 'label': labels})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved quick blended submission.csv with shape:', sub.shape)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes: (64073, 662) (64073,) (64073,) (5, 6473, 662)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CB Fold 0 | train 52005 val 12068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 2.3264630\ttest: 2.3739912\tbest: 2.3739912 (0)\ttotal: 250ms\tremaining: 16m 38s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\tlearn: 0.7069801\ttest: 1.1439862\tbest: 1.1439862 (100)\ttotal: 25.5s\tremaining: 16m 23s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\tlearn: 0.4353933\ttest: 0.8956426\tbest: 0.8956426 (200)\ttotal: 50.5s\tremaining: 15m 55s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300:\tlearn: 0.3019782\ttest: 0.7541823\tbest: 0.7541823 (300)\ttotal: 1m 15s\tremaining: 15m 25s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400:\tlearn: 0.2252558\ttest: 0.6590188\tbest: 0.6590188 (400)\ttotal: 1m 39s\tremaining: 14m 56s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500:\tlearn: 0.1756789\ttest: 0.5925465\tbest: 0.5925465 (500)\ttotal: 2m 4s\tremaining: 14m 28s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600:\tlearn: 0.1409437\ttest: 0.5410470\tbest: 0.5410470 (600)\ttotal: 2m 28s\tremaining: 14m 1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "700:\tlearn: 0.1161046\ttest: 0.5001383\tbest: 0.5001383 (700)\ttotal: 2m 53s\tremaining: 13m 35s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "800:\tlearn: 0.0983941\ttest: 0.4698836\tbest: 0.4698836 (800)\ttotal: 3m 17s\tremaining: 13m 8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "900:\tlearn: 0.0850112\ttest: 0.4460643\tbest: 0.4460643 (900)\ttotal: 3m 41s\tremaining: 12m 42s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000:\tlearn: 0.0758108\ttest: 0.4289278\tbest: 0.4289278 (1000)\ttotal: 4m 5s\tremaining: 12m 15s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1100:\tlearn: 0.0677302\ttest: 0.4138283\tbest: 0.4138283 (1100)\ttotal: 4m 29s\tremaining: 11m 50s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1200:\tlearn: 0.0613607\ttest: 0.4021944\tbest: 0.4021944 (1200)\ttotal: 4m 53s\tremaining: 11m 24s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1300:\tlearn: 0.0565300\ttest: 0.3929822\tbest: 0.3929822 (1300)\ttotal: 5m 17s\tremaining: 10m 59s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400:\tlearn: 0.0518441\ttest: 0.3839963\tbest: 0.3839963 (1400)\ttotal: 5m 41s\tremaining: 10m 33s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1500:\tlearn: 0.0477145\ttest: 0.3761123\tbest: 0.3761123 (1500)\ttotal: 6m 5s\tremaining: 10m 8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1600:\tlearn: 0.0443664\ttest: 0.3693319\tbest: 0.3693319 (1600)\ttotal: 6m 29s\tremaining: 9m 43s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1700:\tlearn: 0.0414395\ttest: 0.3636248\tbest: 0.3636248 (1700)\ttotal: 6m 53s\tremaining: 9m 18s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1800:\tlearn: 0.0386339\ttest: 0.3581414\tbest: 0.3581414 (1800)\ttotal: 7m 17s\tremaining: 8m 54s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1900:\tlearn: 0.0364115\ttest: 0.3534260\tbest: 0.3534260 (1900)\ttotal: 7m 41s\tremaining: 8m 29s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000:\tlearn: 0.0341994\ttest: 0.3489446\tbest: 0.3489446 (2000)\ttotal: 8m 5s\tremaining: 8m 4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2100:\tlearn: 0.0322877\ttest: 0.3451496\tbest: 0.3451496 (2100)\ttotal: 8m 29s\tremaining: 7m 40s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2200:\tlearn: 0.0306075\ttest: 0.3420797\tbest: 0.3420797 (2200)\ttotal: 8m 53s\tremaining: 7m 15s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2300:\tlearn: 0.0290836\ttest: 0.3394389\tbest: 0.3394389 (2300)\ttotal: 9m 17s\tremaining: 6m 51s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2400:\tlearn: 0.0276647\ttest: 0.3366504\tbest: 0.3366504 (2400)\ttotal: 9m 40s\tremaining: 6m 26s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500:\tlearn: 0.0262575\ttest: 0.3338300\tbest: 0.3338300 (2500)\ttotal: 10m 4s\tremaining: 6m 2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2600:\tlearn: 0.0250719\ttest: 0.3314577\tbest: 0.3314577 (2600)\ttotal: 10m 28s\tremaining: 5m 38s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2700:\tlearn: 0.0239791\ttest: 0.3294760\tbest: 0.3294760 (2700)\ttotal: 10m 52s\tremaining: 5m 13s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2800:\tlearn: 0.0229276\ttest: 0.3274562\tbest: 0.3274562 (2800)\ttotal: 11m 16s\tremaining: 4m 49s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2900:\tlearn: 0.0219935\ttest: 0.3256126\tbest: 0.3256110 (2899)\ttotal: 11m 40s\tremaining: 4m 25s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000:\tlearn: 0.0210884\ttest: 0.3238271\tbest: 0.3238271 (3000)\ttotal: 12m 4s\tremaining: 4m 1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3100:\tlearn: 0.0202327\ttest: 0.3221863\tbest: 0.3221863 (3100)\ttotal: 12m 28s\tremaining: 3m 36s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3200:\tlearn: 0.0194385\ttest: 0.3207482\tbest: 0.3207482 (3200)\ttotal: 12m 51s\tremaining: 3m 12s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3300:\tlearn: 0.0186864\ttest: 0.3191236\tbest: 0.3191236 (3300)\ttotal: 13m 15s\tremaining: 2m 48s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3400:\tlearn: 0.0180751\ttest: 0.3180145\tbest: 0.3180145 (3400)\ttotal: 13m 39s\tremaining: 2m 24s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3500:\tlearn: 0.0174056\ttest: 0.3167781\tbest: 0.3167781 (3500)\ttotal: 14m 3s\tremaining: 2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3600:\tlearn: 0.0168033\ttest: 0.3159217\tbest: 0.3159217 (3600)\ttotal: 14m 27s\tremaining: 1m 36s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3700:\tlearn: 0.0161734\ttest: 0.3149079\tbest: 0.3149079 (3700)\ttotal: 14m 51s\tremaining: 1m 11s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3800:\tlearn: 0.0156339\ttest: 0.3138496\tbest: 0.3138496 (3800)\ttotal: 15m 14s\tremaining: 47.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3900:\tlearn: 0.0151371\ttest: 0.3130417\tbest: 0.3130417 (3900)\ttotal: 15m 38s\tremaining: 23.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3999:\tlearn: 0.0146501\ttest: 0.3120864\tbest: 0.3120864 (3999)\ttotal: 16m 2s\tremaining: 0us\n\nbestTest = 0.3120863855\nbestIteration = 3999\n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CB Fold 0 acc: 0.8978 | elapsed 963.7s\nCB Fold 1 | train 51074 val 12999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 2.3232673\ttest: 2.3758249\tbest: 2.3758249 (0)\ttotal: 250ms\tremaining: 16m 39s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\tlearn: 0.6941784\ttest: 1.1649530\tbest: 1.1649530 (100)\ttotal: 25s\tremaining: 16m 3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\tlearn: 0.4202706\ttest: 0.9210064\tbest: 0.9210064 (200)\ttotal: 49.6s\tremaining: 15m 38s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300:\tlearn: 0.2888673\ttest: 0.7825765\tbest: 0.7825765 (300)\ttotal: 1m 14s\tremaining: 15m 10s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400:\tlearn: 0.2136667\ttest: 0.6913296\tbest: 0.6913296 (400)\ttotal: 1m 38s\tremaining: 14m 43s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500:\tlearn: 0.1643169\ttest: 0.6231237\tbest: 0.6231237 (500)\ttotal: 2m 2s\tremaining: 14m 17s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600:\tlearn: 0.1318963\ttest: 0.5747616\tbest: 0.5747616 (600)\ttotal: 2m 26s\tremaining: 13m 50s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "700:\tlearn: 0.1086322\ttest: 0.5367069\tbest: 0.5367069 (700)\ttotal: 2m 50s\tremaining: 13m 24s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "800:\tlearn: 0.0921461\ttest: 0.5088552\tbest: 0.5088552 (800)\ttotal: 3m 14s\tremaining: 12m 58s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "900:\tlearn: 0.0806005\ttest: 0.4883024\tbest: 0.4883024 (900)\ttotal: 3m 38s\tremaining: 12m 32s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000:\tlearn: 0.0711788\ttest: 0.4710500\tbest: 0.4710500 (1000)\ttotal: 4m 2s\tremaining: 12m 6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1100:\tlearn: 0.0638696\ttest: 0.4578410\tbest: 0.4578410 (1100)\ttotal: 4m 26s\tremaining: 11m 41s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1200:\tlearn: 0.0576781\ttest: 0.4459515\tbest: 0.4459515 (1200)\ttotal: 4m 50s\tremaining: 11m 15s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1300:\tlearn: 0.0527036\ttest: 0.4365355\tbest: 0.4365355 (1300)\ttotal: 5m 13s\tremaining: 10m 50s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400:\tlearn: 0.0484658\ttest: 0.4289461\tbest: 0.4289461 (1400)\ttotal: 5m 37s\tremaining: 10m 25s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1500:\tlearn: 0.0451095\ttest: 0.4226403\tbest: 0.4226403 (1500)\ttotal: 6m\tremaining: 10m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1600:\tlearn: 0.0419418\ttest: 0.4171951\tbest: 0.4171951 (1600)\ttotal: 6m 24s\tremaining: 9m 36s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1700:\tlearn: 0.0390595\ttest: 0.4120162\tbest: 0.4120162 (1700)\ttotal: 6m 48s\tremaining: 9m 11s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1800:\tlearn: 0.0364951\ttest: 0.4073155\tbest: 0.4073155 (1800)\ttotal: 7m 11s\tremaining: 8m 47s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1900:\tlearn: 0.0344556\ttest: 0.4035750\tbest: 0.4035750 (1900)\ttotal: 7m 35s\tremaining: 8m 22s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000:\tlearn: 0.0324584\ttest: 0.4003025\tbest: 0.4003025 (2000)\ttotal: 7m 59s\tremaining: 7m 58s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2100:\tlearn: 0.0307605\ttest: 0.3972531\tbest: 0.3972531 (2100)\ttotal: 8m 22s\tremaining: 7m 34s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2200:\tlearn: 0.0291044\ttest: 0.3941476\tbest: 0.3941476 (2200)\ttotal: 8m 46s\tremaining: 7m 10s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2300:\tlearn: 0.0275866\ttest: 0.3916791\tbest: 0.3916791 (2300)\ttotal: 9m 9s\tremaining: 6m 46s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2400:\tlearn: 0.0262014\ttest: 0.3891985\tbest: 0.3891985 (2400)\ttotal: 9m 33s\tremaining: 6m 22s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500:\tlearn: 0.0249721\ttest: 0.3872451\tbest: 0.3872451 (2500)\ttotal: 9m 57s\tremaining: 5m 57s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2600:\tlearn: 0.0237969\ttest: 0.3853876\tbest: 0.3853876 (2600)\ttotal: 10m 20s\tremaining: 5m 33s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2700:\tlearn: 0.0226884\ttest: 0.3835729\tbest: 0.3835729 (2700)\ttotal: 10m 44s\tremaining: 5m 9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2800:\tlearn: 0.0216759\ttest: 0.3817013\tbest: 0.3817013 (2800)\ttotal: 11m 8s\tremaining: 4m 46s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2900:\tlearn: 0.0207299\ttest: 0.3802288\tbest: 0.3802288 (2900)\ttotal: 11m 31s\tremaining: 4m 22s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000:\tlearn: 0.0198695\ttest: 0.3789283\tbest: 0.3789283 (3000)\ttotal: 11m 55s\tremaining: 3m 58s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3100:\tlearn: 0.0191066\ttest: 0.3779728\tbest: 0.3779728 (3100)\ttotal: 12m 18s\tremaining: 3m 34s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3200:\tlearn: 0.0183280\ttest: 0.3768577\tbest: 0.3768577 (3200)\ttotal: 12m 42s\tremaining: 3m 10s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3300:\tlearn: 0.0176244\ttest: 0.3757814\tbest: 0.3757814 (3300)\ttotal: 13m 6s\tremaining: 2m 46s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3400:\tlearn: 0.0169634\ttest: 0.3749378\tbest: 0.3749289 (3394)\ttotal: 13m 29s\tremaining: 2m 22s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3500:\tlearn: 0.0163881\ttest: 0.3740105\tbest: 0.3740105 (3500)\ttotal: 13m 53s\tremaining: 1m 58s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3600:\tlearn: 0.0158175\ttest: 0.3731988\tbest: 0.3731988 (3600)\ttotal: 14m 16s\tremaining: 1m 34s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3700:\tlearn: 0.0152425\ttest: 0.3726561\tbest: 0.3726561 (3700)\ttotal: 14m 40s\tremaining: 1m 11s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3800:\tlearn: 0.0146790\ttest: 0.3718222\tbest: 0.3718222 (3800)\ttotal: 15m 4s\tremaining: 47.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3900:\tlearn: 0.0141670\ttest: 0.3711963\tbest: 0.3711963 (3900)\ttotal: 15m 27s\tremaining: 23.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3999:\tlearn: 0.0136856\ttest: 0.3705486\tbest: 0.3705187 (3998)\ttotal: 15m 51s\tremaining: 0us\n\nbestTest = 0.370518738\nbestIteration = 3998\n\nShrink model to first 3999 iterations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CB Fold 1 acc: 0.8868 | elapsed 952.3s\nCB Fold 2 | train 51792 val 12281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 2.3263974\ttest: 2.3746902\tbest: 2.3746902 (0)\ttotal: 243ms\tremaining: 16m 11s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\tlearn: 0.7078816\ttest: 1.1359938\tbest: 1.1359938 (100)\ttotal: 25.1s\tremaining: 16m 8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\tlearn: 0.4348035\ttest: 0.8940727\tbest: 0.8940727 (200)\ttotal: 49.9s\tremaining: 15m 43s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300:\tlearn: 0.3013509\ttest: 0.7596887\tbest: 0.7596887 (300)\ttotal: 1m 14s\tremaining: 15m 15s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400:\tlearn: 0.2222506\ttest: 0.6652118\tbest: 0.6652118 (400)\ttotal: 1m 39s\tremaining: 14m 48s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500:\tlearn: 0.1713940\ttest: 0.5958891\tbest: 0.5958891 (500)\ttotal: 2m 3s\tremaining: 14m 22s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600:\tlearn: 0.1380385\ttest: 0.5460217\tbest: 0.5460217 (600)\ttotal: 2m 27s\tremaining: 13m 55s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "700:\tlearn: 0.1135662\ttest: 0.5060100\tbest: 0.5060100 (700)\ttotal: 2m 51s\tremaining: 13m 29s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "800:\tlearn: 0.0962968\ttest: 0.4772250\tbest: 0.4772250 (800)\ttotal: 3m 16s\tremaining: 13m 3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "900:\tlearn: 0.0835554\ttest: 0.4545873\tbest: 0.4545873 (900)\ttotal: 3m 40s\tremaining: 12m 37s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000:\tlearn: 0.0732972\ttest: 0.4355442\tbest: 0.4355442 (1000)\ttotal: 4m 4s\tremaining: 12m 12s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1100:\tlearn: 0.0660021\ttest: 0.4219736\tbest: 0.4219736 (1100)\ttotal: 4m 28s\tremaining: 11m 46s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1200:\tlearn: 0.0600859\ttest: 0.4105227\tbest: 0.4105227 (1200)\ttotal: 4m 52s\tremaining: 11m 21s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1300:\tlearn: 0.0545950\ttest: 0.4000863\tbest: 0.4000863 (1300)\ttotal: 5m 16s\tremaining: 10m 56s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400:\tlearn: 0.0501761\ttest: 0.3913041\tbest: 0.3913041 (1400)\ttotal: 5m 40s\tremaining: 10m 31s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1500:\tlearn: 0.0464105\ttest: 0.3836140\tbest: 0.3836140 (1500)\ttotal: 6m 4s\tremaining: 10m 6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1600:\tlearn: 0.0431582\ttest: 0.3772631\tbest: 0.3772631 (1600)\ttotal: 6m 28s\tremaining: 9m 41s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1700:\tlearn: 0.0403533\ttest: 0.3716979\tbest: 0.3716979 (1700)\ttotal: 6m 51s\tremaining: 9m 16s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1800:\tlearn: 0.0378418\ttest: 0.3667826\tbest: 0.3667826 (1800)\ttotal: 7m 15s\tremaining: 8m 52s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1900:\tlearn: 0.0353836\ttest: 0.3620776\tbest: 0.3620776 (1900)\ttotal: 7m 39s\tremaining: 8m 27s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000:\tlearn: 0.0334809\ttest: 0.3579602\tbest: 0.3579602 (2000)\ttotal: 8m 3s\tremaining: 8m 3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2100:\tlearn: 0.0315852\ttest: 0.3541687\tbest: 0.3541687 (2100)\ttotal: 8m 27s\tremaining: 7m 38s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2200:\tlearn: 0.0299228\ttest: 0.3508575\tbest: 0.3508575 (2200)\ttotal: 8m 51s\tremaining: 7m 14s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2300:\tlearn: 0.0284468\ttest: 0.3480874\tbest: 0.3480874 (2300)\ttotal: 9m 15s\tremaining: 6m 49s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2400:\tlearn: 0.0270961\ttest: 0.3452448\tbest: 0.3452448 (2400)\ttotal: 9m 38s\tremaining: 6m 25s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500:\tlearn: 0.0257562\ttest: 0.3425088\tbest: 0.3425088 (2500)\ttotal: 10m 2s\tremaining: 6m 1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2600:\tlearn: 0.0246089\ttest: 0.3403039\tbest: 0.3403039 (2600)\ttotal: 10m 26s\tremaining: 5m 37s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2700:\tlearn: 0.0234915\ttest: 0.3383154\tbest: 0.3383154 (2700)\ttotal: 10m 50s\tremaining: 5m 12s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2800:\tlearn: 0.0224288\ttest: 0.3362533\tbest: 0.3362533 (2800)\ttotal: 11m 14s\tremaining: 4m 48s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2900:\tlearn: 0.0214471\ttest: 0.3342981\tbest: 0.3342981 (2900)\ttotal: 11m 38s\tremaining: 4m 24s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000:\tlearn: 0.0205672\ttest: 0.3327747\tbest: 0.3327747 (3000)\ttotal: 12m 1s\tremaining: 4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3100:\tlearn: 0.0197147\ttest: 0.3311085\tbest: 0.3311085 (3100)\ttotal: 12m 25s\tremaining: 3m 36s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3200:\tlearn: 0.0189339\ttest: 0.3296088\tbest: 0.3296088 (3200)\ttotal: 12m 49s\tremaining: 3m 12s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3300:\tlearn: 0.0182114\ttest: 0.3282727\tbest: 0.3282727 (3300)\ttotal: 13m 13s\tremaining: 2m 47s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3400:\tlearn: 0.0175013\ttest: 0.3268050\tbest: 0.3268050 (3400)\ttotal: 13m 37s\tremaining: 2m 23s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3500:\tlearn: 0.0168963\ttest: 0.3255282\tbest: 0.3255282 (3500)\ttotal: 14m\tremaining: 1m 59s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3600:\tlearn: 0.0163134\ttest: 0.3242678\tbest: 0.3242678 (3600)\ttotal: 14m 24s\tremaining: 1m 35s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3700:\tlearn: 0.0157096\ttest: 0.3230073\tbest: 0.3230041 (3699)\ttotal: 14m 48s\tremaining: 1m 11s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3800:\tlearn: 0.0151318\ttest: 0.3221693\tbest: 0.3221693 (3800)\ttotal: 15m 12s\tremaining: 47.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3900:\tlearn: 0.0146197\ttest: 0.3212543\tbest: 0.3212543 (3900)\ttotal: 15m 36s\tremaining: 23.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3999:\tlearn: 0.0141087\ttest: 0.3202681\tbest: 0.3202681 (3999)\ttotal: 16m\tremaining: 0us\n\nbestTest = 0.3202681163\nbestIteration = 3999\n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CB Fold 2 acc: 0.8954 | elapsed 961.4s\nCB Fold 3 | train 50819 val 13254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 2.3271926\ttest: 2.3776545\tbest: 2.3776545 (0)\ttotal: 242ms\tremaining: 16m 6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\tlearn: 0.6960294\ttest: 1.1453798\tbest: 1.1453798 (100)\ttotal: 24.9s\tremaining: 16m 2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\tlearn: 0.4213800\ttest: 0.9030178\tbest: 0.9030178 (200)\ttotal: 49.6s\tremaining: 15m 36s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300:\tlearn: 0.2888599\ttest: 0.7640228\tbest: 0.7640228 (300)\ttotal: 1m 14s\tremaining: 15m 9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400:\tlearn: 0.2137409\ttest: 0.6735064\tbest: 0.6735064 (400)\ttotal: 1m 38s\tremaining: 14m 42s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500:\tlearn: 0.1633029\ttest: 0.6055639\tbest: 0.6055639 (500)\ttotal: 2m 2s\tremaining: 14m 16s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600:\tlearn: 0.1309825\ttest: 0.5567421\tbest: 0.5567421 (600)\ttotal: 2m 26s\tremaining: 13m 50s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "700:\tlearn: 0.1085605\ttest: 0.5205854\tbest: 0.5205854 (700)\ttotal: 2m 50s\tremaining: 13m 23s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "800:\tlearn: 0.0920992\ttest: 0.4941426\tbest: 0.4941426 (800)\ttotal: 3m 14s\tremaining: 12m 58s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "900:\tlearn: 0.0799258\ttest: 0.4727006\tbest: 0.4727006 (900)\ttotal: 3m 38s\tremaining: 12m 32s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000:\tlearn: 0.0708586\ttest: 0.4564972\tbest: 0.4564972 (1000)\ttotal: 4m 2s\tremaining: 12m 6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1100:\tlearn: 0.0639236\ttest: 0.4435881\tbest: 0.4435881 (1100)\ttotal: 4m 26s\tremaining: 11m 41s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1200:\tlearn: 0.0576446\ttest: 0.4318655\tbest: 0.4318655 (1200)\ttotal: 4m 50s\tremaining: 11m 16s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1300:\tlearn: 0.0524606\ttest: 0.4226319\tbest: 0.4226319 (1300)\ttotal: 5m 13s\tremaining: 10m 51s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400:\tlearn: 0.0485733\ttest: 0.4155394\tbest: 0.4155394 (1400)\ttotal: 5m 37s\tremaining: 10m 26s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1500:\tlearn: 0.0451823\ttest: 0.4094472\tbest: 0.4094472 (1500)\ttotal: 6m 1s\tremaining: 10m 1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1600:\tlearn: 0.0421804\ttest: 0.4039078\tbest: 0.4039078 (1600)\ttotal: 6m 24s\tremaining: 9m 36s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1700:\tlearn: 0.0392520\ttest: 0.3985592\tbest: 0.3985592 (1700)\ttotal: 6m 48s\tremaining: 9m 12s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1800:\tlearn: 0.0368595\ttest: 0.3939059\tbest: 0.3939059 (1800)\ttotal: 7m 12s\tremaining: 8m 47s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1900:\tlearn: 0.0346366\ttest: 0.3897957\tbest: 0.3897957 (1900)\ttotal: 7m 35s\tremaining: 8m 23s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000:\tlearn: 0.0326333\ttest: 0.3859608\tbest: 0.3859608 (2000)\ttotal: 7m 59s\tremaining: 7m 59s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2100:\tlearn: 0.0309505\ttest: 0.3828778\tbest: 0.3828778 (2100)\ttotal: 8m 23s\tremaining: 7m 34s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2200:\tlearn: 0.0294659\ttest: 0.3800981\tbest: 0.3800981 (2200)\ttotal: 8m 46s\tremaining: 7m 10s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2300:\tlearn: 0.0279634\ttest: 0.3773269\tbest: 0.3773269 (2300)\ttotal: 9m 10s\tremaining: 6m 46s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2400:\tlearn: 0.0265859\ttest: 0.3748180\tbest: 0.3748180 (2400)\ttotal: 9m 34s\tremaining: 6m 22s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500:\tlearn: 0.0252216\ttest: 0.3727529\tbest: 0.3727529 (2500)\ttotal: 9m 57s\tremaining: 5m 58s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2600:\tlearn: 0.0240096\ttest: 0.3706941\tbest: 0.3706941 (2600)\ttotal: 10m 21s\tremaining: 5m 34s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2700:\tlearn: 0.0229653\ttest: 0.3688104\tbest: 0.3688104 (2700)\ttotal: 10m 45s\tremaining: 5m 10s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2800:\tlearn: 0.0219878\ttest: 0.3673083\tbest: 0.3673083 (2800)\ttotal: 11m 8s\tremaining: 4m 46s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2900:\tlearn: 0.0210545\ttest: 0.3658638\tbest: 0.3658609 (2899)\ttotal: 11m 32s\tremaining: 4m 22s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000:\tlearn: 0.0202456\ttest: 0.3644087\tbest: 0.3644087 (3000)\ttotal: 11m 55s\tremaining: 3m 58s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3100:\tlearn: 0.0194535\ttest: 0.3632283\tbest: 0.3632283 (3100)\ttotal: 12m 19s\tremaining: 3m 34s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3200:\tlearn: 0.0187420\ttest: 0.3620000\tbest: 0.3620000 (3200)\ttotal: 12m 43s\tremaining: 3m 10s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3300:\tlearn: 0.0179839\ttest: 0.3608879\tbest: 0.3608879 (3300)\ttotal: 13m 6s\tremaining: 2m 46s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3400:\tlearn: 0.0173328\ttest: 0.3597441\tbest: 0.3597441 (3400)\ttotal: 13m 30s\tremaining: 2m 22s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3500:\tlearn: 0.0167255\ttest: 0.3588567\tbest: 0.3588567 (3500)\ttotal: 13m 53s\tremaining: 1m 58s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3600:\tlearn: 0.0160669\ttest: 0.3579155\tbest: 0.3579155 (3600)\ttotal: 14m 17s\tremaining: 1m 34s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3700:\tlearn: 0.0154602\ttest: 0.3569888\tbest: 0.3569888 (3700)\ttotal: 14m 40s\tremaining: 1m 11s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3800:\tlearn: 0.0149648\ttest: 0.3564739\tbest: 0.3564677 (3799)\ttotal: 15m 4s\tremaining: 47.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3900:\tlearn: 0.0144493\ttest: 0.3555559\tbest: 0.3555337 (3899)\ttotal: 15m 27s\tremaining: 23.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3999:\tlearn: 0.0139592\ttest: 0.3545951\tbest: 0.3545951 (3999)\ttotal: 15m 51s\tremaining: 0us\n\nbestTest = 0.354595149\nbestIteration = 3999\n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CB Fold 3 acc: 0.8858 | elapsed 952.5s\nCB Fold 4 | train 50602 val 13471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 2.3267109\ttest: 2.3755326\tbest: 2.3755326 (0)\ttotal: 245ms\tremaining: 16m 19s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100:\tlearn: 0.7006199\ttest: 1.1549268\tbest: 1.1549268 (100)\ttotal: 24.8s\tremaining: 15m 56s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200:\tlearn: 0.4215549\ttest: 0.9112460\tbest: 0.9112460 (200)\ttotal: 49.4s\tremaining: 15m 33s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300:\tlearn: 0.2903616\ttest: 0.7760815\tbest: 0.7760815 (300)\ttotal: 1m 13s\tremaining: 15m 5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400:\tlearn: 0.2132787\ttest: 0.6836565\tbest: 0.6836565 (400)\ttotal: 1m 37s\tremaining: 14m 38s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500:\tlearn: 0.1640590\ttest: 0.6168159\tbest: 0.6168159 (500)\ttotal: 2m 2s\tremaining: 14m 12s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600:\tlearn: 0.1318998\ttest: 0.5675788\tbest: 0.5675788 (600)\ttotal: 2m 26s\tremaining: 13m 46s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "700:\tlearn: 0.1086745\ttest: 0.5294547\tbest: 0.5294547 (700)\ttotal: 2m 50s\tremaining: 13m 20s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "800:\tlearn: 0.0916792\ttest: 0.5002789\tbest: 0.5002789 (800)\ttotal: 3m 13s\tremaining: 12m 54s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "900:\tlearn: 0.0794245\ttest: 0.4784651\tbest: 0.4784651 (900)\ttotal: 3m 37s\tremaining: 12m 29s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000:\tlearn: 0.0705381\ttest: 0.4619954\tbest: 0.4619954 (1000)\ttotal: 4m 1s\tremaining: 12m 3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1100:\tlearn: 0.0637106\ttest: 0.4486748\tbest: 0.4486748 (1100)\ttotal: 4m 25s\tremaining: 11m 38s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1200:\tlearn: 0.0577351\ttest: 0.4370898\tbest: 0.4370898 (1200)\ttotal: 4m 48s\tremaining: 11m 13s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1300:\tlearn: 0.0523429\ttest: 0.4264635\tbest: 0.4264635 (1300)\ttotal: 5m 12s\tremaining: 10m 48s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400:\tlearn: 0.0481001\ttest: 0.4178567\tbest: 0.4178567 (1400)\ttotal: 5m 36s\tremaining: 10m 23s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1500:\tlearn: 0.0446008\ttest: 0.4110411\tbest: 0.4110411 (1500)\ttotal: 5m 59s\tremaining: 9m 59s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1600:\tlearn: 0.0415540\ttest: 0.4050983\tbest: 0.4050983 (1600)\ttotal: 6m 23s\tremaining: 9m 34s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1700:\tlearn: 0.0387012\ttest: 0.3994924\tbest: 0.3994924 (1700)\ttotal: 6m 47s\tremaining: 9m 10s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1800:\tlearn: 0.0360483\ttest: 0.3942555\tbest: 0.3942555 (1800)\ttotal: 7m 10s\tremaining: 8m 45s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1900:\tlearn: 0.0338500\ttest: 0.3900269\tbest: 0.3900269 (1900)\ttotal: 7m 34s\tremaining: 8m 21s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000:\tlearn: 0.0319852\ttest: 0.3862966\tbest: 0.3862962 (1999)\ttotal: 7m 57s\tremaining: 7m 57s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2100:\tlearn: 0.0303472\ttest: 0.3834340\tbest: 0.3834340 (2100)\ttotal: 8m 21s\tremaining: 7m 33s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2200:\tlearn: 0.0287513\ttest: 0.3800912\tbest: 0.3800912 (2200)\ttotal: 8m 44s\tremaining: 7m 8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2300:\tlearn: 0.0272852\ttest: 0.3770562\tbest: 0.3770562 (2300)\ttotal: 9m 8s\tremaining: 6m 44s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2400:\tlearn: 0.0259772\ttest: 0.3746139\tbest: 0.3746139 (2400)\ttotal: 9m 31s\tremaining: 6m 20s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500:\tlearn: 0.0247799\ttest: 0.3723426\tbest: 0.3723426 (2500)\ttotal: 9m 55s\tremaining: 5m 56s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2600:\tlearn: 0.0236496\ttest: 0.3700972\tbest: 0.3700972 (2600)\ttotal: 10m 18s\tremaining: 5m 32s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2700:\tlearn: 0.0225428\ttest: 0.3679053\tbest: 0.3679053 (2700)\ttotal: 10m 42s\tremaining: 5m 9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2800:\tlearn: 0.0214974\ttest: 0.3660534\tbest: 0.3660525 (2799)\ttotal: 11m 6s\tremaining: 4m 45s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2900:\tlearn: 0.0206231\ttest: 0.3645403\tbest: 0.3645403 (2900)\ttotal: 11m 29s\tremaining: 4m 21s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000:\tlearn: 0.0197952\ttest: 0.3630944\tbest: 0.3630944 (3000)\ttotal: 11m 53s\tremaining: 3m 57s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3100:\tlearn: 0.0189550\ttest: 0.3616646\tbest: 0.3616631 (3099)\ttotal: 12m 16s\tremaining: 3m 33s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3200:\tlearn: 0.0182131\ttest: 0.3603911\tbest: 0.3603845 (3199)\ttotal: 12m 40s\tremaining: 3m 9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3300:\tlearn: 0.0174958\ttest: 0.3590287\tbest: 0.3590287 (3300)\ttotal: 13m 3s\tremaining: 2m 46s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3400:\tlearn: 0.0168547\ttest: 0.3579352\tbest: 0.3579352 (3400)\ttotal: 13m 27s\tremaining: 2m 22s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3500:\tlearn: 0.0162005\ttest: 0.3567097\tbest: 0.3567097 (3500)\ttotal: 13m 51s\tremaining: 1m 58s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3600:\tlearn: 0.0156709\ttest: 0.3560293\tbest: 0.3560293 (3600)\ttotal: 14m 14s\tremaining: 1m 34s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3700:\tlearn: 0.0151157\ttest: 0.3551919\tbest: 0.3551817 (3693)\ttotal: 14m 38s\tremaining: 1m 10s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3800:\tlearn: 0.0145841\ttest: 0.3544344\tbest: 0.3544344 (3800)\ttotal: 15m 1s\tremaining: 47.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3900:\tlearn: 0.0140934\ttest: 0.3536008\tbest: 0.3536008 (3900)\ttotal: 15m 25s\tremaining: 23.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3999:\tlearn: 0.0135819\ttest: 0.3527502\tbest: 0.3527421 (3998)\ttotal: 15m 48s\tremaining: 0us\n\nbestTest = 0.3527421281\nbestIteration = 3998\n\nShrink model to first 3999 iterations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CB Fold 4 acc: 0.8853 | elapsed 949.7s\nCatBoost OOF accuracy: 0.8900 | total 4782.1s\nSaved CatBoost preds (small TTA) to test_pred_pooled_cat_tta50.npy.\nQuick XGB+CB alpha=0.5 OOF acc: 0.89503\nSaved quick blended submission.csv with shape: (6473, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "c214f1bc-f17e-435a-8625-79610491c237",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Blending: Alpha sweep (XGB+CB) and LogisticRegression stacker on OOF probs\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "CLASSES = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "num_class = len(CLASSES)\n",
        "\n",
        "paths_ok = all([\n",
        "    os.path.exists('oof_pooled.npy'),\n",
        "    os.path.exists('test_pred_pooled.npy'),\n",
        "    os.path.exists('oof_pooled_cat.npy'),\n",
        "    os.path.exists('test_pred_pooled_cat.npy'),\n",
        "    os.path.exists('y_train_pooled.npy'),\n",
        "    os.path.exists('groups_pooled.npy'),\n",
        "    os.path.exists('test_fnames_pooled.csv')\n",
        "])\n",
        "\n",
        "if not paths_ok:\n",
        "    print('Required files not found yet. Ensure XGB (Cell 7) and CatBoost (Cell 8) have finished.')\n",
        "else:\n",
        "    y = np.load('y_train_pooled.npy')\n",
        "    groups = np.load('groups_pooled.npy')\n",
        "    oof_xgb = np.load('oof_pooled.npy')\n",
        "    test_xgb = np.load('test_pred_pooled.npy')\n",
        "    oof_cb = np.load('oof_pooled_cat.npy')\n",
        "    test_cb = np.load('test_pred_pooled_cat.npy')\n",
        "    test_fnames = pd.read_csv('test_fnames_pooled.csv', header=None)[0].values\n",
        "\n",
        "    # 1) Alpha sweep\n",
        "    best_alpha, best_acc = None, -1.0\n",
        "    for alpha in np.linspace(0.0, 1.0, 21):\n",
        "        oof_blend = (1 - alpha) * oof_xgb + alpha * oof_cb\n",
        "        acc = accuracy_score(y, oof_blend.argmax(1))\n",
        "        print(f'Alpha {alpha:.2f} -> OOF acc {acc:.5f}')\n",
        "        if acc > best_acc:\n",
        "            best_acc, best_alpha = acc, alpha\n",
        "    print(f'Best alpha: {best_alpha:.2f} | OOF acc: {best_acc:.5f}')\n",
        "    test_blend_alpha = (1 - best_alpha) * test_xgb + best_alpha * test_cb\n",
        "    np.save('test_pred_blend_alpha.npy', test_blend_alpha)\n",
        "\n",
        "    # 2) LogisticRegression stacker on probs with SGKF\n",
        "    X_meta = np.concatenate([oof_xgb, oof_cb], axis=1)  # (N, 24)\n",
        "    X_test_meta = np.concatenate([test_xgb, test_cb], axis=1)  # (T, 24)\n",
        "    cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    oof_stack = np.zeros_like(oof_xgb, dtype=np.float32)\n",
        "    test_stack = np.zeros_like(test_xgb, dtype=np.float32)\n",
        "    for fold, (tr_idx, va_idx) in enumerate(cv.split(X_meta, y, groups)):\n",
        "        t0 = time.time()\n",
        "        X_tr, X_va = X_meta[tr_idx], X_meta[va_idx]\n",
        "        y_tr = y[tr_idx]\n",
        "        clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=2000, n_jobs=-1, C=1.0, random_state=42)\n",
        "        clf.fit(X_tr, y_tr)\n",
        "        oof_stack[va_idx] = clf.predict_proba(X_va)\n",
        "        test_stack += clf.predict_proba(X_test_meta) / cv.n_splits\n",
        "        print(f'Stacker fold {fold} done in {time.time()-t0:.1f}s')\n",
        "    oof_acc_stack = accuracy_score(y, oof_stack.argmax(1))\n",
        "    print(f'LR stacker OOF acc: {oof_acc_stack:.5f}')\n",
        "    np.save('oof_blend_stack.npy', oof_stack)\n",
        "    np.save('test_pred_blend_stack.npy', test_stack)\n",
        "\n",
        "    # Choose best of alpha vs stacker\n",
        "    use_stacker = oof_acc_stack > best_acc\n",
        "    final_test = test_stack if use_stacker else test_blend_alpha\n",
        "    choice = 'stacker' if use_stacker else f'alpha={best_alpha:.2f}'\n",
        "    final_oof_acc = oof_acc_stack if use_stacker else best_acc\n",
        "    print(f'Final choice: {choice} | OOF acc: {final_oof_acc:.5f}')\n",
        "\n",
        "    # Build submission\n",
        "    pred_idx = final_test.argmax(1)\n",
        "    labels = [CLASSES[i] for i in pred_idx]\n",
        "    sub = pd.DataFrame({'fname': test_fnames, 'label': labels})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv', sub.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha 0.00 -> OOF acc 0.83800\nAlpha 0.05 -> OOF acc 0.83809\nAlpha 0.10 -> OOF acc 0.83820\nAlpha 0.15 -> OOF acc 0.83812\nAlpha 0.20 -> OOF acc 0.83758\nAlpha 0.25 -> OOF acc 0.83798\nAlpha 0.30 -> OOF acc 0.83751\nAlpha 0.35 -> OOF acc 0.83798\nAlpha 0.40 -> OOF acc 0.83728\nAlpha 0.45 -> OOF acc 0.83641\nAlpha 0.50 -> OOF acc 0.83517\nAlpha 0.55 -> OOF acc 0.83378\nAlpha 0.60 -> OOF acc 0.83205\nAlpha 0.65 -> OOF acc 0.82974\nAlpha 0.70 -> OOF acc 0.82723\nAlpha 0.75 -> OOF acc 0.82425\nAlpha 0.80 -> OOF acc 0.82019\nAlpha 0.85 -> OOF acc 0.81555\nAlpha 0.90 -> OOF acc 0.81098\nAlpha 0.95 -> OOF acc 0.80550\nAlpha 1.00 -> OOF acc 0.79939\nBest alpha: 0.10 | OOF acc: 0.83820\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacker fold 0 done in 2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacker fold 1 done in 2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacker fold 2 done in 2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacker fold 3 done in 2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacker fold 4 done in 2.5s\nLR stacker OOF acc: 0.83659\nFinal choice: alpha=0.10 | OOF acc: 0.83820\nSaved submission.csv (6473, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "c0748c57-110b-452b-8b8f-f0d508a54060",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# LightGBM on pooled features with SGKF, weights, clipping, 5-shift TTA\n",
        "import os, sys, subprocess, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except Exception as e:\n",
        "    print('Installing lightgbm...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'lightgbm==4.6.0'], check=True)\n",
        "    import lightgbm as lgb\n",
        "\n",
        "CLASSES = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "num_class = len(CLASSES)\n",
        "\n",
        "train_feat = 'X_train_pooled.npy'\n",
        "train_y = 'y_train_pooled.npy'\n",
        "train_groups = 'groups_pooled.npy'\n",
        "# Use small-shift TTA features for test\n",
        "test_feat_tta = 'X_test_pooled_tta_small.npy'\n",
        "test_fnames_csv = 'test_fnames_pooled_small.csv'\n",
        "\n",
        "assert os.path.exists(train_feat) and os.path.exists(train_y) and os.path.exists(train_groups), 'Missing train pooled features'\n",
        "assert os.path.exists(test_feat_tta) and os.path.exists(test_fnames_csv), 'Missing test pooled TTA features'\n",
        "\n",
        "X = np.load(train_feat)\n",
        "y = np.load(train_y)\n",
        "groups = np.load(train_groups)\n",
        "X_test_tta = np.load(test_feat_tta)  # [n_shifts, N, D]\n",
        "test_fnames = pd.read_csv(test_fnames_csv, header=None)[0].values\n",
        "n_shifts, n_test, D = X_test_tta.shape\n",
        "print('Shapes:', X.shape, y.shape, groups.shape, X_test_tta.shape)\n",
        "\n",
        "params = dict(\n",
        "    objective='multiclass',\n",
        "    num_class=num_class,\n",
        "    metric='multi_logloss',\n",
        "    learning_rate=0.05,\n",
        "    num_leaves=63,\n",
        "    max_depth=7,\n",
        "    feature_fraction=0.8,\n",
        "    bagging_fraction=0.8,\n",
        "    bagging_freq=1,\n",
        "    min_data_in_leaf=30,\n",
        "    lambda_l1=0.0,\n",
        "    lambda_l2=1.0,\n",
        "    n_jobs=-1,\n",
        "    verbosity=-1,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_lgb = np.zeros((len(y), num_class), dtype=np.float32)\n",
        "test_lgb = np.zeros((n_test, num_class), dtype=np.float32)\n",
        "start = time.time()\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(cv.split(X, y, groups)):\n",
        "    t0 = time.time()\n",
        "    print(f'LGB Fold {fold} | train {len(tr_idx)} val {len(va_idx)}')\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_tr = scaler.fit_transform(X[tr_idx])\n",
        "    X_va = scaler.transform(X[va_idx])\n",
        "    X_tr = np.clip(X_tr, -5, 5)\n",
        "    X_va = np.clip(X_va, -5, 5)\n",
        "    tr_weights = compute_sample_weight('balanced', y=y[tr_idx]).astype(np.float32)\n",
        "    lgb_tr = lgb.Dataset(X_tr, label=y[tr_idx], weight=tr_weights, free_raw_data=False)\n",
        "    lgb_va = lgb.Dataset(X_va, label=y[va_idx], reference=lgb_tr, free_raw_data=False)\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        lgb_tr,\n",
        "        num_boost_round=2000,\n",
        "        valid_sets=[lgb_tr, lgb_va],\n",
        "        valid_names=['train','valid'],\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
        "            lgb.log_evaluation(period=100)\n",
        "        ]\n",
        "    )\n",
        "    best_it = getattr(model, 'best_iteration', None)\n",
        "    oof_lgb[va_idx] = model.predict(X_va, num_iteration=best_it)\n",
        "    va_acc = accuracy_score(y[va_idx], oof_lgb[va_idx].argmax(1))\n",
        "    print(f'LGB Fold {fold} acc: {va_acc:.4f} | elapsed {time.time()-t0:.1f}s')\n",
        "    # Test TTA\n",
        "    fold_test = np.zeros((n_test, num_class), dtype=np.float32)\n",
        "    for s in range(n_shifts):\n",
        "        X_te_s = scaler.transform(X_test_tta[s])\n",
        "        X_te_s = np.clip(X_te_s, -5, 5)\n",
        "        fold_test += model.predict(X_te_s, num_iteration=best_it) / n_shifts\n",
        "    test_lgb += fold_test / cv.n_splits\n",
        "\n",
        "oof_acc_lgb = accuracy_score(y, oof_lgb.argmax(1))\n",
        "print(f'LightGBM OOF accuracy: {oof_acc_lgb:.4f} | total {time.time()-start:.1f}s')\n",
        "np.save('oof_pooled_lgb.npy', oof_lgb)\n",
        "# Save test preds with small-TTA suffix\n",
        "np.save('test_pred_pooled_lgb_tta50.npy', test_lgb)\n",
        "print('Saved LightGBM preds (small TTA) to test_pred_pooled_lgb_tta50.npy.')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes: (64073, 662) (64073,) (64073,) (5, 6473, 662)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB Fold 0 | train 52005 val 12068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain's multi_logloss: 0.122379\tvalid's multi_logloss: 0.551333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain's multi_logloss: 0.031121\tvalid's multi_logloss: 0.368114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain's multi_logloss: 0.0112937\tvalid's multi_logloss: 0.327729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain's multi_logloss: 0.00518802\tvalid's multi_logloss: 0.318784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\ttrain's multi_logloss: 0.00287623\tvalid's multi_logloss: 0.317958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB Fold 0 acc: 0.8975 | elapsed 75.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB Fold 1 | train 51074 val 12999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain's multi_logloss: 0.115255\tvalid's multi_logloss: 0.593447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain's multi_logloss: 0.0286556\tvalid's multi_logloss: 0.423715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain's multi_logloss: 0.0103047\tvalid's multi_logloss: 0.393929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain's multi_logloss: 0.00474633\tvalid's multi_logloss: 0.392743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB Fold 1 acc: 0.8774 | elapsed 55.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB Fold 2 | train 51792 val 12281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain's multi_logloss: 0.121133\tvalid's multi_logloss: 0.567342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain's multi_logloss: 0.030187\tvalid's multi_logloss: 0.385576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain's multi_logloss: 0.0108877\tvalid's multi_logloss: 0.34619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain's multi_logloss: 0.00500143\tvalid's multi_logloss: 0.33793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\ttrain's multi_logloss: 0.00278309\tvalid's multi_logloss: 0.339121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB Fold 2 acc: 0.8913 | elapsed 68.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB Fold 3 | train 50819 val 13254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain's multi_logloss: 0.115546\tvalid's multi_logloss: 0.570206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain's multi_logloss: 0.0286827\tvalid's multi_logloss: 0.400599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain's multi_logloss: 0.010259\tvalid's multi_logloss: 0.367921\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain's multi_logloss: 0.00472504\tvalid's multi_logloss: 0.364423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB Fold 3 acc: 0.8842 | elapsed 63.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB Fold 4 | train 50602 val 13471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain's multi_logloss: 0.115481\tvalid's multi_logloss: 0.588831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain's multi_logloss: 0.0284971\tvalid's multi_logloss: 0.411636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain's multi_logloss: 0.0102752\tvalid's multi_logloss: 0.377183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain's multi_logloss: 0.00472736\tvalid's multi_logloss: 0.37274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB Fold 4 acc: 0.8836 | elapsed 61.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM OOF accuracy: 0.8866 | total 330.3s\nSaved LightGBM preds (small TTA) to test_pred_pooled_lgb_tta50.npy.\n"
          ]
        }
      ]
    },
    {
      "id": "49a78aa9-110d-448c-ac0e-d6ee121e9475",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Blend 3 models (XGB + CB + LGB): alpha sweep (coarse) and LR stacker\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from itertools import product\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "CLASSES = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "\n",
        "have_all = all([\n",
        "    os.path.exists('oof_pooled.npy'),\n",
        "    os.path.exists('oof_pooled_cat.npy'),\n",
        "    os.path.exists('oof_pooled_lgb.npy'),\n",
        "    os.path.exists('test_pred_pooled.npy'),\n",
        "    os.path.exists('test_pred_pooled_cat.npy'),\n",
        "    os.path.exists('test_pred_pooled_lgb.npy'),\n",
        "    os.path.exists('y_train_pooled.npy'),\n",
        "    os.path.exists('groups_pooled.npy'),\n",
        "    os.path.exists('test_fnames_pooled.csv')\n",
        "])\n",
        "\n",
        "if not have_all:\n",
        "    print('Predictions missing. Ensure XGB, CB, and LGB cells finished.')\n",
        "else:\n",
        "    y = np.load('y_train_pooled.npy')\n",
        "    groups = np.load('groups_pooled.npy')\n",
        "    o_x = np.load('oof_pooled.npy')\n",
        "    o_c = np.load('oof_pooled_cat.npy')\n",
        "    o_l = np.load('oof_pooled_lgb.npy')\n",
        "    t_x = np.load('test_pred_pooled.npy')\n",
        "    t_c = np.load('test_pred_pooled_cat.npy')\n",
        "    t_l = np.load('test_pred_pooled_lgb.npy')\n",
        "    test_fnames = pd.read_csv('test_fnames_pooled.csv', header=None)[0].values\n",
        "\n",
        "    # 3-way coarse alpha sweep over simplex with step=0.1\n",
        "    best_acc, best_w = -1.0, (1.0, 0.0, 0.0)\n",
        "    grid = [i/10.0 for i in range(0, 11)]\n",
        "    for ax, ac in product(grid, grid):\n",
        "        if ax + ac <= 1.0:\n",
        "            al = 1.0 - ax - ac\n",
        "            o = ax*o_x + ac*o_c + al*o_l\n",
        "            acc = accuracy_score(y, o.argmax(1))\n",
        "            if acc > best_acc:\n",
        "                best_acc, best_w = acc, (ax, ac, al)\n",
        "    print(f'Coarse 3-way blend best OOF acc: {best_acc:.5f} | weights (XGB,CB,LGB)={best_w}')\n",
        "    t_blend = best_w[0]*t_x + best_w[1]*t_c + best_w[2]*t_l\n",
        "    np.save('test_pred_blend3_alpha.npy', t_blend)\n",
        "\n",
        "    # LR stacker on concatenated probs (36 dims) with SGKF\n",
        "    X_meta = np.concatenate([o_x, o_c, o_l], axis=1)\n",
        "    X_test_meta = np.concatenate([t_x, t_c, t_l], axis=1)\n",
        "    cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    oof_stack = np.zeros_like(o_x, dtype=np.float32)\n",
        "    test_stack = np.zeros_like(t_x, dtype=np.float32)\n",
        "    for fold, (tr_idx, va_idx) in enumerate(cv.split(X_meta, y, groups)):\n",
        "        t0 = time.time()\n",
        "        clf = LogisticRegression(solver='lbfgs', max_iter=2000, n_jobs=-1, random_state=42, C=1.0)\n",
        "        clf.fit(X_meta[tr_idx], y[tr_idx])\n",
        "        oof_stack[va_idx] = clf.predict_proba(X_meta[va_idx])\n",
        "        test_stack += clf.predict_proba(X_test_meta) / cv.n_splits\n",
        "        print(f'Stacker fold {fold} done in {time.time()-t0:.1f}s')\n",
        "    acc_stack = accuracy_score(y, oof_stack.argmax(1))\n",
        "    print(f'LR stacker (3-model) OOF acc: {acc_stack:.5f}')\n",
        "    np.save('oof_blend3_stack.npy', oof_stack)\n",
        "    np.save('test_pred_blend3_stack.npy', test_stack)\n",
        "\n",
        "    use_stack = acc_stack > best_acc\n",
        "    final_pred = test_stack if use_stack else t_blend\n",
        "    final_desc = 'LR stacker' if use_stack else f'alpha blend weights={best_w}'\n",
        "    final_oof = acc_stack if use_stack else best_acc\n",
        "    print(f'Final 3-model choice: {final_desc} | OOF acc: {final_oof:.5f}')\n",
        "\n",
        "    # Save submission\n",
        "    pred_idx = final_pred.argmax(1)\n",
        "    labels = [CLASSES[i] for i in pred_idx]\n",
        "    sub = pd.DataFrame({'fname': test_fnames, 'label': labels})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv', sub.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coarse 3-way blend best OOF acc: 0.83921 | weights (XGB,CB,LGB)=(0.4, 0.2, 0.39999999999999997)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacker fold 0 done in 2.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacker fold 1 done in 2.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacker fold 2 done in 2.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacker fold 3 done in 2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacker fold 4 done in 2.5s\nLR stacker (3-model) OOF acc: 0.83717\nFinal 3-model choice: alpha blend weights=(0.4, 0.2, 0.39999999999999997) | OOF acc: 0.83921\nSaved submission.csv (6473, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "c82c2f95-e8a5-460d-b3cd-9e8570dcc598",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# LightGBM DART on pooled features with SGKF, weights, clipping, 5-shift TTA (diversity model)\n",
        "import os, sys, subprocess, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except Exception as e:\n",
        "    print('Installing lightgbm...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'lightgbm==4.6.0'], check=True)\n",
        "    import lightgbm as lgb\n",
        "\n",
        "CLASSES = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "num_class = len(CLASSES)\n",
        "\n",
        "train_feat = 'X_train_pooled.npy'\n",
        "train_y = 'y_train_pooled.npy'\n",
        "train_groups = 'groups_pooled.npy'\n",
        "# Use small-shift TTA features for test\n",
        "test_feat_tta = 'X_test_pooled_tta_small.npy'\n",
        "test_fnames_csv = 'test_fnames_pooled_small.csv'\n",
        "\n",
        "assert os.path.exists(train_feat) and os.path.exists(train_y) and os.path.exists(train_groups), 'Missing train pooled features'\n",
        "assert os.path.exists(test_feat_tta) and os.path.exists(test_fnames_csv), 'Missing test pooled TTA features'\n",
        "\n",
        "X = np.load(train_feat)\n",
        "y = np.load(train_y)\n",
        "groups = np.load(train_groups)\n",
        "X_test_tta = np.load(test_feat_tta)  # [n_shifts, N, D]\n",
        "test_fnames = pd.read_csv(test_fnames_csv, header=None)[0].values\n",
        "n_shifts, n_test, D = X_test_tta.shape\n",
        "print('Shapes:', X.shape, y.shape, groups.shape, X_test_tta.shape)\n",
        "\n",
        "params = dict(\n",
        "    objective='multiclass',\n",
        "    num_class=num_class,\n",
        "    metric='multi_logloss',\n",
        "    boosting='dart',\n",
        "    learning_rate=0.05,\n",
        "    num_leaves=63,\n",
        "    max_depth=7,\n",
        "    feature_fraction=0.8,\n",
        "    bagging_fraction=0.8,\n",
        "    bagging_freq=1,\n",
        "    drop_rate=0.1,\n",
        "    max_drop=50,\n",
        "    skip_drop=0.5,\n",
        "    min_data_in_leaf=30,\n",
        "    lambda_l2=1.0,\n",
        "    n_jobs=-1,\n",
        "    verbosity=-1,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_lgb_dart = np.zeros((len(y), num_class), dtype=np.float32)\n",
        "test_lgb_dart = np.zeros((n_test, num_class), dtype=np.float32)\n",
        "start = time.time()\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(cv.split(X, y, groups)):\n",
        "    t0 = time.time()\n",
        "    print(f'LGB-DART Fold {fold} | train {len(tr_idx)} val {len(va_idx)}')\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_tr = scaler.fit_transform(X[tr_idx])\n",
        "    X_va = scaler.transform(X[va_idx])\n",
        "    X_tr = np.clip(X_tr, -5, 5)\n",
        "    X_va = np.clip(X_va, -5, 5)\n",
        "    tr_weights = compute_sample_weight('balanced', y=y[tr_idx]).astype(np.float32)\n",
        "    lgb_tr = lgb.Dataset(X_tr, label=y[tr_idx], weight=tr_weights, free_raw_data=False)\n",
        "    lgb_va = lgb.Dataset(X_va, label=y[va_idx], reference=lgb_tr, free_raw_data=False)\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        lgb_tr,\n",
        "        num_boost_round=2000,\n",
        "        valid_sets=[lgb_tr, lgb_va],\n",
        "        valid_names=['train','valid'],\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
        "            lgb.log_evaluation(period=100)\n",
        "        ]\n",
        "    )\n",
        "    best_it = getattr(model, 'best_iteration', None)\n",
        "    oof_lgb_dart[va_idx] = model.predict(X_va, num_iteration=best_it)\n",
        "    va_acc = accuracy_score(y[va_idx], oof_lgb_dart[va_idx].argmax(1))\n",
        "    print(f'LGB-DART Fold {fold} acc: {va_acc:.4f} | elapsed {time.time()-t0:.1f}s')\n",
        "    # Test TTA\n",
        "    fold_test = np.zeros((n_test, num_class), dtype=np.float32)\n",
        "    for s in range(n_shifts):\n",
        "        X_te_s = scaler.transform(X_test_tta[s])\n",
        "        X_te_s = np.clip(X_te_s, -5, 5)\n",
        "        fold_test += model.predict(X_te_s, num_iteration=best_it) / n_shifts\n",
        "    test_lgb_dart += fold_test / cv.n_splits\n",
        "\n",
        "oof_acc_lgb_dart = accuracy_score(y, oof_lgb_dart.argmax(1))\n",
        "print(f'LightGBM DART OOF accuracy: {oof_acc_lgb_dart:.4f} | total {time.time()-start:.1f}s')\n",
        "np.save('oof_pooled_lgb_dart.npy', oof_lgb_dart)\n",
        "np.save('test_pred_pooled_lgb_dart_tta50.npy', test_lgb_dart)\n",
        "print('Saved LightGBM DART preds (small TTA) to test_pred_pooled_lgb_dart_tta50.npy.')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes: (64073, 662) (64073,) (64073,) (5, 6473, 662)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB-DART Fold 0 | train 52005 val 12068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/callback.py:333: UserWarning: Early stopping is not available in dart mode\n  _log_warning(\"Early stopping is not available in dart mode\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain's multi_logloss: 0.552972\tvalid's multi_logloss: 1.05579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain's multi_logloss: 0.406045\tvalid's multi_logloss: 0.89892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain's multi_logloss: 0.230637\tvalid's multi_logloss: 0.695785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain's multi_logloss: 0.172173\tvalid's multi_logloss: 0.609467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\ttrain's multi_logloss: 0.115533\tvalid's multi_logloss: 0.519555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's multi_logloss: 0.0891536\tvalid's multi_logloss: 0.470852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\ttrain's multi_logloss: 0.0543709\tvalid's multi_logloss: 0.403377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain's multi_logloss: 0.0355838\tvalid's multi_logloss: 0.363919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain's multi_logloss: 0.0242916\tvalid's multi_logloss: 0.34036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain's multi_logloss: 0.0200863\tvalid's multi_logloss: 0.33199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1100]\ttrain's multi_logloss: 0.0140754\tvalid's multi_logloss: 0.319771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's multi_logloss: 0.0125469\tvalid's multi_logloss: 0.31553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1300]\ttrain's multi_logloss: 0.0106627\tvalid's multi_logloss: 0.311228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\ttrain's multi_logloss: 0.00859035\tvalid's multi_logloss: 0.306734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\ttrain's multi_logloss: 0.00785125\tvalid's multi_logloss: 0.304626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\ttrain's multi_logloss: 0.005784\tvalid's multi_logloss: 0.302075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1700]\ttrain's multi_logloss: 0.00511294\tvalid's multi_logloss: 0.301232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\ttrain's multi_logloss: 0.00488431\tvalid's multi_logloss: 0.300413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1900]\ttrain's multi_logloss: 0.00430674\tvalid's multi_logloss: 0.299089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\ttrain's multi_logloss: 0.00368486\tvalid's multi_logloss: 0.298733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB-DART Fold 0 acc: 0.9032 | elapsed 410.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB-DART Fold 1 | train 51074 val 12999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/callback.py:333: UserWarning: Early stopping is not available in dart mode\n  _log_warning(\"Early stopping is not available in dart mode\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain's multi_logloss: 0.538793\tvalid's multi_logloss: 1.07971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain's multi_logloss: 0.392718\tvalid's multi_logloss: 0.93031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain's multi_logloss: 0.220396\tvalid's multi_logloss: 0.731963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain's multi_logloss: 0.162706\tvalid's multi_logloss: 0.644658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\ttrain's multi_logloss: 0.108478\tvalid's multi_logloss: 0.559178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's multi_logloss: 0.0831296\tvalid's multi_logloss: 0.512584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\ttrain's multi_logloss: 0.0500785\tvalid's multi_logloss: 0.450655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain's multi_logloss: 0.0327931\tvalid's multi_logloss: 0.415508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain's multi_logloss: 0.0222264\tvalid's multi_logloss: 0.39664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain's multi_logloss: 0.0184305\tvalid's multi_logloss: 0.389162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1100]\ttrain's multi_logloss: 0.0129233\tvalid's multi_logloss: 0.381318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's multi_logloss: 0.0115378\tvalid's multi_logloss: 0.378058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1300]\ttrain's multi_logloss: 0.00988028\tvalid's multi_logloss: 0.375464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\ttrain's multi_logloss: 0.00793044\tvalid's multi_logloss: 0.373113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\ttrain's multi_logloss: 0.00727976\tvalid's multi_logloss: 0.371638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\ttrain's multi_logloss: 0.00537289\tvalid's multi_logloss: 0.37182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1700]\ttrain's multi_logloss: 0.00475554\tvalid's multi_logloss: 0.371292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\ttrain's multi_logloss: 0.00456312\tvalid's multi_logloss: 0.370146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1900]\ttrain's multi_logloss: 0.00402555\tvalid's multi_logloss: 0.369703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\ttrain's multi_logloss: 0.00345813\tvalid's multi_logloss: 0.37004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB-DART Fold 1 acc: 0.8868 | elapsed 399.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB-DART Fold 2 | train 51792 val 12281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/callback.py:333: UserWarning: Early stopping is not available in dart mode\n  _log_warning(\"Early stopping is not available in dart mode\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain's multi_logloss: 0.550818\tvalid's multi_logloss: 1.05999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain's multi_logloss: 0.403782\tvalid's multi_logloss: 0.910084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain's multi_logloss: 0.228138\tvalid's multi_logloss: 0.706133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain's multi_logloss: 0.169978\tvalid's multi_logloss: 0.62186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\ttrain's multi_logloss: 0.114061\tvalid's multi_logloss: 0.532381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's multi_logloss: 0.087686\tvalid's multi_logloss: 0.484715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\ttrain's multi_logloss: 0.0533129\tvalid's multi_logloss: 0.417903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain's multi_logloss: 0.0348068\tvalid's multi_logloss: 0.38009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain's multi_logloss: 0.0234339\tvalid's multi_logloss: 0.355377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain's multi_logloss: 0.019385\tvalid's multi_logloss: 0.346128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1100]\ttrain's multi_logloss: 0.0135883\tvalid's multi_logloss: 0.334664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's multi_logloss: 0.0121347\tvalid's multi_logloss: 0.330085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1300]\ttrain's multi_logloss: 0.0103593\tvalid's multi_logloss: 0.326584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\ttrain's multi_logloss: 0.00836155\tvalid's multi_logloss: 0.323002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\ttrain's multi_logloss: 0.00763734\tvalid's multi_logloss: 0.32072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\ttrain's multi_logloss: 0.00562723\tvalid's multi_logloss: 0.318087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1700]\ttrain's multi_logloss: 0.00497068\tvalid's multi_logloss: 0.317648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\ttrain's multi_logloss: 0.00476862\tvalid's multi_logloss: 0.316904\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1900]\ttrain's multi_logloss: 0.00421681\tvalid's multi_logloss: 0.316727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\ttrain's multi_logloss: 0.00361343\tvalid's multi_logloss: 0.316707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB-DART Fold 2 acc: 0.8972 | elapsed 411.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB-DART Fold 3 | train 50819 val 13254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/callback.py:333: UserWarning: Early stopping is not available in dart mode\n  _log_warning(\"Early stopping is not available in dart mode\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain's multi_logloss: 0.540987\tvalid's multi_logloss: 1.05844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain's multi_logloss: 0.394975\tvalid's multi_logloss: 0.912247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain's multi_logloss: 0.220728\tvalid's multi_logloss: 0.71242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain's multi_logloss: 0.163058\tvalid's multi_logloss: 0.627412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\ttrain's multi_logloss: 0.108582\tvalid's multi_logloss: 0.542639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's multi_logloss: 0.0831684\tvalid's multi_logloss: 0.495684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\ttrain's multi_logloss: 0.0501806\tvalid's multi_logloss: 0.433629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain's multi_logloss: 0.0327617\tvalid's multi_logloss: 0.399172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain's multi_logloss: 0.022153\tvalid's multi_logloss: 0.379187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain's multi_logloss: 0.0183027\tvalid's multi_logloss: 0.372078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1100]\ttrain's multi_logloss: 0.0127991\tvalid's multi_logloss: 0.362405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's multi_logloss: 0.011445\tvalid's multi_logloss: 0.359693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1300]\ttrain's multi_logloss: 0.00980841\tvalid's multi_logloss: 0.356181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\ttrain's multi_logloss: 0.00791497\tvalid's multi_logloss: 0.353112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\ttrain's multi_logloss: 0.00727089\tvalid's multi_logloss: 0.351977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\ttrain's multi_logloss: 0.00536013\tvalid's multi_logloss: 0.350716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1700]\ttrain's multi_logloss: 0.00475219\tvalid's multi_logloss: 0.3498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\ttrain's multi_logloss: 0.00455819\tvalid's multi_logloss: 0.348954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1900]\ttrain's multi_logloss: 0.00403938\tvalid's multi_logloss: 0.348688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\ttrain's multi_logloss: 0.00345908\tvalid's multi_logloss: 0.349689\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB-DART Fold 3 acc: 0.8890 | elapsed 406.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB-DART Fold 4 | train 50602 val 13471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/callback.py:333: UserWarning: Early stopping is not available in dart mode\n  _log_warning(\"Early stopping is not available in dart mode\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain's multi_logloss: 0.541225\tvalid's multi_logloss: 1.07746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain's multi_logloss: 0.395541\tvalid's multi_logloss: 0.928802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain's multi_logloss: 0.222331\tvalid's multi_logloss: 0.728408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain's multi_logloss: 0.163913\tvalid's multi_logloss: 0.640884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\ttrain's multi_logloss: 0.109525\tvalid's multi_logloss: 0.554543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain's multi_logloss: 0.0836597\tvalid's multi_logloss: 0.505432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\ttrain's multi_logloss: 0.0505376\tvalid's multi_logloss: 0.442114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain's multi_logloss: 0.0330616\tvalid's multi_logloss: 0.405372\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain's multi_logloss: 0.0223577\tvalid's multi_logloss: 0.38333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain's multi_logloss: 0.0184745\tvalid's multi_logloss: 0.375388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1100]\ttrain's multi_logloss: 0.0129\tvalid's multi_logloss: 0.365807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain's multi_logloss: 0.0115373\tvalid's multi_logloss: 0.363029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1300]\ttrain's multi_logloss: 0.00984394\tvalid's multi_logloss: 0.359683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\ttrain's multi_logloss: 0.00792423\tvalid's multi_logloss: 0.356459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\ttrain's multi_logloss: 0.00727304\tvalid's multi_logloss: 0.355035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\ttrain's multi_logloss: 0.00536739\tvalid's multi_logloss: 0.353191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1700]\ttrain's multi_logloss: 0.00475491\tvalid's multi_logloss: 0.352928\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\ttrain's multi_logloss: 0.00456607\tvalid's multi_logloss: 0.352186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1900]\ttrain's multi_logloss: 0.00403693\tvalid's multi_logloss: 0.352421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\ttrain's multi_logloss: 0.00346883\tvalid's multi_logloss: 0.353282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB-DART Fold 4 acc: 0.8891 | elapsed 405.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM DART OOF accuracy: 0.8928 | total 2095.2s\nSaved LightGBM DART preds (small TTA) to test_pred_pooled_lgb_dart_tta50.npy.\n"
          ]
        }
      ]
    },
    {
      "id": "a5976adc-ee06-4294-978f-7fcce967c2a0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# K-agnostic blend with per-class bias tuning and silence override; prefer small-TTA preds; exclude CB if not refreshed\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from itertools import product\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "CLASSES = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "num_class = len(CLASSES)\n",
        "EPS = 1e-8\n",
        "\n",
        "def coarse_simplex_blend(oofs, tests, step=0.1):\n",
        "    names = list(oofs.keys())\n",
        "    K = len(names)\n",
        "    assert K >= 2, 'Need at least two models to blend'\n",
        "    M = int(round(1.0 / step))\n",
        "    def gen_weights(K, M):\n",
        "        if K == 1:\n",
        "            yield np.array([M], dtype=float); return\n",
        "        from itertools import product\n",
        "        for parts in product(range(M+1), repeat=K-1):\n",
        "            s = sum(parts)\n",
        "            if s <= M:\n",
        "                last = M - s\n",
        "                yield np.array(list(parts) + [last], dtype=float)\n",
        "    best = {'acc': -1.0, 'w': None, 'order': names}\n",
        "    for w_int in gen_weights(K, M):\n",
        "        w = w_int / M\n",
        "        o = sum(w[i] * oofs[n] for i, n in enumerate(names))\n",
        "        acc = accuracy_score(y, o.argmax(1))\n",
        "        if acc > best['acc']:\n",
        "            best = {'acc': acc, 'w': w.copy(), 'order': names}\n",
        "    tb = sum(best['w'][i]*tests[n] for i,n in enumerate(best['order']))\n",
        "    ob = sum(best['w'][i]*oofs[n] for i,n in enumerate(best['order']))\n",
        "    return best, ob, tb\n",
        "\n",
        "def tune_biases_greedy(log_oof, y, steps=(0.2, 0.1, 0.05), clamp=1.0, max_passes=3):\n",
        "    b = np.zeros(log_oof.shape[1], dtype=np.float32)\n",
        "    best_acc = accuracy_score(y, (log_oof + b).argmax(1))\n",
        "    for step in steps:\n",
        "        improved = True\n",
        "        passes = 0\n",
        "        while improved and passes < max_passes:\n",
        "            improved = False\n",
        "            passes += 1\n",
        "            for c in range(log_oof.shape[1]):\n",
        "                for delta in (-step, step):\n",
        "                    old = b[c]\n",
        "                    b[c] = np.clip(b[c] + delta, -clamp, clamp)\n",
        "                    acc = accuracy_score(y, (log_oof + b).argmax(1))\n",
        "                    if acc > best_acc + 1e-9:\n",
        "                        best_acc = acc\n",
        "                        improved = True\n",
        "                    else:\n",
        "                        b[c] = old\n",
        "    return b, best_acc\n",
        "\n",
        "# Load OOF and available test predictions; prefer refreshed small-TTA files and exclude CB unless refreshed\n",
        "y = np.load('y_train_pooled.npy')\n",
        "test_fnames_path = 'test_fnames_pooled_small.csv' if os.path.exists('test_fnames_pooled_small.csv') else 'test_fnames_pooled.csv'\n",
        "test_fnames = pd.read_csv(test_fnames_path, header=None)[0].values\n",
        "\n",
        "oofs = {}\n",
        "tests = {}\n",
        "\n",
        "# XGB seed1\n",
        "if os.path.exists('oof_pooled.npy'):\n",
        "    oofs['XGB'] = np.load('oof_pooled.npy')\n",
        "    # prefer small-TTA test preds\n",
        "    if os.path.exists('test_pred_pooled_tta50.npy'):\n",
        "        tests['XGB'] = np.load('test_pred_pooled_tta50.npy')\n",
        "    elif os.path.exists('test_pred_pooled.npy'):\n",
        "        tests['XGB'] = np.load('test_pred_pooled.npy')\n",
        "\n",
        "# LGB\n",
        "if os.path.exists('oof_pooled_lgb.npy'):\n",
        "    oofs['LGB'] = np.load('oof_pooled_lgb.npy')\n",
        "    if os.path.exists('test_pred_pooled_lgb_tta50.npy'):\n",
        "        tests['LGB'] = np.load('test_pred_pooled_lgb_tta50.npy')\n",
        "    elif os.path.exists('test_pred_pooled_lgb.npy'):\n",
        "        tests['LGB'] = np.load('test_pred_pooled_lgb.npy')\n",
        "\n",
        "# LGB-DART\n",
        "if os.path.exists('oof_pooled_lgb_dart.npy'):\n",
        "    oofs['LGB_DART'] = np.load('oof_pooled_lgb_dart.npy')\n",
        "    if os.path.exists('test_pred_pooled_lgb_dart_tta50.npy'):\n",
        "        tests['LGB_DART'] = np.load('test_pred_pooled_lgb_dart_tta50.npy')\n",
        "    elif os.path.exists('test_pred_pooled_lgb_dart.npy'):\n",
        "        tests['LGB_DART'] = np.load('test_pred_pooled_lgb_dart.npy')\n",
        "\n",
        "# XGB seed2\n",
        "if os.path.exists('oof_pooled_xgb_seed2.npy'):\n",
        "    oofs['XGB2'] = np.load('oof_pooled_xgb_seed2.npy')\n",
        "    if os.path.exists('test_pred_pooled_xgb_seed2_tta50.npy'):\n",
        "        tests['XGB2'] = np.load('test_pred_pooled_xgb_seed2_tta50.npy')\n",
        "    elif os.path.exists('test_pred_pooled_xgb_seed2.npy'):\n",
        "        tests['XGB2'] = np.load('test_pred_pooled_xgb_seed2.npy')\n",
        "\n",
        "# CatBoost: include ONLY if refreshed small-TTA test preds exist (to avoid CV/LB mismatch)\n",
        "if os.path.exists('oof_pooled_cat.npy') and os.path.exists('test_pred_pooled_cat_tta50.npy'):\n",
        "    oofs['CB'] = np.load('oof_pooled_cat.npy')\n",
        "    tests['CB'] = np.load('test_pred_pooled_cat_tta50.npy')\n",
        "\n",
        "# Filter to models that have both OOF and TEST available\n",
        "keys = [k for k in oofs.keys() if k in tests]\n",
        "oofs = {k: oofs[k] for k in keys}\n",
        "tests = {k: tests[k] for k in keys}\n",
        "print('Models included in blend (with matching test preds):', keys)\n",
        "if len(keys) < 2:\n",
        "    raise RuntimeError('Need at least two models with matching test predictions to blend.')\n",
        "\n",
        "# Coarse then fine blend search\n",
        "best_coarse, oof_blend, test_blend = coarse_simplex_blend(oofs, tests, step=0.1)\n",
        "print(f'Coarse blend best OOF acc: {best_coarse[\"acc\"]:.5f} | weights {dict(zip(best_coarse[\"order\"], best_coarse[\"w\"]))}')\n",
        "names = best_coarse['order']\n",
        "w0 = best_coarse['w']\n",
        "step = 0.05\n",
        "grid = np.arange(-0.1, 0.1001, step)\n",
        "best_acc = best_coarse['acc']\n",
        "best_w = w0.copy()\n",
        "if len(names) >= 4:\n",
        "    for da0, da1, da2 in product(grid, grid, grid):\n",
        "        a0 = np.clip(w0[0] + da0, 0, 1)\n",
        "        a1 = np.clip(w0[1] + da1, 0, 1)\n",
        "        a2 = np.clip(w0[2] + da2, 0, 1)\n",
        "        s = a0 + a1 + a2\n",
        "        if s <= 1.0:\n",
        "            a_rest = 1.0 - s\n",
        "            rest = np.array(w0[3:], dtype=float)\n",
        "            if rest.sum() > 0:\n",
        "                rest = rest / rest.sum() * a_rest\n",
        "            else:\n",
        "                rest = np.zeros_like(rest)\n",
        "            w = np.concatenate([np.array([a0, a1, a2]), rest])\n",
        "        else:\n",
        "            continue\n",
        "        o = sum(w[i]*oofs[n] for i,n in enumerate(names))\n",
        "        acc = accuracy_score(y, o.argmax(1))\n",
        "        if acc > best_acc:\n",
        "            best_acc, best_w = acc, w.copy()\n",
        "else:\n",
        "    for da0, da1 in product(grid, grid):\n",
        "        a0 = np.clip(w0[0] + da0, 0, 1)\n",
        "        a1 = np.clip(w0[1] + da1, 0, 1)\n",
        "        if len(names)==3:\n",
        "            if a0 + a1 <= 1.0:\n",
        "                a2 = 1.0 - a0 - a1\n",
        "                w = np.array([a0,a1,a2])\n",
        "            else:\n",
        "                continue\n",
        "        else:\n",
        "            a1 = 1.0 - a0\n",
        "            w = np.array([a0,a1])\n",
        "        o = sum(w[i]*oofs[n] for i,n in enumerate(names))\n",
        "        acc = accuracy_score(y, o.argmax(1))\n",
        "        if acc > best_acc:\n",
        "            best_acc, best_w = acc, w.copy()\n",
        "oof_blend = sum(best_w[i]*oofs[n] for i,n in enumerate(names))\n",
        "test_blend = sum(best_w[i]*tests[n] for i,n in enumerate(names))\n",
        "print(f'Fine blend best OOF acc: {best_acc:.5f} | weights {dict(zip(names, best_w))}')\n",
        "\n",
        "# Per-class bias tuning\n",
        "log_oof = np.log(np.clip(oof_blend, EPS, 1.0))\n",
        "b, acc_bias = tune_biases_greedy(log_oof, y, steps=(0.2,0.1,0.05), clamp=1.0, max_passes=3)\n",
        "print(f'Bias-tuned OOF acc: {acc_bias:.5f} | biases:', np.round(b, 3))\n",
        "\n",
        "# Apply biases\n",
        "log_oof_b = log_oof + b\n",
        "log_test = np.log(np.clip(test_blend, EPS, 1.0)) + b\n",
        "\n",
        "# Silence override threshold sweep on OOF\n",
        "silence_idx = CLASSES.index('silence')\n",
        "best_override = {'acc': acc_bias, 'thr': None, 'count': 0}\n",
        "try:\n",
        "    X_tr_pool = np.load('X_train_pooled.npy')\n",
        "    spec_desc_start = 256 + 120 + 240  # 616\n",
        "    zcr_mean_idx = spec_desc_start + 6  # 622\n",
        "    rms_mean_idx = spec_desc_start + 8  # 624\n",
        "    zcr_mean_tr = X_tr_pool[:, zcr_mean_idx]\n",
        "    rms_mean_tr = X_tr_pool[:, rms_mean_idx]\n",
        "    top_prob_tr = np.exp(log_oof_b - log_oof_b.max(1, keepdims=True)).max(1)\n",
        "    top_grid = [0.35, 0.40, 0.45]\n",
        "    rms_grid = [0.008, 0.010, 0.012]\n",
        "    zcr_grid = [0.04, 0.05, 0.06]\n",
        "    base_pred_tr = log_oof_b.argmax(1).copy()\n",
        "    for tp in top_grid:\n",
        "        for rt in rms_grid:\n",
        "            for zt in zcr_grid:\n",
        "                mask = (top_prob_tr < tp) & (rms_mean_tr < rt) & (zcr_mean_tr < zt)\n",
        "                pred_ovr = base_pred_tr.copy()\n",
        "                pred_ovr[mask] = silence_idx\n",
        "                acc = accuracy_score(y, pred_ovr)\n",
        "                if acc > best_override['acc'] + 1e-9:\n",
        "                    best_override = {'acc': acc, 'thr': (tp, rt, zt), 'count': int(mask.sum())}\n",
        "    if best_override['thr'] is not None:\n",
        "        print(f'Best silence override OOF acc: {best_override[\"acc\"]:.5f} | thr (top,rms,zcr)={best_override[\"thr\"]} | count={best_override[\"count\"]}')\n",
        "    else:\n",
        "        print('Silence override sweep did not improve OOF.')\n",
        "except Exception as e:\n",
        "    print('Silence override sweep skipped due to error:', e)\n",
        "\n",
        "# Final test predictions with silence override using SMALL-TTA pooled average if available\n",
        "pred_idx = log_test.argmax(1)\n",
        "applied = 0\n",
        "tta_small = 'X_test_pooled_tta_small.npy'\n",
        "tta_orig = 'X_test_pooled_tta.npy'\n",
        "tta_path = tta_small if os.path.exists(tta_small) else (tta_orig if os.path.exists(tta_orig) else None)\n",
        "if tta_path is not None:\n",
        "    X_test_tta = np.load(tta_path)\n",
        "    X_test_avg = X_test_tta.mean(axis=0)\n",
        "    spec_desc_start = 256 + 120 + 240  # 616\n",
        "    zcr_mean_idx = spec_desc_start + 6  # 622\n",
        "    rms_mean_idx = spec_desc_start + 8  # 624\n",
        "    zcr_mean = X_test_avg[:, zcr_mean_idx]\n",
        "    rms_mean = X_test_avg[:, rms_mean_idx]\n",
        "    top_prob = np.exp(log_test - log_test.max(1, keepdims=True)).max(1)\n",
        "    if 'best_override' in locals() and best_override.get('thr') is not None:\n",
        "        tp, rt, zt = best_override['thr']\n",
        "    else:\n",
        "        tp, rt, zt = 0.45, 0.012, 0.06\n",
        "    mask = (top_prob < tp) & (rms_mean < rt) & (zcr_mean < zt)\n",
        "    pred_idx[mask] = silence_idx\n",
        "    applied = int(mask.sum())\n",
        "    print(f'Silence override applied to {applied} samples (thr={tp},{rt},{zt}) using {tta_path}')\n",
        "else:\n",
        "    print('No test TTA feature file found; skipping silence override.')\n",
        "\n",
        "labels = [CLASSES[i] for i in pred_idx]\n",
        "sub = pd.DataFrame({'fname': test_fnames, 'label': labels})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv', sub.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models included in blend (with matching test preds): ['XGB', 'LGB', 'LGB_DART', 'XGB2', 'CB']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coarse blend best OOF acc: 0.89598 | weights {'XGB': 0.0, 'LGB': 0.0, 'LGB_DART': 0.4, 'XGB2': 0.1, 'CB': 0.5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine blend best OOF acc: 0.89606 | weights {'XGB': 0.0, 'LGB': 0.0, 'LGB_DART': 0.45000000000000007, 'XGB2': 0.09166666666666666, 'CB': 0.4583333333333333}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bias-tuned OOF acc: 0.89643 | biases: [-0.05  0.15 -0.3   0.    0.    0.   -0.2   0.    0.    0.2   0.    0.  ]\nSilence override sweep did not improve OOF.\nSilence override applied to 0 samples (thr=0.45,0.012,0.06) using X_test_pooled_tta_small.npy\nSaved submission.csv (6473, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "57d3ce1f-d337-4fce-ac5a-e5760402a8f4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# XGBoost seed2 on pooled features (diversity model) with SGKF, per-fold scaler, clipping, TTA; save oof/test\n",
        "import os, time, numpy as np, pandas as pd, sys, subprocess\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    print('Installing xgboost...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "\n",
        "CLASSES = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "num_class = len(CLASSES)\n",
        "\n",
        "train_feat = 'X_train_pooled.npy'\n",
        "train_y = 'y_train_pooled.npy'\n",
        "train_groups = 'groups_pooled.npy'\n",
        "# Use small-shift TTA features for test\n",
        "test_feat_tta = 'X_test_pooled_tta_small.npy'\n",
        "test_fnames_csv = 'test_fnames_pooled_small.csv'\n",
        "\n",
        "assert os.path.exists(train_feat) and os.path.exists(train_y) and os.path.exists(train_groups), 'Missing train pooled features'\n",
        "assert os.path.exists(test_feat_tta) and os.path.exists(test_fnames_csv), 'Missing test pooled TTA features'\n",
        "\n",
        "X = np.load(train_feat)\n",
        "y = np.load(train_y)\n",
        "groups = np.load(train_groups)\n",
        "X_test_tta = np.load(test_feat_tta)  # [n_shifts, N, D]\n",
        "test_fnames = pd.read_csv(test_fnames_csv, header=None)[0].values\n",
        "n_shifts, n_test, D = X_test_tta.shape\n",
        "print('Shapes:', X.shape, y.shape, groups.shape, X_test_tta.shape)\n",
        "\n",
        "# Slightly different seed/regularization for diversity\n",
        "params = dict(\n",
        "    objective='multi:softprob',\n",
        "    num_class=num_class,\n",
        "    tree_method='hist',\n",
        "    max_bin=256,\n",
        "    max_depth=7,\n",
        "    eta=0.05,\n",
        "    subsample=0.85,\n",
        "    colsample_bytree=0.85,\n",
        "    min_child_weight=2.5,\n",
        "    reg_alpha=0.05,\n",
        "    reg_lambda=1.2,\n",
        "    eval_metric='mlogloss',\n",
        "    n_jobs=-1,\n",
        "    seed=202\n",
        ")\n",
        "\n",
        "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=202)\n",
        "oof2 = np.zeros((len(y), num_class), dtype=np.float32)\n",
        "test2 = np.zeros((n_test, num_class), dtype=np.float32)\n",
        "start = time.time()\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(cv.split(X, y, groups)):\n",
        "    t0 = time.time()\n",
        "    print(f'XGB seed2 Fold {fold} | train {len(tr_idx)} val {len(va_idx)}')\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_tr = scaler.fit_transform(X[tr_idx])\n",
        "    X_va = scaler.transform(X[va_idx])\n",
        "    X_tr = np.clip(X_tr, -5, 5)\n",
        "    X_va = np.clip(X_va, -5, 5)\n",
        "    tr_weights = compute_sample_weight('balanced', y=y[tr_idx])\n",
        "    dtr = xgb.DMatrix(X_tr, label=y[tr_idx], weight=tr_weights)\n",
        "    dva = xgb.DMatrix(X_va, label=y[va_idx])\n",
        "    model = xgb.train(params, dtr, num_boost_round=2000, evals=[(dtr,'train'),(dva,'valid')], early_stopping_rounds=100, verbose_eval=100)\n",
        "    best_iter = getattr(model, 'best_iteration', None)\n",
        "    if best_iter is None:\n",
        "        try:\n",
        "            best_iter = model.num_boosted_rounds() - 1\n",
        "        except Exception:\n",
        "            best_iter = None\n",
        "    if best_iter is not None:\n",
        "        oof2[va_idx] = model.predict(dva, iteration_range=(0, best_iter + 1))\n",
        "    else:\n",
        "        oof2[va_idx] = model.predict(dva)\n",
        "    va_acc = accuracy_score(y[va_idx], oof2[va_idx].argmax(1))\n",
        "    print(f'XGB seed2 Fold {fold} acc: {va_acc:.4f} | elapsed {time.time()-t0:.1f}s')\n",
        "    # Test TTA\n",
        "    fold_test = np.zeros((n_test, num_class), dtype=np.float32)\n",
        "    for s in range(n_shifts):\n",
        "        X_te_s = scaler.transform(X_test_tta[s])\n",
        "        X_te_s = np.clip(X_te_s, -5, 5)\n",
        "        dte = xgb.DMatrix(X_te_s)\n",
        "        if best_iter is not None:\n",
        "            fold_test += model.predict(dte, iteration_range=(0, best_iter + 1)) / n_shifts\n",
        "        else:\n",
        "            fold_test += model.predict(dte) / n_shifts\n",
        "    test2 += fold_test / cv.n_splits\n",
        "\n",
        "oof_acc2 = accuracy_score(y, oof2.argmax(1))\n",
        "print(f'XGB seed2 OOF accuracy: {oof_acc2:.4f} | total {time.time()-start:.1f}s')\n",
        "np.save('oof_pooled_xgb_seed2.npy', oof2)\n",
        "np.save('test_pred_pooled_xgb_seed2_tta50.npy', test2)\n",
        "print('Saved XGB seed2 preds (small TTA) to test_pred_pooled_xgb_seed2_tta50.npy.')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes: (64073, 662) (64073,) (64073,) (5, 6473, 662)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB seed2 Fold 0 | train 51539 val 12534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-mlogloss:2.32888\tvalid-mlogloss:2.38587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain-mlogloss:0.22855\tvalid-mlogloss:0.76153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mlogloss:0.07317\tvalid-mlogloss:0.49157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain-mlogloss:0.03421\tvalid-mlogloss:0.40128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mlogloss:0.01957\tvalid-mlogloss:0.36657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\ttrain-mlogloss:0.01279\tvalid-mlogloss:0.35096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain-mlogloss:0.00915\tvalid-mlogloss:0.34302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\ttrain-mlogloss:0.00702\tvalid-mlogloss:0.33897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain-mlogloss:0.00564\tvalid-mlogloss:0.33639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain-mlogloss:0.00470\tvalid-mlogloss:0.33519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain-mlogloss:0.00402\tvalid-mlogloss:0.33443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1100]\ttrain-mlogloss:0.00352\tvalid-mlogloss:0.33388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain-mlogloss:0.00313\tvalid-mlogloss:0.33399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1257]\ttrain-mlogloss:0.00295\tvalid-mlogloss:0.33384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB seed2 Fold 0 acc: 0.8954 | elapsed 432.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB seed2 Fold 1 | train 50895 val 13178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-mlogloss:2.33118\tvalid-mlogloss:2.38386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain-mlogloss:0.22798\tvalid-mlogloss:0.74507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mlogloss:0.07151\tvalid-mlogloss:0.48788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain-mlogloss:0.03327\tvalid-mlogloss:0.40432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mlogloss:0.01906\tvalid-mlogloss:0.37187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\ttrain-mlogloss:0.01243\tvalid-mlogloss:0.35735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain-mlogloss:0.00893\tvalid-mlogloss:0.35006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\ttrain-mlogloss:0.00684\tvalid-mlogloss:0.34654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain-mlogloss:0.00551\tvalid-mlogloss:0.34462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain-mlogloss:0.00461\tvalid-mlogloss:0.34389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain-mlogloss:0.00395\tvalid-mlogloss:0.34377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1100]\ttrain-mlogloss:0.00346\tvalid-mlogloss:0.34377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1138]\ttrain-mlogloss:0.00330\tvalid-mlogloss:0.34389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB seed2 Fold 1 acc: 0.8922 | elapsed 390.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB seed2 Fold 2 | train 51397 val 12676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-mlogloss:2.32820\tvalid-mlogloss:2.38521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain-mlogloss:0.22810\tvalid-mlogloss:0.76751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mlogloss:0.07298\tvalid-mlogloss:0.49990\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain-mlogloss:0.03407\tvalid-mlogloss:0.41101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mlogloss:0.01951\tvalid-mlogloss:0.37611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\ttrain-mlogloss:0.01277\tvalid-mlogloss:0.36055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain-mlogloss:0.00913\tvalid-mlogloss:0.35212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\ttrain-mlogloss:0.00701\tvalid-mlogloss:0.34811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain-mlogloss:0.00565\tvalid-mlogloss:0.34540\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain-mlogloss:0.00470\tvalid-mlogloss:0.34366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain-mlogloss:0.00404\tvalid-mlogloss:0.34259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1100]\ttrain-mlogloss:0.00353\tvalid-mlogloss:0.34220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain-mlogloss:0.00313\tvalid-mlogloss:0.34195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1300]\ttrain-mlogloss:0.00282\tvalid-mlogloss:0.34210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1302]\ttrain-mlogloss:0.00282\tvalid-mlogloss:0.34211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB seed2 Fold 2 acc: 0.8924 | elapsed 434.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB seed2 Fold 3 | train 51925 val 12148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-mlogloss:2.32952\tvalid-mlogloss:2.38556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain-mlogloss:0.23060\tvalid-mlogloss:0.75254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mlogloss:0.07420\tvalid-mlogloss:0.48058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain-mlogloss:0.03475\tvalid-mlogloss:0.39337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mlogloss:0.01995\tvalid-mlogloss:0.36122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\ttrain-mlogloss:0.01305\tvalid-mlogloss:0.34673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain-mlogloss:0.00935\tvalid-mlogloss:0.33968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\ttrain-mlogloss:0.00714\tvalid-mlogloss:0.33665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain-mlogloss:0.00573\tvalid-mlogloss:0.33471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain-mlogloss:0.00477\tvalid-mlogloss:0.33366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain-mlogloss:0.00407\tvalid-mlogloss:0.33354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1023]\ttrain-mlogloss:0.00394\tvalid-mlogloss:0.33358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB seed2 Fold 3 acc: 0.8954 | elapsed 412.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB seed2 Fold 4 | train 50536 val 13537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-mlogloss:2.32751\tvalid-mlogloss:2.38715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttrain-mlogloss:0.22346\tvalid-mlogloss:0.77904\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mlogloss:0.06996\tvalid-mlogloss:0.52318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\ttrain-mlogloss:0.03265\tvalid-mlogloss:0.44509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mlogloss:0.01869\tvalid-mlogloss:0.41589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\ttrain-mlogloss:0.01223\tvalid-mlogloss:0.40459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain-mlogloss:0.00878\tvalid-mlogloss:0.39956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\ttrain-mlogloss:0.00675\tvalid-mlogloss:0.39716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain-mlogloss:0.00544\tvalid-mlogloss:0.39636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\ttrain-mlogloss:0.00454\tvalid-mlogloss:0.39609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain-mlogloss:0.00389\tvalid-mlogloss:0.39654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1013]\ttrain-mlogloss:0.00382\tvalid-mlogloss:0.39679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB seed2 Fold 4 acc: 0.8789 | elapsed 351.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB seed2 OOF accuracy: 0.8906 | total 2027.8s\nSaved XGB seed2 preds (small TTA) to test_pred_pooled_xgb_seed2_tta50.npy.\n"
          ]
        }
      ]
    },
    {
      "id": "b2baf759-72d2-47c8-b8e1-dabcd25fbb6d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build small-shift test TTA pooled features only ([-50,-25,0,25,50] ms)\n",
        "import time, numpy as np, pandas as pd, multiprocessing as mp\n",
        "from joblib import Parallel, delayed\n",
        "from pathlib import Path\n",
        "\n",
        "SR = 16000\n",
        "\n",
        "def build_test_tta_small(test_meta='test_meta.csv', out_path='X_test_pooled_tta_small.npy', fnames_out='test_fnames_pooled_small.csv', shifts_ms=None, n_jobs=None, seed=42):\n",
        "    if shifts_ms is None:\n",
        "        shifts_ms = [-50, -25, 0, 25, 50]\n",
        "    shifts = [int(ms/1000.0 * SR) for ms in shifts_ms]\n",
        "    if n_jobs is None:\n",
        "        import multiprocessing as mp\n",
        "        n_jobs = max(1, mp.cpu_count()-2)\n",
        "    df_te = pd.read_csv(test_meta)\n",
        "    fnames = df_te['fname'].tolist()\n",
        "    print(f\"[POOL-FE TEST SMALL] Test rows: {len(df_te)} | n_jobs={n_jobs} | shifts(ms)={shifts_ms}\")\n",
        "    def _proc(row, shift_samples):\n",
        "        feats, _, _ = extract_feature_vector(row['path'], None, None, seed=seed, is_silence=False, shift_samples=shift_samples)\n",
        "        return feats\n",
        "    X_tta = []\n",
        "    t0 = time.time()\n",
        "    for s in shifts:\n",
        "        t1 = time.time()\n",
        "        feats_list = Parallel(n_jobs=n_jobs, backend='loky')(delayed(_proc)(row, s) for _, row in df_te.iterrows())\n",
        "        X_s = np.stack(feats_list)\n",
        "        X_tta.append(X_s)\n",
        "        print(f\"[POOL-FE TEST SMALL] shift {s} samples -> {X_s.shape} | elapsed {time.time()-t1:.1f}s\")\n",
        "    X_tta = np.stack(X_tta, axis=0)\n",
        "    np.save(out_path, X_tta)\n",
        "    pd.Series(fnames).to_csv(fnames_out, index=False, header=False)\n",
        "    print(f\"[POOL-FE TEST SMALL] Saved {out_path} {X_tta.shape} and {fnames_out} | total {time.time()-t0:.1f}s\")\n",
        "\n",
        "# Run\n",
        "build_test_tta_small()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[POOL-FE TEST SMALL] Test rows: 6473 | n_jobs=34 | shifts(ms)=[-50, -25, 0, 25, 50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[POOL-FE TEST SMALL] shift -800 samples -> (6473, 662) | elapsed 7.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[POOL-FE TEST SMALL] shift -400 samples -> (6473, 662) | elapsed 5.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[POOL-FE TEST SMALL] shift 0 samples -> (6473, 662) | elapsed 4.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[POOL-FE TEST SMALL] shift 400 samples -> (6473, 662) | elapsed 5.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n/app/.pip-target/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[POOL-FE TEST SMALL] shift 800 samples -> (6473, 662) | elapsed 5.0s\n[POOL-FE TEST SMALL] Saved X_test_pooled_tta_small.npy (5, 6473, 662) and test_fnames_pooled_small.csv | total 26.7s\n"
          ]
        }
      ]
    },
    {
      "id": "258ac513-6516-4bbb-94c4-8216d04ecad0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Robust power-weighted blend (no per-class biases, no silence override); small-TTA only\n",
        "import os, numpy as np, pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "CLASSES = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "\n",
        "# Load OOF and small-TTA test preds for all 5 models\n",
        "oofs = {}\n",
        "tests = {}\n",
        "names_map = {\n",
        "    'XGB': ('oof_pooled.npy', 'test_pred_pooled_tta50.npy'),\n",
        "    'LGB': ('oof_pooled_lgb.npy', 'test_pred_pooled_lgb_tta50.npy'),\n",
        "    'LGB_DART': ('oof_pooled_lgb_dart.npy', 'test_pred_pooled_lgb_dart_tta50.npy'),\n",
        "    'XGB2': ('oof_pooled_xgb_seed2.npy', 'test_pred_pooled_xgb_seed2_tta50.npy'),\n",
        "    'CB': ('oof_pooled_cat.npy', 'test_pred_pooled_cat_tta50.npy'),\n",
        "}\n",
        "\n",
        "for k, (oof_p, te_p) in names_map.items():\n",
        "    if os.path.exists(oof_p) and os.path.exists(te_p):\n",
        "        oofs[k] = np.load(oof_p)\n",
        "        tests[k] = np.load(te_p)\n",
        "\n",
        "models = list(oofs.keys())\n",
        "if len(models) < 2:\n",
        "    raise RuntimeError(f'Need at least 2 models; found {models}')\n",
        "print('Models in robust blend:', models)\n",
        "\n",
        "# Compute OOF acc per model\n",
        "y = np.load('y_train_pooled.npy')\n",
        "accs = {m: float(accuracy_score(y, oofs[m].argmax(1))) for m in models}\n",
        "print('Per-model OOF acc:', accs)\n",
        "\n",
        "# Power weights: w_i \u221d (acc_i)^4 (normalized)\n",
        "pows = {m: accs[m]**4 for m in models}\n",
        "w_sum = sum(pows.values())\n",
        "weights = {m: (pows[m] / w_sum) for m in models}\n",
        "print('Power-weights:', weights)\n",
        "\n",
        "# OOF blend (for sanity) and test blend (linear prob space)\n",
        "oof_blend = np.zeros_like(next(iter(oofs.values())))\n",
        "for m in models:\n",
        "    oof_blend += weights[m] * oofs[m]\n",
        "oof_acc = accuracy_score(y, oof_blend.argmax(1))\n",
        "print(f'Power-weighted OOF acc (no biases/override): {oof_acc:.5f}')\n",
        "\n",
        "test_shape = next(iter(tests.values())).shape\n",
        "test_blend = np.zeros(test_shape, dtype=np.float32)\n",
        "for m in models:\n",
        "    test_blend += weights[m] * tests[m]\n",
        "\n",
        "# Build submission matching sample_submission order\n",
        "test_fnames_path = 'test_fnames_pooled_small.csv' if os.path.exists('test_fnames_pooled_small.csv') else 'test_fnames_pooled.csv'\n",
        "test_fnames = pd.read_csv(test_fnames_path, header=None)[0].values\n",
        "pred_idx = test_blend.argmax(1)\n",
        "labels = [CLASSES[i] for i in pred_idx]\n",
        "pred_df = pd.DataFrame({'fname': test_fnames, 'label': labels})\n",
        "sample_sub = pd.read_csv('sample_submission.csv')\n",
        "sub = sample_sub[['fname']].merge(pred_df, on='fname', how='left')\n",
        "assert sub['label'].notna().all(), 'Missing predictions after merge'\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv', sub.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models in robust blend: ['XGB', 'LGB', 'LGB_DART', 'XGB2', 'CB']\nPer-model OOF acc: {'XGB': 0.8903282193747757, 'LGB': 0.8865512774491595, 'LGB_DART': 0.8928409782591731, 'XGB2': 0.8906403633355704, 'CB': 0.890016075413981}\nPower-weights: {'XGB': 0.20022114819211928, 'LGB': 0.19684520138881842, 'LGB_DART': 0.20249105870942533, 'XGB2': 0.2005020814282891, 'CB': 0.19994051028134788}\nPower-weighted OOF acc (no biases/override): 0.89421\nSaved submission.csv (6473, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "3e166689-b9f7-4e49-8fd8-2468c77a96e8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Submission 2: Geometric (log-space) blend with coord descent (no biases/override)\n",
        "import os, numpy as np, pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "CLASSES = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "EPS = 1e-9\n",
        "\n",
        "# Load OOF and small-TTA test preds for all 5 models\n",
        "names_map = {\n",
        "    'XGB': ('oof_pooled.npy', 'test_pred_pooled_tta50.npy'),\n",
        "    'LGB': ('oof_pooled_lgb.npy', 'test_pred_pooled_lgb_tta50.npy'),\n",
        "    'LGB_DART': ('oof_pooled_lgb_dart.npy', 'test_pred_pooled_lgb_dart_tta50.npy'),\n",
        "    'XGB2': ('oof_pooled_xgb_seed2.npy', 'test_pred_pooled_xgb_seed2_tta50.npy'),\n",
        "    'CB': ('oof_pooled_cat.npy', 'test_pred_pooled_cat_tta50.npy'),\n",
        "}\n",
        "oofs, tests = {}, {}\n",
        "for k, (oof_p, te_p) in names_map.items():\n",
        "    if os.path.exists(oof_p) and os.path.exists(te_p):\n",
        "        oofs[k] = np.load(oof_p)\n",
        "        tests[k] = np.load(te_p)\n",
        "models = list(oofs.keys())\n",
        "assert len(models) >= 2, f'Need >=2 models, have {models}'\n",
        "print('Models in log-space blend:', models)\n",
        "\n",
        "y = np.load('y_train_pooled.npy')\n",
        "accs = {m: float(accuracy_score(y, oofs[m].argmax(1))) for m in models}\n",
        "print('Per-model OOF acc:', accs)\n",
        "pows = {m: accs[m]**4 for m in models}\n",
        "w = np.array([pows[m] for m in models], dtype=np.float64)\n",
        "w = w / w.sum()\n",
        "print('Init weights (power):', dict(zip(models, w)))\n",
        "\n",
        "def oof_acc_from_w(w_vec):\n",
        "    logits = None\n",
        "    for wi, m in zip(w_vec, models):\n",
        "        lp = np.log(np.clip(oofs[m], EPS, 1.0))\n",
        "        logits = lp * wi if logits is None else logits + wi * lp\n",
        "    pred = logits.argmax(1)\n",
        "    return float(accuracy_score(y, pred))\n",
        "\n",
        "best_w = w.copy()\n",
        "best_acc = oof_acc_from_w(best_w)\n",
        "print(f'Init log-space OOF acc: {best_acc:.5f}')\n",
        "\n",
        "# Coordinate descent with step=0.02, 2-3 passes\n",
        "step = 0.02\n",
        "for pass_idx in range(3):\n",
        "    improved = False\n",
        "    for i in range(len(best_w)):\n",
        "        for delta in (+step, -step):\n",
        "            w_try = best_w.copy()\n",
        "            w_try[i] = max(0.0, w_try[i] + delta)\n",
        "            rem = max(1e-12, w_try.sum())\n",
        "            w_try = w_try / rem  # renormalize to simplex\n",
        "            acc = oof_acc_from_w(w_try)\n",
        "            if acc > best_acc + 1e-9:\n",
        "                best_acc = acc\n",
        "                best_w = w_try\n",
        "                improved = True\n",
        "    print(f'Pass {pass_idx}: best OOF acc {best_acc:.5f} | weights {dict(zip(models, best_w))}')\n",
        "    if not improved:\n",
        "        break\n",
        "\n",
        "# Build test logits with best_w\n",
        "test_logits = None\n",
        "for wi, m in zip(best_w, models):\n",
        "    lp = np.log(np.clip(tests[m], EPS, 1.0))\n",
        "    test_logits = lp * wi if test_logits is None else test_logits + wi * lp\n",
        "pred_idx = test_logits.argmax(1)\n",
        "labels = [CLASSES[i] for i in pred_idx]\n",
        "\n",
        "# Submission aligned to sample_submission\n",
        "test_fnames_path = 'test_fnames_pooled_small.csv' if os.path.exists('test_fnames_pooled_small.csv') else 'test_fnames_pooled.csv'\n",
        "test_fnames = pd.read_csv(test_fnames_path, header=None)[0].values\n",
        "pred_df = pd.DataFrame({'fname': test_fnames, 'label': labels})\n",
        "sample_sub = pd.read_csv('sample_submission.csv')\n",
        "sub = sample_sub[['fname']].merge(pred_df, on='fname', how='left')\n",
        "assert sub['label'].notna().all(), 'Missing predictions after merge'\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(f'Saved submission.csv {sub.shape} | Final log-space OOF acc: {best_acc:.5f}')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models in log-space blend: ['XGB', 'LGB', 'LGB_DART', 'XGB2', 'CB']\nPer-model OOF acc: {'XGB': 0.8903282193747757, 'LGB': 0.8865512774491595, 'LGB_DART': 0.8928409782591731, 'XGB2': 0.8906403633355704, 'CB': 0.890016075413981}\nInit weights (power): {'XGB': 0.20022114819211928, 'LGB': 0.19684520138881842, 'LGB_DART': 0.20249105870942533, 'XGB2': 0.2005020814282891, 'CB': 0.19994051028134788}\nInit log-space OOF acc: 0.89417\nPass 0: best OOF acc 0.89432 | weights {'XGB': 0.19244631698588935, 'LGB': 0.1892014623114364, 'LGB_DART': 0.21385145973608738, 'XGB2': 0.19271634124210793, 'CB': 0.2117844197244789}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pass 1: best OOF acc 0.89451 | weights {'XGB': 0.19637379284274425, 'LGB': 0.17265455337901675, 'LGB_DART': 0.21821577524090552, 'XGB2': 0.19664932779806935, 'CB': 0.2161065507392642}\nPass 2: best OOF acc 0.89456 | weights {'XGB': 0.20447083802868, 'LGB': 0.15894893104853888, 'LGB_DART': 0.22721342694804822, 'XGB2': 0.18434957080182146, 'CB': 0.22501723317291147}\nSaved submission.csv (6473, 2) | Final log-space OOF acc: 0.89456\n"
          ]
        }
      ]
    },
    {
      "id": "e9f04232-e6ec-4afa-a670-f0b5a24a2663",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ExtraTrees (fast diversity) on pooled features with SGKF; predict on mean small-TTA features\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "CLASSES = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "num_class = len(CLASSES)\n",
        "\n",
        "train_feat = 'X_train_pooled.npy'\n",
        "train_y = 'y_train_pooled.npy'\n",
        "train_groups = 'groups_pooled.npy'\n",
        "test_feat_tta = 'X_test_pooled_tta_small.npy'\n",
        "test_fnames_csv = 'test_fnames_pooled_small.csv'\n",
        "\n",
        "assert os.path.exists(train_feat) and os.path.exists(train_y) and os.path.exists(train_groups), 'Missing train pooled features'\n",
        "assert os.path.exists(test_feat_tta) and os.path.exists(test_fnames_csv), 'Missing test pooled small-TTA features'\n",
        "\n",
        "X = np.load(train_feat)\n",
        "y = np.load(train_y)\n",
        "groups = np.load(train_groups)\n",
        "X_test_tta = np.load(test_feat_tta)  # [n_shifts, N, D]\n",
        "X_test_avg = X_test_tta.mean(axis=0)  # (N, D)\n",
        "test_fnames = pd.read_csv(test_fnames_csv, header=None)[0].values\n",
        "n_test, D = X_test_avg.shape\n",
        "print('Shapes:', X.shape, y.shape, groups.shape, X_test_tta.shape, '| test_avg', X_test_avg.shape)\n",
        "\n",
        "params = dict(\n",
        "    n_estimators=600,\n",
        "    max_depth=None,\n",
        "    min_samples_leaf=2,\n",
        "    max_features='sqrt',\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_et = np.zeros((len(y), num_class), dtype=np.float32)\n",
        "test_et = np.zeros((n_test, num_class), dtype=np.float32)\n",
        "start = time.time()\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(cv.split(X, y, groups)):\n",
        "    t0 = time.time()\n",
        "    print(f'ET Fold {fold} | train {len(tr_idx)} val {len(va_idx)}')\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_tr = scaler.fit_transform(X[tr_idx])\n",
        "    X_va = scaler.transform(X[va_idx])\n",
        "    X_tr = np.clip(X_tr, -5, 5)\n",
        "    X_va = np.clip(X_va, -5, 5)\n",
        "    clf = ExtraTreesClassifier(**params)\n",
        "    clf.fit(X_tr, y[tr_idx])\n",
        "    oof_et[va_idx] = clf.predict_proba(X_va)\n",
        "    va_acc = accuracy_score(y[va_idx], oof_et[va_idx].argmax(1))\n",
        "    print(f'ET Fold {fold} acc: {va_acc:.4f} | elapsed {time.time()-t0:.1f}s')\n",
        "    # Test uses mean of small-TTA features transformed by this fold's scaler\n",
        "    X_te = scaler.transform(X_test_avg)\n",
        "    X_te = np.clip(X_te, -5, 5)\n",
        "    test_et += clf.predict_proba(X_te) / cv.n_splits\n",
        "\n",
        "oof_acc_et = accuracy_score(y, oof_et.argmax(1))\n",
        "print(f'ExtraTrees OOF accuracy: {oof_acc_et:.4f} | total {time.time()-start:.1f}s')\n",
        "np.save('oof_pooled_et.npy', oof_et)\n",
        "np.save('test_pred_pooled_et_tta50.npy', test_et)\n",
        "print('Saved ExtraTrees preds to oof_pooled_et.npy and test_pred_pooled_et_tta50.npy.')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes: (64073, 662) (64073,) (64073,) (5, 6473, 662) | test_avg (6473, 662)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ET Fold 0 | train 52005 val 12068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ET Fold 0 acc: 0.7406 | elapsed 8.0s\nET Fold 1 | train 51074 val 12999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ET Fold 1 acc: 0.7271 | elapsed 7.6s\nET Fold 2 | train 51792 val 12281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ET Fold 2 acc: 0.7381 | elapsed 8.0s\nET Fold 3 | train 50819 val 13254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ET Fold 3 acc: 0.7249 | elapsed 7.7s\nET Fold 4 | train 50602 val 13471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ET Fold 4 acc: 0.7276 | elapsed 7.7s\nExtraTrees OOF accuracy: 0.7314 | total 41.5s\nSaved ExtraTrees preds to oof_pooled_et.npy and test_pred_pooled_et_tta50.npy.\n"
          ]
        }
      ]
    },
    {
      "id": "dae39d18-e506-4151-a9c4-2fcf7f4a9fb1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Log-space blend including ExtraTrees (if available); no biases/override\n",
        "import os, numpy as np, pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "CLASSES = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "EPS = 1e-9\n",
        "\n",
        "names_map = {\n",
        "    'XGB': ('oof_pooled.npy', 'test_pred_pooled_tta50.npy'),\n",
        "    'LGB': ('oof_pooled_lgb.npy', 'test_pred_pooled_lgb_tta50.npy'),\n",
        "    'LGB_DART': ('oof_pooled_lgb_dart.npy', 'test_pred_pooled_lgb_dart_tta50.npy'),\n",
        "    'XGB2': ('oof_pooled_xgb_seed2.npy', 'test_pred_pooled_xgb_seed2_tta50.npy'),\n",
        "    'CB': ('oof_pooled_cat.npy', 'test_pred_pooled_cat_tta50.npy'),\n",
        "    'ET': ('oof_pooled_et.npy', 'test_pred_pooled_et_tta50.npy'),\n",
        "}\n",
        "oofs, tests = {}, {}\n",
        "for k, (oof_p, te_p) in names_map.items():\n",
        "    if os.path.exists(oof_p) and os.path.exists(te_p):\n",
        "        oofs[k] = np.load(oof_p)\n",
        "        tests[k] = np.load(te_p)\n",
        "models = list(oofs.keys())\n",
        "assert len(models) >= 2, f'Need >=2 models, have {models}'\n",
        "print('Models in log-space blend (+ET if present):', models)\n",
        "\n",
        "y = np.load('y_train_pooled.npy')\n",
        "accs = {m: float(accuracy_score(y, oofs[m].argmax(1))) for m in models}\n",
        "print('Per-model OOF acc:', accs)\n",
        "pows = {m: accs[m]**4 for m in models}\n",
        "w = np.array([pows[m] for m in models], dtype=np.float64)\n",
        "w = w / w.sum()\n",
        "print('Init weights (power):', dict(zip(models, w)))\n",
        "\n",
        "def oof_acc_from_w(w_vec):\n",
        "    logits = None\n",
        "    for wi, m in zip(w_vec, models):\n",
        "        lp = np.log(np.clip(oofs[m], EPS, 1.0))\n",
        "        logits = lp * wi if logits is None else logits + wi * lp\n",
        "    return float(accuracy_score(y, logits.argmax(1)))\n",
        "\n",
        "best_w = w.copy()\n",
        "best_acc = oof_acc_from_w(best_w)\n",
        "print(f'Init log-space OOF acc: {best_acc:.5f}')\n",
        "\n",
        "# 3 passes coord descent, step=0.02\n",
        "step = 0.02\n",
        "for p in range(3):\n",
        "    improved = False\n",
        "    for i in range(len(best_w)):\n",
        "        for d in (+step, -step):\n",
        "            w_try = best_w.copy()\n",
        "            w_try[i] = max(0.0, w_try[i] + d)\n",
        "            w_try = w_try / max(1e-12, w_try.sum())\n",
        "            acc = oof_acc_from_w(w_try)\n",
        "            if acc > best_acc + 1e-9:\n",
        "                best_acc = acc\n",
        "                best_w = w_try\n",
        "                improved = True\n",
        "    print(f'Pass {p}: best OOF acc {best_acc:.5f} | weights {dict(zip(models, best_w))}')\n",
        "    if not improved:\n",
        "        break\n",
        "\n",
        "test_logits = None\n",
        "for wi, m in zip(best_w, models):\n",
        "    lp = np.log(np.clip(tests[m], EPS, 1.0))\n",
        "    test_logits = lp * wi if test_logits is None else test_logits + wi * lp\n",
        "pred_idx = test_logits.argmax(1)\n",
        "labels = [CLASSES[i] for i in pred_idx]\n",
        "\n",
        "test_fnames_path = 'test_fnames_pooled_small.csv' if os.path.exists('test_fnames_pooled_small.csv') else 'test_fnames_pooled.csv'\n",
        "test_fnames = pd.read_csv(test_fnames_path, header=None)[0].values\n",
        "pred_df = pd.DataFrame({'fname': test_fnames, 'label': labels})\n",
        "sample_sub = pd.read_csv('sample_submission.csv')\n",
        "sub = sample_sub[['fname']].merge(pred_df, on='fname', how='left')\n",
        "assert sub['label'].notna().all()\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(f'Saved submission.csv {sub.shape} | Final log-space OOF acc: {best_acc:.5f}')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models in log-space blend (+ET if present): ['XGB', 'LGB', 'LGB_DART', 'XGB2', 'CB', 'ET']\nPer-model OOF acc: {'XGB': 0.8903282193747757, 'LGB': 0.8865512774491595, 'LGB_DART': 0.8928409782591731, 'XGB2': 0.8906403633355704, 'CB': 0.890016075413981, 'ET': 0.7314001217361447}\nInit weights (power): {'XGB': 0.18348942943692537, 'LGB': 0.18039559764972227, 'LGB_DART': 0.18556965217790033, 'XGB2': 0.18374688615256268, 'CB': 0.1832322433674678, 'ET': 0.0835661912154216}\nInit log-space OOF acc: 0.89381\nPass 0: best OOF acc 0.89411 | weights {'XGB': 0.20780848847706157, 'LGB': 0.16339150098643704, 'LGB_DART': 0.1895083643067401, 'XGB2': 0.16723058031201257, 'CB': 0.20712934724267795, 'ET': 0.06493171867507076}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pass 1: best OOF acc 0.89459 | weights {'XGB': 0.19179471986124566, 'LGB': 0.1468434726979344, 'LGB_DART': 0.21314636493418215, 'XGB2': 0.15077204280869214, 'CB': 0.2311335066021635, 'ET': 0.06630989309578228}\nPass 2: best OOF acc 0.89470 | weights {'XGB': 0.1957088978175976, 'LGB': 0.12943211499789226, 'LGB_DART': 0.21749629074916543, 'XGB2': 0.15384902327417563, 'CB': 0.23585051694098313, 'ET': 0.067663156220186}\nSaved submission.csv (6473, 2) | Final log-space OOF acc: 0.89470\n"
          ]
        }
      ]
    },
    {
      "id": "c0b19736-9112-44a4-b448-3886b9e45985",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional tiny global silence boost after log-space blend (no OOF search), factor=1.04\n",
        "import os, numpy as np, pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "CLASSES = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "EPS = 1e-9\n",
        "sil_idx = CLASSES.index('silence')\n",
        "\n",
        "# Load OOF/test preds for 5 core models\n",
        "names_map = {\n",
        "    'XGB': ('oof_pooled.npy', 'test_pred_pooled_tta50.npy'),\n",
        "    'LGB': ('oof_pooled_lgb.npy', 'test_pred_pooled_lgb_tta50.npy'),\n",
        "    'LGB_DART': ('oof_pooled_lgb_dart.npy', 'test_pred_pooled_lgb_dart_tta50.npy'),\n",
        "    'XGB2': ('oof_pooled_xgb_seed2.npy', 'test_pred_pooled_xgb_seed2_tta50.npy'),\n",
        "    'CB': ('oof_pooled_cat.npy', 'test_pred_pooled_cat_tta50.npy'),\n",
        "}\n",
        "oofs, tests = {}, {}\n",
        "for k, (oof_p, te_p) in names_map.items():\n",
        "    if os.path.exists(oof_p) and os.path.exists(te_p):\n",
        "        oofs[k] = np.load(oof_p)\n",
        "        tests[k] = np.load(te_p)\n",
        "models = list(oofs.keys())\n",
        "assert len(models) >= 2, f'Need >=2 models, have {models}'\n",
        "\n",
        "y = np.load('y_train_pooled.npy')\n",
        "\n",
        "# Init weights from power of OOF accs\n",
        "accs = {m: float(accuracy_score(y, oofs[m].argmax(1))) for m in models}\n",
        "pows = {m: accs[m]**4 for m in models}\n",
        "w = np.array([pows[m] for m in models], dtype=np.float64)\n",
        "w = w / w.sum()\n",
        "\n",
        "def oof_acc_from_w(w_vec):\n",
        "    logits = None\n",
        "    for wi, m in zip(w_vec, models):\n",
        "        lp = np.log(np.clip(oofs[m], EPS, 1.0))\n",
        "        logits = lp * wi if logits is None else logits + wi * lp\n",
        "    return float(accuracy_score(y, logits.argmax(1)))\n",
        "\n",
        "best_w = w.copy()\n",
        "best_acc = oof_acc_from_w(best_w)\n",
        "\n",
        "# Light coord descent refinement (step=0.02, up to 3 passes)\n",
        "step = 0.02\n",
        "for _ in range(3):\n",
        "    improved = False\n",
        "    for i in range(len(best_w)):\n",
        "        for d in (+step, -step):\n",
        "            w_try = best_w.copy()\n",
        "            w_try[i] = max(0.0, w_try[i] + d)\n",
        "            w_try = w_try / max(1e-12, w_try.sum())\n",
        "            acc = oof_acc_from_w(w_try)\n",
        "            if acc > best_acc + 1e-9:\n",
        "                best_acc, best_w = acc, w_try\n",
        "                improved = True\n",
        "    if not improved:\n",
        "        break\n",
        "\n",
        "# Build test logits and convert to probs\n",
        "test_logits = None\n",
        "for wi, m in zip(best_w, models):\n",
        "    lp = np.log(np.clip(tests[m], EPS, 1.0))\n",
        "    test_logits = lp * wi if test_logits is None else test_logits + wi * lp\n",
        "probs = np.exp(test_logits - test_logits.max(axis=1, keepdims=True))\n",
        "probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "\n",
        "# Tiny global silence boost\n",
        "boost = 1.04\n",
        "probs[:, sil_idx] *= boost\n",
        "probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "\n",
        "pred_idx = probs.argmax(1)\n",
        "labels = [CLASSES[i] for i in pred_idx]\n",
        "\n",
        "test_fnames_path = 'test_fnames_pooled_small.csv' if os.path.exists('test_fnames_pooled_small.csv') else 'test_fnames_pooled.csv'\n",
        "test_fnames = pd.read_csv(test_fnames_path, header=None)[0].values\n",
        "pred_df = pd.DataFrame({'fname': test_fnames, 'label': labels})\n",
        "sample_sub = pd.read_csv('sample_submission.csv')\n",
        "sub = sample_sub[['fname']].merge(pred_df, on='fname', how='left')\n",
        "assert sub['label'].notna().all(), 'Missing predictions after merge'\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv', sub.shape, '| Used silence boost factor:', boost, '| OOF (unboosted logits) acc:', f'{best_acc:.5f}')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (6473, 2) | Used silence boost factor: 1.04 | OOF (unboosted logits) acc: 0.89456\n"
          ]
        }
      ]
    },
    {
      "id": "52eb3b1e-b4aa-40cf-a728-ed9d65908908",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Submission 3: Model subset selection + log-space blend (no biases/override)\n",
        "import os, itertools, numpy as np, pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "CLASSES = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "EPS = 1e-9\n",
        "\n",
        "# Core models\n",
        "names_map = {\n",
        "    'XGB': ('oof_pooled.npy', 'test_pred_pooled_tta50.npy'),\n",
        "    'LGB': ('oof_pooled_lgb.npy', 'test_pred_pooled_lgb_tta50.npy'),\n",
        "    'LGB_DART': ('oof_pooled_lgb_dart.npy', 'test_pred_pooled_lgb_dart_tta50.npy'),\n",
        "    'XGB2': ('oof_pooled_xgb_seed2.npy', 'test_pred_pooled_xgb_seed2_tta50.npy'),\n",
        "    'CB': ('oof_pooled_cat.npy', 'test_pred_pooled_cat_tta50.npy'),\n",
        "    'ET': ('oof_pooled_et.npy', 'test_pred_pooled_et_tta50.npy'),\n",
        "}\n",
        "oofs_all, tests_all = {}, {}\n",
        "for k, (oof_p, te_p) in names_map.items():\n",
        "    if os.path.exists(oof_p) and os.path.exists(te_p):\n",
        "        oofs_all[k] = np.load(oof_p)\n",
        "        tests_all[k] = np.load(te_p)\n",
        "models_all = list(oofs_all.keys())\n",
        "y = np.load('y_train_pooled.npy')\n",
        "print('Available models:', models_all)\n",
        "\n",
        "def logspace_oof_acc(models_subset, init_w=None):\n",
        "    # init_w: optional initial weights in order of models_subset\n",
        "    accs = [float(accuracy_score(y, oofs_all[m].argmax(1))) for m in models_subset]\n",
        "    if init_w is None:\n",
        "        pows = np.array([a**4 for a in accs], dtype=np.float64)\n",
        "        w = pows / pows.sum()\n",
        "    else:\n",
        "        w = np.array(init_w, dtype=np.float64)\n",
        "        w = w / w.sum()\n",
        "    def oof_acc_from_w(w_vec):\n",
        "        logits = None\n",
        "        for wi, m in zip(w_vec, models_subset):\n",
        "            lp = np.log(np.clip(oofs_all[m], EPS, 1.0))\n",
        "            logits = lp * wi if logits is None else logits + wi * lp\n",
        "        return float(accuracy_score(y, logits.argmax(1)))\n",
        "    best_w = w.copy()\n",
        "    best_acc = oof_acc_from_w(best_w)\n",
        "    step = 0.02\n",
        "    for _ in range(3):\n",
        "        improved = False\n",
        "        for i in range(len(best_w)):\n",
        "            for d in (+step, -step):\n",
        "                w_try = best_w.copy()\n",
        "                w_try[i] = max(0.0, w_try[i] + d)\n",
        "                w_try = w_try / max(1e-12, w_try.sum())\n",
        "                acc = oof_acc_from_w(w_try)\n",
        "                if acc > best_acc + 1e-9:\n",
        "                    best_acc, best_w = acc, w_try\n",
        "                    improved = True\n",
        "        if not improved:\n",
        "            break\n",
        "    return best_acc, best_w\n",
        "\n",
        "candidates = []\n",
        "# Evaluate all subsets of size 3..5 from the 5 core boosters + optionally ET\n",
        "base_list = [m for m in ['XGB','LGB','LGB_DART','XGB2','CB'] if m in models_all]\n",
        "opt_list = base_list.copy()\n",
        "if 'ET' in models_all:\n",
        "    opt_list.append('ET')\n",
        "for k in range(3, min(6, len(opt_list)+1)):\n",
        "    for subset in itertools.combinations(opt_list, k):\n",
        "        acc, w = logspace_oof_acc(list(subset))\n",
        "        candidates.append((acc, list(subset), w))\n",
        "        print(f'Subset {subset} -> OOF {acc:.5f}')\n",
        "best = max(candidates, key=lambda x: x[0])\n",
        "best_acc, best_models, best_w = best\n",
        "print('Best subset:', best_models, '| OOF:', f'{best_acc:.5f}', '| weights:', dict(zip(best_models, best_w)))\n",
        "\n",
        "# Build test logits using best subset\n",
        "test_logits = None\n",
        "for wi, m in zip(best_w, best_models):\n",
        "    lp = np.log(np.clip(tests_all[m], EPS, 1.0))\n",
        "    test_logits = lp * wi if test_logits is None else test_logits + wi * lp\n",
        "pred_idx = test_logits.argmax(1)\n",
        "labels = [CLASSES[i] for i in pred_idx]\n",
        "\n",
        "test_fnames_path = 'test_fnames_pooled_small.csv' if os.path.exists('test_fnames_pooled_small.csv') else 'test_fnames_pooled.csv'\n",
        "test_fnames = pd.read_csv(test_fnames_path, header=None)[0].values\n",
        "pred_df = pd.DataFrame({'fname': test_fnames, 'label': labels})\n",
        "sample_sub = pd.read_csv('sample_submission.csv')\n",
        "sub = sample_sub[['fname']].merge(pred_df, on='fname', how='left')\n",
        "assert sub['label'].notna().all(), 'Missing predictions after merge'\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv', sub.shape, '| Best subset OOF:', f'{best_acc:.5f}')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available models: ['XGB', 'LGB', 'LGB_DART', 'XGB2', 'CB', 'ET']\nSubset ('XGB', 'LGB', 'LGB_DART') -> OOF 0.89183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('XGB', 'LGB', 'XGB2') -> OOF 0.89178\nSubset ('XGB', 'LGB', 'CB') -> OOF 0.89443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('XGB', 'LGB', 'ET') -> OOF 0.88908\nSubset ('XGB', 'LGB_DART', 'XGB2') -> OOF 0.89370\nSubset ('XGB', 'LGB_DART', 'CB') -> OOF 0.89529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('XGB', 'LGB_DART', 'ET') -> OOF 0.89189\nSubset ('XGB', 'XGB2', 'CB') -> OOF 0.89501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('XGB', 'XGB2', 'ET') -> OOF 0.89153\nSubset ('XGB', 'CB', 'ET') -> OOF 0.89320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('LGB', 'LGB_DART', 'XGB2') -> OOF 0.89284\nSubset ('LGB', 'LGB_DART', 'CB') -> OOF 0.89467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('LGB', 'LGB_DART', 'ET') -> OOF 0.89067\nSubset ('LGB', 'XGB2', 'CB') -> OOF 0.89384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('LGB', 'XGB2', 'ET') -> OOF 0.88975\nSubset ('LGB', 'CB', 'ET') -> OOF 0.89239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('LGB_DART', 'XGB2', 'CB') -> OOF 0.89531\nSubset ('LGB_DART', 'XGB2', 'ET') -> OOF 0.89247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('LGB_DART', 'CB', 'ET') -> OOF 0.89515\nSubset ('XGB2', 'CB', 'ET') -> OOF 0.89359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('XGB', 'LGB', 'LGB_DART', 'XGB2') -> OOF 0.89308\nSubset ('XGB', 'LGB', 'LGB_DART', 'CB') -> OOF 0.89407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('XGB', 'LGB', 'LGB_DART', 'ET') -> OOF 0.89119\nSubset ('XGB', 'LGB', 'XGB2', 'CB') -> OOF 0.89357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('XGB', 'LGB', 'XGB2', 'ET') -> OOF 0.89061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('XGB', 'LGB', 'CB', 'ET') -> OOF 0.89382\nSubset ('XGB', 'LGB_DART', 'XGB2', 'CB') -> OOF 0.89545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('XGB', 'LGB_DART', 'XGB2', 'ET') -> OOF 0.89314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('XGB', 'LGB_DART', 'CB', 'ET') -> OOF 0.89545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('XGB', 'XGB2', 'CB', 'ET') -> OOF 0.89465\nSubset ('LGB', 'LGB_DART', 'XGB2', 'CB') -> OOF 0.89450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('LGB', 'LGB_DART', 'XGB2', 'ET') -> OOF 0.89261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('LGB', 'LGB_DART', 'CB', 'ET') -> OOF 0.89426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('LGB', 'XGB2', 'CB', 'ET') -> OOF 0.89356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('LGB_DART', 'XGB2', 'CB', 'ET') -> OOF 0.89518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('XGB', 'LGB', 'LGB_DART', 'XGB2', 'CB') -> OOF 0.89456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('XGB', 'LGB', 'LGB_DART', 'XGB2', 'ET') -> OOF 0.89212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('XGB', 'LGB', 'LGB_DART', 'CB', 'ET') -> OOF 0.89375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('XGB', 'LGB', 'XGB2', 'CB', 'ET') -> OOF 0.89382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('XGB', 'LGB_DART', 'XGB2', 'CB', 'ET') -> OOF 0.89521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset ('LGB', 'LGB_DART', 'XGB2', 'CB', 'ET') -> OOF 0.89435\nBest subset: ['XGB', 'LGB_DART', 'XGB2', 'CB'] | OOF: 0.89545 | weights: {'XGB': 0.2444052425074738, 'LGB_DART': 0.26678391198081647, 'XGB2': 0.24474817109585956, 'CB': 0.24406267441585017}\nSaved submission.csv (6473, 2) | Best subset OOF: 0.89545\n"
          ]
        }
      ]
    },
    {
      "id": "75ef9f52-567e-4139-b0a0-2e2f3d5f9961",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sanity check submission: LGB-DART only (small-TTA) to validate pipeline on LB\n",
        "import os, numpy as np, pandas as pd\n",
        "CLASSES = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "assert os.path.exists('test_pred_pooled_lgb_dart_tta50.npy'), 'Missing LGB-DART small-TTA test preds'\n",
        "test_pred = np.load('test_pred_pooled_lgb_dart_tta50.npy')\n",
        "pred_idx = test_pred.argmax(1)\n",
        "labels = [CLASSES[i] for i in pred_idx]\n",
        "test_fnames_path = 'test_fnames_pooled_small.csv' if os.path.exists('test_fnames_pooled_small.csv') else 'test_fnames_pooled.csv'\n",
        "test_fnames = pd.read_csv(test_fnames_path, header=None)[0].values\n",
        "pred_df = pd.DataFrame({'fname': test_fnames, 'label': labels})\n",
        "sample_sub = pd.read_csv('sample_submission.csv')\n",
        "sub = sample_sub[['fname']].merge(pred_df, on='fname', how='left')\n",
        "assert sub['label'].notna().all(), 'Missing predictions after merge'\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved LGB-DART only submission.csv', sub.shape)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved LGB-DART only submission.csv (6473, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "85d68ba4-3747-4930-b6c0-9585ec641d2a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# HistGradientBoostingClassifier (diversity) on pooled features; predict on mean small-TTA\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "CLASSES = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "num_class = len(CLASSES)\n",
        "\n",
        "train_feat = 'X_train_pooled.npy'\n",
        "train_y = 'y_train_pooled.npy'\n",
        "train_groups = 'groups_pooled.npy'\n",
        "test_feat_tta = 'X_test_pooled_tta_small.npy'\n",
        "test_fnames_csv = 'test_fnames_pooled_small.csv'\n",
        "\n",
        "assert os.path.exists(train_feat) and os.path.exists(train_y) and os.path.exists(train_groups), 'Missing train pooled features'\n",
        "assert os.path.exists(test_feat_tta) and os.path.exists(test_fnames_csv), 'Missing test pooled small-TTA features'\n",
        "\n",
        "X = np.load(train_feat)\n",
        "y = np.load(train_y)\n",
        "groups = np.load(train_groups)\n",
        "X_test_tta = np.load(test_feat_tta)  # [n_shifts, N, D]\n",
        "X_test_avg = X_test_tta.mean(axis=0)  # (N, D)\n",
        "test_fnames = pd.read_csv(test_fnames_csv, header=None)[0].values\n",
        "n_test, D = X_test_avg.shape\n",
        "print('Shapes:', X.shape, y.shape, groups.shape, X_test_tta.shape, '| test_avg', X_test_avg.shape)\n",
        "\n",
        "params = dict(\n",
        "    loss='log_loss',\n",
        "    learning_rate=0.06,\n",
        "    max_depth=7,\n",
        "    max_leaf_nodes=63,\n",
        "    l2_regularization=0.1,\n",
        "    max_bins=255,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_hgb = np.zeros((len(y), num_class), dtype=np.float32)\n",
        "test_hgb = np.zeros((n_test, num_class), dtype=np.float32)\n",
        "start = time.time()\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(cv.split(X, y, groups)):\n",
        "    t0 = time.time()\n",
        "    print(f'HGB Fold {fold} | train {len(tr_idx)} val {len(va_idx)}')\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_tr = scaler.fit_transform(X[tr_idx])\n",
        "    X_va = scaler.transform(X[va_idx])\n",
        "    X_tr = np.clip(X_tr, -5, 5)\n",
        "    X_va = np.clip(X_va, -5, 5)\n",
        "    clf = HistGradientBoostingClassifier(**params)\n",
        "    clf.fit(X_tr, y[tr_idx])\n",
        "    oof_hgb[va_idx] = clf.predict_proba(X_va)\n",
        "    va_acc = accuracy_score(y[va_idx], oof_hgb[va_idx].argmax(1))\n",
        "    print(f'HGB Fold {fold} acc: {va_acc:.4f} | elapsed {time.time()-t0:.1f}s')\n",
        "    X_te = scaler.transform(X_test_avg)\n",
        "    X_te = np.clip(X_te, -5, 5)\n",
        "    test_hgb += clf.predict_proba(X_te) / cv.n_splits\n",
        "\n",
        "oof_acc_hgb = accuracy_score(y, oof_hgb.argmax(1))\n",
        "print(f'HistGBDT OOF accuracy: {oof_acc_hgb:.4f} | total {time.time()-start:.1f}s')\n",
        "np.save('oof_pooled_hgb.npy', oof_hgb)\n",
        "np.save('test_pred_pooled_hgb_tta50.npy', test_hgb)\n",
        "print('Saved HGB preds to oof_pooled_hgb.npy and test_pred_pooled_hgb_tta50.npy.')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes: (64073, 662) (64073,) (64073,) (5, 6473, 662) | test_avg (6473, 662)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HGB Fold 0 | train 52005 val 12068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HGB Fold 0 acc: 0.8549 | elapsed 20.1s\nHGB Fold 1 | train 51074 val 12999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HGB Fold 1 acc: 0.8361 | elapsed 17.8s\nHGB Fold 2 | train 51792 val 12281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HGB Fold 2 acc: 0.8543 | elapsed 19.9s\nHGB Fold 3 | train 50819 val 13254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HGB Fold 3 acc: 0.8375 | elapsed 19.8s\nHGB Fold 4 | train 50602 val 13471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HGB Fold 4 acc: 0.8428 | elapsed 20.0s\nHistGBDT OOF accuracy: 0.8448 | total 99.6s\nSaved HGB preds to oof_pooled_hgb.npy and test_pred_pooled_hgb_tta50.npy.\n"
          ]
        }
      ]
    },
    {
      "id": "390a21c2-d7a3-4a52-8ab5-d515cd699826",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Log-space blend including HGB (and ET if present); no biases/override\n",
        "import os, numpy as np, pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "CLASSES = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "EPS = 1e-9\n",
        "\n",
        "names_map = {\n",
        "    'XGB': ('oof_pooled.npy', 'test_pred_pooled_tta50.npy'),\n",
        "    'LGB': ('oof_pooled_lgb.npy', 'test_pred_pooled_lgb_tta50.npy'),\n",
        "    'LGB_DART': ('oof_pooled_lgb_dart.npy', 'test_pred_pooled_lgb_dart_tta50.npy'),\n",
        "    'XGB2': ('oof_pooled_xgb_seed2.npy', 'test_pred_pooled_xgb_seed2_tta50.npy'),\n",
        "    'CB': ('oof_pooled_cat.npy', 'test_pred_pooled_cat_tta50.npy'),\n",
        "    'ET': ('oof_pooled_et.npy', 'test_pred_pooled_et_tta50.npy'),\n",
        "    'HGB': ('oof_pooled_hgb.npy', 'test_pred_pooled_hgb_tta50.npy'),\n",
        "}\n",
        "oofs, tests = {}, {}\n",
        "for k, (oof_p, te_p) in names_map.items():\n",
        "    if os.path.exists(oof_p) and os.path.exists(te_p):\n",
        "        oofs[k] = np.load(oof_p)\n",
        "        tests[k] = np.load(te_p)\n",
        "models = list(oofs.keys())\n",
        "assert len(models) >= 2, f'Need >=2 models, have {models}'\n",
        "print('Models in log-space blend (+HGB/ET if present):', models)\n",
        "\n",
        "y = np.load('y_train_pooled.npy')\n",
        "accs = {m: float(accuracy_score(y, oofs[m].argmax(1))) for m in models}\n",
        "print('Per-model OOF acc:', accs)\n",
        "pows = {m: accs[m]**4 for m in models}\n",
        "w = np.array([pows[m] for m in models], dtype=np.float64)\n",
        "w = w / w.sum()\n",
        "print('Init weights (power):', dict(zip(models, w)))\n",
        "\n",
        "def oof_acc_from_w(w_vec):\n",
        "    logits = None\n",
        "    for wi, m in zip(w_vec, models):\n",
        "        lp = np.log(np.clip(oofs[m], EPS, 1.0))\n",
        "        logits = lp * wi if logits is None else logits + wi * lp\n",
        "    return float(accuracy_score(y, logits.argmax(1)))\n",
        "\n",
        "best_w = w.copy()\n",
        "best_acc = oof_acc_from_w(best_w)\n",
        "print(f'Init log-space OOF acc: {best_acc:.5f}')\n",
        "\n",
        "# 3 passes coord descent, step=0.02\n",
        "step = 0.02\n",
        "for p in range(3):\n",
        "    improved = False\n",
        "    for i in range(len(best_w)):\n",
        "        for d in (+step, -step):\n",
        "            w_try = best_w.copy()\n",
        "            w_try[i] = max(0.0, w_try[i] + d)\n",
        "            w_try = w_try / max(1e-12, w_try.sum())\n",
        "            acc = oof_acc_from_w(w_try)\n",
        "            if acc > best_acc + 1e-9:\n",
        "                best_acc = acc\n",
        "                best_w = w_try\n",
        "                improved = True\n",
        "    print(f'Pass {p}: best OOF acc {best_acc:.5f} | weights {dict(zip(models, best_w))}')\n",
        "    if not improved:\n",
        "        break\n",
        "\n",
        "test_logits = None\n",
        "for wi, m in zip(best_w, models):\n",
        "    lp = np.log(np.clip(tests[m], EPS, 1.0))\n",
        "    test_logits = lp * wi if test_logits is None else test_logits + wi * lp\n",
        "pred_idx = test_logits.argmax(1)\n",
        "labels = [CLASSES[i] for i in pred_idx]\n",
        "\n",
        "test_fnames_path = 'test_fnames_pooled_small.csv' if os.path.exists('test_fnames_pooled_small.csv') else 'test_fnames_pooled.csv'\n",
        "test_fnames = pd.read_csv(test_fnames_path, header=None)[0].values\n",
        "pred_df = pd.DataFrame({'fname': test_fnames, 'label': labels})\n",
        "sample_sub = pd.read_csv('sample_submission.csv')\n",
        "sub = sample_sub[['fname']].merge(pred_df, on='fname', how='left')\n",
        "assert sub['label'].notna().all()\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(f'Saved submission.csv {sub.shape} | Final log-space OOF acc: {best_acc:.5f}')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models in log-space blend (+HGB/ET if present): ['XGB', 'LGB', 'LGB_DART', 'XGB2', 'CB', 'ET', 'HGB']\nPer-model OOF acc: {'XGB': 0.8903282193747757, 'LGB': 0.8865512774491595, 'LGB_DART': 0.8928409782591731, 'XGB2': 0.8906403633355704, 'CB': 0.890016075413981, 'ET': 0.7314001217361447, 'HGB': 0.8448332370889454}\nInit weights (power): {'XGB': 0.15972785944726164, 'LGB': 0.15703467363063822, 'LGB_DART': 0.16153869687048042, 'XGB2': 0.15995197595476435, 'CB': 0.15950397853771728, 'ET': 0.07274451114682975, 'HGB': 0.12949830441230833}\nInit log-space OOF acc: 0.89128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pass 0: best OOF acc 0.89225 | weights {'XGB': 0.17641539758082186, 'LGB': 0.13411643774096116, 'LGB_DART': 0.17818500793376804, 'XGB2': 0.1770200017445713, 'CB': 0.1769805813049954, 'ET': 0.05057914207890999, 'HGB': 0.10670343161597232}\nPass 1: best OOF acc 0.89300 | weights {'XGB': 0.16299521555428964, 'LGB': 0.11933372124986974, 'LGB_DART': 0.18568059299724807, 'XGB2': 0.2044825889063591, 'CB': 0.20484183038661286, 'ET': 0.03188216335226578, 'HGB': 0.09078388755335486}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pass 2: best OOF acc 0.89347 | weights {'XGB': 0.16312569000618418, 'LGB': 0.11942924553762055, 'LGB_DART': 0.20584523625101106, 'XGB2': 0.2250626029787256, 'CB': 0.20500580222746645, 'ET': 0.011083028001382403, 'HGB': 0.07044839499760974}\nSaved submission.csv (6473, 2) | Final log-space OOF acc: 0.89347\n"
          ]
        }
      ]
    },
    {
      "id": "a361dec1-2917-4de0-80dd-c35c9f05defb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Submission: 4-model best subset (XGB, LGB_DART, XGB2, CB) log-space blend + tiny silence boost x1.04\n",
        "import os, numpy as np, pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "CLASSES = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "EPS = 1e-9\n",
        "sil_idx = CLASSES.index('silence')\n",
        "\n",
        "# Best subset and weights from Cell 21\n",
        "models = ['XGB', 'LGB_DART', 'XGB2', 'CB']\n",
        "weights = np.array([0.2444052425074738, 0.26678391198081647, 0.24474817109585956, 0.24406267441585017], dtype=np.float64)\n",
        "weights = weights / weights.sum()\n",
        "\n",
        "paths = {\n",
        "    'XGB': ('oof_pooled.npy', 'test_pred_pooled_tta50.npy'),\n",
        "    'LGB_DART': ('oof_pooled_lgb_dart.npy', 'test_pred_pooled_lgb_dart_tta50.npy'),\n",
        "    'XGB2': ('oof_pooled_xgb_seed2.npy', 'test_pred_pooled_xgb_seed2_tta50.npy'),\n",
        "    'CB': ('oof_pooled_cat.npy', 'test_pred_pooled_cat_tta50.npy'),\n",
        "}\n",
        "\n",
        "oofs, tests = {}, {}\n",
        "for m in models:\n",
        "    oof_p, te_p = paths[m]\n",
        "    assert os.path.exists(oof_p) and os.path.exists(te_p), f'Missing files for {m}'\n",
        "    oofs[m] = np.load(oof_p)\n",
        "    tests[m] = np.load(te_p)\n",
        "\n",
        "# Log-space blend (geometric mean with fixed weights) -> logits\n",
        "test_logits = None\n",
        "for wi, m in zip(weights, models):\n",
        "    lp = np.log(np.clip(tests[m], EPS, 1.0))\n",
        "    test_logits = lp * wi if test_logits is None else test_logits + wi * lp\n",
        "\n",
        "# Convert to probs, apply tiny global silence boost x1.04\n",
        "probs = np.exp(test_logits - test_logits.max(axis=1, keepdims=True))\n",
        "probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "probs[:, sil_idx] *= 1.04\n",
        "probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "\n",
        "pred_idx = probs.argmax(1)\n",
        "labels = [CLASSES[i] for i in pred_idx]\n",
        "\n",
        "# Build submission aligned to sample_submission\n",
        "test_fnames_path = 'test_fnames_pooled_small.csv' if os.path.exists('test_fnames_pooled_small.csv') else 'test_fnames_pooled.csv'\n",
        "test_fnames = pd.read_csv(test_fnames_path, header=None)[0].values\n",
        "pred_df = pd.DataFrame({'fname': test_fnames, 'label': labels})\n",
        "sample_sub = pd.read_csv('sample_submission.csv')\n",
        "sub = sample_sub[['fname']].merge(pred_df, on='fname', how='left')\n",
        "assert sub['label'].notna().all(), 'Missing predictions after merge'\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv', sub.shape, '| Models:', models, '| Weights:', weights.round(5), '| silence x1.04')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (6473, 2) | Models: ['XGB', 'LGB_DART', 'XGB2', 'CB'] | Weights: [0.24441 0.26678 0.24475 0.24406] | silence x1.04\n"
          ]
        }
      ]
    },
    {
      "id": "1f372a42-323e-4208-bde1-f38639f87feb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Diagnostics: 4-model best subset distributions + write alt submissions with silence x1.03/x1.05\n",
        "import os, numpy as np, pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "CLASSES = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "EPS = 1e-9\n",
        "sil_idx = CLASSES.index('silence')\n",
        "\n",
        "# Best subset fixed\n",
        "models = ['XGB', 'LGB_DART', 'XGB2', 'CB']\n",
        "weights = np.array([0.2444052425074738, 0.26678391198081647, 0.24474817109585956, 0.24406267441585017], dtype=np.float64)\n",
        "weights = weights / weights.sum()\n",
        "paths = {\n",
        "    'XGB': ('oof_pooled.npy', 'test_pred_pooled_tta50.npy'),\n",
        "    'LGB_DART': ('oof_pooled_lgb_dart.npy', 'test_pred_pooled_lgb_dart_tta50.npy'),\n",
        "    'XGB2': ('oof_pooled_xgb_seed2.npy', 'test_pred_pooled_xgb_seed2_tta50.npy'),\n",
        "    'CB': ('oof_pooled_cat.npy', 'test_pred_pooled_cat_tta50.npy'),\n",
        "}\n",
        "\n",
        "# Load\n",
        "y = np.load('y_train_pooled.npy')\n",
        "oofs = {m: np.load(paths[m][0]) for m in models}\n",
        "tests = {m: np.load(paths[m][1]) for m in models}\n",
        "\n",
        "# OOF logits and preds\n",
        "logits_oof = None\n",
        "for wi, m in zip(weights, models):\n",
        "    lp = np.log(np.clip(oofs[m], EPS, 1.0))\n",
        "    logits_oof = lp * wi if logits_oof is None else logits_oof + wi * lp\n",
        "pred_oof = logits_oof.argmax(1)\n",
        "dist_oof = Counter([CLASSES[i] for i in pred_oof])\n",
        "print('OOF class distribution:', dict(dist_oof))\n",
        "\n",
        "# Test logits and base probs\n",
        "logits_te = None\n",
        "for wi, m in zip(weights, models):\n",
        "    lp = np.log(np.clip(tests[m], EPS, 1.0))\n",
        "    logits_te = lp * wi if logits_te is None else logits_te + wi * lp\n",
        "probs = np.exp(logits_te - logits_te.max(axis=1, keepdims=True))\n",
        "probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "\n",
        "def save_variant(probs_in, suffix):\n",
        "    pred_idx = probs_in.argmax(1)\n",
        "    labels = [CLASSES[i] for i in pred_idx]\n",
        "    dist = Counter(labels)\n",
        "    print(f'Test class distribution {suffix}:', dict(dist))\n",
        "    test_fnames_path = 'test_fnames_pooled_small.csv' if os.path.exists('test_fnames_pooled_small.csv') else 'test_fnames_pooled.csv'\n",
        "    test_fnames = pd.read_csv(test_fnames_path, header=None)[0].values\n",
        "    pred_df = pd.DataFrame({'fname': test_fnames, 'label': labels})\n",
        "    sample_sub = pd.read_csv('sample_submission.csv')\n",
        "    sub = sample_sub[['fname']].merge(pred_df, on='fname', how='left')\n",
        "    assert sub['label'].notna().all()\n",
        "    out = f'submission{suffix}.csv'\n",
        "    sub.to_csv(out, index=False)\n",
        "    print('Saved', out, sub.shape)\n",
        "\n",
        "# Save base (no boost) as submission_base.csv\n",
        "save_variant(probs, '_base')\n",
        "# Silence x1.03\n",
        "p103 = probs.copy()\n",
        "p103[:, sil_idx] *= 1.03\n",
        "p103 /= p103.sum(axis=1, keepdims=True)\n",
        "save_variant(p103, '_sil103')\n",
        "# Silence x1.05\n",
        "p105 = probs.copy()\n",
        "p105[:, sil_idx] *= 1.05\n",
        "p105 /= p105.sum(axis=1, keepdims=True)\n",
        "save_variant(p105, '_sil105')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF class distribution: {'yes': 1976, 'down': 1731, 'left': 1726, 'unknown': 39626, 'no': 1781, 'go': 1754, 'right': 1766, 'up': 2145, 'on': 1811, 'off': 1986, 'stop': 1947, 'silence': 5824}\nTest class distribution _base: {'unknown': 4523, 'up': 218, 'down': 174, 'yes': 217, 'left': 194, 'right': 212, 'off': 198, 'go': 180, 'on': 151, 'stop': 209, 'no': 196, 'silence': 1}\nSaved submission_base.csv (6473, 2)\nTest class distribution _sil103: {'unknown': 4523, 'up': 218, 'down': 174, 'yes': 217, 'left': 194, 'right': 212, 'off': 198, 'go': 180, 'on': 151, 'stop': 209, 'no': 196, 'silence': 1}\nSaved submission_sil103.csv (6473, 2)\nTest class distribution _sil105: {'unknown': 4523, 'up': 218, 'down': 174, 'yes': 217, 'left': 194, 'right': 212, 'off': 198, 'go': 180, 'on': 151, 'stop': 209, 'no': 196, 'silence': 1}\nSaved submission_sil105.csv (6473, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "b02f0b2f-dad1-4745-8054-dd55187cf863",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Tiny CNN on 64xT log-mels (CPU-only) with SGKF; save OOF/test logits for blending\n",
        "import os, time, math, numpy as np, pandas as pd, random, torch, librosa, soundfile as sf\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "\n",
        "# Threads and CPU perf settings\n",
        "torch.set_num_threads(max(1, min(os.cpu_count()-2, 16)))\n",
        "try:\n",
        "    torch.set_num_interop_threads(1)\n",
        "except Exception:\n",
        "    pass\n",
        "torch.backends.mkldnn.enabled = True\n",
        "\n",
        "SR = 16000\n",
        "N_MELS = 64\n",
        "N_FFT = 512\n",
        "HOP = 160\n",
        "WIN = 400\n",
        "FMIN, FMAX = 20, 8000\n",
        "FIX_DUR = 1.0\n",
        "NUM_CLASSES = 12\n",
        "CLASSES = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "CLS2IDX = {c:i for i,c in enumerate(CLASSES)}\n",
        "\n",
        "def load_1s(path: str, shift_samples: int = 0) -> np.ndarray:\n",
        "    y, sr = librosa.load(path, sr=SR, mono=True)\n",
        "    target = int(FIX_DUR * SR)\n",
        "    if shift_samples != 0:\n",
        "        if shift_samples > 0:\n",
        "            y = np.pad(y, (shift_samples, 0))\n",
        "        else:\n",
        "            y = np.pad(y, (0, -shift_samples))\n",
        "    if len(y) < target:\n",
        "        y = np.pad(y, (0, target - len(y)))\n",
        "    elif len(y) > target:\n",
        "        start = (len(y) - target) // 2\n",
        "        y = y[start:start+target]\n",
        "    return y.astype(np.float32)\n",
        "\n",
        "def compute_logmel(y: np.ndarray) -> np.ndarray:\n",
        "    mel = librosa.feature.melspectrogram(y=y, sr=SR, n_fft=N_FFT, hop_length=HOP, win_length=WIN, window='hann',\n",
        "                                         n_mels=N_MELS, fmin=FMIN, fmax=FMAX, power=2.0, center=True)\n",
        "    x = np.log(mel + 1e-6).astype(np.float32)  # [64, T]\n",
        "    # per-utterance z-norm (fast fallback to CMVN for speed)\n",
        "    m = x.mean(axis=1, keepdims=True); s = x.std(axis=1, keepdims=True) + 1e-8\n",
        "    x = (x - m) / s\n",
        "    return x\n",
        "\n",
        "def spec_augment(x: torch.Tensor, p: float = 0.5, F: int = 6, T: int = 12) -> torch.Tensor:\n",
        "    # x: (1, 64, T)\n",
        "    if random.random() > p:\n",
        "        return x\n",
        "    _, f, t = x.shape\n",
        "    # freq mask\n",
        "    f0 = random.randint(0, max(0, f - F))\n",
        "    f1 = min(f, f0 + F)\n",
        "    x[:, f0:f1, :] = 0\n",
        "    # time mask\n",
        "    t0 = random.randint(0, max(0, t - T))\n",
        "    t1 = min(t, t0 + T)\n",
        "    x[:, :, t0:t1] = 0\n",
        "    return x\n",
        "\n",
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, train: bool = True, time_shift_ms: int = 80):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.train = train\n",
        "        self.time_shift = int(time_shift_ms/1000.0 * SR)\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        path = row['path']\n",
        "        label = row['label'] if 'label' in row else None\n",
        "        shift = 0\n",
        "        if self.train:\n",
        "            # random shift in [-time_shift, time_shift]\n",
        "            shift = random.randint(-self.time_shift, self.time_shift)\n",
        "        y = load_1s(path, shift_samples=shift)\n",
        "        x = compute_logmel(y)  # [64, T]\n",
        "        x = torch.from_numpy(x).unsqueeze(0)  # (1,64,T) - DataLoader will batch to (B,1,64,T)\n",
        "        if self.train:\n",
        "            x = spec_augment(x, p=0.5, F=6, T=12)\n",
        "        if label is None:\n",
        "            y_t = torch.tensor(-1, dtype=torch.long)\n",
        "        else:\n",
        "            y_t = torch.tensor(CLS2IDX[label], dtype=torch.long)\n",
        "        return x, y_t\n",
        "\n",
        "class TinyCNN(torch.nn.Module):\n",
        "    def __init__(self, num_classes=12):\n",
        "        super().__init__()\n",
        "        self.block1 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            torch.nn.BatchNorm2d(32),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.MaxPool2d(2),\n",
        "            torch.nn.Dropout(0.10)\n",
        "        )\n",
        "        self.block2 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            torch.nn.BatchNorm2d(64),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.MaxPool2d(2),\n",
        "            torch.nn.Dropout(0.10)\n",
        "        )\n",
        "        self.block3 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(64, 96, kernel_size=3, padding=1),\n",
        "            torch.nn.BatchNorm2d(96),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.MaxPool2d(2),\n",
        "            torch.nn.Dropout(0.15)\n",
        "        )\n",
        "        self.head = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(96, 128, kernel_size=1),\n",
        "            torch.nn.BatchNorm2d(128),\n",
        "            torch.nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.gap = torch.nn.AdaptiveAvgPool2d(1)\n",
        "        self.drop = torch.nn.Dropout(0.20)\n",
        "        self.fc = torch.nn.Linear(128, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.head(x)\n",
        "        x = self.gap(x).squeeze(-1).squeeze(-1)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def train_cnn(seed=42, n_folds=3, epochs=20, batch_size=128):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    df_tr = pd.read_csv('train_meta.csv')\n",
        "    df_te = pd.read_csv('test_meta.csv')\n",
        "    groups = df_tr['speaker'].values\n",
        "    y_idx = df_tr['label'].map(CLS2IDX).values\n",
        "    cv = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
        "    oof = np.zeros((len(df_tr), NUM_CLASSES), dtype=np.float32)\n",
        "    test_logits = np.zeros((len(df_te), NUM_CLASSES), dtype=np.float32)\n",
        "    device = torch.device('cpu')\n",
        "    scaler_autocast = hasattr(torch.autocast, '__call__')\n",
        "\n",
        "    for fold, (tr_idx, va_idx) in enumerate(cv.split(np.zeros(len(y_idx)), y_idx, groups)):\n",
        "        t0 = time.time()\n",
        "        print(f'[CNN] Fold {fold} | train {len(tr_idx)} val {len(va_idx)}')\n",
        "        ds_tr = AudioDataset(df_tr.iloc[tr_idx], train=True, time_shift_ms=80)\n",
        "        ds_va = AudioDataset(df_tr.iloc[va_idx], train=False)\n",
        "        ds_te = AudioDataset(df_te, train=False)\n",
        "        dl_tr = torch.utils.data.DataLoader(ds_tr, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "        dl_va = torch.utils.data.DataLoader(ds_va, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "        dl_te = torch.utils.data.DataLoader(ds_te, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "        model = TinyCNN(NUM_CLASSES).to(device)\n",
        "        model = model.to(memory_format=torch.channels_last)\n",
        "        optim = torch.optim.AdamW(model.parameters(), lr=1e-3, betas=(0.9,0.98), weight_decay=1e-4)\n",
        "        loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.05)\n",
        "        # cosine schedule\n",
        "        total_steps = epochs * max(1, len(dl_tr))\n",
        "        def lr_lambda(step):\n",
        "            if step < len(dl_tr):\n",
        "                return float(step + 1) / float(len(dl_tr))\n",
        "            progress = (step - len(dl_tr)) / max(1, total_steps - len(dl_tr))\n",
        "            return 0.5 * (1 + math.cos(math.pi * progress)) * (1 - 1e-2) + 1e-2\n",
        "        sched = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda)\n",
        "\n",
        "        best_acc, best_state, no_improve = 0.0, None, 0\n",
        "        for ep in range(1, epochs+1):\n",
        "            model.train()\n",
        "            tr_loss, n_tr = 0.0, 0\n",
        "            for xb, yb in dl_tr:\n",
        "                # set channels_last after batching to avoid rank error\n",
        "                xb = xb.to(device, memory_format=torch.channels_last, non_blocking=False)\n",
        "                yb = yb.to(device)\n",
        "                optim.zero_grad(set_to_none=True)\n",
        "                if scaler_autocast:\n",
        "                    with torch.autocast('cpu', dtype=torch.bfloat16, enabled=True):\n",
        "                        logits = model(xb)\n",
        "                        loss = loss_fn(logits, yb)\n",
        "                else:\n",
        "                    logits = model(xb)\n",
        "                    loss = loss_fn(logits, yb)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                sched.step()\n",
        "                tr_loss += loss.item() * xb.size(0); n_tr += xb.size(0)\n",
        "            # val\n",
        "            model.eval()\n",
        "            correct, total = 0, 0\n",
        "            with torch.no_grad():\n",
        "                for xb, yb in dl_va:\n",
        "                    xb = xb.to(device, memory_format=torch.channels_last, non_blocking=False)\n",
        "                    logits = model(xb)\n",
        "                    pred = logits.argmax(1).cpu()\n",
        "                    correct += (pred == yb).sum().item()\n",
        "                    total += yb.size(0)\n",
        "            acc = correct / max(1, total)\n",
        "            print(f'[CNN] Fold {fold} Epoch {ep}/{epochs} | tr_loss {tr_loss/max(1,n_tr):.4f} | val_acc {acc:.4f}')\n",
        "            if acc > best_acc + 1e-4:\n",
        "                best_acc = acc\n",
        "                best_state = {k:v.cpu().clone() for k,v in model.state_dict().items()}\n",
        "                no_improve = 0\n",
        "            else:\n",
        "                no_improve += 1\n",
        "            if no_improve >= 3:\n",
        "                print(f'[CNN] Early stopping at epoch {ep} (best_acc={best_acc:.4f})')\n",
        "                break\n",
        "\n",
        "        if best_state is not None:\n",
        "            model.load_state_dict(best_state, strict=True)\n",
        "        # OOF logits\n",
        "        model.eval()\n",
        "        outs = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in dl_va:\n",
        "                xb = xb.to(device, memory_format=torch.channels_last, non_blocking=False)\n",
        "                logits = model(xb).cpu().float()\n",
        "                outs.append(logits.numpy())\n",
        "        oof_logits_fold = np.concatenate(outs, axis=0)\n",
        "        oof[va_idx] = oof_logits_fold  # logits ok for log-space blend (softmax later)\n",
        "\n",
        "        # Test logits (fold)\n",
        "        outs_te = []\n",
        "        with torch.no_grad():\n",
        "            for xb, _ in dl_te:\n",
        "                xb = xb.to(device, memory_format=torch.channels_last, non_blocking=False)\n",
        "                logits = model(xb).cpu().float()\n",
        "                outs_te.append(logits.numpy())\n",
        "        test_logits += np.concatenate(outs_te, axis=0) / n_folds\n",
        "        print(f'[CNN] Fold {fold} done in {time.time()-t0:.1f}s | best_val_acc={best_acc:.4f}')\n",
        "\n",
        "    # Save logits (OOF softmaxed to probs for metrics, but store logits for blending)\n",
        "    # Softmax OOF for quick acc\n",
        "    oof_probs = torch.from_numpy(oof).softmax(dim=1).numpy()\n",
        "    y_all = df_tr['label'].map(CLS2IDX).values\n",
        "    oof_acc = (oof_probs.argmax(1) == y_all).mean()\n",
        "    print(f'[CNN] OOF accuracy (softmaxed): {oof_acc:.4f}')\n",
        "    np.save('oof_cnn_logits.npy', oof)\n",
        "    np.save('test_cnn_logits.npy', test_logits)\n",
        "    print('Saved oof_cnn_logits.npy and test_cnn_logits.npy')\n",
        "\n",
        "train_cnn(seed=42, n_folds=3, epochs=20, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 0 | train 42406 val 21667\n"
          ]
        }
      ]
    },
    {
      "id": "244004b6-f1b3-428a-a530-0035cb0246a2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CNN+Boosters Log-space Blend with Coord Descent, cap weights, optional silence boost\n",
        "import os, json, time, numpy as np, pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "\n",
        "CLASSES = ['yes','no','up','down','left','right','on','off','stop','go','unknown','silence']\n",
        "EPS = 1e-9\n",
        "\n",
        "# Paths for 4-boosters (small-TTA) and CNN logits\n",
        "paths = {\n",
        "    'XGB': {'oof': 'oof_pooled.npy', 'test': 'test_pred_pooled_tta50.npy', 'type': 'prob'},\n",
        "    'LGB_DART': {'oof': 'oof_pooled_lgb_dart.npy', 'test': 'test_pred_pooled_lgb_dart_tta50.npy', 'type': 'prob'},\n",
        "    'XGB2': {'oof': 'oof_pooled_xgb_seed2.npy', 'test': 'test_pred_pooled_xgb_seed2_tta50.npy', 'type': 'prob'},\n",
        "    'CB': {'oof': 'oof_pooled_cat.npy', 'test': 'test_pred_pooled_cat_tta50.npy', 'type': 'prob'},\n",
        "    'CNN': {'oof': 'oof_cnn_logits.npy', 'test': 'test_cnn_logits.npy', 'type': 'logit'},\n",
        "}\n",
        "\n",
        "missing = [k for k,v in paths.items() if not (os.path.exists(v['oof']) and os.path.exists(v['test']))]\n",
        "if missing:\n",
        "    print('Waiting for artifacts to exist (skip execution until ready):', missing)\n",
        "else:\n",
        "    # Load OOF labels\n",
        "    y = np.load('y_train_pooled.npy')\n",
        "    # Load OOF/test for each model\n",
        "    oofs = {}; tests = {}; kinds = {}\n",
        "    for k, v in paths.items():\n",
        "        oo = np.load(v['oof'])\n",
        "        tt = np.load(v['test'])\n",
        "        oofs[k] = oo\n",
        "        tests[k] = tt\n",
        "        kinds[k] = v['type']\n",
        "    models = list(oofs.keys())\n",
        "    print('Models in blend:', models)\n",
        "\n",
        "    # Compute per-model OOF accuracy for initialization\n",
        "    accs = {}\n",
        "    for m in models:\n",
        "        if kinds[m] == 'prob':\n",
        "            pred = oofs[m].argmax(1)\n",
        "        else:\n",
        "            pred = torch.from_numpy(oofs[m]).softmax(dim=1).numpy().argmax(1)\n",
        "        accs[m] = float(accuracy_score(y, pred))\n",
        "    print('Per-model OOF acc:', {m: round(a,5) for m,a in accs.items()})\n",
        "\n",
        "    # Helper to get log-probs for a model's OOF or TEST arrays\n",
        "    def to_log_probs(arr, kind):\n",
        "        if kind == 'prob':\n",
        "            return np.log(np.clip(arr, EPS, 1.0))\n",
        "        else:  # logits\n",
        "            t = torch.from_numpy(arr)\n",
        "            lp = torch.log_softmax(t, dim=1).numpy()\n",
        "            return lp\n",
        "\n",
        "    # Initialize weights from power of OOF acc^4, then normalize; cap later during coord descent\n",
        "    w = np.array([accs[m]**4 for m in models], dtype=np.float64)\n",
        "    w = w / w.sum()\n",
        "    print('Init weights (power):', dict(zip(models, np.round(w,5))))\n",
        "\n",
        "    # OOF acc from weights using log-space sum\n",
        "    log_oofs = {m: to_log_probs(oofs[m], kinds[m]) for m in models}\n",
        "    def oof_acc_from_w(w_vec):\n",
        "        logits = None\n",
        "        for wi, m in zip(w_vec, models):\n",
        "            lp = log_oofs[m]\n",
        "            logits = wi*lp if logits is None else logits + wi*lp\n",
        "        return float(accuracy_score(y, logits.argmax(1)))\n",
        "\n",
        "    best_w = w.copy()\n",
        "    best_acc = oof_acc_from_w(best_w)\n",
        "    print(f'Init log-space OOF acc: {best_acc:.5f}')\n",
        "\n",
        "    # Coordinate descent: step=0.02, 3 passes, cap any single weight <= 0.35\n",
        "    step = 0.02\n",
        "    cap = 0.35\n",
        "    for p in range(3):\n",
        "        improved = False\n",
        "        for i in range(len(best_w)):\n",
        "            for d in (+step, -step):\n",
        "                w_try = best_w.copy()\n",
        "                w_try[i] = max(0.0, min(cap, w_try[i] + d))\n",
        "                s = w_try.sum()\n",
        "                if s <= 0:\n",
        "                    continue\n",
        "                # renormalize and re-cap\n",
        "                w_try = w_try / s\n",
        "                # if any exceed cap after norm, clip and renorm once\n",
        "                if (w_try > cap).any():\n",
        "                    over = w_try > cap\n",
        "                    extra = (w_try[over] - cap).sum()\n",
        "                    w_try[over] = cap\n",
        "                    remain_mask = ~over\n",
        "                    if remain_mask.any():\n",
        "                        w_try[remain_mask] += extra * (w_try[remain_mask] / w_try[remain_mask].sum())\n",
        "                    else:\n",
        "                        # if all capped, just renorm to sum 1\n",
        "                        w_try = w_try / w_try.sum()\n",
        "                acc = oof_acc_from_w(w_try)\n",
        "                if acc > best_acc + 1e-9:\n",
        "                    best_acc = acc; best_w = w_try; improved = True\n",
        "        print(f'Pass {p}: best OOF acc {best_acc:.5f} | weights {dict(zip(models, np.round(best_w,5)))}')\n",
        "        if not improved:\n",
        "            break\n",
        "\n",
        "    # Build blended test logits\n",
        "    log_tests = {m: to_log_probs(tests[m], kinds[m]) for m in models}\n",
        "    test_logits = None\n",
        "    for wi, m in zip(best_w, models):\n",
        "        lp = log_tests[m]\n",
        "        test_logits = wi*lp if test_logits is None else test_logits + wi*lp\n",
        "\n",
        "    # Convert to probs\n",
        "    probs = np.exp(test_logits - test_logits.max(axis=1, keepdims=True))\n",
        "    probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "\n",
        "    # Create submissions: base and silence x1.04\n",
        "    test_fnames_path = 'test_fnames_pooled_small.csv' if os.path.exists('test_fnames_pooled_small.csv') else 'test_fnames_pooled.csv'\n",
        "    test_fnames = pd.read_csv(test_fnames_path, header=None)[0].values\n",
        "    pred_idx_base = probs.argmax(1)\n",
        "    labels_base = [CLASSES[i] for i in pred_idx_base]\n",
        "    pred_df_base = pd.DataFrame({'fname': test_fnames, 'label': labels_base})\n",
        "    sample_sub = pd.read_csv('sample_submission.csv')\n",
        "    sub_base = sample_sub[['fname']].merge(pred_df_base, on='fname', how='left')\n",
        "    assert sub_base['label'].notna().all(), 'Missing predictions (base)'\n",
        "    sub_base.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (base blend):', sub_base.shape, '| OOF acc:', f'{best_acc:.5f}')\n",
        "\n",
        "    # Silence boost variant\n",
        "    sil_idx = CLASSES.index('silence')\n",
        "    probs_boost = probs.copy()\n",
        "    probs_boost[:, sil_idx] *= 1.04\n",
        "    probs_boost /= probs_boost.sum(axis=1, keepdims=True)\n",
        "    pred_idx_b = probs_boost.argmax(1)\n",
        "    labels_b = [CLASSES[i] for i in pred_idx_b]\n",
        "    pred_df_b = pd.DataFrame({'fname': test_fnames, 'label': labels_b})\n",
        "    sub_b = sample_sub[['fname']].merge(pred_df_b, on='fname', how='left')\n",
        "    assert sub_b['label'].notna().all(), 'Missing predictions (silence boost)'\n",
        "    sub_b.to_csv('submission_silence104.csv', index=False)\n",
        "    print('Saved submission_silence104.csv:', sub_b.shape)\n",
        "\n",
        "    # Persist weights\n",
        "    with open('blend_weights_cnn_boosters.json', 'w') as f:\n",
        "        json.dump({'models': models, 'weights': best_w.tolist(), 'oof_acc': best_acc}, f, indent=2)\n",
        "    print('Saved blend_weights_cnn_boosters.json')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}