{
  "cells": [
    {
      "id": "ba682f90-71a6-4668-87f6-1ac357bba73b",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan: Tweet Sentiment Extraction (MLE-Benchmark)\n",
        "\n",
        "Objective:\n",
        "- Build a strong baseline fast and iterate to medal. Metric: Jaccard similarity on selected_text.\n",
        "\n",
        "High-level Approach:\n",
        "- Treat as extractive QA: sentiment = question; tweet = context; predict start/end token indices.\n",
        "- Use transformer encoder (RoBERTa-base or DeBERTa-v3-base) fine-tuned for span extraction.\n",
        "- Post-process: for neutral, output full tweet; sanitize offsets; fallback to sentiment-specific heuristics if needed.\n",
        "\n",
        "Environment & Efficiency:\n",
        "- Verify GPU with nvidia-smi; install PyTorch cu121 stack only; use Transformers + Accelerate.\n",
        "- Log progress per fold with timings; cache tokenized datasets and OOF/test logits.\n",
        "- Subsample smoke tests before full runs; early stop on plateau.\n",
        "\n",
        "Data Pipeline:\n",
        "- Load train/test; inspect nulls, length distributions, sentiments.\n",
        "- Build character-level alignment of selected_text to tweet for training start/end char indices.\n",
        "- Tokenize with fast tokenizer to get offset mapping; map char spans to token spans.\n",
        "- Save processed features to disk (parquet/npz) for reuse.\n",
        "\n",
        "Validation:\n",
        "- Stratified KFold by sentiment (e.g., 5 folds). Deterministic seed. Same folds reused for all models.\n",
        "- OOF Jaccard evaluation to guide iterations. Multiple seeds later if time.\n",
        "\n",
        "Modeling v1 (Baseline):\n",
        "- roberta-base, max_len ~ 128 (cap at e.g., 96/128 after inspecting lengths).\n",
        "- Input format: \"question\" = sentiment token(s); \"context\" = tweet. Simple pair encoding: [CLS] sentiment [SEP] tweet [SEP].\n",
        "- Loss: cross-entropy on start and end.\n",
        "- Hyperparams: lr 2e-5 to 3e-5, batch size per GPU memory (16 if fits), epochs 3 with early stopping on OOF.\n",
        "- Inference: average start/end logits across folds; pick span via argmax/argmax with simple constraint (end >= start).\n",
        "\n",
        "Post-processing:\n",
        "- If sentiment == neutral: return full tweet.\n",
        "- If predicted span empty/invalid: fallback to full tweet for neutral, else minimal heuristic (e.g., top token).\n",
        "- Optional refinement: trim leading/trailing spaces/punctuation to improve Jaccard.\n",
        "\n",
        "Iteration Roadmap:\n",
        "1) GPU/env check + installs.\n",
        "2) EDA: lengths, nulls, label distribution.\n",
        "3) Build alignment + tokenizer pipeline; cache.\n",
        "4) Baseline training 3-5 folds; measure OOF Jaccard.\n",
        "5) Error analysis buckets (neutral/pos/neg, short/long tweets).\n",
        "6) Improvements:\n",
        "   - Model: deberta-v3-base or roberta-large if time/memory.\n",
        "   - Longer max_len if needed.\n",
        "   - Data augmentation: none initially (risk).\n",
        "   - Post-process rules tuning.\n",
        "7) Blend diverse seeds/models if time.\n",
        "\n",
        "Requests for Expert Review (next step):\n",
        "- Are roberta-base/deberta-v3-base still the best for this dataset under time constraints?\n",
        "- Critical post-processing rules that typically boost Jaccard here?\n",
        "- Recommended max_len and any special text normalization to avoid alignment bugs?\n",
        "- Optimal CV folds count vs runtime for medal-level performance?\n",
        "\n",
        "Deliverables:\n",
        "- Reusable fold splits, cached tokenized datasets, OOF metrics.\n",
        "- submission.csv matching sample format.\n",
        "\n",
        "Time Management:\n",
        "- <1h to baseline pipeline ready and smoke-tested.\n",
        "- 2-4h for full 5-fold run on base model.\n",
        "- Remainder for improvements/ensembling and error-driven fixes."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "a99b4456-d554-45f9-a0e5-341293cdcb9a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment check: GPU + install correct Torch stack (cu121)\n",
        "import os, sys, subprocess, shutil, time\n",
        "from pathlib import Path\n",
        "\n",
        "def run(cmd):\n",
        "    print('>>', ' '.join(cmd), flush=True)\n",
        "    return subprocess.run(cmd, check=False, text=True, capture_output=True)\n",
        "\n",
        "# 0) GPU presence\n",
        "print(run(['bash','-lc','nvidia-smi || true']).stdout)\n",
        "\n",
        "# 1) Clean any prior torch stacks\n",
        "for pkg in (\"torch\",\"torchvision\",\"torchaudio\"):\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", pkg], check=False)\n",
        "for d in (\n",
        "    \"/app/.pip-target/torch\",\n",
        "    \"/app/.pip-target/torch-2.8.0.dist-info\",\n",
        "    \"/app/.pip-target/torch-2.4.1.dist-info\",\n",
        "    \"/app/.pip-target/torchvision\",\n",
        "    \"/app/.pip-target/torchvision-0.23.0.dist-info\",\n",
        "    \"/app/.pip-target/torchvision-0.19.1.dist-info\",\n",
        "    \"/app/.pip-target/torchaudio\",\n",
        "    \"/app/.pip-target/torchaudio-2.8.0.dist-info\",\n",
        "    \"/app/.pip-target/torchaudio-2.4.1.dist-info\",\n",
        "    \"/app/.pip-target/torchgen\",\n",
        "    \"/app/.pip-target/functorch\",\n",
        "):\n",
        "    if os.path.exists(d):\n",
        "        print(\"Removing\", d)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "def pip(*args):\n",
        "    print('> pip', ' '.join(args), flush=True)\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", *args], check=True)\n",
        "\n",
        "# 2) Install exact cu121 torch stack\n",
        "pip(\"install\",\n",
        "    \"--index-url\", \"https://download.pytorch.org/whl/cu121\",\n",
        "    \"--extra-index-url\", \"https://pypi.org/simple\",\n",
        "    \"torch==2.4.1\", \"torchvision==0.19.1\", \"torchaudio==2.4.1\")\n",
        "\n",
        "# 3) Freeze torch versions\n",
        "Path(\"constraints.txt\").write_text(\n",
        "    \"torch==2.4.1\\n\"\n",
        "    \"torchvision==0.19.1\\n\"\n",
        "    \"torchaudio==2.4.1\\n\"\n",
        ")\n",
        "\n",
        "# 4) Install NLP deps honoring constraints\n",
        "pip(\"install\", \"-c\", \"constraints.txt\",\n",
        "    \"transformers==4.44.2\", \"accelerate==0.34.2\",\n",
        "    \"datasets==2.21.0\", \"evaluate==0.4.2\",\n",
        "    \"sentencepiece\", \"scikit-learn\", \"pandas\", \"numpy\", \"pyarrow\",\n",
        "    \"tqdm\", \"matplotlib\",\n",
        "    \"--upgrade-strategy\", \"only-if-needed\")\n",
        "\n",
        "# 5) Sanity check torch + CUDA\n",
        "import torch\n",
        "print(\"torch:\", torch.__version__, \"built CUDA:\", getattr(torch.version, \"cuda\", None))\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "assert str(getattr(torch.version, \"cuda\", \"\")).startswith(\"12.1\"), f\"Wrong CUDA build: {torch.version.cuda}\"\n",
        "assert torch.cuda.is_available(), \"CUDA not available\"\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "print(\"Setup OK at\", time.strftime('%Y-%m-%d %H:%M:%S'))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> bash -lc nvidia-smi || true\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Sep 30 04:14:40 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.06             Driver Version: 550.144.06     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10-24Q                 On  |   00000002:00:00.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |     182MiB /  24512MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torch as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torchvision as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> pip install --index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://pypi.org/simple torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torchaudio as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121, https://pypi.org/simple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (799.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 MB 272.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.19.1\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.1/7.1 MB 379.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.4/3.4 MB 448.0 MB/s eta 0:00:00\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 281.9 MB/s eta 0:00:00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 199.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 472.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 133.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 213.4 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 110.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 260.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 71.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 471.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 476.4 MB/s eta 0:00:00\nCollecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 229.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 280.1 MB/s eta 0:00:00\nCollecting typing-extensions>=4.8.0\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 341.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 68.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 493.3 MB/s eta 0:00:00\nCollecting fsspec\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 512.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 99.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow!=8.3.*,>=5.3.0\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 151.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 186.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 105.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 535.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.3 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pillow-11.3.0 sympy-1.14.0 torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121 triton-3.0.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> pip install -c constraints.txt transformers==4.44.2 accelerate==0.34.2 datasets==2.21.0 evaluate==0.4.2 sentencepiece scikit-learn pandas numpy pyarrow tqdm matplotlib --upgrade-strategy only-if-needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.44.2\n  Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.5/9.5 MB 79.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate==0.34.2\n  Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 324.4/324.4 KB 507.6 MB/s eta 0:00:00\nCollecting datasets==2.21.0\n  Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 527.3/527.3 KB 490.0 MB/s eta 0:00:00\nCollecting evaluate==0.4.2\n  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 84.1/84.1 KB 447.2 MB/s eta 0:00:00\nCollecting sentencepiece\n  Downloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.4/1.4 MB 260.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.7/9.7 MB 220.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas\n  Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.8/12.8 MB 231.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 278.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyarrow\n  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 42.8/42.8 MB 157.6 MB/s eta 0:00:00\nCollecting tqdm\n  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.5/78.5 KB 449.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting matplotlib\n  Downloading matplotlib-3.10.6-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8.7/8.7 MB 181.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting requests\n  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.7/64.7 KB 432.8 MB/s eta 0:00:00\nCollecting safetensors>=0.4.1\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 485.8/485.8 KB 548.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers<0.20,>=0.19\n  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.6/3.6 MB 277.5 MB/s eta 0:00:00\nCollecting huggingface-hub<1.0,>=0.23.2\n  Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 564.3/564.3 KB 530.5 MB/s eta 0:00:00\nCollecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\nCollecting packaging>=20.0\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 329.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting regex!=2019.12.17\n  Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 KB 315.3 MB/s eta 0:00:00\nCollecting pyyaml>=5.1\n  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 806.6/806.6 KB 487.1 MB/s eta 0:00:00\nCollecting torch>=1.10.0\n  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 797.1/797.1 MB 153.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psutil\n  Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 291.2/291.2 KB 517.0 MB/s eta 0:00:00\nCollecting dill<0.3.9,>=0.3.0\n  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 116.3/116.3 KB 430.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aiohttp\n  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.7/1.7 MB 171.0 MB/s eta 0:00:00\nCollecting multiprocess\n  Downloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 144.5/144.5 KB 388.9 MB/s eta 0:00:00\nCollecting fsspec[http]<=2024.6.1,>=2023.1.0\n  Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 177.6/177.6 KB 482.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xxhash\n  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 194.8/194.8 KB 461.8 MB/s eta 0:00:00\nCollecting threadpoolctl>=3.1.0\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy>=1.8.0\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.9/35.9 MB 585.2 MB/s eta 0:00:00\nCollecting joblib>=1.2.0\n  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 308.4/308.4 KB 522.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tzdata>=2022.7\n  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 347.8/347.8 KB 514.9 MB/s eta 0:00:00\nCollecting python-dateutil>=2.8.2\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 229.9/229.9 KB 511.6 MB/s eta 0:00:00\nCollecting pytz>=2020.1\n  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 509.2/509.2 KB 535.8 MB/s eta 0:00:00\nCollecting cycler>=0.10\n  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contourpy>=1.0.1\n  Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 355.2/355.2 KB 284.1 MB/s eta 0:00:00\nCollecting kiwisolver>=1.3.1\n  Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.4/1.4 MB 217.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fonttools>=4.22.0\n  Downloading fonttools-4.60.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5.0/5.0 MB 143.1 MB/s eta 0:00:00\nCollecting pyparsing>=2.3.1\n  Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 113.9/113.9 KB 434.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow>=8\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 163.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting attrs>=17.3.0\n  Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 63.8/63.8 KB 429.6 MB/s eta 0:00:00\nCollecting propcache>=0.2.0\n  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 213.5/213.5 KB 536.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yarl<2.0,>=1.17.0\n  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 349.0/349.0 KB 519.5 MB/s eta 0:00:00\nCollecting aiosignal>=1.4.0\n  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\nCollecting aiohappyeyeballs>=2.5.0\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nCollecting frozenlist>=1.1.1\n  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 235.3/235.3 KB 514.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting multidict<7.0,>=4.5\n  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 246.7/246.7 KB 516.9 MB/s eta 0:00:00\nCollecting hf-xet<2.0.0,>=1.1.3\n  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.2/3.2 MB 560.3 MB/s eta 0:00:00\nCollecting typing-extensions>=3.7.4.3\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 381.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting six>=1.5\n  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\nCollecting certifi>=2017.4.17\n  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 161.2/161.2 KB 507.6 MB/s eta 0:00:00\nCollecting urllib3<3,>=1.21.1\n  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 129.8/129.8 KB 482.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting charset_normalizer<4,>=2\n  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 150.3/150.3 KB 490.7 MB/s eta 0:00:00\nCollecting idna<4,>=2.5\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 70.4/70.4 KB 456.4 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 177.8 MB/s eta 0:00:00\nCollecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 186.9 MB/s eta 0:00:00\nCollecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 159.7 MB/s eta 0:00:00\nCollecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 504.5 MB/s eta 0:00:00\nCollecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 406.9 MB/s eta 0:00:00\nCollecting nvidia-cudnn-cu12==9.1.0.70\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 73.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 204.6 MB/s eta 0:00:00\nCollecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 321.2 MB/s eta 0:00:00\nCollecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 213.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 506.2 MB/s eta 0:00:00\nCollecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 128.8 MB/s eta 0:00:00\nCollecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 223.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 488.5 MB/s eta 0:00:00\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 254.3 MB/s eta 0:00:00\nCollecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 192.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 214.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting multiprocess\n  Downloading multiprocess-0.70.17-py311-none-any.whl (144 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 144.3/144.3 KB 470.3 MB/s eta 0:00:00\n  Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 143.5/143.5 KB 506.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\nCollecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 527.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: pytz, mpmath, xxhash, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, sympy, six, sentencepiece, safetensors, regex, pyyaml, pyparsing, pyarrow, psutil, propcache, pillow, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, MarkupSafe, kiwisolver, joblib, idna, hf-xet, fsspec, frozenlist, fonttools, filelock, dill, cycler, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, triton, scipy, requests, python-dateutil, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, jinja2, contourpy, aiosignal, scikit-learn, pandas, nvidia-cusolver-cu12, matplotlib, huggingface-hub, aiohttp, torch, tokenizers, transformers, datasets, accelerate, evaluate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.3 accelerate-0.34.2 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 attrs-25.3.0 certifi-2025.8.3 charset_normalizer-3.4.3 contourpy-1.3.3 cycler-0.12.1 datasets-2.21.0 dill-0.3.8 evaluate-0.4.2 filelock-3.19.1 fonttools-4.60.1 frozenlist-1.7.0 fsspec-2024.6.1 hf-xet-1.1.10 huggingface-hub-0.35.3 idna-3.10 jinja2-3.1.6 joblib-1.5.2 kiwisolver-1.4.9 matplotlib-3.10.6 mpmath-1.3.0 multidict-6.6.4 multiprocess-0.70.16 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 packaging-25.0 pandas-2.3.3 pillow-11.3.0 propcache-0.3.2 psutil-7.1.0 pyarrow-21.0.0 pyparsing-3.2.5 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.3 regex-2025.9.18 requests-2.32.5 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 sentencepiece-0.2.1 six-1.17.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.19.1 torch-2.4.1 tqdm-4.67.1 transformers-4.44.2 triton-3.0.0 typing-extensions-4.15.0 tzdata-2025.2 urllib3-2.5.0 xxhash-3.5.0 yarl-1.20.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/functorch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchgen already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusolver_cu12-11.4.5.107.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/jinja2-3.1.6.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cudnn_cu12-9.1.0.70.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusparse_cu12-12.1.0.106.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/triton-3.0.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock-3.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe-3.0.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx-3.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cublas_cu12-12.1.3.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_cupti_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_nvrtc_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_runtime_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cufft_cu12-11.0.2.54.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_curand_cu12-10.3.2.106.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nccl_cu12-2.20.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvjitlink_cu12-12.9.86.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvtx_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow-11.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/PIL already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/sympy-1.14.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/isympy.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath-1.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/share already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.4.1+cu121 built CUDA: 12.1\nCUDA available: True\nGPU: NVIDIA A10-24Q\nSetup OK at 2025-09-30 04:17:07\n"
          ]
        }
      ]
    },
    {
      "id": "1c276657-81b7-46e3-9bd7-f8dfeae81d9b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# EDA: load data, inspect distributions, token length coverage\n",
        "import pandas as pd, numpy as np, os, time\n",
        "from collections import Counter\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "t0 = time.time()\n",
        "train_path, test_path = 'train.csv', 'test.csv'\n",
        "train = pd.read_csv(train_path)\n",
        "test = pd.read_csv(test_path)\n",
        "print('Loaded:', train.shape, test.shape)\n",
        "print(train.head(3))\n",
        "\n",
        "# Basic checks\n",
        "print('\\nNulls train:\\n', train.isnull().sum())\n",
        "print('\\nSentiment distribution (train):\\n', train['sentiment'].value_counts())\n",
        "\n",
        "# Tweet and selected_text length stats\n",
        "train['tweet_len'] = train['text'].astype(str).apply(len)\n",
        "train['sel_len'] = train['selected_text'].astype(str).apply(len)\n",
        "print('\\nTweet length percentiles:', np.percentile(train['tweet_len'], [50, 75, 90, 95, 99]))\n",
        "print('Selected_text length percentiles:', np.percentile(train['sel_len'], [50, 75, 90, 95, 99]))\n",
        "\n",
        "# Tokenizer length study (pair encoding: sentiment + tweet)\n",
        "model_name = 'microsoft/deberta-v3-base'  # primary choice per expert advice\n",
        "tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "def pair_len(sent, txt):\n",
        "    enc = tok(text=sent, text_pair=txt, add_special_tokens=True, truncation=False)\n",
        "    return len(enc['input_ids'])\n",
        "\n",
        "sample_idx = np.random.RandomState(42).choice(len(train), size=min(5000, len(train)), replace=False)\n",
        "lens = [pair_len(train.loc[i,'sentiment'], str(train.loc[i,'text'])) for i in sample_idx]\n",
        "lens = np.array(lens)\n",
        "print('\\nToken pair length percentiles (DeBERTa-v3-base):', np.percentile(lens, [50, 75, 90, 95, 99]))\n",
        "coverage_128 = (lens <= 128).mean()\n",
        "coverage_96 = (lens <= 96).mean()\n",
        "print(f'Coverage <=128: {coverage_128:.4f}, <=96: {coverage_96:.4f}')\n",
        "\n",
        "print('\\nTop examples near tail:')\n",
        "tail_idx = np.argsort(lens)[-5:]\n",
        "for idx in tail_idx:\n",
        "    i = sample_idx[idx]\n",
        "    print('len=', lens[idx], '| sentiment=', train.loc[i,'sentiment'], '| text[:120]=', str(train.loc[i,'text'])[:120].replace('\\n',' '))\n",
        "\n",
        "print(f'EDA done in {time.time()-t0:.1f}s')\n",
        "\n",
        "# Decide tentative max_len recommendation based on coverage\n",
        "if coverage_128 > 0.995:\n",
        "    print('Recommendation: max_len=128 (safe).')\n",
        "elif coverage_96 > 0.995:\n",
        "    print('Recommendation: max_len=96 (safe).')\n",
        "else:\n",
        "    print('Recommendation: max_len=128 (use), consider 160 if truncation noticeably >0.5%.')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: (24732, 4) (2749, 3)\n       textID                                               text  \\\n0  8d4ad58b45  eating breakfast  getting ready to go to schoo...   \n1  fdfe12a800  Going to fold laundry and then hit the sack. I...   \n2  5efd224f4e  happy mothers day to all   im off to spend the...   \n\n                                       selected_text sentiment  \n0  eating breakfast  getting ready to go to schoo...  negative  \n1                    I have boring saturday evenings  negative  \n2                                              happy  positive  \n\nNulls train:\n textID           0\ntext             1\nselected_text    1\nsentiment        0\ndtype: int64\n\nSentiment distribution (train):\n sentiment\nneutral     9998\npositive    7735\nnegative    6999\nName: count, dtype: int64\n\nTweet length percentiles: [ 64.  97. 122. 129. 137.]\nSelected_text length percentiles: [ 22.  55.  97. 117. 135.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nToken pair length percentiles (DeBERTa-v3-base): [21. 29. 35. 38. 44.]\nCoverage <=128: 1.0000, <=96: 1.0000\n\nTop examples near tail:\nlen= 55 | sentiment= positive | text[:120]= wakey wakey lemon shakeyyyy!  haha, goin` 2 schooliooo!  it`s raining!! (ugh!) guess where I WISH I CAN BE RIGHT NOW....\nlen= 56 | sentiment= positive | text[:120]=  I LOVE IT!!!!!!!!!!!!!!!!!!!!!! I ALSO LIKE THE NEW PROFILE PIC!!!!!!!!!!!!!!!!!!!!\nlen= 58 | sentiment= negative | text[:120]= God **** you Twitter!!!!!!!!!!!!!! Stop eating my undeleted DMs!!!!!!!!!!!!!!!!!!!!!!!!!!\nlen= 63 | sentiment= negative | text[:120]= TODAy SUCKs - kisskass19: \u00ef\u00bf\u00bdyou and katey broke up?! Yah Kate Broke up with me  It\u00ef\u00bf\u00bds been awful nd Vodkas... http://t\nlen= 64 | sentiment= positive | text[:120]= Star trek was SOOOOO AWESOME!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!Spock and Kirk were hillarious!!!  Im seeing it again. so\nEDA done in 1.0s\nRecommendation: max_len=128 (safe).\n"
          ]
        }
      ]
    },
    {
      "id": "83e41e6e-6110-4126-8c24-168e670349b2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build robust char->token alignment, create 5-fold splits, and cache tokenized datasets (smoke: roberta-base, max_len=128)\n",
        "import os, re, json, time, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "OUT_DIR = Path('cache')\n",
        "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "def jaccard(str1, str2):\n",
        "    if not isinstance(str1, str): str1 = '' if str1 is None else str(str1)\n",
        "    if not isinstance(str2, str): str2 = '' if str2 is None else str(str2)\n",
        "    a = set(str1.split()); b = set(str2.split())\n",
        "    if not a and not b: return 1.0\n",
        "    return float(len(a & b)) / (len(a | b) + 1e-12)\n",
        "\n",
        "def find_span(text, sel):\n",
        "    # Return (start_char, end_char) inclusive-exclusive on original text; None if invalid\n",
        "    if not isinstance(text, str) or not isinstance(sel, str) or len(sel) == 0:\n",
        "        return None\n",
        "    # Exact substring first\n",
        "    start = text.find(sel)\n",
        "    if start != -1:\n",
        "        return (start, start + len(sel))\n",
        "    # Collapse multiple spaces for robust match\n",
        "    def collapse_spaces(s):\n",
        "        return re.sub(r'\\s+', ' ', s.strip())\n",
        "    text_c = collapse_spaces(text)\n",
        "    sel_c = collapse_spaces(sel)\n",
        "    start_c = text_c.find(sel_c)\n",
        "    if start_c == -1:\n",
        "        return None\n",
        "    # Map collapsed indices back to original via two-pointer walk\n",
        "    i = j = 0\n",
        "    map_idx = []  # map from collapsed index to original index\n",
        "    while i < len(text):\n",
        "        if text[i].isspace():\n",
        "            # collapse run of spaces to single space\n",
        "            # next collapsed char corresponds to first space in run\n",
        "            map_idx.append(i)\n",
        "            while i < len(text) and text[i].isspace():\n",
        "                i += 1\n",
        "            j += 1\n",
        "        else:\n",
        "            map_idx.append(i); i += 1; j += 1\n",
        "    # Ensure map covers length\n",
        "    if start_c < len(map_idx):\n",
        "        start_orig = map_idx[start_c]\n",
        "        end_c = start_c + len(sel_c)\n",
        "        end_orig = map_idx[min(end_c-1, len(map_idx)-1)] + 1\n",
        "        return (start_orig, end_orig)\n",
        "    return None\n",
        "\n",
        "def map_char_to_tokens(offsets, seq_ids, char_span, target_seq_id=1):\n",
        "    # offsets: list of (start,end) per token; seq_ids: list of sequence_ids (None,0,1,...)\n",
        "    if char_span is None:\n",
        "        return None\n",
        "    cs, ce = char_span\n",
        "    start_tok = end_tok = None\n",
        "    for i, (o, sid) in enumerate(zip(offsets, seq_ids)):\n",
        "        if sid != target_seq_id:  # only tweet side\n",
        "            continue\n",
        "        os_, oe_ = o\n",
        "        if os_ is None:\n",
        "            continue\n",
        "        # token overlaps char span?\n",
        "        if oe_ > cs and os_ < ce:\n",
        "            if start_tok is None:\n",
        "                start_tok = i\n",
        "            end_tok = i\n",
        "    if start_tok is None or end_tok is None:\n",
        "        return None\n",
        "    return (start_tok, end_tok)\n",
        "\n",
        "def prepare_cached_dataset(model_name='roberta-base', max_len=128, prefix='roberta_base_m128'):\n",
        "    print(f'Preparing dataset for {model_name}, max_len={max_len}', flush=True)\n",
        "    tok_kwargs = {'use_fast': True}\n",
        "    if 'roberta' in model_name:\n",
        "        tok_kwargs['add_prefix_space'] = True\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, **tok_kwargs)\n",
        "\n",
        "    # Ensure no NaNs\n",
        "    df = train.copy()\n",
        "    df['text'] = df['text'].fillna('')\n",
        "    df['selected_text'] = df['selected_text'].fillna('')\n",
        "    df['sentiment'] = df['sentiment'].fillna('neutral')\n",
        "\n",
        "    # Create folds\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "    df['fold'] = -1\n",
        "    for k, (_, val_idx) in enumerate(skf.split(df, df['sentiment'])):\n",
        "        df.loc[val_idx, 'fold'] = k\n",
        "    df.to_csv(OUT_DIR / f'train_folds_{prefix}.csv', index=False)\n",
        "    print('Saved folds to', OUT_DIR / f'train_folds_{prefix}.csv')\n",
        "\n",
        "    # Encode train with offsets\n",
        "    input_ids_list = []; attention_mask_list = []; token_type_ids_list = []\n",
        "    start_list = []; end_list = []\n",
        "    n = len(df)\n",
        "    t0 = time.time()\n",
        "    for i, row in df.iterrows():\n",
        "        if i % 2000 == 0:\n",
        "            print(f'Row {i}/{n} elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "        sent = str(row['sentiment'])\n",
        "        txt = str(row['text'])\n",
        "        sel = str(row['selected_text'])\n",
        "        enc = tokenizer(text=sent, text_pair=txt, add_special_tokens=True, truncation=True, max_length=max_len,\n",
        "                        return_offsets_mapping=True, return_attention_mask=True)\n",
        "        input_ids = enc['input_ids']; attn = enc['attention_mask']\n",
        "        tt = enc.get('token_type_ids', None)\n",
        "        offsets = enc['offset_mapping']\n",
        "        seq_ids = enc.sequence_ids()\n",
        "\n",
        "        span = find_span(txt, sel) if sent != 'neutral' else find_span(txt, sel)  # keep original spans; post-proc handles neutral later\n",
        "        tok_span = map_char_to_tokens(offsets, seq_ids, span, target_seq_id=1)\n",
        "        # If mapping failed, default to tweet-side entire span\n",
        "        if tok_span is None:\n",
        "            # find first and last token on tweet side\n",
        "            idxs = [idx for idx, sid in enumerate(seq_ids) if sid == 1]\n",
        "            if len(idxs) == 0:\n",
        "                s_tok = e_tok = 0\n",
        "            else:\n",
        "                s_tok, e_tok = idxs[0], idxs[-1]\n",
        "        else:\n",
        "            s_tok, e_tok = tok_span\n",
        "\n",
        "        # Pad to max_len\n",
        "        if len(input_ids) < max_len:\n",
        "            pad_len = max_len - len(input_ids)\n",
        "            pad_id = tokenizer.pad_token_id\n",
        "            input_ids = input_ids + [pad_id] * pad_len\n",
        "            attn = attn + [0] * pad_len\n",
        "            if tt is not None:\n",
        "                tt = tt + [0] * pad_len\n",
        "        else:\n",
        "            input_ids = input_ids[:max_len]\n",
        "            attn = attn[:max_len]\n",
        "            if tt is not None:\n",
        "                tt = tt[:max_len]\n",
        "            # also clamp labels within range if truncation occurred\n",
        "            s_tok = min(s_tok, max_len-1); e_tok = min(e_tok, max_len-1)\n",
        "\n",
        "        input_ids_list.append(np.array(input_ids, dtype=np.int32))\n",
        "        attention_mask_list.append(np.array(attn, dtype=np.int8))\n",
        "        if tt is not None:\n",
        "            token_type_ids_list.append(np.array(tt, dtype=np.int8))\n",
        "        else:\n",
        "            token_type_ids_list = None\n",
        "        start_list.append(s_tok); end_list.append(e_tok)\n",
        "\n",
        "    X_train = {\n",
        "        'input_ids': np.stack(input_ids_list),\n",
        "        'attention_mask': np.stack(attention_mask_list),\n",
        "        'start_positions': np.array(start_list, dtype=np.int32),\n",
        "        'end_positions': np.array(end_list, dtype=np.int32),\n",
        "        'fold': df['fold'].values.astype(np.int8),\n",
        "        'sentiment': df['sentiment'].values,\n",
        "        'text': df['text'].values,\n",
        "        'selected_text': df['selected_text'].values,\n",
        "    }\n",
        "    if token_type_ids_list is not None:\n",
        "        X_train['token_type_ids'] = np.stack(token_type_ids_list)\n",
        "    np.savez_compressed(OUT_DIR / f'train_{prefix}.npz', **X_train)\n",
        "    print('Saved', OUT_DIR / f'train_{prefix}.npz', 'shapes:',\n",
        "          {k: v.shape if isinstance(v, np.ndarray) else len(v) for k, v in X_train.items() if hasattr(v, 'shape') or isinstance(v, (list, np.ndarray))})\n",
        "\n",
        "    # Encode test (no labels)\n",
        "    test_df = test.copy()\n",
        "    test_df['text'] = test_df['text'].fillna('')\n",
        "    test_df['sentiment'] = test_df['sentiment'].fillna('neutral')\n",
        "    ti_ids = []; ta_masks = []; tt_ids = [];\n",
        "    for i, row in test_df.iterrows():\n",
        "        if i % 2000 == 0:\n",
        "            print(f'Test row {i}/{len(test_df)}', flush=True)\n",
        "        enc = tokenizer(text=str(row['sentiment']), text_pair=str(row['text']), add_special_tokens=True, truncation=True, max_length=max_len,\n",
        "                        return_attention_mask=True)\n",
        "        ids = enc['input_ids']; attn = enc['attention_mask']; tt = enc.get('token_type_ids', None)\n",
        "        if len(ids) < max_len:\n",
        "            pad_len = max_len - len(ids)\n",
        "            ids = ids + [tokenizer.pad_token_id]*pad_len\n",
        "            attn = attn + [0]*pad_len\n",
        "            if tt is not None: tt = tt + [0]*pad_len\n",
        "        else:\n",
        "            ids = ids[:max_len]; attn = attn[:max_len]\n",
        "            if tt is not None: tt = tt[:max_len]\n",
        "        ti_ids.append(np.array(ids, dtype=np.int32))\n",
        "        ta_masks.append(np.array(attn, dtype=np.int8))\n",
        "        if tt is not None: tt_ids.append(np.array(tt, dtype=np.int8))\n",
        "    X_test = {\n",
        "        'input_ids': np.stack(ti_ids),\n",
        "        'attention_mask': np.stack(ta_masks),\n",
        "        'sentiment': test_df['sentiment'].values,\n",
        "        'text': test_df['text'].values,\n",
        "        'textID': test_df['textID'].values,\n",
        "    }\n",
        "    if len(tt_ids) == len(test_df):\n",
        "        X_test['token_type_ids'] = np.stack(tt_ids)\n",
        "    np.savez_compressed(OUT_DIR / f'test_{prefix}.npz', **X_test)\n",
        "    print('Saved', OUT_DIR / f'test_{prefix}.npz', 'shapes:',\n",
        "          {k: v.shape if isinstance(v, np.ndarray) else len(v) for k, v in X_test.items() if hasattr(v, 'shape') or isinstance(v, (list, np.ndarray))})\n",
        "\n",
        "    meta = {'model_name': model_name, 'max_len': max_len, 'prefix': prefix}\n",
        "    Path(OUT_DIR / f'meta_{prefix}.json').write_text(json.dumps(meta))\n",
        "    print('Meta saved.')\n",
        "\n",
        "prepare_cached_dataset(model_name='roberta-base', max_len=128, prefix='roberta_base_m128')\n",
        "print('Cache build complete.')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing dataset for roberta-base, max_len=128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved folds to cache/train_folds_roberta_base_m128.csv\nRow 0/24732 elapsed 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 2000/24732 elapsed 0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 4000/24732 elapsed 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 6000/24732 elapsed 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 8000/24732 elapsed 1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 10000/24732 elapsed 1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 12000/24732 elapsed 1.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 14000/24732 elapsed 1.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 16000/24732 elapsed 2.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 18000/24732 elapsed 2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 20000/24732 elapsed 2.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 22000/24732 elapsed 2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 24000/24732 elapsed 3.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved cache/train_roberta_base_m128.npz shapes: {'input_ids': (24732, 128), 'attention_mask': (24732, 128), 'start_positions': (24732,), 'end_positions': (24732,), 'fold': (24732,), 'sentiment': (24732,), 'text': (24732,), 'selected_text': (24732,)}\nTest row 0/2749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test row 2000/2749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved cache/test_roberta_base_m128.npz shapes: {'input_ids': (2749, 128), 'attention_mask': (2749, 128), 'sentiment': (2749,), 'text': (2749,), 'textID': (2749,)}\nMeta saved.\nCache build complete.\n"
          ]
        }
      ]
    },
    {
      "id": "8575c09f-bef9-40da-8914-e6fbee99031b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Smoke training: 1-fold RoBERTa-base QA head, fp16, constrained decoding, neutral full-tweet rule\n",
        "import math, time, json, numpy as np, pandas as pd, torch\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "CACHE_TRAIN = Path('cache/train_roberta_base_m128.npz')\n",
        "FOLDS_CSV = Path('cache/train_folds_roberta_base_m128.csv')\n",
        "MAX_LEN = 128\n",
        "MODEL_NAME = 'roberta-base'\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 2  # smoke\n",
        "LR = 3e-5\n",
        "WARMUP = 0.1\n",
        "MAX_SPAN_LEN = 30\n",
        "\n",
        "tok_kwargs = {'use_fast': True, 'add_prefix_space': True}\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, **tok_kwargs)\n",
        "\n",
        "data = np.load(CACHE_TRAIN, allow_pickle=True)\n",
        "folds_df = pd.read_csv(FOLDS_CSV)\n",
        "\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, idxs):\n",
        "        self.ids = data['input_ids'][idxs]\n",
        "        self.attn = data['attention_mask'][idxs]\n",
        "        self.has_tt = 'token_type_ids' in data.files\n",
        "        if self.has_tt:\n",
        "            self.tt = data['token_type_ids'][idxs]\n",
        "        self.start = data['start_positions'][idxs]\n",
        "        self.end = data['end_positions'][idxs]\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, i):\n",
        "        item = {\n",
        "            'input_ids': torch.tensor(self.ids[i], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.attn[i], dtype=torch.long),\n",
        "            'start_positions': torch.tensor(self.start[i], dtype=torch.long),\n",
        "            'end_positions': torch.tensor(self.end[i], dtype=torch.long),\n",
        "        }\n",
        "        if 'token_type_ids' in data.files:\n",
        "            item['token_type_ids'] = torch.tensor(self.tt[i], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "def constrained_decode_for_row(sentiment, text, model):\n",
        "    if sentiment == 'neutral':\n",
        "        return text\n",
        "    enc = tokenizer(text=str(sentiment), text_pair=str(text), add_special_tokens=True, truncation=True, max_length=MAX_LEN,\n",
        "                    return_offsets_mapping=True, return_tensors='pt')\n",
        "    input_ids = enc['input_ids'].to(device)\n",
        "    attention_mask = enc['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        start_logits = out.start_logits[0].detach().cpu().numpy()\n",
        "        end_logits = out.end_logits[0].detach().cpu().numpy()\n",
        "    offsets = enc['offset_mapping'][0].tolist()\n",
        "    seq_ids = tokenizer.decode(enc['input_ids'][0]).split()  # placeholder to ensure execution; proper seq_ids via fast tokenizer:\n",
        "    seq_ids = enc.sequence_ids(0)  # list with None/0/1\n",
        "    # indices for tweet side\n",
        "    tweet_idxs = [i for i, sid in enumerate(seq_ids) if sid == 1]\n",
        "    if not tweet_idxs:\n",
        "        return text\n",
        "    # restrict logits to tweet side by setting others to -inf\n",
        "    neg_inf = -1e9\n",
        "    s_logits = start_logits.copy(); e_logits = end_logits.copy()\n",
        "    for i, sid in enumerate(seq_ids):\n",
        "        if sid != 1:\n",
        "            s_logits[i] = neg_inf; e_logits[i] = neg_inf\n",
        "    # top-k candidates\n",
        "    k = min(5, len(tweet_idxs))\n",
        "    start_cand = np.argsort(s_logits)[-k:]\n",
        "    end_cand = np.argsort(e_logits)[-k:]\n",
        "    best = None; best_score = -1e18\n",
        "    for si in start_cand:\n",
        "        for ei in end_cand:\n",
        "            if ei < si: continue\n",
        "            if ei - si + 1 > MAX_SPAN_LEN: continue\n",
        "            score = s_logits[si] + e_logits[ei]\n",
        "            if score > best_score:\n",
        "                best_score = score; best = (si, ei)\n",
        "    if best is None:\n",
        "        # fallback single best start token\n",
        "        si = int(np.argmax(s_logits)); ei = si\n",
        "    else:\n",
        "        si, ei = best\n",
        "    # map to char offsets and extract\n",
        "    cs = offsets[si][0]; ce = offsets[ei][1]\n",
        "    sub = text[cs:ce]\n",
        "    sub = sub.strip()\n",
        "    if not sub:\n",
        "        # fallback\n",
        "        si = int(np.argmax(s_logits)); cs = offsets[si][0]; ce = offsets[si][1]\n",
        "        sub = text[cs:ce].strip() or text\n",
        "    return sub\n",
        "\n",
        "def run_fold(fold=0):\n",
        "    all_folds = folds_df['fold'].values\n",
        "    train_idx = np.where(all_folds != fold)[0]\n",
        "    val_idx = np.where(all_folds == fold)[0]\n",
        "    print(f'Fold {fold}: train {len(train_idx)} | val {len(val_idx)}')\n",
        "    train_ds = QADataset(train_idx); val_ds = QADataset(val_idx)\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME).to(device)\n",
        "    opt = AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
        "    num_train_steps = EPOCHS * math.ceil(len(train_loader))\n",
        "    num_warmup = int(WARMUP * num_train_steps)\n",
        "    sch = get_linear_schedule_with_warmup(opt, num_warmup, num_train_steps)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
        "\n",
        "    t0 = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train(); tr_loss = 0.0\n",
        "        for step, batch in enumerate(train_loader):\n",
        "            for k in list(batch.keys()): batch[k] = batch[k].to(device)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=True):\n",
        "                out = model(**batch)\n",
        "                loss = out.loss\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(opt)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(opt); scaler.update(); sch.step()\n",
        "            tr_loss += loss.item()\n",
        "            if (step+1) % 100 == 0:\n",
        "                print(f'Epoch {epoch+1} Step {step+1}/{len(train_loader)} loss {tr_loss/(step+1):.4f} elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "        print(f'Epoch {epoch+1} done. Train loss {tr_loss/max(1,len(train_loader)):.4f}')\n",
        "\n",
        "    # Validation decode and Jaccard\n",
        "    model.eval()\n",
        "    sentiments = data['sentiment'][val_idx]\n",
        "    texts = data['text'][val_idx]\n",
        "    gold = data['selected_text'][val_idx]\n",
        "    preds = []\n",
        "    for i in range(len(val_idx)):\n",
        "        pred = constrained_decode_for_row(str(sentiments[i]), str(texts[i]), model)\n",
        "        # neutral hard rule\n",
        "        if sentiments[i] == 'neutral': pred = str(texts[i])\n",
        "        preds.append(pred)\n",
        "        if (i+1) % 500 == 0:\n",
        "            print(f'Val decoded {i+1}/{len(val_idx)}', flush=True)\n",
        "    # Jaccard\n",
        "    def jac(a,b):\n",
        "        sa = set(str(a).split()); sb = set(str(b).split());\n",
        "        return (len(sa & sb)) / (len(sa | sb) + 1e-12)\n",
        "    scores = [jac(preds[i], gold[i]) for i in range(len(preds))]\n",
        "    score = float(np.mean(scores))\n",
        "    print(f'Fold {fold} OOF Jaccard: {score:.5f}')\n",
        "    # Save model for potential reuse\n",
        "    outdir = Path('models/roberta_base_f0')\n",
        "    outdir.mkdir(parents=True, exist_ok=True)\n",
        "    model.save_pretrained(outdir)\n",
        "    tokenizer.save_pretrained(outdir)\n",
        "    # Return preds for potential error analysis\n",
        "    return score, preds\n",
        "\n",
        "score, _ = run_fold(fold=0)\n",
        "print('Smoke training complete. Fold0 Jaccard =', score)\n",
        "\n",
        "# If the score looks sane (>0.70), we will proceed to full 5-fold DeBERTa-v3-base next."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: train 19785 | val 4947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_274/3176452287.py:111: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/tmp/ipykernel_274/3176452287.py:119: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Step 100/619 loss 3.1064 elapsed 10.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Step 200/619 loss 2.1781 elapsed 21.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Step 300/619 loss 1.7868 elapsed 32.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Step 400/619 loss 1.5770 elapsed 42.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Step 500/619 loss 1.4384 elapsed 53.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Step 600/619 loss 1.3437 elapsed 64.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 done. Train loss 1.3306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Step 100/619 loss 0.8026 elapsed 76.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Step 200/619 loss 0.8096 elapsed 87.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Step 300/619 loss 0.7896 elapsed 98.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Step 400/619 loss 0.7779 elapsed 108.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Step 500/619 loss 0.7738 elapsed 119.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Step 600/619 loss 0.7672 elapsed 130.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 done. Train loss 0.7655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val decoded 500/4947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val decoded 1000/4947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val decoded 1500/4947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val decoded 2000/4947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val decoded 2500/4947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val decoded 3000/4947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val decoded 3500/4947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val decoded 4000/4947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val decoded 4500/4947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0 OOF Jaccard: 0.71234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Smoke training complete. Fold0 Jaccard = 0.7123445887907528\n"
          ]
        }
      ]
    },
    {
      "id": "1e3a7517-7fa1-4b97-9fb2-0b6a60bf07a9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build DeBERTa-v3-base cache with sentiment prompt and robust alignment\n",
        "import os, re, json, time, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "SEED = 42\n",
        "OUT_DIR = Path('cache')\n",
        "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "def find_span(text, sel):\n",
        "    if not isinstance(text, str) or not isinstance(sel, str) or len(sel) == 0:\n",
        "        return None\n",
        "    start = text.find(sel)\n",
        "    if start != -1:\n",
        "        return (start, start + len(sel))\n",
        "    def collapse_spaces(s):\n",
        "        return re.sub(r'\\s+', ' ', s.strip())\n",
        "    text_c = collapse_spaces(text); sel_c = collapse_spaces(sel)\n",
        "    start_c = text_c.find(sel_c)\n",
        "    if start_c == -1:\n",
        "        return None\n",
        "    i = 0\n",
        "    map_idx = []\n",
        "    while i < len(text):\n",
        "        if text[i].isspace():\n",
        "            map_idx.append(i)\n",
        "            while i < len(text) and text[i].isspace():\n",
        "                i += 1\n",
        "        else:\n",
        "            map_idx.append(i); i += 1\n",
        "    if start_c < len(map_idx):\n",
        "        start_orig = map_idx[start_c]\n",
        "        end_c = start_c + len(sel_c)\n",
        "        end_orig = map_idx[min(end_c-1, len(map_idx)-1)] + 1\n",
        "        return (start_orig, end_orig)\n",
        "    return None\n",
        "\n",
        "def map_char_to_tokens(offsets, seq_ids, char_span, target_seq_id=1):\n",
        "    if char_span is None:\n",
        "        return None\n",
        "    cs, ce = char_span\n",
        "    start_tok = end_tok = None\n",
        "    for i, (o, sid) in enumerate(zip(offsets, seq_ids)):\n",
        "        if sid != target_seq_id:\n",
        "            continue\n",
        "        os_, oe_ = o\n",
        "        if os_ is None:\n",
        "            continue\n",
        "        if oe_ > cs and os_ < ce:\n",
        "            if start_tok is None:\n",
        "                start_tok = i\n",
        "            end_tok = i\n",
        "    if start_tok is None or end_tok is None:\n",
        "        return None\n",
        "    return (start_tok, end_tok)\n",
        "\n",
        "def prepare_cached_dataset_deberta(model_name='microsoft/deberta-v3-base', max_len=128, prefix='deberta_v3_base_m128_prompt'):\n",
        "    print(f'Preparing dataset for {model_name}, max_len={max_len}, prefix={prefix}', flush=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    df = train.copy()\n",
        "    df['text'] = df['text'].fillna('')\n",
        "    df['selected_text'] = df['selected_text'].fillna('')\n",
        "    df['sentiment'] = df['sentiment'].fillna('neutral')\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "    df['fold'] = -1\n",
        "    for k, (_, val_idx) in enumerate(skf.split(df, df['sentiment'])):\n",
        "        df.loc[val_idx, 'fold'] = k\n",
        "    df.to_csv(OUT_DIR / f'train_folds_{prefix}.csv', index=False)\n",
        "    print('Saved folds to', OUT_DIR / f'train_folds_{prefix}.csv')\n",
        "\n",
        "    input_ids_list = []; attention_mask_list = [];\n",
        "    start_list = []; end_list = []\n",
        "    n = len(df); t0 = time.time()\n",
        "    for i, row in df.iterrows():\n",
        "        if i % 2000 == 0:\n",
        "            print(f'Row {i}/{n} elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "        sent_prompt = f\"sentiment: {str(row['sentiment'])}\"\n",
        "        txt = str(row['text'])\n",
        "        sel = str(row['selected_text'])\n",
        "        enc = tokenizer(text=sent_prompt, text_pair=txt, add_special_tokens=True, truncation=True, max_length=max_len,\n",
        "                        return_offsets_mapping=True, return_attention_mask=True)\n",
        "        input_ids = enc['input_ids']; attn = enc['attention_mask']\n",
        "        offsets = enc['offset_mapping']\n",
        "        seq_ids = enc.sequence_ids()\n",
        "        span = find_span(txt, sel)\n",
        "        tok_span = map_char_to_tokens(offsets, seq_ids, span, target_seq_id=1)\n",
        "        if tok_span is None:\n",
        "            idxs = [idx for idx, sid in enumerate(seq_ids) if sid == 1]\n",
        "            if len(idxs) == 0:\n",
        "                s_tok = e_tok = 0\n",
        "            else:\n",
        "                s_tok, e_tok = idxs[0], idxs[-1]\n",
        "        else:\n",
        "            s_tok, e_tok = tok_span\n",
        "        if len(input_ids) < max_len:\n",
        "            pad_len = max_len - len(input_ids)\n",
        "            pad_id = tokenizer.pad_token_id\n",
        "            input_ids = input_ids + [pad_id]*pad_len\n",
        "            attn = attn + [0]*pad_len\n",
        "        else:\n",
        "            input_ids = input_ids[:max_len]; attn = attn[:max_len]\n",
        "            s_tok = min(s_tok, max_len-1); e_tok = min(e_tok, max_len-1)\n",
        "        input_ids_list.append(np.array(input_ids, dtype=np.int32))\n",
        "        attention_mask_list.append(np.array(attn, dtype=np.int8))\n",
        "        start_list.append(s_tok); end_list.append(e_tok)\n",
        "\n",
        "    X_train = {\n",
        "        'input_ids': np.stack(input_ids_list),\n",
        "        'attention_mask': np.stack(attention_mask_list),\n",
        "        'start_positions': np.array(start_list, dtype=np.int32),\n",
        "        'end_positions': np.array(end_list, dtype=np.int32),\n",
        "        'fold': df['fold'].values.astype(np.int8),\n",
        "        'sentiment': df['sentiment'].values,\n",
        "        'text': df['text'].values,\n",
        "        'selected_text': df['selected_text'].values,\n",
        "    }\n",
        "    np.savez_compressed(OUT_DIR / f'train_{prefix}.npz', **X_train)\n",
        "    print('Saved', OUT_DIR / f'train_{prefix}.npz')\n",
        "\n",
        "    # Test encoding\n",
        "    test_df = test.copy()\n",
        "    test_df['text'] = test_df['text'].fillna('')\n",
        "    test_df['sentiment'] = test_df['sentiment'].fillna('neutral')\n",
        "    ti_ids = []; ta_masks = []\n",
        "    for i, row in test_df.iterrows():\n",
        "        if i % 2000 == 0:\n",
        "            print(f'Test row {i}/{len(test_df)}', flush=True)\n",
        "        sent_prompt = f\"sentiment: {str(row['sentiment'])}\"\n",
        "        enc = tokenizer(text=sent_prompt, text_pair=str(row['text']), add_special_tokens=True, truncation=True, max_length=max_len,\n",
        "                        return_attention_mask=True)\n",
        "        ids = enc['input_ids']; attn = enc['attention_mask']\n",
        "        if len(ids) < max_len:\n",
        "            pad_len = max_len - len(ids)\n",
        "            ids = ids + [tokenizer.pad_token_id]*pad_len\n",
        "            attn = attn + [0]*pad_len\n",
        "        else:\n",
        "            ids = ids[:max_len]; attn = attn[:max_len]\n",
        "        ti_ids.append(np.array(ids, dtype=np.int32))\n",
        "        ta_masks.append(np.array(attn, dtype=np.int8))\n",
        "    X_test = {\n",
        "        'input_ids': np.stack(ti_ids),\n",
        "        'attention_mask': np.stack(ta_masks),\n",
        "        'sentiment': test_df['sentiment'].values,\n",
        "        'text': test_df['text'].values,\n",
        "        'textID': test_df['textID'].values,\n",
        "    }\n",
        "    np.savez_compressed(OUT_DIR / f'test_{prefix}.npz', **X_test)\n",
        "    print('Saved', OUT_DIR / f'test_{prefix}.npz')\n",
        "    Path(OUT_DIR / f'meta_{prefix}.json').write_text(json.dumps({'model_name': model_name, 'max_len': max_len, 'prefix': prefix, 'prompt': True}))\n",
        "    print('Meta saved.')\n",
        "\n",
        "prepare_cached_dataset_deberta()\n",
        "print('DeBERTa cache build complete.')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing dataset for microsoft/deberta-v3-base, max_len=128, prefix=deberta_v3_base_m128_prompt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved folds to cache/train_folds_deberta_v3_base_m128_prompt.csv\nRow 0/24732 elapsed 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 2000/24732 elapsed 0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 4000/24732 elapsed 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 6000/24732 elapsed 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 8000/24732 elapsed 1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 10000/24732 elapsed 1.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 12000/24732 elapsed 1.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 14000/24732 elapsed 1.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 16000/24732 elapsed 2.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 18000/24732 elapsed 2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 20000/24732 elapsed 2.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 22000/24732 elapsed 3.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 24000/24732 elapsed 3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved cache/train_deberta_v3_base_m128_prompt.npz\nTest row 0/2749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test row 2000/2749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved cache/test_deberta_v3_base_m128_prompt.npz\nMeta saved.\nDeBERTa cache build complete.\n"
          ]
        }
      ]
    },
    {
      "id": "0e02244f-78b5-41c9-9fa8-edb79ea92ec2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DeBERTa-v3-base 5-fold training with sentiment prompt, fp16, OOF/test logits save, fold-avg decode, submission\n",
        "import math, time, json, numpy as np, pandas as pd, torch, os\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "PREFIX = 'deberta_v3_base_m128_prompt'\n",
        "CACHE_TRAIN = Path(f'cache/train_{PREFIX}.npz')\n",
        "CACHE_TEST = Path(f'cache/test_{PREFIX}.npz')\n",
        "FOLDS_CSV = Path(f'cache/train_folds_{PREFIX}.csv')\n",
        "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
        "MAX_LEN = 128\n",
        "EPOCHS = 3\n",
        "LR = 3e-5\n",
        "WARMUP = 0.1\n",
        "WEIGHT_DECAY = 0.01\n",
        "BATCH_SIZE = 16  # per device\n",
        "GRAD_ACCUM = 4   # effective batch 64\n",
        "CLIP_NORM = 1.0\n",
        "TOP_K = 10\n",
        "SPAN_CAP = 30\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED); np.random.seed(SEED)\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "train_npz = np.load(CACHE_TRAIN, allow_pickle=True)\n",
        "test_npz = np.load(CACHE_TEST, allow_pickle=True)\n",
        "folds_df = pd.read_csv(FOLDS_CSV)\n",
        "\n",
        "class QADatasetCached(Dataset):\n",
        "    def __init__(self, idxs):\n",
        "        self.ids = train_npz['input_ids'][idxs]\n",
        "        self.attn = train_npz['attention_mask'][idxs]\n",
        "        self.start = train_npz['start_positions'][idxs]\n",
        "        self.end = train_npz['end_positions'][idxs]\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, i):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.ids[i], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.attn[i], dtype=torch.long),\n",
        "            'start_positions': torch.tensor(self.start[i], dtype=torch.long),\n",
        "            'end_positions': torch.tensor(self.end[i], dtype=torch.long),\n",
        "        }\n",
        "\n",
        "def decode_with_logits_for_row(sentiment, text, start_logits, end_logits):\n",
        "    # Neutral hard rule\n",
        "    if sentiment == 'neutral':\n",
        "        return text\n",
        "    # Re-encode to get offsets and sequence_ids; MUST pad to max_length to match logits shape\n",
        "    sent_prompt = f'sentiment: {sentiment}'\n",
        "    enc = tok(text=sent_prompt, text_pair=str(text), add_special_tokens=True, truncation=True, max_length=MAX_LEN,\n",
        "              padding='max_length', return_offsets_mapping=True, return_tensors='pt')\n",
        "    offsets = enc['offset_mapping'][0].tolist()\n",
        "    seq_ids = enc.sequence_ids(0)\n",
        "    # Restrict to tweet tokens\n",
        "    tweet_mask = np.array([1 if sid==1 else 0 for sid in seq_ids], dtype=np.int8)\n",
        "    neg_inf = -1e9\n",
        "    s = start_logits.copy(); e = end_logits.copy()\n",
        "    if tweet_mask.shape[0] != s.shape[0]:\n",
        "        # fallback: return full text\n",
        "        return text\n",
        "    s[tweet_mask==0] = neg_inf; e[tweet_mask==0] = neg_inf\n",
        "    k = min(TOP_K, int(tweet_mask.sum()))\n",
        "    start_cand = np.argsort(s)[-k:]\n",
        "    end_cand = np.argsort(e)[-k:]\n",
        "    best = None; best_score = -1e18\n",
        "    for si in start_cand:\n",
        "        for ei in end_cand:\n",
        "            if ei < si: continue\n",
        "            if (ei - si + 1) > SPAN_CAP: continue\n",
        "            sc = s[si] + e[ei]\n",
        "            if sc > best_score:\n",
        "                best_score = sc; best = (si, ei)\n",
        "    if best is None:\n",
        "        si = int(np.argmax(s)); ei = si\n",
        "    else:\n",
        "        si, ei = best\n",
        "    # Guard None offsets by moving inward to nearest valid token on tweet side\n",
        "    def valid_left(i):\n",
        "        while i >= 0 and (seq_ids[i] != 1 or offsets[i][0] is None or offsets[i][1] is None):\n",
        "            i -= 1\n",
        "        return i\n",
        "    def valid_right(i):\n",
        "        n = len(offsets)\n",
        "        while i < n and (seq_ids[i] != 1 or offsets[i][0] is None or offsets[i][1] is None):\n",
        "            i += 1\n",
        "        return i\n",
        "    si = valid_left(si); ei = valid_right(ei)\n",
        "    if si < 0 or ei >= len(offsets) or si > ei:\n",
        "        # fallback to best single start token\n",
        "        si = int(np.argmax(s)); si = valid_left(si)\n",
        "        if si < 0: return text\n",
        "        cs, ce = offsets[si][0], offsets[si][1]\n",
        "        sub = text[cs:ce].strip()\n",
        "        return sub if sub else text\n",
        "    cs = offsets[si][0]; ce = offsets[ei][1]\n",
        "    if cs is None or ce is None:\n",
        "        return text\n",
        "    sub = text[cs:ce].strip()\n",
        "    return sub if sub else text\n",
        "\n",
        "def jaccard_str(a, b):\n",
        "    sa = set(str(a).split()); sb = set(str(b).split())\n",
        "    return (len(sa & sb)) / (len(sa | sb) + 1e-12)\n",
        "\n",
        "def train_fold(fold):\n",
        "    all_folds = folds_df['fold'].values\n",
        "    tr_idx = np.where(all_folds != fold)[0]\n",
        "    va_idx = np.where(all_folds == fold)[0]\n",
        "    print(f'Fold {fold}: train {len(tr_idx)} | val {len(va_idx)}', flush=True)\n",
        "    train_ds = QADatasetCached(tr_idx)\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME).to(device)\n",
        "    opt = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    # Scheduler calibrated to optimizer steps (with grad accumulation)\n",
        "    steps_per_epoch = len(train_loader)\n",
        "    opt_steps_per_epoch = math.ceil(steps_per_epoch / GRAD_ACCUM)\n",
        "    total_opt_steps = opt_steps_per_epoch * EPOCHS\n",
        "    warmup_steps = int(WARMUP * total_opt_steps)\n",
        "    sch = get_linear_schedule_with_warmup(opt, warmup_steps, total_opt_steps)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
        "    best_score = -1.0; best_state = None\n",
        "    t0 = time.time()\n",
        "    model.train()\n",
        "    for epoch in range(EPOCHS):\n",
        "        tr_loss = 0.0\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        for step, batch in enumerate(train_loader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            with torch.cuda.amp.autocast(enabled=True):\n",
        "                out = model(**batch)\n",
        "                loss = out.loss / GRAD_ACCUM\n",
        "            scaler.scale(loss).backward()\n",
        "            if (step + 1) % GRAD_ACCUM == 0:\n",
        "                scaler.unscale_(opt)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
        "                scaler.step(opt); scaler.update(); sch.step()\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "            tr_loss += loss.item() * GRAD_ACCUM\n",
        "            if (step + 1) % 100 == 0:\n",
        "                print(f'fold {fold} epoch {epoch+1} step {step+1}/{steps_per_epoch} loss {tr_loss/(step+1):.4f} elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "        # Eval at epoch end\n",
        "        model.eval()\n",
        "        # Collect OOF logits for val indices by running on cached inputs\n",
        "        va_ids = train_npz['input_ids'][va_idx]\n",
        "        va_attn = train_npz['attention_mask'][va_idx]\n",
        "        start_logits = np.zeros((len(va_idx), MAX_LEN), dtype=np.float32)\n",
        "        end_logits = np.zeros((len(va_idx), MAX_LEN), dtype=np.float32)\n",
        "        bs = 64\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(va_idx), bs):\n",
        "                x_ids = torch.tensor(va_ids[i:i+bs], dtype=torch.long, device=device)\n",
        "                x_attn = torch.tensor(va_attn[i:i+bs], dtype=torch.long, device=device)\n",
        "                out = model(input_ids=x_ids, attention_mask=x_attn)\n",
        "                start_logits[i:i+bs] = out.start_logits.detach().cpu().numpy()\n",
        "                end_logits[i:i+bs] = out.end_logits.detach().cpu().numpy()\n",
        "        # Decode OOF\n",
        "        sentiments = train_npz['sentiment'][va_idx]\n",
        "        texts = train_npz['text'][va_idx]\n",
        "        gold = train_npz['selected_text'][va_idx]\n",
        "        preds = []\n",
        "        for i in range(len(va_idx)):\n",
        "            pred = decode_with_logits_for_row(str(sentiments[i]), str(texts[i]), start_logits[i], end_logits[i])\n",
        "            preds.append(pred)\n",
        "        score = float(np.mean([jaccard_str(preds[i], gold[i]) for i in range(len(preds))]))\n",
        "        print(f'fold {fold} epoch {epoch+1} OOF Jaccard {score:.5f}', flush=True)\n",
        "        if score > best_score + 1e-4:\n",
        "            best_score = score\n",
        "            best_state = model.state_dict()\n",
        "        model.train()\n",
        "    # Load best state\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    # Final OOF logits with best model\n",
        "    model.eval()\n",
        "    va_ids = train_npz['input_ids'][va_idx]\n",
        "    va_attn = train_npz['attention_mask'][va_idx]\n",
        "    oof_start = np.zeros((len(va_idx), MAX_LEN), dtype=np.float32)\n",
        "    oof_end = np.zeros((len(va_idx), MAX_LEN), dtype=np.float32)\n",
        "    bs = 64\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(va_idx), bs):\n",
        "            x_ids = torch.tensor(va_ids[i:i+bs], dtype=torch.long, device=device)\n",
        "            x_attn = torch.tensor(va_attn[i:i+bs], dtype=torch.long, device=device)\n",
        "            out = model(input_ids=x_ids, attention_mask=x_attn)\n",
        "            oof_start[i:i+bs] = out.start_logits.detach().cpu().numpy()\n",
        "            oof_end[i:i+bs] = out.end_logits.detach().cpu().numpy()\n",
        "    # Test logits for this fold\n",
        "    te_ids = test_npz['input_ids']\n",
        "    te_attn = test_npz['attention_mask']\n",
        "    te_start = np.zeros((len(te_ids), MAX_LEN), dtype=np.float32)\n",
        "    te_end = np.zeros((len(te_ids), MAX_LEN), dtype=np.float32)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(te_ids), bs):\n",
        "            x_ids = torch.tensor(te_ids[i:i+bs], dtype=torch.long, device=device)\n",
        "            x_attn = torch.tensor(te_attn[i:i+bs], dtype=torch.long, device=device)\n",
        "            out = model(input_ids=x_ids, attention_mask=x_attn)\n",
        "            te_start[i:i+bs] = out.start_logits.detach().cpu().numpy()\n",
        "            te_end[i:i+bs] = out.end_logits.detach().cpu().numpy()\n",
        "    # Save logits\n",
        "    fold_dir = Path(f'cache/oof_{PREFIX}')\n",
        "    fold_dir.mkdir(parents=True, exist_ok=True)\n",
        "    np.savez_compressed(fold_dir / f'fold{fold}_oof_logits.npz', idx=va_idx, start=oof_start, end=oof_end)\n",
        "    np.savez_compressed(fold_dir / f'fold{fold}_test_logits.npz', start=te_start, end=te_end)\n",
        "    # Report final best OOF using decode\n",
        "    sentiments = train_npz['sentiment'][va_idx]\n",
        "    texts = train_npz['text'][va_idx]\n",
        "    gold = train_npz['selected_text'][va_idx]\n",
        "    preds = []\n",
        "    for i in range(len(va_idx)):\n",
        "        pred = decode_with_logits_for_row(str(sentiments[i]), str(texts[i]), oof_start[i], oof_end[i])\n",
        "        preds.append(pred)\n",
        "    final_oof = float(np.mean([jaccard_str(preds[i], gold[i]) for i in range(len(preds))]))\n",
        "    print(f'fold {fold} best OOF Jaccard {final_oof:.5f}', flush=True)\n",
        "    return final_oof\n",
        "\n",
        "# Train all folds\n",
        "fold_scores = []\n",
        "for f in range(5):\n",
        "    t0 = time.time()\n",
        "    sc = train_fold(f)\n",
        "    fold_scores.append(sc)\n",
        "    print(f'Fold {f} done in {time.time()-t0:.1f}s, OOF {sc:.5f}', flush=True)\n",
        "print('OOF mean:', float(np.mean(fold_scores)))\n",
        "\n",
        "# Average test logits across folds and decode once\n",
        "fold_dir = Path(f'cache/oof_{PREFIX}')\n",
        "te_files = [np.load(fold_dir / f'fold{f}_test_logits.npz') for f in range(5)]\n",
        "te_start = np.mean([f['start'] for f in te_files], axis=0)\n",
        "te_end = np.mean([f['end'] for f in te_files], axis=0)\n",
        "test_sent = test_npz['sentiment']\n",
        "test_text = test_npz['text']\n",
        "preds = []\n",
        "for i in range(len(test_text)):\n",
        "    pred = decode_with_logits_for_row(str(test_sent[i]), str(test_text[i]), te_start[i], te_end[i])\n",
        "    preds.append(pred)\n",
        "# Build submission\n",
        "sub = pd.DataFrame({'textID': test_npz['textID'], 'selected_text': preds})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with', len(sub), 'rows')\n",
        "\n",
        "# Quick sanity: show head\n",
        "print(sub.head())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: train 19785 | val 4947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_274/3611139059.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_274/3611139059.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 step 100/1237 loss 4.2007 elapsed 8.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 step 200/1237 loss 3.3069 elapsed 16.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 step 300/1237 loss 2.6995 elapsed 25.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 step 400/1237 loss 2.3098 elapsed 33.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 step 500/1237 loss 2.0506 elapsed 41.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 step 600/1237 loss 1.8634 elapsed 50.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 step 700/1237 loss 1.7289 elapsed 58.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 step 800/1237 loss 1.6225 elapsed 67.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 step 900/1237 loss 1.5384 elapsed 75.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 step 1000/1237 loss 1.4743 elapsed 83.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 step 1100/1237 loss 1.4201 elapsed 92.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 step 1200/1237 loss 1.3738 elapsed 100.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 OOF Jaccard 0.70714\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 step 100/1237 loss 0.8010 elapsed 129.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 step 200/1237 loss 0.7932 elapsed 138.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 step 300/1237 loss 0.7956 elapsed 146.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 step 400/1237 loss 0.7863 elapsed 155.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 step 500/1237 loss 0.7863 elapsed 163.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 step 600/1237 loss 0.7919 elapsed 172.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 step 700/1237 loss 0.7933 elapsed 180.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 step 800/1237 loss 0.7888 elapsed 189.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 step 900/1237 loss 0.7907 elapsed 197.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 step 1000/1237 loss 0.7913 elapsed 206.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 step 1100/1237 loss 0.7929 elapsed 215.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 step 1200/1237 loss 0.7906 elapsed 223.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 OOF Jaccard 0.71007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 step 100/1237 loss 0.6945 elapsed 252.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 step 200/1237 loss 0.7003 elapsed 261.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 step 300/1237 loss 0.7083 elapsed 269.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 step 400/1237 loss 0.7140 elapsed 278.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 step 500/1237 loss 0.7126 elapsed 286.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 step 600/1237 loss 0.7122 elapsed 295.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 step 700/1237 loss 0.7089 elapsed 303.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 step 800/1237 loss 0.7123 elapsed 312.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 step 900/1237 loss 0.7108 elapsed 321.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 step 1000/1237 loss 0.7121 elapsed 329.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 step 1100/1237 loss 0.7139 elapsed 338.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 step 1200/1237 loss 0.7133 elapsed 346.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 OOF Jaccard 0.71308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 best OOF Jaccard 0.71308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0 done in 394.2s, OOF 0.71308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: train 19785 | val 4947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 step 100/1237 loss 5.0575 elapsed 8.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 step 200/1237 loss 3.9591 elapsed 17.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 step 300/1237 loss 3.1630 elapsed 25.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 step 400/1237 loss 2.6524 elapsed 34.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 step 500/1237 loss 2.3128 elapsed 42.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 step 600/1237 loss 2.0861 elapsed 51.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 step 700/1237 loss 1.9183 elapsed 60.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 step 800/1237 loss 1.7856 elapsed 68.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 step 900/1237 loss 1.6889 elapsed 77.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 step 1000/1237 loss 1.6067 elapsed 86.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 step 1100/1237 loss 1.5379 elapsed 94.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 step 1200/1237 loss 1.4816 elapsed 103.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 OOF Jaccard 0.70076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 step 100/1237 loss 0.7906 elapsed 132.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 step 200/1237 loss 0.7685 elapsed 141.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 step 300/1237 loss 0.7838 elapsed 149.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 step 400/1237 loss 0.7811 elapsed 158.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 step 500/1237 loss 0.7907 elapsed 166.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 step 600/1237 loss 0.7826 elapsed 175.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 step 700/1237 loss 0.7852 elapsed 184.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 step 800/1237 loss 0.7883 elapsed 192.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 step 900/1237 loss 0.7907 elapsed 201.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 step 1000/1237 loss 0.7886 elapsed 209.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 step 1100/1237 loss 0.7883 elapsed 218.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 step 1200/1237 loss 0.7847 elapsed 227.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 OOF Jaccard 0.70544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 step 100/1237 loss 0.7157 elapsed 256.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 step 200/1237 loss 0.7262 elapsed 264.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 step 300/1237 loss 0.7225 elapsed 273.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 step 400/1237 loss 0.7190 elapsed 282.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 step 500/1237 loss 0.7203 elapsed 290.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 step 600/1237 loss 0.7200 elapsed 299.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 step 700/1237 loss 0.7226 elapsed 307.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 step 800/1237 loss 0.7210 elapsed 316.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 step 900/1237 loss 0.7166 elapsed 325.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 step 1000/1237 loss 0.7157 elapsed 333.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 step 1100/1237 loss 0.7124 elapsed 342.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 step 1200/1237 loss 0.7082 elapsed 350.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 OOF Jaccard 0.70956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 best OOF Jaccard 0.70956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 done in 398.5s, OOF 0.70956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: train 19786 | val 4946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 step 100/1237 loss 4.9843 elapsed 8.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 step 200/1237 loss 3.8584 elapsed 17.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 step 300/1237 loss 3.0886 elapsed 25.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 step 400/1237 loss 2.6052 elapsed 34.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 step 500/1237 loss 2.2897 elapsed 43.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 step 600/1237 loss 2.0640 elapsed 51.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 step 700/1237 loss 1.8926 elapsed 60.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 step 800/1237 loss 1.7633 elapsed 68.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 step 900/1237 loss 1.6671 elapsed 77.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 step 1000/1237 loss 1.5862 elapsed 86.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 step 1100/1237 loss 1.5219 elapsed 94.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 step 1200/1237 loss 1.4678 elapsed 103.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 OOF Jaccard 0.71214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 step 100/1237 loss 0.8000 elapsed 132.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 step 200/1237 loss 0.8050 elapsed 141.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 step 300/1237 loss 0.7938 elapsed 149.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 step 400/1237 loss 0.7873 elapsed 158.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 step 500/1237 loss 0.7945 elapsed 166.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 step 600/1237 loss 0.7926 elapsed 175.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 step 700/1237 loss 0.8012 elapsed 183.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 step 800/1237 loss 0.7970 elapsed 192.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 step 900/1237 loss 0.7954 elapsed 201.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 step 1000/1237 loss 0.7907 elapsed 209.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 step 1100/1237 loss 0.7911 elapsed 218.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 step 1200/1237 loss 0.7868 elapsed 226.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 OOF Jaccard 0.71689\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 step 100/1237 loss 0.7003 elapsed 255.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 step 200/1237 loss 0.7104 elapsed 264.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 step 300/1237 loss 0.7135 elapsed 273.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 step 400/1237 loss 0.7152 elapsed 281.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 step 500/1237 loss 0.7252 elapsed 290.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 step 600/1237 loss 0.7262 elapsed 298.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 step 700/1237 loss 0.7205 elapsed 307.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 step 800/1237 loss 0.7163 elapsed 316.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 step 900/1237 loss 0.7145 elapsed 324.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 step 1000/1237 loss 0.7108 elapsed 333.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 step 1100/1237 loss 0.7094 elapsed 341.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 step 1200/1237 loss 0.7071 elapsed 350.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 OOF Jaccard 0.71737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 best OOF Jaccard 0.71737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 done in 398.1s, OOF 0.71737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: train 19786 | val 4946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 1 step 100/1237 loss 4.3199 elapsed 8.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 1 step 200/1237 loss 3.3764 elapsed 17.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 1 step 300/1237 loss 2.7407 elapsed 25.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 1 step 400/1237 loss 2.3289 elapsed 34.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 1 step 500/1237 loss 2.0599 elapsed 42.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 1 step 600/1237 loss 1.8718 elapsed 51.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 1 step 700/1237 loss 1.7367 elapsed 60.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 1 step 800/1237 loss 1.6310 elapsed 68.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 1 step 900/1237 loss 1.5493 elapsed 77.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 1 step 1000/1237 loss 1.4787 elapsed 85.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 1 step 1100/1237 loss 1.4225 elapsed 94.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 1 step 1200/1237 loss 1.3739 elapsed 103.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 1 OOF Jaccard 0.69629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 2 step 100/1237 loss 0.7958 elapsed 132.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 2 step 200/1237 loss 0.8031 elapsed 140.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 2 step 300/1237 loss 0.7935 elapsed 149.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 2 step 400/1237 loss 0.7866 elapsed 158.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 2 step 500/1237 loss 0.7875 elapsed 166.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 2 step 600/1237 loss 0.7836 elapsed 175.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 2 step 700/1237 loss 0.7843 elapsed 183.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 2 step 800/1237 loss 0.7895 elapsed 192.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 2 step 900/1237 loss 0.7894 elapsed 200.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 2 step 1000/1237 loss 0.7881 elapsed 209.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 2 step 1100/1237 loss 0.7870 elapsed 218.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 2 step 1200/1237 loss 0.7849 elapsed 226.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 2 OOF Jaccard 0.70987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 3 step 100/1237 loss 0.7241 elapsed 255.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 3 step 200/1237 loss 0.7148 elapsed 264.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 3 step 300/1237 loss 0.7078 elapsed 273.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 3 step 400/1237 loss 0.7189 elapsed 281.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 3 step 500/1237 loss 0.7197 elapsed 290.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 3 step 600/1237 loss 0.7199 elapsed 298.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 3 step 700/1237 loss 0.7166 elapsed 307.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 3 step 800/1237 loss 0.7177 elapsed 316.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 3 step 900/1237 loss 0.7136 elapsed 324.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 3 step 1000/1237 loss 0.7140 elapsed 333.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 3 step 1100/1237 loss 0.7106 elapsed 341.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 3 step 1200/1237 loss 0.7090 elapsed 350.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 epoch 3 OOF Jaccard 0.70755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 best OOF Jaccard 0.70755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 done in 398.1s, OOF 0.70755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: train 19786 | val 4946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 1 step 100/1237 loss 4.7051 elapsed 8.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 1 step 200/1237 loss 3.6356 elapsed 17.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 1 step 300/1237 loss 2.9339 elapsed 25.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 1 step 400/1237 loss 2.4758 elapsed 34.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 1 step 500/1237 loss 2.1727 elapsed 43.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 1 step 600/1237 loss 1.9551 elapsed 51.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 1 step 700/1237 loss 1.8093 elapsed 60.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 1 step 800/1237 loss 1.6935 elapsed 69.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 1 step 900/1237 loss 1.5999 elapsed 77.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 1 step 1000/1237 loss 1.5275 elapsed 86.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 1 step 1100/1237 loss 1.4679 elapsed 94.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 1 step 1200/1237 loss 1.4168 elapsed 103.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 1 OOF Jaccard 0.70402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 2 step 100/1237 loss 0.8123 elapsed 132.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 2 step 200/1237 loss 0.8173 elapsed 141.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 2 step 300/1237 loss 0.8020 elapsed 150.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 2 step 400/1237 loss 0.7964 elapsed 158.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 2 step 500/1237 loss 0.8024 elapsed 167.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 2 step 600/1237 loss 0.7952 elapsed 175.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 2 step 700/1237 loss 0.7900 elapsed 184.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 2 step 800/1237 loss 0.7889 elapsed 193.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 2 step 900/1237 loss 0.7885 elapsed 201.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 2 step 1000/1237 loss 0.7893 elapsed 210.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 2 step 1100/1237 loss 0.7896 elapsed 219.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 2 step 1200/1237 loss 0.7872 elapsed 227.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 2 OOF Jaccard 0.71489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 3 step 100/1237 loss 0.7368 elapsed 256.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 3 step 200/1237 loss 0.7310 elapsed 265.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 3 step 300/1237 loss 0.7236 elapsed 274.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 3 step 400/1237 loss 0.7237 elapsed 282.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 3 step 500/1237 loss 0.7131 elapsed 291.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 3 step 600/1237 loss 0.7102 elapsed 300.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 3 step 700/1237 loss 0.7078 elapsed 308.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 3 step 800/1237 loss 0.7079 elapsed 317.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 3 step 900/1237 loss 0.7064 elapsed 325.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 3 step 1000/1237 loss 0.7073 elapsed 334.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 3 step 1100/1237 loss 0.7113 elapsed 343.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 3 step 1200/1237 loss 0.7102 elapsed 351.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 epoch 3 OOF Jaccard 0.71474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 best OOF Jaccard 0.71474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 done in 399.2s, OOF 0.71474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF mean: 0.7124612654099624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv with 2749 rows\n       textID                                      selected_text\n0  80a1e6bc32                                               wish\n1  863097735d                                  gosh today sucks!\n2  264cd5277f  tired and didn`t really have an exciting Satur...\n3  baee1e6ffc             i`ve been eating cheetos all morning..\n4  67d06a8dee   haiiii sankQ i`m fineee ima js get a checkup ...\n"
          ]
        }
      ]
    },
    {
      "id": "71dc3f1c-5c40-4951-8483-82f91ad8dbc1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Second-seed DeBERTa-v3-base run (SEED=43) with identical folds; saves logits under new prefix for later blending\n",
        "import math, time, json, numpy as np, pandas as pd, torch, os\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BASE_PREFIX = 'deberta_v3_base_m128_prompt'\n",
        "SEED2 = 43\n",
        "PREFIX2 = BASE_PREFIX + '_s43'\n",
        "CACHE_TRAIN = Path(f'cache/train_{BASE_PREFIX}.npz')\n",
        "CACHE_TEST = Path(f'cache/test_{BASE_PREFIX}.npz')\n",
        "FOLDS_CSV = Path(f'cache/train_folds_{BASE_PREFIX}.csv')  # reuse identical folds for fair CV\n",
        "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
        "MAX_LEN = 128; EPOCHS = 3; LR = 3e-5; WARMUP = 0.1; WEIGHT_DECAY = 0.01\n",
        "BATCH_SIZE = 16; GRAD_ACCUM = 4; CLIP_NORM = 1.0; TOP_K = 10; SPAN_CAP = 30\n",
        "\n",
        "torch.manual_seed(SEED2); np.random.seed(SEED2)\n",
        "tok2 = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "train_npz2 = np.load(CACHE_TRAIN, allow_pickle=True); test_npz2 = np.load(CACHE_TEST, allow_pickle=True)\n",
        "folds_df2 = pd.read_csv(FOLDS_CSV)\n",
        "\n",
        "class QADatasetCached2(Dataset):\n",
        "    def __init__(self, idxs):\n",
        "        self.ids = train_npz2['input_ids'][idxs]\n",
        "        self.attn = train_npz2['attention_mask'][idxs]\n",
        "        self.start = train_npz2['start_positions'][idxs]\n",
        "        self.end = train_npz2['end_positions'][idxs]\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, i):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.ids[i], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.attn[i], dtype=torch.long),\n",
        "            'start_positions': torch.tensor(self.start[i], dtype=torch.long),\n",
        "            'end_positions': torch.tensor(self.end[i], dtype=torch.long),\n",
        "        }\n",
        "\n",
        "def decode_row2(sentiment, text, start_logits, end_logits):\n",
        "    if sentiment == 'neutral':\n",
        "        return text\n",
        "    sent_prompt = f'sentiment: {sentiment}'\n",
        "    enc = tok2(text=sent_prompt, text_pair=str(text), add_special_tokens=True, truncation=True, max_length=MAX_LEN,\n",
        "               padding='max_length', return_offsets_mapping=True, return_tensors='pt')\n",
        "    offsets = enc['offset_mapping'][0].tolist()\n",
        "    seq_ids = enc.sequence_ids(0)\n",
        "    tweet_mask = np.array([1 if sid==1 else 0 for sid in seq_ids], dtype=np.int8)\n",
        "    s = start_logits.copy(); e = end_logits.copy(); neg_inf = -1e9\n",
        "    if tweet_mask.shape[0] != s.shape[0]:\n",
        "        return text\n",
        "    s[tweet_mask==0] = neg_inf; e[tweet_mask==0] = neg_inf\n",
        "    k = min(TOP_K, int(tweet_mask.sum()))\n",
        "    start_cand = np.argsort(s)[-k:]; end_cand = np.argsort(e)[-k:]\n",
        "    best = None; best_score = -1e18\n",
        "    for si in start_cand:\n",
        "        for ei in end_cand:\n",
        "            if ei < si: continue\n",
        "            if (ei - si + 1) > SPAN_CAP: continue\n",
        "            sc = s[si] + e[ei]\n",
        "            if sc > best_score: best_score = sc; best = (si, ei)\n",
        "    if best is None:\n",
        "        si = int(np.argmax(s)); ei = si\n",
        "    else:\n",
        "        si, ei = best\n",
        "    def valid_left(i):\n",
        "        while i >= 0 and (seq_ids[i] != 1 or offsets[i][0] is None or offsets[i][1] is None): i -= 1\n",
        "        return i\n",
        "    def valid_right(i):\n",
        "        n = len(offsets)\n",
        "        while i < n and (seq_ids[i] != 1 or offsets[i][0] is None or offsets[i][1] is None): i += 1\n",
        "        return i\n",
        "    si = valid_left(si); ei = valid_right(ei)\n",
        "    if si < 0 or ei >= len(offsets) or si > ei:\n",
        "        si = int(np.argmax(s)); si = valid_left(si)\n",
        "        if si < 0: return text\n",
        "        cs, ce = offsets[si][0], offsets[si][1]\n",
        "        sub = text[cs:ce].strip()\n",
        "        return sub if sub else text\n",
        "    cs = offsets[si][0]; ce = offsets[ei][1]\n",
        "    if cs is None or ce is None: return text\n",
        "    sub = text[cs:ce].strip()\n",
        "    return sub if sub else text\n",
        "\n",
        "def jaccard_str2(a, b):\n",
        "    sa = set(str(a).split()); sb = set(str(b).split())\n",
        "    return (len(sa & sb)) / (len(sa | sb) + 1e-12)\n",
        "\n",
        "def train_fold_seed2(fold):\n",
        "    all_folds = folds_df2['fold'].values\n",
        "    tr_idx = np.where(all_folds != fold)[0]\n",
        "    va_idx = np.where(all_folds == fold)[0]\n",
        "    print(f'[s43] Fold {fold}: train {len(tr_idx)} | val {len(va_idx)}', flush=True)\n",
        "    train_ds = QADatasetCached2(tr_idx)\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME).to(device)\n",
        "    opt = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    steps_per_epoch = len(train_loader)\n",
        "    opt_steps_per_epoch = math.ceil(steps_per_epoch / GRAD_ACCUM)\n",
        "    total_opt_steps = opt_steps_per_epoch * EPOCHS\n",
        "    warmup_steps = int(WARMUP * total_opt_steps)\n",
        "    sch = get_linear_schedule_with_warmup(opt, warmup_steps, total_opt_steps)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
        "    best_score = -1.0; best_state = None\n",
        "    t0 = time.time()\n",
        "    model.train()\n",
        "    for epoch in range(EPOCHS):\n",
        "        tr_loss = 0.0\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        for step, batch in enumerate(train_loader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            with torch.cuda.amp.autocast(enabled=True):\n",
        "                out = model(**batch)\n",
        "                loss = out.loss / GRAD_ACCUM\n",
        "            scaler.scale(loss).backward()\n",
        "            if (step + 1) % GRAD_ACCUM == 0:\n",
        "                scaler.unscale_(opt)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
        "                scaler.step(opt); scaler.update(); sch.step()\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "            tr_loss += loss.item() * GRAD_ACCUM\n",
        "            if (step + 1) % 100 == 0:\n",
        "                print(f'[s43] fold {fold} epoch {epoch+1} step {step+1}/{steps_per_epoch} loss {tr_loss/(step+1):.4f} elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "        # Eval\n",
        "        model.eval()\n",
        "        va_ids = train_npz2['input_ids'][va_idx]\n",
        "        va_attn = train_npz2['attention_mask'][va_idx]\n",
        "        start_logits = np.zeros((len(va_idx), MAX_LEN), dtype=np.float32)\n",
        "        end_logits = np.zeros((len(va_idx), MAX_LEN), dtype=np.float32)\n",
        "        bs = 64\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(va_idx), bs):\n",
        "                x_ids = torch.tensor(va_ids[i:i+bs], dtype=torch.long, device=device)\n",
        "                x_attn = torch.tensor(va_attn[i:i+bs], dtype=torch.long, device=device)\n",
        "                out = model(input_ids=x_ids, attention_mask=x_attn)\n",
        "                start_logits[i:i+bs] = out.start_logits.detach().cpu().numpy()\n",
        "                end_logits[i:i+bs] = out.end_logits.detach().cpu().numpy()\n",
        "        sentiments = train_npz2['sentiment'][va_idx]\n",
        "        texts = train_npz2['text'][va_idx]\n",
        "        gold = train_npz2['selected_text'][va_idx]\n",
        "        preds = [decode_row2(str(sentiments[i]), str(texts[i]), start_logits[i], end_logits[i]) for i in range(len(va_idx))]\n",
        "        score = float(np.mean([jaccard_str2(preds[i], gold[i]) for i in range(len(preds))]))\n",
        "        print(f'[s43] fold {fold} epoch {epoch+1} OOF Jaccard {score:.5f}', flush=True)\n",
        "        if score > best_score + 1e-4:\n",
        "            best_score = score; best_state = model.state_dict()\n",
        "        model.train()\n",
        "    if best_state is not None: model.load_state_dict(best_state)\n",
        "    # Save final OOF/Test logits\n",
        "    model.eval()\n",
        "    va_ids = train_npz2['input_ids'][va_idx]; va_attn = train_npz2['attention_mask'][va_idx]\n",
        "    oof_start = np.zeros((len(va_idx), MAX_LEN), dtype=np.float32); oof_end = np.zeros_like(oof_start)\n",
        "    bs = 64\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(va_idx), bs):\n",
        "            x_ids = torch.tensor(va_ids[i:i+bs], dtype=torch.long, device=device)\n",
        "            x_attn = torch.tensor(va_attn[i:i+bs], dtype=torch.long, device=device)\n",
        "            out = model(input_ids=x_ids, attention_mask=x_attn)\n",
        "            oof_start[i:i+bs] = out.start_logits.detach().cpu().numpy()\n",
        "            oof_end[i:i+bs] = out.end_logits.detach().cpu().numpy()\n",
        "    te_ids = test_npz2['input_ids']; te_attn = test_npz2['attention_mask']\n",
        "    te_start = np.zeros((len(te_ids), MAX_LEN), dtype=np.float32); te_end = np.zeros_like(te_start)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(te_ids), bs):\n",
        "            x_ids = torch.tensor(te_ids[i:i+bs], dtype=torch.long, device=device)\n",
        "            x_attn = torch.tensor(te_attn[i:i+bs], dtype=torch.long, device=device)\n",
        "            out = model(input_ids=x_ids, attention_mask=x_attn)\n",
        "            te_start[i:i+bs] = out.start_logits.detach().cpu().numpy()\n",
        "            te_end[i:i+bs] = out.end_logits.detach().cpu().numpy()\n",
        "    fold_dir = Path(f'cache/oof_{PREFIX2}'); fold_dir.mkdir(parents=True, exist_ok=True)\n",
        "    np.savez_compressed(fold_dir / f'fold{fold}_oof_logits.npz', idx=va_idx, start=oof_start, end=oof_end)\n",
        "    np.savez_compressed(fold_dir / f'fold{fold}_test_logits.npz', start=te_start, end=te_end)\n",
        "    # report\n",
        "    sentiments = train_npz2['sentiment'][va_idx]; texts = train_npz2['text'][va_idx]; gold = train_npz2['selected_text'][va_idx]\n",
        "    preds = [decode_row2(str(sentiments[i]), str(texts[i]), oof_start[i], oof_end[i]) for i in range(len(va_idx))]\n",
        "    final_oof = float(np.mean([jaccard_str2(preds[i], gold[i]) for i in range(len(preds))]))\n",
        "    print(f'[s43] fold {fold} best OOF Jaccard {final_oof:.5f}', flush=True)\n",
        "    return final_oof\n",
        "\n",
        "def run_seed43_all_folds():\n",
        "    scores = []\n",
        "    for f in range(5):\n",
        "        t0 = time.time()\n",
        "        sc = train_fold_seed2(f)\n",
        "        scores.append(sc)\n",
        "        print(f'[s43] Fold {f} done in {time.time()-t0:.1f}s, OOF {sc:.5f}', flush=True)\n",
        "    print('[s43] OOF mean:', float(np.mean(scores)))\n",
        "\n",
        "# Note: Do NOT execute now while seed=42 run is training. Execute this cell after Cell 6 finishes to train the second seed.\n",
        "# After both seeds finish, blend test logits by averaging across both oof directories and decode once with the same decoder."
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "id": "8a1f5eaf-a4e1-4b0a-b090-a62664ffada0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Execute second-seed training, then blend both seeds' logits and decode once\n",
        "import numpy as np, pandas as pd, torch, os, math, time\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 1) Train seed=43 across all folds\n",
        "run_seed43_all_folds()\n",
        "\n",
        "# 2) Blend test logits across seeds and decode once\n",
        "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
        "MAX_LEN = 128\n",
        "TOP_K = 10\n",
        "SPAN_CAP = 30\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "base = 'deberta_v3_base_m128_prompt'\n",
        "dir1 = Path(f'cache/oof_{base}')\n",
        "dir2 = Path(f'cache/oof_{base}_s43')\n",
        "te_files1 = [np.load(dir1 / f'fold{f}_test_logits.npz') for f in range(5)]\n",
        "te_files2 = [np.load(dir2 / f'fold{f}_test_logits.npz') for f in range(5)]\n",
        "te_start = np.mean([f['start'] for f in te_files1 + te_files2], axis=0)\n",
        "te_end = np.mean([f['end'] for f in te_files1 + te_files2], axis=0)\n",
        "test_npz = np.load(f'cache/test_{base}.npz', allow_pickle=True)\n",
        "test_sent = test_npz['sentiment']\n",
        "test_text = test_npz['text']\n",
        "\n",
        "def decode_row(sentiment, text, start_logits, end_logits):\n",
        "    if sentiment == 'neutral':\n",
        "        return text\n",
        "    sent_prompt = f'sentiment: {sentiment}'\n",
        "    enc = tok(text=sent_prompt, text_pair=str(text), add_special_tokens=True, truncation=True, max_length=MAX_LEN,\n",
        "              padding='max_length', return_offsets_mapping=True, return_tensors='pt')\n",
        "    offsets = enc['offset_mapping'][0].tolist()\n",
        "    seq_ids = enc.sequence_ids(0)\n",
        "    tweet_mask = np.array([1 if sid==1 else 0 for sid in seq_ids], dtype=np.int8)\n",
        "    if tweet_mask.shape[0] != start_logits.shape[0]:\n",
        "        return text\n",
        "    neg_inf = -1e9\n",
        "    s = start_logits.copy(); e = end_logits.copy()\n",
        "    s[tweet_mask==0] = neg_inf; e[tweet_mask==0] = neg_inf\n",
        "    k = min(TOP_K, int(tweet_mask.sum()))\n",
        "    start_cand = np.argsort(s)[-k:]\n",
        "    end_cand = np.argsort(e)[-k:]\n",
        "    best = None; best_score = -1e18\n",
        "    for si in start_cand:\n",
        "        for ei in end_cand:\n",
        "            if ei < si: continue\n",
        "            if (ei - si + 1) > SPAN_CAP: continue\n",
        "            sc = s[si] + e[ei]\n",
        "            if sc > best_score: best_score = sc; best = (si, ei)\n",
        "    if best is None:\n",
        "        si = int(np.argmax(s)); ei = si\n",
        "    else:\n",
        "        si, ei = best\n",
        "    def valid_left(i):\n",
        "        while i >= 0 and (seq_ids[i] != 1 or offsets[i][0] is None or offsets[i][1] is None): i -= 1\n",
        "        return i\n",
        "    def valid_right(i):\n",
        "        n = len(offsets)\n",
        "        while i < n and (seq_ids[i] != 1 or offsets[i][0] is None or offsets[i][1] is None): i += 1\n",
        "        return i\n",
        "    si = valid_left(si); ei = valid_right(ei)\n",
        "    if si < 0 or ei >= len(offsets) or si > ei:\n",
        "        si = int(np.argmax(s)); si = valid_left(si)\n",
        "        if si < 0: return text\n",
        "        cs, ce = offsets[si][0], offsets[si][1]\n",
        "        sub = text[cs:ce].strip()\n",
        "        return sub if sub else text\n",
        "    cs = offsets[si][0]; ce = offsets[ei][1]\n",
        "    if cs is None or ce is None: return text\n",
        "    sub = text[cs:ce].strip()\n",
        "    return sub if sub else text\n",
        "\n",
        "preds = []\n",
        "for i in range(len(test_text)):\n",
        "    preds.append(decode_row(str(test_sent[i]), str(test_text[i]), te_start[i], te_end[i]))\n",
        "\n",
        "sub = pd.DataFrame({'textID': test_npz['textID'], 'selected_text': preds})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Blended submission.csv saved with', len(sub), 'rows')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[s43] Fold 0: train 19785 | val 4947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_274/2971673811.py:104: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_274/2971673811.py:113: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}