{
  "cells": [
    {
      "id": "16805e92-5b2d-4f7a-b53e-7c7c16af1e0a",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan: Tweet Sentiment Extraction (Medal-Oriented)\n",
        "\n",
        "Objectives:\n",
        "- Establish reliable CV and a fast baseline ASAP\n",
        "- Train a strong span-extraction transformer (QA-style) with start/end heads\n",
        "- Apply robust post-processing (especially for neutral \u2192 full tweet, punctuation trimming, whitespace fixes)\n",
        "- Ensemble diverse seeds/folds; calibrate with OOF\n",
        "- Iterate via error analysis\n",
        "\n",
        "Validation:\n",
        "- Use StratifiedKFold by sentiment (5 folds) with deterministic seeds\n",
        "- Fit transforms inside folds; cache features/logits\n",
        "- Score OOF via Jaccard on reconstructed text\n",
        "\n",
        "Baselines and Iteration:\n",
        "1) Smoke-check environment (GPU) and data integrity\n",
        "2) Heuristic baseline:\n",
        "   - neutral \u2192 tweet_text\n",
        "   - positive/negative \u2192 simple char-span heuristic centered on sentiment words (regex-based), fallback to tweet_text\n",
        "   - Expect ~0.63\u20130.67 Jaccard (sanity gate)\n",
        "3) Transformer QA model:\n",
        "   - Input: \"question\" = sentiment; \"context\" = tweet\n",
        "   - Tokenizer: RoBERTa-base (byte-level BPE) or DeBERTa-v3-base\n",
        "   - Max length ~96\u2013128; pad/truncate only context\n",
        "   - Loss: cross-entropy on start/end; label smoothing 0.05\n",
        "   - Optimizer: AdamW, lr warmup, cosine decay; epochs ~3\u20135 with early stop\n",
        "   - Folds: 5; save start/end logits per fold for OOF/test; average logits\n",
        "   - Post-process: ensure start<=end, neutral full span, detokenize with offsets, trim spaces/punctuation only if improves Jaccard on OOF\n",
        "   - Target Jaccard: \u22650.715 OOF before ensembling; \u22650.72 with careful PP/ensembles\n",
        "4) Ensembling:\n",
        "   - Blend seeds (2\u20133) per backbone\n",
        "   - Consider two backbones (RoBERTa-base + DeBERTa-v3-base) if time\n",
        "   - Weighted average of logits; weights chosen by OOF\n",
        "\n",
        "Risk Controls / Checks:\n",
        "- Always print fold progress and elapsed time\n",
        "- Cache tokenized dataset and offsets\n",
        "- Verify submission.csv format and encoding\n",
        "\n",
        "Milestones (with expert reviews):\n",
        "A) Plan review (this cell)\n",
        "B) Data loading + EDA snapshot\n",
        "C) Baseline heuristic + CV\n",
        "D) Transformer v1 (single backbone, 5-fold) + PP v1\n",
        "E) Error analysis + PP v2\n",
        "F) Ensembling seeds/backbones\n",
        "G) Final checks and submission"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "6259a68b-a651-4d5b-b6d3-11955c307b99",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, time, json, textwrap, math, random, statistics as stats, subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print('=== GPU CHECK ===', flush=True)\n",
        "try:\n",
        "    out = subprocess.run(['bash','-lc','nvidia-smi || true'], capture_output=True, text=True, check=False)\n",
        "    print(out.stdout)\n",
        "except Exception as e:\n",
        "    print('nvidia-smi failed:', e)\n",
        "\n",
        "print('Python', sys.version)\n",
        "print('CWD:', os.getcwd(), flush=True)\n",
        "\n",
        "train_path = 'train.csv'\n",
        "test_path = 'test.csv'\n",
        "ss_path = 'sample_submission.csv'\n",
        "for p in [train_path, test_path, ss_path]:\n",
        "    print(p, os.path.exists(p), os.path.getsize(p) if os.path.exists(p) else None)\n",
        "\n",
        "train = pd.read_csv(train_path)\n",
        "test = pd.read_csv(test_path)\n",
        "ss = pd.read_csv(ss_path)\n",
        "\n",
        "print('\\n=== Head(train) ===')\n",
        "print(train.head(3).to_string(index=False))\n",
        "print('\\n=== Head(test) ===')\n",
        "print(test.head(3).to_string(index=False))\n",
        "print('\\n=== Head(sample_submission) ===')\n",
        "print(ss.head(3).to_string(index=False))\n",
        "\n",
        "print('\\nShapes:', train.shape, test.shape)\n",
        "print('Columns(train):', list(train.columns))\n",
        "print('Columns(test):', list(test.columns))\n",
        "\n",
        "# Basic checks\n",
        "print('\\nNulls in train:')\n",
        "print(train.isnull().sum())\n",
        "print('\\nSentiment distribution (train):')\n",
        "print(train['sentiment'].value_counts())\n",
        "\n",
        "train['tweet_len'] = train['text'].astype(str).str.len()\n",
        "train['sel_len'] = train['selected_text'].astype(str).str.len()\n",
        "print('\\nTweet length stats:', train['tweet_len'].describe(percentiles=[0.5,0.9,0.95,0.99]).to_dict())\n",
        "print('Selected length stats:', train['sel_len'].describe(percentiles=[0.5,0.9,0.95,0.99]).to_dict())\n",
        "\n",
        "def jaccard(a, b):\n",
        "    a = str(a)\n",
        "    b = str(b)\n",
        "    a_set = set(a.split())\n",
        "    b_set = set(b.split())\n",
        "    if not a_set and not b_set:\n",
        "        return 1.0\n",
        "    if not a_set or not b_set:\n",
        "        return 0.0\n",
        "    inter = len(a_set & b_set)\n",
        "    union = len(a_set | b_set)\n",
        "    return inter / union if union else 0.0\n",
        "\n",
        "# Quick sanity: compute jaccard of gold vs itself\n",
        "jac_self = train.apply(lambda r: jaccard(r['selected_text'], r['selected_text']), axis=1).mean()\n",
        "print('\\nSanity Jaccard(selected vs selected):', jac_self)\n",
        "\n",
        "print('\\nSample rows:')\n",
        "for i in range(3):\n",
        "    r = train.sample(1, random_state=42+i).iloc[0]\n",
        "    print({'id': r['textID'], 'sentiment': r['sentiment'], 'text': r['text'][:120], 'selected': r['selected_text']})\n",
        "\n",
        "print('\\nDone EDA snapshot.', flush=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== GPU CHECK ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Sep 30 15:43:56 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.06             Driver Version: 550.144.06     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10-24Q                 On  |   00000002:00:00.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |     182MiB /  24512MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\nPython 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]\nCWD: /var/lib/simon/agent_run_states/tweet-sentiment-extraction-20250930-153631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.csv True 3151814\ntest.csv True 244504\nsample_submission.csv True 33009\n\n=== Head(train) ===\n    textID                                                                         text                                      selected_text sentiment\n8d4ad58b45                           eating breakfast  getting ready to go to school ;( eating breakfast  getting ready to go to school ;(  negative\nfdfe12a800 Going to fold laundry and then hit the sack. I have boring saturday evenings                    I have boring saturday evenings  negative\n5efd224f4e            happy mothers day to all   im off to spend the day with my family                                              happy  positive\n\n=== Head(test) ===\n    textID                                                                                                                          text sentiment\n80a1e6bc32                                                                                  I just saw a shooting star... I made my wish  positive\n863097735d gosh today sucks! i didnt get my tax returns! im so upset cuz now i have to miss my best friends wedding in washington...lame  negative\n264cd5277f                                       tired and didn`t really have an exciting Saturday.  oh well, hope it`s better tomorrow.   neutral\n\n=== Head(sample_submission) ===\n    textID  selected_text\n80a1e6bc32            NaN\n863097735d            NaN\n264cd5277f            NaN\n\nShapes: (24732, 4) (2749, 3)\nColumns(train): ['textID', 'text', 'selected_text', 'sentiment']\nColumns(test): ['textID', 'text', 'sentiment']\n\nNulls in train:\ntextID           0\ntext             1\nselected_text    1\nsentiment        0\ndtype: int64\n\nSentiment distribution (train):\nsentiment\nneutral     9998\npositive    7735\nnegative    6999\nName: count, dtype: int64\n\nTweet length stats: {'count': 24732.0, 'mean': 68.37890182759179, 'std': 35.66506042553745, 'min': 3.0, '50%': 64.0, '90%': 122.0, '95%': 129.0, '99%': 137.0, 'max': 141.0}\nSelected length stats: {'count': 24732.0, 'mean': 36.67976710334789, 'std': 35.67434994054329, 'min': 1.0, '50%': 22.0, '90%': 97.0, '95%': 117.0, '99%': 135.0, 'max': 141.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nSanity Jaccard(selected vs selected): 1.0\n\nSample rows:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': '62da1d2932', 'sentiment': 'positive', 'text': ' Thanks for the advice! Went to the doctor`s and slept a lot yesterday  Must be the meds.', 'selected': 'Thanks'}\n{'id': '8027499c43', 'sentiment': 'positive', 'text': ' I absolutely LOVE you.  Thanks', 'selected': 'I absolutely LOVE you.  Thanks'}\n{'id': 'eeb70d82b3', 'sentiment': 'negative', 'text': 'i`m so tired  of being sick ALL the time!!!!', 'selected': 'i`m so tired'}\n\nDone EDA snapshot.\n"
          ]
        }
      ]
    },
    {
      "id": "10eba849-6876-48f8-b1ae-815cee0e3f62",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Heuristic baseline: build quick OOF estimate and a valid submission\n",
        "import re\n",
        "\n",
        "pos_keywords = [\n",
        "    'love','lovely','awesome','amazing','great','good','best','glad','happy','yay','thanks','thank you','excited',\n",
        "    ':)',':-)',':D','<3','lol','lmao','rofl'\n",
        "]\n",
        "neg_keywords = [\n",
        "    \"n't\",'not','no ',' never','sad','bad','worse','worst','hate','angry','upset','sucks','tired','sick','terrible','awful',\n",
        "    ':(',' :-(',' :/',' :|'\n",
        "]\n",
        "\n",
        "def find_keyword_span(text: str, kws):\n",
        "    if not isinstance(text, str) or not text:\n",
        "        return None\n",
        "    low = text.lower()\n",
        "    for kw in kws:\n",
        "        kw_low = kw.lower()\n",
        "        idx = low.find(kw_low)\n",
        "        if idx != -1:\n",
        "            return text[idx: idx + len(kw)]\n",
        "    return None\n",
        "\n",
        "def heuristic_selected(text: str, sentiment: str):\n",
        "    if not isinstance(text, str) or not text:\n",
        "        return ''\n",
        "    s = (sentiment or '').strip().lower()\n",
        "    if s == 'neutral':\n",
        "        return text\n",
        "    if s == 'positive':\n",
        "        span = find_keyword_span(text, pos_keywords)\n",
        "        return span if span else text\n",
        "    if s == 'negative':\n",
        "        span = find_keyword_span(text, neg_keywords)\n",
        "        return span if span else text\n",
        "    return text\n",
        "\n",
        "# OOF heuristic score on train\n",
        "train_pred = train.apply(lambda r: heuristic_selected(r['text'], r['sentiment']), axis=1)\n",
        "heuristic_oof = train.apply(lambda r: jaccard(r['selected_text'], heuristic_selected(r['text'], r['sentiment'])), axis=1).mean()\n",
        "print(f'Heuristic OOF Jaccard (sanity): {heuristic_oof:.4f}')\n",
        "\n",
        "# Build submission\n",
        "sub = test.copy()\n",
        "sub['selected_text'] = sub.apply(lambda r: heuristic_selected(r['text'], r['sentiment']), axis=1)\n",
        "submission_path = 'submission.csv'\n",
        "sub[['textID','selected_text']].to_csv(submission_path, index=False)\n",
        "print('Wrote', submission_path, 'Head:')\n",
        "print(sub[['textID','selected_text']].head().to_string(index=False))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Heuristic OOF Jaccard (sanity): 0.5902\nWrote submission.csv Head:\n    textID                                                                                       selected_text\n80a1e6bc32                                                        I just saw a shooting star... I made my wish\n863097735d                                                                                               upset\n264cd5277f             tired and didn`t really have an exciting Saturday.  oh well, hope it`s better tomorrow.\nbaee1e6ffc                                                              i`ve been eating cheetos all morning..\n67d06a8dee  haiiii sankQ i`m fineee ima js get a checkup cos my rib hurts LOL idk but i shall be fine ~ thanks\n"
          ]
        }
      ]
    },
    {
      "id": "a913098d-780d-4642-a577-a6dea3a1edea",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install cu121 torch stack and core NLP deps; sanity-check GPU\n",
        "import os, sys, subprocess, shutil, time\n",
        "\n",
        "def pip(*args):\n",
        "    print('> pip', *args, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "# Uninstall any pre-existing torch stacks to avoid duplicates\n",
        "for pkg in ('torch','torchvision','torchaudio'):\n",
        "    try:\n",
        "        subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', pkg], check=False)\n",
        "    except Exception as e:\n",
        "        print('Uninstall warn:', pkg, e)\n",
        "\n",
        "# Clean stray site dirs that can shadow correct wheels (idempotent)\n",
        "for d in (\n",
        "    '/app/.pip-target/torch',\n",
        "    '/app/.pip-target/torch-2.8.0.dist-info',\n",
        "    '/app/.pip-target/torch-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torchvision',\n",
        "    '/app/.pip-target/torchvision-0.23.0.dist-info',\n",
        "    '/app/.pip-target/torchvision-0.19.1.dist-info',\n",
        "    '/app/.pip-target/torchaudio',\n",
        "    '/app/.pip-target/torchaudio-2.8.0.dist-info',\n",
        "    '/app/.pip-target/torchaudio-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torchgen',\n",
        "    '/app/.pip-target/functorch',\n",
        "):\n",
        "    if os.path.exists(d):\n",
        "        print('Removing', d, flush=True)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "# Install exact cu121 torch stack\n",
        "pip('install',\n",
        "    '--index-url', 'https://download.pytorch.org/whl/cu121',\n",
        "    '--extra-index-url', 'https://pypi.org/simple',\n",
        "    'torch==2.4.1', 'torchvision==0.19.1', 'torchaudio==2.4.1')\n",
        "\n",
        "# Constraints to pin torch versions for later installs\n",
        "from pathlib import Path\n",
        "Path('constraints.txt').write_text('torch==2.4.1\\ntorchvision==0.19.1\\ntorchaudio==2.4.1\\n')\n",
        "\n",
        "# Install NLP deps honoring constraints\n",
        "pip('install', '-c', 'constraints.txt',\n",
        "    'transformers==4.44.2', 'accelerate==0.34.2',\n",
        "    'datasets==2.21.0', 'evaluate==0.4.2',\n",
        "    'sentencepiece', 'scikit-learn', 'tqdm',\n",
        "    '--upgrade-strategy', 'only-if-needed')\n",
        "\n",
        "import torch\n",
        "print('torch:', torch.__version__, 'built CUDA:', getattr(torch.version, 'cuda', None), flush=True)\n",
        "print('CUDA available:', torch.cuda.is_available(), flush=True)\n",
        "assert str(getattr(torch.version,'cuda','')).startswith('12.1'), f'Wrong CUDA build: {torch.version.cuda}'\n",
        "assert torch.cuda.is_available(), 'CUDA not available'\n",
        "print('GPU:', torch.cuda.get_device_name(0), flush=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torch as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torchvision as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> pip install --index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://pypi.org/simple torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torchaudio as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121, https://pypi.org/simple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (799.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 MB 415.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.19.1\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.1/7.1 MB 517.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.4/3.4 MB 351.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 252.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 259.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 242.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 304.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 269.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 336.9 MB/s eta 0:00:00\nCollecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 504.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 252.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 487.1 MB/s eta 0:00:00\nCollecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 319.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 253.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 302.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 272.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 259.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 511.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fsspec\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 496.8 MB/s eta 0:00:00\nCollecting typing-extensions>=4.8.0\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 391.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow!=8.3.*,>=5.3.0\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 156.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 313.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 297.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\nCollecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 567.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.3 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pillow-11.3.0 sympy-1.14.0 torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121 triton-3.0.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> pip install -c constraints.txt transformers==4.44.2 accelerate==0.34.2 datasets==2.21.0 evaluate==0.4.2 sentencepiece scikit-learn tqdm --upgrade-strategy only-if-needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.44.2\n  Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.5/9.5 MB 132.5 MB/s eta 0:00:00\nCollecting accelerate==0.34.2\n  Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 324.4/324.4 KB 513.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets==2.21.0\n  Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 527.3/527.3 KB 533.4 MB/s eta 0:00:00\nCollecting evaluate==0.4.2\n  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 84.1/84.1 KB 453.4 MB/s eta 0:00:00\nCollecting sentencepiece\n  Downloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.4/1.4 MB 389.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.7/9.7 MB 540.5 MB/s eta 0:00:00\nCollecting tqdm\n  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.5/78.5 KB 406.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy>=1.17\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 511.2 MB/s eta 0:00:00\nCollecting requests\n  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.7/64.7 KB 378.6 MB/s eta 0:00:00\nCollecting huggingface-hub<1.0,>=0.23.2\n  Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 564.3/564.3 KB 520.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyyaml>=5.1\n  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 806.6/806.6 KB 517.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting regex!=2019.12.17\n  Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 KB 512.1 MB/s eta 0:00:00\nCollecting packaging>=20.0\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 408.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting safetensors>=0.4.1\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 485.8/485.8 KB 506.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers<0.20,>=0.19\n  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.6/3.6 MB 66.6 MB/s eta 0:00:00\nCollecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psutil\n  Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 291.2/291.2 KB 495.8 MB/s eta 0:00:00\nCollecting torch>=1.10.0\n  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 797.1/797.1 MB 342.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyarrow>=15.0.0\n  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 42.8/42.8 MB 267.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas\n  Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.8/12.8 MB 571.3 MB/s eta 0:00:00\nCollecting fsspec[http]<=2024.6.1,>=2023.1.0\n  Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 177.6/177.6 KB 517.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aiohttp\n  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.7/1.7 MB 565.7 MB/s eta 0:00:00\nCollecting multiprocess\n  Downloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 144.5/144.5 KB 375.6 MB/s eta 0:00:00\nCollecting xxhash\n  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 194.8/194.8 KB 504.8 MB/s eta 0:00:00\nCollecting dill<0.3.9,>=0.3.0\n  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 116.3/116.3 KB 516.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting joblib>=1.2.0\n  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 308.4/308.4 KB 518.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy>=1.8.0\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.9/35.9 MB 321.9 MB/s eta 0:00:00\nCollecting threadpoolctl>=3.1.0\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aiosignal>=1.4.0\n  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\nCollecting frozenlist>=1.1.1\n  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 235.3/235.3 KB 497.3 MB/s eta 0:00:00\nCollecting propcache>=0.2.0\n  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 213.5/213.5 KB 515.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting attrs>=17.3.0\n  Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 63.8/63.8 KB 367.9 MB/s eta 0:00:00\nCollecting aiohappyeyeballs>=2.5.0\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yarl<2.0,>=1.17.0\n  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 349.0/349.0 KB 503.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting multidict<7.0,>=4.5\n  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 246.7/246.7 KB 520.1 MB/s eta 0:00:00\nCollecting typing-extensions>=3.7.4.3\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 396.0 MB/s eta 0:00:00\nCollecting hf-xet<2.0.0,>=1.1.3\n  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.2/3.2 MB 513.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting certifi>=2017.4.17\n  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 161.2/161.2 KB 513.5 MB/s eta 0:00:00\nCollecting idna<4,>=2.5\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 70.4/70.4 KB 436.4 MB/s eta 0:00:00\nCollecting urllib3<3,>=1.21.1\n  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 129.8/129.8 KB 500.2 MB/s eta 0:00:00\nCollecting charset_normalizer<4,>=2\n  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 150.3/150.3 KB 445.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 264.5 MB/s eta 0:00:00\nCollecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 503.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 502.3 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 506.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 512.1 MB/s eta 0:00:00\nCollecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 478.8 MB/s eta 0:00:00\nCollecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 501.2 MB/s eta 0:00:00\nCollecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 493.0 MB/s eta 0:00:00\nCollecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 468.4 MB/s eta 0:00:00\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 496.8 MB/s eta 0:00:00\nCollecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 489.7 MB/s eta 0:00:00\nCollecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 512.0 MB/s eta 0:00:00\nCollecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 505.2 MB/s eta 0:00:00\nCollecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 571.6 MB/s eta 0:00:00\nCollecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 287.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 566.5 MB/s eta 0:00:00\nCollecting multiprocess\n  Downloading multiprocess-0.70.17-py311-none-any.whl (144 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 144.3/144.3 KB 280.0 MB/s eta 0:00:00\n  Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 143.5/143.5 KB 523.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tzdata>=2022.7\n  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 347.8/347.8 KB 552.5 MB/s eta 0:00:00\nCollecting python-dateutil>=2.8.2\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 229.9/229.9 KB 509.3 MB/s eta 0:00:00\nCollecting pytz>=2020.1\n  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 509.2/509.2 KB 568.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting six>=1.5\n  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\nCollecting MarkupSafe>=2.0\n  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\nCollecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 559.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: pytz, mpmath, xxhash, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, sympy, six, sentencepiece, safetensors, regex, pyyaml, pyarrow, psutil, propcache, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, MarkupSafe, joblib, idna, hf-xet, fsspec, frozenlist, filelock, dill, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, triton, scipy, requests, python-dateutil, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, jinja2, aiosignal, scikit-learn, pandas, nvidia-cusolver-cu12, huggingface-hub, aiohttp, torch, tokenizers, transformers, datasets, accelerate, evaluate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.3 accelerate-0.34.2 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 attrs-25.3.0 certifi-2025.8.3 charset_normalizer-3.4.3 datasets-2.21.0 dill-0.3.8 evaluate-0.4.2 filelock-3.19.1 frozenlist-1.7.0 fsspec-2024.6.1 hf-xet-1.1.10 huggingface-hub-0.35.3 idna-3.10 jinja2-3.1.6 joblib-1.5.2 mpmath-1.3.0 multidict-6.6.4 multiprocess-0.70.16 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 packaging-25.0 pandas-2.3.3 propcache-0.3.2 psutil-7.1.0 pyarrow-21.0.0 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.3 regex-2025.9.18 requests-2.32.5 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 sentencepiece-0.2.1 six-1.17.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.19.1 torch-2.4.1 tqdm-4.67.1 transformers-4.44.2 triton-3.0.0 typing-extensions-4.15.0 tzdata-2025.2 urllib3-2.5.0 xxhash-3.5.0 yarl-1.20.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/functorch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchgen already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusolver_cu12-11.4.5.107.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/jinja2-3.1.6.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cudnn_cu12-9.1.0.70.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusparse_cu12-12.1.0.106.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/triton-3.0.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock-3.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe-3.0.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx-3.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cublas_cu12-12.1.3.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_cupti_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_nvrtc_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_runtime_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cufft_cu12-11.0.2.54.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_curand_cu12-10.3.2.106.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nccl_cu12-2.20.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvjitlink_cu12-12.9.86.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvtx_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/sympy-1.14.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/isympy.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath-1.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/share already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.4.1+cu121 built CUDA: 12.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: NVIDIA A10-24Q\n"
          ]
        }
      ]
    },
    {
      "id": "b282f7a9-37b7-48b0-a997-562c755c7885",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# QA setup: tokenizer, CV folds, and alignment sanity checks\n",
        "import re, numpy as np, random, time\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "SEED = 42\n",
        "def seed_everything(seed=SEED):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "seed_everything()\n",
        "\n",
        "# Drop nulls for label creation\n",
        "train_clean = train.dropna(subset=['text','selected_text']).reset_index(drop=True)\n",
        "\n",
        "# Create stratified folds by sentiment\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "folds = np.full(len(train_clean), -1, dtype=int)\n",
        "for f, (_, val_idx) in enumerate(skf.split(train_clean, train_clean['sentiment'])):\n",
        "    folds[val_idx] = f\n",
        "train_clean['fold'] = folds\n",
        "print('Folds assigned:', np.bincount(folds))\n",
        "\n",
        "model_name = 'roberta-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "IGNORE_INDEX = -100\n",
        "def tokenize_and_align(tokenizer, text, sentiment, selected_text=None, max_len=128):\n",
        "    enc = tokenizer(\n",
        "        str(sentiment), str(text),\n",
        "        max_length=max_len, padding='max_length',\n",
        "        truncation='only_second', add_special_tokens=True,\n",
        "        return_offsets_mapping=True, return_attention_mask=True\n",
        "    )\n",
        "    offsets = enc['offset_mapping']\n",
        "    seq_ids = enc.sequence_ids()\n",
        "    ctx_idx = [i for i, sid in enumerate(seq_ids) if sid == 1]\n",
        "    start_pos = end_pos = IGNORE_INDEX\n",
        "    if isinstance(selected_text, str) and isinstance(text, str) and ctx_idx:\n",
        "        # find all occurrences; choose the one with max token-char overlap\n",
        "        matches = [m.start() for m in re.finditer(re.escape(selected_text), text)]\n",
        "        if matches:\n",
        "            best = None\n",
        "            for st_char in matches:\n",
        "                ed_char = st_char + len(selected_text)\n",
        "                overlap = 0\n",
        "                for i in ctx_idx:\n",
        "                    a, b = offsets[i]\n",
        "                    overlap += max(0, min(ed_char, b) - max(st_char, a))\n",
        "                if (best is None) or (overlap > best[0]):\n",
        "                    best = (overlap, st_char, ed_char)\n",
        "            _, st_char, ed_char = best\n",
        "            chosen = [i for i in ctx_idx\n",
        "                      if offsets[i][1] > offsets[i][0] and\n",
        "                         max(offsets[i][0], st_char) < min(offsets[i][1], ed_char)]\n",
        "            if chosen:\n",
        "                start_pos, end_pos = chosen[0], chosen[-1]\n",
        "            else:\n",
        "                dists = [(abs(offsets[i][0]-st_char)+abs(offsets[i][1]-ed_char), i)\n",
        "                         for i in ctx_idx if offsets[i][1] > offsets[i][0]]\n",
        "                if dists:\n",
        "                    start_pos = end_pos = min(dists)[1]\n",
        "        # else: keep IGNORE_INDEX (rare)\n",
        "    enc['start_positions'] = start_pos\n",
        "    enc['end_positions'] = end_pos\n",
        "    return enc, ctx_idx\n",
        "\n",
        "# Sanity check a few samples per sentiment and fold\n",
        "def reconstruct_from_positions(text, offsets, i, j):\n",
        "    if i < 0 or j < 0: return ''\n",
        "    s_char, e_char = offsets[i][0], offsets[j][1]\n",
        "    return text[s_char:e_char]\n",
        "\n",
        "samples_checked = 0\n",
        "for s in ['neutral','positive','negative']:\n",
        "    df_s = train_clean[train_clean.sentiment==s].head(2)\n",
        "    for _, r in df_s.iterrows():\n",
        "        enc, ctx = tokenize_and_align(tokenizer, r['text'], r['sentiment'], r['selected_text'], max_len=128)\n",
        "        pred_span = reconstruct_from_positions(r['text'], enc['offset_mapping'], enc['start_positions'], enc['end_positions'])\n",
        "        print({'sent': r['sentiment'], 'text_snip': r['text'][:60], 'gold': r['selected_text'], 'recon': pred_span})\n",
        "        samples_checked += 1\n",
        "print('Alignment samples printed:', samples_checked)\n",
        "\n",
        "print('Setup OK. Next: implement model/train loop with 5-fold QA and OOF logging.', flush=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folds assigned: [4947 4946 4946 4946 4946]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sent': 'neutral', 'text_snip': '  not Pimm`s in a can?', 'gold': 'not Pimm`s in a can?', 'recon': 'not Pimm`s in a can?'}\n{'sent': 'neutral', 'text_snip': ' i would, but i don`t know how to do it from the phone...', 'gold': 'i would, but i don`t know how to do it from the phone...', 'recon': 'i would, but i don`t know how to do it from the phone...'}\n{'sent': 'positive', 'text_snip': 'happy mothers day to all   im off to spend the day with my f', 'gold': 'happy', 'recon': 'happy'}\n{'sent': 'positive', 'text_snip': ' one of my favorite quotes ever', 'gold': 'favorite', 'recon': 'favorite'}\n{'sent': 'negative', 'text_snip': 'eating breakfast  getting ready to go to school ;(', 'gold': 'eating breakfast  getting ready to go to school ;(', 'recon': 'eating breakfast  getting ready to go to school ;('}\n{'sent': 'negative', 'text_snip': 'Going to fold laundry and then hit the sack. I have boring s', 'gold': 'I have boring saturday evenings', 'recon': 'I have boring saturday evenings'}\nAlignment samples printed: 6\nSetup OK. Next: implement model/train loop with 5-fold QA and OOF logging.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "id": "73404e09-5e63-4d24-b13b-58246593cfad",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 5-fold QA training with RoBERTa-base, AMP, OOF logging/caching\n",
        "import os, gc, math, json, time, numpy as np, pandas as pd, torch\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Dict, Any, List, Tuple\n",
        "from transformers import (\n",
        "    AutoModelForQuestionAnswering,\n",
        "    Trainer, TrainingArguments,\n",
        "    default_data_collator,\n",
        "    get_cosine_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 3\n",
        "LR = 2e-5\n",
        "WARMUP_RATIO = 0.1\n",
        "WEIGHT_DECAY = 0.01\n",
        "GRAD_CLIP = 1.0\n",
        "\n",
        "def build_encodings(df: pd.DataFrame, include_labels: bool = True):\n",
        "    enc_list = []\n",
        "    for i, r in df.iterrows():\n",
        "        sel = r['selected_text'] if include_labels else None\n",
        "        enc, ctx_idx = tokenize_and_align(tokenizer, r['text'], r['sentiment'], sel, max_len=MAX_LEN)\n",
        "        # Persist needed fields for eval/decoding\n",
        "        enc['text'] = r['text']\n",
        "        enc['sentiment'] = r['sentiment']\n",
        "        enc_list.append(enc)\n",
        "    # Stack into arrays\n",
        "    keys = enc_list[0].keys()\n",
        "    out: Dict[str, Any] = {}\n",
        "    for k in keys:\n",
        "        vals = [e[k] for e in enc_list]\n",
        "        if k in ('text','sentiment'):\n",
        "            out[k] = vals\n",
        "        else:\n",
        "            out[k] = np.array(vals, dtype=object if k=='offset_mapping' else None)\n",
        "    return out\n",
        "\n",
        "class QADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, enc: Dict[str, Any], with_labels: bool):\n",
        "        self.enc = enc\n",
        "        self.with_labels = with_labels\n",
        "    def __len__(self):\n",
        "        return len(self.enc['input_ids'])\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            'input_ids': torch.tensor(self.enc['input_ids'][idx], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.enc['attention_mask'][idx], dtype=torch.long),\n",
        "        }\n",
        "        if self.with_labels:\n",
        "            item['start_positions'] = torch.tensor(self.enc['start_positions'][idx], dtype=torch.long)\n",
        "            item['end_positions'] = torch.tensor(self.enc['end_positions'][idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "def get_sequence_ids_for_pair(sentiment: str, text: str):\n",
        "    tmp = tokenizer(str(sentiment), str(text),\n",
        "                    max_length=MAX_LEN, padding='max_length', truncation='only_second',\n",
        "                    add_special_tokens=True, return_offsets_mapping=True)\n",
        "    return tmp.sequence_ids(), tmp['offset_mapping']\n",
        "\n",
        "def decode_one(start_logits, end_logits, offsets, sequence_ids, text, sentiment, length_penalty_per_char=0.0, low_conf_threshold=None):\n",
        "    if str(sentiment).strip().lower() == 'neutral':\n",
        "        return text\n",
        "    ctx = [i for i, sid in enumerate(sequence_ids) if sid == 1]\n",
        "    if not ctx:\n",
        "        return text\n",
        "    sl = np.asarray(start_logits, dtype=np.float32)\n",
        "    el = np.asarray(end_logits, dtype=np.float32)\n",
        "    mask = np.zeros_like(sl, dtype=bool)\n",
        "    mask[np.array(ctx)] = True\n",
        "    sl[~mask] = -np.inf; el[~mask] = -np.inf\n",
        "    best_score = -1e9; bi = bj = ctx[0]\n",
        "    for i in ctx:\n",
        "        for j in ctx:\n",
        "            if j < i: continue\n",
        "            span_len_chars = offsets[j][1] - offsets[i][0]\n",
        "            score = sl[i] + el[j] - length_penalty_per_char * span_len_chars\n",
        "            if score > best_score:\n",
        "                best_score, bi, bj = score, i, j\n",
        "    if (low_conf_threshold is not None) and (best_score < low_conf_threshold):\n",
        "        return text\n",
        "    s_char, e_char = offsets[bi][0], offsets[bj][1]\n",
        "    pred = text[s_char:e_char].strip()\n",
        "    return pred if pred else text\n",
        "\n",
        "def jaccard_batch(trues: List[str], preds: List[str]):\n",
        "    return float(np.mean([jaccard(t, p) for t, p in zip(trues, preds)]))\n",
        "\n",
        "oof_rows = []\n",
        "start_logits_folds = []\n",
        "end_logits_folds = []\n",
        "\n",
        "for fold in range(5):\n",
        "    t0 = time.time()\n",
        "    print(f'\\n===== Fold {fold} =====', flush=True)\n",
        "    trn_df = train_clean[train_clean.fold != fold].reset_index(drop=True)\n",
        "    val_df = train_clean[train_clean.fold == fold].reset_index(drop=True)\n",
        "    print('Train/Val sizes:', len(trn_df), len(val_df))\n",
        "\n",
        "    trn_enc = build_encodings(trn_df, include_labels=True)\n",
        "    val_enc = build_encodings(val_df, include_labels=True)\n",
        "\n",
        "    train_ds = QADataset(trn_enc, with_labels=True)\n",
        "    val_ds = QADataset(val_enc, with_labels=True)\n",
        "\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained('roberta-base')\n",
        "\n",
        "    total_steps = math.ceil(len(train_ds) / (BATCH_SIZE*2)) * EPOCHS\n",
        "\n",
        "    # Compute metrics closure will do decoding on val set\n",
        "    def compute_metrics(eval_pred):\n",
        "        start_logits, end_logits = eval_pred.predictions\n",
        "        preds = []\n",
        "        trues = list(val_df['selected_text'].astype(str).values)\n",
        "        for i in range(len(val_df)):\n",
        "            text = val_df.iloc[i]['text']\n",
        "            sentiment = val_df.iloc[i]['sentiment']\n",
        "            seq_ids, offs = get_sequence_ids_for_pair(sentiment, text)\n",
        "            pred_text = decode_one(start_logits[i], end_logits[i], offs, seq_ids, text, sentiment,\n",
        "                                   length_penalty_per_char=0.003, low_conf_threshold=None)\n",
        "            preds.append(pred_text)\n",
        "        score = jaccard_batch(trues, preds)\n",
        "        return {'jaccard': score}\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'./outputs_fold{fold}',\n",
        "        evaluation_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='jaccard',\n",
        "        greater_is_better=True,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=BATCH_SIZE*2,\n",
        "        gradient_accumulation_steps=2,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        fp16=True,\n",
        "        learning_rate=LR,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        warmup_ratio=WARMUP_RATIO,\n",
        "        lr_scheduler_type='cosine',\n",
        "        max_grad_norm=GRAD_CLIP,\n",
        "        dataloader_num_workers=2,\n",
        "        logging_steps=50,\n",
        "        save_total_limit=1,\n",
        "        seed=SEED,\n",
        "        report_to=[]\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=default_data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    train_out = trainer.train()\n",
        "    print('Best model metrics:', train_out.metrics, flush=True)\n",
        "    # Save best model for this fold\n",
        "    trainer.save_model(f'fold{fold}_best')\n",
        "    # Record best checkpoint path\n",
        "    if getattr(trainer.state, 'best_model_checkpoint', None):\n",
        "        with open(f'fold{fold}_best/path.txt', 'w') as f:\n",
        "            f.write(trainer.state.best_model_checkpoint)\n",
        "\n",
        "    # Inference on val to get logits for caching and OOF decode\n",
        "    val_preds = trainer.predict(val_ds)\n",
        "    val_start_logits, val_end_logits = val_preds.predictions\n",
        "    start_logits_folds.append(val_start_logits)\n",
        "    end_logits_folds.append(val_end_logits)\n",
        "\n",
        "    # Decode OOF\n",
        "    val_trues = list(val_df['selected_text'].astype(str).values)\n",
        "    val_preds_text = []\n",
        "    for i in range(len(val_df)):\n",
        "        text = val_df.iloc[i]['text']\n",
        "        sentiment = val_df.iloc[i]['sentiment']\n",
        "        seq_ids, offs = get_sequence_ids_for_pair(sentiment, text)\n",
        "        pred_text = decode_one(val_start_logits[i], val_end_logits[i], offs, seq_ids, text, sentiment,\n",
        "                               length_penalty_per_char=0.003, low_conf_threshold=None)\n",
        "        val_preds_text.append(pred_text)\n",
        "        oof_rows.append({\n",
        "            'textID': val_df.iloc[i]['textID'],\n",
        "            'fold': fold,\n",
        "            'sentiment': sentiment,\n",
        "            'text': text,\n",
        "            'selected_text': val_trues[i],\n",
        "            'pred': pred_text\n",
        "        })\n",
        "    fold_j = jaccard_batch(val_trues, val_preds_text)\n",
        "    print(f'Fold {fold} OOF Jaccard: {fold_j:.5f}; elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "    # Cleanup\n",
        "    del trainer, model, train_ds, val_ds, trn_enc, val_enc\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "# Aggregate OOF\n",
        "oof_df = pd.DataFrame(oof_rows)\n",
        "oof_score = jaccard_batch(oof_df['selected_text'].tolist(), oof_df['pred'].tolist())\n",
        "print(f'OOF Jaccard (all folds): {oof_score:.5f}', flush=True)\n",
        "oof_df.to_csv('oof_roberta_base.csv', index=False)\n",
        "np.save('oof_start_logits_roberta_base.npy', np.concatenate(start_logits_folds, axis=0))\n",
        "np.save('oof_end_logits_roberta_base.npy', np.concatenate(end_logits_folds, axis=0))\n",
        "print('Saved OOF artifacts.')\n",
        "\n",
        "# Predict on test with the best single model per fold and average logits across folds\n",
        "test_df = test.copy().reset_index(drop=True)\n",
        "test_enc_list = []\n",
        "for i, r in test_df.iterrows():\n",
        "    enc = tokenizer(\n",
        "        str(r['sentiment']), str(r['text']),\n",
        "        max_length=MAX_LEN, padding='max_length', truncation='only_second',\n",
        "        add_special_tokens=True, return_offsets_mapping=True, return_attention_mask=True\n",
        "    )\n",
        "    test_enc_list.append(enc)\n",
        "test_input_ids = torch.tensor([e['input_ids'] for e in test_enc_list], dtype=torch.long)\n",
        "test_attention_mask = torch.tensor([e['attention_mask'] for e in test_enc_list], dtype=torch.long)\n",
        "\n",
        "all_fold_test_start = []\n",
        "all_fold_test_end = []\n",
        "for fold in range(5):\n",
        "    print(f'Test inference with fold {fold} checkpoint...', flush=True)\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(f'fold{fold}_best').to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        bs = BATCH_SIZE\n",
        "        starts, ends = [], []\n",
        "        for i in range(0, len(test_df), bs):\n",
        "            input_ids = test_input_ids[i:i+bs].to(device)\n",
        "            attn = test_attention_mask[i:i+bs].to(device)\n",
        "            out = model(input_ids=input_ids, attention_mask=attn)\n",
        "            starts.append(out.start_logits.detach().cpu().numpy())\n",
        "            ends.append(out.end_logits.detach().cpu().numpy())\n",
        "        starts = np.vstack(starts); ends = np.vstack(ends)\n",
        "    # cache per-fold test logits\n",
        "    np.save(f'test_start_fold{fold}.npy', starts)\n",
        "    np.save(f'test_end_fold{fold}.npy', ends)\n",
        "    all_fold_test_start.append(starts)\n",
        "    all_fold_test_end.append(ends)\n",
        "    del model; gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "avg_test_start = np.mean(all_fold_test_start, axis=0)\n",
        "avg_test_end = np.mean(all_fold_test_end, axis=0)\n",
        "\n",
        "# Decode test\n",
        "test_preds = []\n",
        "for i in range(len(test_df)):\n",
        "    text = test_df.iloc[i]['text']\n",
        "    sentiment = test_df.iloc[i]['sentiment']\n",
        "    seq_ids, offs = get_sequence_ids_for_pair(sentiment, text)\n",
        "    pred_text = decode_one(avg_test_start[i], avg_test_end[i], offs, seq_ids, text, sentiment,\n",
        "                           length_penalty_per_char=0.003, low_conf_threshold=None)\n",
        "    test_preds.append(pred_text)\n",
        "sub = pd.DataFrame({'textID': test_df['textID'], 'selected_text': test_preds})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Wrote submission.csv (model). Head:\\n', sub.head().to_string(index=False), flush=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== Fold 0 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val sizes: 19784 4947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='927' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/927 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model metrics: {'train_runtime': 205.9042, 'train_samples_per_second': 288.251, 'train_steps_per_second': 4.502, 'total_flos': 3872417934827520.0, 'train_loss': 1.0689040063654336, 'epoch': 2.9951534733441036}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/78 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0 OOF Jaccard: 0.71027; elapsed 224.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== Fold 1 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val sizes: 19785 4946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='927' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/927 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model metrics: {'train_runtime': 207.682, 'train_samples_per_second': 285.798, 'train_steps_per_second': 4.464, 'total_flos': 3872548583205888.0, 'train_loss': 1.066633819119187, 'epoch': 2.9951534733441036}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/78 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 OOF Jaccard: 0.70796; elapsed 223.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== Fold 2 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val sizes: 19785 4946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='927' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/927 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model metrics: {'train_runtime': 208.9406, 'train_samples_per_second': 284.076, 'train_steps_per_second': 4.437, 'total_flos': 3872548583205888.0, 'train_loss': 1.0800937232847738, 'epoch': 2.9951534733441036}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/78 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 OOF Jaccard: 0.71144; elapsed 224.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== Fold 3 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val sizes: 19785 4946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='927' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/927 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model metrics: {'train_runtime': 208.0027, 'train_samples_per_second': 285.357, 'train_steps_per_second': 4.457, 'total_flos': 3872548583205888.0, 'train_loss': 1.072065964485835, 'epoch': 2.9951534733441036}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/78 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 OOF Jaccard: 0.70509; elapsed 223.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== Fold 4 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val sizes: 19785 4946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='927' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/927 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model metrics: {'train_runtime': 208.9131, 'train_samples_per_second': 284.113, 'train_steps_per_second': 4.437, 'total_flos': 3872548583205888.0, 'train_loss': 1.084695676126655, 'epoch': 2.9951534733441036}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/78 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 OOF Jaccard: 0.70851; elapsed 224.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF Jaccard (all folds): 0.70866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved OOF artifacts.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test inference with fold 0 checkpoint...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test inference with fold 1 checkpoint...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test inference with fold 2 checkpoint...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test inference with fold 3 checkpoint...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test inference with fold 4 checkpoint...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv (model). Head:\n     textID                                                                                       selected_text\n80a1e6bc32                                                                                      I made my wish\n863097735d                                                                                              sucks!\n264cd5277f             tired and didn`t really have an exciting Saturday.  oh well, hope it`s better tomorrow.\nbaee1e6ffc                                                              i`ve been eating cheetos all morning..\n67d06a8dee  haiiii sankQ i`m fineee ima js get a checkup cos my rib hurts LOL idk but i shall be fine ~ thanks\n"
          ]
        }
      ]
    },
    {
      "id": "3d14cce5-2517-4dba-b939-332a350bfb5b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DeBERTa-v3-base 5-fold QA training (expect +0.005\u20130.01 OOF vs RoBERTa), same pipeline\n",
        "import os, gc, math, json, time, numpy as np, pandas as pd, torch\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments, default_data_collator\n",
        "\n",
        "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
        "print('Loading tokenizer/model:', MODEL_NAME, flush=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 3\n",
        "LR = 2e-5\n",
        "WARMUP_RATIO = 0.1\n",
        "WEIGHT_DECAY = 0.01\n",
        "GRAD_CLIP = 1.0\n",
        "\n",
        "def build_encodings_df(df: pd.DataFrame, include_labels: bool = True):\n",
        "    enc_list = []\n",
        "    for _, r in df.iterrows():\n",
        "        sel = r['selected_text'] if include_labels else None\n",
        "        enc, _ = tokenize_and_align(tokenizer, r['text'], r['sentiment'], sel, max_len=MAX_LEN)\n",
        "        enc['text'] = r['text']\n",
        "        enc['sentiment'] = r['sentiment']\n",
        "        enc_list.append(enc)\n",
        "    keys = enc_list[0].keys()\n",
        "    out = {}\n",
        "    for k in keys:\n",
        "        vals = [e[k] for e in enc_list]\n",
        "        if k in ('text','sentiment'): out[k] = vals\n",
        "        else: out[k] = np.array(vals, dtype=object if k=='offset_mapping' else None)\n",
        "    return out\n",
        "\n",
        "class QADataset2(torch.utils.data.Dataset):\n",
        "    def __init__(self, enc, with_labels): self.enc, self.with_labels = enc, with_labels\n",
        "    def __len__(self): return len(self.enc['input_ids'])\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            'input_ids': torch.tensor(self.enc['input_ids'][idx], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.enc['attention_mask'][idx], dtype=torch.long),\n",
        "        }\n",
        "        if self.with_labels:\n",
        "            item['start_positions'] = torch.tensor(self.enc['start_positions'][idx], dtype=torch.long)\n",
        "            item['end_positions'] = torch.tensor(self.enc['end_positions'][idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "def get_seq_ids_offsets(sentiment: str, text: str):\n",
        "    tmp = tokenizer(str(sentiment), str(text), max_length=MAX_LEN, padding='max_length', truncation='only_second',\n",
        "                    add_special_tokens=True, return_offsets_mapping=True)\n",
        "    return tmp.sequence_ids(), tmp['offset_mapping']\n",
        "\n",
        "def decode_span(start_logits, end_logits, offsets, sequence_ids, text, sentiment, length_penalty_per_char=0.003):\n",
        "    if str(sentiment).strip().lower() == 'neutral': return text\n",
        "    ctx = [i for i, sid in enumerate(sequence_ids) if sid == 1]\n",
        "    if not ctx: return text\n",
        "    sl = np.asarray(start_logits, dtype=np.float32); el = np.asarray(end_logits, dtype=np.float32)\n",
        "    mask = np.zeros_like(sl, dtype=bool); mask[np.array(ctx)] = True\n",
        "    sl[~mask] = -np.inf; el[~mask] = -np.inf\n",
        "    best, bi, bj = -1e9, ctx[0], ctx[0]\n",
        "    for i in ctx:\n",
        "        for j in ctx:\n",
        "            if j < i: continue\n",
        "            span_len = offsets[j][1] - offsets[i][0]\n",
        "            sc = sl[i] + el[j] - length_penalty_per_char * span_len\n",
        "            if sc > best: best, bi, bj = sc, i, j\n",
        "    s_char, e_char = offsets[bi][0], offsets[bj][1]\n",
        "    pred = text[s_char:e_char].strip()\n",
        "    return pred if pred else text\n",
        "\n",
        "def jaccard_batch_fast(trues, preds):\n",
        "    return float(np.mean([jaccard(t, p) for t, p in zip(trues, preds)]))\n",
        "\n",
        "oof_rows2, start_logits_folds2, end_logits_folds2 = [], [], []\n",
        "for fold in range(5):\n",
        "    t0 = time.time(); print(f'\\n===== DeBERTa Fold {fold} =====', flush=True)\n",
        "    trn_df = train_clean[train_clean.fold != fold].reset_index(drop=True)\n",
        "    val_df = train_clean[train_clean.fold == fold].reset_index(drop=True)\n",
        "    print('Train/Val sizes:', len(trn_df), len(val_df))\n",
        "\n",
        "    trn_enc = build_encodings_df(trn_df, include_labels=True)\n",
        "    val_enc = build_encodings_df(val_df, include_labels=True)\n",
        "    train_ds = QADataset2(trn_enc, with_labels=True)\n",
        "    val_ds = QADataset2(val_enc, with_labels=True)\n",
        "\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        start_logits, end_logits = eval_pred.predictions\n",
        "        preds, trues = [], list(val_df['selected_text'].astype(str).values)\n",
        "        for i in range(len(val_df)):\n",
        "            text, sentiment = val_df.iloc[i]['text'], val_df.iloc[i]['sentiment']\n",
        "            seq_ids, offs = get_seq_ids_offsets(sentiment, text)\n",
        "            preds.append(decode_span(start_logits[i], end_logits[i], offs, seq_ids, text, sentiment))\n",
        "        return {'jaccard': jaccard_batch_fast(trues, preds)}\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'./outputs_{MODEL_NAME.replace(\"/\",\"_\")}_fold{fold}',\n",
        "        evaluation_strategy='epoch', save_strategy='epoch',\n",
        "        load_best_model_at_end=True, metric_for_best_model='jaccard', greater_is_better=True,\n",
        "        per_device_train_batch_size=BATCH_SIZE, per_device_eval_batch_size=BATCH_SIZE*2,\n",
        "        gradient_accumulation_steps=2, num_train_epochs=EPOCHS, fp16=True,\n",
        "        learning_rate=LR, weight_decay=WEIGHT_DECAY, warmup_ratio=WARMUP_RATIO, lr_scheduler_type='cosine',\n",
        "        max_grad_norm=GRAD_CLIP, dataloader_num_workers=2, logging_steps=50, save_total_limit=1, seed=SEED, report_to=[]\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=val_ds,\n",
        "                      tokenizer=tokenizer, data_collator=default_data_collator, compute_metrics=compute_metrics)\n",
        "    train_out = trainer.train()\n",
        "    print('Best model metrics:', train_out.metrics, flush=True)\n",
        "    save_dir = f'deberta_fold{fold}_best'\n",
        "    trainer.save_model(save_dir)\n",
        "    if getattr(trainer.state, 'best_model_checkpoint', None):\n",
        "        with open(os.path.join(save_dir, 'path.txt'), 'w') as f: f.write(trainer.state.best_model_checkpoint)\n",
        "\n",
        "    val_preds = trainer.predict(val_ds)\n",
        "    vsl, vel = val_preds.predictions\n",
        "    start_logits_folds2.append(vsl); end_logits_folds2.append(vel)\n",
        "\n",
        "    trues = list(val_df['selected_text'].astype(str).values)\n",
        "    preds = []\n",
        "    for i in range(len(val_df)):\n",
        "        text, sentiment = val_df.iloc[i]['text'], val_df.iloc[i]['sentiment']\n",
        "        seq_ids, offs = get_seq_ids_offsets(sentiment, text)\n",
        "        preds.append(decode_span(vsl[i], vel[i], offs, seq_ids, text, sentiment))\n",
        "        oof_rows2.append({\n",
        "            'textID': val_df.iloc[i]['textID'], 'fold': fold, 'sentiment': sentiment, 'text': text,\n",
        "            'selected_text': trues[i], 'pred': preds[-1]\n",
        "        })\n",
        "    fj = jaccard_batch_fast(trues, preds)\n",
        "    print(f'DeBERTa Fold {fold} OOF Jaccard: {fj:.5f}; elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "    del trainer, model, train_ds, val_ds, trn_enc, val_enc\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "oof_df2 = pd.DataFrame(oof_rows2)\n",
        "oof_score2 = jaccard_batch_fast(oof_df2['selected_text'].tolist(), oof_df2['pred'].tolist())\n",
        "print(f'DeBERTa OOF Jaccard (all folds): {oof_score2:.5f}', flush=True)\n",
        "oof_df2.to_csv('oof_deberta_v3_base.csv', index=False)\n",
        "np.save('oof_start_logits_deberta_v3_base.npy', np.concatenate(start_logits_folds2, axis=0))\n",
        "np.save('oof_end_logits_deberta_v3_base.npy', np.concatenate(end_logits_folds2, axis=0))\n",
        "print('Saved DeBERTa OOF artifacts.')\n",
        "\n",
        "# Test inference with best fold checkpoints and average logits\n",
        "test_df = test.copy().reset_index(drop=True)\n",
        "test_enc = [tokenizer(str(r['sentiment']), str(r['text']), max_length=MAX_LEN, padding='max_length',\n",
        "                     truncation='only_second', add_special_tokens=True, return_offsets_mapping=True, return_attention_mask=True)\n",
        "            for _, r in test_df.iterrows()]\n",
        "test_input_ids = torch.tensor([e['input_ids'] for e in test_enc], dtype=torch.long)\n",
        "test_attention_mask = torch.tensor([e['attention_mask'] for e in test_enc], dtype=torch.long)\n",
        "fold_starts, fold_ends = [], []\n",
        "for fold in range(5):\n",
        "    print(f'DeBERTa test inference fold {fold}...', flush=True)\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(f'deberta_fold{fold}_best').to(device); model.eval()\n",
        "    with torch.no_grad():\n",
        "        bs = BATCH_SIZE; starts, ends = [], []\n",
        "        for i in range(0, len(test_df), bs):\n",
        "            out = model(input_ids=test_input_ids[i:i+bs].to(device), attention_mask=test_attention_mask[i:i+bs].to(device))\n",
        "            starts.append(out.start_logits.detach().cpu().numpy()); ends.append(out.end_logits.detach().cpu().numpy())\n",
        "        starts, ends = np.vstack(starts), np.vstack(ends)\n",
        "    np.save(f'deberta_test_start_fold{fold}.npy', starts); np.save(f'deberta_test_end_fold{fold}.npy', ends)\n",
        "    fold_starts.append(starts); fold_ends.append(ends)\n",
        "    del model; gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "avg_st = np.mean(fold_starts, axis=0); avg_en = np.mean(fold_ends, axis=0)\n",
        "test_preds = []\n",
        "for i in range(len(test_df)):\n",
        "    text, sentiment = test_df.iloc[i]['text'], test_df.iloc[i]['sentiment']\n",
        "    seq_ids, offs = get_seq_ids_offsets(sentiment, text)\n",
        "    test_preds.append(decode_span(avg_st[i], avg_en[i], offs, seq_ids, text, sentiment))\n",
        "sub2 = pd.DataFrame({'textID': test_df['textID'], 'selected_text': test_preds})\n",
        "sub2.to_csv('submission_deberta.csv', index=False)\n",
        "print('Wrote submission_deberta.csv Head:\\n', sub2.head().to_string(index=False))\n",
        "print('DeBERTa run complete.')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer/model: microsoft/deberta-v3-base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== DeBERTa Fold 0 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val sizes: 19784 4947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='927' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/927 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model metrics: {'train_runtime': 323.9586, 'train_samples_per_second': 183.209, 'train_steps_per_second': 2.861, 'total_flos': 3872487864360960.0, 'train_loss': 1.077058746848163, 'epoch': 2.9951534733441036}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/78 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa Fold 0 OOF Jaccard: 0.71574; elapsed 344.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== DeBERTa Fold 1 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val sizes: 19785 4946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='927' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/927 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model metrics: {'train_runtime': 326.1992, 'train_samples_per_second': 181.959, 'train_steps_per_second': 2.842, 'total_flos': 3872618515098624.0, 'train_loss': 1.0758275070108128, 'epoch': 2.9951534733441036}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/78 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa Fold 1 OOF Jaccard: 0.70747; elapsed 344.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== DeBERTa Fold 2 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val sizes: 19785 4946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='927' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/927 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model metrics: {'train_runtime': 325.889, 'train_samples_per_second': 182.133, 'train_steps_per_second': 2.845, 'total_flos': 3872618515098624.0, 'train_loss': 1.0792794705980417, 'epoch': 2.9951534733441036}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/78 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa Fold 2 OOF Jaccard: 0.71837; elapsed 347.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== DeBERTa Fold 3 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val sizes: 19785 4946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='927' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/927 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model metrics: {'train_runtime': 326.9882, 'train_samples_per_second': 181.52, 'train_steps_per_second': 2.835, 'total_flos': 3872618515098624.0, 'train_loss': 1.0793483944000934, 'epoch': 2.9951534733441036}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/78 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa Fold 3 OOF Jaccard: 0.70504; elapsed 345.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== DeBERTa Fold 4 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val sizes: 19785 4946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='927' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/927 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model metrics: {'train_runtime': 324.1724, 'train_samples_per_second': 183.097, 'train_steps_per_second': 2.86, 'total_flos': 3872618515098624.0, 'train_loss': 1.0810155693638543, 'epoch': 2.9951534733441036}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/78 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa Fold 4 OOF Jaccard: 0.71348; elapsed 343.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa OOF Jaccard (all folds): 0.71202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved DeBERTa OOF artifacts.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa test inference fold 0...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa test inference fold 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa test inference fold 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa test inference fold 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa test inference fold 4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_deberta.csv Head:\n     textID                                                                                       selected_text\n80a1e6bc32                                                                                                wish\n863097735d                                                                                   gosh today sucks!\n264cd5277f             tired and didn`t really have an exciting Saturday.  oh well, hope it`s better tomorrow.\nbaee1e6ffc                                                              i`ve been eating cheetos all morning..\n67d06a8dee  haiiii sankQ i`m fineee ima js get a checkup cos my rib hurts LOL idk but i shall be fine ~ thanks\nDeBERTa run complete.\n"
          ]
        }
      ]
    },
    {
      "id": "673f4c4a-2953-4816-a228-c4a0ebac96ee",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Post-processing sweep on RoBERTa OOF logits; apply tuned PP to test\n",
        "import numpy as np, pandas as pd, time\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "print('PP tuning on RoBERTa OOF...', flush=True)\n",
        "oof_df = pd.read_csv('oof_roberta_base.csv')\n",
        "oof_sl = np.load('oof_start_logits_roberta_base.npy')\n",
        "oof_el = np.load('oof_end_logits_roberta_base.npy')\n",
        "assert len(oof_df) == oof_sl.shape[0] == oof_el.shape[0], 'Mismatch OOF shapes'\n",
        "\n",
        "rb_tokenizer = AutoTokenizer.from_pretrained('roberta-base', use_fast=True)\n",
        "\n",
        "def seq_ids_offs_rb(sentiment, text, max_len=128):\n",
        "    tmp = rb_tokenizer(str(sentiment), str(text), max_length=max_len, padding='max_length',\n",
        "                       truncation='only_second', add_special_tokens=True, return_offsets_mapping=True)\n",
        "    return tmp.sequence_ids(), tmp['offset_mapping']\n",
        "\n",
        "def decode_with_score(sl, el, offs, seq_ids, text, sentiment, lp=0.0):\n",
        "    if str(sentiment).strip().lower() == 'neutral':\n",
        "        return text, 0.0\n",
        "    ctx = [i for i, sid in enumerate(seq_ids) if sid == 1]\n",
        "    if not ctx: return text, -1e9\n",
        "    sl = np.asarray(sl, dtype=np.float32); el = np.asarray(el, dtype=np.float32)\n",
        "    mask = np.zeros_like(sl, dtype=bool); mask[np.array(ctx)] = True\n",
        "    sl[~mask] = -np.inf; el[~mask] = -np.inf\n",
        "    best, bi, bj = -1e9, ctx[0], ctx[0]\n",
        "    for i in ctx:\n",
        "        for j in ctx:\n",
        "            if j < i: continue\n",
        "            span_len = offs[j][1] - offs[i][0]\n",
        "            sc = sl[i] + el[j] - lp * float(span_len)\n",
        "            if sc > best: best, bi, bj = sc, i, j\n",
        "    s_char, e_char = offs[bi][0], offs[bj][1]\n",
        "    pred = text[s_char:e_char].strip()\n",
        "    if not pred: pred = text\n",
        "    return pred, float(best)\n",
        "\n",
        "def jaccard_mean(y_true, y_pred):\n",
        "    def jac(a,b):\n",
        "        sa, sb = set(str(a).split()), set(str(b).split())\n",
        "        if not sa and not sb: return 1.0\n",
        "        if not sa or not sb: return 0.0\n",
        "        inter = len(sa & sb); union = len(sa | sb)\n",
        "        return inter/union if union else 0.0\n",
        "    return float(np.mean([jac(t,p) for t,p in zip(y_true, y_pred)]))\n",
        "\n",
        "def eval_params(lp, fallback_pct=None):\n",
        "    preds = []; scores = []\n",
        "    t0 = time.time()\n",
        "    for i in range(len(oof_df)):\n",
        "        r = oof_df.iloc[i]\n",
        "        seq_ids, offs = seq_ids_offs_rb(r['sentiment'], r['text'])\n",
        "        pred, sc = decode_with_score(oof_sl[i], oof_el[i], offs, seq_ids, r['text'], r['sentiment'], lp=lp)\n",
        "        preds.append(pred); scores.append(sc)\n",
        "    if fallback_pct is not None:\n",
        "        thr = np.percentile(scores, fallback_pct*100.0)\n",
        "        for i in range(len(oof_df)):\n",
        "            if scores[i] < thr:\n",
        "                # fallback: return full text for any sentiment (neutral already full)\n",
        "                preds[i] = oof_df.iloc[i]['text']\n",
        "    score = jaccard_mean(oof_df['selected_text'].tolist(), preds)\n",
        "    return score, preds, scores\n",
        "\n",
        "grid_lp = [0.0, 0.002, 0.003, 0.004, 0.006, 0.008]\n",
        "grid_fb = [None, 0.02, 0.05]\n",
        "best = (-1, None, None)\n",
        "for lp in grid_lp:\n",
        "    for fb in grid_fb:\n",
        "        s, _, _ = eval_params(lp, fb)\n",
        "        print(f'lp={lp:.4f} fb={fb} -> OOF {s:.5f}')\n",
        "        if s > best[0]: best = (s, lp, fb)\n",
        "print('Best PP:', best)\n",
        "\n",
        "# Apply best PP to test using saved per-fold test logits\n",
        "test_df = pd.read_csv('test.csv')\n",
        "fold_starts = []; fold_ends = []\n",
        "for f in range(5):\n",
        "    fold_starts.append(np.load(f'test_start_fold{f}.npy'))\n",
        "    fold_ends.append(np.load(f'test_end_fold{f}.npy'))\n",
        "avg_st = np.mean(fold_starts, axis=0); avg_en = np.mean(fold_ends, axis=0)\n",
        "\n",
        "best_oof, best_lp, best_fb = best\n",
        "print(f'Using tuned params: lp={best_lp} fb={best_fb}', flush=True)\n",
        "\n",
        "test_preds = []\n",
        "if best_fb is not None:\n",
        "    # derive threshold from OOF distribution and reuse on test: use same percentile of OOF scores as threshold value\n",
        "    _, _, oof_scores = eval_params(best_lp, None)\n",
        "    thr = np.percentile(oof_scores, best_fb*100.0)\n",
        "else:\n",
        "    thr = None\n",
        "for i in range(len(test_df)):\n",
        "    text = test_df.iloc[i]['text']; sent = test_df.iloc[i]['sentiment']\n",
        "    seq_ids, offs = seq_ids_offs_rb(sent, text)\n",
        "    pred, sc = decode_with_score(avg_st[i], avg_en[i], offs, seq_ids, text, sent, lp=best_lp)\n",
        "    if (thr is not None) and (sc < thr):\n",
        "        pred = text\n",
        "    test_preds.append(pred)\n",
        "\n",
        "sub_pp = pd.DataFrame({'textID': test_df['textID'], 'selected_text': test_preds})\n",
        "sub_pp.to_csv('submission_pp_roberta.csv', index=False)\n",
        "print('Wrote submission_pp_roberta.csv. Head:\\n', sub_pp.head().to_string(index=False))\n",
        "print('PP tuning complete.')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP tuning on RoBERTa OOF...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lp=0.0000 fb=None -> OOF 0.70694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lp=0.0000 fb=0.02 -> OOF 0.70694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lp=0.0000 fb=0.05 -> OOF 0.70694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lp=0.0020 fb=None -> OOF 0.70817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lp=0.0020 fb=0.02 -> OOF 0.70817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lp=0.0020 fb=0.05 -> OOF 0.70817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lp=0.0030 fb=None -> OOF 0.70866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lp=0.0030 fb=0.02 -> OOF 0.70866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lp=0.0030 fb=0.05 -> OOF 0.70866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lp=0.0040 fb=None -> OOF 0.70868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lp=0.0040 fb=0.02 -> OOF 0.70868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lp=0.0040 fb=0.05 -> OOF 0.70868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lp=0.0060 fb=None -> OOF 0.70854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lp=0.0060 fb=0.02 -> OOF 0.70854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lp=0.0060 fb=0.05 -> OOF 0.70854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lp=0.0080 fb=None -> OOF 0.70887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lp=0.0080 fb=0.02 -> OOF 0.70887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lp=0.0080 fb=0.05 -> OOF 0.70887\nBest PP: (0.7088670288691741, 0.008, None)\nUsing tuned params: lp=0.008 fb=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_pp_roberta.csv. Head:\n     textID                                                                                       selected_text\n80a1e6bc32                                                                                      I made my wish\n863097735d                                                                                              sucks!\n264cd5277f             tired and didn`t really have an exciting Saturday.  oh well, hope it`s better tomorrow.\nbaee1e6ffc                                                              i`ve been eating cheetos all morning..\n67d06a8dee  haiiii sankQ i`m fineee ima js get a checkup cos my rib hurts LOL idk but i shall be fine ~ thanks\nPP tuning complete.\n"
          ]
        }
      ]
    },
    {
      "id": "23147d89-5b49-4a7d-ad50-4aea9381129d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PP tuning and blending utilities (staged). Run after DeBERTa training finishes.\n",
        "import os, json, time, math, numpy as np, pandas as pd\n",
        "from typing import Dict, Tuple, Optional\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "print('Staging PP tuning and blending code. Do not run until all OOF/test logits are saved.', flush=True)\n",
        "\n",
        "MAX_LEN = 128\n",
        "\n",
        "def jaccard_str(a: str, b: str) -> float:\n",
        "    sa, sb = set(str(a).split()), set(str(b).split())\n",
        "    if not sa and not sb: return 1.0\n",
        "    if not sa or not sb: return 0.0\n",
        "    inter = len(sa & sb); union = len(sa | sb)\n",
        "    return inter/union if union else 0.0\n",
        "\n",
        "def jaccard_mean(y_true, y_pred):\n",
        "    return float(np.mean([jaccard_str(t, p) for t, p in zip(y_true, y_pred)]))\n",
        "\n",
        "def load_oof(model_key: str) -> Tuple[pd.DataFrame, np.ndarray, np.ndarray, AutoTokenizer]:\n",
        "    # model_key in {'roberta_base','deberta_v3_base'}\n",
        "    if model_key == 'roberta_base':\n",
        "        oof_df = pd.read_csv('oof_roberta_base.csv')\n",
        "        sl = np.load('oof_start_logits_roberta_base.npy'); el = np.load('oof_end_logits_roberta_base.npy')\n",
        "        tok = AutoTokenizer.from_pretrained('roberta-base', use_fast=True)\n",
        "    elif model_key == 'deberta_v3_base':\n",
        "        oof_df = pd.read_csv('oof_deberta_v3_base.csv')\n",
        "        sl = np.load('oof_start_logits_deberta_v3_base.npy'); el = np.load('oof_end_logits_deberta_v3_base.npy')\n",
        "        tok = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base', use_fast=True)\n",
        "    else:\n",
        "        raise ValueError('Unknown model_key')\n",
        "    assert len(oof_df) == sl.shape[0] == el.shape[0], f'Shape mismatch for {model_key}'\n",
        "    return oof_df, sl, el, tok\n",
        "\n",
        "def get_seq_ids_offs(tok: AutoTokenizer, sentiment: str, text: str):\n",
        "    tmp = tok(str(sentiment), str(text), max_length=MAX_LEN, padding='max_length', truncation='only_second',\n",
        "              add_special_tokens=True, return_offsets_mapping=True)\n",
        "    return tmp.sequence_ids(), tmp['offset_mapping']\n",
        "\n",
        "def decode_with_score(sl_row: np.ndarray, el_row: np.ndarray, offs, seq_ids, text: str, sentiment: str, lp_char: float) -> Tuple[str, float]:\n",
        "    # Guards and neutral rule\n",
        "    if str(sentiment).strip().lower() == 'neutral':\n",
        "        return text, 0.0\n",
        "    ctx = [i for i, sid in enumerate(seq_ids) if sid == 1]\n",
        "    if not ctx: return text, -1e9\n",
        "    sl = np.asarray(sl_row, dtype=np.float32).copy()\n",
        "    el = np.asarray(el_row, dtype=np.float32).copy()\n",
        "    mask = np.zeros_like(sl, dtype=bool); mask[np.array(ctx)] = True\n",
        "    sl[~mask] = -np.inf; el[~mask] = -np.inf\n",
        "    best, bi, bj = -1e9, ctx[0], ctx[0]\n",
        "    # Ignore zero-length offsets\n",
        "    valid = [i for i in ctx if offs[i][1] > offs[i][0]]\n",
        "    if not valid: return text, -1e9\n",
        "    for i in valid:\n",
        "        for j in valid:\n",
        "            if j < i: continue\n",
        "            span_len = offs[j][1] - offs[i][0]\n",
        "            sc = float(sl[i]) + float(el[j]) - lp_char * float(span_len)\n",
        "            if sc > best:\n",
        "                best, bi, bj = sc, i, j\n",
        "    s_char, e_char = offs[bi][0], offs[bj][1]\n",
        "    pred = text[s_char:e_char].strip()\n",
        "    if not pred: pred = text\n",
        "    return pred, float(best)\n",
        "\n",
        "def eval_oof_with_params(oof_df: pd.DataFrame, sl: np.ndarray, el: np.ndarray, tok: AutoTokenizer,\n",
        "                          lp_by_sent: Dict[str, float], fb_pct_by_sent: Dict[str, Optional[float]]):\n",
        "    preds, scores = [], []\n",
        "    for i in range(len(oof_df)):\n",
        "        r = oof_df.iloc[i]\n",
        "        sent = str(r['sentiment']).strip().lower()\n",
        "        seq_ids, offs = get_seq_ids_offs(tok, r['sentiment'], r['text'])\n",
        "        lp = lp_by_sent.get(sent, 0.0)\n",
        "        pred, sc = decode_with_score(sl[i], el[i], offs, seq_ids, r['text'], r['sentiment'], lp_char=lp)\n",
        "        preds.append(pred); scores.append(sc)\n",
        "    # Apply low-confidence fallback per sentiment for pos/neg only\n",
        "    for s_key in ('positive','negative'):\n",
        "        pct = fb_pct_by_sent.get(s_key, None)\n",
        "        if pct is None: continue\n",
        "        idxs = [i for i in range(len(oof_df)) if str(oof_df.iloc[i]['sentiment']).strip().lower()==s_key]\n",
        "        if not idxs: continue\n",
        "        thr = np.percentile([scores[i] for i in idxs], pct*100.0)\n",
        "        for i in idxs:\n",
        "            if scores[i] < thr:\n",
        "                preds[i] = oof_df.iloc[i]['text']\n",
        "    score = jaccard_mean(oof_df['selected_text'].tolist(), preds)\n",
        "    return score, preds, scores\n",
        "\n",
        "def tune_pp_for_model(model_key: str, lp_grid=(0.0, 0.002, 0.003, 0.004, 0.006, 0.008), fb_grid=(None, 0.02, 0.05)):\n",
        "    oof_df, sl, el, tok = load_oof(model_key)\n",
        "    best = (-1.0, None)\n",
        "    for lp_pos in lp_grid:\n",
        "        for lp_neg in lp_grid:\n",
        "            lp_by_sent = {'positive': lp_pos, 'negative': lp_neg}\n",
        "            for fb_pos in fb_grid:\n",
        "                for fb_neg in fb_grid:\n",
        "                    fb_by_sent = {'positive': fb_pos, 'negative': fb_neg}\n",
        "                    s, _, _ = eval_oof_with_params(oof_df, sl, el, tok, lp_by_sent, fb_by_sent)\n",
        "                    print(f'[{model_key}] lp(pos)={lp_pos:.4f} lp(neg)={lp_neg:.4f} fb(pos)={fb_pos} fb(neg)={fb_neg} -> OOF {s:.5f}')\n",
        "                    if s > best[0]:\n",
        "                        best = (s, (lp_by_sent, fb_by_sent))\n",
        "    print(f'Best {model_key} PP:', best)\n",
        "    return best, (oof_df, sl, el, tok)\n",
        "\n",
        "def load_test_logits(model_key: str) -> Tuple[pd.DataFrame, np.ndarray, np.ndarray, AutoTokenizer]:\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    if model_key == 'roberta_base':\n",
        "        st = np.mean([np.load(f'test_start_fold{f}.npy') for f in range(5)], axis=0)\n",
        "        en = np.mean([np.load(f'test_end_fold{f}.npy') for f in range(5)], axis=0)\n",
        "        tok = AutoTokenizer.from_pretrained('roberta-base', use_fast=True)\n",
        "    elif model_key == 'deberta_v3_base':\n",
        "        st = np.mean([np.load(f'deberta_test_start_fold{f}.npy') for f in range(5)], axis=0)\n",
        "        en = np.mean([np.load(f'deberta_test_end_fold{f}.npy') for f in range(5)], axis=0)\n",
        "        tok = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base', use_fast=True)\n",
        "    else:\n",
        "        raise ValueError('Unknown model_key')\n",
        "    assert st.shape[0] == len(test_df) == en.shape[0], 'Test shape mismatch'\n",
        "    return test_df, st, en, tok\n",
        "\n",
        "def apply_pp_to_test(model_key: str, lp_by_sent: Dict[str, float], fb_pct_by_sent: Dict[str, Optional[float]],\n",
        "                     ref_oof_scores_by_sent: Dict[str, list], out_path: str):\n",
        "    test_df, st, en, tok = load_test_logits(model_key)\n",
        "    # Derive numeric thresholds from OOF score distributions per sentiment (reuse same percentiles)\n",
        "    thr_by_sent: Dict[str, Optional[float]] = {}\n",
        "    for s_key, pct in fb_pct_by_sent.items():\n",
        "        if pct is None: thr_by_sent[s_key] = None\n",
        "        else:\n",
        "            vals = ref_oof_scores_by_sent.get(s_key, [])\n",
        "            thr_by_sent[s_key] = float(np.percentile(vals, pct*100.0)) if len(vals) else None\n",
        "    preds = []\n",
        "    for i in range(len(test_df)):\n",
        "        text, sent = test_df.iloc[i]['text'], test_df.iloc[i]['sentiment']\n",
        "        s_key = str(sent).strip().lower()\n",
        "        if s_key == 'neutral':\n",
        "            preds.append(text); continue\n",
        "        seq_ids, offs = get_seq_ids_offs(tok, sent, text)\n",
        "        pred, sc = decode_with_score(st[i], en[i], offs, seq_ids, text, sent, lp_char=lp_by_sent.get(s_key, 0.0))\n",
        "        thr = thr_by_sent.get(s_key, None)\n",
        "        if (thr is not None) and (sc < thr): pred = text\n",
        "        preds.append(pred)\n",
        "    sub = pd.DataFrame({'textID': test_df['textID'], 'selected_text': preds})\n",
        "    sub.to_csv(out_path, index=False)\n",
        "    print('Wrote', out_path, 'Head:\\n', sub.head().to_string(index=False))\n",
        "\n",
        "def try_blend_oof(w: float,\n",
        "                  oof_rb: Tuple[pd.DataFrame, np.ndarray, np.ndarray, AutoTokenizer],\n",
        "                  oof_deb: Tuple[pd.DataFrame, np.ndarray, np.ndarray, AutoTokenizer],\n",
        "                  lp_grid, fb_grid):\n",
        "    df_r, sl_r, el_r, tok_r = oof_rb\n",
        "    df_d, sl_d, el_d, tok_d = oof_deb\n",
        "    # Sanity: ordering must match; we created folds deterministically so row order of OOFs should match original train_clean order\n",
        "    assert len(df_r)==len(df_d), 'OOF length mismatch between models'\n",
        "    # Use tokenizer of one backbone for decoding; we must use the same tokenization setup we will use at test time.\n",
        "    # Decode with each model's tokenizer produces slightly different offsets; to avoid mismatch, decode using each model's tokenizer separately is not feasible when blending logits.\n",
        "    # Instead, blend logits at token index level requires identical tokenization, which we do not have across backbones.\n",
        "    # Therefore, we blend at score level by aligning via recomputing logits per model during test; for OOF, we approximate by averaging per-position logits after re-tokenization is NOT possible.\n",
        "    # Pragmatic approach: align by using the model whose tokenizer we will decode with for both (not strictly aligned across models).\n",
        "    # Empirically, blending raw logits from different tokenizers is not valid; instead, we will blend predictions at span-score level:\n",
        "    # For each candidate span (i,j) over the chosen tokenizer, compute score as w*s_d + (1-w)*s_r, where s_* = start[i] + end[j].\n",
        "    # To support that, we need start/end logits from both models mapped to the same tokenization. Without re-running models, we cannot remap.\n",
        "    # Hence: switch to submission-time blend by averaging decoded spans is suboptimal; Experts advised blending logits before decode within same tokenization.\n",
        "    # Conclusion: Do NOT attempt cross-tokenizer logit blending offline here. We'll perform blending per-backbone separately and pick DeBERTa as primary,\n",
        "    # and provide optional simple text-level blend (choose shorter/longer span) only if needed.\n",
        "    raise NotImplementedError('Cross-backbone logit blending requires identical tokenization; stage left as future work with re-forward if needed.')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(' Guidance: ',\n",
        "          '\\n1) Run tune_pp_for_model(\"roberta_base\") to get best params and OOF scores per sentiment. Save JSON.',\n",
        "          '\\n2) After DeBERTa finishes and its OOF logits exist, run tune_pp_for_model(\"deberta_v3_base\").',\n",
        "          '\\n3) Apply to test with apply_pp_to_test using thresholds derived from respective OOF.',\n",
        "          '\\n4) For cross-backbone blending at logit level, re-forward both models on test using a shared tokenization is required; otherwise, prefer DeBERTa-only for primary submission.', flush=True)\n",
        "    # Example manual run after training completes:\n",
        "    # best_rb, pack_rb = tune_pp_for_model('roberta_base')\n",
        "    # (score_rb, (lp_rb, fb_rb)) = best_rb\n",
        "    # # Derive OOF score distributions per sentiment for thresholds\n",
        "    # oof_df_rb, sl_rb, el_rb, tok_rb = pack_rb\n",
        "    # _, preds_rb, scores_rb = eval_oof_with_params(oof_df_rb, sl_rb, el_rb, tok_rb, lp_rb, fb_rb)\n",
        "    # scores_by_sent_rb = {'positive': [scores_rb[i] for i in range(len(oof_df_rb)) if str(oof_df_rb.iloc[i]['sentiment']).strip().lower()=='positive'],\n",
        "    #                      'negative': [scores_rb[i] for i in range(len(oof_df_rb)) if str(oof_df_rb.iloc[i]['sentiment']).strip().lower()=='negative']}\n",
        "    # with open('pp_params_roberta.json','w') as f: json.dump({'lp': lp_rb, 'fb': fb_rb, 'oof_score': score_rb}, f)\n",
        "    # apply_pp_to_test('roberta_base', lp_rb, fb_rb, scores_by_sent_rb, 'submission_pp_roberta.csv')\n",
        "\n",
        "    # # After DeBERTa OOF exists:\n",
        "    # best_deb, pack_deb = tune_pp_for_model('deberta_v3_base')\n",
        "    # (score_deb, (lp_deb, fb_deb)) = best_deb\n",
        "    # oof_df_deb, sl_deb, el_deb, tok_deb = pack_deb\n",
        "    # _, preds_deb, scores_deb = eval_oof_with_params(oof_df_deb, sl_deb, el_deb, tok_deb, lp_deb, fb_deb)\n",
        "    # scores_by_sent_deb = {'positive': [scores_deb[i] for i in range(len(oof_df_deb)) if str(oof_df_deb.iloc[i]['sentiment']).strip().lower()=='positive'],\n",
        "    #                       'negative': [scores_deb[i] for i in range(len(oof_df_deb)) if str(oof_df_deb.iloc[i]['sentiment']).strip().lower()=='negative']}\n",
        "    # with open('pp_params_deberta.json','w') as f: json.dump({'lp': lp_deb, 'fb': fb_deb, 'oof_score': score_deb}, f)\n",
        "    # apply_pp_to_test('deberta_v3_base', lp_deb, fb_deb, scores_by_sent_deb, 'submission_pp_deberta.csv')\n",
        "\n",
        "    print('PP utilities ready.')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Staging PP tuning and blending code. Do not run until all OOF/test logits are saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Guidance:  \n1) Run tune_pp_for_model(\"roberta_base\") to get best params and OOF scores per sentiment. Save JSON. \n2) After DeBERTa finishes and its OOF logits exist, run tune_pp_for_model(\"deberta_v3_base\"). \n3) Apply to test with apply_pp_to_test using thresholds derived from respective OOF. \n4) For cross-backbone blending at logit level, re-forward both models on test using a shared tokenization is required; otherwise, prefer DeBERTa-only for primary submission.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP utilities ready.\n"
          ]
        }
      ]
    },
    {
      "id": "b1b24b6b-b117-4c18-89d0-f9d5089d1564",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run PP tuning for DeBERTa, derive per-sentiment thresholds, and write submission\n",
        "import json, numpy as np, pandas as pd\n",
        "print('Tuning PP for DeBERTa-v3-base...', flush=True)\n",
        "best_deb, pack_deb = tune_pp_for_model('deberta_v3_base')\n",
        "(score_deb, (lp_deb, fb_deb)) = best_deb\n",
        "print('Best DeBERTa OOF:', score_deb, 'params:', lp_deb, fb_deb)\n",
        "oof_df_deb, sl_deb, el_deb, tok_deb = pack_deb\n",
        "# Get OOF preds and scores to calibrate thresholds\n",
        "oof_score_eval, preds_deb, scores_deb = eval_oof_with_params(oof_df_deb, sl_deb, el_deb, tok_deb, lp_deb, fb_deb)\n",
        "scores_by_sent_deb = {\n",
        "    'positive': [scores_deb[i] for i in range(len(oof_df_deb)) if str(oof_df_deb.iloc[i]['sentiment']).strip().lower()=='positive'],\n",
        "    'negative': [scores_deb[i] for i in range(len(oof_df_deb)) if str(oof_df_deb.iloc[i]['sentiment']).strip().lower()=='negative']\n",
        "}\n",
        "with open('pp_params_deberta.json','w') as f:\n",
        "    json.dump({'lp': lp_deb, 'fb': fb_deb, 'oof_score': float(score_deb)}, f)\n",
        "print('Saved pp_params_deberta.json')\n",
        "apply_pp_to_test('deberta_v3_base', lp_deb, fb_deb, scores_by_sent_deb, 'submission_pp_deberta.csv')\n",
        "print('DeBERTa PP tuning + submission complete.')\n",
        "\n",
        "# Optionally, save DeBERTa OOF decoded predictions for error analysis\n",
        "pd.DataFrame({\n",
        "    'textID': oof_df_deb['textID'],\n",
        "    'sentiment': oof_df_deb['sentiment'],\n",
        "    'text': oof_df_deb['text'],\n",
        "    'selected_text': oof_df_deb['selected_text'],\n",
        "    'pred': preds_deb\n",
        "}).to_csv('oof_preds_deberta_pp.csv', index=False)\n",
        "print('Saved oof_preds_deberta_pp.csv')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning PP for DeBERTa-v3-base...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0000 fb(pos)=None fb(neg)=None -> OOF 0.71116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0000 fb(pos)=None fb(neg)=0.02 -> OOF 0.71089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0000 fb(pos)=None fb(neg)=0.05 -> OOF 0.71039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0000 fb(pos)=0.02 fb(neg)=None -> OOF 0.71104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0000 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71077\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0000 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0000 fb(pos)=0.05 fb(neg)=None -> OOF 0.71047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0000 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0000 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.70970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0020 fb(pos)=None fb(neg)=None -> OOF 0.71153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0020 fb(pos)=None fb(neg)=0.02 -> OOF 0.71129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0020 fb(pos)=None fb(neg)=0.05 -> OOF 0.71077\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0020 fb(pos)=0.02 fb(neg)=None -> OOF 0.71141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0020 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0020 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0020 fb(pos)=0.05 fb(neg)=None -> OOF 0.71085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0020 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0020 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0030 fb(pos)=None fb(neg)=None -> OOF 0.71147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0030 fb(pos)=None fb(neg)=0.02 -> OOF 0.71125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0030 fb(pos)=None fb(neg)=0.05 -> OOF 0.71068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0030 fb(pos)=0.02 fb(neg)=None -> OOF 0.71134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0030 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0030 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0030 fb(pos)=0.05 fb(neg)=None -> OOF 0.71078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0030 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0030 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.70999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0040 fb(pos)=None fb(neg)=None -> OOF 0.71144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0040 fb(pos)=None fb(neg)=0.02 -> OOF 0.71123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0040 fb(pos)=None fb(neg)=0.05 -> OOF 0.71073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0040 fb(pos)=0.02 fb(neg)=None -> OOF 0.71132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0040 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0040 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0040 fb(pos)=0.05 fb(neg)=None -> OOF 0.71076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0040 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0040 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0060 fb(pos)=None fb(neg)=None -> OOF 0.71111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0060 fb(pos)=None fb(neg)=0.02 -> OOF 0.71093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0060 fb(pos)=None fb(neg)=0.05 -> OOF 0.71052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0060 fb(pos)=0.02 fb(neg)=None -> OOF 0.71099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0060 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71081\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0060 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0060 fb(pos)=0.05 fb(neg)=None -> OOF 0.71043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0060 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0060 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.70984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0080 fb(pos)=None fb(neg)=None -> OOF 0.71117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0080 fb(pos)=None fb(neg)=0.02 -> OOF 0.71087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0080 fb(pos)=None fb(neg)=0.05 -> OOF 0.71049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0080 fb(pos)=0.02 fb(neg)=None -> OOF 0.71105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0080 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0080 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0080 fb(pos)=0.05 fb(neg)=None -> OOF 0.71048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0080 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0000 lp(neg)=0.0080 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.70981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0000 fb(pos)=None fb(neg)=None -> OOF 0.71145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0000 fb(pos)=None fb(neg)=0.02 -> OOF 0.71119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0000 fb(pos)=None fb(neg)=0.05 -> OOF 0.71068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0000 fb(pos)=0.02 fb(neg)=None -> OOF 0.71131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0000 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0000 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0000 fb(pos)=0.05 fb(neg)=None -> OOF 0.71059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0000 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0000 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.70982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0020 fb(pos)=None fb(neg)=None -> OOF 0.71183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0020 fb(pos)=None fb(neg)=0.02 -> OOF 0.71159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0020 fb(pos)=None fb(neg)=0.05 -> OOF 0.71107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0020 fb(pos)=0.02 fb(neg)=None -> OOF 0.71168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0020 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0020 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0020 fb(pos)=0.05 fb(neg)=None -> OOF 0.71096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0020 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0020 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0030 fb(pos)=None fb(neg)=None -> OOF 0.71176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0030 fb(pos)=None fb(neg)=0.02 -> OOF 0.71154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0030 fb(pos)=None fb(neg)=0.05 -> OOF 0.71097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0030 fb(pos)=0.02 fb(neg)=None -> OOF 0.71162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0030 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0030 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0030 fb(pos)=0.05 fb(neg)=None -> OOF 0.71090\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0030 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0030 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0040 fb(pos)=None fb(neg)=None -> OOF 0.71174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0040 fb(pos)=None fb(neg)=0.02 -> OOF 0.71152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0040 fb(pos)=None fb(neg)=0.05 -> OOF 0.71102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0040 fb(pos)=0.02 fb(neg)=None -> OOF 0.71160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0040 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0040 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0040 fb(pos)=0.05 fb(neg)=None -> OOF 0.71088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0040 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0040 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0060 fb(pos)=None fb(neg)=None -> OOF 0.71141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0060 fb(pos)=None fb(neg)=0.02 -> OOF 0.71122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0060 fb(pos)=None fb(neg)=0.05 -> OOF 0.71082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0060 fb(pos)=0.02 fb(neg)=None -> OOF 0.71127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0060 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0060 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0060 fb(pos)=0.05 fb(neg)=None -> OOF 0.71055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0060 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0060 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.70996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0080 fb(pos)=None fb(neg)=None -> OOF 0.71146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0080 fb(pos)=None fb(neg)=0.02 -> OOF 0.71117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0080 fb(pos)=None fb(neg)=0.05 -> OOF 0.71078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0080 fb(pos)=0.02 fb(neg)=None -> OOF 0.71132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0080 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0080 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0080 fb(pos)=0.05 fb(neg)=None -> OOF 0.71060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0080 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0020 lp(neg)=0.0080 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.70992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0000 fb(pos)=None fb(neg)=None -> OOF 0.71171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0000 fb(pos)=None fb(neg)=0.02 -> OOF 0.71145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0000 fb(pos)=None fb(neg)=0.05 -> OOF 0.71094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0000 fb(pos)=0.02 fb(neg)=None -> OOF 0.71159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0000 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0000 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0000 fb(pos)=0.05 fb(neg)=None -> OOF 0.71088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0000 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0000 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0020 fb(pos)=None fb(neg)=None -> OOF 0.71209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0020 fb(pos)=None fb(neg)=0.02 -> OOF 0.71185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0020 fb(pos)=None fb(neg)=0.05 -> OOF 0.71133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0020 fb(pos)=0.02 fb(neg)=None -> OOF 0.71196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0020 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0020 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0020 fb(pos)=0.05 fb(neg)=None -> OOF 0.71125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0020 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0020 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0030 fb(pos)=None fb(neg)=None -> OOF 0.71202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0030 fb(pos)=None fb(neg)=0.02 -> OOF 0.71180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0030 fb(pos)=None fb(neg)=0.05 -> OOF 0.71123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0030 fb(pos)=0.02 fb(neg)=None -> OOF 0.71190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0030 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0030 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0030 fb(pos)=0.05 fb(neg)=None -> OOF 0.71118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0030 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0030 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0040 fb(pos)=None fb(neg)=None -> OOF 0.71200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0040 fb(pos)=None fb(neg)=0.02 -> OOF 0.71178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0040 fb(pos)=None fb(neg)=0.05 -> OOF 0.71128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0040 fb(pos)=0.02 fb(neg)=None -> OOF 0.71188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0040 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0040 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0040 fb(pos)=0.05 fb(neg)=None -> OOF 0.71116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0040 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0040 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0060 fb(pos)=None fb(neg)=None -> OOF 0.71167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0060 fb(pos)=None fb(neg)=0.02 -> OOF 0.71148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0060 fb(pos)=None fb(neg)=0.05 -> OOF 0.71108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0060 fb(pos)=0.02 fb(neg)=None -> OOF 0.71154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0060 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0060 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0060 fb(pos)=0.05 fb(neg)=None -> OOF 0.71083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0060 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0060 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0080 fb(pos)=None fb(neg)=None -> OOF 0.71172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0080 fb(pos)=None fb(neg)=0.02 -> OOF 0.71143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0080 fb(pos)=None fb(neg)=0.05 -> OOF 0.71104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0080 fb(pos)=0.02 fb(neg)=None -> OOF 0.71160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0080 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0080 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0080 fb(pos)=0.05 fb(neg)=None -> OOF 0.71089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0080 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0030 lp(neg)=0.0080 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0000 fb(pos)=None fb(neg)=None -> OOF 0.71187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0000 fb(pos)=None fb(neg)=0.02 -> OOF 0.71160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0000 fb(pos)=None fb(neg)=0.05 -> OOF 0.71110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0000 fb(pos)=0.02 fb(neg)=None -> OOF 0.71179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0000 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0000 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0000 fb(pos)=0.05 fb(neg)=None -> OOF 0.71101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0000 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0000 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0020 fb(pos)=None fb(neg)=None -> OOF 0.71224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0020 fb(pos)=None fb(neg)=0.02 -> OOF 0.71200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0020 fb(pos)=None fb(neg)=0.05 -> OOF 0.71148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0020 fb(pos)=0.02 fb(neg)=None -> OOF 0.71216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0020 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0020 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0020 fb(pos)=0.05 fb(neg)=None -> OOF 0.71138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0020 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0020 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0030 fb(pos)=None fb(neg)=None -> OOF 0.71218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0030 fb(pos)=None fb(neg)=0.02 -> OOF 0.71196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0030 fb(pos)=None fb(neg)=0.05 -> OOF 0.71139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0030 fb(pos)=0.02 fb(neg)=None -> OOF 0.71209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0030 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0030 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0030 fb(pos)=0.05 fb(neg)=None -> OOF 0.71132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0030 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0030 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0040 fb(pos)=None fb(neg)=None -> OOF 0.71216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0040 fb(pos)=None fb(neg)=0.02 -> OOF 0.71194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0040 fb(pos)=None fb(neg)=0.05 -> OOF 0.71144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0040 fb(pos)=0.02 fb(neg)=None -> OOF 0.71207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0040 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0040 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0040 fb(pos)=0.05 fb(neg)=None -> OOF 0.71129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0040 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0040 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0060 fb(pos)=None fb(neg)=None -> OOF 0.71183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0060 fb(pos)=None fb(neg)=0.02 -> OOF 0.71164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0060 fb(pos)=None fb(neg)=0.05 -> OOF 0.71124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0060 fb(pos)=0.02 fb(neg)=None -> OOF 0.71174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0060 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0060 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0060 fb(pos)=0.05 fb(neg)=None -> OOF 0.71096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0060 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0060 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0080 fb(pos)=None fb(neg)=None -> OOF 0.71188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0080 fb(pos)=None fb(neg)=0.02 -> OOF 0.71159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0080 fb(pos)=None fb(neg)=0.05 -> OOF 0.71120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0080 fb(pos)=0.02 fb(neg)=None -> OOF 0.71180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0080 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0080 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0080 fb(pos)=0.05 fb(neg)=None -> OOF 0.71102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0080 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0040 lp(neg)=0.0080 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0000 fb(pos)=None fb(neg)=None -> OOF 0.71185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0000 fb(pos)=None fb(neg)=0.02 -> OOF 0.71159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0000 fb(pos)=None fb(neg)=0.05 -> OOF 0.71108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0000 fb(pos)=0.02 fb(neg)=None -> OOF 0.71159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0000 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0000 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71081\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0000 fb(pos)=0.05 fb(neg)=None -> OOF 0.71094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0000 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0000 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0020 fb(pos)=None fb(neg)=None -> OOF 0.71223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0020 fb(pos)=None fb(neg)=0.02 -> OOF 0.71199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0020 fb(pos)=None fb(neg)=0.05 -> OOF 0.71147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0020 fb(pos)=0.02 fb(neg)=None -> OOF 0.71196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0020 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0020 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0020 fb(pos)=0.05 fb(neg)=None -> OOF 0.71131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0020 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0020 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0030 fb(pos)=None fb(neg)=None -> OOF 0.71216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0030 fb(pos)=None fb(neg)=0.02 -> OOF 0.71194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0030 fb(pos)=None fb(neg)=0.05 -> OOF 0.71137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0030 fb(pos)=0.02 fb(neg)=None -> OOF 0.71190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0030 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0030 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0030 fb(pos)=0.05 fb(neg)=None -> OOF 0.71125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0030 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0030 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0040 fb(pos)=None fb(neg)=None -> OOF 0.71214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0040 fb(pos)=None fb(neg)=0.02 -> OOF 0.71192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0040 fb(pos)=None fb(neg)=0.05 -> OOF 0.71142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0040 fb(pos)=0.02 fb(neg)=None -> OOF 0.71187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0040 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0040 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0040 fb(pos)=0.05 fb(neg)=None -> OOF 0.71122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0040 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0040 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0060 fb(pos)=None fb(neg)=None -> OOF 0.71181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0060 fb(pos)=None fb(neg)=0.02 -> OOF 0.71162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0060 fb(pos)=None fb(neg)=0.05 -> OOF 0.71122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0060 fb(pos)=0.02 fb(neg)=None -> OOF 0.71154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0060 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0060 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0060 fb(pos)=0.05 fb(neg)=None -> OOF 0.71089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0060 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0060 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71030\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0080 fb(pos)=None fb(neg)=None -> OOF 0.71186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0080 fb(pos)=None fb(neg)=0.02 -> OOF 0.71157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0080 fb(pos)=None fb(neg)=0.05 -> OOF 0.71118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0080 fb(pos)=0.02 fb(neg)=None -> OOF 0.71160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0080 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0080 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0080 fb(pos)=0.05 fb(neg)=None -> OOF 0.71095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0080 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0060 lp(neg)=0.0080 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0000 fb(pos)=None fb(neg)=None -> OOF 0.71201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0000 fb(pos)=None fb(neg)=0.02 -> OOF 0.71175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0000 fb(pos)=None fb(neg)=0.05 -> OOF 0.71124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0000 fb(pos)=0.02 fb(neg)=None -> OOF 0.71173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0000 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0000 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0000 fb(pos)=0.05 fb(neg)=None -> OOF 0.71110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0000 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0000 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0020 fb(pos)=None fb(neg)=None -> OOF 0.71239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0020 fb(pos)=None fb(neg)=0.02 -> OOF 0.71215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0020 fb(pos)=None fb(neg)=0.05 -> OOF 0.71163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0020 fb(pos)=0.02 fb(neg)=None -> OOF 0.71210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0020 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0020 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0020 fb(pos)=0.05 fb(neg)=None -> OOF 0.71147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0020 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0020 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0030 fb(pos)=None fb(neg)=None -> OOF 0.71232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0030 fb(pos)=None fb(neg)=0.02 -> OOF 0.71210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0030 fb(pos)=None fb(neg)=0.05 -> OOF 0.71153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0030 fb(pos)=0.02 fb(neg)=None -> OOF 0.71203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0030 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0030 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0030 fb(pos)=0.05 fb(neg)=None -> OOF 0.71140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0030 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0030 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0040 fb(pos)=None fb(neg)=None -> OOF 0.71230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0040 fb(pos)=None fb(neg)=0.02 -> OOF 0.71208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0040 fb(pos)=None fb(neg)=0.05 -> OOF 0.71158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0040 fb(pos)=0.02 fb(neg)=None -> OOF 0.71201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0040 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0040 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0040 fb(pos)=0.05 fb(neg)=None -> OOF 0.71138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0040 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0040 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0060 fb(pos)=None fb(neg)=None -> OOF 0.71197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0060 fb(pos)=None fb(neg)=0.02 -> OOF 0.71178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0060 fb(pos)=None fb(neg)=0.05 -> OOF 0.71138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0060 fb(pos)=0.02 fb(neg)=None -> OOF 0.71168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0060 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0060 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0060 fb(pos)=0.05 fb(neg)=None -> OOF 0.71105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0060 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0060 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0080 fb(pos)=None fb(neg)=None -> OOF 0.71202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0080 fb(pos)=None fb(neg)=0.02 -> OOF 0.71173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0080 fb(pos)=None fb(neg)=0.05 -> OOF 0.71134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0080 fb(pos)=0.02 fb(neg)=None -> OOF 0.71174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0080 fb(pos)=0.02 fb(neg)=0.02 -> OOF 0.71144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0080 fb(pos)=0.02 fb(neg)=0.05 -> OOF 0.71106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0080 fb(pos)=0.05 fb(neg)=None -> OOF 0.71111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0080 fb(pos)=0.05 fb(neg)=0.02 -> OOF 0.71081\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[deberta_v3_base] lp(pos)=0.0080 lp(neg)=0.0080 fb(pos)=0.05 fb(neg)=0.05 -> OOF 0.71043\nBest deberta_v3_base PP: (0.712386302687562, ({'positive': 0.008, 'negative': 0.002}, {'positive': None, 'negative': None}))\nBest DeBERTa OOF: 0.712386302687562 params: {'positive': 0.008, 'negative': 0.002} {'positive': None, 'negative': None}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved pp_params_deberta.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_pp_deberta.csv Head:\n     textID                                                                                       selected_text\n80a1e6bc32                                                                                                wish\n863097735d                                                                                   gosh today sucks!\n264cd5277f             tired and didn`t really have an exciting Saturday.  oh well, hope it`s better tomorrow.\nbaee1e6ffc                                                              i`ve been eating cheetos all morning..\n67d06a8dee  haiiii sankQ i`m fineee ima js get a checkup cos my rib hurts LOL idk but i shall be fine ~ thanks\nDeBERTa PP tuning + submission complete.\nSaved oof_preds_deberta_pp.csv\n"
          ]
        }
      ]
    },
    {
      "id": "bb9c2798-fefa-4c11-b67e-6b3e08b71920",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set final submission from DeBERTa PP\n",
        "import pandas as pd, os\n",
        "src = 'submission_pp_deberta.csv'\n",
        "dst = 'submission.csv'\n",
        "assert os.path.exists(src), f'Missing {src}. Run DeBERTa PP tuning first.'\n",
        "pd.read_csv(src).to_csv(dst, index=False)\n",
        "print(f'Wrote {dst} from {src}. Head:')\n",
        "print(pd.read_csv(dst).head().to_string(index=False))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv from submission_pp_deberta.csv. Head:\n    textID                                                                                       selected_text\n80a1e6bc32                                                                                                wish\n863097735d                                                                                   gosh today sucks!\n264cd5277f             tired and didn`t really have an exciting Saturday.  oh well, hope it`s better tomorrow.\nbaee1e6ffc                                                              i`ve been eating cheetos all morning..\n67d06a8dee  haiiii sankQ i`m fineee ima js get a checkup cos my rib hurts LOL idk but i shall be fine ~ thanks\n"
          ]
        }
      ]
    },
    {
      "id": "e2ffe000-18ab-4990-8e7b-4bc8924dc9c9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# OOF-calibrated chooser ensemble: pick model with higher standardized span score per row\n",
        "import numpy as np, pandas as pd, json\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "print('Building chooser ensemble (OOF-calibrated z-scores)...', flush=True)\n",
        "\n",
        "# Load OOF for both models and decode with tuned PP to get per-row scores\n",
        "rb_oof = pd.read_csv('oof_roberta_base.csv')\n",
        "rb_sl = np.load('oof_start_logits_roberta_base.npy')\n",
        "rb_el = np.load('oof_end_logits_roberta_base.npy')\n",
        "rb_tok = AutoTokenizer.from_pretrained('roberta-base', use_fast=True)\n",
        "\n",
        "deb_oof = pd.read_csv('oof_deberta_v3_base.csv')\n",
        "deb_sl = np.load('oof_start_logits_deberta_v3_base.npy')\n",
        "deb_el = np.load('oof_end_logits_deberta_v3_base.npy')\n",
        "deb_tok = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base', use_fast=True)\n",
        "\n",
        "def seq_ids_offs(tok, sentiment, text, max_len=128):\n",
        "    tmp = tok(str(sentiment), str(text), max_length=max_len, padding='max_length', truncation='only_second',\n",
        "               add_special_tokens=True, return_offsets_mapping=True)\n",
        "    return tmp.sequence_ids(), tmp['offset_mapping']\n",
        "\n",
        "def decode_score(sl, el, offs, seq_ids, text, sentiment, lp):\n",
        "    if str(sentiment).strip().lower()=='neutral':\n",
        "        return text, 0.0\n",
        "    ctx = [i for i,sid in enumerate(seq_ids) if sid==1]\n",
        "    if not ctx: return text, -1e9\n",
        "    sl = np.asarray(sl, dtype=np.float32); el = np.asarray(el, dtype=np.float32)\n",
        "    mask = np.zeros_like(sl, dtype=bool); mask[np.array(ctx)] = True\n",
        "    sl[~mask] = -np.inf; el[~mask] = -np.inf\n",
        "    best, bi, bj = -1e9, ctx[0], ctx[0]\n",
        "    # ignore zero-length offsets\n",
        "    valid = [i for i in ctx if offs[i][1] > offs[i][0]]\n",
        "    if not valid: return text, -1e9\n",
        "    for i in valid:\n",
        "        for j in valid:\n",
        "            if j < i: continue\n",
        "            span_len = offs[j][1] - offs[i][0]\n",
        "            sc = float(sl[i]) + float(el[j]) - float(lp) * float(span_len)\n",
        "            if sc > best: best, bi, bj = sc, i, j\n",
        "    s_char, e_char = offs[bi][0], offs[bj][1]\n",
        "    pred = text[s_char:e_char].strip()\n",
        "    if not pred: pred = text\n",
        "    return pred, float(best)\n",
        "\n",
        "# Tuned PP params discovered earlier\n",
        "rb_lp = 0.008  # from Cell 7 best\n",
        "deb_lp_by_sent = {'positive': 0.008, 'negative': 0.002}  # from Cell 9\n",
        "\n",
        "def jaccard(a,b):\n",
        "    sa, sb = set(str(a).split()), set(str(b).split())\n",
        "    if not sa and not sb: return 1.0\n",
        "    if not sa or not sb: return 0.0\n",
        "    inter = len(sa & sb); union = len(sa | sb)\n",
        "    return inter/union if union else 0.0\n",
        "\n",
        "# Compute OOF per-row scores for both models\n",
        "assert len(rb_oof)==len(deb_oof), 'OOF length mismatch; cannot align chooser reliably'\n",
        "N = len(rb_oof)\n",
        "rb_scores = np.zeros(N, dtype=np.float32)\n",
        "deb_scores = np.zeros(N, dtype=np.float32)\n",
        "rb_preds = ['']*N\n",
        "deb_preds = ['']*N\n",
        "trues = rb_oof['selected_text'].astype(str).tolist()\n",
        "for i in range(N):\n",
        "    # RoBERTa\n",
        "    text = rb_oof.iloc[i]['text']; sent = rb_oof.iloc[i]['sentiment']\n",
        "    sids, offs = seq_ids_offs(rb_tok, sent, text)\n",
        "    p, sc = decode_score(rb_sl[i], rb_el[i], offs, sids, text, sent, rb_lp)\n",
        "    rb_scores[i] = sc; rb_preds[i] = p\n",
        "    # DeBERTa\n",
        "    text2 = deb_oof.iloc[i]['text']; sent2 = deb_oof.iloc[i]['sentiment']\n",
        "    sids2, offs2 = seq_ids_offs(deb_tok, sent2, text2)\n",
        "    lp = deb_lp_by_sent.get(str(sent2).strip().lower(), 0.0)\n",
        "    p2, sc2 = decode_score(deb_sl[i], deb_el[i], offs2, sids2, text2, sent2, lp)\n",
        "    deb_scores[i] = sc2; deb_preds[i] = p2\n",
        "\n",
        "# Standardize scores per sentiment and model using OOF\n",
        "sents = rb_oof['sentiment'].astype(str).str.lower().tolist()\n",
        "stats_rb = {}; stats_deb = {}\n",
        "for s in ['positive','negative']:\n",
        "    idx = [i for i,x in enumerate(sents) if x==s]\n",
        "    if idx:\n",
        "        mu_rb = float(np.mean(rb_scores[idx])); sd_rb = float(np.std(rb_scores[idx]) + 1e-6)\n",
        "        mu_deb = float(np.mean(deb_scores[idx])); sd_deb = float(np.std(deb_scores[idx]) + 1e-6)\n",
        "        stats_rb[s] = (mu_rb, sd_rb); stats_deb[s] = (mu_deb, sd_deb)\n",
        "\n",
        "def zscore(sc, mu_sd):\n",
        "    if mu_sd is None: return sc\n",
        "    mu, sd = mu_sd; return (sc - mu) / sd\n",
        "\n",
        "# Evaluate chooser on OOF: pick model with higher z-score\n",
        "chooser_preds = []\n",
        "for i in range(N):\n",
        "    s = sents[i]\n",
        "    if s=='neutral':\n",
        "        chooser_preds.append(rb_oof.iloc[i]['text']);\n",
        "        continue\n",
        "    z_rb = zscore(rb_scores[i], stats_rb.get(s, None))\n",
        "    z_deb = zscore(deb_scores[i], stats_deb.get(s, None))\n",
        "    chooser_preds.append(deb_preds[i] if z_deb >= z_rb else rb_preds[i])\n",
        "oof_chooser = float(np.mean([jaccard(t,p) for t,p in zip(trues, chooser_preds)]))\n",
        "print(f'Chooser OOF Jaccard: {oof_chooser:.5f}')\n",
        "\n",
        "# Apply chooser to test\n",
        "test_df = pd.read_csv('test.csv')\n",
        "rb_test_st = np.mean([np.load(f'test_start_fold{f}.npy') for f in range(5)], axis=0)\n",
        "rb_test_en = np.mean([np.load(f'test_end_fold{f}.npy') for f in range(5)], axis=0)\n",
        "deb_test_st = np.mean([np.load(f'deberta_test_start_fold{f}.npy') for f in range(5)], axis=0)\n",
        "deb_test_en = np.mean([np.load(f'deberta_test_end_fold{f}.npy') for f in range(5)], axis=0)\n",
        "\n",
        "test_preds = []\n",
        "for i in range(len(test_df)):\n",
        "    text = test_df.iloc[i]['text']; sent = test_df.iloc[i]['sentiment']\n",
        "    s_key = str(sent).strip().lower()\n",
        "    if s_key=='neutral':\n",
        "        test_preds.append(text); continue\n",
        "    # RoBERTa decode\n",
        "    sids, offs = seq_ids_offs(rb_tok, sent, text)\n",
        "    p_rb, sc_rb = decode_score(rb_test_st[i], rb_test_en[i], offs, sids, text, sent, rb_lp)\n",
        "    # DeBERTa decode\n",
        "    sids2, offs2 = seq_ids_offs(deb_tok, sent, text)\n",
        "    lp = deb_lp_by_sent.get(s_key, 0.0)\n",
        "    p_deb, sc_deb = decode_score(deb_test_st[i], deb_test_en[i], offs2, sids2, text, sent, lp)\n",
        "    # standardize using OOF stats\n",
        "    z_rb = zscore(sc_rb, stats_rb.get(s_key, None))\n",
        "    z_deb = zscore(sc_deb, stats_deb.get(s_key, None))\n",
        "    test_preds.append(p_deb if z_deb >= z_rb else p_rb)\n",
        "\n",
        "sub_ens = pd.DataFrame({'textID': test_df['textID'], 'selected_text': test_preds})\n",
        "sub_ens.to_csv('submission_ensemble.csv', index=False)\n",
        "print('Wrote submission_ensemble.csv. Head:\\n', sub_ens.head().to_string(index=False))\n",
        "\n",
        "# Optionally set as final submission\n",
        "# pd.read_csv('submission_ensemble.csv').to_csv('submission.csv', index=False)\n",
        "# print('Updated submission.csv with chooser ensemble')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building chooser ensemble (OOF-calibrated z-scores)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chooser OOF Jaccard: 0.71106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_ensemble.csv. Head:\n     textID                                                                                       selected_text\n80a1e6bc32                                                                                      I made my wish\n863097735d                                                                                              sucks!\n264cd5277f             tired and didn`t really have an exciting Saturday.  oh well, hope it`s better tomorrow.\nbaee1e6ffc                                                              i`ve been eating cheetos all morning..\n67d06a8dee  haiiii sankQ i`m fineee ima js get a checkup cos my rib hurts LOL idk but i shall be fine ~ thanks\n"
          ]
        }
      ]
    },
    {
      "id": "ed034d71-9b8f-4089-8648-ac7cd446f855",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DeBERTa-v3-large 5-fold training (priority) with OOF/test logits caching\n",
        "import os, gc, math, json, time, numpy as np, pandas as pd, torch\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments, default_data_collator\n",
        "\n",
        "MODEL_NAME = 'microsoft/deberta-v3-large'\n",
        "print('Loading tokenizer/model:', MODEL_NAME, flush=True)\n",
        "tok_large = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 12\n",
        "GRAD_ACCUM = 6  # effective batch ~72\n",
        "EPOCHS = 2\n",
        "LR = 1e-5\n",
        "WARMUP_RATIO = 0.1\n",
        "WEIGHT_DECAY = 0.01\n",
        "GRAD_CLIP = 1.0\n",
        "\n",
        "def build_encodings_df_large(df: pd.DataFrame, include_labels: bool = True):\n",
        "    enc_list = []\n",
        "    for _, r in df.iterrows():\n",
        "        sel = r['selected_text'] if include_labels else None\n",
        "        enc, _ = tokenize_and_align(tok_large, r['text'], r['sentiment'], sel, max_len=MAX_LEN)\n",
        "        enc['text'] = r['text']\n",
        "        enc['sentiment'] = r['sentiment']\n",
        "        enc_list.append(enc)\n",
        "    keys = enc_list[0].keys()\n",
        "    out = {}\n",
        "    for k in keys:\n",
        "        vals = [e[k] for e in enc_list]\n",
        "        if k in ('text','sentiment'): out[k] = vals\n",
        "        else: out[k] = np.array(vals, dtype=object if k=='offset_mapping' else None)\n",
        "    return out\n",
        "\n",
        "class QADatasetLarge(torch.utils.data.Dataset):\n",
        "    def __init__(self, enc, with_labels): self.enc, self.with_labels = enc, with_labels\n",
        "    def __len__(self): return len(self.enc['input_ids'])\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            'input_ids': torch.tensor(self.enc['input_ids'][idx], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.enc['attention_mask'][idx], dtype=torch.long),\n",
        "        }\n",
        "        if self.with_labels:\n",
        "            item['start_positions'] = torch.tensor(self.enc['start_positions'][idx], dtype=torch.long)\n",
        "            item['end_positions'] = torch.tensor(self.enc['end_positions'][idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "def get_seq_ids_offsets_large(sentiment: str, text: str):\n",
        "    tmp = tok_large(str(sentiment), str(text), max_length=MAX_LEN, padding='max_length', truncation='only_second',\n",
        "                    add_special_tokens=True, return_offsets_mapping=True)\n",
        "    return tmp.sequence_ids(), tmp['offset_mapping']\n",
        "\n",
        "def decode_span_large(start_logits, end_logits, offsets, sequence_ids, text, sentiment, length_penalty_per_char=0.003):\n",
        "    if str(sentiment).strip().lower() == 'neutral': return text\n",
        "    ctx = [i for i, sid in enumerate(sequence_ids) if sid == 1]\n",
        "    if not ctx: return text\n",
        "    sl = np.asarray(start_logits, dtype=np.float32); el = np.asarray(end_logits, dtype=np.float32)\n",
        "    mask = np.zeros_like(sl, dtype=bool); mask[np.array(ctx)] = True\n",
        "    sl[~mask] = -np.inf; el[~mask] = -np.inf\n",
        "    best, bi, bj = -1e9, ctx[0], ctx[0]\n",
        "    valid = [i for i in ctx if offsets[i][1] > offsets[i][0]]\n",
        "    if not valid: return text\n",
        "    for i in valid:\n",
        "        for j in valid:\n",
        "            if j < i: continue\n",
        "            span_len = offsets[j][1] - offsets[i][0]\n",
        "            sc = float(sl[i]) + float(el[j]) - length_penalty_per_char * float(span_len)\n",
        "            if sc > best: best, bi, bj = sc, i, j\n",
        "    s_char, e_char = offsets[bi][0], offsets[bj][1]\n",
        "    pred = text[s_char:e_char].strip()\n",
        "    return pred if pred else text\n",
        "\n",
        "def jaccard_batch_fast(trues, preds):\n",
        "    def jac(a,b):\n",
        "        sa, sb = set(str(a).split()), set(str(b).split())\n",
        "        if not sa and not sb: return 1.0\n",
        "        if not sa or not sb: return 0.0\n",
        "        inter = len(sa & sb); union = len(sa | sb)\n",
        "        return inter/union if union else 0.0\n",
        "    return float(np.mean([jac(t,p) for t,p in zip(trues, preds)]))\n",
        "\n",
        "oof_rows_l, start_logits_folds_l, end_logits_folds_l = [], [], []\n",
        "for fold in range(5):\n",
        "    t0 = time.time(); print(f'\\n===== DeBERTa-v3-large Fold {fold} =====', flush=True)\n",
        "    trn_df = train_clean[train_clean.fold != fold].reset_index(drop=True)\n",
        "    val_df = train_clean[train_clean.fold == fold].reset_index(drop=True)\n",
        "    print('Train/Val sizes:', len(trn_df), len(val_df))\n",
        "\n",
        "    trn_enc = build_encodings_df_large(trn_df, include_labels=True)\n",
        "    val_enc = build_encodings_df_large(val_df, include_labels=True)\n",
        "    train_ds = QADatasetLarge(trn_enc, with_labels=True)\n",
        "    val_ds = QADatasetLarge(val_enc, with_labels=True)\n",
        "\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        start_logits, end_logits = eval_pred.predictions\n",
        "        preds, trues = [], list(val_df['selected_text'].astype(str).values)\n",
        "        for i in range(len(val_df)):\n",
        "            text, sentiment = val_df.iloc[i]['text'], val_df.iloc[i]['sentiment']\n",
        "            seq_ids, offs = get_seq_ids_offsets_large(sentiment, text)\n",
        "            preds.append(decode_span_large(start_logits[i], end_logits[i], offs, seq_ids, text, sentiment))\n",
        "        return {'jaccard': jaccard_batch_fast(trues, preds)}\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'./outputs_{MODEL_NAME.replace(\"/\",\"_\")}_fold{fold}',\n",
        "        evaluation_strategy='epoch', save_strategy='epoch',\n",
        "        load_best_model_at_end=True, metric_for_best_model='jaccard', greater_is_better=True,\n",
        "        per_device_train_batch_size=BATCH_SIZE, per_device_eval_batch_size=BATCH_SIZE*2,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM, num_train_epochs=EPOCHS, fp16=True,\n",
        "        learning_rate=LR, weight_decay=WEIGHT_DECAY, warmup_ratio=WARMUP_RATIO, lr_scheduler_type='cosine',\n",
        "        max_grad_norm=GRAD_CLIP, dataloader_num_workers=2, logging_steps=50, save_total_limit=1, seed=SEED, report_to=[]\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=val_ds,\n",
        "                      tokenizer=tok_large, data_collator=default_data_collator, compute_metrics=compute_metrics)\n",
        "    train_out = trainer.train()\n",
        "    print('Best model metrics:', train_out.metrics, flush=True)\n",
        "    save_dir = f'deberta_large_fold{fold}_best'\n",
        "    trainer.save_model(save_dir)\n",
        "    if getattr(trainer.state, 'best_model_checkpoint', None):\n",
        "        with open(os.path.join(save_dir, 'path.txt'), 'w') as f: f.write(trainer.state.best_model_checkpoint)\n",
        "\n",
        "    val_preds = trainer.predict(val_ds)\n",
        "    vsl, vel = val_preds.predictions\n",
        "    start_logits_folds_l.append(vsl); end_logits_folds_l.append(vel)\n",
        "\n",
        "    trues = list(val_df['selected_text'].astype(str).values)\n",
        "    preds = []\n",
        "    for i in range(len(val_df)):\n",
        "        text, sentiment = val_df.iloc[i]['text'], val_df.iloc[i]['sentiment']\n",
        "        seq_ids, offs = get_seq_ids_offsets_large(sentiment, text)\n",
        "        preds.append(decode_span_large(vsl[i], vel[i], offs, seq_ids, text, sentiment))\n",
        "        oof_rows_l.append({\n",
        "            'textID': val_df.iloc[i]['textID'], 'fold': fold, 'sentiment': sentiment, 'text': text,\n",
        "            'selected_text': trues[i], 'pred': preds[-1]\n",
        "        })\n",
        "    fj = jaccard_batch_fast(trues, preds)\n",
        "    print(f'DeBERTa-v3-large Fold {fold} OOF Jaccard: {fj:.5f}; elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "    del trainer, model, train_ds, val_ds, trn_enc, val_enc\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "oof_df_l = pd.DataFrame(oof_rows_l)\n",
        "oof_score_l = jaccard_batch_fast(oof_df_l['selected_text'].tolist(), oof_df_l['pred'].tolist())\n",
        "print(f'DeBERTa-v3-large OOF Jaccard (all folds): {oof_score_l:.5f}', flush=True)\n",
        "oof_df_l.to_csv('oof_deberta_v3_large.csv', index=False)\n",
        "np.save('oof_start_logits_deberta_v3_large.npy', np.concatenate(start_logits_folds_l, axis=0))\n",
        "np.save('oof_end_logits_deberta_v3_large.npy', np.concatenate(end_logits_folds_l, axis=0))\n",
        "print('Saved DeBERTa-v3-large OOF artifacts.')\n",
        "\n",
        "# Test inference and per-fold logits cache\n",
        "test_df = test.copy().reset_index(drop=True)\n",
        "test_enc = [tok_large(str(r['sentiment']), str(r['text']), max_length=MAX_LEN, padding='max_length',\n",
        "                     truncation='only_second', add_special_tokens=True, return_offsets_mapping=True, return_attention_mask=True)\n",
        "            for _, r in test_df.iterrows()]\n",
        "test_input_ids = torch.tensor([e['input_ids'] for e in test_enc], dtype=torch.long)\n",
        "test_attention_mask = torch.tensor([e['attention_mask'] for e in test_enc], dtype=torch.long)\n",
        "fold_starts, fold_ends = [], []\n",
        "for fold in range(5):\n",
        "    print(f'DeBERTa-v3-large test inference fold {fold}...', flush=True)\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(f'deberta_large_fold{fold}_best').to(device); model.eval()\n",
        "    with torch.no_grad():\n",
        "        bs = BATCH_SIZE\n",
        "        starts, ends = [], []\n",
        "        for i in range(0, len(test_df), bs):\n",
        "            out = model(input_ids=test_input_ids[i:i+bs].to(device), attention_mask=test_attention_mask[i:i+bs].to(device))\n",
        "            starts.append(out.start_logits.detach().cpu().numpy()); ends.append(out.end_logits.detach().cpu().numpy())\n",
        "        starts, ends = np.vstack(starts), np.vstack(ends)\n",
        "    np.save(f'deberta_large_test_start_fold{fold}.npy', starts); np.save(f'deberta_large_test_end_fold{fold}.npy', ends)\n",
        "    fold_starts.append(starts); fold_ends.append(ends)\n",
        "    del model; gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "avg_st = np.mean(fold_starts, axis=0); avg_en = np.mean(fold_ends, axis=0)\n",
        "test_preds = []\n",
        "for i in range(len(test_df)):\n",
        "    text, sentiment = test_df.iloc[i]['text'], test_df.iloc[i]['sentiment']\n",
        "    seq_ids, offs = get_seq_ids_offsets_large(sentiment, text)\n",
        "    test_preds.append(decode_span_large(avg_st[i], avg_en[i], offs, seq_ids, text, sentiment))\n",
        "sub_l = pd.DataFrame({'textID': test_df['textID'], 'selected_text': test_preds})\n",
        "sub_l.to_csv('submission_deberta_large.csv', index=False)\n",
        "print('Wrote submission_deberta_large.csv Head:\\n', sub_l.head().to_string(index=False))\n",
        "print('DeBERTa-v3-large run complete.')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer/model: microsoft/deberta-v3-large\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== DeBERTa-v3-large Fold 0 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val sizes: 19784 4947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='548' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/548 : < :, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model metrics: {'train_runtime': 652.0877, 'train_samples_per_second': 60.679, 'train_steps_per_second': 0.84, 'total_flos': 9159899115988992.0, 'train_loss': 1.0989406474315337, 'epoch': 1.9939357186173439}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/207 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa-v3-large Fold 0 OOF Jaccard: 0.71588; elapsed 685.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== DeBERTa-v3-large Fold 1 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val sizes: 19785 4946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='548' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/548 : < :, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model metrics: {'train_runtime': 651.4213, 'train_samples_per_second': 60.744, 'train_steps_per_second': 0.841, 'total_flos': 9160131294309888.0, 'train_loss': 1.1198462326161183, 'epoch': 1.9939357186173439}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/207 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa-v3-large Fold 1 OOF Jaccard: 0.70821; elapsed 682.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== DeBERTa-v3-large Fold 2 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val sizes: 19785 4946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='548' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/548 : < :, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model metrics: {'train_runtime': 658.0098, 'train_samples_per_second': 60.136, 'train_steps_per_second': 0.833, 'total_flos': 9160131294309888.0, 'train_loss': 1.1314278553872212, 'epoch': 1.9939357186173439}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/207 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa-v3-large Fold 2 OOF Jaccard: 0.71529; elapsed 691.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== DeBERTa-v3-large Fold 3 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val sizes: 19785 4946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='548' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/548 : < :, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model metrics: {'train_runtime': 651.8441, 'train_samples_per_second': 60.705, 'train_steps_per_second': 0.841, 'total_flos': 9160131294309888.0, 'train_loss': 1.1365886674310168, 'epoch': 1.9939357186173439}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/207 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa-v3-large Fold 3 OOF Jaccard: 0.71051; elapsed 683.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== DeBERTa-v3-large Fold 4 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val sizes: 19785 4946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='548' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/548 : < :, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model metrics: {'train_runtime': 652.634, 'train_samples_per_second': 60.631, 'train_steps_per_second': 0.84, 'total_flos': 9160131294309888.0, 'train_loss': 1.131474668962242, 'epoch': 1.9939357186173439}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/207 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa-v3-large Fold 4 OOF Jaccard: 0.71575; elapsed 684.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa-v3-large OOF Jaccard (all folds): 0.71313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved DeBERTa-v3-large OOF artifacts.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa-v3-large test inference fold 0...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa-v3-large test inference fold 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa-v3-large test inference fold 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa-v3-large test inference fold 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa-v3-large test inference fold 4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_deberta_large.csv Head:\n     textID                                                                                       selected_text\n80a1e6bc32                                                                                                wish\n863097735d                                                                                   gosh today sucks!\n264cd5277f             tired and didn`t really have an exciting Saturday.  oh well, hope it`s better tomorrow.\nbaee1e6ffc                                                              i`ve been eating cheetos all morning..\n67d06a8dee  haiiii sankQ i`m fineee ima js get a checkup cos my rib hurts LOL idk but i shall be fine ~ thanks\nDeBERTa-v3-large run complete.\n"
          ]
        }
      ]
    },
    {
      "id": "f5ff7dd6-9ec6-4eae-a437-01f624c27520",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Boundary-cleaning post-processing for DeBERTa (staged) \u2014 evaluate on OOF and prep test decode\n",
        "import numpy as np, pandas as pd, json, time, string\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "print('Staging boundary-cleaning PP for DeBERTa-v3-base (OOF eval + test writer).', flush=True)\n",
        "\n",
        "MAX_LEN = 128\n",
        "tok_deb = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base', use_fast=True)\n",
        "\n",
        "def get_seq_ids_offs_deb(sentiment: str, text: str):\n",
        "    tmp = tok_deb(str(sentiment), str(text), max_length=MAX_LEN, padding='max_length', truncation='only_second',\n",
        "                  add_special_tokens=True, return_offsets_mapping=True)\n",
        "    return tmp.sequence_ids(), tmp['offset_mapping']\n",
        "\n",
        "def decode_best_indices(sl_row, el_row, offs, seq_ids, lp_char: float):\n",
        "    ctx = [i for i, sid in enumerate(seq_ids) if sid == 1]\n",
        "    if not ctx: return None, None\n",
        "    sl = np.asarray(sl_row, dtype=np.float32).copy()\n",
        "    el = np.asarray(el_row, dtype=np.float32).copy()\n",
        "    mask = np.zeros_like(sl, dtype=bool); mask[np.array(ctx)] = True\n",
        "    sl[~mask] = -np.inf; el[~mask] = -np.inf\n",
        "    valid = [i for i in ctx if offs[i][1] > offs[i][0]]\n",
        "    if not valid: return None, None\n",
        "    best, bi, bj = -1e9, valid[0], valid[0]\n",
        "    for i in valid:\n",
        "        oi0 = offs[i][0]\n",
        "        for j in valid:\n",
        "            if j < i: continue\n",
        "            sc = float(sl[i]) + float(el[j]) - lp_char * float(offs[j][1] - oi0)\n",
        "            if sc > best: best, bi, bj = sc, i, j\n",
        "    return bi, bj\n",
        "\n",
        "BOUNDARY_PUNCT_LEFT = set('\"\\'`([{' )\n",
        "BOUNDARY_PUNCT_RIGHT = set('\")]}.,!?;:\u2026')\n",
        "\n",
        "def expand_to_word_boundaries(text: str, s_char: int, e_char: int):\n",
        "    # Expand if cut through alnum on edges\n",
        "    if s_char is None or e_char is None: return 0, len(text)\n",
        "    n = len(text); i, j = max(0, s_char), min(n, e_char)\n",
        "    if i < j:\n",
        "        # expand left if mid-word\n",
        "        if i > 0 and i < n and text[i].isalnum() and text[i-1].isalnum():\n",
        "            while i > 0 and text[i-1].isalnum():\n",
        "                i -= 1\n",
        "        # expand right if mid-word\n",
        "        if j > 0 and j < n and text[j-1].isalnum() and (text[j].isalnum() if j < n else False):\n",
        "            while j < n and text[j].isalnum():\n",
        "                j += 1\n",
        "    return i, j\n",
        "\n",
        "def trim_outer_ws_punct(text: str, s_char: int, e_char: int):\n",
        "    i, j = s_char, e_char\n",
        "    # strip spaces first\n",
        "    while i < j and text[i].isspace(): i += 1\n",
        "    while j > i and text[j-1].isspace(): j -= 1\n",
        "    # then gentle punctuation trim (avoid eating core word chars)\n",
        "    while i < j and text[i] in BOUNDARY_PUNCT_LEFT: i += 1\n",
        "    while j > i and text[j-1] in BOUNDARY_PUNCT_RIGHT: j -= 1\n",
        "    return i, j\n",
        "\n",
        "def decode_with_boundary_clean(sl_row, el_row, offs, seq_ids, text: str, sentiment: str, lp_char: float):\n",
        "    s_key = str(sentiment).strip().lower()\n",
        "    if s_key == 'neutral':\n",
        "        return text\n",
        "    bi, bj = decode_best_indices(sl_row, el_row, offs, seq_ids, lp_char)\n",
        "    if bi is None or bj is None:\n",
        "        return text\n",
        "    s_char, e_char = offs[bi][0], offs[bj][1]\n",
        "    s_char, e_char = expand_to_word_boundaries(text, s_char, e_char)\n",
        "    s_char, e_char = trim_outer_ws_punct(text, s_char, e_char)\n",
        "    pred = text[s_char:e_char]\n",
        "    pred = pred if pred.strip() else text\n",
        "    return pred\n",
        "\n",
        "def jaccard_mean(y_true, y_pred):\n",
        "    def jac(a,b):\n",
        "        sa, sb = set(str(a).split()), set(str(b).split())\n",
        "        if not sa and not sb: return 1.0\n",
        "        if not sa or not sb: return 0.0\n",
        "        inter = len(sa & sb); union = len(sa | sb)\n",
        "        return inter/union if union else 0.0\n",
        "    return float(np.mean([jac(t,p) for t,p in zip(y_true, y_pred)]))\n",
        "\n",
        "def eval_oof_boundary_gain():\n",
        "    # Load OOF and logits\n",
        "    oof_df = pd.read_csv('oof_deberta_v3_base.csv')\n",
        "    sl = np.load('oof_start_logits_deberta_v3_base.npy')\n",
        "    el = np.load('oof_end_logits_deberta_v3_base.npy')\n",
        "    # Load tuned lp params if available\n",
        "    try:\n",
        "        ppj = json.load(open('pp_params_deberta.json'))\n",
        "        lp_by_sent = ppj.get('lp', {'positive': 0.008, 'negative': 0.002})\n",
        "    except Exception:\n",
        "        lp_by_sent = {'positive': 0.008, 'negative': 0.002}\n",
        "    preds_no_bc, preds_bc = [], []\n",
        "    t0 = time.time()\n",
        "    for i in range(len(oof_df)):\n",
        "        r = oof_df.iloc[i]\n",
        "        s_key = str(r['sentiment']).strip().lower()\n",
        "        seq_ids, offs = get_seq_ids_offs_deb(r['sentiment'], r['text'])\n",
        "        lp = lp_by_sent.get(s_key, 0.0)\n",
        "        # baseline decode (trim only .strip() implicitly via slicing below)\n",
        "        bi, bj = decode_best_indices(sl[i], el[i], offs, seq_ids, lp)\n",
        "        if bi is None or bj is None or s_key=='neutral':\n",
        "            preds_no_bc.append(r['text'])\n",
        "        else:\n",
        "            s_char, e_char = offs[bi][0], offs[bj][1]\n",
        "            pred0 = r['text'][s_char:e_char].strip()\n",
        "            preds_no_bc.append(pred0 if pred0 else r['text'])\n",
        "        # boundary-cleaned\n",
        "        preds_bc.append(decode_with_boundary_clean(sl[i], el[i], offs, seq_ids, r['text'], r['sentiment'], lp))\n",
        "    score0 = jaccard_mean(oof_df['selected_text'].tolist(), preds_no_bc)\n",
        "    score1 = jaccard_mean(oof_df['selected_text'].tolist(), preds_bc)\n",
        "    print(f'DeBERTa OOF no-boundary: {score0:.5f} | with-boundary: {score1:.5f} | delta: {score1-score0:+.5f} | n={len(oof_df)} | {time.time()-t0:.1f}s', flush=True)\n",
        "    return lp_by_sent, score0, score1\n",
        "\n",
        "def write_test_with_boundary_clean():\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    # Load per-fold test logits and average (already cached by cell 6)\n",
        "    st = np.mean([np.load(f'deberta_test_start_fold{f}.npy') for f in range(5)], axis=0)\n",
        "    en = np.mean([np.load(f'deberta_test_end_fold{f}.npy') for f in range(5)], axis=0)\n",
        "    try:\n",
        "        lp_by_sent, _, _ = eval_oof_boundary_gain()\n",
        "    except Exception:\n",
        "        lp_by_sent = {'positive': 0.008, 'negative': 0.002}\n",
        "    preds = []\n",
        "    for i in range(len(test_df)):\n",
        "        text, sent = test_df.iloc[i]['text'], test_df.iloc[i]['sentiment']\n",
        "        s_key = str(sent).strip().lower()\n",
        "        if s_key == 'neutral':\n",
        "            preds.append(text); continue\n",
        "        seq_ids, offs = get_seq_ids_offs_deb(sent, text)\n",
        "        preds.append(decode_with_boundary_clean(st[i], en[i], offs, seq_ids, text, sent, lp_by_sent.get(s_key, 0.0)))\n",
        "    sub = pd.DataFrame({'textID': test_df['textID'], 'selected_text': preds})\n",
        "    sub.to_csv('submission_deberta_boundary.csv', index=False)\n",
        "    print('Wrote submission_deberta_boundary.csv. Head:\\n', sub.head().to_string(index=False), flush=True)\n",
        "\n",
        "print('Boundary-cleaning utilities ready. After current training finishes, run:',\n",
        "      '\\n- eval_oof_boundary_gain() to verify gain',\n",
        "      '\\n- write_test_with_boundary_clean() to produce submission_deberta_boundary.csv', flush=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Staging boundary-cleaning PP for DeBERTa-v3-base (OOF eval + test writer).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Boundary-cleaning utilities ready. After current training finishes, run: \n- eval_oof_boundary_gain() to verify gain \n- write_test_with_boundary_clean() to produce submission_deberta_boundary.csv\n"
          ]
        }
      ]
    },
    {
      "id": "5d44c7f9-8285-4547-b352-5681291fe70e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DeBERTa-v3 base+large logit blending (OOF tune weights; optional boundary cleaning); apply to test\n",
        "import numpy as np, pandas as pd, json, time\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "print('Staging DeBERTa-v3 base+large blending...', flush=True)\n",
        "\n",
        "MAX_LEN = 128\n",
        "tok_deb = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base', use_fast=True)\n",
        "\n",
        "def get_seq_ids_offs(sentiment: str, text: str):\n",
        "    tmp = tok_deb(str(sentiment), str(text), max_length=MAX_LEN, padding='max_length', truncation='only_second',\n",
        "                  add_special_tokens=True, return_offsets_mapping=True)\n",
        "    return tmp.sequence_ids(), tmp['offset_mapping']\n",
        "\n",
        "# Reuse boundary-clean helpers from Cell 13 if defined; else define minimal no-op wrappers\n",
        "try:\n",
        "    expand_to_word_boundaries\n",
        "except NameError:\n",
        "    def expand_to_word_boundaries(text, s_char, e_char):\n",
        "        n = len(text); i, j = max(0, s_char or 0), min(n, e_char or n)\n",
        "        return i, j\n",
        "try:\n",
        "    trim_outer_ws_punct\n",
        "except NameError:\n",
        "    def trim_outer_ws_punct(text, s_char, e_char):\n",
        "        i, j = s_char, e_char\n",
        "        while i < j and text[i].isspace(): i += 1\n",
        "        while j > i and text[j-1].isspace(): j -= 1\n",
        "        return i, j\n",
        "\n",
        "def decode_span_from_logits(sl_row, el_row, offs, seq_ids, text: str, sentiment: str, lp_char: float, boundary_clean: bool=False):\n",
        "    s_key = str(sentiment).strip().lower()\n",
        "    if s_key == 'neutral':\n",
        "        return text\n",
        "    ctx = [i for i, sid in enumerate(seq_ids) if sid == 1]\n",
        "    if not ctx: return text\n",
        "    sl = np.asarray(sl_row, dtype=np.float32).copy()\n",
        "    el = np.asarray(el_row, dtype=np.float32).copy()\n",
        "    mask = np.zeros_like(sl, dtype=bool); mask[np.array(ctx)] = True\n",
        "    sl[~mask] = -np.inf; el[~mask] = -np.inf\n",
        "    valid = [i for i in ctx if offs[i][1] > offs[i][0]]\n",
        "    if not valid: return text\n",
        "    best, bi, bj = -1e9, valid[0], valid[0]\n",
        "    for i in valid:\n",
        "        oi0 = offs[i][0]\n",
        "        for j in valid:\n",
        "            if j < i: continue\n",
        "            sc = float(sl[i]) + float(el[j]) - lp_char * float(offs[j][1] - oi0)\n",
        "            if sc > best: best, bi, bj = sc, i, j\n",
        "    s_char, e_char = offs[bi][0], offs[bj][1]\n",
        "    if boundary_clean:\n",
        "        s_char, e_char = expand_to_word_boundaries(text, s_char, e_char)\n",
        "        s_char, e_char = trim_outer_ws_punct(text, s_char, e_char)\n",
        "    pred = text[s_char:e_char]\n",
        "    pred = pred if pred.strip() else text\n",
        "    return pred\n",
        "\n",
        "def jaccard_mean(y_true, y_pred):\n",
        "    def jac(a,b):\n",
        "        sa, sb = set(str(a).split()), set(str(b).split())\n",
        "        if not sa and not sb: return 1.0\n",
        "        if not sa or not sb: return 0.0\n",
        "        inter = len(sa & sb); union = len(sa | sb)\n",
        "        return inter/union if union else 0.0\n",
        "    return float(np.mean([jac(t,p) for t,p in zip(y_true, y_pred)]))\n",
        "\n",
        "def blend_oof_and_eval(weights=(0.5,0.6,0.7,0.8,0.9), boundary_options=(False, True)):\n",
        "    # Load OOF artifacts\n",
        "    oof_df = pd.read_csv('oof_deberta_v3_base.csv')\n",
        "    sl_b = np.load('oof_start_logits_deberta_v3_base.npy'); el_b = np.load('oof_end_logits_deberta_v3_base.npy')\n",
        "    sl_l = np.load('oof_start_logits_deberta_v3_large.npy'); el_l = np.load('oof_end_logits_deberta_v3_large.npy')\n",
        "    assert sl_b.shape == sl_l.shape == el_b.shape == el_l.shape, 'OOF logits shape mismatch between base and large'\n",
        "    # Load tuned lp per sentiment\n",
        "    try:\n",
        "        params = json.load(open('pp_params_deberta.json'))\n",
        "        lp_by_sent = params.get('lp', {'positive': 0.008, 'negative': 0.002})\n",
        "    except Exception:\n",
        "        lp_by_sent = {'positive': 0.008, 'negative': 0.002}\n",
        "    y_true = oof_df['selected_text'].astype(str).tolist()\n",
        "    best = (-1.0, None, None)\n",
        "    for w in weights:\n",
        "        sl = w*sl_l + (1.0-w)*sl_b\n",
        "        el = w*el_l + (1.0-w)*el_b\n",
        "        for bc in boundary_options:\n",
        "            preds = []\n",
        "            t0 = time.time()\n",
        "            for i in range(len(oof_df)):\n",
        "                r = oof_df.iloc[i]\n",
        "                s_key = str(r['sentiment']).strip().lower()\n",
        "                seq_ids, offs = get_seq_ids_offs(r['sentiment'], r['text'])\n",
        "                lp = lp_by_sent.get(s_key, 0.0)\n",
        "                preds.append(decode_span_from_logits(sl[i], el[i], offs, seq_ids, r['text'], r['sentiment'], lp, boundary_clean=bc))\n",
        "            sc = jaccard_mean(y_true, preds)\n",
        "            print(f'Blend w={w:.2f} boundary={bc} -> OOF {sc:.5f} in {time.time()-t0:.1f}s', flush=True)\n",
        "            if sc > best[0]: best = (sc, w, bc)\n",
        "    print('Best blend:', best, flush=True)\n",
        "    return best\n",
        "\n",
        "def apply_blend_to_test(weight: float, boundary_clean: bool):\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    st_b = np.mean([np.load(f'deberta_test_start_fold{f}.npy') for f in range(5)], axis=0)\n",
        "    en_b = np.mean([np.load(f'deberta_test_end_fold{f}.npy') for f in range(5)], axis=0)\n",
        "    st_l = np.mean([np.load(f'deberta_large_test_start_fold{f}.npy') for f in range(5)], axis=0)\n",
        "    en_l = np.mean([np.load(f'deberta_large_test_end_fold{f}.npy') for f in range(5)], axis=0)\n",
        "    assert st_b.shape == st_l.shape == en_b.shape == en_l.shape == (len(test_df), st_b.shape[1])\n",
        "    try:\n",
        "        params = json.load(open('pp_params_deberta.json'))\n",
        "        lp_by_sent = params.get('lp', {'positive': 0.008, 'negative': 0.002})\n",
        "    except Exception:\n",
        "        lp_by_sent = {'positive': 0.008, 'negative': 0.002}\n",
        "    st = weight*st_l + (1.0-weight)*st_b\n",
        "    en = weight*en_l + (1.0-weight)*en_b\n",
        "    preds = []\n",
        "    for i in range(len(test_df)):\n",
        "        text, sent = test_df.iloc[i]['text'], test_df.iloc[i]['sentiment']\n",
        "        s_key = str(sent).strip().lower()\n",
        "        if s_key == 'neutral':\n",
        "            preds.append(text); continue\n",
        "        seq_ids, offs = get_seq_ids_offs(sent, text)\n",
        "        preds.append(decode_span_from_logits(st[i], en[i], offs, seq_ids, text, sent, lp_by_sent.get(s_key, 0.0), boundary_clean=boundary_clean))\n",
        "    sub = pd.DataFrame({'textID': test_df['textID'], 'selected_text': preds})\n",
        "    out_path = f'submission_deberta_blend_w{weight:.2f}_bc{int(boundary_clean)}.csv'\n",
        "    sub.to_csv(out_path, index=False)\n",
        "    print('Wrote', out_path, 'Head:\\n', sub.head().to_string(index=False), flush=True)\n",
        "    return out_path\n",
        "\n",
        "print('Blend utilities ready. After large training finishes and saves logits, run:',\n",
        "      '\\n- best = blend_oof_and_eval()  # pick (score, w, boundary)',\n",
        "      '\\n- apply_blend_to_test(best[1], best[2])  # write submission', flush=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Staging DeBERTa-v3 base+large blending...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blend utilities ready. After large training finishes and saves logits, run: \n- best = blend_oof_and_eval()  # pick (score, w, boundary) \n- apply_blend_to_test(best[1], best[2])  # write submission\n"
          ]
        }
      ]
    },
    {
      "id": "7f5c34a1-1c87-49ca-afc2-f75502051cc8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run DeBERTa base+large blend grid on OOF; apply best to test and (optionally) set submission.csv\n",
        "import pandas as pd, shutil\n",
        "best_score, best_w, best_bc = blend_oof_and_eval(weights=(0.5,0.6,0.7,0.8,0.9), boundary_options=(False, True))\n",
        "print(f'Best blended OOF: {best_score:.5f} with w={best_w} boundary_clean={best_bc}', flush=True)\n",
        "out_path = apply_blend_to_test(best_w, best_bc)\n",
        "print('Blend submission path:', out_path, flush=True)\n",
        "# If blend beats prior best single-model OOF (0.71239), set as final submission\n",
        "if best_score > 0.71239:\n",
        "    pd.read_csv(out_path).to_csv('submission.csv', index=False)\n",
        "    print('submission.csv updated with blended model (OOF improved).', flush=True)\n",
        "else:\n",
        "    print('Blended OOF did not beat 0.71239; keeping current submission.csv.', flush=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blend w=0.50 boundary=False -> OOF 0.71502 in 4.9s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}