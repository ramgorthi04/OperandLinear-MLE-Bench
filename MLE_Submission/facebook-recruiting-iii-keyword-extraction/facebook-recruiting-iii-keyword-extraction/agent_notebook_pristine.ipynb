{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\\n",
    "# Facebook Recruiting III - Keyword Extraction (Pristine Setup)\n",
    "# Single-cell, idempotent setup: imports, constants, helpers, env hygiene, and full-data caching.\n",
    "\n",
    "import os, re, gc, time, shutil\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# --- Environment hygiene: suppress matplotlib_inline post-run callback noise ---\n",
    "from IPython import get_ipython\n",
    "print('[Env] Activating matplotlib_inline suppression...')\n",
    "try:\n",
    "    import matplotlib as _mpl\n",
    "    if not hasattr(_mpl, 'backend_bases'):\n",
    "        _mpl.backend_bases = type('backend_bases', (), {'_Backend': type('_Backend', (), {})})\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        try:\n",
    "            plt.switch_backend('Agg')\n",
    "        except Exception:\n",
    "            pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    ip = get_ipython()\n",
    "    if ip is not None and hasattr(ip, 'events'):\n",
    "        cbs = ip.events.callbacks.get('post_run_cell', [])\n",
    "        new_cbs = []\n",
    "        for cb in cbs:\n",
    "            name = getattr(cb, '__name__', '')\n",
    "            mod = getattr(cb, '__module__', '')\n",
    "            qual = getattr(cb, '__qualname__', '')\n",
    "            if ('matplotlib_inline' in mod) or ('backend_inline' in mod) or (name == 'configure_once') or ('configure_once' in qual):\n",
    "                continue\n",
    "            new_cbs.append(cb)\n",
    "        ip.events.callbacks['post_run_cell'] = new_cbs\n",
    "    print('[Env] matplotlib_inline suppression active.')\n",
    "except Exception as e:\n",
    "    print('[Env] matplotlib_inline suppression failed:', repr(e))\n",
    "\n",
    "GLOBAL_SEED = 1337\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "def backup_notebook():\n",
    "    nb_path = 'agent_notebook_pristine.ipynb'\n",
    "    if os.path.exists(nb_path):\n",
    "        ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        dst = f'agent_notebook_pristine_backup_{ts}.ipynb'\n",
    "        shutil.copy2(nb_path, dst)\n",
    "        print(f'[Backup] Pristine notebook copied to {dst}')\n",
    "backup_notebook()\n",
    "\n",
    "# Normalization and parsing (approved) - RAW regex strings to preserve \\b, \\d etc.\n",
    "URL_RE   = re.compile(r'https?://\\S+|www\\.\\S+', flags=re.IGNORECASE)\n",
    "EMAIL_RE = re.compile(r'(?i)\\b[a-z0-9._%+\\-]+@[a-z0-9.\\-]+\\.[a-z]{2,}\\b')\n",
    "HEX_RE   = re.compile(r'\\b0x[0-9a-f]+\\b', flags=re.IGNORECASE)\n",
    "NUM_RE   = re.compile(r'\\b\\d+\\b')\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return ''\n",
    "    s = s.lower()\n",
    "    s = URL_RE.sub(' URL ', s)\n",
    "    s = EMAIL_RE.sub(' EMAIL ', s)\n",
    "    s = HEX_RE.sub(' HEX ', s)\n",
    "    s = NUM_RE.sub(' 0 ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "def extract_text_and_code_pre_only(html: str):\n",
    "    if not isinstance(html, str):\n",
    "        return '', '', 0, 0, 0.0\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    pre_blocks = soup.find_all('pre')\n",
    "    code_texts = []\n",
    "    for pre in pre_blocks:\n",
    "        code_texts.append(pre.get_text(' ', strip=True))\n",
    "        pre.extract()\n",
    "    text = soup.get_text(' ', strip=True)\n",
    "    text_norm = normalize_text(text)\n",
    "    # Use a simple space to join code blocks to avoid escape issues in code generation\n",
    "    code_norm = normalize_text(' '.join(code_texts))\n",
    "    url_count = len(URL_RE.findall(text.lower()))\n",
    "    puncts = re.findall(r'[\\!\\?\\.,;:\\-\\(\\)\\[\\]\\{\\}\\#\\+\\*/\\\\\\|\\<\\>\\=\\_\\~\\^\\`\\\"]', text_norm)\n",
    "    punct_density = (len(puncts) / max(1, len(text_norm)))\n",
    "    return text_norm, code_norm, len(pre_blocks), url_count, punct_density\n",
    "\n",
    "def build_cache(csv_path: str, is_train: bool = True, chunksize: int = 100_000) -> pd.DataFrame:\n",
    "    t0 = time.time()\n",
    "    all_parts = []\n",
    "    usecols = ['Id', 'Title', 'Body'] + (['Tags'] if is_train else [])\n",
    "    reader = pd.read_csv(csv_path, usecols=usecols, chunksize=chunksize)\n",
    "    total_rows = 0\n",
    "    for i, chunk in enumerate(reader):\n",
    "        if 'Id' in chunk.columns:\n",
    "            chunk['Id'] = pd.to_numeric(chunk['Id'], downcast='integer')\n",
    "        out_records = {\n",
    "            'Id': [], 'title_norm': [], 'body_text': [], 'code_text': [],\n",
    "            'title_len': [], 'body_len': [], 'code_len': [], 'num_block_code': [], 'num_urls': [], 'punct_density': []\n",
    "        }\n",
    "        if is_train:\n",
    "            out_records['Tags'] = []\n",
    "        titles = chunk['Title'].fillna('').astype(str).tolist()\n",
    "        titles_norm = [normalize_text(t) for t in titles]\n",
    "        bodies = chunk['Body'].fillna('').astype(str).tolist()\n",
    "        for idx in range(len(chunk)):\n",
    "            body_txt, code_txt, n_code, n_url, pden = extract_text_and_code_pre_only(bodies[idx])\n",
    "            out_records['Id'].append(int(chunk.iloc[idx]['Id']))\n",
    "            out_records['title_norm'].append(titles_norm[idx])\n",
    "            out_records['body_text'].append(body_txt)\n",
    "            out_records['code_text'].append(code_txt)\n",
    "            out_records['title_len'].append(len(titles_norm[idx]))\n",
    "            out_records['body_len'].append(len(body_txt))\n",
    "            out_records['code_len'].append(len(code_txt))\n",
    "            out_records['num_block_code'].append(int(n_code))\n",
    "            out_records['num_urls'].append(int(n_url))\n",
    "            out_records['punct_density'].append(float(pden))\n",
    "            if is_train:\n",
    "                out_records['Tags'].append(chunk.iloc[idx]['Tags'])\n",
    "        out_df = pd.DataFrame(out_records)\n",
    "        all_parts.append(out_df)\n",
    "        total_rows += len(out_df)\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f'[Cache] Processed ~{total_rows} rows so far for {os.path.basename(csv_path)}')\n",
    "        del chunk, out_df, out_records, titles, titles_norm, bodies\n",
    "        gc.collect()\n",
    "    result = pd.concat(all_parts, ignore_index=True) if len(all_parts) else pd.DataFrame()\n",
    "    print(f'[Cache] Built DataFrame with {len(result)} rows in {time.time()-t0:.1f}s from {os.path.basename(csv_path)}')\n",
    "    return result\n",
    "\n",
    "TRAIN_CSV = 'train.csv'\n",
    "TEST_CSV  = 'test.csv'\n",
    "PARSED_TRAIN_FULL_PKL = 'parsed_train_full.pkl'\n",
    "PARSED_TEST_PKL  = 'parsed_test.pkl'\n",
    "\n",
    "print('[Setup] Starting pristine setup...')\n",
    "built_any = False\n",
    "\n",
    "# Load-or-build TRAIN cache, and ensure df_train_full in memory\n",
    "if os.path.exists(PARSED_TRAIN_FULL_PKL):\n",
    "    df_train_full = pd.read_pickle(PARSED_TRAIN_FULL_PKL)\n",
    "    print(f\"[Cache] Loaded {PARSED_TRAIN_FULL_PKL} with shape {df_train_full.shape}\")\n",
    "else:\n",
    "    print('[Cache] Building FULL train cache (no subsample)...')\n",
    "    df_train_full = build_cache(TRAIN_CSV, is_train=True, chunksize=75_000)\n",
    "    df_train_full.to_pickle(PARSED_TRAIN_FULL_PKL)\n",
    "    print(f'[Cache] Wrote {PARSED_TRAIN_FULL_PKL} with {len(df_train_full)} rows')\n",
    "    built_any = True\n",
    "\n",
    "# Load-or-build TEST cache, and ensure df_test_cache in memory\n",
    "if os.path.exists(PARSED_TEST_PKL):\n",
    "    df_test_cache = pd.read_pickle(PARSED_TEST_PKL)\n",
    "    print(f\"[Cache] Loaded {PARSED_TEST_PKL} with shape {df_test_cache.shape}\")\n",
    "else:\n",
    "    print('[Cache] Building test cache (pickle)...')\n",
    "    df_test_cache = build_cache(TEST_CSV, is_train=False, chunksize=75_000)\n",
    "    df_test_cache.to_pickle(PARSED_TEST_PKL)\n",
    "    print(f'[Cache] Wrote {PARSED_TEST_PKL} with {len(df_test_cache)} rows')\n",
    "    built_any = True\n",
    "\n",
    "print('[Setup] Completed. Built any:', built_any)\n",
    "print('[Setup] df_train_full shape:', df_train_full.shape if 'df_train_full' in globals() else None)\n",
    "print('[Setup] df_test_cache shape:', df_test_cache.shape if 'df_test_cache' in globals() else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455f3829",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\\n",
    "# Phase 2 (Refactored v3): Scalable 5-fold CV with streaming features + online OVR via SGD (partial_fit)\n",
    "# - Avoids materializing full X_tr/X_va: transforms batches on the fly per channel and hstack per-batch only\n",
    "# - Per-fold label pruning + dynamic label sharding (4GB coef_ budget)\n",
    "# - Streaming threshold optimization with correct FN accounting for excluded labels\n",
    "# - Hygiene: no nested parallelism, sparse-safe ops, per-fold vectorizer fit, artifact persistence, safety rule\n",
    "# - Micro-pilot mode (conditional): deterministic subsample and run only the first fold to validate pipeline\n",
    "\n",
    "import os, gc, time, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Dependency guard: iterative-stratification\n",
    "try:\n",
    "    from skmultilearn.model_selection import IterativeStratification\n",
    "except Exception:\n",
    "    import sys\n",
    "    from subprocess import run\n",
    "    run([sys.executable, '-m', 'pip', 'install', '--quiet', 'scikit-multilearn'])\n",
    "    from skmultilearn.model_selection import IterativeStratification\n",
    "\n",
    "GLOBAL_SEED = 1337\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "# Expect df_train_full and df_test_cache in memory from setup cell\n",
    "assert 'df_train_full' in globals() and 'df_test_cache' in globals(), 'Run setup cell first to load caches.'\n",
    "\n",
    "# Prepare texts and labels\n",
    "def to_list_tags(s):\n",
    "    return s.split() if isinstance(s, str) else []\n",
    "y_lists = df_train_full['Tags'].astype(str).apply(to_list_tags)\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "Y_all = mlb.fit_transform(y_lists)\n",
    "labels_list = mlb.classes_.tolist()\n",
    "n_samples, n_labels = Y_all.shape\n",
    "print('[Labels] #samples:', n_samples, '#labels:', n_labels)\n",
    "\n",
    "# Channels\n",
    "title_text = df_train_full['title_norm'].fillna('').astype(str)\n",
    "body_text  = df_train_full['body_text'].fillna('').astype(str)\n",
    "code_text  = df_train_full['code_text'].fillna('').astype(str)\n",
    "meta_cols = ['title_len','body_len','code_len','num_block_code','num_urls','punct_density']\n",
    "meta_all = df_train_full[meta_cols].astype(np.float32).values\n",
    "\n",
    "# Micro-pilot configuration (auditor-approved): subsample deterministically and run only first fold\n",
    "MICRO_PILOT = True\n",
    "PILOT_N = 50_000  # target rows for micro-pilot (reduced for faster turnaround)\n",
    "if MICRO_PILOT:\n",
    "    n_keep = int(min(PILOT_N, n_samples))\n",
    "    rng = np.random.RandomState(GLOBAL_SEED)\n",
    "    idx_keep = rng.choice(n_samples, size=n_keep, replace=False)\n",
    "    idx_keep.sort()\n",
    "    # Subset all channels and labels consistently\n",
    "    title_text = title_text.iloc[idx_keep].reset_index(drop=True)\n",
    "    body_text  = body_text.iloc[idx_keep].reset_index(drop=True)\n",
    "    code_text  = code_text.iloc[idx_keep].reset_index(drop=True)\n",
    "    meta_all   = meta_all[idx_keep]\n",
    "    Y_all      = Y_all[idx_keep]\n",
    "    n_samples  = Y_all.shape[0]\n",
    "    print(f'[Pilot] Subsampled to {n_samples} rows for micro-pilot.')\n",
    "\n",
    "# CV setup\n",
    "n_splits = 5\n",
    "mskf = IterativeStratification(n_splits=n_splits, order=1)\n",
    "\n",
    "# Vectorizer configs (fit within fold on train split)\n",
    "title_vec_cfg = dict(analyzer='word', ngram_range=(1,3), min_df=3, max_df=0.95,\n",
    "                     max_features=200_000, sublinear_tf=True, dtype=np.float32)\n",
    "# Use streaming hashing with built-in l2 normalization (no IDF) to allow partial/batch processing\n",
    "body_hash_cfg  = dict(analyzer='word', ngram_range=(1,3), n_features=2**19,\n",
    "                      alternate_sign=False, norm='l2', dtype=np.float32)\n",
    "char_hash_cfg  = dict(analyzer='char_wb', ngram_range=(3,6), n_features=2**18,\n",
    "                      alternate_sign=False, norm='l2', dtype=np.float32)\n",
    "code_vec_cfg   = dict(analyzer='word', ngram_range=(1,3), min_df=2, max_df=0.999,\n",
    "                      token_pattern=r'(?u)\\b\\w[\\w_\\+\\-\\#\\.]*\\b', max_features=100_000,\n",
    "                      sublinear_tf=True, dtype=np.float32)\n",
    "title_alpha = 3.0\n",
    "MIN_LABEL_FREQ_TRAIN = 50  # higher for micro-pilot to reduce labels per shard and speed up\n",
    "THS = np.linspace(0.05, 0.6, 12)\n",
    "COEF_BUDGET_BYTES = 4_000_000_000  # 4GB\n",
    "BATCH_SIZE = 8192  # larger batch for better throughput in micro-pilot\n",
    "\n",
    "# Global label support for per-tag thresholds\n",
    "global_support = np.asarray(Y_all.sum(axis=0)).ravel()\n",
    "hi_support_mask = (global_support >= 50)\n",
    "hi_label_idx = np.where(hi_support_mask)[0]\n",
    "print('[Labels] High-support labels (>=50):', hi_label_idx.size)\n",
    "\n",
    "# Streaming accumulators for global micro-F1\n",
    "tp_tot = np.zeros_like(THS, dtype=np.int64)\n",
    "fp_tot = np.zeros_like(THS, dtype=np.int64)\n",
    "fn_tot = np.zeros_like(THS, dtype=np.int64)\n",
    "\n",
    "# Per-tag (high-support only) streaming counts per threshold\n",
    "tp_hi = np.zeros((hi_label_idx.size, THS.size), dtype=np.int64)\n",
    "fp_hi = np.zeros((hi_label_idx.size, THS.size), dtype=np.int64)\n",
    "fn_hi = np.zeros((hi_label_idx.size, THS.size), dtype=np.int64)\n",
    "hi_pos = {lab: pos for pos, lab in enumerate(hi_label_idx)}\n",
    "\n",
    "def update_counts_batch(Y_true_batch_csr, probs_batch, label_idx_batch, ths, tp, fp, fn,\n",
    "                        tp_hi, fp_hi, fn_hi, hi_pos_map):\n",
    "    bs, Lb = probs_batch.shape\n",
    "    for ti, thr in enumerate(ths):\n",
    "        pred_bin = (probs_batch >= thr).astype(np.uint8)\n",
    "        pred_csr = sparse.csr_matrix(pred_bin, dtype=np.uint8)\n",
    "        tp_mat = pred_csr.multiply(Y_true_batch_csr)\n",
    "        tp_count = int(tp_mat.sum())\n",
    "        pred_pos = int(pred_bin.sum())\n",
    "        true_pos = int(Y_true_batch_csr.sum())\n",
    "        tp[ti] += tp_count\n",
    "        fp[ti] += (pred_pos - tp_count)\n",
    "        fn[ti] += (true_pos - tp_count)\n",
    "        if hi_pos_map:\n",
    "            for j_local in range(Lb):\n",
    "                g_lab = int(label_idx_batch[j_local])\n",
    "                pos = hi_pos_map.get(g_lab, None)\n",
    "                if pos is None:\n",
    "                    continue\n",
    "                col_true = Y_true_batch_csr[:, j_local]\n",
    "                col_pred = pred_csr[:, j_local]\n",
    "                tp_j = int(col_true.multiply(col_pred).sum())\n",
    "                p_j = int(col_pred.sum())\n",
    "                t_j = int(col_true.sum())\n",
    "                tp_hi[pos, ti] += tp_j\n",
    "                fp_hi[pos, ti] += (p_j - tp_j)\n",
    "                fn_hi[pos, ti] += (t_j - tp_j)\n",
    "\n",
    "def micro_f1(tp, fp, fn):\n",
    "    denom = (2*tp + fp + fn)\n",
    "    return 0.0 if denom == 0 else (2.0*tp)/denom\n",
    "\n",
    "# Helper: yield batches of indices\n",
    "def batch_indices(idxs, batch_size):\n",
    "    for s in range(0, idxs.size, batch_size):\n",
    "        yield idxs[s:min(idxs.size, s+batch_size)]\n",
    "\n",
    "fold_times = []\n",
    "X_dummy = np.zeros((n_samples, 1))\n",
    "fold_id = 0\n",
    "t_all = time.time()\n",
    "for tr_idx, va_idx in mskf.split(X_dummy, Y_all):\n",
    "    t0 = time.time()\n",
    "    Y_tr_full = Y_all[tr_idx]\n",
    "    Y_va_full = Y_all[va_idx]\n",
    "\n",
    "    # Per-fold label pruning (train split only)\n",
    "    sup_tr = np.asarray(Y_tr_full.sum(axis=0)).ravel()\n",
    "    kept_labels = np.where(sup_tr >= MIN_LABEL_FREQ_TRAIN)[0]\n",
    "    excluded_labels = np.setdiff1d(np.arange(n_labels), kept_labels)\n",
    "    print(f'[Fold {fold_id}] Train={len(tr_idx)}, Valid={len(va_idx)}, kept_labels={kept_labels.size}, excluded={excluded_labels.size}')\n",
    "    if kept_labels.size == 0:\n",
    "        print(f'[Fold {fold_id}] No labels meet freq >= {MIN_LABEL_FREQ_TRAIN}; skipping fold.')\n",
    "        continue\n",
    "\n",
    "    # Fit vectorizers/scaler on training split only\n",
    "    title_vec = TfidfVectorizer(**title_vec_cfg)\n",
    "    code_vec  = TfidfVectorizer(**code_vec_cfg)\n",
    "    body_hash = HashingVectorizer(**body_hash_cfg)\n",
    "    char_hash = HashingVectorizer(**char_hash_cfg)\n",
    "    meta_scaler = StandardScaler(with_mean=False)\n",
    "\n",
    "    # Fit title/code on full train split (fit only, no transform to avoid materializing large matrices)\n",
    "    title_vec.fit(title_text.iloc[tr_idx])\n",
    "    code_vec.fit(code_text.iloc[tr_idx])\n",
    "\n",
    "    # Fit meta scaler in batches\n",
    "    for b_idx in batch_indices(tr_idx, BATCH_SIZE):\n",
    "        meta_scaler.partial_fit(meta_all[b_idx])\n",
    "\n",
    "    # Compute feature dimension D approximately by transforming one small batch\n",
    "    probe_idx = tr_idx[:min(BATCH_SIZE, tr_idx.size)]\n",
    "    Xt_probe = title_vec.transform(title_text.iloc[probe_idx]).astype(np.float32)\n",
    "    Xt_probe = Xt_probe.multiply(title_alpha)\n",
    "    Xb_probe = body_hash.transform(body_text.iloc[probe_idx]).astype(np.float32)\n",
    "    Xc_probe = char_hash.transform((title_text.iloc[probe_idx] + ' ' + body_text.iloc[probe_idx])).astype(np.float32)\n",
    "    Xcode_probe = code_vec.transform(code_text.iloc[probe_idx]).astype(np.float32)\n",
    "    Xmeta_probe = sparse.csr_matrix(meta_scaler.transform(meta_all[probe_idx]), dtype=np.float32)\n",
    "    D = sparse.hstack([Xt_probe, Xb_probe, Xc_probe, Xcode_probe, Xmeta_probe], format='csr', dtype=np.float32).shape[1]\n",
    "    del Xt_probe, Xb_probe, Xc_probe, Xcode_probe, Xmeta_probe; gc.collect()\n",
    "    # SGDClassifier stores coef_ as float64 by default -> 8 bytes per weight\n",
    "    shard_cap_by_budget = max(1, int(COEF_BUDGET_BYTES // (8 * D)))\n",
    "    dyn_shard_size = max(1, min(2000, shard_cap_by_budget))\n",
    "    print(f'[Fold {fold_id}] Approx feature dim D={D:,}. Dynamic SHARD_SIZE={dyn_shard_size} (budget {COEF_BUDGET_BYTES/1e9:.1f}GB)')\n",
    "\n",
    "    # Shard labels\n",
    "    shards = [kept_labels[i:i+dyn_shard_size] for i in range(0, kept_labels.size, dyn_shard_size)]\n",
    "    print(f'[Fold {fold_id}] #shards: {len(shards)} (size {dyn_shard_size})')\n",
    "\n",
    "    # Train + validate per shard using online binary classifiers\n",
    "    for si, shard in enumerate(shards):\n",
    "        Lb = len(shard)\n",
    "        if Lb == 0:\n",
    "            continue\n",
    "        print(f'[Fold {fold_id}] Shard {si+1}/{len(shards)} with {Lb} labels')\n",
    "\n",
    "        # Create per-label SGD models\n",
    "        models = []\n",
    "        for _ in range(Lb):\n",
    "            models.append(SGDClassifier(loss='log_loss', penalty='l2', alpha=2e-4,\n",
    "                                       max_iter=1, tol=None, random_state=GLOBAL_SEED))  # single-epoch per partial_fit call\n",
    "\n",
    "        # Training: stream over training indices in batches\n",
    "        for b_idx in batch_indices(tr_idx, BATCH_SIZE):\n",
    "            # Build batch features on the fly\n",
    "            X_title = title_vec.transform(title_text.iloc[b_idx]).astype(np.float32).multiply(title_alpha)\n",
    "            X_body  = body_hash.transform(body_text.iloc[b_idx]).astype(np.float32)\n",
    "            X_char  = char_hash.transform((title_text.iloc[b_idx] + ' ' + body_text.iloc[b_idx])).astype(np.float32)\n",
    "            X_code  = code_vec.transform(code_text.iloc[b_idx]).astype(np.float32)\n",
    "            X_meta  = sparse.csr_matrix(meta_scaler.transform(meta_all[b_idx]), dtype=np.float32)\n",
    "            X_batch = sparse.hstack([X_title, X_body, X_char, X_code, X_meta], format='csr', dtype=np.float32)\n",
    "            Y_b = Y_tr_full[b_idx][:, shard].toarray().astype(np.int8, copy=False)\n",
    "            # partial_fit for each label binary model\n",
    "            for j in range(Lb):\n",
    "                yj = Y_b[:, j]\n",
    "                models[j].partial_fit(X_batch, yj, classes=np.array([0,1], dtype=np.int32))\n",
    "            del X_title, X_body, X_char, X_code, X_meta, X_batch, Y_b; gc.collect()\n",
    "\n",
    "        # Validation: stream over validation batches, predict probs and update counts\n",
    "        for b_idx in batch_indices(va_idx, BATCH_SIZE):\n",
    "            X_title = title_vec.transform(title_text.iloc[b_idx]).astype(np.float32).multiply(title_alpha)\n",
    "            X_body  = body_hash.transform(body_text.iloc[b_idx]).astype(np.float32)\n",
    "            X_char  = char_hash.transform((title_text.iloc[b_idx] + ' ' + body_text.iloc[b_idx])).astype(np.float32)\n",
    "            X_code  = code_vec.transform(code_text.iloc[b_idx]).astype(np.float32)\n",
    "            X_meta  = sparse.csr_matrix(meta_scaler.transform(meta_all[b_idx]), dtype=np.float32)\n",
    "            X_batch = sparse.hstack([X_title, X_body, X_char, X_code, X_meta], format='csr', dtype=np.float32)\n",
    "            # Collect probs per label model\n",
    "            P = np.zeros((X_batch.shape[0], Lb), dtype=np.float32)\n",
    "            for j in range(Lb):\n",
    "                try:\n",
    "                    prob = models[j].predict_proba(X_batch)[:, 1]\n",
    "                except Exception:\n",
    "                    from scipy.special import expit\n",
    "                    scores = models[j].decision_function(X_batch)\n",
    "                    prob = expit(scores)\n",
    "                P[:, j] = prob.astype(np.float32, copy=False)\n",
    "            Y_true_batch = Y_va_full[b_idx][:, shard]\n",
    "            update_counts_batch(Y_true_batch.tocsr(), P, shard, THS, tp_tot, fp_tot, fn_tot,\n",
    "                                tp_hi, fp_hi, fn_hi, hi_pos)\n",
    "            del X_title, X_body, X_char, X_code, X_meta, X_batch, Y_true_batch, P; gc.collect()\n",
    "\n",
    "        # Free models\n",
    "        del models\n",
    "        gc.collect()\n",
    "\n",
    "    # Add FN from excluded labels to avoid optimistic bias\n",
    "    fn_excluded = int(Y_va_full[:, excluded_labels].sum()) if excluded_labels.size > 0 else 0\n",
    "    if fn_excluded > 0:\n",
    "        for ti in range(THS.size):\n",
    "            fn_tot[ti] += fn_excluded\n",
    "    print(f'[Fold {fold_id}] Added FN from excluded labels: {fn_excluded}')\n",
    "\n",
    "    del Y_tr_full, Y_va_full\n",
    "    gc.collect()\n",
    "    dt = time.time() - t0\n",
    "    fold_times.append(dt)\n",
    "    print(f'[Fold {fold_id}] Completed in {dt/60:.1f} min')\n",
    "    fold_id += 1\n",
    "\n",
    "    # Micro-pilot: run only the first fold\n",
    "    if MICRO_PILOT:\n",
    "        print('[Pilot] Completed first fold only (micro-pilot mode).')\n",
    "        break\n",
    "\n",
    "print('[CV] Completed. Optimizing thresholds...')\n",
    "f1s = [micro_f1(tp_tot[i], fp_tot[i], fn_tot[i]) for i in range(THS.size)]\n",
    "best_idx = int(np.argmax(f1s))\n",
    "best_thr = float(THS[best_idx])\n",
    "best_f1 = float(f1s[best_idx])\n",
    "print('[OOF] Global best micro-F1 = {:.5f} at thr = {:.3f}'.format(best_f1, best_thr))\n",
    "\n",
    "# Per-tag thresholds for high-support labels only\n",
    "per_tag_thr = np.full(n_labels, best_thr, dtype=np.float32)\n",
    "for k, lab in enumerate(hi_label_idx):\n",
    "    tps = tp_hi[k]; fps = fp_hi[k]; fns = fn_hi[k]\n",
    "    f1s_lab = np.array([micro_f1(tps[i], fps[i], fns[i]) for i in range(THS.size)], dtype=np.float32)\n",
    "    j = int(np.argmax(f1s_lab))\n",
    "    per_tag_thr[lab] = float(THS[j])\n",
    "print('[OOF] Per-tag thresholds computed for', hi_label_idx.size, 'labels; others use global.')\n",
    "\n",
    "# Persist artifacts\n",
    "pd.DataFrame({'label': labels_list}).to_csv('labels.csv', index=False)\n",
    "np.save('per_tag_thresholds.npy', per_tag_thr)\n",
    "np.save('global_threshold.npy', np.array([best_thr], dtype=np.float32))\n",
    "pd.DataFrame({'threshold': THS, 'f1': f1s}).to_csv('oof_global_f1_curve.csv', index=False)\n",
    "print('[Artifacts] Saved labels.csv, per_tag_thresholds.npy, global_threshold.npy, oof_global_f1_curve.csv')\n",
    "\n",
    "# Safety rule for inference\n",
    "def apply_thresholds_with_safety(prob_row, label_indices, per_thr_vec, global_thr, min_k=1):\n",
    "    sel = []\n",
    "    for j in label_indices:\n",
    "        thr = per_thr_vec[j]\n",
    "        if prob_row[j] >= thr:\n",
    "            sel.append(j)\n",
    "    if len(sel) == 0:\n",
    "        if len(label_indices) == 0:\n",
    "            return []\n",
    "        j_top = int(max(label_indices, key=lambda jj: prob_row[jj]))\n",
    "        sel = [j_top]\n",
    "    return sel\n",
    "\n",
    "print('[Phase 2 v3] DONE. Global OOF micro-F1 ~', round(best_f1, 5), 'at thr', round(best_thr, 3))\n",
    "print('[Timing] Avg fold time: {:.1f} min; total: {:.1f} min'.format(np.mean(fold_times)/60.0, np.sum(fold_times)/60.0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
