{
  "cells": [
    {
      "id": "dc293f72-f146-4209-a888-dfc7911f6cd0",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Facebook Recruiting III - Keyword Extraction (MLE-Benchmark)\n",
        "\n",
        "Objective: Predict multi-label tags from question Title + Body. Metric: micro-F1. Goal: WIN A MEDAL (Bronze \u2192 Silver \u2192 Gold).\n",
        "\n",
        "## Strategy Overview (Gold-Trajectory, Concrete)\n",
        "- Text construction and cleaning:\n",
        "  - Parse HTML with BeautifulSoup (lxml parser). Extract:\n",
        "    - Title (plain text)\n",
        "    - Body text without code (strip tags, keep meaningful punctuation)\n",
        "    - Code text from <pre><code> blocks (joined, as separate feature space)\n",
        "  - Normalize: lowercase, collapse whitespace, replace URLs with token URL, emails with EMAIL, numbers with 0, hex with HEX.\n",
        "  - Preserve punctuation for char-ngrams. Remove remaining HTML.\n",
        "- Features (sparse, float32):\n",
        "  - Title TF-IDF: word ngrams 1\u20133, max_df=0.95, min_df\u2208{2\u20135}, max_features 200k\u2013400k, sublinear_tf=True.\n",
        "  - Body TF-IDF: word ngrams 1\u20133, same params, max_features 400k\u2013800k.\n",
        "  - Char TF-IDF (body+title combined): analyzer='char_wb', ngrams 3\u20136, min_df\u2208{2\u20135}, max_features 300k\u2013800k.\n",
        "  - Code TF-IDF: word ngrams 1\u20133, token_pattern to include symbols (e.g., r'(?u)\\b\\w[\\w_\\+\\-\\#\\.]*\\b'), min_df 2\u20135, max_features 100k\u2013200k.\n",
        "  - Combine via hstack with Title boosting \u03b1 \u2208 {2,3,4} (Title features multiplied by \u03b1).\n",
        "  - Meta features (scaled and hstacked via sparse csr): length (title/body/code char and token counts), num_urls, num_code_blocks, punctuation density, avg token length. Use StandardScaler(with_mean=False).\n",
        "- Label space management:\n",
        "  - Primary training retains all tags; for fast iterations, prune to tags with freq \u2265 3 or \u2265 5. Report coverage.\n",
        "  - Compute label coverage % after pruning and ensure final model trains on full label set.\n",
        "- Models:\n",
        "  - Primary: OneVsRest LogisticRegression(solver='saga', penalty='l2', C \u2208 {0.5, 1.0, 1.5, 2.0}, max_iter 2000, n_jobs=-1, class_weight=None).\n",
        "  - Diversity: LinearSVC wrapped in CalibratedClassifierCV(method='sigmoid', cv=3) for probabilities.\n",
        "  - Optional: SGDClassifier(loss='log_loss', early_stopping=True) for quick checks.\n",
        "- Thresholding (critical):\n",
        "  - Optimize global threshold t \u2208 [0.05, 0.6].\n",
        "  - Then per-tag thresholds t_k for tags with support \u2265 50 (else fall back to global). Optimize on OOF to maximize micro-F1.\n",
        "  - Safety rule: if no tag exceeds threshold for a sample, emit top-1 highest score (optionally min_k tuned in OOF).\n",
        "- Ensembling & dependencies:\n",
        "  - Blend probabilities from LR(word+char+code+meta) and calibrated SVC; optionally add char-only LR with 0.2\u20130.4 weight. Tune weights on OOF.\n",
        "  - Optional small co-occurrence nudge (validated): add +0.01\u20130.02 to scores of frequent tag pairs before thresholding.\n",
        "- Contingency (if CV stalls < gold):\n",
        "  - Classifier Chains with LR on frequent labels subset.\n",
        "  - Lightweight transformer (DistilBERT/RoBERTa-base) multi-label head (2\u20133 epochs) and ensemble with linear stack.\n",
        "\n",
        "## Formal CV & OOF Protocol (Leak-proof)\n",
        "- Random seeds: global_seed=1337; fold seeds fixed [1337, 2025, 4242, 7777, 8888]. All PRNGs seeded.\n",
        "- Folds: 5-fold iterative stratification (iterative-stratification library). For speed iterations, 3 folds.\n",
        "- Within each fold:\n",
        "  - Fit vectorizers (Title/Body/Char/Code) on training split only.\n",
        "  - Fit scaler for meta-features on training split only.\n",
        "  - Train model on training split; predict probabilities on validation split to produce OOF.\n",
        "- OOF predictions:\n",
        "  - Store per-model OOF probs for every label; use exclusively for threshold optimization, ensembling weight tuning, and error analysis.\n",
        "  - Report OOF micro-F1 using optimized thresholds (global and per-tag).\n",
        "- Final model for inference:\n",
        "  - Refit vectorizers and models on full training data using best hyperparams and thresholds determined on OOF.\n",
        "\n",
        "## Milestones with Acceptance Criteria\n",
        "1) Data load + schema + dummy submission [Checkpoint]\n",
        "   - Validate reading train/test; inspect columns; ensure Tags is space-separated.\n",
        "   - Create a tiny dummy submission with correct format (Id, Tags) and pass local schema checks.\n",
        "   - Acceptance: dummy submission created; basic EDA table produced; no memory issues.\n",
        "2) Baseline OVR-LR with Title/Body concat (word+char) [Checkpoint]\n",
        "   - 3-fold iterative CV; global threshold sweep; no code/meta features yet.\n",
        "   - Acceptance: OOF micro-F1 \u2265 0.70, train time \u2264 60 min, feature_dim \u2264 1.2M.\n",
        "3) Full feature stack (Title boost, Char WB, Code, Meta) + per-tag thresholds [Checkpoint]\n",
        "   - 5-fold iterative CV; per-tag thresholds for support \u2265 50.\n",
        "   - Acceptance: OOF micro-F1 \u2265 0.75, train time \u2264 120 min, label_coverage \u2265 98%.\n",
        "4) Model diversity + blending [Checkpoint]\n",
        "   - Add calibrated LinearSVC; blend; re-tune thresholds on blended OOF.\n",
        "   - Acceptance: OOF micro-F1 \u2265 0.77, stable across folds (std \u2264 0.005).\n",
        "5) Error analysis + targeted fixes [Checkpoint]\n",
        "   - Analyze by tag frequency buckets, top false negatives/positives, tag co-occurrence errors; adjust features (e.g., code tokenizer, title alpha, min_df) accordingly.\n",
        "   - Acceptance: Documented improvements with before/after OOF; no regression in \u226580% of folds.\n",
        "6) Final training + test inference + submission.csv [Final]\n",
        "   - Generate predictions with safety rule (min 1 tag); save submission.csv.\n",
        "   - Acceptance: passes local validator; sizes sane; backed by notebook log.\n",
        "\n",
        "## Error Analysis Loop (after first baseline)\n",
        "- Report per-tag precision/recall, support; confusion via co-occurrence matrix deltas.\n",
        "- Inspect 50 worst FNs/FPs; categorize by: code-heavy, very short title, rare tags, OOV tokens.\n",
        "- Actions: adjust title weighting, char n-gram range, code token pattern, min_df, and thresholds.\n",
        "\n",
        "## Experiment Log (Reproducibility-Grade)\n",
        "| ID | Date/Time | random_seed | fold_scheme | label_pruning_rule | vec_title(word ngram, min_df, max_df, max_feat, sublinear) | vec_body | vec_char | vec_code | meta_feats | model(hypers) | thresholding_method | feature_dim | label_coverage_% | OOF micro-F1 | Holdout micro-F1 | train_time | peak_memory | notes |\n",
        "|----|-----------|-------------|-------------|--------------------|--------------------------------------------------------------|----------|----------|----------|------------|----------------|---------------------|-------------|------------------|--------------|------------------|------------|-------------|-------|\n",
        "| 001 | | 1337 | 3-fold iter | freq\u22655 | title w(1-2) min_df=3 max_df=0.95 max_feat=200k sub=True | body w(1-2) ... | char wb(3-6) | code w(1-2) | len,url,code, punct | OVR-LR C=1.0 saga | global sweep |  |  |  |  |  |  | baseline |\n",
        "| 002 | | 1337 | 5-fold iter | none | title w(1-3) ... | body w(1-3) ... | char wb(3-6) | code w(1-3) | +scaled | OVR-LR C=1.5 saga | per-tag + min1 |  |  |  |  |  |  |  |\n",
        "| 003 | | 1337 | 5-fold iter | none | ... | ... | ... | ... | ... | Calibrated LinearSVC blend 30% | per-tag |  |  |  |  |  |  |  |\n",
        "\n",
        "## Notebook Hygiene & Repro Policy\n",
        "- Every milestone: backup notebook programmatically, then DELETE stale code after logging results here.\n",
        "- Use CSR sparse float32 matrices; cap max_features to avoid OOM; prefer HashingVectorizer+TfidfTransformer if memory-bound.\n",
        "- All vectorizers/scalers fit within folds only; refit on full train for final model.\n",
        "- Submit for audit at each checkpoint.\n",
        "\n",
        "## Risks / Mitigations\n",
        "- Memory/time: limit feature dims; parallelize OVR; possibly train top-N frequent labels first to validate pipeline.\n",
        "- Long-tail labels: per-tag thresholds; do not drop rare labels in final training.\n",
        "- Format issues: early dummy submission to validate schema.\n",
        "\n",
        "Proceed to: data loading, schema check, dummy submission, and EDA."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "a36df557-2d06-4f89-875c-b6903d812e5d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Milestone 1: Data loading, schema validation, dummy submission, initial EDA, and early memory optimization (clean)\n",
        "import os, sys, json, shutil, gc, re, time, math, random\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Reproducibility\n",
        "GLOBAL_SEED = 1337\n",
        "random.seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "\n",
        "# Utility: backup notebook\n",
        "def backup_notebook():\n",
        "    nb_path = 'agent_notebook.ipynb'\n",
        "    if os.path.exists(nb_path):\n",
        "        ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        dst = f'agent_notebook_backup_{ts}.ipynb'\n",
        "        shutil.copy2(nb_path, dst)\n",
        "        print(f'[Backup] Notebook copied to {dst}')\n",
        "    else:\n",
        "        print('[Backup] Notebook file not found; skipping backup.')\n",
        "backup_notebook()\n",
        "\n",
        "# HTML parsing and basic cleaning utilities\n",
        "URL_RE = re.compile(r'https?://\\S+|www\\.\\S+', flags=re.IGNORECASE)\n",
        "EMAIL_RE = re.compile(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.[A-Za-z]{2,}\\b')\n",
        "HEX_RE = re.compile(r'\\b0x[0-9A-Fa-f]+\\b')\n",
        "NUM_RE = re.compile(r'\\b\\d+\\b')\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return ''\n",
        "    s = s.lower()\n",
        "    s = URL_RE.sub(' URL ', s)\n",
        "    s = EMAIL_RE.sub(' EMAIL ', s)\n",
        "    s = HEX_RE.sub(' HEX ', s)\n",
        "    s = NUM_RE.sub(' 0 ', s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "def extract_text_and_code_pre_only(html: str):\n",
        "    \"\"\"\n",
        "    Extracts block-level code within <pre>...</pre> (including nested <code>) as code_text,\n",
        "    keeps inline <code> in body text. Returns:\n",
        "    (text_wo_block_code, block_code_text, num_block_code_blocks, url_count, punct_density)\n",
        "    \"\"\"\n",
        "    if not isinstance(html, str):\n",
        "        return '', '', 0, 0, 0.0\n",
        "    soup = BeautifulSoup(html, 'lxml')\n",
        "    pre_blocks = soup.find_all('pre')\n",
        "    code_texts = []\n",
        "    for pre in pre_blocks:\n",
        "        code_texts.append(pre.get_text(' ', strip=True))\n",
        "        pre.extract()\n",
        "    text = soup.get_text(' ', strip=True)\n",
        "    text_norm = normalize_text(text)\n",
        "    code_norm = normalize_text(' \\n '.join(code_texts))\n",
        "    url_count = len(URL_RE.findall(text.lower()))\n",
        "    puncts = re.findall(r'[\\!\\?\\.,;:\\-\\(\\)\\[\\]\\{\\}\\#\\+\\*/\\\\\\|\\<\\>\\=\\_\\~\\^\\`\\\"]', text_norm)\n",
        "    punct_density = (len(puncts) / max(1, len(text_norm)))\n",
        "    return text_norm, code_norm, len(pre_blocks), url_count, punct_density\n",
        "\n",
        "# Paths\n",
        "TRAIN_PATH = 'train.csv'\n",
        "TEST_PATH = 'test.csv'\n",
        "SAMPLE_SUB_PATH = 'sample_submission.csv'\n",
        "\n",
        "print('[Info] Listing CWD files:')\n",
        "print('\\n'.join(sorted(os.listdir('.'))))\n",
        "\n",
        "# Load train/test\n",
        "t0 = time.time()\n",
        "train = pd.read_csv(TRAIN_PATH)\n",
        "test = pd.read_csv(TEST_PATH)\n",
        "print(f'[Load] train shape={train.shape}, test shape={test.shape}, time={time.time()-t0:.2f}s')\n",
        "print('[Schema] train columns:', train.columns.tolist())\n",
        "print('[Schema] test columns:', test.columns.tolist())\n",
        "\n",
        "# Early memory optimization (downcast + category) and schema validation\n",
        "def mem_usage_mb(df: pd.DataFrame) -> float:\n",
        "    return round(df.memory_usage(deep=True).sum() / (1024**2), 2)\n",
        "print('[Memory] BEFORE optimization - train:', mem_usage_mb(train), 'MB; test:', mem_usage_mb(test), 'MB')\n",
        "for df_name, df in [('train', train), ('test', test)]:\n",
        "    if 'Id' in df.columns and pd.api.types.is_integer_dtype(df['Id']):\n",
        "        df['Id'] = pd.to_numeric(df['Id'], downcast='integer')\n",
        "        print(f'[Memory] {df_name}.Id dtype ->', df['Id'].dtype)\n",
        "if 'Tags' in train.columns and train['Tags'].dtype == object:\n",
        "    train['Tags'] = train['Tags'].astype('category')\n",
        "    print('[Memory] train.Tags dtype ->', train['Tags'].dtype)\n",
        "print('[Memory] AFTER optimization  - train:', mem_usage_mb(train), 'MB; test:', mem_usage_mb(test), 'MB')\n",
        "# Minor schema validation for Tags delimiters\n",
        "if 'Tags' in train.columns:\n",
        "    tags_str = train['Tags'].astype(str)\n",
        "    bad_delim_mask = tags_str.str.contains(r'[;,\\|]', regex=True)\n",
        "    n_bad = int(bad_delim_mask.sum())\n",
        "    print(f\"[Schema Check] Rows with unexpected delimiters in Tags (commas/semicolons/pipes): {n_bad}\")\n",
        "\n",
        "# Basic checks\n",
        "required_train_cols = {'Id','Title','Body','Tags'}\n",
        "required_test_cols = {'Id','Title','Body'}\n",
        "missing_train = required_train_cols - set(train.columns)\n",
        "missing_test = required_test_cols - set(test.columns)\n",
        "if missing_train:\n",
        "    print('[Warning] Missing expected train columns:', missing_train)\n",
        "if missing_test:\n",
        "    print('[Warning] Missing expected test columns:', missing_test)\n",
        "\n",
        "print('\\n[Head] train:')\n",
        "display(train.head(3))\n",
        "print('\\n[Head] test:')\n",
        "display(test.head(3))\n",
        "\n",
        "# Create and validate a tiny dummy submission to ensure schema is right\n",
        "def make_dummy_submission(output_path='submission_dummy.csv', n_rows=5):\n",
        "    if os.path.exists(SAMPLE_SUB_PATH):\n",
        "        ss = pd.read_csv(SAMPLE_SUB_PATH)\n",
        "        if 'Id' in ss.columns and 'Id' in test.columns:\n",
        "            ss = ss[ss['Id'].isin(test['Id'])]\n",
        "        ss_small = ss.head(n_rows).copy()\n",
        "        ss_small.to_csv(output_path, index=False)\n",
        "        print(f'[Dummy] Wrote {output_path} with {len(ss_small)} rows from sample_submission.csv')\n",
        "    else:\n",
        "        sub = pd.DataFrame({'Id': test['Id'], 'Tags': ['tag'] * len(test)})\n",
        "        sub.head(n_rows).to_csv(output_path, index=False)\n",
        "        print(f'[Dummy] Wrote {output_path} with {n_rows} rows constructed from test Ids')\n",
        "make_dummy_submission()\n",
        "\n",
        "# Initial EDA: label stats\n",
        "def split_tags(s):\n",
        "    if isinstance(s, str):\n",
        "        return s.strip().split()\n",
        "    return []\n",
        "tag_lists = train['Tags'].astype(str).apply(split_tags)\n",
        "train['__n_tags__'] = tag_lists.apply(len)\n",
        "avg_tags = train['__n_tags__'].mean()\n",
        "print(f'[EDA] Avg tags per sample: {avg_tags:.3f}')\n",
        "print(f'[EDA] Min/Max tags per sample: {train[\"__n_tags__\"].min()} / {train[\"__n_tags__\"].max()}')\n",
        "top_tags = pd.Series([t for lst in tag_lists for t in lst]).value_counts().head(20)\n",
        "print('[EDA] Top-20 tags by frequency:')\n",
        "display(top_tags)\n",
        "\n",
        "# Quick parsing demo on a few rows to validate HTML/code extraction (using <pre>-only)\n",
        "demo_rows = train.sample(min(3, len(train)), random_state=GLOBAL_SEED)\n",
        "demo_out = []\n",
        "for _, r in demo_rows.iterrows():\n",
        "    body_txt, code_txt, n_code, n_url, pden = extract_text_and_code_pre_only(r.get('Body', ''))\n",
        "    title_txt = normalize_text(r.get('Title',''))\n",
        "    demo_out.append({\n",
        "        'Id': r.get('Id'),\n",
        "        'title_len': len(title_txt),\n",
        "        'body_len': len(body_txt),\n",
        "        'code_len': len(code_txt),\n",
        "        'num_block_code': n_code,\n",
        "        'num_urls': n_url,\n",
        "        'punct_density': round(pden, 4),\n",
        "        'tags': str(r.get('Tags'))[:80] if not pd.isna(r.get('Tags')) else ''\n",
        "    })\n",
        "print('[Parse Demo] Title/Body/Code lengths and meta for a few samples:')\n",
        "display(pd.DataFrame(demo_out))\n",
        "\n",
        "# Memory footprint info (post-optimization)\n",
        "print('[Memory] train dtypes:')\n",
        "print(train.dtypes)\n",
        "print('[Memory] train memory usage (MB):', round(train.memory_usage(deep=True).sum() / (1024**2), 2))\n",
        "print('[Memory] test memory usage (MB):', round(test.memory_usage(deep=True).sum() / (1024**2), 2))\n",
        "\n",
        "print('[Milestone 1] Completed data load, schema check, dummy submission, EDA, and early memory optimization (clean).')\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "5394c276-7946-48d2-a7ca-b33740d62279",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install missing dependencies for HTML parsing\n",
        "%pip install --quiet beautifulsoup4 lxml\n",
        "import importlib\n",
        "assert importlib.util.find_spec('bs4') is not None, 'bs4 not installed'\n",
        "assert importlib.util.find_spec('lxml') is not None, 'lxml not installed'\n",
        "print('Dependencies installed: beautifulsoup4, lxml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "ea6598f5-bd2d-41e9-9b6e-e9df534074b4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Milestone 1 fixes (headless-safe, no matplotlib): parsing correction + expanded EDA outputs to files\n",
        "import os, re, math, random, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "random.seed(GLOBAL_SEED if 'GLOBAL_SEED' in globals() else 1337)\n",
        "np.random.seed(GLOBAL_SEED if 'GLOBAL_SEED' in globals() else 1337)\n",
        "\n",
        "# Ensure train/test exist from previous cells\n",
        "assert 'train' in globals() and 'test' in globals(), 'train/test DataFrames not found in environment.'\n",
        "\n",
        "# Define regex and normalizer if missing\n",
        "if 'URL_RE' not in globals():\n",
        "    URL_RE = re.compile(r'https?://\\S+|www\\.\\S+', flags=re.IGNORECASE)\n",
        "if 'EMAIL_RE' not in globals():\n",
        "    EMAIL_RE = re.compile(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.[A-Za-z]{2,}\\b')\n",
        "if 'HEX_RE' not in globals():\n",
        "    HEX_RE = re.compile(r'\\b0x[0-9A-Fa-f]+\\b')\n",
        "if 'NUM_RE' not in globals():\n",
        "    NUM_RE = re.compile(r'\\b\\d+\\b')\n",
        "if 'normalize_text' not in globals():\n",
        "    def normalize_text(s: str) -> str:\n",
        "        if not isinstance(s, str):\n",
        "            return ''\n",
        "        s = s.lower()\n",
        "        s = URL_RE.sub(' URL ', s)\n",
        "        s = EMAIL_RE.sub(' EMAIL ', s)\n",
        "        s = HEX_RE.sub(' HEX ', s)\n",
        "        s = NUM_RE.sub(' 0 ', s)\n",
        "        s = re.sub(r'\\s+', ' ', s).strip()\n",
        "        return s\n",
        "\n",
        "# Correct parsing: only block-level code from <pre> (keeping inline <code> in body)\n",
        "def extract_text_and_code_pre_only(html: str):\n",
        "    if not isinstance(html, str):\n",
        "        return '', '', 0, 0, 0.0\n",
        "    soup = BeautifulSoup(html, 'lxml')\n",
        "    pre_blocks = soup.find_all('pre')\n",
        "    code_texts = []\n",
        "    for pre in pre_blocks:\n",
        "        code_texts.append(pre.get_text(' ', strip=True))\n",
        "        pre.extract()\n",
        "    text = soup.get_text(' ', strip=True)\n",
        "    text_norm = normalize_text(text)\n",
        "    code_norm = normalize_text(' \\n '.join(code_texts))\n",
        "    url_count = len(URL_RE.findall(text.lower()))\n",
        "    puncts = re.findall(r'[\\!\\?\\.,;:\\-\\(\\)\\[\\]\\{\\}\\#\\+\\*/\\\\\\|\\<\\>\\=\\_\\~\\^\\`\\\"]', text_norm)\n",
        "    punct_density = (len(puncts) / max(1, len(text_norm)))\n",
        "    return text_norm, code_norm, len(pre_blocks), url_count, punct_density\n",
        "\n",
        "out_dir = '.'\n",
        "\n",
        "# Label space analysis and frequency CSV (top 5000)\n",
        "tag_lists = train['Tags'].apply(lambda s: s.split() if isinstance(s, str) else [])\n",
        "all_tags = pd.Series([t for lst in tag_lists for t in lst])\n",
        "freq = all_tags.value_counts()\n",
        "unique_tags = int(freq.shape[0])\n",
        "freq_head = freq.head(5000).rename('count').to_frame()\n",
        "freq_head.index.name = 'tag'\n",
        "freq_head.reset_index().to_csv(os.path.join(out_dir, 'eda_tag_freq_top5000.csv'), index=False)\n",
        "with open(os.path.join(out_dir, 'eda_tag_freq_summary.txt'), 'w') as f:\n",
        "    f.write(f'Total unique tags: {unique_tags}\\n')\n",
        "    f.write(f'min={int(freq.min())}, median={int(freq.median())}, mean={freq.mean():.4f}, max={int(freq.max())}\\n')\n",
        "print('[EDA] Saved tag frequency CSV and summary stats.')\n",
        "\n",
        "# Co-occurrence top-50 on sample, save as CSV\n",
        "K = 50\n",
        "topK = list(freq.head(K).index)\n",
        "idx_map = {t:i for i,t in enumerate(topK)}\n",
        "sample_size = min(200_000, len(train))\n",
        "sample_idx = train.sample(sample_size, random_state=(GLOBAL_SEED if 'GLOBAL_SEED' in globals() else 1337)).index\n",
        "co_mat = np.zeros((K, K), dtype=np.int32)\n",
        "for tags in tag_lists.loc[sample_idx]:\n",
        "    present = [idx_map[t] for t in tags if t in idx_map]\n",
        "    for i in range(len(present)):\n",
        "        a = present[i]\n",
        "        for j in range(i, len(present)):\n",
        "            b = present[j]\n",
        "            co_mat[a, b] += 1\n",
        "            if a != b:\n",
        "                co_mat[b, a] += 1\n",
        "co_df = pd.DataFrame(co_mat, index=topK, columns=topK)\n",
        "co_df.to_csv(os.path.join(out_dir, 'eda_top50_cooccurrence.csv'))\n",
        "print('[EDA] Saved co-occurrence matrix CSV for top-50 tags.')\n",
        "\n",
        "# Length distributions (quantiles + hist counts) for Title/Body/Code on sample\n",
        "sample_size_len = min(100_000, len(train))\n",
        "sample_df = train.sample(sample_size_len, random_state=(GLOBAL_SEED if 'GLOBAL_SEED' in globals() else 1337))\n",
        "title_lens = sample_df['Title'].fillna('').apply(lambda s: len(normalize_text(s)))\n",
        "body_len_list = []\n",
        "code_len_list = []\n",
        "for html in sample_df['Body'].fillna(''):\n",
        "    body_txt, code_txt, _, _, _ = extract_text_and_code_pre_only(html)\n",
        "    body_len_list.append(len(body_txt))\n",
        "    code_len_list.append(len(code_txt))\n",
        "lengths = {\n",
        "    'title': np.array(title_lens, dtype=np.int32),\n",
        "    'body': np.array(body_len_list, dtype=np.int32),\n",
        "    'code': np.array(code_len_list, dtype=np.int32)\n",
        "}\n",
        "quantiles = [0.5, 0.75, 0.9, 0.95, 0.99]\n",
        "rows = []\n",
        "for name, arr in lengths.items():\n",
        "    qs = np.quantile(arr, quantiles)\n",
        "    rows.append({'channel': name, 'mean': float(arr.mean()), 'std': float(arr.std()), 'min': int(arr.min()), 'max': int(arr.max()), **{f'q{int(q*100)}': float(val) for q, val in zip(quantiles, qs)}})\n",
        "pd.DataFrame(rows).to_csv(os.path.join(out_dir, 'eda_length_stats.csv'), index=False)\n",
        "\n",
        "# Also save histogram bin counts (no plotting)\n",
        "hist_rows = []\n",
        "for name, arr in lengths.items():\n",
        "    counts, edges = np.histogram(arr, bins=50)\n",
        "    hist_rows.append(pd.DataFrame({'channel': name, 'bin_left': edges[:-1], 'bin_right': edges[1:], 'count': counts}))\n",
        "pd.concat(hist_rows, ignore_index=True).to_csv(os.path.join(out_dir, 'eda_length_hist_counts.csv'), index=False)\n",
        "print('[EDA] Saved length stats and histogram counts CSVs.')\n",
        "\n",
        "# Zero-tag analysis\n",
        "n_tags_col_exists = '__n_tags__' in train.columns\n",
        "zero_tag_mask = (train['__n_tags__'] == 0) if n_tags_col_exists else (tag_lists.apply(len) == 0)\n",
        "n_zero = int(zero_tag_mask.sum())\n",
        "pct_zero = 100.0 * n_zero / len(train)\n",
        "print(f\"[EDA] Samples with zero tags: {n_zero} ({pct_zero:.3f}%)\")\n",
        "train.loc[zero_tag_mask, ['Id', 'Title']].head(20).to_csv(os.path.join(out_dir, 'eda_zero_tag_samples_head.csv'), index=False)\n",
        "print('[EDA] Saved head(20) of zero-tag samples to CSV.')\n",
        "\n",
        "print('[Milestone 1 Fixes - Headless] Completed parsing correction and expanded EDA (CSV outputs).')\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "e1aea48e-ed2e-47bf-9e63-d72a675210aa",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Dedicated memory optimization + schema validation + notebook hygiene fixes\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "assert 'train' in globals() and 'test' in globals(), 'train/test DataFrames not found.'\n",
        "\n",
        "def mem_usage_mb(df: pd.DataFrame) -> float:\n",
        "    return round(df.memory_usage(deep=True).sum() / (1024**2), 2)\n",
        "\n",
        "print('[Memory] BEFORE optimization - train:', mem_usage_mb(train), 'MB; test:', mem_usage_mb(test), 'MB')\n",
        "\n",
        "# Downcast Id columns to reduce footprint\n",
        "for df_name, df in [('train', train), ('test', test)]:\n",
        "    if 'Id' in df.columns and pd.api.types.is_integer_dtype(df['Id']):\n",
        "        df['Id'] = pd.to_numeric(df['Id'], downcast='integer')\n",
        "        print(f'[Memory] {df_name}.Id dtype ->', df['Id'].dtype)\n",
        "\n",
        "# Convert Tags to category (kept as strings later for ML when needed)\n",
        "if 'Tags' in train.columns and train['Tags'].dtype == object:\n",
        "    train['Tags'] = train['Tags'].astype('category')\n",
        "    print('[Memory] train.Tags dtype ->', train['Tags'].dtype)\n",
        "\n",
        "print('[Memory] AFTER optimization  - train:', mem_usage_mb(train), 'MB; test:', mem_usage_mb(test), 'MB')\n",
        "\n",
        "# Minor EDA validation: ensure Tags schema is space-separated without other delimiters\n",
        "if 'Tags' in train.columns:\n",
        "    # Convert to string temporarily for regex checks (category -> str)\n",
        "    tags_str = train['Tags'].astype(str)\n",
        "    bad_delim_mask = tags_str.str.contains(r'[;,\\|]', regex=True)\n",
        "    n_bad = int(bad_delim_mask.sum())\n",
        "    print(f\"[Schema Check] Rows with unexpected delimiters in Tags (commas/semicolons/pipes): {n_bad}\")\n",
        "    if n_bad > 0:\n",
        "        display(train.loc[bad_delim_mask, ['Id','Title','Tags']].head(5))\n",
        "\n",
        "# Notebook hygiene: remove legacy incorrect parsing function from namespace, ensure only <pre>-aware version stays\n",
        "removed_legacy = False\n",
        "if 'extract_text_and_code' in globals():\n",
        "    try:\n",
        "        del globals()['extract_text_and_code']\n",
        "        removed_legacy = True\n",
        "    except Exception as e:\n",
        "        print('[Hygiene] Could not delete legacy function:', e)\n",
        "print('[Hygiene] Legacy extract_text_and_code removed from namespace:', removed_legacy)\n",
        "\n",
        "# Optional: expose the correct function under a common name for downstream code (without redefining)\n",
        "if 'extract_text_and_code_pre_only' in globals():\n",
        "    extract_text_and_code = extract_text_and_code_pre_only  # alias to the correct implementation\n",
        "    print('[Hygiene] Alias set: extract_text_and_code -> extract_text_and_code_pre_only')\n",
        "\n",
        "print('[Milestone 1] Memory optimization and schema validation complete. Ready for audit.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "5f06a7f3-11de-40fb-841d-8e28f0fafcc9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment hygiene: suppress matplotlib_inline post-run callback errors (non-blocking noise)\n",
        "from IPython import get_ipython\n",
        "print('[Env] Activating matplotlib_inline suppression...')\n",
        "try:\n",
        "    import matplotlib as _mpl\n",
        "    # Monkey-patch missing attribute to satisfy backend_inline access\n",
        "    if not hasattr(_mpl, 'backend_bases'):\n",
        "        _mpl.backend_bases = type('backend_bases', (), {'_Backend': type('_Backend', (), {})})\n",
        "    # Prefer headless backend\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        try:\n",
        "            plt.switch_backend('Agg')\n",
        "        except Exception:\n",
        "            pass\n",
        "    except Exception:\n",
        "        pass\n",
        "    ip = get_ipython()\n",
        "    if ip is not None and hasattr(ip, 'events'):\n",
        "        # Remove any matplotlib_inline configure_once callback from post_run_cell\n",
        "        cbs = ip.events.callbacks.get('post_run_cell', [])\n",
        "        new_cbs = []\n",
        "        for cb in cbs:\n",
        "            name = getattr(cb, '__name__', '')\n",
        "            mod = getattr(cb, '__module__', '')\n",
        "            qual = getattr(cb, '__qualname__', '')\n",
        "            # Heuristic: drop callbacks coming from matplotlib_inline backend_inline\n",
        "            if ('matplotlib_inline' in mod) or ('backend_inline' in mod) or (name == 'configure_once') or ('configure_once' in qual):\n",
        "                continue\n",
        "            new_cbs.append(cb)\n",
        "        ip.events.callbacks['post_run_cell'] = new_cbs\n",
        "    print('[Env] matplotlib_inline suppression active.')\n",
        "except Exception as e:\n",
        "    print('[Env] matplotlib_inline suppression failed:', repr(e))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "759685e8-f79b-492b-b0df-4e56656eea75",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Milestone 2 Setup (Clean, Linear): Cache parsed fields to Pickle for fast, leak-proof CV\n",
        "# - Consolidates required M1 logic (HTML parsing, normalization) into a single executable cell\n",
        "# - Processes train/test in chunks, extracts title_norm, body_wo_pre, code_pre, and meta\n",
        "# - Saves compact pickle caches to disk (avoids pyarrow/fastparquet issues)\n",
        "\n",
        "import os, re, gc, time, math, shutil\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "GLOBAL_SEED = 1337\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "\n",
        "def backup_notebook():\n",
        "    nb_path = 'agent_notebook.ipynb'\n",
        "    if os.path.exists(nb_path):\n",
        "        ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        dst = f'agent_notebook_backup_{ts}.ipynb'\n",
        "        shutil.copy2(nb_path, dst)\n",
        "        print(f'[Backup] Notebook copied to {dst}')\n",
        "backup_notebook()\n",
        "\n",
        "# Robust normalization and HTML parsing (block-level code only)\n",
        "URL_RE   = re.compile(r'https?://\\S+|www\\.\\S+', flags=re.IGNORECASE)\n",
        "EMAIL_RE = re.compile(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.[A-Za-z]{2,}\\b')\n",
        "HEX_RE   = re.compile(r'\\b0x[0-9A-Fa-f]+\\b')\n",
        "NUM_RE   = re.compile(r'\\b\\d+\\b')\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return ''\n",
        "    s = s.lower()\n",
        "    s = URL_RE.sub(' URL ', s)\n",
        "    s = EMAIL_RE.sub(' EMAIL ', s)\n",
        "    s = HEX_RE.sub(' HEX ', s)\n",
        "    s = NUM_RE.sub(' 0 ', s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "def extract_text_and_code_pre_only(html: str):\n",
        "    if not isinstance(html, str):\n",
        "        return '', '', 0, 0, 0.0\n",
        "    soup = BeautifulSoup(html, 'lxml')\n",
        "    pre_blocks = soup.find_all('pre')\n",
        "    code_texts = []\n",
        "    for pre in pre_blocks:\n",
        "        code_texts.append(pre.get_text(' ', strip=True))\n",
        "        pre.extract()\n",
        "    text = soup.get_text(' ', strip=True)\n",
        "    text_norm = normalize_text(text)\n",
        "    code_norm = normalize_text(' \\n '.join(code_texts))\n",
        "    url_count = len(URL_RE.findall(text.lower()))\n",
        "    puncts = re.findall(r'[\\!\\?\\.,;:\\-\\(\\)\\[\\]\\{\\}\\#\\+\\*/\\\\\\|\\<\\>\\=\\_\\~\\^\\`\\\"]', text_norm)\n",
        "    punct_density = (len(puncts) / max(1, len(text_norm)))\n",
        "    return text_norm, code_norm, len(pre_blocks), url_count, punct_density\n",
        "\n",
        "# Chunked cache builder -> returns a single DataFrame (accumulated) to be written once to pickle\n",
        "def build_cache(csv_path: str, is_train: bool = True, chunksize: int = 100_000, subsample_frac: float | None = None) -> pd.DataFrame:\n",
        "    t0 = time.time()\n",
        "    all_parts = []\n",
        "    usecols = ['Id', 'Title', 'Body'] + (['Tags'] if is_train else [])\n",
        "    reader = pd.read_csv(csv_path, usecols=usecols, chunksize=chunksize)\n",
        "    total_rows = 0\n",
        "    for i, chunk in enumerate(reader):\n",
        "        if subsample_frac is not None and 0 < subsample_frac < 1:\n",
        "            chunk = chunk.sample(frac=subsample_frac, random_state=GLOBAL_SEED)\n",
        "        if 'Id' in chunk.columns:\n",
        "            chunk['Id'] = pd.to_numeric(chunk['Id'], downcast='integer')\n",
        "        out_records = {\n",
        "            'Id': [], 'title_norm': [], 'body_text': [], 'code_text': [],\n",
        "            'title_len': [], 'body_len': [], 'code_len': [], 'num_block_code': [], 'num_urls': [], 'punct_density': []\n",
        "        }\n",
        "        if is_train:\n",
        "            out_records['Tags'] = []\n",
        "        titles = chunk['Title'].fillna('').astype(str).tolist()\n",
        "        titles_norm = [normalize_text(t) for t in titles]\n",
        "        bodies = chunk['Body'].fillna('').astype(str).tolist()\n",
        "        for idx in range(len(chunk)):\n",
        "            body_txt, code_txt, n_code, n_url, pden = extract_text_and_code_pre_only(bodies[idx])\n",
        "            out_records['Id'].append(int(chunk.iloc[idx]['Id']))\n",
        "            out_records['title_norm'].append(titles_norm[idx])\n",
        "            out_records['body_text'].append(body_txt)\n",
        "            out_records['code_text'].append(code_txt)\n",
        "            out_records['title_len'].append(len(titles_norm[idx]))\n",
        "            out_records['body_len'].append(len(body_txt))\n",
        "            out_records['code_len'].append(len(code_txt))\n",
        "            out_records['num_block_code'].append(int(n_code))\n",
        "            out_records['num_urls'].append(int(n_url))\n",
        "            out_records['punct_density'].append(float(pden))\n",
        "            if is_train:\n",
        "                out_records['Tags'].append(chunk.iloc[idx]['Tags'])\n",
        "        out_df = pd.DataFrame(out_records)\n",
        "        all_parts.append(out_df)\n",
        "        total_rows += len(out_df)\n",
        "        if (i + 1) % 5 == 0:\n",
        "            print(f'[Cache] Processed ~{total_rows} rows so far for {os.path.basename(csv_path)}')\n",
        "        del chunk, out_df, out_records, titles, titles_norm, bodies\n",
        "        gc.collect()\n",
        "    if len(all_parts) == 0:\n",
        "        result = pd.DataFrame(columns=['Id','title_norm','body_text','code_text','title_len','body_len','code_len','num_block_code','num_urls','punct_density'] + (['Tags'] if is_train else []))\n",
        "    else:\n",
        "        result = pd.concat(all_parts, ignore_index=True)\n",
        "    print(f'[Cache] Built DataFrame with {len(result)} rows in {time.time()-t0:.1f}s from {os.path.basename(csv_path)}')\n",
        "    return result\n",
        "\n",
        "# Paths and configuration\n",
        "TRAIN_CSV = 'train.csv'\n",
        "TEST_CSV  = 'test.csv'\n",
        "PARSED_TRAIN_PKL = 'parsed_train.pkl'\n",
        "PARSED_TEST_PKL  = 'parsed_test.pkl'\n",
        "\n",
        "# For speed during first baseline, use 5% of train; keep full test to enable full inference later\n",
        "SUBSAMPLE_FRAC_TRAIN = 0.05  # quick baseline cache on 5% of train; set to None for full cache later\n",
        "SUBSAMPLE_FRAC_TEST  = None  # do not subsample test\n",
        "\n",
        "if not os.path.exists(PARSED_TRAIN_PKL):\n",
        "    print('[Cache] Creating train cache (pickle) ...')\n",
        "    df_train_cache = build_cache(TRAIN_CSV, is_train=True, chunksize=50_000, subsample_frac=SUBSAMPLE_FRAC_TRAIN)\n",
        "    df_train_cache.to_pickle(PARSED_TRAIN_PKL)\n",
        "    print(f'[Cache] Wrote {PARSED_TRAIN_PKL} ({len(df_train_cache)} rows)')\n",
        "    del df_train_cache; gc.collect()\n",
        "else:\n",
        "    print('[Cache] Found existing', PARSED_TRAIN_PKL)\n",
        "\n",
        "if not os.path.exists(PARSED_TEST_PKL):\n",
        "    print('[Cache] Creating test cache (pickle) ...')\n",
        "    df_test_cache = build_cache(TEST_CSV, is_train=False, chunksize=50_000, subsample_frac=SUBSAMPLE_FRAC_TEST)\n",
        "    df_test_cache.to_pickle(PARSED_TEST_PKL)\n",
        "    print(f'[Cache] Wrote {PARSED_TEST_PKL} ({len(df_test_cache)} rows)')\n",
        "    del df_test_cache; gc.collect()\n",
        "else:\n",
        "    print('[Cache] Found existing', PARSED_TEST_PKL)\n",
        "\n",
        "print('[Milestone 2 Setup] Parsed caches ready (pickle). Next: 3-fold leak-proof baseline with TF-IDF + OVR-LR.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "71f79464-f684-4de0-9d36-226d12da8bbc",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install and configure Parquet engine to avoid pyarrow issues\n",
        "%pip install --quiet fastparquet\n",
        "import importlib, pandas as pd\n",
        "assert importlib.util.find_spec('fastparquet') is not None, 'fastparquet not installed'\n",
        "pd.set_option('io.parquet.engine', 'fastparquet')\n",
        "print('[Parquet] Using engine:', pd.get_option('io.parquet.engine'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "6d5d9683-98a7-40ac-bc30-5abfe42154eb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Milestone 2: Fast 3-fold leak-proof baseline (word + char) with OVR Logistic (SGD) and global threshold sweep\n",
        "# - Uses cached parsed_train.pkl (5% subsample) and parsed_test.pkl (full)\n",
        "# - Speed-tuned: subsample train to 120k, prune labels by freq >= 100, smaller TF-IDF caps, early-stopping SGD\n",
        "# - Iterative stratification (multilabel) for CV (scikit-multilearn)\n",
        "# - Vectorizers fit within folds only (no leakage)\n",
        "# - Outputs: OOF micro-F1, best threshold, submission.csv (baseline)\n",
        "\n",
        "%pip install --quiet scikit-multilearn\n",
        "import os, gc, time, math, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from skmultilearn.model_selection import IterativeStratification\n",
        "\n",
        "GLOBAL_SEED = 1337\n",
        "rng = np.random.RandomState(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "\n",
        "t0_all = time.time()\n",
        "TRAIN_PKL = 'parsed_train.pkl'\n",
        "TEST_PKL  = 'parsed_test.pkl'\n",
        "assert os.path.exists(TRAIN_PKL) and os.path.exists(TEST_PKL), 'Missing parsed caches. Run cache cell first.'\n",
        "\n",
        "df_tr = pd.read_pickle(TRAIN_PKL)\n",
        "df_te = pd.read_pickle(TEST_PKL)\n",
        "print('[Data] Loaded caches:', df_tr.shape, df_te.shape)\n",
        "\n",
        "# Build texts\n",
        "def build_text(df: pd.DataFrame):\n",
        "    # Baseline: concat title + body (ignore code for baseline)\n",
        "    return (df['title_norm'].fillna('') + ' ' + df['body_text'].fillna('')).astype(str)\n",
        "X_text_full = build_text(df_tr)\n",
        "X_text_test = build_text(df_te)\n",
        "\n",
        "# Labels: split Tags to list\n",
        "y_tags_full = df_tr['Tags'].astype(str).apply(lambda s: s.split())\n",
        "\n",
        "# Label pruning for speed\n",
        "MIN_FREQ = 100\n",
        "freq = pd.Series([t for lst in y_tags_full for t in lst]).value_counts()\n",
        "kept_labels = set(freq[freq >= MIN_FREQ].index.tolist())\n",
        "y_pruned_full = y_tags_full.apply(lambda lst: [t for t in lst if t in kept_labels])\n",
        "coverage = 100.0 * (y_pruned_full.apply(len) > 0).mean()\n",
        "print(f'[Labels] Kept labels >= {MIN_FREQ}: {len(kept_labels)}; samples with >=1 kept label: {coverage:.2f}%')\n",
        "\n",
        "# Filter out samples with zero kept labels to keep CV stable\n",
        "mask_keep = (y_pruned_full.apply(len) > 0).values\n",
        "X_text_f = X_text_full.loc[mask_keep].reset_index(drop=True)\n",
        "y_pruned_f = y_pruned_full.loc[mask_keep].reset_index(drop=True)\n",
        "print('[Data] After filtering:', X_text_f.shape[0], 'samples')\n",
        "\n",
        "# Subsample for speed (still leak-proof inside subset)\n",
        "SUBSAMPLE_N = 120_000\n",
        "if X_text_f.shape[0] > SUBSAMPLE_N:\n",
        "    idx_sub = rng.choice(X_text_f.shape[0], size=SUBSAMPLE_N, replace=False)\n",
        "    idx_sub.sort()\n",
        "    X_text = X_text_f.iloc[idx_sub].reset_index(drop=True)\n",
        "    y_pruned = y_pruned_f.iloc[idx_sub].reset_index(drop=True)\n",
        "    print(f'[Data] Subsampled to {SUBSAMPLE_N} for fast baseline')\n",
        "else:\n",
        "    X_text = X_text_f\n",
        "    y_pruned = y_pruned_f\n",
        "\n",
        "mlb = MultiLabelBinarizer(sparse_output=True)\n",
        "Y = mlb.fit_transform(y_pruned)\n",
        "labels_list = mlb.classes_.tolist()\n",
        "print('[Labels] Final label dimension:', Y.shape[1])\n",
        "\n",
        "# CV setup using scikit-multilearn (supports sparse Y)\n",
        "n_splits = 3\n",
        "mskf = IterativeStratification(n_splits=n_splits, order=1)\n",
        "\n",
        "# Model and vectorizers configs (baseline, fast)\n",
        "word_vec_params = dict(\n",
        "    analyzer='word', ngram_range=(1,2), min_df=3, max_df=0.95, max_features=100_000,\n",
        "    sublinear_tf=True, dtype=np.float32\n",
        ")\n",
        "char_vec_params = dict(\n",
        "    analyzer='char_wb', ngram_range=(3,5), min_df=3, max_features=150_000,\n",
        "    dtype=np.float32\n",
        ")\n",
        "\n",
        "def vectorize_fit_transform(X_train_text, X_valid_text):\n",
        "    wv = TfidfVectorizer(**word_vec_params)\n",
        "    cv = TfidfVectorizer(**char_vec_params)\n",
        "    Xw_tr = wv.fit_transform(X_train_text)\n",
        "    Xc_tr = cv.fit_transform(X_train_text)\n",
        "    Xw_va = wv.transform(X_valid_text)\n",
        "    Xc_va = cv.transform(X_valid_text)\n",
        "    X_tr = sparse.hstack([Xw_tr, Xc_tr]).tocsr().astype(np.float32)\n",
        "    X_va = sparse.hstack([Xw_va, Xc_va]).tocsr().astype(np.float32)\n",
        "    return (X_tr, X_va, wv, cv)\n",
        "\n",
        "def train_ovr_sgd(X_tr, Y_tr):\n",
        "    base = SGDClassifier(loss='log_loss', penalty='l2', alpha=2e-4, max_iter=10, tol=1e-3,\n",
        "                         n_jobs=-1, random_state=GLOBAL_SEED, early_stopping=True, n_iter_no_change=3)\n",
        "    clf = OneVsRestClassifier(base, n_jobs=-1)\n",
        "    clf.fit(X_tr, Y_tr)\n",
        "    return clf\n",
        "\n",
        "def predict_proba_ovr(clf, X):\n",
        "    # SGDClassifier decision_function -> convert via sigmoid\n",
        "    from scipy.special import expit\n",
        "    scores = clf.decision_function(X)\n",
        "    return expit(scores)\n",
        "\n",
        "# OOF loop with streaming F1 aggregation to avoid dense arrays\n",
        "ths = np.linspace(0.1, 0.5, 8)\n",
        "tp_tot = np.zeros_like(ths, dtype=np.int64)\n",
        "fp_tot = np.zeros_like(ths, dtype=np.int64)\n",
        "fn_tot = np.zeros_like(ths, dtype=np.int64)\n",
        "fold_times = []\n",
        "n_samples = X_text.shape[0]\n",
        "X_dummy = np.zeros((n_samples, 1))\n",
        "for fold, (tr_idx, va_idx) in enumerate(mskf.split(X_dummy, Y)):\n",
        "    t0 = time.time()\n",
        "    X_tr_text, X_va_text = X_text.iloc[tr_idx], X_text.iloc[va_idx]\n",
        "    Y_tr, Y_va = Y[tr_idx], Y[va_idx]\n",
        "    X_tr, X_va, wv, cv = vectorize_fit_transform(X_tr_text, X_va_text)\n",
        "    print(f'[Fold {fold}] Shapes train={X_tr.shape}, valid={X_va.shape}, labels={Y_tr.shape[1]}')\n",
        "    clf = train_ovr_sgd(X_tr, Y_tr)\n",
        "    va_probs = predict_proba_ovr(clf, X_va)\n",
        "    # Aggregate TP/FP/FN per threshold\n",
        "    for ti, thr in enumerate(ths):\n",
        "        tp = 0; fp = 0; fn = 0\n",
        "        bs = 4096\n",
        "        for s in range(0, va_probs.shape[0], bs):\n",
        "            e = min(va_probs.shape[0], s + bs)\n",
        "            batch_probs = va_probs[s:e]\n",
        "            Y_batch = Y_va[s:e]\n",
        "            for i in range(e - s):\n",
        "                true_idx = Y_batch[i].indices\n",
        "                pred_idx = np.where(batch_probs[i] >= thr)[0]\n",
        "                if true_idx.size == 0 and pred_idx.size == 0:\n",
        "                    continue\n",
        "                if pred_idx.size == 0 or true_idx.size == 0:\n",
        "                    tp_i = 0\n",
        "                else:\n",
        "                    a = true_idx; b = pred_idx; ia = ib = 0; tp_i = 0\n",
        "                    while ia < a.size and ib < b.size:\n",
        "                        if a[ia] == b[ib]:\n",
        "                            tp_i += 1; ia += 1; ib += 1\n",
        "                        elif a[ia] < b[ib]:\n",
        "                            ia += 1\n",
        "                        else:\n",
        "                            ib += 1\n",
        "                tp += tp_i\n",
        "                fp += int(pred_idx.size - tp_i)\n",
        "                fn += int(true_idx.size - tp_i)\n",
        "        tp_tot[ti] += tp; fp_tot[ti] += fp; fn_tot[ti] += fn\n",
        "    dt = time.time() - t0\n",
        "    fold_times.append(dt)\n",
        "    print(f'[Fold {fold}] Done in {dt/60:.1f} min')\n",
        "    del X_tr, X_va, X_tr_text, X_va_text, Y_tr, Y_va, wv, cv, clf, va_probs\n",
        "    gc.collect()\n",
        "\n",
        "# Compute micro-F1 per threshold\n",
        "f1s = []\n",
        "for ti in range(len(ths)):\n",
        "    tp, fp, fn = tp_tot[ti], fp_tot[ti], fn_tot[ti]\n",
        "    denom = (2*tp + fp + fn)\n",
        "    f1s.append(0.0 if denom == 0 else (2.0 * tp) / denom)\n",
        "best_idx = int(np.argmax(f1s))\n",
        "best_thr = float(ths[best_idx])\n",
        "best_f1 = float(f1s[best_idx])\n",
        "for t, f in zip(ths, f1s):\n",
        "    print(f'[OOF] thr={t:.3f} micro-F1={f:.5f}')\n",
        "print(f'[OOF] Best micro-F1={best_f1:.5f} at thr={best_thr:.3f}')\n",
        "print(f'[Timing] Avg fold time: {np.mean(fold_times)/60:.1f} min, total {np.sum(fold_times)/60:.1f} min')\n",
        "\n",
        "# Refit on all filtered (subsampled) data and predict test for a baseline submission\n",
        "print('[Final Fit] Vectorizing full filtered train and test...')\n",
        "wv_full = TfidfVectorizer(**word_vec_params)\n",
        "cv_full = TfidfVectorizer(**char_vec_params)\n",
        "Xw_full = wv_full.fit_transform(X_text)\n",
        "Xc_full = cv_full.fit_transform(X_text)\n",
        "X_full = sparse.hstack([Xw_full, Xc_full]).tocsr().astype(np.float32)\n",
        "Xw_test = wv_full.transform(X_text_test)\n",
        "Xc_test = cv_full.transform(X_text_test)\n",
        "X_test_mat = sparse.hstack([Xw_test, Xc_test]).tocsr().astype(np.float32)\n",
        "\n",
        "clf_full = train_ovr_sgd(X_full, Y)\n",
        "test_probs = predict_proba_ovr(clf_full, X_test_mat)\n",
        "pred_bin = (test_probs >= best_thr).astype(np.int32)\n",
        "\n",
        "# Safety rule: ensure at least 1 tag per sample\n",
        "rowsums = pred_bin.sum(axis=1)\n",
        "if rowsums.ndim == 2:\n",
        "    rowsums = rowsums.ravel()\n",
        "for i in range(pred_bin.shape[0]):\n",
        "    if rowsums[i] == 0:\n",
        "        j = int(np.argmax(test_probs[i]))\n",
        "        pred_bin[i, j] = 1\n",
        "\n",
        "# Build submission\n",
        "id_test = df_te['Id'].astype(int).values\n",
        "pred_tags = []\n",
        "for i in range(pred_bin.shape[0]):\n",
        "    inds = np.where(pred_bin[i] == 1)[0]\n",
        "    tags = [labels_list[j] for j in inds]\n",
        "    pred_tags.append(' '.join(tags) if len(tags) > 0 else labels_list[int(np.argmax(test_probs[i]))])\n",
        "sub = pd.DataFrame({'Id': id_test, 'Tags': pred_tags})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('[Submission] Wrote submission.csv with shape:', sub.shape)\n",
        "print('[Milestone 2] Baseline complete. OOF micro-F1 (global):', f'{best_f1:.5f}', 'thr=', f'{best_thr:.3f}')\n",
        "print('[Total time] {:.1f} min'.format((time.time()-t0_all)/60.0))\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "dceeb91a-df9c-40fa-844c-6e1d472c230b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Milestone 2 (Reset Caching): Build FULL training cache (no subsample) for leak-proof CV\n",
        "# - Writes parsed_train_full.pkl to avoid clobbering the 5% cache\n",
        "# - Reuses the approved HTML parsing: extract block-level <pre> code only\n",
        "\n",
        "import os, re, gc, time, shutil\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "GLOBAL_SEED = 1337\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "\n",
        "def backup_notebook():\n",
        "    nb_path = 'agent_notebook.ipynb'\n",
        "    if os.path.exists(nb_path):\n",
        "        ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        dst = f'agent_notebook_backup_{ts}.ipynb'\n",
        "        shutil.copy2(nb_path, dst)\n",
        "        print(f'[Backup] Notebook copied to {dst}')\n",
        "backup_notebook()\n",
        "\n",
        "# Normalization and parsing (as approved in M1)\n",
        "URL_RE   = re.compile(r'https?://\\S+|www\\.\\S+', flags=re.IGNORECASE)\n",
        "EMAIL_RE = re.compile(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.[A-Za-z]{2,}\\b')\n",
        "HEX_RE   = re.compile(r'\\b0x[0-9A-Fa-f]+\\b')\n",
        "NUM_RE   = re.compile(r'\\b\\d+\\b')\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return ''\n",
        "    s = s.lower()\n",
        "    s = URL_RE.sub(' URL ', s)\n",
        "    s = EMAIL_RE.sub(' EMAIL ', s)\n",
        "    s = HEX_RE.sub(' HEX ', s)\n",
        "    s = NUM_RE.sub(' 0 ', s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "def extract_text_and_code_pre_only(html: str):\n",
        "    if not isinstance(html, str):\n",
        "        return '', '', 0, 0, 0.0\n",
        "    soup = BeautifulSoup(html, 'lxml')\n",
        "    pre_blocks = soup.find_all('pre')\n",
        "    code_texts = []\n",
        "    for pre in pre_blocks:\n",
        "        code_texts.append(pre.get_text(' ', strip=True))\n",
        "        pre.extract()\n",
        "    text = soup.get_text(' ', strip=True)\n",
        "    text_norm = normalize_text(text)\n",
        "    code_norm = normalize_text(' \\n '.join(code_texts))\n",
        "    url_count = len(URL_RE.findall(text.lower()))\n",
        "    puncts = re.findall(r'[\\!\\?\\.,;:\\-\\(\\)\\[\\]\\{\\}\\#\\+\\*/\\\\\\|\\<\\>\\=\\_\\~\\^\\`\\\"]', text_norm)\n",
        "    punct_density = (len(puncts) / max(1, len(text_norm)))\n",
        "    return text_norm, code_norm, len(pre_blocks), url_count, punct_density\n",
        "\n",
        "def build_cache(csv_path: str, is_train: bool = True, chunksize: int = 100_000) -> pd.DataFrame:\n",
        "    t0 = time.time()\n",
        "    all_parts = []\n",
        "    usecols = ['Id', 'Title', 'Body'] + (['Tags'] if is_train else [])\n",
        "    reader = pd.read_csv(csv_path, usecols=usecols, chunksize=chunksize)\n",
        "    total_rows = 0\n",
        "    for i, chunk in enumerate(reader):\n",
        "        # Downcast Id\n",
        "        if 'Id' in chunk.columns:\n",
        "            chunk['Id'] = pd.to_numeric(chunk['Id'], downcast='integer')\n",
        "        out_records = {\n",
        "            'Id': [], 'title_norm': [], 'body_text': [], 'code_text': [],\n",
        "            'title_len': [], 'body_len': [], 'code_len': [], 'num_block_code': [], 'num_urls': [], 'punct_density': []\n",
        "        }\n",
        "        if is_train:\n",
        "            out_records['Tags'] = []\n",
        "        titles = chunk['Title'].fillna('').astype(str).tolist()\n",
        "        titles_norm = [normalize_text(t) for t in titles]\n",
        "        bodies = chunk['Body'].fillna('').astype(str).tolist()\n",
        "        for idx in range(len(chunk)):\n",
        "            body_txt, code_txt, n_code, n_url, pden = extract_text_and_code_pre_only(bodies[idx])\n",
        "            out_records['Id'].append(int(chunk.iloc[idx]['Id']))\n",
        "            out_records['title_norm'].append(titles_norm[idx])\n",
        "            out_records['body_text'].append(body_txt)\n",
        "            out_records['code_text'].append(code_txt)\n",
        "            out_records['title_len'].append(len(titles_norm[idx]))\n",
        "            out_records['body_len'].append(len(body_txt))\n",
        "            out_records['code_len'].append(len(code_txt))\n",
        "            out_records['num_block_code'].append(int(n_code))\n",
        "            out_records['num_urls'].append(int(n_url))\n",
        "            out_records['punct_density'].append(float(pden))\n",
        "            if is_train:\n",
        "                out_records['Tags'].append(chunk.iloc[idx]['Tags'])\n",
        "        out_df = pd.DataFrame(out_records)\n",
        "        all_parts.append(out_df)\n",
        "        total_rows += len(out_df)\n",
        "        if (i + 1) % 5 == 0:\n",
        "            print(f'[Cache] Processed ~{total_rows} rows so far for {os.path.basename(csv_path)}')\n",
        "        del chunk, out_df, out_records, titles, titles_norm, bodies\n",
        "        gc.collect()\n",
        "    result = pd.concat(all_parts, ignore_index=True) if len(all_parts) else pd.DataFrame()\n",
        "    print(f'[Cache] Built DataFrame with {len(result)} rows in {time.time()-t0:.1f}s from {os.path.basename(csv_path)}')\n",
        "    return result\n",
        "\n",
        "TRAIN_CSV = 'train.csv'\n",
        "PARSED_TRAIN_FULL_PKL = 'parsed_train_full.pkl'\n",
        "\n",
        "if not os.path.exists(PARSED_TRAIN_FULL_PKL):\n",
        "    print('[Cache] Building FULL train cache (no subsample)...')\n",
        "    df_train_full = build_cache(TRAIN_CSV, is_train=True, chunksize=75_000)\n",
        "    df_train_full.to_pickle(PARSED_TRAIN_FULL_PKL)\n",
        "    print(f'[Cache] Wrote {PARSED_TRAIN_FULL_PKL} with {len(df_train_full)} rows')\n",
        "    del df_train_full; gc.collect()\n",
        "else:\n",
        "    print('[Cache] Found existing', PARSED_TRAIN_FULL_PKL)\n",
        "\n",
        "print('[M2 Cache Reset] Full training cache ready. Next: implement leak-proof per-fold label pruning + multi-channel features.')\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "cfb94a96-3497-493f-956f-b342bbf9b652",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Process Remediation: Create a pristine, single-cell notebook per audit mandate\n",
        "# - Archives current notebook as a devlog\n",
        "# - Writes a new notebook (agent_notebook_pristine.ipynb) with ONE idempotent setup cell\n",
        "#   that imports dependencies, defines helpers, suppresses matplotlib_inline noise,\n",
        "#   and builds/loads full caches into memory (df_train_full, df_test_cache).\n",
        "# - Does NOT run the setup; it's authored for a clean Run-All experience.\n",
        "\n",
        "import os, shutil, time, re, gc\n",
        "from datetime import datetime\n",
        "import nbformat as nbf\n",
        "\n",
        "ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "src_nb = 'agent_notebook.ipynb'\n",
        "devlog_nb = f'agent_notebook_devlog_{ts}.ipynb'\n",
        "pristine_nb = 'agent_notebook_pristine.ipynb'\n",
        "\n",
        "# 1) Archive current notebook\n",
        "if os.path.exists(src_nb):\n",
        "    shutil.copy2(src_nb, devlog_nb)\n",
        "    print(f'[Archive] Current notebook archived to {devlog_nb}')\n",
        "else:\n",
        "    print('[Archive] Source notebook not found; skipping archive.')\n",
        "\n",
        "# 2) Compose the single, consolidated setup cell (idempotent)\n",
        "setup_source = r'''\\\n",
        "# Facebook Recruiting III - Keyword Extraction (Pristine Setup)\n",
        "# Single-cell, idempotent setup: imports, constants, helpers, env hygiene, and full-data caching.\n",
        "\n",
        "import os, re, gc, time, shutil\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- Environment hygiene: suppress matplotlib_inline post-run callback noise ---\n",
        "from IPython import get_ipython\n",
        "print('[Env] Activating matplotlib_inline suppression...')\n",
        "try:\n",
        "    import matplotlib as _mpl\n",
        "    if not hasattr(_mpl, 'backend_bases'):\n",
        "        _mpl.backend_bases = type('backend_bases', (), {'_Backend': type('_Backend', (), {})})\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        try:\n",
        "            plt.switch_backend('Agg')\n",
        "        except Exception:\n",
        "            pass\n",
        "    except Exception:\n",
        "        pass\n",
        "    ip = get_ipython()\n",
        "    if ip is not None and hasattr(ip, 'events'):\n",
        "        cbs = ip.events.callbacks.get('post_run_cell', [])\n",
        "        new_cbs = []\n",
        "        for cb in cbs:\n",
        "            name = getattr(cb, '__name__', '')\n",
        "            mod = getattr(cb, '__module__', '')\n",
        "            qual = getattr(cb, '__qualname__', '')\n",
        "            if ('matplotlib_inline' in mod) or ('backend_inline' in mod) or (name == 'configure_once') or ('configure_once' in qual):\n",
        "                continue\n",
        "            new_cbs.append(cb)\n",
        "        ip.events.callbacks['post_run_cell'] = new_cbs\n",
        "    print('[Env] matplotlib_inline suppression active.')\n",
        "except Exception as e:\n",
        "    print('[Env] matplotlib_inline suppression failed:', repr(e))\n",
        "\n",
        "GLOBAL_SEED = 1337\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "\n",
        "def backup_notebook():\n",
        "    nb_path = 'agent_notebook_pristine.ipynb'\n",
        "    if os.path.exists(nb_path):\n",
        "        ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        dst = f'agent_notebook_pristine_backup_{ts}.ipynb'\n",
        "        shutil.copy2(nb_path, dst)\n",
        "        print(f'[Backup] Pristine notebook copied to {dst}')\n",
        "backup_notebook()\n",
        "\n",
        "# Normalization and parsing (approved) - RAW regex strings to preserve \\b, \\d etc.\n",
        "URL_RE   = re.compile(r'https?://\\S+|www\\.\\S+', flags=re.IGNORECASE)\n",
        "EMAIL_RE = re.compile(r'(?i)\\b[a-z0-9._%+\\-]+@[a-z0-9.\\-]+\\.[a-z]{2,}\\b')\n",
        "HEX_RE   = re.compile(r'\\b0x[0-9a-f]+\\b', flags=re.IGNORECASE)\n",
        "NUM_RE   = re.compile(r'\\b\\d+\\b')\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return ''\n",
        "    s = s.lower()\n",
        "    s = URL_RE.sub(' URL ', s)\n",
        "    s = EMAIL_RE.sub(' EMAIL ', s)\n",
        "    s = HEX_RE.sub(' HEX ', s)\n",
        "    s = NUM_RE.sub(' 0 ', s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "def extract_text_and_code_pre_only(html: str):\n",
        "    if not isinstance(html, str):\n",
        "        return '', '', 0, 0, 0.0\n",
        "    soup = BeautifulSoup(html, 'lxml')\n",
        "    pre_blocks = soup.find_all('pre')\n",
        "    code_texts = []\n",
        "    for pre in pre_blocks:\n",
        "        code_texts.append(pre.get_text(' ', strip=True))\n",
        "        pre.extract()\n",
        "    text = soup.get_text(' ', strip=True)\n",
        "    text_norm = normalize_text(text)\n",
        "    # Use a simple space to join code blocks to avoid escape issues in code generation\n",
        "    code_norm = normalize_text(' '.join(code_texts))\n",
        "    url_count = len(URL_RE.findall(text.lower()))\n",
        "    puncts = re.findall(r'[\\!\\?\\.,;:\\-\\(\\)\\[\\]\\{\\}\\#\\+\\*/\\\\\\|\\<\\>\\=\\_\\~\\^\\`\\\"]', text_norm)\n",
        "    punct_density = (len(puncts) / max(1, len(text_norm)))\n",
        "    return text_norm, code_norm, len(pre_blocks), url_count, punct_density\n",
        "\n",
        "def build_cache(csv_path: str, is_train: bool = True, chunksize: int = 100_000) -> pd.DataFrame:\n",
        "    t0 = time.time()\n",
        "    all_parts = []\n",
        "    usecols = ['Id', 'Title', 'Body'] + (['Tags'] if is_train else [])\n",
        "    reader = pd.read_csv(csv_path, usecols=usecols, chunksize=chunksize)\n",
        "    total_rows = 0\n",
        "    for i, chunk in enumerate(reader):\n",
        "        if 'Id' in chunk.columns:\n",
        "            chunk['Id'] = pd.to_numeric(chunk['Id'], downcast='integer')\n",
        "        out_records = {\n",
        "            'Id': [], 'title_norm': [], 'body_text': [], 'code_text': [],\n",
        "            'title_len': [], 'body_len': [], 'code_len': [], 'num_block_code': [], 'num_urls': [], 'punct_density': []\n",
        "        }\n",
        "        if is_train:\n",
        "            out_records['Tags'] = []\n",
        "        titles = chunk['Title'].fillna('').astype(str).tolist()\n",
        "        titles_norm = [normalize_text(t) for t in titles]\n",
        "        bodies = chunk['Body'].fillna('').astype(str).tolist()\n",
        "        for idx in range(len(chunk)):\n",
        "            body_txt, code_txt, n_code, n_url, pden = extract_text_and_code_pre_only(bodies[idx])\n",
        "            out_records['Id'].append(int(chunk.iloc[idx]['Id']))\n",
        "            out_records['title_norm'].append(titles_norm[idx])\n",
        "            out_records['body_text'].append(body_txt)\n",
        "            out_records['code_text'].append(code_txt)\n",
        "            out_records['title_len'].append(len(titles_norm[idx]))\n",
        "            out_records['body_len'].append(len(body_txt))\n",
        "            out_records['code_len'].append(len(code_txt))\n",
        "            out_records['num_block_code'].append(int(n_code))\n",
        "            out_records['num_urls'].append(int(n_url))\n",
        "            out_records['punct_density'].append(float(pden))\n",
        "            if is_train:\n",
        "                out_records['Tags'].append(chunk.iloc[idx]['Tags'])\n",
        "        out_df = pd.DataFrame(out_records)\n",
        "        all_parts.append(out_df)\n",
        "        total_rows += len(out_df)\n",
        "        if (i + 1) % 5 == 0:\n",
        "            print(f'[Cache] Processed ~{total_rows} rows so far for {os.path.basename(csv_path)}')\n",
        "        del chunk, out_df, out_records, titles, titles_norm, bodies\n",
        "        gc.collect()\n",
        "    result = pd.concat(all_parts, ignore_index=True) if len(all_parts) else pd.DataFrame()\n",
        "    print(f'[Cache] Built DataFrame with {len(result)} rows in {time.time()-t0:.1f}s from {os.path.basename(csv_path)}')\n",
        "    return result\n",
        "\n",
        "TRAIN_CSV = 'train.csv'\n",
        "TEST_CSV  = 'test.csv'\n",
        "PARSED_TRAIN_FULL_PKL = 'parsed_train_full.pkl'\n",
        "PARSED_TEST_PKL  = 'parsed_test.pkl'\n",
        "\n",
        "print('[Setup] Starting pristine setup...')\n",
        "built_any = False\n",
        "\n",
        "# Load-or-build TRAIN cache, and ensure df_train_full in memory\n",
        "if os.path.exists(PARSED_TRAIN_FULL_PKL):\n",
        "    df_train_full = pd.read_pickle(PARSED_TRAIN_FULL_PKL)\n",
        "    print(f\"[Cache] Loaded {PARSED_TRAIN_FULL_PKL} with shape {df_train_full.shape}\")\n",
        "else:\n",
        "    print('[Cache] Building FULL train cache (no subsample)...')\n",
        "    df_train_full = build_cache(TRAIN_CSV, is_train=True, chunksize=75_000)\n",
        "    df_train_full.to_pickle(PARSED_TRAIN_FULL_PKL)\n",
        "    print(f'[Cache] Wrote {PARSED_TRAIN_FULL_PKL} with {len(df_train_full)} rows')\n",
        "    built_any = True\n",
        "\n",
        "# Load-or-build TEST cache, and ensure df_test_cache in memory\n",
        "if os.path.exists(PARSED_TEST_PKL):\n",
        "    df_test_cache = pd.read_pickle(PARSED_TEST_PKL)\n",
        "    print(f\"[Cache] Loaded {PARSED_TEST_PKL} with shape {df_test_cache.shape}\")\n",
        "else:\n",
        "    print('[Cache] Building test cache (pickle)...')\n",
        "    df_test_cache = build_cache(TEST_CSV, is_train=False, chunksize=75_000)\n",
        "    df_test_cache.to_pickle(PARSED_TEST_PKL)\n",
        "    print(f'[Cache] Wrote {PARSED_TEST_PKL} with {len(df_test_cache)} rows')\n",
        "    built_any = True\n",
        "\n",
        "print('[Setup] Completed. Built any:', built_any)\n",
        "print('[Setup] df_train_full shape:', df_train_full.shape if 'df_train_full' in globals() else None)\n",
        "print('[Setup] df_test_cache shape:', df_test_cache.shape if 'df_test_cache' in globals() else None)\n",
        "'''\n",
        "\n",
        "# 3) Create the pristine notebook object with a single code cell\n",
        "nb = nbf.v4.new_notebook()\n",
        "nb.cells = [nbf.v4.new_code_cell(setup_source)]\n",
        "nb.metadata[\"kernelspec\"] = {\n",
        "    \"display_name\": \"Python 3\",\n",
        "    \"language\": \"python\",\n",
        "    \"name\": \"python3\"\n",
        "}\n",
        "nb.metadata[\"language_info\"] = {\n",
        "    \"name\": \"python\",\n",
        "    \"version\": \"3.11\"\n",
        "}\n",
        "\n",
        "# 4) Write pristine notebook to disk\n",
        "with open(pristine_nb, 'w', encoding='utf-8') as f:\n",
        "    nbf.write(nb, f)\n",
        "print(f'[Pristine] Wrote {pristine_nb} with a single idempotent setup cell.')\n",
        "print('[Instruction] Open agent_notebook_pristine.ipynb and Run-All. It will load/build caches, suppress matplotlib_inline noise, and expose df_train_full/df_test_cache in memory.')\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "7a6e25f2-4b0f-4c52-ab24-72ddce214cd9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install nbformat to programmatically create a pristine notebook per audit requirements\n",
        "%pip install --quiet nbformat\n",
        "import importlib\n",
        "assert importlib.util.find_spec('nbformat') is not None, 'nbformat failed to install'\n",
        "print('nbformat installed and importable.')\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "42ea4caf-b27c-4b7a-af4d-93a5d73ae7d6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Verification step (mandatory): validate pristine setup regex normalization and parsing\n",
        "# - Reads agent_notebook_pristine.ipynb\n",
        "# - Executes its single setup cell in an isolated namespace\n",
        "# - Asserts normalize_text and extract_text_and_code_pre_only behave correctly\n",
        "\n",
        "import os\n",
        "import nbformat as nbf\n",
        "\n",
        "pristine_nb = 'agent_notebook_pristine.ipynb'\n",
        "assert os.path.exists(pristine_nb), '[Verify] Pristine notebook not found.'\n",
        "\n",
        "# Load pristine notebook and extract the single code cell\n",
        "nb = nbf.read(pristine_nb, as_version=4)\n",
        "cells = [c for c in nb.cells if c.cell_type == 'code']\n",
        "assert len(cells) == 1, f'[Verify] Expected 1 code cell in pristine notebook, found {len(cells)}'\n",
        "setup_code = cells[0].source\n",
        "print('[Verify] Loaded setup cell code length:', len(setup_code))\n",
        "\n",
        "# Execute the setup cell in an isolated global namespace\n",
        "g = {}\n",
        "exec(setup_code, g, g)\n",
        "\n",
        "# Pull required symbols\n",
        "normalize_text = g.get('normalize_text')\n",
        "extract_text_and_code_pre_only = g.get('extract_text_and_code_pre_only')\n",
        "URL_RE = g.get('URL_RE'); EMAIL_RE = g.get('EMAIL_RE'); HEX_RE = g.get('HEX_RE'); NUM_RE = g.get('NUM_RE')\n",
        "assert callable(normalize_text), '[Verify] normalize_text is not defined/callable.'\n",
        "assert callable(extract_text_and_code_pre_only), '[Verify] extract_text_and_code_pre_only is not defined/callable.'\n",
        "assert URL_RE is not None and EMAIL_RE is not None and HEX_RE is not None and NUM_RE is not None, '[Verify] One or more regex patterns missing.'\n",
        "print('[Verify] Patterns:', 'URL=', URL_RE.pattern, 'EMAIL=', EMAIL_RE.pattern, 'HEX=', HEX_RE.pattern, 'NUM=', NUM_RE.pattern)\n",
        "\n",
        "# Tests for normalize_text\n",
        "s_in = 'Email: Foo.Bar-123@example.co.uk visit: https://example.com/path?q=1&a=2 hex=0xDEADbeef and numbers 42, 007.'\n",
        "s_out = normalize_text(s_in)\n",
        "print('[Verify] normalize_text output:', s_out[:160], '...')\n",
        "# Validate replacements\n",
        "assert ' URL ' in s_out, '[Verify] URL replacement failed'\n",
        "assert ' EMAIL ' in s_out, '[Verify] EMAIL replacement failed'\n",
        "assert '@' not in s_out, '[Verify] EMAIL address not removed'\n",
        "assert ' HEX ' in s_out, '[Verify] HEX replacement failed'\n",
        "assert '0x' not in s_out, '[Verify] HEX literal not removed'\n",
        "assert ' 0 ' in s_out, '[Verify] number normalization failed'\n",
        "\n",
        "\n",
        "# Tests for extract_text_and_code_pre_only\n",
        "html = '''<p>Body has URL https://x.y and inline <code>print(123)</code>.</p>\\n<pre><code>int main(){return 0;}</code></pre>\\n<p>mail me: a@b.cc</p>'''\n",
        "body_txt, code_txt, n_code, n_url, pden = extract_text_and_code_pre_only(html)\n",
        "print('[Verify] body_txt:', body_txt)\n",
        "print('[Verify] code_txt:', code_txt)\n",
        "print('[Verify] n_code:', n_code, 'n_url:', n_url, 'punct_density:', pden)\n",
        "# Inline <code> should remain in body text (numbers normalized to 0). Check for 'print' token presence.\n",
        "assert 'print' in body_txt, '[Verify] Inline <code> content missing from body text.'\n",
        "assert 'int main' in code_txt, '[Verify] <pre> code should be extracted to code_txt.'\n",
        "assert n_code == 1, '[Verify] num_block_code should be 1.'\n",
        "assert n_url >= 1, '[Verify] URL count should reflect URL presence.'\n",
        "\n",
        "print('[Verify] PASSED: Pristine setup normalization and parsing behaviors are correct.')\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "492938ac-488c-4bdf-99a8-7f631f10b80c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Append REFACTORED Phase 2 modeling cell to agent_notebook_pristine.ipynb (v3: streaming/online learning)\n",
        "import nbformat as nbf, os\n",
        "\n",
        "pristine_nb = 'agent_notebook_pristine.ipynb'\n",
        "assert os.path.exists(pristine_nb), 'Pristine notebook not found.'\n",
        "nb = nbf.read(pristine_nb, as_version=4)\n",
        "\n",
        "# Keep only the first setup cell; drop any previously appended modeling cells for cleanliness\n",
        "code_cells = [i for i,c in enumerate(nb.cells) if c.cell_type=='code']\n",
        "if len(code_cells) > 1:\n",
        "    nb.cells = [nb.cells[code_cells[0]]]\n",
        "\n",
        "modeling_source = r'''\\\n",
        "# Phase 2 (Refactored v3): Scalable 5-fold CV with streaming features + online OVR via SGD (partial_fit)\n",
        "# - Avoids materializing full X_tr/X_va: transforms batches on the fly per channel and hstack per-batch only\n",
        "# - Per-fold label pruning + dynamic label sharding (4GB coef_ budget)\n",
        "# - Streaming threshold optimization with correct FN accounting for excluded labels\n",
        "# - Hygiene: no nested parallelism, sparse-safe ops, per-fold vectorizer fit, artifact persistence, safety rule\n",
        "# - Micro-pilot mode (conditional): deterministic subsample and run only the first fold to validate pipeline\n",
        "\n",
        "import os, gc, time, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# Dependency guard: iterative-stratification\n",
        "try:\n",
        "    from skmultilearn.model_selection import IterativeStratification\n",
        "except Exception:\n",
        "    import sys\n",
        "    from subprocess import run\n",
        "    run([sys.executable, '-m', 'pip', 'install', '--quiet', 'scikit-multilearn'])\n",
        "    from skmultilearn.model_selection import IterativeStratification\n",
        "\n",
        "GLOBAL_SEED = 1337\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "\n",
        "# Expect df_train_full and df_test_cache in memory from setup cell\n",
        "assert 'df_train_full' in globals() and 'df_test_cache' in globals(), 'Run setup cell first to load caches.'\n",
        "\n",
        "# Prepare texts and labels\n",
        "def to_list_tags(s):\n",
        "    return s.split() if isinstance(s, str) else []\n",
        "y_lists = df_train_full['Tags'].astype(str).apply(to_list_tags)\n",
        "mlb = MultiLabelBinarizer(sparse_output=True)\n",
        "Y_all = mlb.fit_transform(y_lists)\n",
        "labels_list = mlb.classes_.tolist()\n",
        "n_samples, n_labels = Y_all.shape\n",
        "print('[Labels] #samples:', n_samples, '#labels:', n_labels)\n",
        "\n",
        "# Channels\n",
        "title_text = df_train_full['title_norm'].fillna('').astype(str)\n",
        "body_text  = df_train_full['body_text'].fillna('').astype(str)\n",
        "code_text  = df_train_full['code_text'].fillna('').astype(str)\n",
        "meta_cols = ['title_len','body_len','code_len','num_block_code','num_urls','punct_density']\n",
        "meta_all = df_train_full[meta_cols].astype(np.float32).values\n",
        "\n",
        "# Micro-pilot configuration (auditor-approved): subsample deterministically and run only first fold\n",
        "MICRO_PILOT = True\n",
        "PILOT_N = 50_000  # target rows for micro-pilot (reduced for faster turnaround)\n",
        "if MICRO_PILOT:\n",
        "    n_keep = int(min(PILOT_N, n_samples))\n",
        "    rng = np.random.RandomState(GLOBAL_SEED)\n",
        "    idx_keep = rng.choice(n_samples, size=n_keep, replace=False)\n",
        "    idx_keep.sort()\n",
        "    # Subset all channels and labels consistently\n",
        "    title_text = title_text.iloc[idx_keep].reset_index(drop=True)\n",
        "    body_text  = body_text.iloc[idx_keep].reset_index(drop=True)\n",
        "    code_text  = code_text.iloc[idx_keep].reset_index(drop=True)\n",
        "    meta_all   = meta_all[idx_keep]\n",
        "    Y_all      = Y_all[idx_keep]\n",
        "    n_samples  = Y_all.shape[0]\n",
        "    print(f'[Pilot] Subsampled to {n_samples} rows for micro-pilot.')\n",
        "\n",
        "# CV setup\n",
        "n_splits = 5\n",
        "mskf = IterativeStratification(n_splits=n_splits, order=1)\n",
        "\n",
        "# Vectorizer configs (fit within fold on train split)\n",
        "title_vec_cfg = dict(analyzer='word', ngram_range=(1,3), min_df=3, max_df=0.95,\n",
        "                     max_features=200_000, sublinear_tf=True, dtype=np.float32)\n",
        "# Use streaming hashing with built-in l2 normalization (no IDF) to allow partial/batch processing\n",
        "body_hash_cfg  = dict(analyzer='word', ngram_range=(1,3), n_features=2**19,\n",
        "                      alternate_sign=False, norm='l2', dtype=np.float32)\n",
        "char_hash_cfg  = dict(analyzer='char_wb', ngram_range=(3,6), n_features=2**18,\n",
        "                      alternate_sign=False, norm='l2', dtype=np.float32)\n",
        "code_vec_cfg   = dict(analyzer='word', ngram_range=(1,3), min_df=2, max_df=0.999,\n",
        "                      token_pattern=r'(?u)\\b\\w[\\w_\\+\\-\\#\\.]*\\b', max_features=100_000,\n",
        "                      sublinear_tf=True, dtype=np.float32)\n",
        "title_alpha = 3.0\n",
        "MIN_LABEL_FREQ_TRAIN = 50  # higher for micro-pilot to reduce labels per shard and speed up\n",
        "THS = np.linspace(0.05, 0.6, 12)\n",
        "COEF_BUDGET_BYTES = 4_000_000_000  # 4GB\n",
        "BATCH_SIZE = 8192  # larger batch for better throughput in micro-pilot\n",
        "\n",
        "# Global label support for per-tag thresholds\n",
        "global_support = np.asarray(Y_all.sum(axis=0)).ravel()\n",
        "hi_support_mask = (global_support >= 50)\n",
        "hi_label_idx = np.where(hi_support_mask)[0]\n",
        "print('[Labels] High-support labels (>=50):', hi_label_idx.size)\n",
        "\n",
        "# Streaming accumulators for global micro-F1\n",
        "tp_tot = np.zeros_like(THS, dtype=np.int64)\n",
        "fp_tot = np.zeros_like(THS, dtype=np.int64)\n",
        "fn_tot = np.zeros_like(THS, dtype=np.int64)\n",
        "\n",
        "# Per-tag (high-support only) streaming counts per threshold\n",
        "tp_hi = np.zeros((hi_label_idx.size, THS.size), dtype=np.int64)\n",
        "fp_hi = np.zeros((hi_label_idx.size, THS.size), dtype=np.int64)\n",
        "fn_hi = np.zeros((hi_label_idx.size, THS.size), dtype=np.int64)\n",
        "hi_pos = {lab: pos for pos, lab in enumerate(hi_label_idx)}\n",
        "\n",
        "def update_counts_batch(Y_true_batch_csr, probs_batch, label_idx_batch, ths, tp, fp, fn,\n",
        "                        tp_hi, fp_hi, fn_hi, hi_pos_map):\n",
        "    bs, Lb = probs_batch.shape\n",
        "    for ti, thr in enumerate(ths):\n",
        "        pred_bin = (probs_batch >= thr).astype(np.uint8)\n",
        "        pred_csr = sparse.csr_matrix(pred_bin, dtype=np.uint8)\n",
        "        tp_mat = pred_csr.multiply(Y_true_batch_csr)\n",
        "        tp_count = int(tp_mat.sum())\n",
        "        pred_pos = int(pred_bin.sum())\n",
        "        true_pos = int(Y_true_batch_csr.sum())\n",
        "        tp[ti] += tp_count\n",
        "        fp[ti] += (pred_pos - tp_count)\n",
        "        fn[ti] += (true_pos - tp_count)\n",
        "        if hi_pos_map:\n",
        "            for j_local in range(Lb):\n",
        "                g_lab = int(label_idx_batch[j_local])\n",
        "                pos = hi_pos_map.get(g_lab, None)\n",
        "                if pos is None:\n",
        "                    continue\n",
        "                col_true = Y_true_batch_csr[:, j_local]\n",
        "                col_pred = pred_csr[:, j_local]\n",
        "                tp_j = int(col_true.multiply(col_pred).sum())\n",
        "                p_j = int(col_pred.sum())\n",
        "                t_j = int(col_true.sum())\n",
        "                tp_hi[pos, ti] += tp_j\n",
        "                fp_hi[pos, ti] += (p_j - tp_j)\n",
        "                fn_hi[pos, ti] += (t_j - tp_j)\n",
        "\n",
        "def micro_f1(tp, fp, fn):\n",
        "    denom = (2*tp + fp + fn)\n",
        "    return 0.0 if denom == 0 else (2.0*tp)/denom\n",
        "\n",
        "# Helper: yield batches of indices\n",
        "def batch_indices(idxs, batch_size):\n",
        "    for s in range(0, idxs.size, batch_size):\n",
        "        yield idxs[s:min(idxs.size, s+batch_size)]\n",
        "\n",
        "fold_times = []\n",
        "X_dummy = np.zeros((n_samples, 1))\n",
        "fold_id = 0\n",
        "t_all = time.time()\n",
        "for tr_idx, va_idx in mskf.split(X_dummy, Y_all):\n",
        "    t0 = time.time()\n",
        "    Y_tr_full = Y_all[tr_idx]\n",
        "    Y_va_full = Y_all[va_idx]\n",
        "\n",
        "    # Per-fold label pruning (train split only)\n",
        "    sup_tr = np.asarray(Y_tr_full.sum(axis=0)).ravel()\n",
        "    kept_labels = np.where(sup_tr >= MIN_LABEL_FREQ_TRAIN)[0]\n",
        "    excluded_labels = np.setdiff1d(np.arange(n_labels), kept_labels)\n",
        "    print(f'[Fold {fold_id}] Train={len(tr_idx)}, Valid={len(va_idx)}, kept_labels={kept_labels.size}, excluded={excluded_labels.size}')\n",
        "    if kept_labels.size == 0:\n",
        "        print(f'[Fold {fold_id}] No labels meet freq >= {MIN_LABEL_FREQ_TRAIN}; skipping fold.')\n",
        "        continue\n",
        "\n",
        "    # Fit vectorizers/scaler on training split only\n",
        "    title_vec = TfidfVectorizer(**title_vec_cfg)\n",
        "    code_vec  = TfidfVectorizer(**code_vec_cfg)\n",
        "    body_hash = HashingVectorizer(**body_hash_cfg)\n",
        "    char_hash = HashingVectorizer(**char_hash_cfg)\n",
        "    meta_scaler = StandardScaler(with_mean=False)\n",
        "\n",
        "    # Fit title/code on full train split (fit only, no transform to avoid materializing large matrices)\n",
        "    title_vec.fit(title_text.iloc[tr_idx])\n",
        "    code_vec.fit(code_text.iloc[tr_idx])\n",
        "\n",
        "    # Fit meta scaler in batches\n",
        "    for b_idx in batch_indices(tr_idx, BATCH_SIZE):\n",
        "        meta_scaler.partial_fit(meta_all[b_idx])\n",
        "\n",
        "    # Compute feature dimension D approximately by transforming one small batch\n",
        "    probe_idx = tr_idx[:min(BATCH_SIZE, tr_idx.size)]\n",
        "    Xt_probe = title_vec.transform(title_text.iloc[probe_idx]).astype(np.float32)\n",
        "    Xt_probe = Xt_probe.multiply(title_alpha)\n",
        "    Xb_probe = body_hash.transform(body_text.iloc[probe_idx]).astype(np.float32)\n",
        "    Xc_probe = char_hash.transform((title_text.iloc[probe_idx] + ' ' + body_text.iloc[probe_idx])).astype(np.float32)\n",
        "    Xcode_probe = code_vec.transform(code_text.iloc[probe_idx]).astype(np.float32)\n",
        "    Xmeta_probe = sparse.csr_matrix(meta_scaler.transform(meta_all[probe_idx]), dtype=np.float32)\n",
        "    D = sparse.hstack([Xt_probe, Xb_probe, Xc_probe, Xcode_probe, Xmeta_probe], format='csr', dtype=np.float32).shape[1]\n",
        "    del Xt_probe, Xb_probe, Xc_probe, Xcode_probe, Xmeta_probe; gc.collect()\n",
        "    # SGDClassifier stores coef_ as float64 by default -> 8 bytes per weight\n",
        "    shard_cap_by_budget = max(1, int(COEF_BUDGET_BYTES // (8 * D)))\n",
        "    dyn_shard_size = max(1, min(2000, shard_cap_by_budget))\n",
        "    print(f'[Fold {fold_id}] Approx feature dim D={D:,}. Dynamic SHARD_SIZE={dyn_shard_size} (budget {COEF_BUDGET_BYTES/1e9:.1f}GB)')\n",
        "\n",
        "    # Shard labels\n",
        "    shards = [kept_labels[i:i+dyn_shard_size] for i in range(0, kept_labels.size, dyn_shard_size)]\n",
        "    print(f'[Fold {fold_id}] #shards: {len(shards)} (size {dyn_shard_size})')\n",
        "\n",
        "    # Train + validate per shard using online binary classifiers\n",
        "    for si, shard in enumerate(shards):\n",
        "        Lb = len(shard)\n",
        "        if Lb == 0:\n",
        "            continue\n",
        "        print(f'[Fold {fold_id}] Shard {si+1}/{len(shards)} with {Lb} labels')\n",
        "\n",
        "        # Create per-label SGD models\n",
        "        models = []\n",
        "        for _ in range(Lb):\n",
        "            models.append(SGDClassifier(loss='log_loss', penalty='l2', alpha=2e-4,\n",
        "                                       max_iter=1, tol=None, random_state=GLOBAL_SEED))  # single-epoch per partial_fit call\n",
        "\n",
        "        # Training: stream over training indices in batches\n",
        "        for b_idx in batch_indices(tr_idx, BATCH_SIZE):\n",
        "            # Build batch features on the fly\n",
        "            X_title = title_vec.transform(title_text.iloc[b_idx]).astype(np.float32).multiply(title_alpha)\n",
        "            X_body  = body_hash.transform(body_text.iloc[b_idx]).astype(np.float32)\n",
        "            X_char  = char_hash.transform((title_text.iloc[b_idx] + ' ' + body_text.iloc[b_idx])).astype(np.float32)\n",
        "            X_code  = code_vec.transform(code_text.iloc[b_idx]).astype(np.float32)\n",
        "            X_meta  = sparse.csr_matrix(meta_scaler.transform(meta_all[b_idx]), dtype=np.float32)\n",
        "            X_batch = sparse.hstack([X_title, X_body, X_char, X_code, X_meta], format='csr', dtype=np.float32)\n",
        "            Y_b = Y_tr_full[b_idx][:, shard].toarray().astype(np.int8, copy=False)\n",
        "            # partial_fit for each label binary model\n",
        "            for j in range(Lb):\n",
        "                yj = Y_b[:, j]\n",
        "                models[j].partial_fit(X_batch, yj, classes=np.array([0,1], dtype=np.int32))\n",
        "            del X_title, X_body, X_char, X_code, X_meta, X_batch, Y_b; gc.collect()\n",
        "\n",
        "        # Validation: stream over validation batches, predict probs and update counts\n",
        "        for b_idx in batch_indices(va_idx, BATCH_SIZE):\n",
        "            X_title = title_vec.transform(title_text.iloc[b_idx]).astype(np.float32).multiply(title_alpha)\n",
        "            X_body  = body_hash.transform(body_text.iloc[b_idx]).astype(np.float32)\n",
        "            X_char  = char_hash.transform((title_text.iloc[b_idx] + ' ' + body_text.iloc[b_idx])).astype(np.float32)\n",
        "            X_code  = code_vec.transform(code_text.iloc[b_idx]).astype(np.float32)\n",
        "            X_meta  = sparse.csr_matrix(meta_scaler.transform(meta_all[b_idx]), dtype=np.float32)\n",
        "            X_batch = sparse.hstack([X_title, X_body, X_char, X_code, X_meta], format='csr', dtype=np.float32)\n",
        "            # Collect probs per label model\n",
        "            P = np.zeros((X_batch.shape[0], Lb), dtype=np.float32)\n",
        "            for j in range(Lb):\n",
        "                try:\n",
        "                    prob = models[j].predict_proba(X_batch)[:, 1]\n",
        "                except Exception:\n",
        "                    from scipy.special import expit\n",
        "                    scores = models[j].decision_function(X_batch)\n",
        "                    prob = expit(scores)\n",
        "                P[:, j] = prob.astype(np.float32, copy=False)\n",
        "            Y_true_batch = Y_va_full[b_idx][:, shard]\n",
        "            update_counts_batch(Y_true_batch.tocsr(), P, shard, THS, tp_tot, fp_tot, fn_tot,\n",
        "                                tp_hi, fp_hi, fn_hi, hi_pos)\n",
        "            del X_title, X_body, X_char, X_code, X_meta, X_batch, Y_true_batch, P; gc.collect()\n",
        "\n",
        "        # Free models\n",
        "        del models\n",
        "        gc.collect()\n",
        "\n",
        "    # Add FN from excluded labels to avoid optimistic bias\n",
        "    fn_excluded = int(Y_va_full[:, excluded_labels].sum()) if excluded_labels.size > 0 else 0\n",
        "    if fn_excluded > 0:\n",
        "        for ti in range(THS.size):\n",
        "            fn_tot[ti] += fn_excluded\n",
        "    print(f'[Fold {fold_id}] Added FN from excluded labels: {fn_excluded}')\n",
        "\n",
        "    del Y_tr_full, Y_va_full\n",
        "    gc.collect()\n",
        "    dt = time.time() - t0\n",
        "    fold_times.append(dt)\n",
        "    print(f'[Fold {fold_id}] Completed in {dt/60:.1f} min')\n",
        "    fold_id += 1\n",
        "\n",
        "    # Micro-pilot: run only the first fold\n",
        "    if MICRO_PILOT:\n",
        "        print('[Pilot] Completed first fold only (micro-pilot mode).')\n",
        "        break\n",
        "\n",
        "print('[CV] Completed. Optimizing thresholds...')\n",
        "f1s = [micro_f1(tp_tot[i], fp_tot[i], fn_tot[i]) for i in range(THS.size)]\n",
        "best_idx = int(np.argmax(f1s))\n",
        "best_thr = float(THS[best_idx])\n",
        "best_f1 = float(f1s[best_idx])\n",
        "print('[OOF] Global best micro-F1 = {:.5f} at thr = {:.3f}'.format(best_f1, best_thr))\n",
        "\n",
        "# Per-tag thresholds for high-support labels only\n",
        "per_tag_thr = np.full(n_labels, best_thr, dtype=np.float32)\n",
        "for k, lab in enumerate(hi_label_idx):\n",
        "    tps = tp_hi[k]; fps = fp_hi[k]; fns = fn_hi[k]\n",
        "    f1s_lab = np.array([micro_f1(tps[i], fps[i], fns[i]) for i in range(THS.size)], dtype=np.float32)\n",
        "    j = int(np.argmax(f1s_lab))\n",
        "    per_tag_thr[lab] = float(THS[j])\n",
        "print('[OOF] Per-tag thresholds computed for', hi_label_idx.size, 'labels; others use global.')\n",
        "\n",
        "# Persist artifacts\n",
        "pd.DataFrame({'label': labels_list}).to_csv('labels.csv', index=False)\n",
        "np.save('per_tag_thresholds.npy', per_tag_thr)\n",
        "np.save('global_threshold.npy', np.array([best_thr], dtype=np.float32))\n",
        "pd.DataFrame({'threshold': THS, 'f1': f1s}).to_csv('oof_global_f1_curve.csv', index=False)\n",
        "print('[Artifacts] Saved labels.csv, per_tag_thresholds.npy, global_threshold.npy, oof_global_f1_curve.csv')\n",
        "\n",
        "# Safety rule for inference\n",
        "def apply_thresholds_with_safety(prob_row, label_indices, per_thr_vec, global_thr, min_k=1):\n",
        "    sel = []\n",
        "    for j in label_indices:\n",
        "        thr = per_thr_vec[j]\n",
        "        if prob_row[j] >= thr:\n",
        "            sel.append(j)\n",
        "    if len(sel) == 0:\n",
        "        if len(label_indices) == 0:\n",
        "            return []\n",
        "        j_top = int(max(label_indices, key=lambda jj: prob_row[jj]))\n",
        "        sel = [j_top]\n",
        "    return sel\n",
        "\n",
        "print('[Phase 2 v3] DONE. Global OOF micro-F1 ~', round(best_f1, 5), 'at thr', round(best_thr, 3))\n",
        "print('[Timing] Avg fold time: {:.1f} min; total: {:.1f} min'.format(np.mean(fold_times)/60.0, np.sum(fold_times)/60.0))\n",
        "'''\n",
        "\n",
        "# Append the corrected modeling cell (v3)\n",
        "nb.cells.append(nbf.v4.new_code_cell(modeling_source))\n",
        "nb.metadata.setdefault('kernelspec', {\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"})\n",
        "with open(pristine_nb, 'w', encoding='utf-8') as f:\n",
        "    nbf.write(nb, f)\n",
        "print('[Pristine] Appended REFACTORED Phase 2 modeling cell (v3, streaming SGD) to agent_notebook_pristine.ipynb. Submit for audit, then Run-All to execute CV safely.')\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "d79f3dd5-14bc-4279-bbd2-e7725ebde1f8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Execute pristine notebook (micro-pilot): Run-All on agent_notebook_pristine.ipynb\n",
        "# - This will run the setup cell and the approved v3 modeling cell with MICRO_PILOT enabled\n",
        "# - Collects runtime logs and persists artifacts (labels.csv, thresholds, OOF curve, etc.)\n",
        "import os, time, shutil\n",
        "from datetime import datetime\n",
        "\n",
        "# Ensure dependencies for programmatic notebook execution\n",
        "try:\n",
        "    import nbformat as nbf\n",
        "except Exception:\n",
        "    import sys\n",
        "    get_ipython().run_line_magic('pip', 'install --quiet nbformat')\n",
        "    import nbformat as nbf\n",
        "try:\n",
        "    from nbclient import NotebookClient\n",
        "except Exception:\n",
        "    import sys\n",
        "    get_ipython().run_line_magic('pip', 'install --quiet nbclient')\n",
        "    from nbclient import NotebookClient\n",
        "\n",
        "pristine_nb_path = 'agent_notebook_pristine.ipynb'\n",
        "assert os.path.exists(pristine_nb_path), 'Pristine notebook not found. Please generate it first.'\n",
        "\n",
        "# Backup pristine notebook before execution\n",
        "ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "backup_path = f'agent_notebook_pristine_backup_{ts}_preexec.ipynb'\n",
        "shutil.copy2(pristine_nb_path, backup_path)\n",
        "print(f'[Backup] Pristine notebook backed up to {backup_path}')\n",
        "\n",
        "t0 = time.time()\n",
        "nb = nbf.read(pristine_nb_path, as_version=4)\n",
        "client = NotebookClient(nb, timeout=0, kernel_name='python3', allow_errors=False)\n",
        "print('[Execute] Starting Run-All for agent_notebook_pristine.ipynb (micro-pilot mode)...')\n",
        "client.execute()\n",
        "elapsed = time.time() - t0\n",
        "print(f'[Execute] Completed Run-All in {elapsed/60:.1f} minutes.')\n",
        "\n",
        "# Save executed notebook to preserve outputs\n",
        "executed_path = f'agent_notebook_pristine_executed_{ts}.ipynb'\n",
        "with open(executed_path, 'w', encoding='utf-8') as f:\n",
        "    nbf.write(nb, f)\n",
        "print(f'[Execute] Executed notebook saved to {executed_path}')\n",
        "\n",
        "# List expected artifacts\n",
        "expected = ['labels.csv', 'per_tag_thresholds.npy', 'global_threshold.npy', 'oof_global_f1_curve.csv']\n",
        "for art in expected:\n",
        "    print(f'[Artifact] {art}:', 'FOUND' if os.path.exists(art) else 'MISSING')\n",
        "print('[Micro-Pilot] Done. Review the console logs above for D, SHARD_SIZE, shards count, OOF micro-F1, and timings.')\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "9b6bbb94-1728-4446-a4ed-0c961699f868",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Micro-pilot executor without nbclient: load caches here and exec the pristine modeling cell with smaller N\n",
        "import os, gc, time, re, nbformat as nbf\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# 1) Load caches directly into this kernel (fast, avoids running pristine setup)\n",
        "TRAIN_FULL = 'parsed_train_full.pkl'\n",
        "TEST_PKL   = 'parsed_test.pkl'\n",
        "assert os.path.exists(TRAIN_FULL) and os.path.exists(TEST_PKL), 'Missing caches. Ensure parsed_train_full.pkl and parsed_test.pkl exist.'\n",
        "df_train_full = pd.read_pickle(TRAIN_FULL)\n",
        "df_test_cache = pd.read_pickle(TEST_PKL)\n",
        "print('[Kernel] Loaded df_train_full:', df_train_full.shape, 'df_test_cache:', df_test_cache.shape)\n",
        "\n",
        "# 2) Read pristine notebook and extract the modeling cell\n",
        "pristine_nb = 'agent_notebook_pristine.ipynb'\n",
        "assert os.path.exists(pristine_nb), 'Pristine notebook not found.'\n",
        "nb = nbf.read(pristine_nb, as_version=4)\n",
        "code_cells = [c for c in nb.cells if c.cell_type == 'code']\n",
        "assert len(code_cells) >= 2, f'Expected >=2 code cells (setup + modeling), found {len(code_cells)}'\n",
        "model_cell_src = code_cells[-1].source\n",
        "\n",
        "# 3) Patch micro-pilot params to be lighter and faster inside this kernel run\n",
        "def patch_param(src: str, pattern: str, replacement: str) -> str:\n",
        "    return re.sub(pattern, replacement, src)\n",
        "\n",
        "# Force MICRO_PILOT=True, PILOT_N=10000, MIN_LABEL_FREQ_TRAIN=100, BATCH_SIZE=8192\n",
        "patched = model_cell_src\n",
        "patched = patch_param(patched, r'MICRO_PILOT\\s*=\\s*\\w+', 'MICRO_PILOT = True')\n",
        "patched = patch_param(patched, r'PILOT_N\\s*=\\s*\\d[\\d_]*', 'PILOT_N = 10_000')\n",
        "patched = patch_param(patched, r'MIN_LABEL_FREQ_TRAIN\\s*=\\s*\\d+', 'MIN_LABEL_FREQ_TRAIN = 100')\n",
        "patched = patch_param(patched, r'BATCH_SIZE\\s*=\\s*\\d+', 'BATCH_SIZE = 8192')\n",
        "\n",
        "# 3b) Precise bugfix: replace fold-local label batch indexing with absolute indexing on Y_all\n",
        "cnt_before_yb = len(re.findall(r\"Y_b\\s*=\\s*Y_tr_full\\[.*?\\]\\s*\\[:,\\s*shard\\]\", patched))\n",
        "cnt_before_yt = len(re.findall(r\"Y_true_batch\\s*=\\s*Y_va_full\\[.*?\\]\\s*\\[:,\\s*shard\\]\", patched))\n",
        "patched = re.sub(r'Y_b\\s*=\\s*Y_tr_full\\[(.*?)\\]\\s*\\[:,\\s*shard\\]', r'Y_b = Y_all[\\1][:, shard]', patched)\n",
        "patched = re.sub(r'Y_true_batch\\s*=\\s*Y_va_full\\[(.*?)\\]\\s*\\[:,\\s*shard\\]', r'Y_true_batch = Y_all[\\1][:, shard]', patched)\n",
        "cnt_after_yb = len(re.findall(r\"Y_b\\s*=\\s*Y_tr_full\\[.*?\\]\\s*\\[:,\\s*shard\\]\", patched))\n",
        "cnt_after_yt = len(re.findall(r\"Y_true_batch\\s*=\\s*Y_va_full\\[.*?\\]\\s*\\[:,\\s*shard\\]\", patched))\n",
        "print(f\"[Patch] Y_b replacement occurrences: before={cnt_before_yb}, after={cnt_after_yb}\")\n",
        "print(f\"[Patch] Y_true_batch replacement occurrences: before={cnt_before_yt}, after={cnt_after_yt}\")\n",
        "\n",
        "# Optional: excluded FN computation remains valid using Y_va_full[:, excluded_labels]\n",
        "\n",
        "# 4) Execute the patched modeling cell code in this kernel\n",
        "print('[Exec] Starting micro-pilot with PILOT_N=10k, MIN_LABEL_FREQ_TRAIN=100, BATCH_SIZE=8192...')\n",
        "t0 = time.time()\n",
        "g = globals()\n",
        "try:\n",
        "    exec(patched, g, g)\n",
        "    print('[Exec] Micro-pilot completed in {:.1f} min'.format((time.time()-t0)/60.0))\n",
        "except Exception as e:\n",
        "    import traceback\n",
        "    print('[Exec] Micro-pilot failed:', repr(e))\n",
        "    traceback.print_exc()\n",
        "\n",
        "# 5) Verify artifacts existence after run\n",
        "expected = ['labels.csv', 'per_tag_thresholds.npy', 'global_threshold.npy', 'oof_global_f1_curve.csv']\n",
        "for art in expected:\n",
        "    print(f'[Artifact] {art}:', 'FOUND' if os.path.exists(art) else 'MISSING')\n",
        "print('[Note] Review above logs for D, SHARD_SIZE, shard count, OOF F1, and timings. This is a micro-pilot sanity check.')\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "449461a4-f543-4364-a640-7dbb85deeaef",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Vectorized micro-pilot (batch-major, shard-wise SGD) \u2014 1-fold\n",
        "# Mandatory fixes applied: remove per-batch class weighting, global bias init via prior_logit,\n",
        "# LR=0.05, EPOCHS=3, threshold sweep widened to [0.02, 0.5] with 30 points.\n",
        "import os, gc, time, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
        "\n",
        "GLOBAL_SEED = 1337\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "\n",
        "TRAIN_FULL = 'parsed_train_full.pkl'\n",
        "TEST_PKL = 'parsed_test.pkl'\n",
        "assert os.path.exists(TRAIN_FULL) and os.path.exists(TEST_PKL), 'Missing caches parsed_train_full.pkl / parsed_test.pkl'\n",
        "df_train_full = pd.read_pickle(TRAIN_FULL)\n",
        "df_test_cache = pd.read_pickle(TEST_PKL)\n",
        "print('[Kernel] Loaded caches:', df_train_full.shape, df_test_cache.shape)\n",
        "\n",
        "# Labels\n",
        "def to_list_tags(s):\n",
        "    return s.split() if isinstance(s, str) else []\n",
        "y_lists = df_train_full['Tags'].astype(str).apply(to_list_tags)\n",
        "mlb = MultiLabelBinarizer(sparse_output=True)\n",
        "Y_all = mlb.fit_transform(y_lists)\n",
        "labels_list = mlb.classes_.tolist()\n",
        "n_samples, n_labels = Y_all.shape\n",
        "print('[Labels] #samples:', n_samples, '#labels:', n_labels)\n",
        "\n",
        "# Channels\n",
        "title_text = df_train_full['title_norm'].fillna('').astype(str)\n",
        "body_text  = df_train_full['body_text'].fillna('').astype(str)\n",
        "code_text  = df_train_full['code_text'].fillna('').astype(str)\n",
        "meta_cols = ['title_len','body_len','code_len','num_block_code','num_urls','punct_density']\n",
        "meta_all = df_train_full[meta_cols].astype(np.float32).values\n",
        "\n",
        "# Pilot config (aim for realistic yet fast)\n",
        "MICRO_PILOT = True\n",
        "PILOT_N = 100_000\n",
        "MIN_LABEL_FREQ_TRAIN = 20\n",
        "BATCH_SIZE = 8_192  # batches per epoch\n",
        "COEF_BUDGET_BYTES = 4_000_000_000  # 4GB\n",
        "EPOCHS = 3\n",
        "LR = 0.05\n",
        "L2 = 2e-4\n",
        "title_alpha = 3.0\n",
        "THS = np.linspace(0.02, 0.5, 30)\n",
        "\n",
        "if MICRO_PILOT:\n",
        "    n_keep = int(min(PILOT_N, n_samples))\n",
        "    rng = np.random.RandomState(GLOBAL_SEED)\n",
        "    idx_keep = rng.choice(n_samples, size=n_keep, replace=False)\n",
        "    idx_keep.sort()\n",
        "    title_text = title_text.iloc[idx_keep].reset_index(drop=True)\n",
        "    body_text  = body_text.iloc[idx_keep].reset_index(drop=True)\n",
        "    code_text  = code_text.iloc[idx_keep].reset_index(drop=True)\n",
        "    meta_all   = meta_all[idx_keep]\n",
        "    Y_all      = Y_all[idx_keep]\n",
        "    n_samples  = Y_all.shape[0]\n",
        "    print(f'[Pilot] Subsampled to {n_samples} rows')\n",
        "\n",
        "# Iterative stratification (dependency guard)\n",
        "try:\n",
        "    from skmultilearn.model_selection import IterativeStratification\n",
        "except Exception:\n",
        "    import sys\n",
        "    from subprocess import run\n",
        "    run([sys.executable, '-m', 'pip', 'install', '--quiet', 'scikit-multilearn'])\n",
        "    from skmultilearn.model_selection import IterativeStratification\n",
        "\n",
        "mskf = IterativeStratification(n_splits=5, order=1)\n",
        "X_dummy = np.zeros((n_samples, 1))\n",
        "\n",
        "# Vectorizers (reduced caps to keep D in check)\n",
        "title_vec_cfg = dict(analyzer='word', ngram_range=(1,3), min_df=3, max_df=0.95,\n",
        "                     max_features=150_000, sublinear_tf=True, dtype=np.float32)\n",
        "body_vec_cfg  = dict(analyzer='word', ngram_range=(1,3), min_df=3, max_df=0.95,\n",
        "                     max_features=200_000, sublinear_tf=True, dtype=np.float32)\n",
        "code_vec_cfg  = dict(analyzer='word', ngram_range=(1,3), min_df=2, max_df=0.999,\n",
        "                     token_pattern=r'(?u)\\b\\w[\\w_\\+\\-\\#\\.]*\\b', max_features=80_000,\n",
        "                     sublinear_tf=True, dtype=np.float32)\n",
        "char_vec_cfg  = dict(analyzer='char_wb', ngram_range=(3,6), min_df=2, max_features=150_000, dtype=np.float32)\n",
        "meta_scaler = StandardScaler(with_mean=False)\n",
        "\n",
        "def batch_indices(idxs, bs):\n",
        "    for s in range(0, idxs.size, bs):\n",
        "        yield idxs[s:min(idxs.size, s+bs)]\n",
        "\n",
        "def sigmoid_stable(Z):\n",
        "    Z = np.clip(Z, -8.0, 8.0)\n",
        "    return 1.0 / (1.0 + np.exp(-Z))\n",
        "\n",
        "def micro_f1(tp, fp, fn):\n",
        "    denom = (2*tp + fp + fn)\n",
        "    return 0.0 if denom == 0 else (2.0*tp)/denom\n",
        "\n",
        "def prior_logit(p):\n",
        "    p = np.clip(p, 1e-6, 1-1e-6)\n",
        "    return np.log(p/(1.0-p))\n",
        "\n",
        "# Run first fold only for acceptance\n",
        "fold_id = 0\n",
        "tp_tot = np.zeros_like(THS, dtype=np.int64)\n",
        "fp_tot = np.zeros_like(THS, dtype=np.int64)\n",
        "fn_tot = np.zeros_like(THS, dtype=np.int64)\n",
        "t_fold = time.time()\n",
        "for tr_idx, va_idx in mskf.split(X_dummy, Y_all):\n",
        "    print(f'[Fold {fold_id}] Train={tr_idx.size}, Valid={va_idx.size}')\n",
        "    Y_tr = Y_all[tr_idx]\n",
        "    Y_va = Y_all[va_idx]\n",
        "    sup_tr = np.asarray(Y_tr.sum(axis=0)).ravel()\n",
        "    kept_labels = np.where(sup_tr >= MIN_LABEL_FREQ_TRAIN)[0]\n",
        "    excluded_labels = np.setdiff1d(np.arange(n_labels), kept_labels)\n",
        "    print(f'[Fold {fold_id}] kept_labels={kept_labels.size}, excluded={excluded_labels.size}')\n",
        "    if kept_labels.size == 0:\n",
        "        print('[Fold {fold_id}] No labels after pruning; abort fold')\n",
        "        break\n",
        "\n",
        "    # Fit vectorizers on train split only\n",
        "    title_vec = TfidfVectorizer(**title_vec_cfg)\n",
        "    body_vec  = TfidfVectorizer(**body_vec_cfg)\n",
        "    code_vec  = TfidfVectorizer(**code_vec_cfg)\n",
        "    char_vec  = TfidfVectorizer(**char_vec_cfg)\n",
        "    title_vec.fit(title_text.iloc[tr_idx])\n",
        "    body_vec.fit(body_text.iloc[tr_idx])\n",
        "    code_vec.fit(code_text.iloc[tr_idx])\n",
        "    char_vec.fit((title_text.iloc[tr_idx] + ' ' + body_text.iloc[tr_idx]))\n",
        "    # Fit meta scaler\n",
        "    for b_idx in batch_indices(tr_idx, BATCH_SIZE):\n",
        "        meta_scaler.partial_fit(meta_all[b_idx])\n",
        "\n",
        "    # Probe to get D\n",
        "    probe_idx = tr_idx[:min(BATCH_SIZE, tr_idx.size)]\n",
        "    Xt_probe = title_vec.transform(title_text.iloc[probe_idx]).astype(np.float32)\n",
        "    Xt_probe = Xt_probe.multiply(title_alpha)\n",
        "    Xb_probe = body_vec.transform(body_text.iloc[probe_idx]).astype(np.float32)\n",
        "    Xcode_probe = code_vec.transform(code_text.iloc[probe_idx]).astype(np.float32)\n",
        "    Xchar_probe = char_vec.transform((title_text.iloc[probe_idx] + ' ' + body_text.iloc[probe_idx])).astype(np.float32)\n",
        "    Xm_probe = sparse.csr_matrix(meta_scaler.transform(meta_all[probe_idx]), dtype=np.float32)\n",
        "    D = sparse.hstack([Xt_probe, Xb_probe, Xchar_probe, Xcode_probe, Xm_probe], format='csr', dtype=np.float32).shape[1]\n",
        "    del Xt_probe, Xb_probe, Xchar_probe, Xcode_probe, Xm_probe; gc.collect()\n",
        "    bytes_per_coef = 4  # float32\n",
        "    shard_cap = max(1, int(COEF_BUDGET_BYTES // (bytes_per_coef * D)))\n",
        "    SHARD_SIZE = max(1, min(2000, shard_cap))\n",
        "    SHARD_SIZE = min(SHARD_SIZE, 600)\n",
        "    shards = [kept_labels[i:i+SHARD_SIZE] for i in range(0, kept_labels.size, SHARD_SIZE)]\n",
        "    est_w_mb = (D * SHARD_SIZE * bytes_per_coef) / (1024**2)\n",
        "    print(f'[Fold {fold_id}] D={D:,}, SHARD_SIZE={SHARD_SIZE}, #shards={len(shards)} (~{est_w_mb:.1f} MB per-shard W, budget={COEF_BUDGET_BYTES/1e9:.1f}GB, fp32)')\n",
        "\n",
        "    # Initialize per-shard weights with global bias using prior logit from training split\n",
        "    shard_params = []  # list of tuples (labels_idx, W[D x Lb], b[Lb])\n",
        "    n_tr = tr_idx.size\n",
        "    for shard in shards:\n",
        "        Lb = len(shard)\n",
        "        W = np.zeros((D, Lb), dtype=np.float32)\n",
        "        # prior per label from training split\n",
        "        pos = sup_tr[shard].astype(np.float64)\n",
        "        p = pos / float(n_tr)\n",
        "        b = prior_logit(p).astype(np.float32, copy=False)\n",
        "        shard_params.append((shard, W, b))\n",
        "\n",
        "    # Precompute list of batch index arrays for shuffling\n",
        "    tr_batches = [b for b in batch_indices(tr_idx, BATCH_SIZE)]\n",
        "\n",
        "    # Training: multi-epoch, batch-major, compute X_batch ONCE then update all shards\n",
        "    t_tr = time.time()\n",
        "    rng = np.random.RandomState(GLOBAL_SEED)\n",
        "    for ep in range(EPOCHS):\n",
        "        rng.shuffle(tr_batches)\n",
        "        t_ep = time.time()\n",
        "        for bi, b_idx in enumerate(tr_batches):\n",
        "            # Build features once\n",
        "            X_title = title_vec.transform(title_text.iloc[b_idx]).astype(np.float32).multiply(title_alpha)\n",
        "            X_body  = body_vec.transform(body_text.iloc[b_idx]).astype(np.float32)\n",
        "            X_code  = code_vec.transform(code_text.iloc[b_idx]).astype(np.float32)\n",
        "            X_char  = char_vec.transform((title_text.iloc[b_idx] + ' ' + body_text.iloc[b_idx])).astype(np.float32)\n",
        "            X_meta  = sparse.csr_matrix(meta_scaler.transform(meta_all[b_idx]), dtype=np.float32)\n",
        "            X_batch = sparse.hstack([X_title, X_body, X_char, X_code, X_meta], format='csr', dtype=np.float32)\n",
        "            bs = X_batch.shape[0]\n",
        "            # Update each shard with standard logistic loss (no per-batch class weighting)\n",
        "            for shard, W, b in shard_params:\n",
        "                Y_b = Y_all[b_idx][:, shard].toarray().astype(np.float32, copy=False)\n",
        "                Z = X_batch @ W\n",
        "                Z += b  # broadcast bias\n",
        "                P = sigmoid_stable(Z)\n",
        "                E = (P - Y_b)  # (bs x Lb)\n",
        "                # Gradients\n",
        "                grad_W = (X_batch.T @ E) / float(bs)\n",
        "                if isinstance(grad_W, np.matrix):\n",
        "                    grad_W = np.asarray(grad_W)\n",
        "                grad_W = grad_W.astype(np.float32, copy=False) + (L2 * W)\n",
        "                grad_b = E.mean(axis=0).astype(np.float32, copy=False)\n",
        "                # Update\n",
        "                W -= LR * grad_W\n",
        "                b -= LR * grad_b\n",
        "            if (bi % 5 == 0) and len(shard_params) > 0:\n",
        "                shard0, W0, b0 = shard_params[0]\n",
        "                Z0 = X_batch @ W0; Z0 += b0\n",
        "                P0 = sigmoid_stable(Z0)\n",
        "                Y0 = Y_all[b_idx][:, shard0].toarray().astype(np.float32, copy=False)\n",
        "                eps = 1e-7\n",
        "                loss = -np.mean(Y0*np.log(P0+eps) + (1-Y0)*np.log(1-P0+eps))\n",
        "                print(f'[Fold {fold_id}] Ep {ep+1}/{EPOCHS} batch {bi+1}/{len(tr_batches)} loss~{loss:.4f}; meanP~{P0.mean():.4f}')\n",
        "            del X_title, X_body, X_code, X_char, X_meta, X_batch; gc.collect()\n",
        "        print(f'[Fold {fold_id}] Epoch {ep+1}/{EPOCHS} time: {(time.time()-t_ep)/60:.2f} min')\n",
        "    print(f'[Fold {fold_id}] Train total time: {(time.time()-t_tr)/60:.2f} min')\n",
        "\n",
        "    # Validation: batch-major, compute once, vectorized prediction per shard\n",
        "    def update_counts(Y_true_csr, probs):\n",
        "        for ti, thr in enumerate(THS):\n",
        "            pred_bin = (probs >= thr).astype(np.uint8)\n",
        "            pred_csr = sparse.csr_matrix(pred_bin, dtype=np.uint8)\n",
        "            tp = int(pred_csr.multiply(Y_true_csr).sum())\n",
        "            ppos = int(pred_bin.sum())\n",
        "            tpos = int(Y_true_csr.sum())\n",
        "            tp_tot[ti] += tp\n",
        "            fp_tot[ti] += (ppos - tp)\n",
        "            fn_tot[ti] += (tpos - tp)\n",
        "\n",
        "    t_va = time.time()\n",
        "    for b_idx in batch_indices(va_idx, BATCH_SIZE):\n",
        "        X_title = title_vec.transform(title_text.iloc[b_idx]).astype(np.float32).multiply(title_alpha)\n",
        "        X_body  = body_vec.transform(body_text.iloc[b_idx]).astype(np.float32)\n",
        "        X_code  = code_vec.transform(code_text.iloc[b_idx]).astype(np.float32)\n",
        "        X_char  = char_vec.transform((title_text.iloc[b_idx] + ' ' + body_text.iloc[b_idx])).astype(np.float32)\n",
        "        X_meta  = sparse.csr_matrix(meta_scaler.transform(meta_all[b_idx]), dtype=np.float32)\n",
        "        X_batch = sparse.hstack([X_title, X_body, X_char, X_code, X_meta], format='csr', dtype=np.float32)\n",
        "        P_all = None; col_slices = []\n",
        "        for shard, W, b in shard_params:\n",
        "            Z = (X_batch @ W); Z += b\n",
        "            P = sigmoid_stable(Z)\n",
        "            if P_all is None:\n",
        "                P_all = P; col_slices = [np.array(shard)]\n",
        "            else:\n",
        "                P_all = np.concatenate([P_all, P], axis=1)\n",
        "                col_slices.append(np.array(shard))\n",
        "        if len(col_slices) > 1:\n",
        "            order = np.concatenate(col_slices)\n",
        "            pos_map = {lab:i for i, lab in enumerate(order)}\n",
        "            kept_pos = np.array([pos_map[k] for k in kept_labels], dtype=np.int32)\n",
        "            P_kept = P_all[:, kept_pos]\n",
        "        else:\n",
        "            P_kept = P_all\n",
        "        Y_true_batch = Y_all[b_idx][:, kept_labels]\n",
        "        update_counts(Y_true_batch.tocsr(), P_kept)\n",
        "        del X_title, X_body, X_char, X_code, X_meta, X_batch, P_all, P_kept; gc.collect()\n",
        "    fn_excluded = int(Y_va[:, excluded_labels].sum()) if excluded_labels.size > 0 else 0\n",
        "    if fn_excluded > 0:\n",
        "        for i in range(THS.size):\n",
        "            fn_tot[i] += fn_excluded\n",
        "    print(f'[Fold {fold_id}] Validation time: {(time.time()-t_va)/60:.2f} min, FN(excluded)={fn_excluded}')\n",
        "\n",
        "    print(f'[Fold {fold_id}] Total fold time: {(time.time()-t_fold)/60:.2f} min')\n",
        "    break\n",
        "\n",
        "# Optimize global threshold\n",
        "f1s = [micro_f1(tp_tot[i], fp_tot[i], fn_tot[i]) for i in range(THS.size)]\n",
        "best_idx = int(np.argmax(f1s))\n",
        "best_thr = float(THS[best_idx])\n",
        "best_f1 = float(f1s[best_idx])\n",
        "print('[OOF] Global best micro-F1 = {:.5f} at thr = {:.3f}'.format(best_f1, best_thr))\n",
        "\n",
        "# Save artifacts compatible with downstream steps\n",
        "np.save('global_threshold.npy', np.array([best_thr], dtype=np.float32))\n",
        "pd.DataFrame({'threshold': THS, 'f1': f1s}).to_csv('oof_global_f1_curve.csv', index=False)\n",
        "pd.DataFrame({'label': labels_list}).to_csv('labels.csv', index=False)\n",
        "per_tag_thr = np.full(n_labels, best_thr, dtype=np.float32)\n",
        "np.save('per_tag_thresholds.npy', per_tag_thr)\n",
        "print('[Artifacts] Saved labels.csv, per_tag_thresholds.npy, global_threshold.npy, oof_global_f1_curve.csv')\n",
        "print('[Pilot Vectorized] DONE (with stable bias init and LR/EPOCHS adjustments)')\n",
        ""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Kernel] Loaded caches: (5430775, 11) (603420, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Labels] #samples: 5430775 #labels: 41781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Pilot] Subsampled to 100000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Train=80041, Valid=19959\n[Fold 0] kept_labels=1556, excluded=40225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] D=480,087, SHARD_SIZE=600, #shards=3 (~1098.8 MB per-shard W, budget=4.0GB, fp32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 1/3 batch 1/10 loss~0.0093; meanP~0.0015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 1/3 batch 6/10 loss~0.0091; meanP~0.0015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Epoch 1/3 time: 6.24 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 2/3 batch 1/10 loss~0.0094; meanP~0.0015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 2/3 batch 6/10 loss~0.0093; meanP~0.0015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Epoch 2/3 time: 6.74 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 3/3 batch 1/10 loss~0.0092; meanP~0.0015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 3/3 batch 6/10 loss~0.0094; meanP~0.0015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Epoch 3/3 time: 6.54 min\n[Fold 0] Train total time: 19.53 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Validation time: 1.15 min, FN(excluded)=13245\n[Fold 0] Total fold time: 24.23 min\n[OOF] Global best micro-F1 = 0.08454 at thr = 0.053\n[Artifacts] Saved labels.csv, per_tag_thresholds.npy, global_threshold.npy, oof_global_f1_curve.csv\n[Pilot Vectorized] DONE (with stable bias init and LR/EPOCHS adjustments)\n"
          ]
        }
      ]
    },
    {
      "id": "56ab4fae-a8ca-4812-8b75-1bf34e7d66a4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Vectorized micro-pilot vNext-opt: weighted + momentum + clipping + hashing (1-fold)\n",
        "# Batch-major optimization: build X_batch once and reuse across shards per batch\n",
        "# - Global per-label positive weighting (fold-level, clipped to [1,10])\n",
        "# - Nesterov momentum, gradient clipping\n",
        "# - Hashing for body/char (reduced dims)\n",
        "# - Xavier warm-start, prior_logit bias\n",
        "# - Batch-major training across shards to remove redundant feature work\n",
        "import os, gc, time, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
        "\n",
        "GLOBAL_SEED = 1337\n",
        "rng_global = np.random.RandomState(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "\n",
        "TRAIN_FULL = 'parsed_train_full.pkl'\n",
        "TEST_PKL = 'parsed_test.pkl'\n",
        "assert os.path.exists(TRAIN_FULL) and os.path.exists(TEST_PKL), 'Missing caches parsed_train_full.pkl / parsed_test.pkl'\n",
        "df_train_full = pd.read_pickle(TRAIN_FULL)\n",
        "df_test_cache = pd.read_pickle(TEST_PKL)\n",
        "print('[Kernel] Loaded caches:', df_train_full.shape, df_test_cache.shape)\n",
        "\n",
        "# Labels\n",
        "def to_list_tags(s):\n",
        "    return s.split() if isinstance(s, str) else []\n",
        "y_lists = df_train_full['Tags'].astype(str).apply(to_list_tags)\n",
        "mlb = MultiLabelBinarizer(sparse_output=True)\n",
        "Y_all = mlb.fit_transform(y_lists)\n",
        "labels_list = mlb.classes_.tolist()\n",
        "n_samples, n_labels = Y_all.shape\n",
        "print('[Labels] #samples:', n_samples, '#labels:', n_labels)\n",
        "\n",
        "# Channels\n",
        "title_text = df_train_full['title_norm'].fillna('').astype(str)\n",
        "body_text  = df_train_full['body_text'].fillna('').astype(str)\n",
        "code_text  = df_train_full['code_text'].fillna('').astype(str)\n",
        "meta_cols = ['title_len','body_len','code_len','num_block_code','num_urls','punct_density']\n",
        "meta_all = df_train_full[meta_cols].astype(np.float32).values\n",
        "\n",
        "# Pilot config\n",
        "MICRO_PILOT = True\n",
        "PILOT_N = 100_000\n",
        "MIN_LABEL_FREQ_TRAIN = 20\n",
        "BATCH_SIZE = 8_192\n",
        "COEF_BUDGET_BYTES = 4_000_000_000  # budget used for estimating shard size (we hold all shards concurrently here)\n",
        "EPOCHS = 8\n",
        "LR = 0.12\n",
        "MU = 0.9\n",
        "L2 = 2e-4\n",
        "title_alpha = 3.0\n",
        "THS = np.linspace(0.02, 0.5, 30)\n",
        "\n",
        "if MICRO_PILOT:\n",
        "    n_keep = int(min(PILOT_N, n_samples))\n",
        "    idx_keep = rng_global.choice(n_samples, size=n_keep, replace=False)\n",
        "    idx_keep.sort()\n",
        "    title_text = title_text.iloc[idx_keep].reset_index(drop=True)\n",
        "    body_text  = body_text.iloc[idx_keep].reset_index(drop=True)\n",
        "    code_text  = code_text.iloc[idx_keep].reset_index(drop=True)\n",
        "    meta_all   = meta_all[idx_keep]\n",
        "    Y_all      = Y_all[idx_keep]\n",
        "    n_samples  = Y_all.shape[0]\n",
        "    print(f'[Pilot] Subsampled to {n_samples} rows')\n",
        "\n",
        "# Iterative stratification (dependency guard)\n",
        "try:\n",
        "    from skmultilearn.model_selection import IterativeStratification\n",
        "except Exception:\n",
        "    import sys\n",
        "    from subprocess import run\n",
        "    run([sys.executable, '-m', 'pip', 'install', '--quiet', 'scikit-multilearn'])\n",
        "    from skmultilearn.model_selection import IterativeStratification\n",
        "\n",
        "mskf = IterativeStratification(n_splits=5, order=1)\n",
        "X_dummy = np.zeros((n_samples, 1))\n",
        "\n",
        "# Vectorizers (hashing for heavy channels, reduced dims)\n",
        "title_vec_cfg = dict(analyzer='word', ngram_range=(1,3), min_df=3, max_df=0.95,\n",
        "                     max_features=150_000, sublinear_tf=True, dtype=np.float32)\n",
        "body_hash_cfg  = dict(analyzer='word', ngram_range=(1,3), n_features=2**18,\n",
        "                      alternate_sign=False, norm='l2', dtype=np.float32)\n",
        "char_hash_cfg  = dict(analyzer='char_wb', ngram_range=(3,6), n_features=2**17,\n",
        "                      alternate_sign=False, norm='l2', dtype=np.float32)\n",
        "code_vec_cfg  = dict(analyzer='word', ngram_range=(1,3), min_df=2, max_df=0.999,\n",
        "                     token_pattern=r'(?u)\\b\\w[\\w_\\+\\-\\#\\.]*\\b', max_features=80_000,\n",
        "                     sublinear_tf=True, dtype=np.float32)\n",
        "meta_scaler = StandardScaler(with_mean=False)\n",
        "\n",
        "def batch_indices(idxs, bs):\n",
        "    for s in range(0, idxs.size, bs):\n",
        "        yield idxs[s:min(idxs.size, s+bs)]\n",
        "\n",
        "def sigmoid_stable(Z):\n",
        "    Z = np.clip(Z, -12.0, 12.0)\n",
        "    return 1.0 / (1.0 + np.exp(-Z))\n",
        "\n",
        "def micro_f1(tp, fp, fn):\n",
        "    denom = (2*tp + fp + fn)\n",
        "    return 0.0 if denom == 0 else (2.0*tp)/denom\n",
        "\n",
        "def prior_logit(p):\n",
        "    p = np.clip(p, 1e-6, 1-1e-6)\n",
        "    return np.log(p/(1.0-p))\n",
        "\n",
        "fold_id = 0\n",
        "tp_tot = np.zeros_like(THS, dtype=np.int64)\n",
        "fp_tot = np.zeros_like(THS, dtype=np.int64)\n",
        "fn_tot = np.zeros_like(THS, dtype=np.int64)\n",
        "t_fold = time.time()\n",
        "for tr_idx, va_idx in mskf.split(X_dummy, Y_all):\n",
        "    print(f'[Fold {fold_id}] Train={tr_idx.size}, Valid={va_idx.size}')\n",
        "    Y_tr = Y_all[tr_idx]\n",
        "    Y_va = Y_all[va_idx]\n",
        "    sup_tr = np.asarray(Y_tr.sum(axis=0)).ravel()\n",
        "    kept_labels = np.where(sup_tr >= MIN_LABEL_FREQ_TRAIN)[0]\n",
        "    excluded_labels = np.setdiff1d(np.arange(n_labels), kept_labels)\n",
        "    print(f'[Fold {fold_id}] kept_labels={kept_labels.size}, excluded={excluded_labels.size}')\n",
        "    if kept_labels.size == 0:\n",
        "        print('[Fold {fold_id}] No labels after pruning; abort fold')\n",
        "        break\n",
        "\n",
        "    # Global per-label positive weighting (fold-level)\n",
        "    p_tr = np.clip(sup_tr / float(tr_idx.size), 1e-6, 1-1e-6)\n",
        "    pos_weights_global = np.clip(1.0 / p_tr, 1.0, 10.0).astype(np.float32)\n",
        "\n",
        "    # Fit vectorizers on train split only\n",
        "    title_vec = TfidfVectorizer(**title_vec_cfg)\n",
        "    code_vec  = TfidfVectorizer(**code_vec_cfg)\n",
        "    body_hash = HashingVectorizer(**body_hash_cfg)\n",
        "    char_hash = HashingVectorizer(**char_hash_cfg)\n",
        "    title_vec.fit(title_text.iloc[tr_idx])\n",
        "    code_vec.fit(code_text.iloc[tr_idx])\n",
        "    # Fit meta scaler\n",
        "    for b_idx in batch_indices(tr_idx, BATCH_SIZE):\n",
        "        meta_scaler.partial_fit(meta_all[b_idx])\n",
        "\n",
        "    # Probe to get D and shard size\n",
        "    probe_idx = tr_idx[:min(BATCH_SIZE, tr_idx.size)]\n",
        "    Xt_probe = title_vec.transform(title_text.iloc[probe_idx]).astype(np.float32).multiply(title_alpha)\n",
        "    Xb_probe = body_hash.transform(body_text.iloc[probe_idx]).astype(np.float32)\n",
        "    Xchar_probe = char_hash.transform((title_text.iloc[probe_idx] + ' ' + body_text.iloc[probe_idx])).astype(np.float32)\n",
        "    Xcode_probe = code_vec.transform(code_text.iloc[probe_idx]).astype(np.float32)\n",
        "    Xm_probe = sparse.csr_matrix(meta_scaler.transform(meta_all[probe_idx]), dtype=np.float32)\n",
        "    D = sparse.hstack([Xt_probe, Xb_probe, Xchar_probe, Xcode_probe, Xm_probe], format='csr', dtype=np.float32).shape[1]\n",
        "    del Xt_probe, Xb_probe, Xchar_probe, Xcode_probe, Xm_probe; gc.collect()\n",
        "    bytes_per_coef = 4\n",
        "    shard_cap = max(1, int(COEF_BUDGET_BYTES // (bytes_per_coef * D)))\n",
        "    SHARD_SIZE = max(1, min(600, shard_cap))\n",
        "    shards = [kept_labels[i:i+SHARD_SIZE] for i in range(0, kept_labels.size, SHARD_SIZE)]\n",
        "    est_w_mb = (D * SHARD_SIZE * bytes_per_coef) / (1024**2)\n",
        "    print(f'[Fold {fold_id}] D={D:,}, SHARD_SIZE={SHARD_SIZE}, #shards={len(shards)} (~{est_w_mb:.1f} MB per-shard W, fp32)')\n",
        "\n",
        "    # Initialize all shard parameters (we hold all shards concurrently to enable batch-major reuse)\n",
        "    shard_params = []  # list of tuples: (labels_idx, W, b, Vw, Vb)\n",
        "    n_tr = tr_idx.size\n",
        "    for shard in shards:\n",
        "        Lb = len(shard)\n",
        "        std = 1e-3 / math.sqrt(D)\n",
        "        W = (rng_global.randn(D, Lb).astype(np.float32) * std).astype(np.float32)\n",
        "        pos = sup_tr[shard].astype(np.float64)\n",
        "        p = pos / float(n_tr)\n",
        "        b = prior_logit(p).astype(np.float32, copy=False)\n",
        "        Vw = np.zeros_like(W, dtype=np.float32)\n",
        "        Vb = np.zeros_like(b, dtype=np.float32)\n",
        "        shard_params.append((shard, W, b, Vw, Vb))\n",
        "\n",
        "    # Precompute shuffled batch index arrays\n",
        "    tr_batches = [b for b in batch_indices(tr_idx, BATCH_SIZE)]\n",
        "\n",
        "    # Training: batch-major \u2014 build X_batch once and reuse across all shards\n",
        "    t_tr = time.time()\n",
        "    rng = np.random.RandomState(GLOBAL_SEED)\n",
        "    for ep in range(EPOCHS):\n",
        "        rng.shuffle(tr_batches)\n",
        "        t_ep = time.time()\n",
        "        for bi, b_idx in enumerate(tr_batches):\n",
        "            # Build batch features once\n",
        "            X_title = title_vec.transform(title_text.iloc[b_idx]).astype(np.float32).multiply(title_alpha)\n",
        "            X_body  = body_hash.transform(body_text.iloc[b_idx]).astype(np.float32)\n",
        "            X_char  = char_hash.transform((title_text.iloc[b_idx] + ' ' + body_text.iloc[b_idx])).astype(np.float32)\n",
        "            X_code  = code_vec.transform(code_text.iloc[b_idx]).astype(np.float32)\n",
        "            X_meta  = sparse.csr_matrix(meta_scaler.transform(meta_all[b_idx]), dtype=np.float32)\n",
        "            X_batch = sparse.hstack([X_title, X_body, X_char, X_code, X_meta], format='csr', dtype=np.float32)\n",
        "            bs = X_batch.shape[0]\n",
        "            # Update each shard using the same X_batch\n",
        "            for si, (shard, W, b, Vw, Vb) in enumerate(shard_params):\n",
        "                Y_b = Y_all[b_idx][:, shard].toarray().astype(np.float32, copy=False)\n",
        "                Z = X_batch @ W; Z += b\n",
        "                P = sigmoid_stable(Z)\n",
        "                E = (P - Y_b)\n",
        "                # Positive weighting mask\n",
        "                w_pos = pos_weights_global[shard][None, :]\n",
        "                Wmat = 1.0 + (w_pos - 1.0) * Y_b\n",
        "                E_weighted = E * Wmat\n",
        "                # Gradients\n",
        "                grad_W = (X_batch.T @ E_weighted) / float(bs)\n",
        "                if isinstance(grad_W, np.matrix):\n",
        "                    grad_W = np.asarray(grad_W)\n",
        "                grad_W = grad_W.astype(np.float32, copy=False) + (L2 * W)\n",
        "                grad_b = E_weighted.mean(axis=0).astype(np.float32, copy=False)\n",
        "                # Gradient clipping\n",
        "                col_norms = np.linalg.norm(grad_W, axis=0) + 1e-8\n",
        "                clip_scale = np.minimum(1.0, 5.0 / col_norms).astype(np.float32, copy=False)\n",
        "                grad_W *= clip_scale\n",
        "                grad_b = np.clip(grad_b, -1.0, 1.0)\n",
        "                # Nesterov momentum (in-place)\n",
        "                Vw *= MU; Vw += LR * grad_W\n",
        "                Vb *= MU; Vb += LR * grad_b\n",
        "                W -= (MU * Vw + LR * grad_W)\n",
        "                b -= (MU * Vb + LR * grad_b)\n",
        "                shard_params[si] = (shard, W, b, Vw, Vb)\n",
        "            if (bi % 3 == 0) and len(shard_params) > 0:\n",
        "                shard0, W0, b0, _, _ = shard_params[0]\n",
        "                Z0 = X_batch @ W0; Z0 += b0\n",
        "                P0 = sigmoid_stable(Z0)\n",
        "                Y0 = Y_all[b_idx][:, shard0].toarray().astype(np.float32, copy=False)\n",
        "                eps = 1e-7\n",
        "                loss = -np.mean(Y0*np.log(P0+eps) + (1-Y0)*np.log(1-P0+eps))\n",
        "                print(f'[Fold {fold_id}] Ep {ep+1}/{EPOCHS} batch {bi+1}/{len(tr_batches)} loss~{loss:.4f}; meanP~{P0.mean():.4f}')\n",
        "            del X_title, X_body, X_char, X_code, X_meta, X_batch; gc.collect()\n",
        "        print(f'[Fold {fold_id}] Epoch {ep+1}/{EPOCHS} time: {(time.time()-t_ep)/60:.2f} min')\n",
        "    print(f'[Fold {fold_id}] Train total time: {(time.time()-t_tr)/60:.2f} min')\n",
        "\n",
        "    # Validation: update TP/FP/FN per shard directly (no concatenation)\n",
        "    def update_counts(Y_true_csr, probs):\n",
        "        for ti, thr in enumerate(THS):\n",
        "            pred_bin = (probs >= thr).astype(np.uint8)\n",
        "            pred_csr = sparse.csr_matrix(pred_bin, dtype=np.uint8)\n",
        "            tp = int(pred_csr.multiply(Y_true_csr).sum())\n",
        "            ppos = int(pred_bin.sum())\n",
        "            tpos = int(Y_true_csr.sum())\n",
        "            tp_tot[ti] += tp\n",
        "            fp_tot[ti] += (ppos - tp)\n",
        "            fn_tot[ti] += (tpos - tp)\n",
        "\n",
        "    t_va = time.time()\n",
        "    for b_idx in batch_indices(va_idx, BATCH_SIZE):\n",
        "        X_title = title_vec.transform(title_text.iloc[b_idx]).astype(np.float32).multiply(title_alpha)\n",
        "        X_body  = body_hash.transform(body_text.iloc[b_idx]).astype(np.float32)\n",
        "        X_char  = char_hash.transform((title_text.iloc[b_idx] + ' ' + body_text.iloc[b_idx])).astype(np.float32)\n",
        "        X_code  = code_vec.transform(code_text.iloc[b_idx]).astype(np.float32)\n",
        "        X_meta  = sparse.csr_matrix(meta_scaler.transform(meta_all[b_idx]), dtype=np.float32)\n",
        "        X_batch = sparse.hstack([X_title, X_body, X_char, X_code, X_meta], format='csr', dtype=np.float32)\n",
        "        for shard, W, b, _, _ in shard_params:\n",
        "            Z = (X_batch @ W); Z += b\n",
        "            P = sigmoid_stable(Z)\n",
        "            Y_true_batch = Y_all[b_idx][:, shard]\n",
        "            update_counts(Y_true_batch.tocsr(), P)\n",
        "        del X_title, X_body, X_char, X_code, X_meta, X_batch; gc.collect()\n",
        "    print(f'[Fold {fold_id}] Validation time: {(time.time()-t_va)/60:.2f} min')\n",
        "\n",
        "    # Account for excluded labels' positives as FN\n",
        "    fn_excluded = int(Y_va[:, excluded_labels].sum()) if excluded_labels.size > 0 else 0\n",
        "    if fn_excluded > 0:\n",
        "        for i in range(THS.size):\n",
        "            fn_tot[i] += fn_excluded\n",
        "    print(f'[Fold {fold_id}] Added FN(excluded)={fn_excluded}')\n",
        "\n",
        "    print(f'[Fold {fold_id}] Total fold time: {(time.time()-t_fold)/60:.2f} min')\n",
        "    break\n",
        "\n",
        "# Optimize global threshold\n",
        "f1s = [micro_f1(tp_tot[i], fp_tot[i], fn_tot[i]) for i in range(THS.size)]\n",
        "best_idx = int(np.argmax(f1s))\n",
        "best_thr = float(THS[best_idx])\n",
        "best_f1 = float(f1s[best_idx])\n",
        "print('[OOF] Global best micro-F1 = {:.5f} at thr = {:.3f}'.format(best_f1, best_thr))\n",
        "\n",
        "# Save artifacts\n",
        "np.save('global_threshold.npy', np.array([best_thr], dtype=np.float32))\n",
        "pd.DataFrame({'threshold': THS, 'f1': f1s}).to_csv('oof_global_f1_curve.csv', index=False)\n",
        "pd.DataFrame({'label': labels_list}).to_csv('labels.csv', index=False)\n",
        "per_tag_thr = np.full(n_labels, best_thr, dtype=np.float32)\n",
        "np.save('per_tag_thresholds.npy', per_tag_thr)\n",
        "print('[Artifacts] Saved labels.csv, per_tag_thresholds.npy, global_threshold.npy, oof_global_f1_curve.csv')\n",
        "print('[Pilot Vectorized vNext-opt] DONE (batch-major across shards)')\n",
        ""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Kernel] Loaded caches: (5430775, 11) (603420, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Labels] #samples: 5430775 #labels: 41781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Pilot] Subsampled to 100000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Train=80041, Valid=19959\n[Fold 0] kept_labels=1556, excluded=40225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] D=523,303, SHARD_SIZE=600, #shards=3 (~1197.7 MB per-shard W, fp32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 1/8 batch 1/10 loss~0.0098; meanP~0.0024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 1/8 batch 4/10 loss~0.0138; meanP~0.0060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 1/8 batch 7/10 loss~0.0130; meanP~0.0064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 1/8 batch 10/10 loss~0.0140; meanP~0.0075\n[Fold 0] Epoch 1/8 time: 7.60 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 2/8 batch 1/10 loss~0.0144; meanP~0.0080\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 2/8 batch 4/10 loss~0.0148; meanP~0.0087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 2/8 batch 7/10 loss~0.0151; meanP~0.0091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 2/8 batch 10/10 loss~0.0153; meanP~0.0094\n[Fold 0] Epoch 2/8 time: 8.30 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 3/8 batch 1/10 loss~0.0151; meanP~0.0095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 3/8 batch 4/10 loss~0.0154; meanP~0.0097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 3/8 batch 7/10 loss~0.0152; meanP~0.0099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 3/8 batch 10/10 loss~0.0152; meanP~0.0098\n[Fold 0] Epoch 3/8 time: 8.37 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 4/8 batch 1/10 loss~0.0150; meanP~0.0098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 4/8 batch 4/10 loss~0.0154; meanP~0.0102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 4/8 batch 7/10 loss~0.0152; meanP~0.0101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 4/8 batch 10/10 loss~0.0154; meanP~0.0102\n[Fold 0] Epoch 4/8 time: 8.43 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 5/8 batch 1/10 loss~0.0154; meanP~0.0102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 5/8 batch 4/10 loss~0.0152; meanP~0.0103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 5/8 batch 7/10 loss~0.0153; meanP~0.0104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 5/8 batch 10/10 loss~0.0149; meanP~0.0102\n[Fold 0] Epoch 5/8 time: 8.66 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 6/8 batch 1/10 loss~0.0153; meanP~0.0103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 6/8 batch 4/10 loss~0.0149; meanP~0.0103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 6/8 batch 7/10 loss~0.0151; meanP~0.0105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 6/8 batch 10/10 loss~0.0152; meanP~0.0105\n[Fold 0] Epoch 6/8 time: 8.08 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 7/8 batch 1/10 loss~0.0150; meanP~0.0104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 7/8 batch 4/10 loss~0.0148; meanP~0.0103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 7/8 batch 7/10 loss~0.0148; meanP~0.0103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 7/8 batch 10/10 loss~0.0151; meanP~0.0106\n[Fold 0] Epoch 7/8 time: 7.86 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 8/8 batch 1/10 loss~0.0149; meanP~0.0105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 8/8 batch 4/10 loss~0.0147; meanP~0.0103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 8/8 batch 7/10 loss~0.0150; meanP~0.0104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Ep 8/8 batch 10/10 loss~0.0148; meanP~0.0104\n[Fold 0] Epoch 8/8 time: 8.62 min\n[Fold 0] Train total time: 65.92 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Validation time: 1.07 min\n[Fold 0] Added FN(excluded)=13245\n[Fold 0] Total fold time: 68.72 min\n[OOF] Global best micro-F1 = 0.22046 at thr = 0.368\n[Artifacts] Saved labels.csv, per_tag_thresholds.npy, global_threshold.npy, oof_global_f1_curve.csv\n[Pilot Vectorized vNext-opt] DONE (batch-major across shards)\n"
          ]
        }
      ]
    },
    {
      "id": "282f0ce8-68a8-47bc-a7ed-b77062f6c914",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Track 2: OVR-SAGA Baseline (1-fold, 100k micro-pilot) with the same feature stack as Cell 17 (FAST CONFIG)\n",
        "# - Apples-to-apples channels: Title TF-IDF (boosted), Body Hashing, Char Hashing, Code TF-IDF, Meta (scaled)\n",
        "# - Leak-proof: fit vectorizers/scaler on train split only\n",
        "# - Per-fold label pruning (train-only) with FN addback for excluded labels\n",
        "# - Global threshold sweep + per-tag thresholds for high-support labels (>=50)\n",
        "\n",
        "import os, gc, time, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Dependency guard for iterative stratification\n",
        "try:\n",
        "    from skmultilearn.model_selection import IterativeStratification\n",
        "except Exception:\n",
        "    import sys\n",
        "    from subprocess import run\n",
        "    run([sys.executable, '-m', 'pip', 'install', '--quiet', 'scikit-multilearn'])\n",
        "    from skmultilearn.model_selection import IterativeStratification\n",
        "\n",
        "GLOBAL_SEED = 1337\n",
        "rng = np.random.RandomState(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "\n",
        "TRAIN_FULL = 'parsed_train_full.pkl'\n",
        "TEST_PKL   = 'parsed_test.pkl'\n",
        "assert os.path.exists(TRAIN_FULL) and os.path.exists(TEST_PKL), 'Missing caches parsed_train_full.pkl / parsed_test.pkl'\n",
        "df_train_full = pd.read_pickle(TRAIN_FULL)\n",
        "df_test_cache = pd.read_pickle(TEST_PKL)\n",
        "print('[SAGA] Loaded caches:', df_train_full.shape, df_test_cache.shape)\n",
        "\n",
        "# Labels\n",
        "def to_list_tags(s):\n",
        "    return s.split() if isinstance(s, str) else []\n",
        "y_lists = df_train_full['Tags'].astype(str).apply(to_list_tags)\n",
        "mlb = MultiLabelBinarizer(sparse_output=True)\n",
        "Y_all = mlb.fit_transform(y_lists)\n",
        "labels_list = mlb.classes_.tolist()\n",
        "n_samples, n_labels = Y_all.shape\n",
        "print('[SAGA][Labels] #samples:', n_samples, '#labels:', n_labels)\n",
        "\n",
        "# Channels\n",
        "title_text = df_train_full['title_norm'].fillna('').astype(str)\n",
        "body_text  = df_train_full['body_text'].fillna('').astype(str)\n",
        "code_text  = df_train_full['code_text'].fillna('').astype(str)\n",
        "meta_cols = ['title_len','body_len','code_len','num_block_code','num_urls','punct_density']\n",
        "meta_all = df_train_full[meta_cols].astype(np.float32).values\n",
        "\n",
        "# Micro-pilot: subsample to 100k and run first fold only\n",
        "MICRO_PILOT = True\n",
        "PILOT_N = 100_000\n",
        "if MICRO_PILOT:\n",
        "    n_keep = int(min(PILOT_N, n_samples))\n",
        "    idx_keep = rng.choice(n_samples, size=n_keep, replace=False)\n",
        "    idx_keep.sort()\n",
        "    title_text = title_text.iloc[idx_keep].reset_index(drop=True)\n",
        "    body_text  = body_text.iloc[idx_keep].reset_index(drop=True)\n",
        "    code_text  = code_text.iloc[idx_keep].reset_index(drop=True)\n",
        "    meta_all   = meta_all[idx_keep]\n",
        "    Y_all      = Y_all[idx_keep]\n",
        "    n_samples  = Y_all.shape[0]\n",
        "    print(f'[SAGA][Pilot] Subsampled to {n_samples} rows')\n",
        "\n",
        "# CV\n",
        "mskf = IterativeStratification(n_splits=5, order=1)\n",
        "X_dummy = np.zeros((n_samples, 1))\n",
        "\n",
        "# Feature configs (reduced for speed, still apples-to-apples channels)\n",
        "title_vec_cfg = dict(analyzer='word', ngram_range=(1,3), min_df=3, max_df=0.95,\n",
        "                     max_features=100_000, sublinear_tf=True, dtype=np.float32)\n",
        "body_hash_cfg  = dict(analyzer='word', ngram_range=(1,3), n_features=2**17,\n",
        "                      alternate_sign=False, norm='l2', dtype=np.float32)\n",
        "char_hash_cfg  = dict(analyzer='char_wb', ngram_range=(3,6), n_features=2**16,\n",
        "                      alternate_sign=False, norm='l2', dtype=np.float32)\n",
        "code_vec_cfg   = dict(analyzer='word', ngram_range=(1,3), min_df=2, max_df=0.999,\n",
        "                      token_pattern=r'(?u)\\b\\w[\\w_\\+\\-\\#\\.]*\\b', max_features=50_000,\n",
        "                      sublinear_tf=True, dtype=np.float32)\n",
        "title_alpha = 3.0\n",
        "MIN_LABEL_FREQ_TRAIN = 100  # more aggressive pruning for speed (train split only)\n",
        "THS = np.linspace(0.05, 0.6, 12)\n",
        "\n",
        "def micro_f1(tp, fp, fn):\n",
        "    denom = (2*tp + fp + fn)\n",
        "    return 0.0 if denom == 0 else (2.0*tp)/denom\n",
        "\n",
        "def batch_indices(idxs, bs):\n",
        "    for s in range(0, idxs.size, bs):\n",
        "        yield idxs[s:min(idxs.size, s+bs)]\n",
        "\n",
        "BATCH_SIZE = 8192\n",
        "\n",
        "tp_tot = np.zeros_like(THS, dtype=np.int64)\n",
        "fp_tot = np.zeros_like(THS, dtype=np.int64)\n",
        "fn_tot = np.zeros_like(THS, dtype=np.int64)\n",
        "\n",
        "# Per-tag thresholds (for high-support labels only)\n",
        "global_support = np.asarray(Y_all.sum(axis=0)).ravel()\n",
        "hi_mask = (global_support >= 50)\n",
        "hi_idx = np.where(hi_mask)[0]\n",
        "tp_hi = np.zeros((hi_idx.size, THS.size), dtype=np.int64)\n",
        "fp_hi = np.zeros((hi_idx.size, THS.size), dtype=np.int64)\n",
        "fn_hi = np.zeros((hi_idx.size, THS.size), dtype=np.int64)\n",
        "hi_pos = {lab: pos for pos, lab in enumerate(hi_idx)}\n",
        "\n",
        "def update_counts_batch(Y_true_batch_csr, probs_batch, label_idx_batch, ths, tp, fp, fn,\n",
        "                        tp_hi, fp_hi, fn_hi, hi_pos_map):\n",
        "    for ti, thr in enumerate(ths):\n",
        "        pred_bin = (probs_batch >= thr).astype(np.uint8)\n",
        "        pred_csr = sparse.csr_matrix(pred_bin, dtype=np.uint8)\n",
        "        tp_b = int(pred_csr.multiply(Y_true_batch_csr).sum())\n",
        "        ppos = int(pred_bin.sum())\n",
        "        tpos = int(Y_true_batch_csr.sum())\n",
        "        tp[ti] += tp_b\n",
        "        fp[ti] += (ppos - tp_b)\n",
        "        fn[ti] += (tpos - tp_b)\n",
        "        if hi_pos_map:\n",
        "            for j_local in range(probs_batch.shape[1]):\n",
        "                g_lab = int(label_idx_batch[j_local])\n",
        "                pos = hi_pos_map.get(g_lab, None)\n",
        "                if pos is None:\n",
        "                    continue\n",
        "                col_true = Y_true_batch_csr[:, j_local]\n",
        "                col_pred = pred_csr[:, j_local]\n",
        "                tp_j = int(col_true.multiply(col_pred).sum())\n",
        "                p_j = int(col_pred.sum())\n",
        "                t_j = int(col_true.sum())\n",
        "                tp_hi[pos, ti] += tp_j\n",
        "                fp_hi[pos, ti] += (p_j - tp_j)\n",
        "                fn_hi[pos, ti] += (t_j - tp_j)\n",
        "\n",
        "fold_id = 0\n",
        "t0_fold = time.time()\n",
        "for tr_idx, va_idx in mskf.split(X_dummy, Y_all):\n",
        "    print(f'[SAGA][Fold {fold_id}] Train={tr_idx.size}, Valid={va_idx.size}')\n",
        "    Y_tr_full = Y_all[tr_idx]\n",
        "    Y_va_full = Y_all[va_idx]\n",
        "    sup_tr = np.asarray(Y_tr_full.sum(axis=0)).ravel()\n",
        "    kept_labels = np.where(sup_tr >= MIN_LABEL_FREQ_TRAIN)[0]\n",
        "    excluded_labels = np.setdiff1d(np.arange(n_labels), kept_labels)\n",
        "    print(f'[SAGA][Fold {fold_id}] kept_labels={kept_labels.size}, excluded={excluded_labels.size}')\n",
        "    if kept_labels.size == 0:\n",
        "        print('[SAGA] No labels after pruning; abort fold')\n",
        "        break\n",
        "\n",
        "    # Fit vectorizers/scaler on train split only\n",
        "    title_vec = TfidfVectorizer(**title_vec_cfg)\n",
        "    code_vec  = TfidfVectorizer(**code_vec_cfg)\n",
        "    body_hash = HashingVectorizer(**body_hash_cfg)\n",
        "    char_hash = HashingVectorizer(**char_hash_cfg)\n",
        "    title_vec.fit(title_text.iloc[tr_idx])\n",
        "    code_vec.fit(code_text.iloc[tr_idx])\n",
        "    meta_scaler = StandardScaler(with_mean=False)\n",
        "    for b in batch_indices(tr_idx, BATCH_SIZE):\n",
        "        meta_scaler.partial_fit(meta_all[b])\n",
        "\n",
        "    # Transform helpers\n",
        "    def transform_block(idxs):\n",
        "        X_title = title_vec.transform(title_text.iloc[idxs]).astype(np.float32).multiply(title_alpha)\n",
        "        X_body  = body_hash.transform(body_text.iloc[idxs]).astype(np.float32)\n",
        "        X_char  = char_hash.transform((title_text.iloc[idxs] + ' ' + body_text.iloc[idxs])).astype(np.float32)\n",
        "        X_code  = code_vec.transform(code_text.iloc[idxs]).astype(np.float32)\n",
        "        X_meta  = sparse.csr_matrix(meta_scaler.transform(meta_all[idxs]), dtype=np.float32)\n",
        "        return sparse.hstack([X_title, X_body, X_char, X_code, X_meta], format='csr', dtype=np.float32)\n",
        "\n",
        "    # Build sparse matrices in batches to avoid peak RAM: collect then vstack\n",
        "    X_tr_parts, X_va_parts = [], []\n",
        "    for b in batch_indices(tr_idx, BATCH_SIZE):\n",
        "        X_tr_parts.append(transform_block(b))\n",
        "    for b in batch_indices(va_idx, BATCH_SIZE):\n",
        "        X_va_parts.append(transform_block(b))\n",
        "    X_tr = sparse.vstack(X_tr_parts, format='csr', dtype=np.float32)\n",
        "    X_va = sparse.vstack(X_va_parts, format='csr', dtype=np.float32)\n",
        "    del X_tr_parts, X_va_parts; gc.collect()\n",
        "\n",
        "    # Reduce label space to kept_labels for training; track excluded FNs during eval\n",
        "    Y_tr = Y_tr_full[:, kept_labels]\n",
        "    Y_va = Y_va_full[:, kept_labels]\n",
        "\n",
        "    # Model: OVR Logistic Regression with SAGA (fast config)\n",
        "    base = LogisticRegression(\n",
        "        solver='saga', penalty='l2', C=1.0, max_iter=1000, tol=1e-3,\n",
        "        class_weight=None, n_jobs=1, random_state=GLOBAL_SEED, verbose=0\n",
        "    )\n",
        "    clf = OneVsRestClassifier(base, n_jobs=16)\n",
        "    t_fit = time.time()\n",
        "    clf.fit(X_tr, Y_tr)\n",
        "    print(f'[SAGA][Fold {fold_id}] Fit time: {(time.time()-t_fit)/60:.2f} min')\n",
        "\n",
        "    # Predict probabilities on validation\n",
        "    try:\n",
        "        va_probs = clf.predict_proba(X_va)\n",
        "    except Exception:\n",
        "        from scipy.special import expit\n",
        "        scores = clf.decision_function(X_va)\n",
        "        va_probs = expit(scores)\n",
        "\n",
        "    # Update TP/FP/FN across thresholds (streaming by batches to limit memory)\n",
        "    bs = 4096\n",
        "    for s in range(0, va_probs.shape[0], bs):\n",
        "        e = min(va_probs.shape[0], s + bs)\n",
        "        P = va_probs[s:e].astype(np.float32, copy=False)\n",
        "        Y_true = Y_va[s:e]\n",
        "        update_counts_batch(Y_true.tocsr(), P, kept_labels, THS, tp_tot, fp_tot, fn_tot,\n",
        "                            tp_hi, fp_hi, fn_hi, hi_pos)\n",
        "    # FN addback for excluded labels\n",
        "    fn_excluded = int(Y_va_full[:, excluded_labels].sum()) if excluded_labels.size > 0 else 0\n",
        "    if fn_excluded > 0:\n",
        "        fn_tot += fn_excluded\n",
        "    print(f'[SAGA][Fold {fold_id}] Excluded FN addback: {fn_excluded}')\n",
        "\n",
        "    # Cleanup large mats before breaking (micro-pilot)\n",
        "    del X_tr, X_va, Y_tr, Y_va, Y_tr_full, Y_va_full, va_probs, clf\n",
        "    gc.collect()\n",
        "    print(f'[SAGA][Fold {fold_id}] Total fold time: {(time.time()-t0_fold)/60:.2f} min')\n",
        "    break\n",
        "\n",
        "# Compute global micro-F1 across thresholds and choose best\n",
        "f1s = [micro_f1(tp_tot[i], fp_tot[i], fn_tot[i]) for i in range(THS.size)]\n",
        "best_idx = int(np.argmax(f1s))\n",
        "best_thr = float(THS[best_idx])\n",
        "best_f1 = float(f1s[best_idx])\n",
        "print('[SAGA][OOF] Best micro-F1 = {:.5f} at thr = {:.3f}'.format(best_f1, best_thr))\n",
        "\n",
        "# Per-tag thresholds for high-support labels only (based on collected counts)\n",
        "per_tag_thr = np.full(n_labels, best_thr, dtype=np.float32)\n",
        "for k, lab in enumerate(hi_idx):\n",
        "    tps = tp_hi[k]; fps = fp_hi[k]; fns = fn_hi[k]\n",
        "    f1s_lab = np.array([micro_f1(tps[i], fps[i], fns[i]) for i in range(THS.size)], dtype=np.float32)\n",
        "    j = int(np.argmax(f1s_lab))\n",
        "    per_tag_thr[lab] = float(THS[j])\n",
        "print('[SAGA][OOF] Per-tag thresholds computed for', hi_idx.size, 'labels; others use global.')\n",
        "\n",
        "# Persist artifacts aligned with the pipeline\n",
        "pd.DataFrame({'label': labels_list}).to_csv('labels.csv', index=False)\n",
        "np.save('per_tag_thresholds.npy', per_tag_thr)\n",
        "np.save('global_threshold.npy', np.array([best_thr], dtype=np.float32))\n",
        "pd.DataFrame({'threshold': THS, 'f1': f1s}).to_csv('oof_global_f1_curve.csv', index=False)\n",
        "print('[SAGA][Artifacts] Saved labels.csv, per_tag_thresholds.npy, global_threshold.npy, oof_global_f1_curve.csv')\n",
        "print('[SAGA Baseline Pilot - FAST] DONE. Ready to scale folds and data after review.')\n",
        ""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAGA] Loaded caches: (5430775, 11) (603420, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAGA][Labels] #samples: 5430775 #labels: 41781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAGA][Pilot] Subsampled to 100000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAGA][Fold 0] Train=80041, Valid=19959\n[SAGA][Fold 0] kept_labels=307, excluded=41474\n"
          ]
        }
      ]
    },
    {
      "id": "0e97e710-36ce-4e02-af89-3b3272c6b80e",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment Log (Emergency Track + Micro-Pilots) \u2014 To Be Archived Before Cleanup\n",
        "\n",
        "This cell records the key outcomes from emergency fallback and pilot runs, per hygiene mandate. Cells 20\u201322 will be deleted next.\n",
        "\n",
        "- Cell 16 (Vectorized micro-pilot, early version):\n",
        "  - Config: 100k subset, 1-fold; TF-IDF (title/body/code/char) + meta; batch-major trainer without momentum.\n",
        "  - Result: OOF micro-F1 = 0.08454 @ thr=0.053. Notes: under-learning; very low mean probabilities; served as a diagnostic baseline.\n",
        "\n",
        "- Cell 17 (Batch-major custom trainer vNext-opt):\n",
        "  - Config: 100k subset, 1-fold; Title TF-IDF (alpha=3), Body/Char Hashing, Code TF-IDF, Meta; positive weighting; Nesterov momentum; grad clipping; prior-logit bias; EPOCHS=8, BATCH=8192.\n",
        "  - Result: OOF micro-F1 = 0.22046 @ thr=0.368. Notes: major architectural fix (batch-major across shards) validated learning dynamics; fastest scalable path per audits.\n",
        "\n",
        "- Cell 18 (SAGA micro-pilot FAST):\n",
        "  - Config: 100k subset, 1-fold; same channels as Cell 17; per-fold vectorizers; LogisticRegression(saga), OVR; threading guarded. \n",
        "  - Status: Began with kept_labels=307; run did not complete to metrics here (prior SAGA attempts in env stalled). Marked high-risk; retained for Track B2 time-boxed try only.\n",
        "\n",
        "- Cell 20 (Emergency OVR-SGD fallback, hashing-only):\n",
        "  - Config: 50k subset; top-1000 labels; word-hash(2^18)+char-wb(2^16); OneVsRest(SGDClassifier, max_iter=8). \n",
        "  - Holdout: micro-F1 = 0.25443 @ thr=0.100. ConvergenceWarnings due to low max_iter.\n",
        "  - Submission (Cell 21 fast inference K=200): LB = 0.20057.\n",
        "  - Submission (Cell 22 fast inference++ K=400): LB = 0.19910.\n",
        "  - Notes: Hashing-only simplification and aggressive label pruning led to poor LB; retained only as emergency record.\n",
        "\n",
        "Summary and Mandate Alignment:\n",
        "- Custom batch-major trainer (Cell 17) is the primary asset to scale (Track A). \n",
        "- SAGA is kept as a time-boxed, guarded experiment (Track B2) only; prior stalls noted.\n",
        "- Next actions (immediate):\n",
        "  1) Delete Cells 20\u201322 (emergency fallback) to restore a clean, production notebook.\n",
        "  2) Implement Track A scale-up: 300k subset, 3-fold iterative CV; feature caps (Title TF-IDF 150k, Body Hash 2^18, Char Hash 2^17, Code TF-IDF 80k), EPOCHS=5, BATCH=12,288, LR schedule (warmup + cosine). Persist kept_labels per fold and compute per-tag thresholds for support \u2265 50.\n",
        "  3) Track B1: Systematize OVR-SGD on same folds for OOF ensembling data.\n",
        "  4) Track B2: One last SAGA attempt with strict threading and TF-IDF on all channels; abort on stall.\n",
        "  5) Track C: Blend OOF from Track A with best of B, re-optimize thresholds, refit, and infer."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "c60cbf72-7688-4848-a656-9bbaa74fb8b8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Track A: Productionize custom batch-major trainer on 300k subset with 3-fold iterative CV\n",
        "# - Deterministic 300k sample (seed=1337). Persist fold indices for reuse across tracks.\n",
        "# - Leak-proof per-fold vectorizers (Title TF-IDF 150k, Body Hash 2^18, Char Hash 2^17, Code TF-IDF 80k) + meta scaler.\n",
        "# - Custom OVR logistic trainer: batch-major updates across label shards; Nesterov momentum, grad clipping,\n",
        "#   positive weighting (clipped [1,10]), prior-logit bias. EPOCHS=5, BATCH=12,288.\n",
        "# - LR schedule: Epoch 1 linear warmup 0.04->0.12; Epochs 2-5 cosine decay 0.12->0.02.\n",
        "# - OOF aggregation: global threshold sweep + per-tag thresholds for labels with OOF support >= 50.\n",
        "# - Artifacts: fold indices, kept_labels per fold, oof fold npz (va_idx, kept_labels, probs float16),\n",
        "#   global_threshold.npy, per_tag_thresholds.npy, oof_global_f1_curve.csv\n",
        "\n",
        "import os, gc, time, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
        "\n",
        "# Dependency guard for iterative stratification\n",
        "try:\n",
        "    from skmultilearn.model_selection import IterativeStratification\n",
        "except Exception:\n",
        "    import sys\n",
        "    from subprocess import run\n",
        "    run([sys.executable, '-m', 'pip', 'install', '--quiet', 'scikit-multilearn'])\n",
        "    from skmultilearn.model_selection import IterativeStratification\n",
        "\n",
        "GLOBAL_SEED = 1337\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "\n",
        "# Load parsed caches\n",
        "TRAIN_FULL = 'parsed_train_full.pkl'\n",
        "TEST_PKL   = 'parsed_test.pkl'\n",
        "assert os.path.exists(TRAIN_FULL) and os.path.exists(TEST_PKL), 'Missing caches: parsed_train_full.pkl / parsed_test.pkl'\n",
        "df_tr = pd.read_pickle(TRAIN_FULL)\n",
        "df_te = pd.read_pickle(TEST_PKL)\n",
        "print('[TrackA] Loaded caches:', df_tr.shape, df_te.shape)\n",
        "\n",
        "# Prepare subset (deterministic 300k)\n",
        "N_TARGET = 300_000\n",
        "n_all = len(df_tr)\n",
        "idx_all = np.arange(n_all)\n",
        "if n_all > N_TARGET:\n",
        "    rng = np.random.RandomState(GLOBAL_SEED)\n",
        "    idx_sub = rng.choice(n_all, size=N_TARGET, replace=False)\n",
        "    idx_sub.sort()\n",
        "else:\n",
        "    idx_sub = idx_all\n",
        "print(f'[TrackA] Using {idx_sub.size} rows for 3-fold CV')\n",
        "\n",
        "# Build channels on subset\n",
        "title_text = df_tr['title_norm'].iloc[idx_sub].fillna('').astype(str).reset_index(drop=True)\n",
        "body_text  = df_tr['body_text'].iloc[idx_sub].fillna('').astype(str).reset_index(drop=True)\n",
        "code_text  = df_tr['code_text'].iloc[idx_sub].fillna('').astype(str).reset_index(drop=True)\n",
        "meta_cols = ['title_len','body_len','code_len','num_block_code','num_urls','punct_density']\n",
        "meta_all = df_tr.loc[idx_sub, meta_cols].astype(np.float32).values\n",
        "y_lists = df_tr['Tags'].iloc[idx_sub].astype(str).apply(lambda s: s.split())\n",
        "mlb = MultiLabelBinarizer(sparse_output=True)\n",
        "Y_all = mlb.fit_transform(y_lists)\n",
        "labels_list = mlb.classes_.tolist()\n",
        "n_samples, n_labels = Y_all.shape\n",
        "print('[TrackA][Labels] #samples:', n_samples, '#labels:', n_labels)\n",
        "\n",
        "# Persist top-level subset indices for reproducibility\n",
        "np.save('subset300k_idx.npy', idx_sub)\n",
        "\n",
        "# CV setup: 3-fold iterative stratification\n",
        "mskf = IterativeStratification(n_splits=3, order=1)\n",
        "X_dummy = np.zeros((n_samples, 1))\n",
        "\n",
        "# Feature configurations\n",
        "title_vec_cfg = dict(analyzer='word', ngram_range=(1,3), min_df=3, max_df=0.95,\n",
        "                     max_features=150_000, sublinear_tf=True, dtype=np.float32)\n",
        "body_hash_cfg  = dict(analyzer='word', ngram_range=(1,3), n_features=2**18,\n",
        "                      alternate_sign=False, norm='l2', dtype=np.float32)\n",
        "char_hash_cfg  = dict(analyzer='char_wb', ngram_range=(3,6), n_features=2**17,\n",
        "                      alternate_sign=False, norm='l2', dtype=np.float32)\n",
        "code_vec_cfg   = dict(analyzer='word', ngram_range=(1,3), min_df=2, max_df=0.999,\n",
        "                      token_pattern=r'(?u)\\b\\w[\\w_\\+\\-\\#\\.]*\\b', max_features=80_000,\n",
        "                      sublinear_tf=True, dtype=np.float32)\n",
        "title_alpha = 3.0\n",
        "\n",
        "# Trainer hyperparameters\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 12_288\n",
        "L2 = 2e-4\n",
        "MU = 0.9\n",
        "COEF_BUDGET_BYTES = 4_000_000_000  # fp32 budget for W per shard sizing\n",
        "THS = np.linspace(0.05, 0.6, 12)\n",
        "\n",
        "# Label pruning within-train to stabilize (count as FN on valid for excluded)\n",
        "MIN_LABEL_FREQ_TRAIN = 20\n",
        "\n",
        "def sigmoid_stable(Z):\n",
        "    Z = np.clip(Z, -12.0, 12.0)\n",
        "    return 1.0 / (1.0 + np.exp(-Z))\n",
        "\n",
        "def prior_logit(p):\n",
        "    p = np.clip(p, 1e-6, 1-1e-6)\n",
        "    return np.log(p/(1.0-p))\n",
        "\n",
        "def micro_f1(tp, fp, fn):\n",
        "    denom = (2*tp + fp + fn)\n",
        "    return 0.0 if denom == 0 else (2.0*tp)/denom\n",
        "\n",
        "def batch_indices(idxs, bs):\n",
        "    for s in range(0, idxs.size, bs):\n",
        "        yield idxs[s:min(idxs.size, s+bs)]\n",
        "\n",
        "def lr_for_batch(epoch_idx, batch_idx, batches_per_epoch):\n",
        "    # Epochs indexed 0..EPOCHS-1\n",
        "    if epoch_idx == 0:\n",
        "        # linear warmup: 0.04 -> 0.12 across this epoch's batches\n",
        "        start, end = 0.04, 0.12\n",
        "        t = batch_idx / max(1, (batches_per_epoch - 1))\n",
        "        return start + (end - start) * t\n",
        "    else:\n",
        "        # cosine decay over epochs 2-5 from 0.12 -> 0.02 (4 epochs total)\n",
        "        # map epoch 1..4 (relative) -> progress in [0,1]\n",
        "        rel = (epoch_idx) / 4.0  # epoch_idx in {1,2,3,4}\n",
        "        lr_max, lr_min = 0.12, 0.02\n",
        "        cos_val = 0.5 * (1 + math.cos(math.pi * rel))\n",
        "        return lr_min + (lr_max - lr_min) * cos_val\n",
        "\n",
        "# Global support to decide high-support labels for per-tag thresholds\n",
        "global_support = np.asarray(Y_all.sum(axis=0)).ravel()\n",
        "hi_mask = (global_support >= 50)\n",
        "hi_idx = np.where(hi_mask)[0]\n",
        "hi_pos = {lab: pos for pos, lab in enumerate(hi_idx)}\n",
        "print('[TrackA] High-support labels (>=50):', hi_idx.size)\n",
        "\n",
        "# Aggregators\n",
        "tp_tot = np.zeros_like(THS, dtype=np.int64)\n",
        "fp_tot = np.zeros_like(THS, dtype=np.int64)\n",
        "fn_tot = np.zeros_like(THS, dtype=np.int64)\n",
        "tp_hi = np.zeros((hi_idx.size, THS.size), dtype=np.int64)\n",
        "fp_hi = np.zeros((hi_idx.size, THS.size), dtype=np.int64)\n",
        "fn_hi = np.zeros((hi_idx.size, THS.size), dtype=np.int64)\n",
        "\n",
        "# For OOF persistence (per-fold): write npz with va_idx, kept_label_idx, probs float16\n",
        "oof_fold_paths = []\n",
        "\n",
        "fold_num = 0\n",
        "for tr_idx, va_idx in mskf.split(X_dummy, Y_all):\n",
        "    t_fold = time.time()\n",
        "    print(f\"\\n[TrackA][Fold {fold_num}] Train={tr_idx.size}, Valid={va_idx.size}\")\n",
        "    # Persist fold indices (aligned to subset)\n",
        "    np.save(f'fold{fold_num}_train_idx.npy', tr_idx)\n",
        "    np.save(f'fold{fold_num}_valid_idx.npy', va_idx)\n",
        "\n",
        "    # Label pruning on train\n",
        "    sup_tr = np.asarray(Y_all[tr_idx].sum(axis=0)).ravel()\n",
        "    kept_labels = np.where(sup_tr >= MIN_LABEL_FREQ_TRAIN)[0]\n",
        "    excluded_labels = np.setdiff1d(np.arange(n_labels), kept_labels)\n",
        "    print(f'[TrackA][Fold {fold_num}] kept_labels={kept_labels.size}, excluded={excluded_labels.size}')\n",
        "    # Persist kept_labels for this fold\n",
        "    np.save(f'fold{fold_num}_kept_labels.npy', kept_labels)\n",
        "\n",
        "    # Per-label positive weights (fold-level)\n",
        "    p_tr = np.clip(sup_tr / float(tr_idx.size), 1e-6, 1-1e-6)\n",
        "    pos_weights_global = np.clip(1.0 / p_tr, 1.0, 10.0).astype(np.float32)\n",
        "\n",
        "    # Fit vectorizers/scaler on train split only\n",
        "    title_vec = TfidfVectorizer(**title_vec_cfg)\n",
        "    code_vec  = TfidfVectorizer(**code_vec_cfg)\n",
        "    body_hash = HashingVectorizer(**body_hash_cfg)\n",
        "    char_hash = HashingVectorizer(**char_hash_cfg)\n",
        "    title_vec.fit(title_text.iloc[tr_idx])\n",
        "    code_vec.fit(code_text.iloc[tr_idx])\n",
        "    meta_scaler = StandardScaler(with_mean=False)\n",
        "    for b in batch_indices(tr_idx, BATCH_SIZE):\n",
        "        meta_scaler.partial_fit(meta_all[b])\n",
        "\n",
        "    # Probe D and compute shard sizing\n",
        "    probe_idx = tr_idx[:min(BATCH_SIZE, tr_idx.size)]\n",
        "    Xt_probe = title_vec.transform(title_text.iloc[probe_idx]).astype(np.float32).multiply(title_alpha)\n",
        "    Xb_probe = body_hash.transform(body_text.iloc[probe_idx]).astype(np.float32)\n",
        "    Xc_probe = char_hash.transform((title_text.iloc[probe_idx] + ' ' + body_text.iloc[probe_idx])).astype(np.float32)\n",
        "    Xcode_probe = code_vec.transform(code_text.iloc[probe_idx]).astype(np.float32)\n",
        "    Xmeta_probe = sparse.csr_matrix(meta_scaler.transform(meta_all[probe_idx]), dtype=np.float32)\n",
        "    D = sparse.hstack([Xt_probe, Xb_probe, Xc_probe, Xcode_probe, Xmeta_probe], format='csr', dtype=np.float32).shape[1]\n",
        "    del Xt_probe, Xb_probe, Xc_probe, Xcode_probe, Xmeta_probe; gc.collect()\n",
        "    bytes_per_coef = 4  # fp32\n",
        "    shard_cap = max(1, int(COEF_BUDGET_BYTES // (bytes_per_coef * D)))\n",
        "    SHARD_SIZE = max(1, min(600, shard_cap))\n",
        "    shards = [kept_labels[i:i+SHARD_SIZE] for i in range(0, kept_labels.size, SHARD_SIZE)]\n",
        "    est_w_mb = (D * SHARD_SIZE * bytes_per_coef) / (1024**2)\n",
        "    print(f'[TrackA][Fold {fold_num}] D={D:,}, SHARD_SIZE={SHARD_SIZE}, #shards={len(shards)} (~{est_w_mb:.1f} MB per-shard W)')\n",
        "\n",
        "    # Initialize shard parameters (held concurrently)\n",
        "    shard_params = []  # (labels_idx, W, b, Vw, Vb)\n",
        "    n_tr = tr_idx.size\n",
        "    for shard in shards:\n",
        "        Lb = len(shard)\n",
        "        std = 1e-3 / math.sqrt(max(1, D))\n",
        "        W = (np.random.randn(D, Lb).astype(np.float32) * std).astype(np.float32)\n",
        "        pos = sup_tr[shard].astype(np.float64)\n",
        "        p = pos / float(n_tr)\n",
        "        b = prior_logit(p).astype(np.float32, copy=False)\n",
        "        Vw = np.zeros_like(W, dtype=np.float32)\n",
        "        Vb = np.zeros_like(b, dtype=np.float32)\n",
        "        shard_params.append((shard, W, b, Vw, Vb))\n",
        "\n",
        "    # Precompute batch index list\n",
        "    tr_batches = [b for b in batch_indices(tr_idx, BATCH_SIZE)]\n",
        "    batches_per_epoch = len(tr_batches)\n",
        "\n",
        "    # Precompute mapping from kept_labels -> hi_idx row positions (or -1 if not high-support)\n",
        "    kept_is_hi = np.array([int(lab) in hi_pos for lab in kept_labels], dtype=bool)\n",
        "    kept_hi_rows = np.array([hi_pos.get(int(lab), -1) for lab in kept_labels], dtype=np.int32)\n",
        "\n",
        "    # Training loop: batch-major with LR schedule\n",
        "    t_tr = time.time()\n",
        "    for ep in range(EPOCHS):\n",
        "        rng = np.random.RandomState(GLOBAL_SEED + ep)\n",
        "        rng.shuffle(tr_batches)\n",
        "        t_ep = time.time()\n",
        "        for bi, b_idx in enumerate(tr_batches):\n",
        "            LR = lr_for_batch(ep, bi, batches_per_epoch)\n",
        "            # Log LR for first and last batch per epoch for verification\n",
        "            if bi == 0 or bi == (batches_per_epoch - 1):\n",
        "                print(f'[TrackA][Fold {fold_num}] Epoch {ep+1}/{EPOCHS} batch {bi+1}/{batches_per_epoch} LR={LR:.5f}')\n",
        "            # Build features once\n",
        "            X_title = title_vec.transform(title_text.iloc[b_idx]).astype(np.float32).multiply(title_alpha)\n",
        "            X_body  = body_hash.transform(body_text.iloc[b_idx]).astype(np.float32)\n",
        "            X_char  = char_hash.transform((title_text.iloc[b_idx] + ' ' + body_text.iloc[b_idx])).astype(np.float32)\n",
        "            X_code  = code_vec.transform(code_text.iloc[b_idx]).astype(np.float32)\n",
        "            X_meta  = sparse.csr_matrix(meta_scaler.transform(meta_all[b_idx]), dtype=np.float32)\n",
        "            X_batch = sparse.hstack([X_title, X_body, X_char, X_code, X_meta], format='csr', dtype=np.float32)\n",
        "            bs = X_batch.shape[0]\n",
        "            for si, (shard, W, b, Vw, Vb) in enumerate(shard_params):\n",
        "                Y_b = Y_all[b_idx][:, shard].toarray().astype(np.float32, copy=False)\n",
        "                Z = X_batch @ W; Z += b\n",
        "                P = sigmoid_stable(Z)\n",
        "                E = (P - Y_b)\n",
        "                # Positive weighting\n",
        "                w_pos = pos_weights_global[shard][None, :]\n",
        "                Wmat = 1.0 + (w_pos - 1.0) * Y_b\n",
        "                E_weighted = E * Wmat\n",
        "                # Gradients\n",
        "                grad_W = (X_batch.T @ E_weighted) / float(bs)\n",
        "                if isinstance(grad_W, np.matrix):\n",
        "                    grad_W = np.asarray(grad_W)\n",
        "                grad_W = grad_W.astype(np.float32, copy=False) + (L2 * W)\n",
        "                grad_b = E_weighted.mean(axis=0).astype(np.float32, copy=False)\n",
        "                # Gradient clipping (per-column L2) and bias clip\n",
        "                col_norms = np.linalg.norm(grad_W, axis=0) + 1e-8\n",
        "                clip_scale = np.minimum(1.0, 10.0 / col_norms).astype(np.float32, copy=False)\n",
        "                grad_W *= clip_scale\n",
        "                grad_b = np.clip(grad_b, -1.0, 1.0)\n",
        "                # Nesterov momentum\n",
        "                Vw *= MU; Vw += LR * grad_W\n",
        "                Vb *= MU; Vb += LR * grad_b\n",
        "                W -= (MU * Vw + LR * grad_W)\n",
        "                b -= (MU * Vb + LR * grad_b)\n",
        "                shard_params[si] = (shard, W, b, Vw, Vb)\n",
        "            del X_title, X_body, X_char, X_code, X_meta, X_batch; gc.collect()\n",
        "        print(f'[TrackA][Fold {fold_num}] Epoch {ep+1}/{EPOCHS} time: {(time.time()-t_ep)/60:.2f} min')\n",
        "    print(f'[TrackA][Fold {fold_num}] Train total time: {(time.time()-t_tr)/60:.2f} min')\n",
        "\n",
        "    # Validation streaming: update global and per-tag counts, and persist OOF probs per fold\n",
        "    # We'll collect probs for kept_labels only and write float16 to disk with va indices and kept label indices\n",
        "    P_va_chunks = []\n",
        "    warned_mismatch = False\n",
        "    for b in batch_indices(va_idx, BATCH_SIZE):\n",
        "        # Normalize batch indices to avoid any dtype/shape surprises on last partial batch\n",
        "        b = np.asarray(b, dtype=np.int64)\n",
        "        b_list = b.tolist()\n",
        "        X_title = title_vec.transform(title_text.iloc[b_list]).astype(np.float32).multiply(title_alpha)\n",
        "        X_body  = body_hash.transform(body_text.iloc[b_list]).astype(np.float32)\n",
        "        X_char  = char_hash.transform((title_text.iloc[b_list] + ' ' + body_text.iloc[b_list])).astype(np.float32)\n",
        "        X_code  = code_vec.transform(code_text.iloc[b_list]).astype(np.float32)\n",
        "        X_meta  = sparse.csr_matrix(meta_scaler.transform(meta_all[b]), dtype=np.float32)\n",
        "        X_batch = sparse.hstack([X_title, X_body, X_char, X_code, X_meta], format='csr', dtype=np.float32)\n",
        "        # collect probs across shards for kept_labels in proper order\n",
        "        P_collect = []\n",
        "        col_order = []\n",
        "        for shard, W, b_sh, _, _ in shard_params:\n",
        "            Z = (X_batch @ W); Z += b_sh\n",
        "            P = sigmoid_stable(Z)\n",
        "            P_collect.append(P)\n",
        "            col_order.append(np.array(shard, dtype=np.int32))\n",
        "        if len(P_collect) == 1:\n",
        "            P_all = P_collect[0]\n",
        "            order = col_order[0]\n",
        "        else:\n",
        "            P_all = np.concatenate(P_collect, axis=1)\n",
        "            order = np.concatenate(col_order)\n",
        "        # reorder columns to match kept_labels sorted order\n",
        "        pos_map = {lab:i for i, lab in enumerate(order)}\n",
        "        kept_pos = np.array([pos_map[k] for k in kept_labels], dtype=np.int32)\n",
        "        P_kept = P_all[:, kept_pos].astype(np.float32, copy=False)\n",
        "        # Dense ground-truth for safe counting (avoid sparse shape inconsistencies)\n",
        "        Y_true_kept_dense = Y_all[b][:, kept_labels].toarray().astype(np.uint8, copy=False)\n",
        "        # SAFEGUARD: align row counts between predictions and truths for this batch\n",
        "        if P_kept.shape[0] != Y_true_kept_dense.shape[0]:\n",
        "            min_rows = min(P_kept.shape[0], Y_true_kept_dense.shape[0])\n",
        "            if not warned_mismatch:\n",
        "                print(f\"[TrackA][Fold {fold_num}] Warning: pred/true batch rows mismatch {P_kept.shape[0]} vs {Y_true_kept_dense.shape[0]} -> slicing to {min_rows}\")\n",
        "                warned_mismatch = True\n",
        "            if P_kept.shape[0] > min_rows:\n",
        "                P_kept = P_kept[:min_rows]\n",
        "            if Y_true_kept_dense.shape[0] > min_rows:\n",
        "                Y_true_kept_dense = Y_true_kept_dense[:min_rows]\n",
        "        # Update counts for all thresholds (global) and per-tag (vectorized for hi labels)\n",
        "        for ti, thr in enumerate(THS):\n",
        "            pred_bin = (P_kept >= thr).astype(np.uint8)\n",
        "            # Global counts\n",
        "            tp = int((pred_bin & Y_true_kept_dense).sum())\n",
        "            ppos = int(pred_bin.sum())\n",
        "            tpos = int(Y_true_kept_dense.sum())\n",
        "            tp_tot[ti] += tp\n",
        "            fp_tot[ti] += (ppos - tp)\n",
        "            fn_tot[ti] += (tpos - tp)\n",
        "            # Per-tag counts for high-support labels present in kept_labels (vectorized)\n",
        "            if hi_pos and kept_is_hi.any():\n",
        "                pred_hi = pred_bin[:, kept_is_hi]\n",
        "                true_hi = Y_true_kept_dense[:, kept_is_hi]\n",
        "                # Column-wise sums\n",
        "                tp_vec = (pred_hi & true_hi).sum(axis=0).astype(np.int64)\n",
        "                p_vec = pred_hi.sum(axis=0).astype(np.int64)\n",
        "                t_vec = true_hi.sum(axis=0).astype(np.int64)\n",
        "                rows = kept_hi_rows[kept_is_hi]\n",
        "                tp_hi[rows, ti] += np.asarray(tp_vec).ravel()\n",
        "                fp_hi[rows, ti] += (np.asarray(p_vec).ravel() - np.asarray(tp_vec).ravel())\n",
        "                fn_hi[rows, ti] += (np.asarray(t_vec).ravel() - np.asarray(tp_vec).ravel())\n",
        "        P_va_chunks.append(P_kept.astype(np.float16, copy=False))\n",
        "        del X_title, X_body, X_char, X_code, X_meta, X_batch, P_collect, col_order, P_all, order, kept_pos, P_kept, Y_true_kept_dense\n",
        "        gc.collect()\n",
        "\n",
        "    # FN addback for excluded labels to keep OOF honest\n",
        "    fn_excluded = int(Y_all[va_idx][:, excluded_labels].sum()) if excluded_labels.size > 0 else 0\n",
        "    if fn_excluded > 0:\n",
        "        fn_tot += fn_excluded\n",
        "    print(f'[TrackA][Fold {fold_num}] Excluded FN addback: {fn_excluded}')\n",
        "\n",
        "    # Persist fold OOF npz (compressed)\n",
        "    P_va = np.vstack(P_va_chunks) if len(P_va_chunks) > 0 else np.zeros((va_idx.size, kept_labels.size), dtype=np.float16)\n",
        "    np.savez_compressed(f'fold{fold_num}_oof_probs.npz', va_idx=va_idx, kept_labels=kept_labels, probs=P_va)\n",
        "    oof_fold_paths.append(f'fold{fold_num}_oof_probs.npz')\n",
        "    print(f'[TrackA][Fold {fold_num}] Saved OOF npz: probs shape {P_va.shape}')\n",
        "\n",
        "    # Cleanup heavy objects per fold\n",
        "    del shard_params, P_va_chunks, P_va\n",
        "    gc.collect()\n",
        "    print(f'[TrackA][Fold {fold_num}] Time: {(time.time()-t_fold)/60:.2f} min')\n",
        "    fold_num += 1\n",
        "\n",
        "# Optimize global threshold\n",
        "f1s = [micro_f1(tp_tot[i], fp_tot[i], fn_tot[i]) for i in range(THS.size)]\n",
        "best_idx = int(np.argmax(f1s))\n",
        "best_thr = float(THS[best_idx])\n",
        "best_f1 = float(f1s[best_idx])\n",
        "print('[TrackA][OOF] Global best micro-F1 = {:.5f} at thr = {:.3f}'.format(best_f1, best_thr))\n",
        "\n",
        "# Per-tag thresholds for high-support labels only (based on aggregated counts)\n",
        "per_tag_thr = np.full(n_labels, best_thr, dtype=np.float32)\n",
        "for k, lab in enumerate(hi_idx):\n",
        "    tps = tp_hi[k]; fps = fp_hi[k]; fns = fn_hi[k]\n",
        "    f1s_lab = np.array([micro_f1(tps[i], fps[i], fns[i]) for i in range(THS.size)], dtype=np.float32)\n",
        "    j = int(np.argmax(f1s_lab))\n",
        "    per_tag_thr[lab] = float(THS[j])\n",
        "print('[TrackA][OOF] Per-tag thresholds set for', hi_idx.size, 'labels; others use global.')\n",
        "\n",
        "# Persist global/per-tag thresholds and OOF curve\n",
        "pd.DataFrame({'label': labels_list}).to_csv('labels.csv', index=False)\n",
        "np.save('per_tag_thresholds.npy', per_tag_thr)\n",
        "np.save('global_threshold.npy', np.array([best_thr], dtype=np.float32))\n",
        "pd.DataFrame({'threshold': THS, 'f1': f1s}).to_csv('oof_global_f1_curve.csv', index=False)\n",
        "print('[TrackA][Artifacts] Saved labels.csv, per_tag_thresholds.npy, global_threshold.npy, oof_global_f1_curve.csv')\n",
        "\n",
        "# Log summary\n",
        "print('\\n[TrackA] SUMMARY:')\n",
        "print(' - Subset size:', n_samples)\n",
        "print(' - Labels:', n_labels)\n",
        "print(' - High-support labels (>=50):', hi_idx.size)\n",
        "print(' - Folds OOF files:', oof_fold_paths)\n",
        "print(' - Best global thr:', round(best_thr,3), 'OOF micro-F1:', round(best_f1,5))\n",
        "print('[TrackA] DONE. Proceed to Track B using persisted fold indices and kept_labels per fold.')\n",
        ""
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA] Loaded caches: (5430775, 11) (603420, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA] Using 300000 rows for 3-fold CV\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Labels] #samples: 300000 #labels: 26770\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA] High-support labels (>=50): 2147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[TrackA][Fold 0] Train=199994, Valid=100006\n[TrackA][Fold 0] kept_labels=3232, excluded=23538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 0] D=597,039, SHARD_SIZE=600, #shards=6 (~1366.5 MB per-shard W)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 0] Epoch 1/5 batch 1/17 LR=0.04000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 0] Epoch 1/5 batch 17/17 LR=0.12000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 0] Epoch 1/5 time: 30.16 min\n[TrackA][Fold 0] Epoch 2/5 batch 1/17 LR=0.10536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 0] Epoch 2/5 batch 17/17 LR=0.10536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 0] Epoch 2/5 time: 29.75 min\n[TrackA][Fold 0] Epoch 3/5 batch 1/17 LR=0.07000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 0] Epoch 3/5 batch 17/17 LR=0.07000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 0] Epoch 3/5 time: 30.96 min\n[TrackA][Fold 0] Epoch 4/5 batch 1/17 LR=0.03464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 0] Epoch 4/5 batch 17/17 LR=0.03464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 0] Epoch 4/5 time: 30.09 min\n[TrackA][Fold 0] Epoch 5/5 batch 1/17 LR=0.02000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 0] Epoch 5/5 batch 17/17 LR=0.02000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 0] Epoch 5/5 time: 31.33 min\n[TrackA][Fold 0] Train total time: 152.29 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 0] Excluded FN addback: 40455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 0] Saved OOF npz: probs shape (100006, 3232)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 0] Time: 162.53 min\n\n[TrackA][Fold 1] Train=200076, Valid=99924\n[TrackA][Fold 1] kept_labels=3227, excluded=23543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 1] D=596,170, SHARD_SIZE=600, #shards=6 (~1364.5 MB per-shard W)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 1] Epoch 1/5 batch 1/17 LR=0.04000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 1] Epoch 1/5 batch 17/17 LR=0.12000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 1] Epoch 1/5 time: 28.95 min\n[TrackA][Fold 1] Epoch 2/5 batch 1/17 LR=0.10536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 1] Epoch 2/5 batch 17/17 LR=0.10536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 1] Epoch 2/5 time: 31.27 min\n[TrackA][Fold 1] Epoch 3/5 batch 1/17 LR=0.07000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 1] Epoch 3/5 batch 17/17 LR=0.07000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 1] Epoch 3/5 time: 27.76 min\n[TrackA][Fold 1] Epoch 4/5 batch 1/17 LR=0.03464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 1] Epoch 4/5 batch 17/17 LR=0.03464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 1] Epoch 4/5 time: 29.13 min\n[TrackA][Fold 1] Epoch 5/5 batch 1/17 LR=0.02000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 1] Epoch 5/5 batch 17/17 LR=0.02000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 1] Epoch 5/5 time: 33.07 min\n[TrackA][Fold 1] Train total time: 150.17 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 1] Excluded FN addback: 40109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 1] Saved OOF npz: probs shape (99924, 3227)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 1] Time: 160.75 min\n\n[TrackA][Fold 2] Train=199930, Valid=100070\n[TrackA][Fold 2] kept_labels=3233, excluded=23537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 2] D=596,389, SHARD_SIZE=600, #shards=6 (~1365.0 MB per-shard W)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 2] Epoch 1/5 batch 1/17 LR=0.04000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 2] Epoch 1/5 batch 17/17 LR=0.12000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 2] Epoch 1/5 time: 31.76 min\n[TrackA][Fold 2] Epoch 2/5 batch 1/17 LR=0.10536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 2] Epoch 2/5 batch 17/17 LR=0.10536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 2] Epoch 2/5 time: 31.48 min\n[TrackA][Fold 2] Epoch 3/5 batch 1/17 LR=0.07000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 2] Epoch 3/5 batch 17/17 LR=0.07000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 2] Epoch 3/5 time: 30.35 min\n[TrackA][Fold 2] Epoch 4/5 batch 1/17 LR=0.03464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 2] Epoch 4/5 batch 17/17 LR=0.03464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 2] Epoch 4/5 time: 28.57 min\n[TrackA][Fold 2] Epoch 5/5 batch 1/17 LR=0.02000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 2] Epoch 5/5 batch 17/17 LR=0.02000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 2] Epoch 5/5 time: 33.28 min\n[TrackA][Fold 2] Train total time: 155.44 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 2] Excluded FN addback: 40372\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 2] Saved OOF npz: probs shape (100070, 3233)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][Fold 2] Time: 166.21 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA][OOF] Global best micro-F1 = 0.18089 at thr = 0.400\n[TrackA][OOF] Per-tag thresholds set for 2147 labels; others use global.\n[TrackA][Artifacts] Saved labels.csv, per_tag_thresholds.npy, global_threshold.npy, oof_global_f1_curve.csv\n\n[TrackA] SUMMARY:\n - Subset size: 300000\n - Labels: 26770\n - High-support labels (>=50): 2147\n - Folds OOF files: ['fold0_oof_probs.npz', 'fold1_oof_probs.npz', 'fold2_oof_probs.npz']\n - Best global thr: 0.4 OOF micro-F1: 0.18089\n[TrackA] DONE. Proceed to Track B using persisted fold indices and kept_labels per fold.\n"
          ]
        }
      ]
    },
    {
      "id": "e8e4c92c-e8fb-4b03-bad7-0cd87b07c72c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Track A Rerun (Fixed): Critical per-tag FN bugfix + underfitting fixes + safer shard sizing\n",
        "# - Reuse the EXACT same 300k subset and 3-fold splits from prior Track A (for comparability/ensembling)\n",
        "# - Bugfix: add per-tag FN for any high-support label excluded in a fold (was missing before)\n",
        "# - Learning: EPOCHS=8, more aggressive LR schedule (2-epoch warmup to 0.15, cosine decay to 0.03)\n",
        "# - Memory headroom: cap SHARD_SIZE at 400 (prev 600)\n",
        "# - Persist new OOF per-fold npz, thresholds, and curve (overwrites prior Track A artifacts)\n",
        "\n",
        "import os, gc, time, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
        "\n",
        "GLOBAL_SEED = 1337\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "\n",
        "# Load caches\n",
        "TRAIN_FULL = 'parsed_train_full.pkl'\n",
        "TEST_PKL   = 'parsed_test.pkl'\n",
        "assert os.path.exists(TRAIN_FULL) and os.path.exists(TEST_PKL), 'Missing caches: parsed_train_full.pkl / parsed_test.pkl'\n",
        "df_tr = pd.read_pickle(TRAIN_FULL)\n",
        "df_te = pd.read_pickle(TEST_PKL)\n",
        "print('[TrackA-FIX] Loaded caches:', df_tr.shape, df_te.shape)\n",
        "\n",
        "# Reuse the exact 300k subset indices if available; else recreate deterministically\n",
        "if os.path.exists('subset300k_idx.npy'):\n",
        "    idx_sub = np.load('subset300k_idx.npy')\n",
        "    print('[TrackA-FIX] Loaded subset indices from subset300k_idx.npy:', idx_sub.size)\n",
        "else:\n",
        "    N_TARGET = 300_000\n",
        "    n_all = len(df_tr)\n",
        "    if n_all > N_TARGET:\n",
        "        rng = np.random.RandomState(GLOBAL_SEED)\n",
        "        idx_sub = rng.choice(n_all, size=N_TARGET, replace=False)\n",
        "        idx_sub.sort()\n",
        "    else:\n",
        "        idx_sub = np.arange(n_all)\n",
        "    np.save('subset300k_idx.npy', idx_sub)\n",
        "    print('[TrackA-FIX] Saved subset indices; size=', idx_sub.size)\n",
        "\n",
        "# Build channels on subset\n",
        "title_text = df_tr['title_norm'].iloc[idx_sub].fillna('').astype(str).reset_index(drop=True)\n",
        "body_text  = df_tr['body_text'].iloc[idx_sub].fillna('').astype(str).reset_index(drop=True)\n",
        "code_text  = df_tr['code_text'].iloc[idx_sub].fillna('').astype(str).reset_index(drop=True)\n",
        "meta_cols = ['title_len','body_len','code_len','num_block_code','num_urls','punct_density']\n",
        "meta_all = df_tr.loc[idx_sub, meta_cols].astype(np.float32).values\n",
        "y_lists = df_tr['Tags'].iloc[idx_sub].astype(str).apply(lambda s: s.split())\n",
        "mlb = MultiLabelBinarizer(sparse_output=True)\n",
        "Y_all = mlb.fit_transform(y_lists)\n",
        "labels_list = mlb.classes_.tolist()\n",
        "n_samples, n_labels = Y_all.shape\n",
        "print('[TrackA-FIX][Labels] #samples:', n_samples, '#labels:', n_labels)\n",
        "\n",
        "# Attempt to reuse prior fold indices for perfect alignment with Track B; else derive them from disk\n",
        "fold_splits = []\n",
        "reuse_ok = True\n",
        "for i in range(3):\n",
        "    tr_path = f'fold{i}_train_idx.npy'\n",
        "    va_path = f'fold{i}_valid_idx.npy'\n",
        "    if os.path.exists(tr_path) and os.path.exists(va_path):\n",
        "        tr_idx = np.load(tr_path)\n",
        "        va_idx = np.load(va_path)\n",
        "        fold_splits.append((tr_idx, va_idx))\n",
        "    else:\n",
        "        reuse_ok = False\n",
        "        break\n",
        "if not reuse_ok or len(fold_splits) != 3:\n",
        "    # As a fallback, rebuild the same splits deterministically using iterative stratification\n",
        "    print('[TrackA-FIX] Prior fold indices not found/complete. Rebuilding deterministic 3-fold splits...')\n",
        "    try:\n",
        "        from skmultilearn.model_selection import IterativeStratification\n",
        "    except Exception:\n",
        "        import sys\n",
        "        from subprocess import run\n",
        "        run([sys.executable, '-m', 'pip', 'install', '--quiet', 'scikit-multilearn'])\n",
        "        from skmultilearn.model_selection import IterativeStratification\n",
        "    mskf = IterativeStratification(n_splits=3, order=1)\n",
        "    X_dummy = np.zeros((n_samples, 1))\n",
        "    for k, (tr_idx, va_idx) in enumerate(mskf.split(X_dummy, Y_all)):\n",
        "        np.save(f'fold{k}_train_idx.npy', tr_idx)\n",
        "        np.save(f'fold{k}_valid_idx.npy', va_idx)\n",
        "        fold_splits.append((tr_idx, va_idx))\n",
        "    print('[TrackA-FIX] Saved fold indices.')\n",
        "else:\n",
        "    print('[TrackA-FIX] Reusing existing 3-fold indices from disk.')\n",
        "\n",
        "# Feature configs (same channels; safer caps)\n",
        "title_vec_cfg = dict(analyzer='word', ngram_range=(1,3), min_df=3, max_df=0.95,\n",
        "                     max_features=150_000, sublinear_tf=True, dtype=np.float32)\n",
        "body_hash_cfg  = dict(analyzer='word', ngram_range=(1,3), n_features=2**18,\n",
        "                      alternate_sign=False, norm='l2', dtype=np.float32)\n",
        "char_hash_cfg  = dict(analyzer='char_wb', ngram_range=(3,6), n_features=2**17,\n",
        "                      alternate_sign=False, norm='l2', dtype=np.float32)\n",
        "code_vec_cfg   = dict(analyzer='word', ngram_range=(1,3), min_df=2, max_df=0.999,\n",
        "                      token_pattern=r'(?u)\\b\\w[\\w_\\+\\-\\#\\.]*\\b', max_features=80_000,\n",
        "                      sublinear_tf=True, dtype=np.float32)\n",
        "title_alpha = 3.0\n",
        "\n",
        "# Trainer hyperparameters (fixed per audit):\n",
        "EPOCHS = 8\n",
        "BATCH_SIZE = 12_288\n",
        "L2 = 2e-4\n",
        "MU = 0.9\n",
        "COEF_BUDGET_BYTES = 4_000_000_000  # fp32 budget for W per shard sizing\n",
        "MAX_SHARD_CAP = 400  # safer headroom (was 600)\n",
        "THS = np.linspace(0.05, 0.6, 12)\n",
        "MIN_LABEL_FREQ_TRAIN = 20\n",
        "\n",
        "def sigmoid_stable(Z):\n",
        "    Z = np.clip(Z, -12.0, 12.0)\n",
        "    return 1.0 / (1.0 + np.exp(-Z))\n",
        "\n",
        "def prior_logit(p):\n",
        "    p = np.clip(p, 1e-6, 1-1e-6)\n",
        "    return np.log(p/(1.0-p))\n",
        "\n",
        "def micro_f1(tp, fp, fn):\n",
        "    denom = (2*tp + fp + fn)\n",
        "    return 0.0 if denom == 0 else (2.0*tp)/denom\n",
        "\n",
        "def batch_indices(idxs, bs):\n",
        "    for s in range(0, idxs.size, bs):\n",
        "        yield idxs[s:min(idxs.size, s+bs)]\n",
        "\n",
        "def lr_for_batch(epoch_idx, batch_idx, batches_per_epoch):\n",
        "    # Aggressive schedule: 2-epoch warmup to 0.15, then cosine decay to 0.03 over epochs 2..7\n",
        "    if epoch_idx <= 1:\n",
        "        start, end = 0.06, 0.15\n",
        "        steps = max(1, (2 * batches_per_epoch - 1))\n",
        "        t = (epoch_idx * batches_per_epoch + batch_idx) / steps\n",
        "        return start + (end - start) * t\n",
        "    else:\n",
        "        lr_max, lr_min = 0.15, 0.03\n",
        "        # epochs 2..7 inclusive -> 6 epochs of decay\n",
        "        rel = (epoch_idx - 2 + batch_idx / max(1, (batches_per_epoch - 1))) / 6.0\n",
        "        rel = np.clip(rel, 0.0, 1.0)\n",
        "        cos_val = 0.5 * (1 + math.cos(math.pi * rel))\n",
        "        return lr_min + (lr_max - lr_min) * cos_val\n",
        "\n",
        "# Global support to decide high-support labels for per-tag thresholds\n",
        "global_support = np.asarray(Y_all.sum(axis=0)).ravel()\n",
        "hi_mask = (global_support >= 50)\n",
        "hi_idx = np.where(hi_mask)[0]\n",
        "hi_pos = {lab: pos for pos, lab in enumerate(hi_idx)}\n",
        "print('[TrackA-FIX] High-support labels (>=50):', hi_idx.size)\n",
        "\n",
        "# Aggregators\n",
        "tp_tot = np.zeros_like(THS, dtype=np.int64)\n",
        "fp_tot = np.zeros_like(THS, dtype=np.int64)\n",
        "fn_tot = np.zeros_like(THS, dtype=np.int64)\n",
        "tp_hi = np.zeros((hi_idx.size, THS.size), dtype=np.int64)\n",
        "fp_hi = np.zeros((hi_idx.size, THS.size), dtype=np.int64)\n",
        "fn_hi = np.zeros((hi_idx.size, THS.size), dtype=np.int64)\n",
        "\n",
        "oof_fold_paths = []\n",
        "for fold_num, (tr_idx, va_idx) in enumerate(fold_splits[1:], start=1):  # RESUME FROM FOLD 1\n",
        "    t_fold = time.time()\n",
        "    print(f\"\\n[TrackA-FIX][Fold {fold_num}] Train={tr_idx.size}, Valid={va_idx.size}\")\n",
        "    # Save indices again to confirm reuse\n",
        "    np.save(f'fold{fold_num}_train_idx.npy', tr_idx)\n",
        "    np.save(f'fold{fold_num}_valid_idx.npy', va_idx)\n",
        "\n",
        "    # Label pruning on train\n",
        "    Y_tr_full = Y_all[tr_idx]\n",
        "    Y_va_full = Y_all[va_idx]\n",
        "    sup_tr = np.asarray(Y_tr_full.sum(axis=0)).ravel()\n",
        "    kept_labels = np.where(sup_tr >= MIN_LABEL_FREQ_TRAIN)[0]\n",
        "    excluded_labels = np.setdiff1d(np.arange(n_labels), kept_labels)\n",
        "    np.save(f'fold{fold_num}_kept_labels.npy', kept_labels)\n",
        "    print(f'[TrackA-FIX][Fold {fold_num}] kept_labels={kept_labels.size}, excluded={excluded_labels.size}')\n",
        "\n",
        "    # Per-label positive weights (fold-level)\n",
        "    p_tr = np.clip(sup_tr / float(tr_idx.size), 1e-6, 1-1e-6)\n",
        "    pos_weights_global = np.clip(1.0 / p_tr, 1.0, 10.0).astype(np.float32)\n",
        "\n",
        "    # Fit vectorizers/scaler on train split only\n",
        "    title_vec = TfidfVectorizer(**title_vec_cfg)\n",
        "    code_vec  = TfidfVectorizer(**code_vec_cfg)\n",
        "    body_hash = HashingVectorizer(**body_hash_cfg)\n",
        "    char_hash = HashingVectorizer(**char_hash_cfg)\n",
        "    title_vec.fit(title_text.iloc[tr_idx])\n",
        "    code_vec.fit(code_text.iloc[tr_idx])\n",
        "    meta_scaler = StandardScaler(with_mean=False)\n",
        "    for b in batch_indices(tr_idx, BATCH_SIZE):\n",
        "        meta_scaler.partial_fit(meta_all[b])\n",
        "\n",
        "    # Probe D and compute shard sizing (with safer cap)\n",
        "    probe_idx = tr_idx[:min(BATCH_SIZE, tr_idx.size)]\n",
        "    Xt_probe = title_vec.transform(title_text.iloc[probe_idx]).astype(np.float32).multiply(title_alpha)\n",
        "    Xb_probe = body_hash.transform(body_text.iloc[probe_idx]).astype(np.float32)\n",
        "    Xc_probe = char_hash.transform((title_text.iloc[probe_idx] + ' ' + body_text.iloc[probe_idx])).astype(np.float32)\n",
        "    Xcode_probe = code_vec.transform(code_text.iloc[probe_idx]).astype(np.float32)\n",
        "    Xmeta_probe = sparse.csr_matrix(meta_scaler.transform(meta_all[probe_idx]), dtype=np.float32)\n",
        "    D = sparse.hstack([Xt_probe, Xb_probe, Xc_probe, Xcode_probe, Xmeta_probe], format='csr', dtype=np.float32).shape[1]\n",
        "    del Xt_probe, Xb_probe, Xc_probe, Xcode_probe, Xmeta_probe; gc.collect()\n",
        "    bytes_per_coef = 4\n",
        "    shard_cap = max(1, int(COEF_BUDGET_BYTES // (bytes_per_coef * D)))\n",
        "    SHARD_SIZE = max(1, min(MAX_SHARD_CAP, shard_cap))\n",
        "    shards = [kept_labels[i:i+SHARD_SIZE] for i in range(0, kept_labels.size, SHARD_SIZE)]\n",
        "    est_w_mb = (D * SHARD_SIZE * bytes_per_coef) / (1024**2)\n",
        "    print(f'[TrackA-FIX][Fold {fold_num}] D={D:,}, SHARD_SIZE={SHARD_SIZE}, #shards={len(shards)} (~{est_w_mb:.1f} MB per-shard W)')\n",
        "\n",
        "    # Initialize shard params\n",
        "    shard_params = []  # (labels_idx, W, b, Vw, Vb)\n",
        "    n_tr = tr_idx.size\n",
        "    for shard in shards:\n",
        "        Lb = len(shard)\n",
        "        std = 1e-3 / math.sqrt(max(1, D))\n",
        "        W = (np.random.randn(D, Lb).astype(np.float32) * std).astype(np.float32)\n",
        "        pos = sup_tr[shard].astype(np.float64)\n",
        "        p = pos / float(n_tr)\n",
        "        b = prior_logit(p).astype(np.float32, copy=False)\n",
        "        Vw = np.zeros_like(W, dtype=np.float32)\n",
        "        Vb = np.zeros_like(b, dtype=np.float32)\n",
        "        shard_params.append((shard, W, b, Vw, Vb))\n",
        "\n",
        "    # Precompute batches\n",
        "    tr_batches = [b for b in batch_indices(tr_idx, BATCH_SIZE)]\n",
        "    batches_per_epoch = len(tr_batches)\n",
        "\n",
        "    # Precompute mapping from kept_labels -> hi_idx row positions\n",
        "    kept_is_hi = np.array([int(lab) in hi_pos for lab in kept_labels], dtype=bool)\n",
        "    kept_hi_rows = np.array([hi_pos.get(int(lab), -1) for lab in kept_labels], dtype=np.int32)\n",
        "\n",
        "    # Training loop with new LR schedule\n",
        "    t_tr = time.time()\n",
        "    for ep in range(EPOCHS):\n",
        "        rng = np.random.RandomState(GLOBAL_SEED + ep)\n",
        "        rng.shuffle(tr_batches)\n",
        "        t_ep = time.time()\n",
        "        for bi, b_idx in enumerate(tr_batches):\n",
        "            LR = lr_for_batch(ep, bi, batches_per_epoch)\n",
        "            if bi == 0 or bi == (batches_per_epoch - 1):\n",
        "                print(f'[TrackA-FIX][Fold {fold_num}] Epoch {ep+1}/{EPOCHS} batch {bi+1}/{batches_per_epoch} LR={LR:.5f}')\n",
        "            X_title = title_vec.transform(title_text.iloc[b_idx]).astype(np.float32).multiply(title_alpha)\n",
        "            X_body  = body_hash.transform(body_text.iloc[b_idx]).astype(np.float32)\n",
        "            X_char  = char_hash.transform((title_text.iloc[b_idx] + ' ' + body_text.iloc[b_idx])).astype(np.float32)\n",
        "            X_code  = code_vec.transform(code_text.iloc[b_idx]).astype(np.float32)\n",
        "            X_meta  = sparse.csr_matrix(meta_scaler.transform(meta_all[b_idx]), dtype=np.float32)\n",
        "            X_batch = sparse.hstack([X_title, X_body, X_char, X_code, X_meta], format='csr', dtype=np.float32)\n",
        "            bs = X_batch.shape[0]\n",
        "            for si, (shard, W, b_sh, Vw, Vb) in enumerate(shard_params):\n",
        "                Y_b = Y_all[b_idx][:, shard].toarray().astype(np.float32, copy=False)\n",
        "                Z = X_batch @ W; Z += b_sh\n",
        "                P = sigmoid_stable(Z)\n",
        "                E = (P - Y_b)\n",
        "                # Positive weighting\n",
        "                w_pos = pos_weights_global[shard][None, :]\n",
        "                Wmat = 1.0 + (w_pos - 1.0) * Y_b\n",
        "                E_weighted = E * Wmat\n",
        "                # Gradients\n",
        "                grad_W = (X_batch.T @ E_weighted) / float(bs)\n",
        "                if isinstance(grad_W, np.matrix):\n",
        "                    grad_W = np.asarray(grad_W)\n",
        "                grad_W = grad_W.astype(np.float32, copy=False) + (L2 * W)\n",
        "                grad_b = E_weighted.mean(axis=0).astype(np.float32, copy=False)\n",
        "                # Gradient clipping\n",
        "                col_norms = np.linalg.norm(grad_W, axis=0) + 1e-8\n",
        "                clip_scale = np.minimum(1.0, 10.0 / col_norms).astype(np.float32, copy=False)\n",
        "                grad_W *= clip_scale\n",
        "                grad_b = np.clip(grad_b, -1.0, 1.0)\n",
        "                # Nesterov momentum update\n",
        "                Vw *= MU; Vw += LR * grad_W\n",
        "                Vb *= MU; Vb += LR * grad_b\n",
        "                W -= (MU * Vw + LR * grad_W)\n",
        "                b_sh -= (MU * Vb + LR * grad_b)\n",
        "                shard_params[si] = (shard, W, b_sh, Vw, Vb)\n",
        "            del X_title, X_body, X_char, X_code, X_meta, X_batch; gc.collect()\n",
        "        print(f'[TrackA-FIX][Fold {fold_num}] Epoch {ep+1}/{EPOCHS} time: {(time.time()-t_ep)/60:.2f} min')\n",
        "    print(f'[TrackA-FIX][Fold {fold_num}] Train total time: {(time.time()-t_tr)/60:.2f} min')\n",
        "\n",
        "    # Validation + OOF collection\n",
        "    P_va_chunks = []\n",
        "    warned_mismatch = False\n",
        "    for b in batch_indices(va_idx, BATCH_SIZE):\n",
        "        b = np.asarray(b, dtype=np.int64)\n",
        "        b_list = b.tolist()\n",
        "        X_title = title_vec.transform(title_text.iloc[b_list]).astype(np.float32).multiply(title_alpha)\n",
        "        X_body  = body_hash.transform(body_text.iloc[b_list]).astype(np.float32)\n",
        "        X_char  = char_hash.transform((title_text.iloc[b_list] + ' ' + body_text.iloc[b_list])).astype(np.float32)\n",
        "        X_code  = code_vec.transform(code_text.iloc[b_list]).astype(np.float32)\n",
        "        X_meta  = sparse.csr_matrix(meta_scaler.transform(meta_all[b]), dtype=np.float32)\n",
        "        X_batch = sparse.hstack([X_title, X_body, X_char, X_code, X_meta], format='csr', dtype=np.float32)\n",
        "        P_collect = []\n",
        "        col_order = []\n",
        "        for shard, W, b_sh, _, _ in shard_params:\n",
        "            Z = (X_batch @ W); Z += b_sh\n",
        "            P = sigmoid_stable(Z)\n",
        "            P_collect.append(P)\n",
        "            col_order.append(np.array(shard, dtype=np.int32))\n",
        "        if len(P_collect) == 1:\n",
        "            P_all = P_collect[0]; order = col_order[0]\n",
        "        else:\n",
        "            P_all = np.concatenate(P_collect, axis=1); order = np.concatenate(col_order)\n",
        "        pos_map = {lab:i for i, lab in enumerate(order)}\n",
        "        kept_pos = np.array([pos_map[k] for k in kept_labels], dtype=np.int32)\n",
        "        P_kept = P_all[:, kept_pos].astype(np.float32, copy=False)\n",
        "        Y_true_kept_dense = Y_all[b][:, kept_labels].toarray().astype(np.uint8, copy=False)\n",
        "        if P_kept.shape[0] != Y_true_kept_dense.shape[0]:\n",
        "            min_rows = min(P_kept.shape[0], Y_true_kept_dense.shape[0])\n",
        "            if not warned_mismatch:\n",
        "                print(f\"[TrackA-FIX][Fold {fold_num}] Warning: pred/true batch rows mismatch {P_kept.shape[0]} vs {Y_true_kept_dense.shape[0]} -> slicing to {min_rows}\")\n",
        "                warned_mismatch = True\n",
        "            P_kept = P_kept[:min_rows]\n",
        "            Y_true_kept_dense = Y_true_kept_dense[:min_rows]\n",
        "        for ti, thr in enumerate(THS):\n",
        "            pred_bin = (P_kept >= thr).astype(np.uint8)\n",
        "            tp = int((pred_bin & Y_true_kept_dense).sum())\n",
        "            ppos = int(pred_bin.sum())\n",
        "            tpos = int(Y_true_kept_dense.sum())\n",
        "            tp_tot[ti] += tp\n",
        "            fp_tot[ti] += (ppos - tp)\n",
        "            fn_tot[ti] += (tpos - tp)\n",
        "            if hi_pos and kept_is_hi.any():\n",
        "                pred_hi = pred_bin[:, kept_is_hi]\n",
        "                true_hi = Y_true_kept_dense[:, kept_is_hi]\n",
        "                tp_vec = (pred_hi & true_hi).sum(axis=0).astype(np.int64)\n",
        "                p_vec = pred_hi.sum(axis=0).astype(np.int64)\n",
        "                t_vec = true_hi.sum(axis=0).astype(np.int64)\n",
        "                rows = kept_hi_rows[kept_is_hi]\n",
        "                tp_hi[rows, ti] += np.asarray(tp_vec).ravel()\n",
        "                fp_hi[rows, ti] += (np.asarray(p_vec).ravel() - np.asarray(tp_vec).ravel())\n",
        "                fn_hi[rows, ti] += (np.asarray(t_vec).ravel() - np.asarray(tp_vec).ravel())\n",
        "        P_va_chunks.append(P_kept.astype(np.float16, copy=False))\n",
        "        del X_title, X_body, X_char, X_code, X_meta, X_batch, P_collect, col_order, P_all, order, kept_pos, P_kept, Y_true_kept_dense\n",
        "        gc.collect()\n",
        "\n",
        "    # FN addback for globally excluded labels (global counts)\n",
        "    fn_excluded = int(Y_va_full[:, excluded_labels].sum()) if excluded_labels.size > 0 else 0\n",
        "    if fn_excluded > 0:\n",
        "        fn_tot += fn_excluded\n",
        "    print(f'[TrackA-FIX][Fold {fold_num}] Excluded FN addback (global): {fn_excluded}')\n",
        "\n",
        "    # CRITICAL BUGFIX: per-tag FN addback for high-support labels excluded from this fold\n",
        "    hi_missing = np.array([lab for lab in hi_idx if lab not in set(kept_labels)], dtype=np.int32)\n",
        "    add_count_total = 0\n",
        "    if hi_missing.size > 0:\n",
        "        # For each missing high-support label, all its validation positives are FN across ALL thresholds\n",
        "        for lab in hi_missing:\n",
        "            row = hi_pos[int(lab)]\n",
        "            val_pos = int(Y_va_full[:, lab].sum())\n",
        "            if val_pos > 0:\n",
        "                fn_hi[row, :] += val_pos\n",
        "                add_count_total += val_pos\n",
        "        print(f'[TrackA-FIX][Fold {fold_num}] Per-tag FN bugfix: missing_hi_labels={hi_missing.size}, added_FN_total={add_count_total}')\n",
        "    else:\n",
        "        print(f'[TrackA-FIX][Fold {fold_num}] Per-tag FN bugfix: no high-support labels missing in this fold.')\n",
        "\n",
        "    # Persist fold OOF npz\n",
        "    P_va = np.vstack(P_va_chunks) if len(P_va_chunks) > 0 else np.zeros((va_idx.size, kept_labels.size), dtype=np.float16)\n",
        "    np.savez_compressed(f'fold{fold_num}_oof_probs.npz', va_idx=va_idx, kept_labels=kept_labels, probs=P_va)\n",
        "    oof_fold_paths.append(f'fold{fold_num}_oof_probs.npz')\n",
        "    print(f'[TrackA-FIX][Fold {fold_num}] Saved OOF npz: probs shape {P_va.shape}')\n",
        "\n",
        "    # Cleanup per fold\n",
        "    del shard_params, P_va_chunks, P_va, Y_tr_full, Y_va_full\n",
        "    gc.collect()\n",
        "    print(f'[TrackA-FIX][Fold {fold_num}] Time: {(time.time()-t_fold)/60:.2f} min')\n",
        "\n",
        "# Optimize global threshold\n",
        "f1s = [micro_f1(tp_tot[i], fp_tot[i], fn_tot[i]) for i in range(THS.size)]\n",
        "best_idx = int(np.argmax(f1s))\n",
        "best_thr = float(THS[best_idx])\n",
        "best_f1 = float(f1s[best_idx])\n",
        "print('[TrackA-FIX][OOF] Global best micro-F1 = {:.5f} at thr = {:.3f}'.format(best_f1, best_thr))\n",
        "\n",
        "# Per-tag thresholds for high-support labels only\n",
        "per_tag_thr = np.full(n_labels, best_thr, dtype=np.float32)\n",
        "for k, lab in enumerate(hi_idx):\n",
        "    tps = tp_hi[k]; fps = fp_hi[k]; fns = fn_hi[k]\n",
        "    f1s_lab = np.array([micro_f1(tps[i], fps[i], fns[i]) for i in range(THS.size)], dtype=np.float32)\n",
        "    j = int(np.argmax(f1s_lab))\n",
        "    per_tag_thr[lab] = float(THS[j])\n",
        "print('[TrackA-FIX][OOF] Per-tag thresholds set for', hi_idx.size, 'labels; others use global.')\n",
        "\n",
        "# Persist artifacts\n",
        "pd.DataFrame({'label': labels_list}).to_csv('labels.csv', index=False)\n",
        "np.save('per_tag_thresholds.npy', per_tag_thr)\n",
        "np.save('global_threshold.npy', np.array([best_thr], dtype=np.float32))\n",
        "pd.DataFrame({'threshold': THS, 'f1': f1s}).to_csv('oof_global_f1_curve.csv', index=False)\n",
        "print('[TrackA-FIX][Artifacts] Saved labels.csv, per_tag_thresholds.npy, global_threshold.npy, oof_global_f1_curve.csv')\n",
        "\n",
        "print('\\n[TrackA-FIX] SUMMARY:')\n",
        "print(' - Subset size:', n_samples)\n",
        "print(' - Labels:', n_labels)\n",
        "print(' - High-support labels (>=50):', hi_idx.size)\n",
        "print(' - Folds OOF files:', oof_fold_paths)\n",
        "print(' - Best global thr:', round(best_thr,3), 'OOF micro-F1:', round(best_f1,5))\n",
        "print('[TrackA-FIX] DONE. Next: Start Track B1 (OVR-SGD) on the same folds for ensembling.')\n",
        ""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX] Loaded caches: (5430775, 11) (603420, 10)\n[TrackA-FIX] Loaded subset indices from subset300k_idx.npy: 300000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Labels] #samples: 300000 #labels: 26770\n[TrackA-FIX] Reusing existing 3-fold indices from disk.\n[TrackA-FIX] High-support labels (>=50): 2147\n\n[TrackA-FIX][Fold 0] Train=199994, Valid=100006\n[TrackA-FIX][Fold 0] kept_labels=3232, excluded=23538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 0] D=597,039, SHARD_SIZE=400, #shards=9 (~911.0 MB per-shard W)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 0] Epoch 1/8 batch 1/17 LR=0.06000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 0] Epoch 1/8 batch 17/17 LR=0.10364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 0] Epoch 1/8 time: 35.87 min\n[TrackA-FIX][Fold 0] Epoch 2/8 batch 1/17 LR=0.10636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 0] Epoch 2/8 batch 17/17 LR=0.15000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 0] Epoch 2/8 time: 34.93 min\n[TrackA-FIX][Fold 0] Epoch 3/8 batch 1/17 LR=0.15000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 0] Epoch 3/8 batch 17/17 LR=0.14196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 0] Epoch 3/8 time: 34.50 min\n[TrackA-FIX][Fold 0] Epoch 4/8 batch 1/17 LR=0.14196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 0] Epoch 4/8 batch 17/17 LR=0.12000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 0] Epoch 4/8 time: 35.91 min\n[TrackA-FIX][Fold 0] Epoch 5/8 batch 1/17 LR=0.12000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 0] Epoch 5/8 batch 17/17 LR=0.09000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 0] Epoch 5/8 time: 32.78 min\n[TrackA-FIX][Fold 0] Epoch 6/8 batch 1/17 LR=0.09000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 0] Epoch 6/8 batch 17/17 LR=0.06000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 0] Epoch 6/8 time: 34.36 min\n[TrackA-FIX][Fold 0] Epoch 7/8 batch 1/17 LR=0.06000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 0] Epoch 7/8 batch 17/17 LR=0.03804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 0] Epoch 7/8 time: 37.41 min\n[TrackA-FIX][Fold 0] Epoch 8/8 batch 1/17 LR=0.03804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 0] Epoch 8/8 batch 17/17 LR=0.03000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 0] Epoch 8/8 time: 36.62 min\n[TrackA-FIX][Fold 0] Train total time: 282.38 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 0] Excluded FN addback (global): 40455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 0] Per-tag FN bugfix: no high-support labels missing in this fold.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 0] Saved OOF npz: probs shape (100006, 3232)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 0] Time: 294.08 min\n\n[TrackA-FIX][Fold 1] Train=200076, Valid=99924\n[TrackA-FIX][Fold 1] kept_labels=3227, excluded=23543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 1] D=596,170, SHARD_SIZE=400, #shards=9 (~909.7 MB per-shard W)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrackA-FIX][Fold 1] Epoch 1/8 batch 1/17 LR=0.06000\n"
          ]
        }
      ]
    },
    {
      "id": "41fb11d8-5868-4be9-b4a1-d979988e3f8b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Track B1: OVR-SGD baseline on EXACT same 300k subset and 3-fold splits as Track A\n",
        "# - Uses persisted fold{i}_train_idx.npy, fold{i}_valid_idx.npy, and fold{i}_kept_labels.npy\n",
        "# - Channels match Track A: Title TF-IDF (alpha=3), Body Hash 2^18, Char Hash 2^17, Code TF-IDF 80k, Meta scaler\n",
        "# - Model: OneVsRest(SGDClassifier, loss='log_loss', early_stopping) with reasonable iterations\n",
        "# - Outputs: fold{i}_oof_probs_sgd.npz (va_idx, kept_labels, probs float16), plus global OOF micro-F1 and thresholds\n",
        "\n",
        "import os, gc, time, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "GLOBAL_SEED = 1337\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "\n",
        "# Load caches and subset indices\n",
        "TRAIN_FULL = 'parsed_train_full.pkl'\n",
        "TEST_PKL   = 'parsed_test.pkl'\n",
        "assert os.path.exists(TRAIN_FULL) and os.path.exists(TEST_PKL), 'Missing caches parsed_train_full.pkl / parsed_test.pkl'\n",
        "assert os.path.exists('subset300k_idx.npy'), 'subset300k_idx.npy not found. Run Track A first.'\n",
        "df_tr = pd.read_pickle(TRAIN_FULL)\n",
        "idx_sub = np.load('subset300k_idx.npy')\n",
        "print('[TrackB1] Loaded caches:', df_tr.shape, 'subset size=', idx_sub.size)\n",
        "\n",
        "# Build channels on subset (identical to Track A)\n",
        "title_text = df_tr['title_norm'].iloc[idx_sub].fillna('').astype(str).reset_index(drop=True)\n",
        "body_text  = df_tr['body_text'].iloc[idx_sub].fillna('').astype(str).reset_index(drop=True)\n",
        "code_text  = df_tr['code_text'].iloc[idx_sub].fillna('').astype(str).reset_index(drop=True)\n",
        "meta_cols = ['title_len','body_len','code_len','num_block_code','num_urls','punct_density']\n",
        "meta_all = df_tr.loc[idx_sub, meta_cols].astype(np.float32).values\n",
        "y_lists = df_tr['Tags'].iloc[idx_sub].astype(str).apply(lambda s: s.split())\n",
        "mlb = MultiLabelBinarizer(sparse_output=True)\n",
        "Y_all = mlb.fit_transform(y_lists)\n",
        "labels_list = mlb.classes_.tolist()\n",
        "n_samples, n_labels = Y_all.shape\n",
        "print('[TrackB1][Labels] #samples:', n_samples, '#labels:', n_labels)\n",
        "\n",
        "# Feature configurations (match Track A)\n",
        "title_vec_cfg = dict(analyzer='word', ngram_range=(1,3), min_df=3, max_df=0.95,\n",
        "                     max_features=150_000, sublinear_tf=True, dtype=np.float32)\n",
        "body_hash_cfg  = dict(analyzer='word', ngram_range=(1,3), n_features=2**18,\n",
        "                      alternate_sign=False, norm='l2', dtype=np.float32)\n",
        "char_hash_cfg  = dict(analyzer='char_wb', ngram_range=(3,6), n_features=2**17,\n",
        "                      alternate_sign=False, norm='l2', dtype=np.float32)\n",
        "code_vec_cfg   = dict(analyzer='word', ngram_range=(1,3), min_df=2, max_df=0.999,\n",
        "                      token_pattern=r'(?u)\\b\\w[\\w_\\+\\-\\#\\.]*\\b', max_features=80_000,\n",
        "                      sublinear_tf=True, dtype=np.float32)\n",
        "title_alpha = 3.0\n",
        "\n",
        "BATCH_SIZE = 8192\n",
        "THS = np.linspace(0.05, 0.6, 12)\n",
        "\n",
        "def batch_indices(idxs, bs):\n",
        "    for s in range(0, idxs.size, bs):\n",
        "        yield idxs[s:min(idxs.size, s+bs)]\n",
        "\n",
        "def micro_f1(tp, fp, fn):\n",
        "    denom = (2*tp + fp + fn)\n",
        "    return 0.0 if denom == 0 else (2.0*tp)/denom\n",
        "\n",
        "# Global high-support mask (for per-tag thresholds)\n",
        "global_support = np.asarray(Y_all.sum(axis=0)).ravel()\n",
        "hi_mask = (global_support >= 50)\n",
        "hi_idx = np.where(hi_mask)[0]\n",
        "hi_pos = {lab: pos for pos, lab in enumerate(hi_idx)}\n",
        "print('[TrackB1] High-support labels (>=50):', hi_idx.size)\n",
        "\n",
        "tp_tot = np.zeros_like(THS, dtype=np.int64)\n",
        "fp_tot = np.zeros_like(THS, dtype=np.int64)\n",
        "fn_tot = np.zeros_like(THS, dtype=np.int64)\n",
        "tp_hi = np.zeros((hi_idx.size, THS.size), dtype=np.int64)\n",
        "fp_hi = np.zeros((hi_idx.size, THS.size), dtype=np.int64)\n",
        "fn_hi = np.zeros((hi_idx.size, THS.size), dtype=np.int64)\n",
        "\n",
        "fold_oof_paths = []\n",
        "for fold_num in range(3):\n",
        "    tr_path = f'fold{fold_num}_train_idx.npy'\n",
        "    va_path = f'fold{fold_num}_valid_idx.npy'\n",
        "    kept_path = f'fold{fold_num}_kept_labels.npy'\n",
        "    assert os.path.exists(tr_path) and os.path.exists(va_path) and os.path.exists(kept_path), f'Missing fold artifacts for fold {fold_num}'\n",
        "    tr_idx = np.load(tr_path)\n",
        "    va_idx = np.load(va_path)\n",
        "    kept_labels = np.load(kept_path)\n",
        "    print(f\"\\n[TrackB1][Fold {fold_num}] Train={tr_idx.size}, Valid={va_idx.size}, kept_labels={kept_labels.size}\")\n",
        "\n",
        "    # Fit vectorizers/scaler on train split only\n",
        "    title_vec = TfidfVectorizer(**title_vec_cfg)\n",
        "    code_vec  = TfidfVectorizer(**code_vec_cfg)\n",
        "    body_hash = HashingVectorizer(**body_hash_cfg)\n",
        "    char_hash = HashingVectorizer(**char_hash_cfg)\n",
        "    title_vec.fit(title_text.iloc[tr_idx])\n",
        "    code_vec.fit(code_text.iloc[tr_idx])\n",
        "    meta_scaler = StandardScaler(with_mean=False)\n",
        "    for b in batch_indices(tr_idx, BATCH_SIZE):\n",
        "        meta_scaler.partial_fit(meta_all[b])\n",
        "\n",
        "    # Transform helper (batch)\n",
        "    def transform_block(idxs):\n",
        "        idxs_list = np.asarray(idxs, dtype=np.int64).tolist()\n",
        "        X_title = title_vec.transform(title_text.iloc[idxs_list]).astype(np.float32).multiply(title_alpha)\n",
        "        X_body  = body_hash.transform(body_text.iloc[idxs_list]).astype(np.float32)\n",
        "        X_char  = char_hash.transform((title_text.iloc[idxs_list] + ' ' + body_text.iloc[idxs_list])).astype(np.float32)\n",
        "        X_code  = code_vec.transform(code_text.iloc[idxs_list]).astype(np.float32)\n",
        "        X_meta  = sparse.csr_matrix(meta_scaler.transform(meta_all[idxs]), dtype=np.float32)\n",
        "        return sparse.hstack([X_title, X_body, X_char, X_code, X_meta], format='csr', dtype=np.float32)\n",
        "\n",
        "    # Build train/valid matrices in chunks to control memory\n",
        "    X_tr_parts, X_va_parts = [], []\n",
        "    for b in batch_indices(tr_idx, BATCH_SIZE):\n",
        "        X_tr_parts.append(transform_block(b))\n",
        "    for b in batch_indices(va_idx, BATCH_SIZE):\n",
        "        X_va_parts.append(transform_block(b))\n",
        "    X_tr = sparse.vstack(X_tr_parts, format='csr', dtype=np.float32)\n",
        "    X_va = sparse.vstack(X_va_parts, format='csr', dtype=np.float32)\n",
        "    del X_tr_parts, X_va_parts; gc.collect()\n",
        "\n",
        "    Y_tr = Y_all[tr_idx][:, kept_labels]\n",
        "    Y_va = Y_all[va_idx][:, kept_labels]\n",
        "\n",
        "    # Model: OVR-SGD (logistic loss); keep jobs modest to avoid contention\n",
        "    base = SGDClassifier(loss='log_loss', penalty='l2', alpha=1e-4, max_iter=120, tol=1e-3,\n",
        "                         early_stopping=True, n_iter_no_change=5, learning_rate='optimal',\n",
        "                         random_state=GLOBAL_SEED)\n",
        "    clf = OneVsRestClassifier(base, n_jobs=8)\n",
        "    t_fit = time.time()\n",
        "    clf.fit(X_tr, Y_tr)\n",
        "    print(f'[TrackB1][Fold {fold_num}] Fit time: {(time.time()-t_fit)/60:.2f} min')\n",
        "\n",
        "    # Predict probabilities on validation\n",
        "    try:\n",
        "        va_probs = clf.predict_proba(X_va)\n",
        "    except Exception:\n",
        "        from scipy.special import expit\n",
        "        scores = clf.decision_function(X_va)\n",
        "        va_probs = expit(scores)\n",
        "    va_probs = va_probs.astype(np.float32, copy=False)\n",
        "\n",
        "    # Update OOF counts by threshold (streamed by blocks) and save per-fold npz for ensembling\n",
        "    bs = 4096\n",
        "    for s in range(0, va_probs.shape[0], bs):\n",
        "        e = min(va_probs.shape[0], s + bs)\n",
        "        P = va_probs[s:e]\n",
        "        Y_true = Y_va[s:e]\n",
        "        # global counts\n",
        "        for ti, thr in enumerate(THS):\n",
        "            pred_bin = (P >= thr).astype(np.uint8)\n",
        "            tp = int(sparse.csr_matrix(pred_bin, dtype=np.uint8).multiply(Y_true).sum())\n",
        "            ppos = int(pred_bin.sum())\n",
        "            tpos = int(Y_true.sum())\n",
        "            tp_tot[ti] += tp\n",
        "            fp_tot[ti] += (ppos - tp)\n",
        "            fn_tot[ti] += (tpos - tp)\n",
        "        # per-tag for high-support labels that are in kept_labels\n",
        "        kept_set = set(int(x) for x in kept_labels.tolist())\n",
        "        hi_in_fold = [lab for lab in hi_idx if lab in kept_set]\n",
        "        if len(hi_in_fold) > 0:\n",
        "            col_map = {lab: j for j, lab in enumerate(kept_labels.tolist())}\n",
        "            cols = np.array([col_map[lab] for lab in hi_in_fold], dtype=np.int32)\n",
        "            Y_true_hi = Y_true[:, cols]\n",
        "            P_hi = P[:, cols]\n",
        "            rows = np.array([hi_pos[lab] for lab in hi_in_fold], dtype=np.int32)\n",
        "            for ti, thr in enumerate(THS):\n",
        "                pred_bin_hi = (P_hi >= thr).astype(np.uint8)\n",
        "                pred_csr = sparse.csr_matrix(pred_bin_hi, dtype=np.uint8)\n",
        "                tp_vec = np.asarray(pred_csr.multiply(Y_true_hi).sum(axis=0)).ravel().astype(np.int64)\n",
        "                p_vec = pred_bin_hi.sum(axis=0).astype(np.int64)\n",
        "                t_vec = np.asarray(Y_true_hi.sum(axis=0)).ravel().astype(np.int64)\n",
        "                tp_hi[rows, ti] += tp_vec\n",
        "                fp_hi[rows, ti] += (p_vec - tp_vec)\n",
        "                fn_hi[rows, ti] += (t_vec - tp_vec)\n",
        "\n",
        "    # FN addback for labels excluded in this fold (global counts only)\n",
        "    excluded_labels = np.setdiff1d(np.arange(n_labels), kept_labels)\n",
        "    fn_excluded = int(Y_all[va_idx][:, excluded_labels].sum()) if excluded_labels.size > 0 else 0\n",
        "    if fn_excluded > 0:\n",
        "        fn_tot += fn_excluded\n",
        "    print(f'[TrackB1][Fold {fold_num}] Excluded FN addback (global): {fn_excluded}')\n",
        "\n",
        "    # Persist per-fold OOF probs for alignment with Track A\n",
        "    np.savez_compressed(f'fold{fold_num}_oof_probs_sgd.npz', va_idx=va_idx, kept_labels=kept_labels, probs=va_probs.astype(np.float16))\n",
        "    fold_oof_paths.append(f'fold{fold_num}_oof_probs_sgd.npz')\n",
        "    print(f'[TrackB1][Fold {fold_num}] Saved OOF npz (SGD): probs shape {va_probs.shape}')\n",
        "\n",
        "    # Cleanup\n",
        "    del X_tr, X_va, Y_tr, Y_va, va_probs, clf, title_vec, code_vec, body_hash, char_hash, meta_scaler\n",
        "    gc.collect()\n",
        "\n",
        "# Optimize global threshold and compute per-tag thresholds (high-support only)\n",
        "f1s = [micro_f1(tp_tot[i], fp_tot[i], fn_tot[i]) for i in range(THS.size)]\n",
        "best_idx = int(np.argmax(f1s))\n",
        "best_thr = float(THS[best_idx])\n",
        "best_f1 = float(f1s[best_idx])\n",
        "print('[TrackB1][OOF] Global best micro-F1 (SGD) = {:.5f} at thr = {:.3f}'.format(best_f1, best_thr))\n",
        "\n",
        "per_tag_thr_sgd = np.full(n_labels, best_thr, dtype=np.float32)\n",
        "for k, lab in enumerate(hi_idx):\n",
        "    tps = tp_hi[k]; fps = fp_hi[k]; fns = fn_hi[k]\n",
        "    f1s_lab = np.array([micro_f1(tps[i], fps[i], fns[i]) for i in range(THS.size)], dtype=np.float32)\n",
        "    j = int(np.argmax(f1s_lab))\n",
        "    per_tag_thr_sgd[lab] = float(THS[j])\n",
        "print('[TrackB1][OOF] Per-tag thresholds computed for', hi_idx.size, 'labels (SGD).')\n",
        "\n",
        "# Persist Track B1 artifacts (distinct filenames)\n",
        "pd.DataFrame({'label': labels_list}).to_csv('labels.csv', index=False)\n",
        "np.save('per_tag_thresholds_sgd.npy', per_tag_thr_sgd)\n",
        "np.save('global_threshold_sgd.npy', np.array([best_thr], dtype=np.float32))\n",
        "pd.DataFrame({'threshold': THS, 'f1': f1s}).to_csv('oof_global_f1_curve_sgd.csv', index=False)\n",
        "print('[TrackB1][Artifacts] Saved per_tag_thresholds_sgd.npy, global_threshold_sgd.npy, oof_global_f1_curve_sgd.csv')\n",
        "\n",
        "print('\\n[TrackB1] SUMMARY:')\n",
        "print(' - Folds OOF files (SGD):', fold_oof_paths)\n",
        "print(' - Best global thr (SGD):', round(best_thr,3), 'OOF micro-F1:', round(best_f1,5))\n",
        "print('[TrackB1] DONE. Ready to blend with Track A after Track A FIX completes all folds.')\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}