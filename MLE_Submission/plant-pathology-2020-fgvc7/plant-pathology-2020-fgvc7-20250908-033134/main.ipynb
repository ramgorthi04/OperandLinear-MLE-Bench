{
  "cells": [
    {
      "id": "c3c79b43-e246-4879-a120-4fc8c74e4605",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plant Pathology 2020 - FGVC7 | Plan and Experiment Log\n",
        "\n",
        "## Plan\n",
        "- Goal: Win a medal via strong mean-column-wise-roc-auc on multi-label leaf disease classification.\n",
        "- Steps:\n",
        "  1) Load train/test CSVs, inspect columns and label structure (confirm if multi-label: healthy, multiple_diseases, rust, scab).\n",
        "  2) Verify image paths and counts, basic EDA: label prevalence, leakage checks.\n",
        "  3) Baseline model: timm CNN (e.g., tf_efficientnet_b3/b4) with cross-validation, BCEWithLogitsLoss, AUC metrics.\n",
        "  4) Strong aug: flips, rotations, color jitter, slight blur; image size 512 to start.\n",
        "  5) 5-fold StratifiedKFold (multilabel stratification via iterative stratification if needed).\n",
        "  6) Train with early stopping; log fold metrics, times; produce out-of-fold AUC and test predictions.\n",
        "  7) Iterate: try larger img size (576/640), CutMix/Mixup, label-smoothing, TTA, model ensembling (b3+b4+nfnet), and balanced sampling.\n",
        "  8) Generate submission.csv; target medal thresholds.\n",
        "\n",
        "## Experiment Log\n",
        "- [T0] Init notebook, inspect data and labels.\n",
        "- [T1] Baseline EDA and CV plan.\n",
        "- [T2] Implement dataset/dataloader and baseline model with timm; train 5-fold @size=512.\n",
        "- [T3] Evaluate OOF AUC; refine aug/hparams.\n",
        "- [T4] TTA and ensemble; finalize submission.\n",
        "\n",
        "## Environment/Runtime Notes\n",
        "- HW: T4 16GB, plenty of RAM. Use mixed precision and num_workers.\n",
        "- Always print progress and timing per fold/epoch."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "dcdc24b0-1894-4866-810b-51499e22f19e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# T0: Load data, inspect schema, check files, basic EDA\n",
        "import os, sys, time, json, random, math, gc\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "DATA_DIR = Path('.')\n",
        "IMG_DIR = DATA_DIR / 'images'\n",
        "TRAIN_CSV = DATA_DIR / 'train.csv'\n",
        "TEST_CSV = DATA_DIR / 'test.csv'\n",
        "\n",
        "print('CWD:', os.getcwd())\n",
        "print('Files present:', os.listdir())\n",
        "print('Images dir exists:', IMG_DIR.exists())\n",
        "\n",
        "train = pd.read_csv(TRAIN_CSV)\n",
        "test = pd.read_csv(TEST_CSV)\n",
        "print('\\nTrain shape:', train.shape)\n",
        "print('Test shape:', test.shape)\n",
        "print('\\nTrain head:')\n",
        "print(train.head())\n",
        "print('\\nColumns:', list(train.columns))\n",
        "\n",
        "# Identify label columns (expect 4 classes: healthy, multiple_diseases, rust, scab)\n",
        "label_cols = [c for c in train.columns if c not in ['image_id']]\n",
        "print('\\nLabel columns detected:', label_cols)\n",
        "\n",
        "# Check basic stats and class prevalence\n",
        "print('\\nLabel prevalence:')\n",
        "print(train[label_cols].mean().sort_values(ascending=False))\n",
        "\n",
        "# Verify a few image files exist\n",
        "missing = []\n",
        "for img_id in train['image_id'].head(10).tolist():\n",
        "    p = IMG_DIR / f\"{img_id}.jpg\"\n",
        "    if not p.exists():\n",
        "        missing.append(str(p))\n",
        "print(f\"\\nSample path checks missing={len(missing)}\")\n",
        "if missing:\n",
        "    print('Missing examples (first 5):', missing[:5])\n",
        "\n",
        "# Show a small grid of sample images with labels\n",
        "def show_samples(df, n=9):\n",
        "    ids = df.sample(n=min(n, len(df)), random_state=42)['image_id'].tolist()\n",
        "    cols = int(math.sqrt(len(ids))) or 1\n",
        "    rows = math.ceil(len(ids)/cols)\n",
        "    plt.figure(figsize=(3*cols, 3*rows))\n",
        "    for i, img_id in enumerate(ids, 1):\n",
        "        img_path = IMG_DIR / f\"{img_id}.jpg\"\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print('Error opening', img_path, e)\n",
        "            continue\n",
        "        ax = plt.subplot(rows, cols, i)\n",
        "        ax.imshow(img)\n",
        "        lbl = train.loc[train.image_id==img_id, label_cols].iloc[0].to_dict()\n",
        "        ax.set_title('\\n'.join([f\"{k}:{v}\" for k,v in lbl.items()]))\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_samples(train, n=9)\n",
        "\n",
        "# Save basic info for downstream steps\n",
        "meta = {\n",
        "    'n_train': int(len(train)),\n",
        "    'n_test': int(len(test)),\n",
        "    'label_cols': label_cols,\n",
        "    'img_dir': str(IMG_DIR.resolve()),\n",
        "}\n",
        "with open('data_meta.json', 'w') as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "print('\\nSaved data_meta.json:', meta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "bafc9696-456a-409f-80e7-8e5ef8e4e802",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# T1: Install required packages (PyTorch CUDA 12.1, timm, albumentations, iterstrat)\n",
        "import sys, subprocess, time\n",
        "def pip_install(pkgs):\n",
        "    print('Installing:', pkgs); sys.stdout.flush()\n",
        "    start=time.time()\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q'] + pkgs)\n",
        "    print(f'Done in {time.time()-start:.1f}s'); sys.stdout.flush()\n",
        "\n",
        "# Check torch first\n",
        "try:\n",
        "    import torch\n",
        "    print('Torch version:', torch.__version__)\n",
        "except Exception as e:\n",
        "    print('Torch not installed or import error:', e)\n",
        "\n",
        "pkgs = [\n",
        "    'torch==2.4.0+cu121',\n",
        "    'torchvision==0.19.0+cu121',\n",
        "    'torchaudio==2.4.0+cu121',\n",
        "    '--extra-index-url', 'https://download.pytorch.org/whl/cu121'\n",
        "]\n",
        "pip_install(pkgs)\n",
        "\n",
        "pip_install(['timm>=1.0.7', 'albumentations>=1.4.8', 'iterative-stratification', 'scikit-learn>=1.4.0'])\n",
        "import torch, torchvision, timm, albumentations as A\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "import sklearn\n",
        "print('Installed versions -> torch:', torch.__version__, '| torchvision:', torchvision.__version__, '| timm:', timm.__version__, '| albumentations:', A.__version__, '| sklearn:', sklearn.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "1977bee7-993e-4cda-9fa4-15301afbf596",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# T2: Create 5-fold Multilabel Stratified CV splits and save\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "import time\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "label_cols = ['healthy','multiple_diseases','rust','scab']\n",
        "\n",
        "mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = np.full(len(train), -1, dtype=int)\n",
        "X = train[['image_id']].values\n",
        "Y = train[label_cols].values\n",
        "\n",
        "t0=time.time()\n",
        "for i, (trn_idx, val_idx) in enumerate(mskf.split(X, Y)):\n",
        "    folds[val_idx] = i\n",
        "    print(f'Assigned fold {i}: val size={len(val_idx)}; elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "train['fold'] = folds\n",
        "assert (train['fold']>=0).all(), 'Unassigned fold entries exist!'\n",
        "\n",
        "# Show distribution per fold\n",
        "summary = []\n",
        "for f in range(5):\n",
        "    df = train[train.fold==f]\n",
        "    cnt = len(df)\n",
        "    means = df[label_cols].mean().to_dict()\n",
        "    summary.append({'fold': f, 'count': cnt, **{f'mean_{k}': v for k,v in means.items()}})\n",
        "    print(f\"Fold {f}: n={cnt}, means={{\" + ', '.join([f'{k}:{v:.3f}' for k,v in means.items()]) + '}}')\n",
        "\n",
        "train.to_csv('train_folds.csv', index=False)\n",
        "print('\\nSaved train_folds.csv with fold assignments.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "1035fdc7-12e1-43b3-aa33-1a01f1f25bca",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# T3: Dataset, model, training loop (5-fold) and inference to submission (with Mixup, LS, RRCrop, Warmup+Cosine, 8-way TTA logit-avg)\n",
        "import os, time, math, gc, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import timm\n",
        "import cv2\n",
        "\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "SEED = 42\n",
        "def set_seed(seed=SEED):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "set_seed()\n",
        "torch.backends.cudnn.benchmark = False  # reduce workspace usage\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "IMG_DIR = Path('images')\n",
        "LABEL_COLS = ['healthy','multiple_diseases','rust','scab']\n",
        "\n",
        "class LeafDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transforms=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = Path(img_dir)\n",
        "        self.transforms = transforms\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = self.img_dir / f\"{row.image_id}.jpg\"\n",
        "        img = np.array(Image.open(img_path).convert('RGB'))\n",
        "        if self.transforms:\n",
        "            img = self.transforms(image=img)['image']\n",
        "        target = row[LABEL_COLS].values.astype('float32') if 'fold' in self.df.columns else None\n",
        "        return img, (torch.from_numpy(target) if target is not None else row.image_id)\n",
        "\n",
        "def get_transforms(img_size=512):\n",
        "    train_tfms = A.Compose([\n",
        "        A.RandomResizedCrop(size=(img_size, img_size), scale=(0.7, 1.0), ratio=(0.95, 1.05)),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.6, border_mode=cv2.BORDER_REFLECT_101),\n",
        "        A.ColorJitter(0.2,0.2,0.2,0.05, p=0.7),\n",
        "        A.CLAHE(clip_limit=2.0, p=0.3),\n",
        "        A.GaussianBlur(blur_limit=(3,3), p=0.2),\n",
        "        A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "    valid_tfms = A.Compose([\n",
        "        A.Resize(height=img_size, width=img_size),\n",
        "        A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "    return train_tfms, valid_tfms\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, backbone='tf_efficientnet_b2_ns', pretrained=True, n_out=4):\n",
        "        super().__init__()\n",
        "        self.net = timm.create_model(backbone, pretrained=pretrained, num_classes=n_out, in_chans=3)\n",
        "        if hasattr(self.net, 'set_grad_checkpointing'):\n",
        "            self.net.set_grad_checkpointing(True)\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def compute_auc(y_true, y_pred):\n",
        "    aucs = []\n",
        "    for i in range(y_true.shape[1]):\n",
        "        try:\n",
        "            aucs.append(roc_auc_score(y_true[:, i], y_pred[:, i]))\n",
        "        except Exception:\n",
        "            aucs.append(np.nan)\n",
        "    return float(np.nanmean(aucs)), aucs\n",
        "\n",
        "# Mixup utilities\n",
        "def sample_beta_distribution(alpha, size):\n",
        "    return np.random.beta(alpha, alpha, size).astype('float32')\n",
        "\n",
        "def mixup_batch(x, y, alpha=0.3):\n",
        "    lam = sample_beta_distribution(alpha, 1)[0]\n",
        "    bs = x.size(0)\n",
        "    index = torch.randperm(bs, device=x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def smooth_targets(y, eps=0.1):\n",
        "    # Multilabel smoothing towards 0.5\n",
        "    return y * (1.0 - eps) + 0.5 * eps\n",
        "\n",
        "def train_one_fold(fold, df_folds, img_size=320, epochs=18, batch_size=2, lr=2e-4, weight_decay=1e-4, device='cuda', mixup_p=0.7, mixup_alpha=0.3, label_smoothing=0.1, accum_steps=4):\n",
        "    t0=time.time()\n",
        "    trn_df = df_folds[df_folds.fold!=fold].reset_index(drop=True)\n",
        "    val_df = df_folds[df_folds.fold==fold].reset_index(drop=True)\n",
        "    train_tfms, valid_tfms = get_transforms(img_size)\n",
        "    trn_ds = LeafDataset(trn_df, IMG_DIR, train_tfms)\n",
        "    val_ds = LeafDataset(val_df, IMG_DIR, valid_tfms)\n",
        "    trn_dl = DataLoader(trn_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False, drop_last=True, persistent_workers=False)\n",
        "    val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False, persistent_workers=False)\n",
        "\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "    model = Model().to(device)\n",
        "    model = model.to(memory_format=torch.channels_last)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    # Warmup (2 epochs) + Cosine\n",
        "    warmup_epochs = 2\n",
        "    main_epochs = max(1, epochs - warmup_epochs)\n",
        "    sched_warmup = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=warmup_epochs)\n",
        "    sched_cos = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=main_epochs)\n",
        "    scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[sched_warmup, sched_cos], milestones=[warmup_epochs])\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    scaler = torch.amp.GradScaler('cuda', enabled=True)\n",
        "\n",
        "    best_auc = -1.0\n",
        "    best_path = f'fold{fold}_best.pt'\n",
        "    patience, wait = 4, 0\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        tr_loss = 0.0\n",
        "        start = time.time()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        for bi, (imgs, targets) in enumerate(trn_dl):\n",
        "            imgs = imgs.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            targets = targets.to(device)\n",
        "            use_mix = (random.random() < mixup_p)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                if use_mix:\n",
        "                    mix_imgs, y_a, y_b, lam = mixup_batch(imgs, targets, alpha=mixup_alpha)\n",
        "                    y_a_s = smooth_targets(y_a, label_smoothing)\n",
        "                    y_b_s = smooth_targets(y_b, label_smoothing)\n",
        "                    logits = model(mix_imgs)\n",
        "                    loss = lam * loss_fn(logits, y_a_s) + (1 - lam) * loss_fn(logits, y_b_s)\n",
        "                else:\n",
        "                    logits = model(imgs)\n",
        "                    targets_s = smooth_targets(targets, label_smoothing)\n",
        "                    loss = loss_fn(logits, targets_s)\n",
        "                loss = loss / accum_steps\n",
        "            scaler.scale(loss).backward()\n",
        "            if (bi + 1) % accum_steps == 0 or (bi + 1) == len(trn_dl):\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "            tr_loss += loss.item() * accum_steps\n",
        "            if (bi+1) % 20 == 0:\n",
        "                print(f'[Fold {fold}] Epoch {epoch} Batch {bi+1}/{len(trn_dl)} loss={tr_loss/(bi+1):.4f}', flush=True)\n",
        "        scheduler.step()\n",
        "        # Validate\n",
        "        model.eval()\n",
        "        v_preds = []; v_targets = []\n",
        "        with torch.no_grad():\n",
        "            for imgs, targets in val_dl:\n",
        "                imgs = imgs.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "                logits = model(imgs)\n",
        "                v_preds.append(torch.sigmoid(logits).cpu().numpy())\n",
        "                v_targets.append(targets.numpy())\n",
        "        v_preds = np.concatenate(v_preds); v_targets = np.concatenate(v_targets)\n",
        "        val_auc, per_col = compute_auc(v_targets, v_preds)\n",
        "        print(f'[Fold {fold}] Epoch {epoch} done in {time.time()-start:.1f}s | tr_loss={tr_loss/max(1,len(trn_dl)):.4f} | val_auc={val_auc:.5f} | cols={per_col}', flush=True)\n",
        "        if val_auc > best_auc:\n",
        "            best_auc = val_auc; wait = 0\n",
        "            torch.save({'state_dict': model.state_dict(), 'auc': best_auc}, best_path)\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(f'[Fold {fold}] Early stopping at epoch {epoch}', flush=True)\n",
        "                break\n",
        "    # Load best and create OOF preds\n",
        "    ckpt = torch.load(best_path, map_location=device)\n",
        "    model.load_state_dict(ckpt['state_dict'])\n",
        "    model.eval()\n",
        "    v_preds = []; v_targets = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in val_dl:\n",
        "            imgs = imgs.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            logits = model(imgs)\n",
        "            v_preds.append(torch.sigmoid(logits).cpu().numpy())\n",
        "            v_targets.append(targets.numpy())\n",
        "    v_preds = np.concatenate(v_preds); v_targets = np.concatenate(v_targets)\n",
        "    fold_time = time.time()-t0\n",
        "    print(f'[Fold {fold}] Best AUC={best_auc:.5f} | time={fold_time/60:.1f} min', flush=True)\n",
        "    return model, v_preds, v_targets, best_auc, optimizer, scheduler\n",
        "\n",
        "def tta_logits(model, imgs):\n",
        "    # 8-way dihedral TTA, return averaged logits (before sigmoid)\n",
        "    outs = []\n",
        "    with torch.no_grad():\n",
        "        outs.append(model(imgs))\n",
        "        outs.append(model(torch.flip(imgs, dims=[3])))\n",
        "        outs.append(model(torch.flip(imgs, dims=[2])))\n",
        "        outs.append(model(torch.rot90(imgs, k=1, dims=(2,3))))\n",
        "        outs.append(model(torch.rot90(imgs, k=2, dims=(2,3))))\n",
        "        outs.append(model(torch.rot90(imgs, k=3, dims=(2,3))))\n",
        "        tmp = torch.rot90(imgs, k=1, dims=(2,3))\n",
        "        outs.append(model(torch.flip(tmp, dims=[3])))\n",
        "        outs.append(model(torch.flip(tmp, dims=[2])))\n",
        "    return torch.stack(outs, dim=0).mean(0)\n",
        "\n",
        "def run_training():\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    df = pd.read_csv('train_folds.csv')\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    IMG_SIZE = 320\n",
        "    EPOCHS = 18\n",
        "    BATCH_SIZE = 2\n",
        "    LR = 2e-4\n",
        "    WD = 1e-4\n",
        "    ACCUM_STEPS = 4\n",
        "\n",
        "    oof_preds = np.zeros((len(df), 4), dtype=float)\n",
        "    oof_targets = df[LABEL_COLS].values.astype(float)\n",
        "    fold_aucs = []\n",
        "\n",
        "    for fold in range(5):\n",
        "        print(f'==== Training fold {fold} ====', flush=True)\n",
        "        model, v_pred, v_true, best_auc, optimizer, scheduler = train_one_fold(\n",
        "            fold, df, img_size=IMG_SIZE, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR, weight_decay=WD, device=device, accum_steps=ACCUM_STEPS)\n",
        "        val_idx = df.index[df.fold==fold].to_numpy()\n",
        "        oof_preds[val_idx] = v_pred\n",
        "        fold_aucs.append(best_auc)\n",
        "        del model, optimizer, scheduler; gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    oof_auc, per_col = compute_auc(oof_targets, oof_preds)\n",
        "    print(f'OOF mean AUC: {oof_auc:.5f} | per-col {per_col}', flush=True)\n",
        "    np.save('oof_preds.npy', oof_preds)\n",
        "    pd.DataFrame(oof_preds, columns=LABEL_COLS).to_csv('oof_preds.csv', index=False)\n",
        "\n",
        "    # Inference on test with TTA and fold-averaged logits\n",
        "    _, valid_tfms = get_transforms(IMG_SIZE)\n",
        "    test_ds = LeafDataset(test_df, IMG_DIR, valid_tfms)\n",
        "    test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=False, persistent_workers=False)\n",
        "    all_logits = []\n",
        "    for fold in range(5):\n",
        "        print(f'Loading best model fold {fold} for test inference', flush=True)\n",
        "        model = Model().to(device)\n",
        "        ckpt = torch.load(f'fold{fold}_best.pt', map_location=device)\n",
        "        model.load_state_dict(ckpt['state_dict'])\n",
        "        model.eval()\n",
        "        fold_logits = []\n",
        "        with torch.no_grad():\n",
        "            for imgs, ids in test_dl:\n",
        "                imgs = imgs.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "                logits = tta_logits(model, imgs)  # average logits across TTA\n",
        "                fold_logits.append(logits.cpu().numpy())\n",
        "        all_logits.append(np.concatenate(fold_logits))\n",
        "        del model; gc.collect(); torch.cuda.empty_cache()\n",
        "    mean_logits = np.mean(np.stack(all_logits, axis=0), axis=0)\n",
        "    test_pred = 1.0 / (1.0 + np.exp(-mean_logits))  # sigmoid\n",
        "    sub = pd.read_csv('sample_submission.csv')\n",
        "    sub[LABEL_COLS] = test_pred\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv. OOF AUC:', oof_auc)\n",
        "\n",
        "run_training()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "f069a650-1b17-473b-9288-b7eee6100f29",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# T4: EMA fine-tune existing fold checkpoints at higher resolution and re-infer\n",
        "import gc, time, random, numpy as np, pandas as pd, torch, torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from timm.utils import ModelEmaV2\n",
        "\n",
        "def fine_tune_one_fold_with_ema(fold, df_folds, img_size=352, epochs=3, batch_size=2, lr=1e-4, weight_decay=1e-4, device='cuda',\n",
        "                                mixup_p=0.7, mixup_alpha=0.3, label_smoothing=0.1, accum_steps=4, ema_decay=0.9998):\n",
        "    from albumentations.pytorch import ToTensorV2\n",
        "    import albumentations as A, cv2\n",
        "    def smooth_targets(y, eps=0.1):\n",
        "        return y * (1.0 - eps) + 0.5 * eps\n",
        "\n",
        "    trn_df = df_folds[df_folds.fold!=fold].reset_index(drop=True)\n",
        "    val_df = df_folds[df_folds.fold==fold].reset_index(drop=True)\n",
        "    train_tfms, valid_tfms = get_transforms(img_size)\n",
        "    trn_ds = LeafDataset(trn_df, IMG_DIR, train_tfms)\n",
        "    val_ds = LeafDataset(val_df, IMG_DIR, valid_tfms)\n",
        "    trn_dl = DataLoader(trn_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False, drop_last=True, persistent_workers=False)\n",
        "    val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False, persistent_workers=False)\n",
        "\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "    model = Model().to(device)\n",
        "    # Load previous best weights\n",
        "    ckpt = torch.load(f'fold{fold}_best.pt', map_location=device)\n",
        "    model.load_state_dict(ckpt['state_dict'], strict=True)\n",
        "    model = model.to(memory_format=torch.channels_last)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    warmup_epochs = 1\n",
        "    main_epochs = max(1, epochs - warmup_epochs)\n",
        "    sched_warm = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=warmup_epochs)\n",
        "    sched_cos = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=main_epochs)\n",
        "    scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, [sched_warm, sched_cos], milestones=[warmup_epochs])\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    scaler = torch.amp.GradScaler('cuda', enabled=True)\n",
        "    ema = ModelEmaV2(model, decay=ema_decay)  # track EMA\n",
        "\n",
        "    best_auc = -1.0\n",
        "    best_path = f'fold{fold}_best_ft.pt'\n",
        "    patience, wait = 2, 0\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        tr_loss = 0.0; t0 = time.time()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        for bi, (imgs, targets) in enumerate(trn_dl):\n",
        "            imgs = imgs.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            targets = targets.to(device)\n",
        "            use_mix = (random.random() < mixup_p)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                if use_mix:\n",
        "                    bs = imgs.size(0)\n",
        "                    index = torch.randperm(bs, device=imgs.device)\n",
        "                    lam = float(np.random.beta(mixup_alpha, mixup_alpha))\n",
        "                    mix_imgs = lam * imgs + (1 - lam) * imgs[index]\n",
        "                    y_a, y_b = targets, targets[index]\n",
        "                    y_a_s = smooth_targets(y_a, label_smoothing)\n",
        "                    y_b_s = smooth_targets(y_b, label_smoothing)\n",
        "                    logits = model(mix_imgs)\n",
        "                    loss = lam * loss_fn(logits, y_a_s) + (1 - lam) * loss_fn(logits, y_b_s)\n",
        "                else:\n",
        "                    logits = model(imgs)\n",
        "                    targets_s = smooth_targets(targets, label_smoothing)\n",
        "                    loss = loss_fn(logits, targets_s)\n",
        "                loss = loss / accum_steps\n",
        "            scaler.scale(loss).backward()\n",
        "            if (bi + 1) % accum_steps == 0 or (bi + 1) == len(trn_dl):\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                ema.update(model)  # update EMA after optimizer step\n",
        "            tr_loss += loss.item() * accum_steps\n",
        "            if (bi+1) % 20 == 0:\n",
        "                print(f'[FT Fold {fold}] Epoch {epoch} Batch {bi+1}/{len(trn_dl)} loss={tr_loss/(bi+1):.4f}', flush=True)\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validate with EMA weights\n",
        "        model.eval()\n",
        "        v_preds = []; v_targets = []\n",
        "        with torch.no_grad():\n",
        "            for imgs, targets in val_dl:\n",
        "                imgs = imgs.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "                logits = ema.module(imgs)\n",
        "                v_preds.append(torch.sigmoid(logits).cpu().numpy())\n",
        "                v_targets.append(targets.numpy())\n",
        "        v_preds = np.concatenate(v_preds); v_targets = np.concatenate(v_targets)\n",
        "        val_auc, per_col = compute_auc(v_targets, v_preds)\n",
        "        print(f'[FT Fold {fold}] Epoch {epoch} time={time.time()-t0:.1f}s | tr_loss={tr_loss/max(1,len(trn_dl)):.4f} | val_auc={val_auc:.5f}', flush=True)\n",
        "        if val_auc > best_auc:\n",
        "            best_auc = val_auc; wait = 0\n",
        "            torch.save({'state_dict': ema.module.state_dict(), 'auc': best_auc}, best_path)\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(f'[FT Fold {fold}] Early stopping at epoch {epoch}', flush=True)\n",
        "                break\n",
        "\n",
        "    # Load best ft and return val preds for OOF\n",
        "    ckpt = torch.load(best_path, map_location=device)\n",
        "    ema.module.load_state_dict(ckpt['state_dict'], strict=True)\n",
        "    ema.module.eval()\n",
        "    v_preds = []; v_targets = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in val_dl:\n",
        "            imgs = imgs.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            logits = ema.module(imgs)\n",
        "            v_preds.append(torch.sigmoid(logits).cpu().numpy())\n",
        "            v_targets.append(targets.numpy())\n",
        "    v_preds = np.concatenate(v_preds); v_targets = np.concatenate(v_targets)\n",
        "    return best_auc, v_preds, v_targets\n",
        "\n",
        "def run_finetune_and_infer():\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    df = pd.read_csv('train_folds.csv')\n",
        "    IMG_SIZE_FT = 352\n",
        "    EPOCHS_FT = 3\n",
        "    BATCH_SIZE = 2\n",
        "    LR = 1e-4\n",
        "    WD = 1e-4\n",
        "    ACCUM_STEPS = 4\n",
        "    EMA_DECAY = 0.9998\n",
        "\n",
        "    print('=== Fine-tuning existing fold checkpoints with EMA ===', flush=True)\n",
        "    oof_preds = np.zeros((len(df), 4), dtype=float)\n",
        "    oof_targets = df[LABEL_COLS].values.astype(float)\n",
        "    fold_aucs = []\n",
        "    for fold in range(5):\n",
        "        print(f'-- Fine-tune fold {fold} --', flush=True)\n",
        "        best_auc, v_pred, v_true = fine_tune_one_fold_with_ema(\n",
        "            fold, df, img_size=IMG_SIZE_FT, epochs=EPOCHS_FT, batch_size=BATCH_SIZE, lr=LR, weight_decay=WD,\n",
        "            device=device, accum_steps=ACCUM_STEPS, ema_decay=EMA_DECAY)\n",
        "        val_idx = df.index[df.fold==fold].to_numpy()\n",
        "        oof_preds[val_idx] = v_pred\n",
        "        fold_aucs.append(best_auc)\n",
        "        gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    oof_auc, per_col = compute_auc(oof_targets, oof_preds)\n",
        "    print(f'[FT] OOF mean AUC: {oof_auc:.5f} | per-col {per_col}', flush=True)\n",
        "    np.save('oof_preds_ft.npy', oof_preds)\n",
        "    pd.DataFrame(oof_preds, columns=LABEL_COLS).to_csv('oof_preds_ft.csv', index=False)\n",
        "\n",
        "    # Inference on test using EMA fine-tuned checkpoints with TTA\n",
        "    _, valid_tfms = get_transforms(IMG_SIZE_FT)\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    test_ds = LeafDataset(test_df, IMG_DIR, valid_tfms)\n",
        "    test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=False, persistent_workers=False)\n",
        "    all_logits = []\n",
        "    for fold in range(5):\n",
        "        print(f'Loading fine-tuned EMA model fold {fold} for test inference', flush=True)\n",
        "        model = Model().to(device)\n",
        "        ckpt = torch.load(f'fold{fold}_best_ft.pt', map_location=device)\n",
        "        model.load_state_dict(ckpt['state_dict'], strict=True)\n",
        "        model.eval()\n",
        "        fold_logits = []\n",
        "        with torch.no_grad():\n",
        "            for imgs, ids in test_dl:\n",
        "                imgs = imgs.to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "                logits = tta_logits(model, imgs)\n",
        "                fold_logits.append(logits.cpu().numpy())\n",
        "        all_logits.append(np.concatenate(fold_logits))\n",
        "        del model; gc.collect(); torch.cuda.empty_cache()\n",
        "    mean_logits = np.mean(np.stack(all_logits, axis=0), axis=0)\n",
        "    test_pred = 1.0 / (1.0 + np.exp(-mean_logits))\n",
        "    sub = pd.read_csv('sample_submission.csv')\n",
        "    sub[LABEL_COLS] = test_pred\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('[FT] Saved submission.csv | OOF AUC:', oof_auc)\n",
        "\n",
        "run_finetune_and_infer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "4203832f-cb1b-4e1a-9c6e-e55f65610b52",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# T5: Evaluate fine-tuned OOF AUC and sanity-check submission\n",
        "import pandas as pd, numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "LABEL_COLS = ['healthy','multiple_diseases','rust','scab']\n",
        "df = pd.read_csv('train_folds.csv')\n",
        "oof = pd.read_csv('oof_preds_ft.csv')\n",
        "assert len(df)==len(oof), f'Length mismatch: {len(df)} vs {len(oof)}'\n",
        "y_true = df[LABEL_COLS].values.astype(float)\n",
        "y_pred = oof[LABEL_COLS].values.astype(float)\n",
        "per_col = []\n",
        "for i, c in enumerate(LABEL_COLS):\n",
        "    auc = roc_auc_score(y_true[:, i], y_pred[:, i])\n",
        "    per_col.append(auc)\n",
        "mean_auc = float(np.mean(per_col))\n",
        "print(f'[CHECK] FT OOF mean AUC: {mean_auc:.6f} | per-col: {per_col}')\n",
        "\n",
        "# Submission sanity checks\n",
        "sub = pd.read_csv('submission.csv')\n",
        "print('[CHECK] submission.csv shape:', sub.shape)\n",
        "print('[CHECK] submission head:')\n",
        "print(sub.head())\n",
        "print('[CHECK] value ranges per column:')\n",
        "for c in LABEL_COLS:\n",
        "    s = sub[c]\n",
        "    print(c, 'min', float(s.min()), 'max', float(s.max()), 'mean', float(s.mean()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "c8cbac74-fd63-4c40-a661-d7984e516889",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# T6: Mega-Ensemble (pre-FT + FT) with multi-scale TTA; OOF validation and test submission with optional post-processing\n",
        "import gc, time, numpy as np, pandas as pd, torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "LABEL_COLS = ['healthy','multiple_diseases','rust','scab']\n",
        "\n",
        "def get_valid_tfms_size(img_size):\n",
        "    import albumentations as A\n",
        "    from albumentations.pytorch import ToTensorV2\n",
        "    return A.Compose([A.Resize(height=img_size, width=img_size), A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)), ToTensorV2()])\n",
        "\n",
        "def predict_logits_with_tta(model, dl):\n",
        "    model.eval()\n",
        "    logits_list = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, idx_or_id in dl:\n",
        "            imgs = imgs.to(DEVICE, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            logits = tta_logits(model, imgs)  # uses global tta function from earlier\n",
        "            logits_list.append(logits.cpu().numpy())\n",
        "    return np.concatenate(logits_list)\n",
        "\n",
        "def load_model_for_fold(fold, use_ft):\n",
        "    m = Model().to(DEVICE)\n",
        "    ckpt_path = f'fold{fold}_best_ft.pt' if use_ft else f'fold{fold}_best.pt'\n",
        "    ckpt = torch.load(ckpt_path, map_location=DEVICE)\n",
        "    m.load_state_dict(ckpt['state_dict'], strict=True)\n",
        "    m = m.to(memory_format=torch.channels_last)\n",
        "    m.eval()\n",
        "    return m\n",
        "\n",
        "def compute_auc(y_true, y_pred):\n",
        "    aucs = []\n",
        "    for i in range(y_true.shape[1]):\n",
        "        try:\n",
        "            aucs.append(roc_auc_score(y_true[:, i], y_pred[:, i]))\n",
        "        except Exception:\n",
        "            aucs.append(np.nan)\n",
        "    return float(np.nanmean(aucs)), aucs\n",
        "\n",
        "def make_oof_ensemble(scales=(320,352), ft_weight=0.7, batch_size=2):\n",
        "    df = pd.read_csv('train_folds.csv')\n",
        "    from pathlib import Path\n",
        "    IMG_DIR = Path('images')\n",
        "    oof_logits = np.zeros((len(df), 4), dtype=np.float32)\n",
        "    y_true = df[LABEL_COLS].values.astype(float)\n",
        "    for f in range(5):\n",
        "        val_df = df[df.fold==f].reset_index(drop=True)\n",
        "        print(f'[ENS OOF] Fold {f} | val_n={len(val_df)}')\n",
        "        logits_scales = []\n",
        "        for sz in scales:\n",
        "            try:\n",
        "                t_sz = time.time()\n",
        "                valid_tfms = get_valid_tfms_size(sz)\n",
        "                val_ds = LeafDataset(val_df, 'images', valid_tfms)\n",
        "                val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n",
        "                # Pre-FT ensemble using other folds\n",
        "                sum_logits_pre = 0.0\n",
        "                cnt_models = 0\n",
        "                for mfold in range(5):\n",
        "                    if mfold == f: continue\n",
        "                    m = load_model_for_fold(mfold, use_ft=False)\n",
        "                    sum_logits_pre += predict_logits_with_tta(m, val_dl)\n",
        "                    cnt_models += 1\n",
        "                    del m; gc.collect(); torch.cuda.empty_cache()\n",
        "                logits_pre = sum_logits_pre / max(1, cnt_models)\n",
        "                # FT ensemble using other folds\n",
        "                sum_logits_ft = 0.0\n",
        "                cnt_models = 0\n",
        "                for mfold in range(5):\n",
        "                    if mfold == f: continue\n",
        "                    m = load_model_for_fold(mfold, use_ft=True)\n",
        "                    sum_logits_ft += predict_logits_with_tta(m, val_dl)\n",
        "                    cnt_models += 1\n",
        "                    del m; gc.collect(); torch.cuda.empty_cache()\n",
        "                logits_ft = sum_logits_ft / max(1, cnt_models)\n",
        "                logits_blend = ft_weight * logits_ft + (1.0 - ft_weight) * logits_pre\n",
        "                logits_scales.append(logits_blend.astype(np.float32))\n",
        "                print(f'[ENS OOF] Fold {f} size {sz} done in {time.time()-t_sz:.1f}s')\n",
        "                del val_dl, val_ds; gc.collect(); torch.cuda.empty_cache()\n",
        "            except RuntimeError as e:\n",
        "                print(f'[ENS OOF] Skipping size {sz} due to error: {e}')\n",
        "                gc.collect(); torch.cuda.empty_cache()\n",
        "                continue\n",
        "        if len(logits_scales)==0:\n",
        "            raise RuntimeError('All scales failed for OOF ensemble')\n",
        "        logits_avg = np.mean(np.stack(logits_scales, axis=0), axis=0)\n",
        "        val_idx = df.index[df.fold==f].to_numpy()\n",
        "        oof_logits[val_idx] = logits_avg\n",
        "    oof_probs = 1.0 / (1.0 + np.exp(-oof_logits))\n",
        "    oof_auc, per_col = compute_auc(y_true, oof_probs)\n",
        "    pd.DataFrame(oof_probs, columns=LABEL_COLS).to_csv('oof_preds_ens.csv', index=False)\n",
        "    print(f'[ENS OOF] mean AUC: {oof_auc:.6f} | per-col: {per_col}')\n",
        "    return oof_probs, y_true, oof_auc, per_col\n",
        "\n",
        "def make_test_ensemble(scales=(320,352), ft_weight=0.7, batch_size=2):\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    logits_scales_all = []\n",
        "    for sz in scales:\n",
        "        try:\n",
        "            t_sz = time.time()\n",
        "            valid_tfms = get_valid_tfms_size(sz)\n",
        "            test_ds = LeafDataset(test_df, 'images', valid_tfms)\n",
        "            test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n",
        "            # Pre-FT across all 5 folds\n",
        "            sum_logits_pre = 0.0\n",
        "            for mfold in range(5):\n",
        "                m = load_model_for_fold(mfold, use_ft=False)\n",
        "                sum_logits_pre = (sum_logits_pre + predict_logits_with_tta(m, test_dl)) if isinstance(sum_logits_pre, float) else (sum_logits_pre + predict_logits_with_tta(m, test_dl))\n",
        "                del m; gc.collect(); torch.cuda.empty_cache()\n",
        "            logits_pre = sum_logits_pre / 5.0\n",
        "            # FT across all 5 folds\n",
        "            sum_logits_ft = 0.0\n",
        "            for mfold in range(5):\n",
        "                m = load_model_for_fold(mfold, use_ft=True)\n",
        "                sum_logits_ft = (sum_logits_ft + predict_logits_with_tta(m, test_dl)) if isinstance(sum_logits_ft, float) else (sum_logits_ft + predict_logits_with_tta(m, test_dl))\n",
        "                del m; gc.collect(); torch.cuda.empty_cache()\n",
        "            logits_ft = sum_logits_ft / 5.0\n",
        "            logits_blend = ft_weight * logits_ft + (1.0 - ft_weight) * logits_pre\n",
        "            logits_scales_all.append(logits_blend.astype(np.float32))\n",
        "            print(f'[ENS TEST] size {sz} done in {time.time()-t_sz:.1f}s')\n",
        "            del test_dl, test_ds; gc.collect(); torch.cuda.empty_cache()\n",
        "        except RuntimeError as e:\n",
        "            print(f'[ENS TEST] Skipping size {sz} due to error: {e}')\n",
        "            gc.collect(); torch.cuda.empty_cache()\n",
        "            continue\n",
        "    if len(logits_scales_all)==0:\n",
        "        raise RuntimeError('All scales failed for TEST ensemble')\n",
        "    logits_avg = np.mean(np.stack(logits_scales_all, axis=0), axis=0)\n",
        "    probs = 1.0 / (1.0 + np.exp(-logits_avg))\n",
        "    sub = pd.read_csv('sample_submission.csv')\n",
        "    sub[LABEL_COLS] = probs\n",
        "    return sub\n",
        "\n",
        "def apply_md_rule(probs, k):\n",
        "    # probs: numpy array shape (N,4) with order LABEL_COLS\n",
        "    res = probs.copy()\n",
        "    md_idx = LABEL_COLS.index('multiple_diseases')\n",
        "    rust_idx = LABEL_COLS.index('rust')\n",
        "    scab_idx = LABEL_COLS.index('scab')\n",
        "    res[:, md_idx] = np.maximum(res[:, md_idx], k * res[:, rust_idx] * res[:, scab_idx])\n",
        "    return res\n",
        "\n",
        "def run_mega_ensemble():\n",
        "    scales = [320, 352]  # reduced for speed\n",
        "    ft_weight = 0.7\n",
        "    print('[RUN] Building OOF ensemble ...')\n",
        "    oof_probs, y_true, base_auc, base_cols = make_oof_ensemble(tuple(scales), ft_weight=ft_weight, batch_size=2)\n",
        "    print(f'[RUN] Base ENS OOF AUC: {base_auc:.6f} | per-col: {base_cols}')\n",
        "    # Try MD heuristic\n",
        "    best_auc = base_auc; best_k = None; best_probs = oof_probs\n",
        "    for k in [0.7, 0.8, 0.9]:\n",
        "        mod = apply_md_rule(oof_probs, k)\n",
        "        auc, cols = compute_auc(y_true, mod)\n",
        "        print(f'[RUN] k={k} -> AUC={auc:.6f} | per-col={cols}')\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_k, best_probs = auc, k, mod\n",
        "    pd.DataFrame(best_probs, columns=LABEL_COLS).to_csv('oof_preds_ens_pp.csv', index=False)\n",
        "    print(f'[RUN] Selected OOF AUC: {best_auc:.6f} (k={best_k})')\n",
        "\n",
        "    print('[RUN] Building TEST ensemble ...')\n",
        "    sub = make_test_ensemble(tuple(scales), ft_weight=ft_weight, batch_size=2)\n",
        "    test_probs = sub[LABEL_COLS].values.astype(float)\n",
        "    if best_k is not None:\n",
        "        test_probs = apply_md_rule(test_probs, best_k)\n",
        "        sub[LABEL_COLS] = test_probs\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('[RUN] Saved submission.csv | ENS OOF AUC:', best_auc, '| k:', best_k)\n",
        "\n",
        "run_mega_ensemble()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "f644c843-1c60-47d9-9707-9e894df7b670",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# T7: Fast blend using existing OOF preds (pre-FT vs FT) via logit-avg; build test blend using pre-FT inference + FT submission logits\n",
        "import numpy as np, pandas as pd, torch, gc, time, os\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "LABEL_COLS = ['healthy','multiple_diseases','rust','scab']\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def logit(p, eps=1e-6):\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p / (1 - p))\n",
        "\n",
        "def compute_auc(y_true, y_pred):\n",
        "    aucs = []\n",
        "    for i in range(y_true.shape[1]):\n",
        "        try:\n",
        "            aucs.append(roc_auc_score(y_true[:, i], y_pred[:, i]))\n",
        "        except Exception:\n",
        "            aucs.append(np.nan)\n",
        "    return float(np.nanmean(aucs)), aucs\n",
        "\n",
        "def apply_md_rule_np(probs, k):\n",
        "    res = probs.copy()\n",
        "    md = LABEL_COLS.index('multiple_diseases')\n",
        "    rust = LABEL_COLS.index('rust')\n",
        "    scab = LABEL_COLS.index('scab')\n",
        "    res[:, md] = np.maximum(res[:, md], k * res[:, rust] * res[:, scab])\n",
        "    return res\n",
        "\n",
        "def infer_test_preft_logits(img_size=320, batch_size=8, use_tta=False):\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    # Build DL\n",
        "    import albumentations as A\n",
        "    from albumentations.pytorch import ToTensorV2\n",
        "    valid_tfms = A.Compose([A.Resize(height=img_size, width=img_size), A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)), ToTensorV2()])\n",
        "    ds = LeafDataset(test_df, 'images', valid_tfms)\n",
        "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)\n",
        "    fold_logits_all = []\n",
        "    for fold in range(5):\n",
        "        cache_path = f'test_logits_preft_fold{fold}_{img_size}.npy'\n",
        "        if os.path.exists(cache_path):\n",
        "            print(f'[BLEND TEST] Using cached pre-FT logits fold {fold} @ {img_size}')\n",
        "            fold_logits = np.load(cache_path)\n",
        "            fold_logits_all.append(fold_logits)\n",
        "            continue\n",
        "        print(f'[BLEND TEST] Pre-FT fold {fold} inference @ {img_size} | bs={batch_size} | TTA={use_tta}')\n",
        "        t_fold = time.time()\n",
        "        m = Model().to(DEVICE)\n",
        "        ckpt = torch.load(f'fold{fold}_best.pt', map_location=DEVICE)\n",
        "        m.load_state_dict(ckpt['state_dict'], strict=True)\n",
        "        m = m.to(memory_format=torch.channels_last)\n",
        "        m.eval()\n",
        "        fold_batches = []\n",
        "        with torch.no_grad():\n",
        "            for bi, (imgs, ids) in enumerate(dl):\n",
        "                imgs = imgs.to(DEVICE, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "                if use_tta:\n",
        "                    logits = tta_logits(m, imgs)\n",
        "                else:\n",
        "                    logits = m(imgs)\n",
        "                fold_batches.append(logits.cpu().numpy())\n",
        "                if (bi + 1) % 10 == 0:\n",
        "                    print(f'  - fold {fold} batch {bi+1}/{len(dl)}')\n",
        "        fold_logits = np.concatenate(fold_batches)\n",
        "        np.save(cache_path, fold_logits)\n",
        "        print(f'[BLEND TEST] Fold {fold} done in {time.time()-t_fold:.1f}s | saved {cache_path}')\n",
        "        fold_logits_all.append(fold_logits)\n",
        "        del m; gc.collect(); torch.cuda.empty_cache()\n",
        "    mean_logits = np.mean(np.stack(fold_logits_all, axis=0), axis=0)\n",
        "    return mean_logits\n",
        "\n",
        "def run_fast_blend(ft_weight=0.7):\n",
        "    # OOF blend\n",
        "    df = pd.read_csv('train_folds.csv')\n",
        "    y_true = df[LABEL_COLS].values.astype(float)\n",
        "    oof_pre = pd.read_csv('oof_preds.csv')[LABEL_COLS].values.astype(float)\n",
        "    oof_ft = pd.read_csv('oof_preds_ft.csv')[LABEL_COLS].values.astype(float)\n",
        "    logits_pre = logit(oof_pre)\n",
        "    logits_ft = logit(oof_ft)\n",
        "    logits_blend = ft_weight * logits_ft + (1.0 - ft_weight) * logits_pre\n",
        "    oof_blend = 1.0 / (1.0 + np.exp(-logits_blend))\n",
        "    base_auc, base_cols = compute_auc(y_true, oof_blend)\n",
        "    print(f'[BLEND OOF] base AUC={base_auc:.6f} | per-col={base_cols}')\n",
        "    best_auc, best_k, best_probs = base_auc, None, oof_blend\n",
        "    for k in [0.7, 0.8, 0.9]:\n",
        "        mod = apply_md_rule_np(oof_blend, k)\n",
        "        auc, cols = compute_auc(y_true, mod)\n",
        "        print(f'[BLEND OOF] k={k} -> AUC={auc:.6f} | per-col={cols}')\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_k, best_probs = auc, k, mod\n",
        "    pd.DataFrame(best_probs, columns=LABEL_COLS).to_csv('oof_preds_blend.csv', index=False)\n",
        "    print(f'[BLEND OOF] selected AUC={best_auc:.6f} (k={best_k})')\n",
        "\n",
        "    # Test blend: use FT submission probs as FT logits via inverse sigmoid; compute pre-FT logits via fast inference @320 (no TTA) and blend\n",
        "    sub_ft = pd.read_csv('submission.csv')  # current file from FT run\n",
        "    ft_probs = sub_ft[LABEL_COLS].values.astype(float)\n",
        "    ft_logits = logit(ft_probs)\n",
        "    t0 = time.time()\n",
        "    pre_logits = infer_test_preft_logits(img_size=320, batch_size=8, use_tta=False)\n",
        "    print(f'[BLEND TEST] Pre-FT inference done in {time.time()-t0:.1f}s')\n",
        "    blend_logits = ft_weight * ft_logits + (1.0 - ft_weight) * pre_logits\n",
        "    blend_probs = 1.0 / (1.0 + np.exp(-blend_logits))\n",
        "    if best_k is not None:\n",
        "        blend_probs = apply_md_rule_np(blend_probs, best_k)\n",
        "    sub = pd.read_csv('sample_submission.csv')\n",
        "    sub[LABEL_COLS] = blend_probs\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('[BLEND TEST] Saved submission.csv | OOF AUC:', best_auc, '| k:', best_k)\n",
        "\n",
        "run_fast_blend(ft_weight=0.7)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BLEND OOF] base AUC=0.964111 | per-col=[0.9929541968003507, 0.8920116662247641, 0.9916535461993498, 0.9798231151278294]\n[BLEND OOF] k=0.7 -> AUC=0.960181 | per-col=[0.9929541968003507, 0.8762925646755805, 0.9916535461993498, 0.9798231151278294]\n[BLEND OOF] k=0.8 -> AUC=0.958683 | per-col=[0.9929541968003507, 0.8703003674103255, 0.9916535461993498, 0.9798231151278294]\n[BLEND OOF] k=0.9 -> AUC=0.957092 | per-col=[0.9929541968003507, 0.8639369720843906, 0.9916535461993498, 0.9798231151278294]\n[BLEND OOF] selected AUC=0.964111 (k=None)\n[BLEND TEST] Pre-FT fold 0 inference @ 320 | bs=8 | TTA=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name tf_efficientnet_b2_ns to current tf_efficientnet_b2.ns_jft_in1k.\n  model = create_fn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_1509/3635334111.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  ckpt = torch.load(f'fold{fold}_best.pt', map_location=DEVICE)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - fold 0 batch 10/23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - fold 0 batch 20/23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BLEND TEST] Fold 0 done in 5.7s | saved test_logits_preft_fold0_320.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BLEND TEST] Pre-FT fold 1 inference @ 320 | bs=8 | TTA=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - fold 1 batch 10/23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - fold 1 batch 20/23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BLEND TEST] Fold 1 done in 4.7s | saved test_logits_preft_fold1_320.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BLEND TEST] Pre-FT fold 2 inference @ 320 | bs=8 | TTA=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - fold 2 batch 10/23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - fold 2 batch 20/23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BLEND TEST] Fold 2 done in 4.8s | saved test_logits_preft_fold2_320.npy\n[BLEND TEST] Pre-FT fold 3 inference @ 320 | bs=8 | TTA=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - fold 3 batch 10/23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - fold 3 batch 20/23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BLEND TEST] Fold 3 done in 4.8s | saved test_logits_preft_fold3_320.npy\n[BLEND TEST] Pre-FT fold 4 inference @ 320 | bs=8 | TTA=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - fold 4 batch 10/23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - fold 4 batch 20/23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BLEND TEST] Fold 4 done in 4.9s | saved test_logits_preft_fold4_320.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BLEND TEST] Pre-FT inference done in 25.9s\n[BLEND TEST] Saved submission.csv | OOF AUC: 0.9641106310880734 | k: None\n"
          ]
        }
      ]
    },
    {
      "id": "e241852e-3533-4222-9acb-7d9f54dd126f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# T8: Quick OOF-only blend check (no inference) to validate uplift\n",
        "import pandas as pd, numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "LABEL_COLS = ['healthy','multiple_diseases','rust','scab']\n",
        "\n",
        "def logit(p, eps=1e-6):\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p / (1 - p))\n",
        "\n",
        "def compute_auc(y_true, y_pred):\n",
        "    aucs = []\n",
        "    for i in range(y_true.shape[1]):\n",
        "        try:\n",
        "            aucs.append(roc_auc_score(y_true[:, i], y_pred[:, i]))\n",
        "        except Exception:\n",
        "            aucs.append(np.nan)\n",
        "    return float(np.nanmean(aucs)), aucs\n",
        "\n",
        "def apply_md_rule_np(probs, k):\n",
        "    res = probs.copy()\n",
        "    md = LABEL_COLS.index('multiple_diseases')\n",
        "    rust = LABEL_COLS.index('rust')\n",
        "    scab = LABEL_COLS.index('scab')\n",
        "    res[:, md] = np.maximum(res[:, md], k * res[:, rust] * res[:, scab])\n",
        "    return res\n",
        "\n",
        "df = pd.read_csv('train_folds.csv')\n",
        "y_true = df[LABEL_COLS].values.astype(float)\n",
        "oof_pre = pd.read_csv('oof_preds.csv')[LABEL_COLS].values.astype(float)\n",
        "oof_ft = pd.read_csv('oof_preds_ft.csv')[LABEL_COLS].values.astype(float)\n",
        "logits_pre = logit(oof_pre)\n",
        "logits_ft = logit(oof_ft)\n",
        "for w in [0.5, 0.6, 0.7, 0.8]:\n",
        "    logits_blend = w * logits_ft + (1.0 - w) * logits_pre\n",
        "    oof_blend = 1.0 / (1.0 + np.exp(-logits_blend))\n",
        "    base_auc, base_cols = compute_auc(y_true, oof_blend)\n",
        "    print(f'[OOF BLEND] w_ft={w:.1f} -> AUC={base_auc:.6f} | per-col={base_cols}')\n",
        "    best_auc, best_k = base_auc, None\n",
        "    for k in [0.7, 0.8, 0.9]:\n",
        "        mod = apply_md_rule_np(oof_blend, k)\n",
        "        auc, cols = compute_auc(y_true, mod)\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_k = auc, k\n",
        "    print(f'   -> with MD rule best AUC={best_auc:.6f} (k={best_k})')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OOF BLEND] w_ft=0.5 -> AUC=0.962927 | per-col=[0.9930911680911682, 0.8889436006211887, 0.9914314861162359, 0.9782422108234272]\n   -> with MD rule best AUC=0.962927 (k=None)\n[OOF BLEND] w_ft=0.6 -> AUC=0.963633 | per-col=[0.9930637738330046, 0.8907541381008295, 0.991603457458798, 0.9791106904412962]\n   -> with MD rule best AUC=0.963633 (k=None)\n[OOF BLEND] w_ft=0.7 -> AUC=0.964111 | per-col=[0.9929541968003507, 0.8920116662247641, 0.9916535461993498, 0.9798231151278294]\n   -> with MD rule best AUC=0.964111 (k=None)\n[OOF BLEND] w_ft=0.8 -> AUC=0.964442 | per-col=[0.9928044415223902, 0.8929813264649066, 0.9915917700860024, 0.9803913586278021]\n   -> with MD rule best AUC=0.964442 (k=None)\n"
          ]
        }
      ]
    },
    {
      "id": "e6a2952a-6ccd-4e5e-8573-4eb976f964b4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# T9: Heuristic post-processing search on multiple_diseases to boost OOF; apply best rule to submission.csv\n",
        "import pandas as pd, numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "LABEL_COLS = ['healthy','multiple_diseases','rust','scab']\n",
        "\n",
        "def compute_auc(y_true, y_pred):\n",
        "    aucs = []\n",
        "    for i in range(y_true.shape[1]):\n",
        "        try:\n",
        "            aucs.append(roc_auc_score(y_true[:, i], y_pred[:, i]))\n",
        "        except Exception:\n",
        "            aucs.append(np.nan)\n",
        "    return float(np.nanmean(aucs)), aucs\n",
        "\n",
        "df = pd.read_csv('train_folds.csv')\n",
        "y_true = df[LABEL_COLS].values.astype(float)\n",
        "oof_ft = pd.read_csv('oof_preds_ft.csv')[LABEL_COLS].values.astype(float)\n",
        "\n",
        "def apply_rule(probs, rule, k):\n",
        "    p = probs.copy()\n",
        "    h = p[:, LABEL_COLS.index('healthy')]\n",
        "    md = p[:, LABEL_COLS.index('multiple_diseases')]\n",
        "    r = p[:, LABEL_COLS.index('rust')]\n",
        "    s = p[:, LABEL_COLS.index('scab')]\n",
        "    if rule == 'k_r_s':\n",
        "        new_md = np.maximum(md, k * r * s)\n",
        "    elif rule == 'k_max_rs_1mh':\n",
        "        new_md = np.maximum(md, k * np.maximum(r, s) * (1.0 - h))\n",
        "    elif rule == 'k_mean_rs_1mh':\n",
        "        new_md = np.maximum(md, k * (0.5 * (r + s)) * (1.0 - h))\n",
        "    elif rule == 'k_rs_1mh':\n",
        "        new_md = np.maximum(md, k * r * s * (1.0 - h))\n",
        "    elif rule == 'k_sum_rs_min1':\n",
        "        new_md = np.maximum(md, k * np.minimum(1.0, r + s) * (1.0 - h))\n",
        "    else:\n",
        "        new_md = md\n",
        "    p[:, LABEL_COLS.index('multiple_diseases')] = new_md\n",
        "    return p\n",
        "\n",
        "base_auc, base_cols = compute_auc(y_true, oof_ft)\n",
        "print(f'[PP SEARCH] Base FT OOF AUC={base_auc:.6f} | per-col={base_cols}')\n",
        "rules = ['k_r_s', 'k_max_rs_1mh', 'k_mean_rs_1mh', 'k_rs_1mh', 'k_sum_rs_min1']\n",
        "ks = [0.3, 0.5, 0.7, 0.8, 0.9, 1.0, 1.2]\n",
        "best_auc = base_auc; best_rule=None; best_k=None; best_probs = oof_ft\n",
        "for rule in rules:\n",
        "    for k in ks:\n",
        "        mod = apply_rule(oof_ft, rule, k)\n",
        "        auc, cols = compute_auc(y_true, mod)\n",
        "        print(f'[PP SEARCH] rule={rule} k={k} -> AUC={auc:.6f} | per-col={cols}')\n",
        "        if auc > best_auc:\n",
        "            best_auc, best_rule, best_k, best_probs = auc, rule, k, mod\n",
        "print(f'[PP SEARCH] Selected AUC={best_auc:.6f} | rule={best_rule} | k={best_k}')\n",
        "pd.DataFrame(best_probs, columns=LABEL_COLS).to_csv('oof_preds_ft_pp.csv', index=False)\n",
        "\n",
        "# Apply best rule to submission.csv if any improvement found\n",
        "sub = pd.read_csv('submission.csv')\n",
        "test_probs = sub[LABEL_COLS].values.astype(float)\n",
        "if best_rule is not None:\n",
        "    test_probs_pp = apply_rule(test_probs, best_rule, best_k)\n",
        "    sub[LABEL_COLS] = np.clip(test_probs_pp, 1e-6, 1-1e-6)\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('[PP APPLY] Applied post-processing to submission.csv with', best_rule, best_k)\n",
        "else:\n",
        "    print('[PP APPLY] No OOF improvement from rules; submission.csv left unchanged.')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PP SEARCH] Base FT OOF AUC=0.964544 | per-col=[0.992190810139528, 0.8935646377031172, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_r_s k=0.3 -> AUC=0.964292 | per-col=[0.992190810139528, 0.8925571001098443, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_r_s k=0.5 -> AUC=0.962635 | per-col=[0.992190810139528, 0.8859285633119958, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_r_s k=0.7 -> AUC=0.960187 | per-col=[0.992190810139528, 0.8761334797924322, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_r_s k=0.8 -> AUC=0.958779 | per-col=[0.992190810139528, 0.8705049051172304, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_r_s k=0.9 -> AUC=0.957408 | per-col=[0.992190810139528, 0.8650202643839249, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_r_s k=1.0 -> AUC=0.956041 | per-col=[0.992190810139528, 0.8595507745918717, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_r_s k=1.2 -> AUC=0.952778 | per-col=[0.992190810139528, 0.8464982387030794, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_max_rs_1mh k=0.3 -> AUC=0.942848 | per-col=[0.992190810139528, 0.8067800462103708, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_max_rs_1mh k=0.5 -> AUC=0.923214 | per-col=[0.992190810139528, 0.728245142229461, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_max_rs_1mh k=0.7 -> AUC=0.905160 | per-col=[0.992190810139528, 0.6560281807507291, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_max_rs_1mh k=0.8 -> AUC=0.898715 | per-col=[0.992190810139528, 0.6302488542100678, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_max_rs_1mh k=0.9 -> AUC=0.889098 | per-col=[0.992190810139528, 0.5917806143706678, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_max_rs_1mh k=1.0 -> AUC=0.877820 | per-col=[0.992190810139528, 0.546668686792167, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_max_rs_1mh k=1.2 -> AUC=0.861376 | per-col=[0.992190810139528, 0.48089087534563085, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_mean_rs_1mh k=0.3 -> AUC=0.954971 | per-col=[0.992190810139528, 0.8552706336881177, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_mean_rs_1mh k=0.5 -> AUC=0.945948 | per-col=[0.992190810139528, 0.8191810916253172, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_mean_rs_1mh k=0.7 -> AUC=0.933576 | per-col=[0.992190810139528, 0.7696905420249234, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_mean_rs_1mh k=0.8 -> AUC=0.928654 | per-col=[0.992190810139528, 0.7500018938676565, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_mean_rs_1mh k=0.9 -> AUC=0.923194 | per-col=[0.992190810139528, 0.7281618120525739, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_mean_rs_1mh k=1.0 -> AUC=0.916976 | per-col=[0.992190810139528, 0.7032915419870459, 0.9911960690356414, 0.9812259132605982]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PP SEARCH] rule=k_mean_rs_1mh k=1.2 -> AUC=0.907484 | per-col=[0.992190810139528, 0.6653232832089694, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_rs_1mh k=0.3 -> AUC=0.964249 | per-col=[0.992190810139528, 0.8923828642854438, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_rs_1mh k=0.5 -> AUC=0.963088 | per-col=[0.992190810139528, 0.8877391007916366, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_rs_1mh k=0.7 -> AUC=0.961012 | per-col=[0.992190810139528, 0.8794363849854172, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_rs_1mh k=0.8 -> AUC=0.959874 | per-col=[0.992190810139528, 0.8748835271391237, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_rs_1mh k=0.9 -> AUC=0.958649 | per-col=[0.992190810139528, 0.8699821976440287, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_rs_1mh k=1.0 -> AUC=0.957439 | per-col=[0.992190810139528, 0.8651414719139428, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_rs_1mh k=1.2 -> AUC=0.954838 | per-col=[0.992190810139528, 0.85474035074429, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_sum_rs_min1 k=0.3 -> AUC=0.942310 | per-col=[0.992190810139528, 0.8046286125525548, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_sum_rs_min1 k=0.5 -> AUC=0.923660 | per-col=[0.992190810139528, 0.7300253778265975, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_sum_rs_min1 k=0.7 -> AUC=0.906478 | per-col=[0.992190810139528, 0.6613007083065034, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_sum_rs_min1 k=0.8 -> AUC=0.899547 | per-col=[0.992190810139528, 0.6335744858149313, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_sum_rs_min1 k=0.9 -> AUC=0.885110 | per-col=[0.992190810139528, 0.5758266732320746, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_sum_rs_min1 k=1.0 -> AUC=0.871684 | per-col=[0.992190810139528, 0.522124161963562, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] rule=k_sum_rs_min1 k=1.2 -> AUC=0.859639 | per-col=[0.992190810139528, 0.4739441687814856, 0.9911960690356414, 0.9812259132605982]\n[PP SEARCH] Selected AUC=0.964544 | rule=None | k=None\n[PP APPLY] No OOF improvement from rules; submission.csv left unchanged.\n"
          ]
        }
      ]
    },
    {
      "id": "f6fd0ca9-7784-4f71-8995-0e59e1a4066e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# T10: Minimal helpers for inference (LeafDataset, Model) to support fast blend without re-running training cell\n",
        "import numpy as np, torch\n",
        "from torch.utils.data import Dataset\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import timm\n",
        "\n",
        "IMG_DIR = Path('images')\n",
        "LABEL_COLS = ['healthy','multiple_diseases','rust','scab']\n",
        "\n",
        "class LeafDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transforms=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = Path(img_dir)\n",
        "        self.transforms = transforms\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = self.img_dir / f\"{row.image_id}.jpg\"\n",
        "        img = np.array(Image.open(img_path).convert('RGB'))\n",
        "        if self.transforms:\n",
        "            img = self.transforms(image=img)['image']\n",
        "        return img, row.image_id\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, backbone='tf_efficientnet_b2_ns', n_out=4):\n",
        "        super().__init__()\n",
        "        self.net = timm.create_model(backbone, pretrained=False, num_classes=n_out, in_chans=3)\n",
        "        if hasattr(self.net, 'set_grad_checkpointing'):\n",
        "            self.net.set_grad_checkpointing(True)\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "execution_count": 4,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}