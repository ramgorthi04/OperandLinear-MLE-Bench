{
  "cells": [
    {
      "id": "130b98bc-25ab-4e85-a0a3-5bdbcfed9c61",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Production Final Plan\n",
        "\n",
        "Goal: Achieve a medal via a clean, alignment-safe ensemble using only the strongest, well-aligned streams and robust consensus rules.\n",
        "\n",
        "Key assets (alignment-safe only):\n",
        "- submission_charfusion_512_single_seed_alignsafe.csv\n",
        "- submission_tokendp_512.csv  (improved Token-DP from Cell 24)\n",
        "- submission_tokenselect_512single_or_384_lambda012.csv\n",
        "- Optional reference: submission_zeroshot_xlmr_squad2.csv for ultra-safe micro-merge (low weight, override only when high-agreement and short).\n",
        "\n",
        "Do NOT use misaligned artifacts (e.g., 3-seed 512 averaged logits). Exclude 384-only from HI pool unless experts insist.\n",
        "\n",
        "Ensemble variants to implement (fast, deterministic, alignment-safe):\n",
        "1) Per-language consensus (base):\n",
        "   - Candidates: {charfusion_512_single_seed_alignsafe, tokendp_512, tokenselect_512-or-384}.\n",
        "   - Voting: mean Jaccard (punct-insensitive), with per-language length regularization.\n",
        "   - Length regularization (initial): lambda_hi=0.02, lambda_ta=0.045 (tunable).\n",
        "   - Overrides:\n",
        "     * Numeric/date micro-override if all candidates numeric/date-like and len <= cap (HI<=18, TA<=16).\n",
        "     * Tie rules: if A != B and norm_text(A) == norm_text(B) then pick B; if TS==TD, tie-break preference order (tokenselect > tokendp > charfusion).\n",
        "     * Per-language confidence gate: if consensus score < thresh_lang, fall back to Token-DP (thresh_hi ~= 0.35, thresh_ta ~= 0.38; tunable).\n",
        "\n",
        "2) Per-id hedge:\n",
        "   - Compute consensus as in (1) and keep Token-DP as backup.\n",
        "   - If per-id confidence < CONF_THRESH_LANG, use Token-DP; else use consensus.\n",
        "   - Apply the same tie-break and numeric/date overrides.\n",
        "\n",
        "3) Meta-hedge:\n",
        "   - Combine outputs from (1), (2), and pure Token-DP.\n",
        "   - If answers are text-normalization-equal, snap to the more confident candidate; otherwise pick majority; break ties with per-language length reg.\n",
        "\n",
        "Diagnostics for each run:\n",
        "- mean_len overall and by language, histogram tail checks.\n",
        "- Route percentages (% handled by consensus vs token-DP, % numeric overrides).\n",
        "- Sanity: no empty strings unless CLS intended; print 10 random samples per language.\n",
        "- Performance hygiene: no long loops without logging; always print elapsed time.\n",
        "\n",
        "Submission flow:\n",
        "- Generate submission files for (1), (2), and Meta-hedge (3).\n",
        "- Point submission.csv to best candidate and submit.\n",
        "\n",
        "Open questions for experts:\n",
        "- Confirm final candidate set (drop 384-only in HI?).\n",
        "- Recommended per-language lambdas and confidence thresholds to stabilize mean_len ~10.6\u201310.9.\n",
        "- Any extra micro-rules (e.g., safe containment, digit normalization on TA only, punctuation snapping) that improved late LB.\n",
        "- Known problematic patterns to avoid (e.g., length drift triggers) and exact preferred tiebreak order.\n",
        "- Whether to include ultra-safe zero-shot stream as override on high-agreement, short answers.\n",
        "\n",
        "Next steps:\n",
        "- Implement Variant (1) and (2) as small, clean functions; log diagnostics.\n",
        "- Build Meta-hedge (3); evaluate mean_len and routes; prepare for quick submissions."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "57bded31-cc93-4a51-b902-c229e54d71a4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd, numpy as np, unicodedata as ud, re, time, random, os, sys\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "# Locked params from expert advice\n",
        "LAMBDA_HI = 0.020\n",
        "LAMBDA_TA = 0.042\n",
        "GATE_HI = 0.33\n",
        "GATE_TA = 0.37\n",
        "\n",
        "# Filepaths\n",
        "fp_ts = 'submission_tokenselect_512single_or_384_lambda012.csv'\n",
        "fp_td = 'submission_tokendp_512.csv'\n",
        "fp_cf = 'submission_charfusion_512_single_seed_alignsafe.csv'\n",
        "\n",
        "sub_ts = pd.read_csv(fp_ts)\n",
        "sub_td = pd.read_csv(fp_td)\n",
        "sub_cf = pd.read_csv(fp_cf)\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Heuristic language detection (script-based) \u2014 test may lack language column\n",
        "def detect_lang(text):\n",
        "    if not isinstance(text, str):\n",
        "        return 'hi'\n",
        "    return 'ta' if any('\\u0B80' <= c <= '\\u0BFF' for c in text) else 'hi'\n",
        "\n",
        "col = 'question' if 'question' in test_df.columns else ('question_text' if 'question_text' in test_df.columns else ('context' if 'context' in test_df.columns else None))\n",
        "if col:\n",
        "    test_df['language'] = test_df[col].apply(detect_lang)\n",
        "else:\n",
        "    test_df['language'] = 'hi'\n",
        "\n",
        "# Align and sanity\n",
        "for df in (sub_ts, sub_td, sub_cf):\n",
        "    assert 'id' in df.columns and 'PredictionString' in df.columns\n",
        "    df['id'] = df['id'].astype(str)\n",
        "test_df['id'] = test_df['id'].astype(str)\n",
        "\n",
        "# Merge\n",
        "base = test_df[['id']].merge(sub_ts.rename(columns={'PredictionString':'TS'}), on='id', how='left')\\\n",
        "                   .merge(sub_td.rename(columns={'PredictionString':'TD'}), on='id', how='left')\\\n",
        "                   .merge(sub_cf.rename(columns={'PredictionString':'CF'}), on='id', how='left')\n",
        "\n",
        "# Context map for expansion\n",
        "ctx_col = 'context' if 'context' in test_df.columns else None\n",
        "id2ctx = dict(zip(test_df['id'], test_df[ctx_col])) if ctx_col else {}\n",
        "\n",
        "def is_space_or_punct(c):\n",
        "    return c.isspace() or ud.category(c).startswith('P')\n",
        "\n",
        "def expand_in_context(ans, ctx, cap, majority_key):\n",
        "    if not ans or not isinstance(ctx, str) or not ctx:\n",
        "        return ans\n",
        "    i = ctx.find(ans)\n",
        "    if i == -1 or ctx.rfind(ans) != i:\n",
        "        return ans  # require unique occurrence\n",
        "    L0, R0 = i, i + len(ans)\n",
        "    left_adj = (L0 > 0 and not is_space_or_punct(ctx[L0-1]))\n",
        "    right_adj = (R0 < len(ctx) and not is_space_or_punct(ctx[R0]))\n",
        "    if not (left_adj or right_adj):\n",
        "        return ans\n",
        "    L, R = L0, R0\n",
        "    while L > 0 and not is_space_or_punct(ctx[L-1]):\n",
        "        L -= 1\n",
        "    while R < len(ctx) and not is_space_or_punct(ctx[R]):\n",
        "        R += 1\n",
        "    expanded = ctx[L:R]\n",
        "    if ctx.count(expanded) > 1: return ans\n",
        "    if len(expanded) > cap:\n",
        "        return ans\n",
        "    if majority_key and majority_key not in norm_basic(expanded):\n",
        "        return ans\n",
        "    return expanded\n",
        "\n",
        "# Utilities\n",
        "PUNCT_CLASS = ''.join(chr(i) for i in range(sys.maxunicode) if ud.category(chr(i)).startswith('P'))\n",
        "PUNCT_RE = re.compile(f\"[\\s{re.escape(PUNCT_CLASS)}]+\")\n",
        "DANDA = '\\u0964'  # Hindi danda\n",
        "TA_PULLI = '\\u0bcd'  # Tamil pulli/virama\n",
        "DEV_VIRAMA = '\\u094d'\n",
        "\n",
        "def nfc(s):\n",
        "    try:\n",
        "        return ud.normalize('NFC', s)\n",
        "    except Exception:\n",
        "        return s\n",
        "\n",
        "def norm_basic(s):\n",
        "    if not isinstance(s, str):\n",
        "        s = '' if s is np.nan else str(s)\n",
        "    s = nfc(s.strip())\n",
        "    # punctuation-insensitive: collapse punctuation+spaces to single space\n",
        "    s = PUNCT_RE.sub(' ', s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "# Digit maps for HI/TA to ASCII\n",
        "HI_DIGITS = {ord(c): ord('0')+i for i, c in enumerate('\\u0966\\u0967\\u0968\\u0969\\u096a\\u096b\\u096c\\u096d\\u096e\\u096f')}\n",
        "TA_DIGITS = {ord(c): ord('0')+i for i, c in enumerate('\\u0be6\\u0be7\\u0be8\\u0be9\\u0bea\\u0beb\\u0bec\\u0bed\\u0bee\\u0bef')}\n",
        "\n",
        "def ascii_digits(s):\n",
        "    s = s.translate(HI_DIGITS).translate(TA_DIGITS)\n",
        "    return s\n",
        "\n",
        "SEP_RE = re.compile(r'[\\./\\-,:\\s]+')\n",
        "NUM_CHARS_RE = re.compile(r'^[0-9\\-]+$')\n",
        "\n",
        "def is_numeric_or_date_like(s, lang, cap):\n",
        "    if not isinstance(s, str) or not s:\n",
        "        return False\n",
        "    s2 = nfc(s)\n",
        "    s2 = ascii_digits(s2)\n",
        "    # keep only digits and common separators, normalize runs of seps to '-'\n",
        "    s2 = SEP_RE.sub('-', s2).strip('-')\n",
        "    if not s2:\n",
        "        return False\n",
        "    if len(s2) > cap:\n",
        "        return False\n",
        "    return bool(NUM_CHARS_RE.match(s2))\n",
        "\n",
        "def maybe_trim_trailing_mark(ans, candidates_norm):\n",
        "    # minimal snap: trim single trailing danda/virama/pulli only if trimmed normalized form matches another candidate\n",
        "    if not ans:\n",
        "        return ans\n",
        "    trimmed = None\n",
        "    if ans.endswith(DANDA) or ans.endswith(TA_PULLI) or ans.endswith(DEV_VIRAMA):\n",
        "        trimmed = ans[:-1]\n",
        "    if trimmed is not None:\n",
        "        if norm_basic(trimmed) in candidates_norm:\n",
        "            return trimmed\n",
        "    return ans\n",
        "\n",
        "# Priority order\n",
        "PRIORITY = {'TS': 3, 'TD': 2, 'CF': 1}\n",
        "\n",
        "id2lang = dict(zip(test_df['id'], test_df['language']))\n",
        "\n",
        "out = []\n",
        "routes = {'majority':0, 'fallback_td':0, 'numeric_override':0, 'norm_tie':0, 'trim_snap':0, 'expansion':0, 'substr_snap':0}\n",
        "\n",
        "for i, row in base.iterrows():\n",
        "    if (i % 500 == 0) and i:\n",
        "        print(f'Processed {i}/{len(base)} in {time.time()-t0:.1f}s', flush=True)\n",
        "    _id = row['id']\n",
        "    lang = id2lang.get(_id, 'hi')\n",
        "    ts, td, cf = row['TS'] if isinstance(row['TS'], str) else '', row['TD'] if isinstance(row['TD'], str) else '', row['CF'] if isinstance(row['CF'], str) else ''\n",
        "    n_ts, n_td, n_cf = norm_basic(ts), norm_basic(td), norm_basic(cf)\n",
        "\n",
        "    # majority by normalized form\n",
        "    counts = {}\n",
        "    for k, n in (('TS', n_ts), ('TD', n_td), ('CF', n_cf)):\n",
        "        counts.setdefault(n, []).append(k)\n",
        "\n",
        "    chosen = None\n",
        "    chosen_src = None\n",
        "    # any normalized string supported by >=2 sources\n",
        "    majority_key = None\n",
        "    for n, srcs in counts.items():\n",
        "        if len(srcs) >= 2 and n != '':\n",
        "            # pick highest-priority source string among srcs\n",
        "            srcs_sorted = sorted(srcs, key=lambda x: -PRIORITY[x])\n",
        "            top = srcs_sorted[0]\n",
        "            if top == 'TS':\n",
        "                chosen, chosen_src = ts, 'TS'    \n",
        "            elif top == 'TD':\n",
        "                chosen, chosen_src = td, 'TD'\n",
        "            else:\n",
        "                chosen, chosen_src = cf, 'CF'\n",
        "            majority_key = n\n",
        "            routes['majority'] += 1\n",
        "            # Cap-aware tie within majority (expert one-liner)\n",
        "            cap = 18 if lang == 'hi' else 16\n",
        "            if chosen_src == 'TS' and majority_key is not None and len(ts) > cap and len(td) <= cap and n_td == majority_key:\n",
        "                chosen, chosen_src = td, 'TD'\n",
        "            # Extended: TS too long; TD is strict substring under norm\n",
        "            if chosen_src == 'TS' and len(ts) > cap and len(td) <= cap and (n_td and n_td in n_ts and n_td != n_ts): chosen, chosen_src = td, 'TD'\n",
        "            # Tamil snap: trim trailing pulli when it matches majority norm\n",
        "            if lang=='ta' and chosen and chosen.endswith(TA_PULLI) and majority_key and norm_basic(chosen[:-1])==majority_key: chosen=chosen[:-1]\n",
        "            break\n",
        "\n",
        "    if chosen is None:\n",
        "        # no majority: fallback to TD\n",
        "        chosen, chosen_src = td, 'TD'\n",
        "        routes['fallback_td'] += 1\n",
        "\n",
        "    # normalization tie rule: if chosen != some candidate but normalized equals, snap to higher-priority stream's raw string\n",
        "    # compare against TS then CF with priority\n",
        "    if chosen_src != 'TS' and norm_basic(chosen) == n_ts and ts:\n",
        "        chosen, chosen_src = ts, 'TS'\n",
        "        routes['norm_tie'] += 1\n",
        "    elif chosen_src not in ('TS','TD') and norm_basic(chosen) == n_td and td:\n",
        "        # TD outranks CF\n",
        "        chosen, chosen_src = td, 'TD'\n",
        "        routes['norm_tie'] += 1\n",
        "\n",
        "    # minimal punctuation/virama snap if it aligns to another candidate\n",
        "    cand_norm_set = {n_ts, n_td, n_cf}\n",
        "    snapped = maybe_trim_trailing_mark(chosen, cand_norm_set)\n",
        "    if snapped != chosen:\n",
        "        chosen = snapped\n",
        "        routes['trim_snap'] += 1\n",
        "\n",
        "    # numeric/date micro-override last\n",
        "    cap = 18 if lang == 'hi' else 16\n",
        "    if is_numeric_or_date_like(ts, lang, cap) and is_numeric_or_date_like(td, lang, cap) and is_numeric_or_date_like(cf, lang, cap):\n",
        "        chosen, chosen_src = td, 'TD'\n",
        "        routes['numeric_override'] += 1\n",
        "\n",
        "    # Substring safety snaps before expansion\n",
        "    if ctx_col:\n",
        "        ctx = id2ctx.get(_id, '')\n",
        "        if isinstance(ctx, str) and ctx and chosen:\n",
        "            # If chosen not in context but TD is, snap to TD\n",
        "            if ctx.find(chosen) == -1 and td and ctx.find(td) != -1:\n",
        "                chosen, chosen_src = td, 'TD'\n",
        "                routes['substr_snap'] += 1\n",
        "            # Generalized: if chosen not in context but any of TS/TD/CF is present, pick highest-priority present\n",
        "            elif ctx.find(chosen) == -1:\n",
        "                present = []\n",
        "                if ts and ctx.find(ts) != -1: present.append('TS')\n",
        "                if td and ctx.find(td) != -1: present.append('TD')\n",
        "                if cf and ctx.find(cf) != -1: present.append('CF')\n",
        "                if present:\n",
        "                    best = sorted(present, key=lambda x: -PRIORITY[x])[0]\n",
        "                    if best == 'TS':\n",
        "                        chosen, chosen_src = ts, 'TS'\n",
        "                    elif best == 'TD':\n",
        "                        chosen, chosen_src = td, 'TD'\n",
        "                    else:\n",
        "                        chosen, chosen_src = cf, 'CF'\n",
        "                    routes['substr_snap'] += 1\n",
        "\n",
        "    # safe word-boundary expansion on chosen (post-numeric, post-pulli)\n",
        "    if ctx_col:\n",
        "        ctx = id2ctx.get(_id, None)\n",
        "        if chosen and not is_numeric_or_date_like(chosen, lang, cap) and isinstance(ctx, str) and ctx:\n",
        "            expanded = expand_in_context(chosen, ctx, cap, majority_key)\n",
        "            if expanded != chosen:\n",
        "                chosen = expanded\n",
        "                routes['expansion'] += 1\n",
        "\n",
        "    out.append((_id, chosen))\n",
        "\n",
        "sub = pd.DataFrame(out, columns=['id','PredictionString'])\n",
        "\n",
        "# Diagnostics\n",
        "sub['len'] = sub['PredictionString'].astype(str).str.len()\n",
        "df_lang = sub.merge(test_df[['id','language']], on='id', how='left')\n",
        "mean_len = sub['len'].mean()\n",
        "mean_len_hi = df_lang.loc[df_lang['language']=='hi','len'].mean()\n",
        "mean_len_ta = df_lang.loc[df_lang['language']=='ta','len'].mean()\n",
        "print(f'mean_len overall: {mean_len:.2f} | HI: {mean_len_hi:.2f} | TA: {mean_len_ta:.2f}', flush=True)\n",
        "total = len(sub)\n",
        "print('Routes %: ' + ', '.join(f\"{k}={v/total*100:.1f}%\" for k,v in routes.items()), flush=True)\n",
        "\n",
        "# Random samples\n",
        "for lg in ('hi','ta'):\n",
        "    samp = df_lang[df_lang['language']==lg].sample(min(5, (df_lang['language']==lg).sum()), random_state=42)\n",
        "    print(f'\\nSamples {lg}:', flush=True)\n",
        "    for _, r in samp.iterrows():\n",
        "        print(r['id'], '->', r['PredictionString'][:120])\n",
        "\n",
        "out_fp = 'submission_primary_majority_fallback.csv'\n",
        "sub[['id','PredictionString']].to_csv(out_fp, index=False)\n",
        "print('Wrote', out_fp, 'in', f'{time.time()-t0:.1f}s')\n",
        "\n",
        "# Point submission.csv to this output\n",
        "sub[['id','PredictionString']].to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_len overall: 10.79 | HI: 11.01 | TA: 10.14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Routes %: majority=86.6%, fallback_td=13.4%, numeric_override=8.9%, norm_tie=0.0%, trim_snap=0.0%, expansion=1.8%, substr_snap=1.8%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nSamples hi:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9b04631cf -> \u0969\u0966 \u091c\u0928\u0935\u0930\u0940 \u0968\u0966\u0967\u096b\nbe799d365 -> \u092e\u0941\u0902\u092c\u0908, \u092e\u0939\u093e\u0930\u093e\u0937\u094d\u091f\u094d\u0930\n33d679522 -> \u096c\u096a\n8e10fecdf -> \u092e\u093e\u0930\u094d\u0915\u094d\u0938-\u090f\u0902\u0917\u0947\u0932\u094d\u0938\n0c35b67ae -> \u0968\u096e \u0938\u093f\u0924\u092e\u094d\u092c\u0930 \u0967\u096f\u0966\u096d\n\nSamples ta:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b151705b8 -> 1900\n5e1f9bca8 -> \u0bb2\u0bbf\u0bb8\u0bcd\u0baa\u0ba9\u0bcd\n921a348f2 -> 5488\n1df390d9a -> \u0b9a\u0bbe\u0bb0\u0bcd\u0bb2\u0bb8\u0bcd \u0bb0\u0bbe\u0baa\u0bb0\u0bcd\u0b9f\u0bcd \u0b9f\u0bbe\u0bb0\u0bcd\u0bb5\u0bbf\u0ba9\n57a56c43f -> \u0baa\u0bc1\u0bb1\u0ba3\u0bbf\nWrote submission_primary_majority_fallback.csv in 0.3s\nsubmission.csv updated\n"
          ]
        }
      ]
    },
    {
      "id": "d5c7f577-9dea-48e2-a39d-6f22c5404034",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd, numpy as np, unicodedata as ud, re, time, random, sys\n",
        "\n",
        "t1 = time.time()\n",
        "\n",
        "# Locked params\n",
        "LAMBDA_HI = 0.020\n",
        "LAMBDA_TA = 0.032\n",
        "GATE_HI = 0.33\n",
        "GATE_TA = 0.37\n",
        "\n",
        "fp_ts = 'submission_tokenselect_512single_or_384_lambda012.csv'\n",
        "fp_td = 'submission_tokendp_512.csv'\n",
        "fp_cf = 'submission_charfusion_512_single_seed_alignsafe.csv'\n",
        "\n",
        "sub_ts = pd.read_csv(fp_ts)\n",
        "sub_td = pd.read_csv(fp_td)\n",
        "sub_cf = pd.read_csv(fp_cf)\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Heuristic language detection (script-based) \u2014 test may lack language column\n",
        "def detect_lang(text):\n",
        "    if not isinstance(text, str):\n",
        "        return 'hi'\n",
        "    return 'ta' if any('\\u0B80' <= c <= '\\u0BFF' for c in text) else 'hi'\n",
        "\n",
        "col = 'question' if 'question' in test_df.columns else ('question_text' if 'question_text' in test_df.columns else ('context' if 'context' in test_df.columns else None))\n",
        "if col:\n",
        "    test_df['language'] = test_df[col].apply(detect_lang)\n",
        "else:\n",
        "    test_df['language'] = 'hi'\n",
        "\n",
        "for df in (sub_ts, sub_td, sub_cf):\n",
        "    df['id'] = df['id'].astype(str)\n",
        "test_df['id'] = test_df['id'].astype(str)\n",
        "\n",
        "base = test_df[['id']].merge(sub_ts.rename(columns={'PredictionString':'TS'}), on='id', how='left')\\\n",
        "                   .merge(sub_td.rename(columns={'PredictionString':'TD'}), on='id', how='left')\\\n",
        "                   .merge(sub_cf.rename(columns={'PredictionString':'CF'}), on='id', how='left')\n",
        "\n",
        "# Context map for expansion (we will not use expansion in this no-exp variant)\n",
        "ctx_col = 'context' if 'context' in test_df.columns else None\n",
        "id2ctx = dict(zip(test_df['id'], test_df[ctx_col])) if ctx_col else {}\n",
        "\n",
        "# Utilities (reuse from previous cell light version)\n",
        "PUNCT_CLASS = ''.join(chr(i) for i in range(sys.maxunicode) if ud.category(chr(i)).startswith('P'))\n",
        "PUNCT_RE = re.compile(f\"[\\s{re.escape(PUNCT_CLASS)}]+\")\n",
        "DANDA = '\\u0964'\n",
        "TA_PULLI = '\\u0bcd'\n",
        "DEV_VIRAMA = '\\u094d'\n",
        "\n",
        "def nfc(s):\n",
        "    try:\n",
        "        return ud.normalize('NFC', s)\n",
        "    except Exception:\n",
        "        return s\n",
        "\n",
        "def norm_basic(s):\n",
        "    if not isinstance(s, str):\n",
        "        s = ''\n",
        "    s = nfc(s.strip())\n",
        "    s = PUNCT_RE.sub(' ', s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "def jaccard_words(a, b):\n",
        "    a = norm_basic(a)\n",
        "    b = norm_basic(b)\n",
        "    if not a and not b:\n",
        "        return 1.0\n",
        "    A = set(a.split())\n",
        "    B = set(b.split())\n",
        "    if not A and not B:\n",
        "        return 1.0\n",
        "    if not A or not B:\n",
        "        return 0.0\n",
        "    inter = len(A & B)\n",
        "    union = len(A | B)\n",
        "    return inter / union if union else 0.0\n",
        "\n",
        "# Numeric/date helpers\n",
        "HI_DIGITS = {ord(c): ord('0')+i for i, c in enumerate('\\u0966\\u0967\\u0968\\u0969\\u096a\\u096b\\u096c\\u096d\\u096e\\u096f')}\n",
        "TA_DIGITS = {ord(c): ord('0')+i for i, c in enumerate('\\u0be6\\u0be7\\u0be8\\u0be9\\u0bea\\u0beb\\u0bec\\u0bed\\u0bee\\u0bef')}\n",
        "def ascii_digits(s):\n",
        "    return s.translate(HI_DIGITS).translate(TA_DIGITS)\n",
        "SEP_RE = re.compile(r'[\\./\\-,:\\s]+')\n",
        "NUM_CHARS_RE = re.compile(r'^[0-9\\-]+$')\n",
        "def is_numeric_or_date_like(s, lang, cap):\n",
        "    if not isinstance(s, str) or not s:\n",
        "        return False\n",
        "    s2 = ascii_digits(nfc(s))\n",
        "    s2 = SEP_RE.sub('-', s2).strip('-')\n",
        "    if not s2 or len(s2) > cap:\n",
        "        return False\n",
        "    return bool(NUM_CHARS_RE.match(s2))\n",
        "def maybe_trim_trailing_mark(ans, candidates_norm):\n",
        "    if not ans:\n",
        "        return ans\n",
        "    trimmed = None\n",
        "    if ans.endswith(DANDA) or ans.endswith(TA_PULLI) or ans.endswith(DEV_VIRAMA):\n",
        "        trimmed = ans[:-1]\n",
        "    if trimmed is not None and norm_basic(trimmed) in candidates_norm:\n",
        "        return trimmed\n",
        "    return ans\n",
        "\n",
        "# Expansion helpers\n",
        "def is_space_or_punct(c):\n",
        "    return c.isspace() or ud.category(c).startswith('P')\n",
        "\n",
        "# No expansion variant: keep function but do not call it\n",
        "def expand_in_context(ans, ctx, cap):\n",
        "    return ans\n",
        "\n",
        "PRIORITY = {'TS': 3, 'TD': 2, 'CF': 1}\n",
        "id2lang = dict(zip(test_df['id'], test_df['language']))\n",
        "\n",
        "out = []\n",
        "routes = {'consensus_used':0, 'fallback_td':0, 'trim_snap':0, 'numeric_override':0, 'expansion':0, 'ta_boundary_extend':0}\n",
        "\n",
        "for i, row in base.iterrows():\n",
        "    if (i % 500 == 0) and i:\n",
        "        print(f'Processed {i}/{len(base)} in {time.time()-t1:.1f}s', flush=True)\n",
        "    _id = row['id']\n",
        "    lang = id2lang.get(_id, 'hi')\n",
        "    lam = LAMBDA_HI if lang == 'hi' else LAMBDA_TA\n",
        "    gate = GATE_HI if lang == 'hi' else GATE_TA\n",
        "    cap = 18 if lang == 'hi' else 16\n",
        "\n",
        "    cand = {'TS': row['TS'] if isinstance(row['TS'], str) else '',\n",
        "            'TD': row['TD'] if isinstance(row['TD'], str) else '',\n",
        "            'CF': row['CF'] if isinstance(row['CF'], str) else ''}\n",
        "    norm = {k: norm_basic(v) for k, v in cand.items()}\n",
        "\n",
        "    # Majority by normalized text\n",
        "    votes = {}\n",
        "    for k, n in norm.items():\n",
        "        votes.setdefault(n, []).append(k)\n",
        "    majority_norm = None\n",
        "    maj_srcs = []\n",
        "    for n, srcs in votes.items():\n",
        "        if n and len(srcs) >= 2:\n",
        "            majority_norm = n\n",
        "            maj_srcs = sorted(srcs, key=lambda x: -PRIORITY[x])\n",
        "            break\n",
        "\n",
        "    # Score each candidate by mean jaccard vs others minus length penalty\n",
        "    scores = {}\n",
        "    keys = ['TS','TD','CF']\n",
        "    for k in keys:\n",
        "        others = [cand[o] for o in keys if o != k]\n",
        "        jac = 0.0\n",
        "        for o in others:\n",
        "            jac += jaccard_words(cand[k], o)\n",
        "        jac /= 2.0\n",
        "        score = jac - lam * len(cand[k])\n",
        "        scores[k] = score\n",
        "\n",
        "    # Pick consensus candidate\n",
        "    if majority_norm is not None:\n",
        "        best_src = maj_srcs[0]\n",
        "        consensus_ans = cand[best_src]\n",
        "        consensus_score = scores[best_src]\n",
        "    else:\n",
        "        max_score = max(scores.values())\n",
        "        best_srcs = [k for k, v in scores.items() if abs(v - max_score) < 1e-9]\n",
        "        best_src = sorted(best_srcs, key=lambda x: -PRIORITY[x])[0]\n",
        "        consensus_ans = cand[best_src]\n",
        "        consensus_score = scores[best_src]\n",
        "\n",
        "    # Micro post-processing: minimal trailing mark snap and numeric/date override (tight, safe)\n",
        "    snapped = maybe_trim_trailing_mark(consensus_ans, set(norm.values()))\n",
        "    if snapped != consensus_ans:\n",
        "        consensus_ans = snapped\n",
        "        routes['trim_snap'] += 1\n",
        "\n",
        "    # Tight numeric override: require all three numeric-like AND TD ascii form present uniquely with clean boundaries\n",
        "    if is_numeric_or_date_like(cand['TS'], lang, cap) and is_numeric_or_date_like(cand['TD'], lang, cap) and is_numeric_or_date_like(cand['CF'], lang, cap):\n",
        "        if ctx_col:\n",
        "            ctx = id2ctx.get(_id, '')\n",
        "        else:\n",
        "            ctx = ''\n",
        "        if isinstance(ctx, str) and ctx:\n",
        "            td_ascii = ascii_digits(cand['TD'])\n",
        "            st = ctx.find(td_ascii)\n",
        "            if st != -1:\n",
        "                ed = st + len(td_ascii)\n",
        "                left = (st == 0) or is_space_or_punct(ctx[st-1])\n",
        "                right = (ed == len(ctx)) or is_space_or_punct(ctx[ed])\n",
        "                if left and right and len(cand['TD']) <= cap and ctx.count(td_ascii) == 1:\n",
        "                    consensus_ans = cand['TD']\n",
        "                    routes['numeric_override'] += 1\n",
        "\n",
        "    # Tamil boundary-extend (very safe): TA only, short, non-numeric, unique, clean boundaries, cap<=16\n",
        "    if lang == 'ta' and isinstance(consensus_ans, str) and consensus_ans and len(consensus_ans) < 16:\n",
        "        if not is_numeric_or_date_like(consensus_ans, lang, cap):\n",
        "            ctx = id2ctx.get(_id, '') if ctx_col else ''\n",
        "            if isinstance(ctx, str) and ctx:\n",
        "                for k in ['TS', 'TD']:\n",
        "                    v = cand.get(k, '')\n",
        "                    if (v and len(v) > len(consensus_ans) and len(v) <= cap and (consensus_ans in v)):\n",
        "                        st = ctx.find(v)\n",
        "                        if st != -1 and ctx.count(v) == 1:\n",
        "                            ed = st + len(v)\n",
        "                            left_ok = (st == 0) or is_space_or_punct(ctx[st-1])\n",
        "                            right_ok = (ed == len(ctx)) or is_space_or_punct(ctx[ed])\n",
        "                            if left_ok and right_ok:\n",
        "                                consensus_ans = v\n",
        "                                routes['ta_boundary_extend'] += 1\n",
        "                                break\n",
        "\n",
        "    # No expansion in this variant (safer late-LB)\n",
        "    # if ctx_col:\n",
        "    #     ctx = id2ctx.get(_id, None)\n",
        "    #     if consensus_ans and not is_numeric_or_date_like(consensus_ans, lang, cap) and isinstance(ctx, str) and ctx:\n",
        "    #         expanded = expand_in_context(consensus_ans, ctx, cap)\n",
        "    #         if expanded != consensus_ans:\n",
        "    #             consensus_ans = expanded\n",
        "    #             routes['expansion'] += 1\n",
        "\n",
        "    if consensus_score >= gate:\n",
        "        out.append((_id, consensus_ans))\n",
        "        routes['consensus_used'] += 1\n",
        "    else:\n",
        "        out.append((_id, cand['TD']))\n",
        "        routes['fallback_td'] += 1\n",
        "\n",
        "sub = pd.DataFrame(out, columns=['id','PredictionString'])\n",
        "sub['len'] = sub['PredictionString'].astype(str).str.len()\n",
        "df_lang = sub.merge(test_df[['id','language']], on='id', how='left')\n",
        "print(f\"mean_len overall: {sub['len'].mean():.2f} | HI: {df_lang.loc[df_lang['language']=='hi','len'].mean():.2f} | TA: {df_lang.loc[df_lang['language']=='ta','len'].mean():.2f}\")\n",
        "total = len(sub)\n",
        "print('Routes %: ' + ', '.join(f\"{k}={v/total*100:.1f}%\" for k,v in routes.items()), flush=True)\n",
        "\n",
        "out_fp = 'submission_consensus_with_gates_plusmicros_noexp.csv'\n",
        "sub[['id','PredictionString']].to_csv(out_fp, index=False)\n",
        "print('Wrote', out_fp, 'in', f'{time.time()-t1:.1f}s')\n",
        "sub[['id','PredictionString']].to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated \u2192 consensus_with_gates_plusmicros_noexp')"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_len overall: 10.71 | HI: 11.01 | TA: 9.82\nRoutes %: consensus_used=59.8%, fallback_td=40.2%, trim_snap=0.0%, numeric_override=1.8%, expansion=0.0%, ta_boundary_extend=0.0%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_consensus_with_gates_plusmicros_noexp.csv in 0.3s\nsubmission.csv updated \u2192 consensus_with_gates_plusmicros_noexp\n"
          ]
        }
      ]
    },
    {
      "id": "857f2af4-cbb7-4b23-86de-906bcab6c292",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Variant: Majority-fallback, NO expansion, strict caps, substring safety\n",
        "import pandas as pd, numpy as np, unicodedata as ud, re, time, random, sys\n",
        "\n",
        "t2 = time.time()\n",
        "\n",
        "fp_ts = 'submission_tokenselect_512single_or_384_lambda012.csv'\n",
        "fp_td = 'submission_tokendp_512.csv'\n",
        "fp_cf = 'submission_charfusion_512_single_seed_alignsafe.csv'\n",
        "\n",
        "sub_ts = pd.read_csv(fp_ts)\n",
        "sub_td = pd.read_csv(fp_td)\n",
        "sub_cf = pd.read_csv(fp_cf)\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "for df in (sub_ts, sub_td, sub_cf):\n",
        "    df['id'] = df['id'].astype(str)\n",
        "test_df['id'] = test_df['id'].astype(str)\n",
        "\n",
        "def detect_lang(text):\n",
        "    if not isinstance(text, str):\n",
        "        return 'hi'\n",
        "    return 'ta' if any('\\u0B80' <= c <= '\\u0BFF' for c in text) else 'hi'\n",
        "\n",
        "col = 'question' if 'question' in test_df.columns else ('question_text' if 'question_text' in test_df.columns else ('context' if 'context' in test_df.columns else None))\n",
        "if col:\n",
        "    test_df['language'] = test_df[col].apply(detect_lang)\n",
        "else:\n",
        "    test_df['language'] = 'hi'\n",
        "\n",
        "base = test_df[['id']].merge(sub_ts.rename(columns={'PredictionString':'TS'}), on='id', how='left')\\\n",
        "                   .merge(sub_td.rename(columns={'PredictionString':'TD'}), on='id', how='left')\\\n",
        "                   .merge(sub_cf.rename(columns={'PredictionString':'CF'}), on='id', how='left')\n",
        "\n",
        "ctx_col = 'context' if 'context' in test_df.columns else None\n",
        "id2ctx = dict(zip(test_df['id'], test_df[ctx_col])) if ctx_col else {}\n",
        "\n",
        "DANDA='\\u0964'; TA_PULLI='\\u0bcd'; DEV_VIRAMA='\\u094d'\n",
        "PUNCT_CLASS = ''.join(chr(i) for i in range(sys.maxunicode) if ud.category(chr(i)).startswith('P'))\n",
        "PUNCT_RE = re.compile(f\"[\\s{re.escape(PUNCT_CLASS)}]+\")\n",
        "\n",
        "def nfc(s):\n",
        "    try: return ud.normalize('NFC', s)\n",
        "    except Exception: return s\n",
        "\n",
        "def norm_basic(s):\n",
        "    if not isinstance(s, str): s = ''\n",
        "    s = nfc(s.strip())\n",
        "    s = PUNCT_RE.sub(' ', s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "HI_DIGITS = {ord(c): ord('0')+i for i, c in enumerate('\\u0966\\u0967\\u0968\\u0969\\u096a\\u096b\\u096c\\u096d\\u096e\\u096f')}\n",
        "TA_DIGITS = {ord(c): ord('0')+i for i, c in enumerate('\\u0be6\\u0be7\\u0be8\\u0be9\\u0bea\\u0beb\\u0bec\\u0bed\\u0bee\\u0bef')}\n",
        "def ascii_digits(s):\n",
        "    return s.translate(HI_DIGITS).translate(TA_DIGITS)\n",
        "SEP_RE = re.compile(r'[\\./\\-,:\\s]+')\n",
        "NUM_CHARS_RE = re.compile(r'^[0-9\\-]+$')\n",
        "def is_numeric_or_date_like(s, lang, cap):\n",
        "    if not isinstance(s, str) or not s: return False\n",
        "    s2 = ascii_digits(nfc(s)); s2 = SEP_RE.sub('-', s2).strip('-')\n",
        "    if not s2 or len(s2)>cap: return False\n",
        "    return bool(NUM_CHARS_RE.match(s2))\n",
        "\n",
        "def maybe_trim_trailing_mark(ans, candidates_norm):\n",
        "    if not ans: return ans\n",
        "    trimmed = None\n",
        "    if ans.endswith(DANDA) or ans.endswith(TA_PULLI) or ans.endswith(DEV_VIRAMA):\n",
        "        trimmed = ans[:-1]\n",
        "    if trimmed is not None and norm_basic(trimmed) in candidates_norm:\n",
        "        return trimmed\n",
        "    return ans\n",
        "\n",
        "PRIORITY = {'TS':3,'TD':2,'CF':1}\n",
        "id2lang = dict(zip(test_df['id'], test_df['language']))\n",
        "\n",
        "out=[]\n",
        "routes={'majority':0,'fallback_td':0,'numeric_override':0,'norm_tie':0,'trim_snap':0,'cap_swap':0,'substr_snap':0}\n",
        "\n",
        "for i, row in base.iterrows():\n",
        "    if (i % 500 == 0) and i:\n",
        "        print(f'Processed {i}/{len(base)} in {time.time()-t2:.1f}s', flush=True)\n",
        "    _id = row['id']\n",
        "    lang = id2lang.get(_id, 'hi')\n",
        "    cap = 18 if lang=='hi' else 16\n",
        "    ts = row['TS'] if isinstance(row['TS'], str) else ''\n",
        "    td = row['TD'] if isinstance(row['TD'], str) else ''\n",
        "    cf = row['CF'] if isinstance(row['CF'], str) else ''\n",
        "    n_ts, n_td, n_cf = norm_basic(ts), norm_basic(td), norm_basic(cf)\n",
        "\n",
        "    counts = {}\n",
        "    for k, n in (('TS', n_ts), ('TD', n_td), ('CF', n_cf)):\n",
        "        counts.setdefault(n, []).append(k)\n",
        "\n",
        "    chosen=None; chosen_src=None; majority_key=None\n",
        "    for n, srcs in counts.items():\n",
        "        if n and len(srcs)>=2:\n",
        "            top = sorted(srcs, key=lambda x: -PRIORITY[x])[0]\n",
        "            chosen = ts if top=='TS' else (td if top=='TD' else cf)\n",
        "            chosen_src = top\n",
        "            majority_key = n\n",
        "            routes['majority']+=1\n",
        "            break\n",
        "\n",
        "    if chosen is None:\n",
        "        chosen, chosen_src = td, 'TD'\n",
        "        routes['fallback_td']+=1\n",
        "\n",
        "    cand_norm_set = {n_ts, n_td, n_cf}\n",
        "    snapped = maybe_trim_trailing_mark(chosen, cand_norm_set)\n",
        "    if snapped != chosen:\n",
        "        chosen = snapped\n",
        "        routes['trim_snap']+=1\n",
        "\n",
        "    if is_numeric_or_date_like(ts, lang, cap) and is_numeric_or_date_like(td, lang, cap) and is_numeric_or_date_like(cf, lang, cap):\n",
        "        chosen, chosen_src = td, 'TD'\n",
        "        routes['numeric_override']+=1\n",
        "\n",
        "    # Strict cap enforcement by swapping to shorter candidate that matches majority norm or is substring-safe\n",
        "    if len(chosen) > cap:\n",
        "        # prefer TD then TS then CF if within cap and either matches majority norm or is a shorter substring of chosen under normalization\n",
        "        candidates = [('TD', td, n_td), ('TS', ts, n_ts), ('CF', cf, n_cf)]\n",
        "        replaced=False\n",
        "        for src, txt, nrm in candidates:\n",
        "            if txt and len(txt) <= cap:\n",
        "                if (majority_key and nrm == majority_key) or (norm_basic(txt) in norm_basic(chosen)):\n",
        "                    chosen, chosen_src = txt, src\n",
        "                    routes['cap_swap']+=1\n",
        "                    replaced=True\n",
        "                    break\n",
        "        # else keep chosen as-is (avoid truncation not guaranteed in-context)\n",
        "\n",
        "    # Substring safety: ensure output occurs in context when available; if not, prefer TD if it occurs\n",
        "    if ctx_col:\n",
        "        ctx = id2ctx.get(_id, '')\n",
        "        if isinstance(ctx, str) and ctx:\n",
        "            if chosen and ctx.find(chosen) == -1:\n",
        "                if td and ctx.find(td) != -1:\n",
        "                    chosen, chosen_src = td, 'TD'\n",
        "                    routes['substr_snap']+=1\n",
        "\n",
        "    out.append((_id, chosen))\n",
        "\n",
        "sub = pd.DataFrame(out, columns=['id','PredictionString'])\n",
        "sub['len'] = sub['PredictionString'].astype(str).str.len()\n",
        "df_lang = sub.merge(test_df[['id','language']], on='id', how='left')\n",
        "print(f\"mean_len overall: {sub['len'].mean():.2f} | HI: {df_lang.loc[df_lang['language']=='hi','len'].mean():.2f} | TA: {df_lang.loc[df_lang['language']=='ta','len'].mean():.2f}\")\n",
        "total = len(sub)\n",
        "print('Routes %: ' + ', '.join(f\"{k}={v/total*100:.1f}%\" for k,v in routes.items()), flush=True)\n",
        "\n",
        "out_fp = 'submission_primary_majority_noexp_strictcaps.csv'\n",
        "sub[['id','PredictionString']].to_csv(out_fp, index=False)\n",
        "print('Wrote', out_fp, 'in', f'{time.time()-t2:.1f}s')\n",
        "sub[['id','PredictionString']].to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated \u2192 majority_noexp_strictcaps')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_len overall: 10.71 | HI: 11.01 | TA: 9.82\nRoutes %: majority=86.6%, fallback_td=13.4%, numeric_override=8.9%, norm_tie=0.0%, trim_snap=0.0%, cap_swap=0.0%, substr_snap=0.0%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_primary_majority_noexp_strictcaps.csv in 0.3s\nsubmission.csv updated \u2192 majority_noexp_strictcaps\n"
          ]
        }
      ]
    },
    {
      "id": "5dcc0f1d-b680-49ad-8843-14939d04b0d1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Logit-level decoder: XLM-R large (alignment-safe), per-language length prior and caps\n",
        "import numpy as np, pandas as pd, json, time, unicodedata as ud, re, sys\n",
        "\n",
        "t = time.time()\n",
        "LOGITS_DIR = 'xlmr_large_test_logits'  # alignment-safe XLM-R large 384\n",
        "START_GLOB = [f'{LOGITS_DIR}/test_start_logits_f{i}.npy' for i in range(5)]\n",
        "END_GLOB   = [f'{LOGITS_DIR}/test_end_logits_f{i}.npy'   for i in range(5)]\n",
        "OFFSET_FP  = f'{LOGITS_DIR}/test_offset_mapping.npy'\n",
        "EID_FP     = f'{LOGITS_DIR}/test_example_id.json'\n",
        "\n",
        "# Params\n",
        "K = 20  # beam size for starts/ends\n",
        "LAMBDA_HI = 0.020\n",
        "LAMBDA_TA = 0.042\n",
        "CAP_HI = 18\n",
        "CAP_TA = 16\n",
        "BOUNDARY_BONUS = 0.12  # increased bonus per expert nudge\n",
        "\n",
        "# Load test and language detection (script-based)\n",
        "test_df = pd.read_csv('test.csv')\n",
        "def detect_lang(text):\n",
        "    if not isinstance(text, str):\n",
        "        return 'hi'\n",
        "    return 'ta' if any('\\u0B80' <= c <= '\\u0BFF' for c in text) else 'hi'\n",
        "col = 'question' if 'question' in test_df.columns else ('question_text' if 'question_text' in test_df.columns else ('context' if 'context' in test_df.columns else None))\n",
        "if col:\n",
        "    test_df['language'] = test_df[col].apply(detect_lang)\n",
        "else:\n",
        "    test_df['language'] = 'hi'\n",
        "test_df['id'] = test_df['id'].astype(str)\n",
        "id2lang = dict(zip(test_df['id'], test_df['language']))\n",
        "id2ctx = dict(zip(test_df['id'], test_df['context'])) if 'context' in test_df.columns else {}\n",
        "\n",
        "# Load logits and metadata\n",
        "starts = [np.load(fp) for fp in START_GLOB]\n",
        "ends   = [np.load(fp) for fp in END_GLOB]\n",
        "start_logits = np.mean(np.stack(starts, axis=0), axis=0)  # [Nfeat, L]\n",
        "end_logits   = np.mean(np.stack(ends,   axis=0), axis=0)  # [Nfeat, L]\n",
        "offsets = np.load(OFFSET_FP, allow_pickle=True)            # list/array of [L, 2] pairs (object arrays)\n",
        "with open(EID_FP, 'r') as f:\n",
        "    example_ids = json.load(f)  # list of example ids per feature row\n",
        "\n",
        "assert len(start_logits) == len(end_logits) == len(offsets) == len(example_ids), 'Mismatched logits/meta lengths'\n",
        "N = len(example_ids)\n",
        "print(f'Loaded logits: features={N}, seq_len={start_logits.shape[1]}', flush=True)\n",
        "\n",
        "def is_space_or_punct(c):\n",
        "    return c.isspace() or ud.category(c).startswith('P')\n",
        "\n",
        "def ensure_offset_array(off_raw):\n",
        "    # Convert possibly 1D object array of tuples/lists to (L,2) int32 array safely\n",
        "    if isinstance(off_raw, np.ndarray) and off_raw.ndim == 2 and off_raw.shape[1] == 2:\n",
        "        return off_raw.astype(np.int32, copy=False)\n",
        "    # off_raw is likely 1D object array/list of pairs\n",
        "    try:\n",
        "        seq = off_raw.tolist() if hasattr(off_raw, 'tolist') else list(off_raw)\n",
        "    except Exception:\n",
        "        return None\n",
        "    pairs = []\n",
        "    for p in seq:\n",
        "        try:\n",
        "            a = int(p[0]) if (p is not None and len(p) > 0 and p[0] is not None) else 0\n",
        "            b = int(p[1]) if (p is not None and len(p) > 1 and p[1] is not None) else 0\n",
        "        except Exception:\n",
        "            a, b = 0, 0\n",
        "        pairs.append((a, b))\n",
        "    arr = np.asarray(pairs, dtype=np.int32)\n",
        "    if arr.ndim != 2 or arr.shape[1] != 2:\n",
        "        return None\n",
        "    return arr\n",
        "\n",
        "# Decode best span per example id across all feature windows\n",
        "best_by_eid = {}  # eid -> (score, (start_char, end_char))\n",
        "for i in range(N):\n",
        "    if i and (i % 1000 == 0):\n",
        "        print(f'Processed {i}/{N} features in {time.time()-t:.1f}s', flush=True)\n",
        "    eid = str(example_ids[i])\n",
        "    ctx = id2ctx.get(eid, '')\n",
        "    lang = id2lang.get(eid, 'hi')\n",
        "    lam = LAMBDA_HI if lang == 'hi' else LAMBDA_TA\n",
        "    cap = CAP_HI if lang == 'hi' else CAP_TA\n",
        "    off = ensure_offset_array(offsets[i])\n",
        "    if off is None:\n",
        "        continue\n",
        "    s_log = start_logits[i].copy()\n",
        "    e_log = end_logits[i].copy()\n",
        "\n",
        "    # Mask invalid tokens: where offsets are (0,0), and CLS at position 0\n",
        "    valid = (off[:,0] + off[:,1]) > 0\n",
        "    if valid.shape[0] > 0:\n",
        "        valid[0] = False  # drop CLS/no-answer\n",
        "    s_log[~valid] = -1e30\n",
        "    e_log[~valid] = -1e30\n",
        "\n",
        "    # Top-K indices\n",
        "    k_eff = int(min(K, int(valid.sum()))) if valid.ndim == 1 else K\n",
        "    if k_eff <= 0:\n",
        "        continue\n",
        "    try:\n",
        "        s_idx = np.argpartition(-s_log, k_eff)[:k_eff]\n",
        "        e_idx = np.argpartition(-e_log, k_eff)[:k_eff]\n",
        "    except ValueError:\n",
        "        s_idx = np.argsort(-s_log)[:k_eff]\n",
        "        e_idx = np.argsort(-e_log)[:k_eff]\n",
        "\n",
        "    # Evaluate combinations\n",
        "    local_best = None\n",
        "    for si in s_idx:\n",
        "        if si < 0 or si >= valid.shape[0] or not valid[si]:\n",
        "            continue\n",
        "        for ei in e_idx:\n",
        "            if ei < 0 or ei >= valid.shape[0] or not valid[ei] or ei < si:\n",
        "                continue\n",
        "            st_char = int(off[si, 0])\n",
        "            ed_char = int(off[ei, 1])\n",
        "            if ed_char <= st_char:\n",
        "                continue\n",
        "            span_len_chars = ed_char - st_char\n",
        "            if span_len_chars <= 0 or span_len_chars > cap:\n",
        "                continue\n",
        "            score = float(s_log[si] + e_log[ei] - lam * span_len_chars)\n",
        "            # Word-boundary bonus (safe only adds bonus, never penalizes)\n",
        "            if isinstance(ctx, str) and ctx:\n",
        "                Lb = (st_char == 0) or is_space_or_punct(ctx[st_char-1])\n",
        "                Rb = (ed_char >= len(ctx)) or is_space_or_punct(ctx[ed_char:ed_char+1])\n",
        "                if Lb and Rb:\n",
        "                    score += BOUNDARY_BONUS\n",
        "            if (local_best is None) or (score > local_best[0]):\n",
        "                local_best = (score, (st_char, ed_char))\n",
        "\n",
        "    if local_best is None:\n",
        "        continue\n",
        "    # Keep the best across all windows for this example id\n",
        "    if (eid not in best_by_eid) or (local_best[0] > best_by_eid[eid][0]):\n",
        "        best_by_eid[eid] = local_best\n",
        "\n",
        "# Build submission\n",
        "pred_rows = []\n",
        "missing = 0\n",
        "for eid in test_df['id'].astype(str).tolist():\n",
        "    ctx = id2ctx.get(eid, '')\n",
        "    if eid in best_by_eid and isinstance(ctx, str) and ctx:\n",
        "        _, (st_char, ed_char) = best_by_eid[eid]\n",
        "        st_char = max(0, min(len(ctx), int(st_char)))\n",
        "        ed_char = max(st_char, min(len(ctx), int(ed_char)))\n",
        "        ans = ctx[st_char:ed_char]\n",
        "        pred_rows.append((eid, ans))\n",
        "    else:\n",
        "        pred_rows.append((eid, ''))\n",
        "        missing += 1\n",
        "\n",
        "sub = pd.DataFrame(pred_rows, columns=['id','PredictionString'])\n",
        "sub['len'] = sub['PredictionString'].astype(str).str.len()\n",
        "df_lang = sub.merge(test_df[['id','language']], on='id', how='left')\n",
        "print(f\"mean_len overall: {sub['len'].mean():.2f} | HI: {df_lang.loc[df_lang['language']=='hi','len'].mean():.2f} | TA: {df_lang.loc[df_lang['language']=='ta','len'].mean():.2f}\")\n",
        "print('Missing preds:', missing, 'Elapsed:', f'{time.time()-t:.1f}s', flush=True)\n",
        "\n",
        "out_fp = 'submission_xlmr_logitdecoder_lenprior_caps.csv'\n",
        "sub[['id','PredictionString']].to_csv(out_fp, index=False)\n",
        "sub[['id','PredictionString']].to_csv('submission.csv', index=False)\n",
        "print('Wrote', out_fp, 'and updated submission.csv')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded logits: features=1921, seq_len=384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1000/1921 features in 1.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_len overall: 9.65 | HI: 9.98 | TA: 8.68\nMissing preds: 0 Elapsed: 2.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_xlmr_logitdecoder_lenprior_caps.csv and updated submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "e918f437-9976-4bd5-a46f-c8ce5a3d76a5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Logit-level decode for XLM-R and MuRIL separately, then per-id choose higher-scoring span\n",
        "import numpy as np, pandas as pd, json, time, unicodedata as ud, re, sys\n",
        "\n",
        "t = time.time()\n",
        "CAP_HI = 18\n",
        "CAP_TA = 16\n",
        "LAMBDA_HI = 0.020\n",
        "LAMBDA_TA = 0.042\n",
        "K = 20\n",
        "BOUNDARY_BONUS = 0.05\n",
        "\n",
        "test_df = pd.read_csv('test.csv')\n",
        "def detect_lang(text):\n",
        "    if not isinstance(text, str): return 'hi'\n",
        "    return 'ta' if any('\\u0B80' <= c <= '\\u0BFF' for c in text) else 'hi'\n",
        "col = 'question' if 'question' in test_df.columns else ('question_text' if 'question_text' in test_df.columns else ('context' if 'context' in test_df.columns else None))\n",
        "if col: test_df['language'] = test_df[col].apply(detect_lang)\n",
        "else: test_df['language'] = 'hi'\n",
        "test_df['id'] = test_df['id'].astype(str)\n",
        "id2lang = dict(zip(test_df['id'], test_df['language']))\n",
        "id2ctx = dict(zip(test_df['id'], test_df['context'])) if 'context' in test_df.columns else {}\n",
        "\n",
        "def is_space_or_punct(c):\n",
        "    return c.isspace() or ud.category(c).startswith('P')\n",
        "\n",
        "def ensure_offset_array(off_raw):\n",
        "    if isinstance(off_raw, np.ndarray) and off_raw.ndim == 2 and off_raw.shape[1] == 2:\n",
        "        return off_raw.astype(np.int32, copy=False)\n",
        "    try:\n",
        "        seq = off_raw.tolist() if hasattr(off_raw, 'tolist') else list(off_raw)\n",
        "    except Exception:\n",
        "        return None\n",
        "    pairs = []\n",
        "    for p in seq:\n",
        "        try:\n",
        "            a = int(p[0]) if (p is not None and len(p)>0 and p[0] is not None) else 0\n",
        "            b = int(p[1]) if (p is not None and len(p)>1 and p[1] is not None) else 0\n",
        "        except Exception:\n",
        "            a, b = 0, 0\n",
        "        pairs.append((a, b))\n",
        "    arr = np.asarray(pairs, dtype=np.int32)\n",
        "    if arr.ndim != 2 or arr.shape[1] != 2: return None\n",
        "    return arr\n",
        "\n",
        "def decode_dir(logits_dir, name):\n",
        "    starts = [np.load(f'{logits_dir}/test_start_logits_f{i}.npy') for i in range(5)]\n",
        "    ends   = [np.load(f'{logits_dir}/test_end_logits_f{i}.npy') for i in range(5)]\n",
        "    start_logits = np.mean(np.stack(starts, axis=0), axis=0)\n",
        "    end_logits   = np.mean(np.stack(ends,   axis=0), axis=0)\n",
        "    offsets = np.load(f'{logits_dir}/test_offset_mapping.npy', allow_pickle=True)\n",
        "    with open(f'{logits_dir}/test_example_id.json','r') as f:\n",
        "        example_ids = json.load(f)\n",
        "    N = len(example_ids)\n",
        "    assert len(start_logits)==len(end_logits)==len(offsets)==N, f'mismatch {name}'\n",
        "    print(f'[{name}] features={N}, seq_len={start_logits.shape[1]}', flush=True)\n",
        "    best = {}  # eid -> (score, st, ed)\n",
        "    for i in range(N):\n",
        "        if i and (i%1000==0):\n",
        "            print(f'[{name}] {i}/{N} in {time.time()-t:.1f}s', flush=True)\n",
        "        eid = str(example_ids[i])\n",
        "        ctx = id2ctx.get(eid, '')\n",
        "        lang = id2lang.get(eid, 'hi')\n",
        "        lam = LAMBDA_HI if lang=='hi' else LAMBDA_TA\n",
        "        cap = CAP_HI if lang=='hi' else CAP_TA\n",
        "        off = ensure_offset_array(offsets[i])\n",
        "        if off is None: continue\n",
        "        s_log = start_logits[i].copy(); e_log = end_logits[i].copy()\n",
        "        valid = (off[:,0] + off[:,1]) > 0\n",
        "        if valid.shape[0] > 0: valid[0] = False\n",
        "        s_log[~valid] = -1e30; e_log[~valid] = -1e30\n",
        "        k_eff = int(min(K, int(valid.sum()))) if valid.ndim==1 else K\n",
        "        if k_eff <= 0: continue\n",
        "        try:\n",
        "            s_idx = np.argpartition(-s_log, k_eff)[:k_eff]\n",
        "            e_idx = np.argpartition(-e_log, k_eff)[:k_eff]\n",
        "        except ValueError:\n",
        "            s_idx = np.argsort(-s_log)[:k_eff]\n",
        "            e_idx = np.argsort(-e_log)[:k_eff]\n",
        "        local_best = None\n",
        "        for si in s_idx:\n",
        "            if si<0 or si>=valid.shape[0] or not valid[si]: continue\n",
        "            for ei in e_idx:\n",
        "                if ei<0 or ei>=valid.shape[0] or not valid[ei] or ei<si: continue\n",
        "                st_char = int(off[si,0]); ed_char = int(off[ei,1])\n",
        "                if ed_char <= st_char: continue\n",
        "                span_len = ed_char - st_char\n",
        "                if span_len <= 0 or span_len > cap: continue\n",
        "                score = float(s_log[si] + e_log[ei] - lam*span_len)\n",
        "                if isinstance(ctx, str) and ctx:\n",
        "                    Lb = (st_char==0) or is_space_or_punct(ctx[st_char-1])\n",
        "                    Rb = (ed_char>=len(ctx)) or is_space_or_punct(ctx[ed_char:ed_char+1])\n",
        "                    if Lb and Rb: score += BOUNDARY_BONUS\n",
        "                if (local_best is None) or (score>local_best[0]):\n",
        "                    local_best = (score, st_char, ed_char)\n",
        "        if local_best is None: continue\n",
        "        if (eid not in best) or (local_best[0] > best[eid][0]):\n",
        "            best[eid] = local_best\n",
        "    # build predictions and scores\n",
        "    preds = {}; scores = {}\n",
        "    for eid in test_df['id']:\n",
        "        ctx = id2ctx.get(eid, '')\n",
        "        if (eid in best) and isinstance(ctx, str) and ctx:\n",
        "            sc, st, ed = best[eid]\n",
        "            st = max(0, min(len(ctx), int(st))); ed = max(st, min(len(ctx), int(ed)))\n",
        "            preds[eid] = ctx[st:ed]; scores[eid] = float(sc)\n",
        "        else:\n",
        "            preds[eid] = ''; scores[eid] = -1e9\n",
        "    return preds, scores\n",
        "\n",
        "# Decode XLM-R and MuRIL\n",
        "xlmr_preds, xlmr_scores = decode_dir('xlmr_large_test_logits', 'xlmr')\n",
        "muril_preds, muril_scores = decode_dir('muril_large_test_logits', 'muril')\n",
        "\n",
        "# Per-id choose higher scoring; ensure substring validity\n",
        "out_rows = []\n",
        "for eid in test_df['id']:\n",
        "    ctx = id2ctx.get(eid, '')\n",
        "    ax, sx = xlmr_preds.get(eid, ''), xlmr_scores.get(eid, -1e9)\n",
        "    am, sm = muril_preds.get(eid, ''), muril_scores.get(eid, -1e9)\n",
        "    cand = [('xlmr', ax, sx), ('muril', am, sm)]\n",
        "    # prefer answers that occur in context\n",
        "    cand_valid = [c for c in cand if isinstance(ctx, str) and ctx and c[1] and ctx.find(c[1])!=-1]\n",
        "    use = None\n",
        "    if cand_valid:\n",
        "        use = max(cand_valid, key=lambda x: x[2])\n",
        "    else:\n",
        "        use = max(cand, key=lambda x: x[2])\n",
        "    out_rows.append((eid, use[1]))\n",
        "\n",
        "sub = pd.DataFrame(out_rows, columns=['id','PredictionString'])\n",
        "sub['len'] = sub['PredictionString'].astype(str).str.len()\n",
        "df_lang = sub.merge(test_df[['id','language']], on='id', how='left')\n",
        "print(f\"mean_len overall: {sub['len'].mean():.2f} | HI: {df_lang.loc[df_lang['language']=='hi','len'].mean():.2f} | TA: {df_lang.loc[df_lang['language']=='ta','len'].mean():.2f}\")\n",
        "out_fp = 'submission_choose_xlmr_or_muril_logitdecoder.csv'\n",
        "sub[['id','PredictionString']].to_csv(out_fp, index=False)\n",
        "sub[['id','PredictionString']].to_csv('submission.csv', index=False)\n",
        "print('Wrote', out_fp, 'and updated submission.csv in', f'{time.time()-t:.1f}s')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[xlmr] features=1921, seq_len=384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[xlmr] 1000/1921 in 1.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[muril] features=1513, seq_len=384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[muril] 1000/1513 in 3.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_len overall: 9.54 | HI: 9.94 | TA: 8.36\nWrote submission_choose_xlmr_or_muril_logitdecoder.csv and updated submission.csv in 3.7s\n"
          ]
        }
      ]
    },
    {
      "id": "c2afa50e-f7b7-485a-b19b-baf8614981c4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# XLM-R logit decoder: quick lambda sweep to target safer mean lengths\n",
        "import numpy as np, pandas as pd, json, time, unicodedata as ud, re, sys\n",
        "\n",
        "t = time.time()\n",
        "LOGITS_DIR = 'xlmr_large_test_logits'\n",
        "START_GLOB = [f'{LOGITS_DIR}/test_start_logits_f{i}.npy' for i in range(5)]\n",
        "END_GLOB   = [f'{LOGITS_DIR}/test_end_logits_f{i}.npy'   for i in range(5)]\n",
        "OFFSET_FP  = f'{LOGITS_DIR}/test_offset_mapping.npy'\n",
        "EID_FP     = f'{LOGITS_DIR}/test_example_id.json'\n",
        "\n",
        "CAP_HI = 18\n",
        "CAP_TA = 16\n",
        "K = 20\n",
        "BOUNDARY_BONUS = 0.05\n",
        "\n",
        "test_df = pd.read_csv('test.csv')\n",
        "def detect_lang(text):\n",
        "    if not isinstance(text, str):\n",
        "        return 'hi'\n",
        "    return 'ta' if any('\\u0B80' <= c <= '\\u0BFF' for c in text) else 'hi'\n",
        "col = 'question' if 'question' in test_df.columns else ('question_text' if 'question_text' in test_df.columns else ('context' if 'context' in test_df.columns else None))\n",
        "if col:\n",
        "    test_df['language'] = test_df[col].apply(detect_lang)\n",
        "else:\n",
        "    test_df['language'] = 'hi'\n",
        "test_df['id'] = test_df['id'].astype(str)\n",
        "id2lang = dict(zip(test_df['id'], test_df['language']))\n",
        "id2ctx = dict(zip(test_df['id'], test_df['context'])) if 'context' in test_df.columns else {}\n",
        "\n",
        "def is_space_or_punct(c):\n",
        "    return c.isspace() or ud.category(c).startswith('P')\n",
        "\n",
        "def ensure_offset_array(off_raw):\n",
        "    if isinstance(off_raw, np.ndarray) and off_raw.ndim == 2 and off_raw.shape[1] == 2:\n",
        "        return off_raw.astype(np.int32, copy=False)\n",
        "    try:\n",
        "        seq = off_raw.tolist() if hasattr(off_raw, 'tolist') else list(off_raw)\n",
        "    except Exception:\n",
        "        return None\n",
        "    pairs = []\n",
        "    for p in seq:\n",
        "        try:\n",
        "            a = int(p[0]) if (p is not None and len(p) > 0 and p[0] is not None) else 0\n",
        "            b = int(p[1]) if (p is not None and len(p) > 1 and p[1] is not None) else 0\n",
        "        except Exception:\n",
        "            a, b = 0, 0\n",
        "        pairs.append((a, b))\n",
        "    arr = np.asarray(pairs, dtype=np.int32)\n",
        "    if arr.ndim != 2 or arr.shape[1] != 2:\n",
        "        return None\n",
        "    return arr\n",
        "\n",
        "def decode_once(lambda_hi, lambda_ta):\n",
        "    starts = [np.load(fp) for fp in START_GLOB]\n",
        "    ends   = [np.load(fp) for fp in END_GLOB]\n",
        "    start_logits = np.mean(np.stack(starts, axis=0), axis=0)\n",
        "    end_logits   = np.mean(np.stack(ends,   axis=0), axis=0)\n",
        "    offsets = np.load(OFFSET_FP, allow_pickle=True)\n",
        "    with open(EID_FP, 'r') as f:\n",
        "        example_ids = json.load(f)\n",
        "    N = len(example_ids)\n",
        "    best_by_eid = {}\n",
        "    for i in range(N):\n",
        "        eid = str(example_ids[i])\n",
        "        ctx = id2ctx.get(eid, '')\n",
        "        lang = id2lang.get(eid, 'hi')\n",
        "        lam = lambda_hi if lang == 'hi' else lambda_ta\n",
        "        cap = CAP_HI if lang == 'hi' else CAP_TA\n",
        "        off = ensure_offset_array(offsets[i])\n",
        "        if off is None:\n",
        "            continue\n",
        "        s_log = start_logits[i].copy()\n",
        "        e_log = end_logits[i].copy()\n",
        "        valid = (off[:,0] + off[:,1]) > 0\n",
        "        if valid.shape[0] > 0:\n",
        "            valid[0] = False\n",
        "        s_log[~valid] = -1e30\n",
        "        e_log[~valid] = -1e30\n",
        "        k_eff = int(min(K, int(valid.sum()))) if valid.ndim == 1 else K\n",
        "        if k_eff <= 0:\n",
        "            continue\n",
        "        try:\n",
        "            s_idx = np.argpartition(-s_log, k_eff)[:k_eff]\n",
        "            e_idx = np.argpartition(-e_log, k_eff)[:k_eff]\n",
        "        except ValueError:\n",
        "            s_idx = np.argsort(-s_log)[:k_eff]\n",
        "            e_idx = np.argsort(-e_log)[:k_eff]\n",
        "        local_best = None\n",
        "        for si in s_idx:\n",
        "            if si < 0 or si >= valid.shape[0] or not valid[si]:\n",
        "                continue\n",
        "            for ei in e_idx:\n",
        "                if ei < 0 or ei >= valid.shape[0] or not valid[ei] or ei < si:\n",
        "                    continue\n",
        "                st_char = int(off[si,0]); ed_char = int(off[ei,1])\n",
        "                if ed_char <= st_char:\n",
        "                    continue\n",
        "                span_len = ed_char - st_char\n",
        "                if span_len <= 0 or span_len > cap:\n",
        "                    continue\n",
        "                score = float(s_log[si] + e_log[ei] - lam * span_len)\n",
        "                if isinstance(ctx, str) and ctx:\n",
        "                    Lb = (st_char == 0) or is_space_or_punct(ctx[st_char-1])\n",
        "                    Rb = (ed_char >= len(ctx)) or is_space_or_punct(ctx[ed_char:ed_char+1])\n",
        "                    if Lb and Rb:\n",
        "                        score += BOUNDARY_BONUS\n",
        "                if (local_best is None) or (score > local_best[0]):\n",
        "                    local_best = (score, st_char, ed_char)\n",
        "        if local_best is None:\n",
        "            continue\n",
        "        if (eid not in best_by_eid) or (local_best[0] > best_by_eid[eid][0]):\n",
        "            best_by_eid[eid] = local_best\n",
        "    pred_rows = []\n",
        "    for eid in test_df['id']:\n",
        "        ctx = id2ctx.get(eid, '')\n",
        "        if eid in best_by_eid and isinstance(ctx, str) and ctx:\n",
        "            _, st, ed = best_by_eid[eid]\n",
        "            st = max(0, min(len(ctx), int(st))); ed = max(st, min(len(ctx), int(ed)))\n",
        "            pred_rows.append((eid, ctx[st:ed]))\n",
        "        else:\n",
        "            pred_rows.append((eid, ''))\n",
        "    sub = pd.DataFrame(pred_rows, columns=['id','PredictionString'])\n",
        "    sub['len'] = sub['PredictionString'].astype(str).str.len()\n",
        "    df_lang = sub.merge(test_df[['id','language']], on='id', how='left')\n",
        "    mean_all = sub['len'].mean(); mean_hi = df_lang.loc[df_lang['language']=='hi','len'].mean(); mean_ta = df_lang.loc[df_lang['language']=='ta','len'].mean()\n",
        "    return sub[['id','PredictionString']], mean_all, mean_hi, mean_ta\n",
        "\n",
        "grid = [\n",
        "    (0.015, 0.038),\n",
        "    (0.018, 0.040),\n",
        "    (0.020, 0.042),\n",
        "    (0.022, 0.045),\n",
        "    (0.025, 0.048),\n",
        "]\n",
        "\n",
        "for lam_hi, lam_ta in grid:\n",
        "    sub, m_all, m_hi, m_ta = decode_once(lam_hi, lam_ta)\n",
        "    out_fp = f'submission_xlmr_logitdecoder_lenprior_caps_hi{lam_hi:.3f}_ta{lam_ta:.3f}.csv'\n",
        "    sub.to_csv(out_fp, index=False)\n",
        "    print(f'lambda_hi={lam_hi:.3f} lambda_ta={lam_ta:.3f} -> mean_len: all {m_all:.2f} | HI {m_hi:.2f} | TA {m_ta:.2f} -> wrote {out_fp}', flush=True)\n",
        "\n",
        "print('Lambda sweep done in', f'{time.time()-t:.1f}s')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lambda_hi=0.015 lambda_ta=0.038 -> mean_len: all 9.65 | HI 9.98 | TA 8.68 -> wrote submission_xlmr_logitdecoder_lenprior_caps_hi0.015_ta0.038.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lambda_hi=0.018 lambda_ta=0.040 -> mean_len: all 9.65 | HI 9.98 | TA 8.68 -> wrote submission_xlmr_logitdecoder_lenprior_caps_hi0.018_ta0.040.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lambda_hi=0.020 lambda_ta=0.042 -> mean_len: all 9.65 | HI 9.98 | TA 8.68 -> wrote submission_xlmr_logitdecoder_lenprior_caps_hi0.020_ta0.042.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lambda_hi=0.022 lambda_ta=0.045 -> mean_len: all 9.65 | HI 9.98 | TA 8.68 -> wrote submission_xlmr_logitdecoder_lenprior_caps_hi0.022_ta0.045.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lambda_hi=0.025 lambda_ta=0.048 -> mean_len: all 9.65 | HI 9.98 | TA 8.68 -> wrote submission_xlmr_logitdecoder_lenprior_caps_hi0.025_ta0.048.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lambda sweep done in 10.1s\n"
          ]
        }
      ]
    },
    {
      "id": "b3c97b9f-62ff-4fc2-b9d2-ed327186ec51",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Improved XLM-R logit decoder: start-only top-K, forward end scan, per-feature z-norm so length prior bites; do not touch submission.csv\n",
        "import numpy as np, pandas as pd, json, time, unicodedata as ud, re, sys\n",
        "\n",
        "t = time.time()\n",
        "LOGITS_DIR = 'xlmr_large_test_logits'\n",
        "START_GLOB = [f'{LOGITS_DIR}/test_start_logits_f{i}.npy' for i in range(5)]\n",
        "END_GLOB   = [f'{LOGITS_DIR}/test_end_logits_f{i}.npy'   for i in range(5)]\n",
        "OFFSET_FP  = f'{LOGITS_DIR}/test_offset_mapping.npy'\n",
        "EID_FP     = f'{LOGITS_DIR}/test_example_id.json'\n",
        "\n",
        "# Per-expert: bump caps slightly to allow useful longer spans without drift\n",
        "CAP_HI = 19\n",
        "CAP_TA = 17\n",
        "K_START = 120  # top-K for starts only; ends are scanned forward until char-cap\n",
        "BOUNDARY_BONUS = 0.12\n",
        "TGT_HI = 11\n",
        "TGT_TA = 10\n",
        "\n",
        "test_df = pd.read_csv('test.csv')\n",
        "def detect_lang(text):\n",
        "    if not isinstance(text, str):\n",
        "        return 'hi'\n",
        "    return 'ta' if any('\\u0B80' <= c <= '\\u0BFF' for c in text) else 'hi'\n",
        "col = 'question' if 'question' in test_df.columns else ('question_text' if 'question_text' in test_df.columns else ('context' if 'context' in test_df.columns else None))\n",
        "if col:\n",
        "    test_df['language'] = test_df[col].apply(detect_lang)\n",
        "else:\n",
        "    test_df['language'] = 'hi'\n",
        "test_df['id'] = test_df['id'].astype(str)\n",
        "id2lang = dict(zip(test_df['id'], test_df['language']))\n",
        "id2ctx = dict(zip(test_df['id'], test_df['context'])) if 'context' in test_df.columns else {}\n",
        "\n",
        "def is_space_or_punct(c):\n",
        "    return c.isspace() or ud.category(c).startswith('P')\n",
        "\n",
        "def ensure_offset_array(off_raw):\n",
        "    if isinstance(off_raw, np.ndarray) and off_raw.ndim == 2 and off_raw.shape[1] == 2:\n",
        "        return off_raw.astype(np.int32, copy=False)\n",
        "    try:\n",
        "        seq = off_raw.tolist() if hasattr(off_raw, 'tolist') else list(off_raw)\n",
        "    except Exception:\n",
        "        return None\n",
        "    pairs = []\n",
        "    for p in seq:\n",
        "        try:\n",
        "            a = int(p[0]) if (p is not None and len(p) > 0 and p[0] is not None) else 0\n",
        "            b = int(p[1]) if (p is not None and len(p) > 1 and p[1] is not None) else 0\n",
        "        except Exception:\n",
        "            a, b = 0, 0\n",
        "        pairs.append((a, b))\n",
        "    arr = np.asarray(pairs, dtype=np.int32)\n",
        "    if arr.ndim != 2 or arr.shape[1] != 2:\n",
        "        return None\n",
        "    return arr\n",
        "\n",
        "def znorm_row(x, valid_mask):\n",
        "    v = x[valid_mask]\n",
        "    if v.size == 0:\n",
        "        return x\n",
        "    m = v.mean()\n",
        "    s = v.std()\n",
        "    y = x.copy()\n",
        "    if (not np.isfinite(s)) or (s < 1e-6):\n",
        "        y[valid_mask] = v - m\n",
        "        return y\n",
        "    y[valid_mask] = (v - m) / (s + 1e-6)\n",
        "    return y\n",
        "\n",
        "def decode_once(lambda_hi, lambda_ta, tag):\n",
        "    starts = [np.load(fp) for fp in START_GLOB]\n",
        "    ends   = [np.load(fp) for fp in END_GLOB]\n",
        "    start_logits = np.mean(np.stack(starts, axis=0), axis=0)\n",
        "    end_logits   = np.mean(np.stack(ends,   axis=0), axis=0)\n",
        "    offsets = np.load(OFFSET_FP, allow_pickle=True)\n",
        "    with open(EID_FP, 'r') as f:\n",
        "        example_ids = json.load(f)\n",
        "    N = len(example_ids)\n",
        "    best_by_eid = {}  # eid -> (score, st_char, ed_char, span_len)\n",
        "    for i in range(N):\n",
        "        if i and (i % 1000 == 0):\n",
        "            print(f'{i}/{N} in {time.time()-t:.1f}s', flush=True)\n",
        "        eid = str(example_ids[i])\n",
        "        ctx = id2ctx.get(eid, '')\n",
        "        lang = id2lang.get(eid, 'hi')\n",
        "        lam = lambda_hi if lang == 'hi' else lambda_ta\n",
        "        tgt = TGT_HI if lang == 'hi' else TGT_TA\n",
        "        cap = CAP_HI if lang == 'hi' else CAP_TA\n",
        "        off = ensure_offset_array(offsets[i])\n",
        "        if off is None:\n",
        "            continue\n",
        "        s_log = start_logits[i].copy(); e_log = end_logits[i].copy()\n",
        "        valid = (off[:,0] + off[:,1]) > 0\n",
        "        if valid.shape[0] > 0:\n",
        "            valid[0] = False  # drop CLS\n",
        "        s_log[~valid] = -1e30; e_log[~valid] = -1e30\n",
        "        # z-normalize over valid tokens so lambda competes with logits\n",
        "        s_log = znorm_row(s_log, valid)\n",
        "        e_log = znorm_row(e_log, valid)\n",
        "        k_eff = int(min(K_START, int(valid.sum()))) if valid.ndim == 1 else K_START\n",
        "        if k_eff <= 0:\n",
        "            continue\n",
        "        try:\n",
        "            s_idx = np.argpartition(-s_log, k_eff)[:k_eff]\n",
        "        except ValueError:\n",
        "            s_idx = np.argsort(-s_log)[:k_eff]\n",
        "        local_best = None\n",
        "        L = valid.shape[0]\n",
        "        for si in s_idx:\n",
        "            if si < 0 or si >= L or not valid[si]:\n",
        "                continue\n",
        "            st_char = int(off[si,0])\n",
        "            # forward scan ends until char-cap exceeded; offsets are monotonic so we can break early\n",
        "            for ei in range(si, L):\n",
        "                if not valid[ei]:\n",
        "                    continue\n",
        "                ed_char = int(off[ei,1])\n",
        "                if ed_char <= st_char:\n",
        "                    continue\n",
        "                span_len = ed_char - st_char\n",
        "                if span_len > cap:\n",
        "                    break\n",
        "                score = float(s_log[si] + e_log[ei] - lam * abs(span_len - tgt))\n",
        "                if isinstance(ctx, str) and ctx:\n",
        "                    Lb = (st_char == 0) or is_space_or_punct(ctx[st_char-1])\n",
        "                    Rb = (ed_char >= len(ctx)) or is_space_or_punct(ctx[ed_char:ed_char+1])\n",
        "                    if Lb and Rb:\n",
        "                        score += BOUNDARY_BONUS\n",
        "                if (local_best is None) or (score > local_best[0]):\n",
        "                    local_best = (score, st_char, ed_char, span_len)\n",
        "        if local_best is None:\n",
        "            continue\n",
        "        if (eid not in best_by_eid) or (local_best[0] > best_by_eid[eid][0]):\n",
        "            best_by_eid[eid] = local_best\n",
        "    pred_rows = []\n",
        "    for eid in test_df['id']:\n",
        "        ctx = id2ctx.get(eid, '')\n",
        "        if (eid in best_by_eid) and isinstance(ctx, str) and ctx:\n",
        "            _, st, ed, _ = best_by_eid[eid]\n",
        "            st = max(0, min(len(ctx), int(st))); ed = max(st, min(len(ctx), int(ed)))\n",
        "            pred_rows.append((eid, ctx[st:ed]))\n",
        "        else:\n",
        "            pred_rows.append((eid, ''))\n",
        "    sub = pd.DataFrame(pred_rows, columns=['id','PredictionString'])\n",
        "    sub['len'] = sub['PredictionString'].astype(str).str.len()\n",
        "    df_lang = sub.merge(test_df[['id','language']], on='id', how='left')\n",
        "    m_all = sub['len'].mean(); m_hi = df_lang.loc[df_lang['language']=='hi','len'].mean(); m_ta = df_lang.loc[df_lang['language']=='ta','len'].mean()\n",
        "    out_fp = f'submission_xlmr_logitdecoder_lenprior_caps_{tag}_scan.csv'\n",
        "    sub[['id','PredictionString']].to_csv(out_fp, index=False)\n",
        "    print(f'{tag}_scan: mean_len all {m_all:.2f} | HI {m_hi:.2f} | TA {m_ta:.2f} -> {out_fp}', flush=True)\n",
        "    return out_fp, m_all, m_hi, m_ta\n",
        "\n",
        "# Per-expert: run caps-bumped settings closest to target means\n",
        "grid = [\n",
        "    (1.4, 1.8, 'hi1.40_ta1.80_caps19_17'),\n",
        "    (1.6, 2.0, 'hi1.60_ta2.00_caps19_17'),\n",
        "]\n",
        "\n",
        "results = []\n",
        "for lam_hi, lam_ta, tag in grid:\n",
        "    fp, m_all, m_hi, m_ta = decode_once(lam_hi, lam_ta, tag)\n",
        "    results.append((fp, m_all))\n",
        "\n",
        "print('Done. Not updating submission.csv. Elapsed:', f'{time.time()-t:.1f}s')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000/1921 in 3.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi1.40_ta1.80_caps19_17_scan: mean_len all 10.57 | HI 10.96 | TA 9.39 -> submission_xlmr_logitdecoder_lenprior_caps_hi1.40_ta1.80_caps19_17_scan.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000/1921 in 10.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi1.60_ta2.00_caps19_17_scan: mean_len all 10.53 | HI 10.90 | TA 9.39 -> submission_xlmr_logitdecoder_lenprior_caps_hi1.60_ta2.00_caps19_17_scan.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. Not updating submission.csv. Elapsed: 12.9s\n"
          ]
        }
      ]
    },
    {
      "id": "f46dbd62-2150-4d90-a879-ec6df66ca1a4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 4-stream no-expansion ensemble simplified: majority over all 4, boundary presence gate, numeric override enabled, substring safety, TA boundary-extend\n",
        "import pandas as pd, numpy as np, unicodedata as ud, re, time, sys, os\n",
        "\n",
        "t4 = time.time()\n",
        "\n",
        "# Inputs\n",
        "fp_ts = 'submission_tokenselect_512single_or_384_lambda012.csv'\n",
        "fp_td = 'submission_tokendp_512.csv'\n",
        "fp_cf = 'submission_charfusion_512_single_seed_alignsafe.csv'\n",
        "# Choose an LD file produced by Cell 7; prefer newest *_caps19_17_scan (z-norm + forward scan) first\n",
        "fp_ld_candidates = [\n",
        "    'submission_xlmr_logitdecoder_lenprior_caps_hi1.40_ta1.80_caps19_17_scan.csv',\n",
        "    'submission_xlmr_logitdecoder_lenprior_caps_hi1.60_ta2.00_caps19_17_scan.csv',\n",
        "    'submission_xlmr_logitdecoder_lenprior_caps_hi1.40_ta1.80_scan.csv',\n",
        "    'submission_xlmr_logitdecoder_lenprior_caps_hi1.60_ta2.00_scan.csv',\n",
        "    'submission_xlmr_logitdecoder_lenprior_caps_hi1.20_ta1.60_scan.csv',\n",
        "    'submission_xlmr_logitdecoder_lenprior_caps_hi1.00_ta1.40_scan.csv'\n",
        "]\n",
        "fp_ld = next((c for c in fp_ld_candidates if os.path.exists(c)), None)\n",
        "if fp_ld is None:\n",
        "    fp_ld = 'submission_xlmr_logitdecoder_lenprior_caps_hi0.020_ta0.042_K200.csv' if os.path.exists('submission_xlmr_logitdecoder_lenprior_caps_hi0.020_ta0.042_K200.csv') else 'submission_xlmr_logitdecoder_lenprior_caps.csv'\n",
        "\n",
        "sub_ts = pd.read_csv(fp_ts)\n",
        "sub_td = pd.read_csv(fp_td)\n",
        "sub_cf = pd.read_csv(fp_cf)\n",
        "sub_ld = pd.read_csv(fp_ld)\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "for df in (sub_ts, sub_td, sub_cf, sub_ld):\n",
        "    assert 'id' in df.columns and 'PredictionString' in df.columns\n",
        "    df['id'] = df['id'].astype(str)\n",
        "test_df['id'] = test_df['id'].astype(str)\n",
        "\n",
        "def detect_lang(text):\n",
        "    if not isinstance(text, str):\n",
        "        return 'hi'\n",
        "    return 'ta' if any('\\u0B80' <= c <= '\\u0BFF' for c in text) else 'hi'\n",
        "col = 'question' if 'question' in test_df.columns else ('question_text' if 'question_text' in test_df.columns else ('context' if 'context' in test_df.columns else None))\n",
        "if col:\n",
        "    test_df['language'] = test_df[col].apply(detect_lang)\n",
        "else:\n",
        "    test_df['language'] = 'hi'\n",
        "\n",
        "base = (test_df[['id']]\n",
        "    .merge(sub_ts.rename(columns={'PredictionString':'TS'}), on='id', how='left')\n",
        "    .merge(sub_ld.rename(columns={'PredictionString':'LD'}), on='id', how='left')\n",
        "    .merge(sub_td.rename(columns={'PredictionString':'TD'}), on='id', how='left')\n",
        "    .merge(sub_cf.rename(columns={'PredictionString':'CF'}), on='id', how='left'))\n",
        "\n",
        "ctx_col = 'context' if 'context' in test_df.columns else None\n",
        "id2ctx = dict(zip(test_df['id'], test_df[ctx_col])) if ctx_col else {}\n",
        "id2lang = dict(zip(test_df['id'], test_df['language']))\n",
        "\n",
        "PUNCT_CLASS = ''.join(chr(i) for i in range(sys.maxunicode) if ud.category(chr(i)).startswith('P'))\n",
        "PUNCT_RE = re.compile(f\"[\\s{re.escape(PUNCT_CLASS)}]+\")\n",
        "DANDA='\\u0964'; TA_PULLI='\\u0bcd'; DEV_VIRAMA='\\u094d'\n",
        "\n",
        "def nfc(s):\n",
        "    try: return ud.normalize('NFC', s)\n",
        "    except Exception: return s\n",
        "\n",
        "def norm_basic(s):\n",
        "    if not isinstance(s, str): s = ''\n",
        "    s = nfc(s.strip())\n",
        "    # remove ZWJ/ZWNJ\n",
        "    s = s.replace('\\u200c','').replace('\\u200d','')\n",
        "    # punctuation+spaces collapse\n",
        "    s = PUNCT_RE.sub(' ', s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "HI_DIGITS = {ord(c): ord('0')+i for i, c in enumerate('\\u0966\\u0967\\u0968\\u0969\\u096a\\u096b\\u096c\\u096d\\u096e\\u096f')}\n",
        "TA_DIGITS = {ord(c): ord('0')+i for i, c in enumerate('\\u0be6\\u0be7\\u0be8\\u0be9\\u0bea\\u0beb\\u0bec\\u0bed\\u0bee\\u0bef')}\n",
        "def ascii_digits(s):\n",
        "    if not isinstance(s, str):\n",
        "        return ''\n",
        "    return s.translate(HI_DIGITS).translate(TA_DIGITS)\n",
        "SEP_RE = re.compile(r'[\\./\\-,:\\s]+')\n",
        "NUM_CHARS_RE = re.compile(r'^[0-9\\-]+$')\n",
        "def is_numeric_or_date_like(s, lang, cap):\n",
        "    if not isinstance(s, str) or not s: return False\n",
        "    s2 = ascii_digits(nfc(s)); s2 = SEP_RE.sub('-', s2).strip('-')\n",
        "    if not s2 or len(s2) > cap: return False\n",
        "    return bool(NUM_CHARS_RE.match(s2))\n",
        "\n",
        "def is_space_or_punct(c):\n",
        "    return c.isspace() or ud.category(c).startswith('P')\n",
        "\n",
        "def is_present_boundary(ans, ctx):\n",
        "    if not (isinstance(ans,str) and ans and isinstance(ctx,str) and ctx): return False\n",
        "    st = ctx.find(ans)\n",
        "    if st == -1: return False\n",
        "    ed = st + len(ans)\n",
        "    left = (st==0) or is_space_or_punct(ctx[st-1])\n",
        "    right = (ed==len(ctx)) or is_space_or_punct(ctx[ed])\n",
        "    return left and right\n",
        "\n",
        "PRIORITY = {'TS':3,'LD':2,'TD':1,'CF':0}\n",
        "\n",
        "out = []\n",
        "routes = {'majority':0,'present_priority':0,'fallback_td':0,'numeric_override':0,'substr_gate':0,'trail_shorter':0,'tamil_digit_norm':0,'ts_ld_flip':0,'tamil_rescue':0,'ta_boundary_extend':0}\n",
        "\n",
        "for i, row in base.iterrows():\n",
        "    if (i % 500 == 0) and i:\n",
        "        print(f'Processed {i}/{len(base)} in {time.time()-t4:.1f}s', flush=True)\n",
        "    _id = row['id']\n",
        "    lang = id2lang.get(_id, 'hi')\n",
        "    cap = 18 if lang=='hi' else 16\n",
        "    # Micro cap-override per expert: TA uses 17\n",
        "    cap_override = 18 if lang=='hi' else 17\n",
        "    ctx = id2ctx.get(_id, '') if ctx_col else ''\n",
        "    ts = row['TS'] if isinstance(row['TS'], str) else ''\n",
        "    ld = row['LD'] if isinstance(row['LD'], str) else ''\n",
        "    td = row['TD'] if isinstance(row['TD'], str) else ''\n",
        "    cf = row['CF'] if isinstance(row['CF'], str) else ''\n",
        "    cand = {'TS':ts,'LD':ld,'TD':td,'CF':cf}\n",
        "    nrm = {k: norm_basic(v) for k,v in cand.items()}\n",
        "\n",
        "    # Majority vote over all four streams (TS, LD, TD, CF) with TA tie pref (LD over TS when norms tie)\n",
        "    counts = {}\n",
        "    for k in ('TS','LD','TD','CF'):\n",
        "        n = nrm[k]\n",
        "        counts.setdefault(n, []).append(k)\n",
        "    chosen = None; chosen_src = None; majority_key = None\n",
        "    for n, srcs in counts.items():\n",
        "        if n and len(srcs) >= 2:\n",
        "            top = sorted(srcs, key=lambda x: -PRIORITY[x])[0]\n",
        "            if lang == 'ta' and ('TS' in srcs) and ('LD' in srcs) and top == 'TS':\n",
        "                top = 'LD'\n",
        "            chosen, chosen_src = cand[top], top\n",
        "            majority_key = n\n",
        "            routes['majority'] += 1\n",
        "            break\n",
        "\n",
        "    # Presence gating: among TS/LD/TD that occur in context, prefer boundary-robust first, then priority TS>LD>TD\n",
        "    if chosen is None:\n",
        "        present = []\n",
        "        if isinstance(ctx,str) and ctx:\n",
        "            for key in ('TS','LD','TD'):\n",
        "                v = cand[key]\n",
        "                if v and ctx.find(v) != -1:\n",
        "                    present.append(key)\n",
        "        if present:\n",
        "            pref = [(key, cand[key], is_present_boundary(cand[key], ctx)) for key in present]\n",
        "            robust = [(key,v) for key,v,rob in pref if rob]\n",
        "            pool = robust if robust else [(key,v) for key,v,_rob in pref]\n",
        "            best = sorted(pool, key=lambda kv: -PRIORITY[kv[0]])[0]\n",
        "            chosen, chosen_src = best[1], best[0]\n",
        "            routes['present_priority'] += 1\n",
        "        else:\n",
        "            # nothing TS/LD/TD in context: allow CF if present, else TD\n",
        "            if isinstance(ctx,str) and ctx and cf and ctx.find(cf) != -1:\n",
        "                chosen, chosen_src = cf, 'CF'\n",
        "                routes['present_priority'] += 1\n",
        "            else:\n",
        "                chosen, chosen_src = td, 'TD'\n",
        "                routes['fallback_td'] += 1\n",
        "\n",
        "    # Strengthened TS->LD flip: rarer and never longer than TS\n",
        "    if chosen_src == 'TS' and isinstance(ld, str) and ld and isinstance(ctx, str) and ctx:\n",
        "        ts_in_ctx = (ctx.find(ts) != -1)\n",
        "        ts_too_long = len(ts) > cap\n",
        "        st_ts = ctx.find(ts); ed_ts = st_ts + len(ts) if st_ts != -1 else -1\n",
        "        ts_left_bad = (st_ts > 0) and (not is_space_or_punct(ctx[st_ts-1]))\n",
        "        ts_right_bad = (0 <= ed_ts < len(ctx)) and (not is_space_or_punct(ctx[ed_ts]))\n",
        "        ts_not_clean = (st_ts == -1) or ts_left_bad or ts_right_bad\n",
        "        ts_bad = ts_too_long or (not ts_in_ctx) or ts_not_clean\n",
        "\n",
        "        st_ld = ctx.find(ld); ed_ld = st_ld + len(ld) if st_ld != -1 else -1\n",
        "        ld_present = (st_ld != -1)\n",
        "        ld_rare = ld_present and (ctx.count(ld) <= 1)\n",
        "        ld_left_ok = (st_ld == 0) or is_space_or_punct(ctx[st_ld-1])\n",
        "        ld_right_ok = (ed_ld == len(ctx)) or (0 <= ed_ld < len(ctx) and is_space_or_punct(ctx[ed_ld]))\n",
        "        ld_clean = ld_present and ld_left_ok and ld_right_ok\n",
        "\n",
        "        if ts_bad and ld_clean and ld_rare and (len(ld) <= cap) and (len(ld) < len(ts)):\n",
        "            chosen, chosen_src = ld, 'LD'\n",
        "            routes['ts_ld_flip'] += 1\n",
        "\n",
        "    # Trailing mark rule when TS and LD normalize equal but raw differ: pick shorter if longer ends with terminal mark\n",
        "    if nrm.get('TS','') and (nrm.get('TS') == nrm.get('LD')) and ts and ld and (ts != ld):\n",
        "        t_short, t_long = (ts, ld) if len(ts) < len(ld) else (ld, ts)\n",
        "        if t_long and (t_long.endswith(DANDA) or t_long.endswith(DEV_VIRAMA) or t_long.endswith(TA_PULLI) or (ud.category(t_long[-1]).startswith('P'))):\n",
        "            if chosen in (ts, ld) and chosen != t_short:\n",
        "                chosen = t_short\n",
        "                routes['trail_shorter'] += 1\n",
        "\n",
        "    # Hindi danda trim: ultra-safe\n",
        "    if lang == 'hi' and isinstance(chosen, str) and chosen.endswith(DANDA) and isinstance(ctx, str) and ctx:\n",
        "        trimmed = chosen[:-1].strip()\n",
        "        if trimmed and is_present_boundary(trimmed, ctx):\n",
        "            chosen = trimmed\n",
        "            routes['trail_shorter'] += 1\n",
        "\n",
        "    # Tamil rescue: stricter to cut overfiring\n",
        "    if lang == 'ta' and isinstance(chosen, str) and chosen and len(chosen) < 5 and isinstance(ts, str) and ts:\n",
        "        if len(ts) <= cap and isinstance(ctx, str) and ctx and ctx.count(ts) == 1:\n",
        "            st = ctx.find(ts)\n",
        "            if st != -1:\n",
        "                ed = st + len(ts)\n",
        "                left = (st == 0) or is_space_or_punct(ctx[st-1])\n",
        "                right = (ed == len(ctx)) or is_space_or_punct(ctx[ed])\n",
        "                if left and right:\n",
        "                    chosen, chosen_src = ts, 'TS'\n",
        "                    routes['tamil_rescue'] += 1\n",
        "\n",
        "    # Numeric/date override (enabled, tight): require TS, LD, TD all numeric-like; pick TD if unique boundary match\n",
        "    if is_numeric_or_date_like(ts, lang, cap_override) and is_numeric_or_date_like(ld, lang, cap_override) and is_numeric_or_date_like(td, lang, cap_override):\n",
        "        if isinstance(ctx,str) and ctx:\n",
        "            td_ascii = ascii_digits(td)\n",
        "            st = ctx.find(td_ascii)\n",
        "            if st != -1:\n",
        "                ed = st + len(td_ascii)\n",
        "                left = (st==0) or is_space_or_punct(ctx[st-1])\n",
        "                right = (ed==len(ctx)) or is_space_or_punct(ctx[ed])\n",
        "                if left and right and len(td) <= cap_override and ctx.count(td_ascii) <= 2:\n",
        "                    chosen, chosen_src = td, 'TD'\n",
        "                    routes['numeric_override'] += 1\n",
        "\n",
        "    # Tamil boundary-extend (guarded, safer): literal containment, unique occurrence, +1 growth, clean boundaries, cap_override\n",
        "    if lang == 'ta' and isinstance(chosen, str) and chosen and isinstance(ctx, str) and ctx and len(chosen) < cap_override:\n",
        "        if not is_numeric_or_date_like(chosen, lang, cap_override):\n",
        "            choices = []\n",
        "            for k in ('TS','LD','TD'):\n",
        "                v = cand.get(k, '')\n",
        "                if not v or len(v) <= len(chosen) or len(v) > cap_override:\n",
        "                    continue\n",
        "                if len(v) > len(chosen) + 1:\n",
        "                    continue\n",
        "                if ctx.count(v) != 1:\n",
        "                    continue\n",
        "                if not is_present_boundary(v, ctx):\n",
        "                    continue\n",
        "                if chosen not in v:\n",
        "                    continue\n",
        "                choices.append((len(v), PRIORITY.get(k,0), k, v))\n",
        "            if choices:\n",
        "                choices.sort()\n",
        "                _lv, _pr, k_best, v_best = choices[-1]\n",
        "                chosen, chosen_src = v_best, k_best\n",
        "                routes['ta_boundary_extend'] += 1\n",
        "\n",
        "    # Substring safety: if chosen not present but some others are, snap to highest-priority present among TS/LD/TD, else CF, else TD\n",
        "    if isinstance(ctx,str) and ctx and chosen:\n",
        "        if ctx.find(chosen) == -1:\n",
        "            present = []\n",
        "            for key in ('TS','LD','TD'):\n",
        "                v = cand[key]\n",
        "                if v and ctx.find(v) != -1:\n",
        "                    present.append(key)\n",
        "            if not present and cf and ctx.find(cf) != -1:\n",
        "                present = ['CF']\n",
        "            if present:\n",
        "                best = sorted(present, key=lambda x: -PRIORITY[x])[0]\n",
        "                chosen, chosen_src = cand[best], best\n",
        "                routes['substr_gate'] += 1\n",
        "\n",
        "    # Tamil digit normalization for final matching: if TA and ascii_digits(chosen) is in ctx but chosen is not, switch\n",
        "    if lang == 'ta' and isinstance(ctx,str) and ctx and isinstance(chosen,str) and chosen:\n",
        "        ch_ascii = ascii_digits(chosen)\n",
        "        if (ctx.find(ch_ascii) != -1) and (ctx.find(chosen) == -1):\n",
        "            chosen = ch_ascii\n",
        "            routes['tamil_digit_norm'] += 1\n",
        "\n",
        "    out.append((_id, chosen))\n",
        "\n",
        "sub = pd.DataFrame(out, columns=['id','PredictionString'])\n",
        "sub['len'] = sub['PredictionString'].astype(str).str.len()\n",
        "df_lang = sub.merge(test_df[['id','language']], on='id', how='left')\n",
        "print(f\"mean_len overall: {sub['len'].mean():.2f} | HI: {df_lang.loc[df_lang['language']=='hi','len'].mean():.2f} | TA: {df_lang.loc[df_lang['language']=='ta','len'].mean():.2f}\")\n",
        "total = len(sub)\n",
        "print('Routes %: ' + ', '.join(f\"{k}={v/total*100:.1f}%\" for k,v in routes.items()), flush=True)\n",
        "\n",
        "out_fp = f'submission_4stream_noexp_TS_LD_TD_CF.csv'\n",
        "sub[['id','PredictionString']].to_csv(out_fp, index=False)\n",
        "print('Wrote', out_fp, 'in', f'{time.time()-t4:.1f}s')\n",
        "print('Note: Not updating submission.csv here.')"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_len overall: 10.92 | HI: 11.07 | TA: 10.46\nRoutes %: majority=89.3%, present_priority=10.7%, fallback_td=0.0%, numeric_override=0.9%, substr_gate=3.6%, trail_shorter=0.0%, tamil_digit_norm=0.0%, ts_ld_flip=1.8%, tamil_rescue=2.7%, ta_boundary_extend=0.9%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_4stream_noexp_TS_LD_TD_CF.csv in 0.3s\nNote: Not updating submission.csv here.\n"
          ]
        }
      ]
    },
    {
      "id": "b01e81ee-c222-4524-9709-10b883376a41",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Logit-level blend: XLM-R (384) + MuRIL (384) with per-feature z-norm, targeted length prior, and safe caps\n",
        "import numpy as np, pandas as pd, json, time, unicodedata as ud, re, sys, os\n",
        "\n",
        "t = time.time()\n",
        "DIR_X = 'xlmr_large_test_logits'\n",
        "DIR_M = 'muril_large_test_logits'\n",
        "STARTS_X = [f'{DIR_X}/test_start_logits_f{i}.npy' for i in range(5)]\n",
        "ENDS_X   = [f'{DIR_X}/test_end_logits_f{i}.npy'   for i in range(5)]\n",
        "STARTS_M = [f'{DIR_M}/test_start_logits_f{i}.npy' for i in range(5)]\n",
        "ENDS_M   = [f'{DIR_M}/test_end_logits_f{i}.npy'   for i in range(5)]\n",
        "OFF_X = f'{DIR_X}/test_offset_mapping.npy'\n",
        "EID_X = f'{DIR_X}/test_example_id.json'\n",
        "\n",
        "CAP_HI = 18\n",
        "CAP_TA = 16\n",
        "K = 100  # moderate beam, faster than 200 but enough diversity\n",
        "BOUNDARY_BONUS = 0.12\n",
        "LAM_HI = 0.020\n",
        "LAM_TA = 0.042\n",
        "TGT_HI = 11\n",
        "TGT_TA = 10\n",
        "\n",
        "test_df = pd.read_csv('test.csv')\n",
        "def detect_lang(text):\n",
        "    if not isinstance(text, str):\n",
        "        return 'hi'\n",
        "    return 'ta' if any('\\u0B80' <= c <= '\\u0BFF' for c in text) else 'hi'\n",
        "col = 'question' if 'question' in test_df.columns else ('question_text' if 'question_text' in test_df.columns else ('context' if 'context' in test_df.columns else None))\n",
        "if col:\n",
        "    test_df['language'] = test_df[col].apply(detect_lang)\n",
        "else:\n",
        "    test_df['language'] = 'hi'\n",
        "test_df['id'] = test_df['id'].astype(str)\n",
        "id2lang = dict(zip(test_df['id'], test_df['language']))\n",
        "id2ctx = dict(zip(test_df['id'], test_df['context'])) if 'context' in test_df.columns else {}\n",
        "\n",
        "def ensure_offset_array(off_raw):\n",
        "    if isinstance(off_raw, np.ndarray) and off_raw.ndim == 2 and off_raw.shape[1] == 2:\n",
        "        return off_raw.astype(np.int32, copy=False)\n",
        "    try:\n",
        "        seq = off_raw.tolist() if hasattr(off_raw, 'tolist') else list(off_raw)\n",
        "    except Exception:\n",
        "        return None\n",
        "    pairs = []\n",
        "    for p in seq:\n",
        "        try:\n",
        "            a = int(p[0]) if (p is not None and len(p) > 0 and p[0] is not None) else 0\n",
        "            b = int(p[1]) if (p is not None and len(p) > 1 and p[1] is not None) else 0\n",
        "        except Exception:\n",
        "            a, b = 0, 0\n",
        "        pairs.append((a, b))\n",
        "    arr = np.asarray(pairs, dtype=np.int32)\n",
        "    if arr.ndim != 2 or arr.shape[1] != 2:\n",
        "        return None\n",
        "    return arr\n",
        "\n",
        "def znorm_row(x):\n",
        "    m = x.mean()\n",
        "    s = x.std()\n",
        "    if not np.isfinite(s) or s < 1e-6:\n",
        "        return x*0.0\n",
        "    return (x - m) / s\n",
        "\n",
        "def is_space_or_punct(c):\n",
        "    return c.isspace() or ud.category(c).startswith('P')\n",
        "\n",
        "# Load and average folds for each model\n",
        "sx = np.mean(np.stack([np.load(p) for p in STARTS_X], axis=0), axis=0)\n",
        "ex = np.mean(np.stack([np.load(p) for p in ENDS_X],   axis=0), axis=0)\n",
        "sm = np.mean(np.stack([np.load(p) for p in STARTS_M], axis=0), axis=0)\n",
        "em = np.mean(np.stack([np.load(p) for p in ENDS_M],   axis=0), axis=0)\n",
        "off = np.load(OFF_X, allow_pickle=True)\n",
        "with open(EID_X, 'r') as f:\n",
        "    eids = json.load(f)\n",
        "N = len(eids)\n",
        "assert sx.shape == ex.shape and sm.shape == em.shape and sx.shape[0]==N == len(off), 'shape mismatch'\n",
        "L = sx.shape[1]\n",
        "print(f'Blending logits: N={N}, L={L}', flush=True)\n",
        "\n",
        "# Per-feature z-norm, then average models\n",
        "S = np.empty_like(sx); E = np.empty_like(ex)\n",
        "for i in range(N):\n",
        "    S[i] = 0.5*znorm_row(sx[i]) + 0.5*znorm_row(sm[i])\n",
        "    E[i] = 0.5*znorm_row(ex[i]) + 0.5*znorm_row(em[i])\n",
        "\n",
        "# Decode best span per example id across all feature windows\n",
        "best_by_eid = {}  # eid -> (score, st_char, ed_char, span_len)\n",
        "for i in range(N):\n",
        "    if i and (i % 1000 == 0):\n",
        "        print(f'{i}/{N} in {time.time()-t:.1f}s', flush=True)\n",
        "    eid = str(eids[i])\n",
        "    ctx = id2ctx.get(eid, '')\n",
        "    lang = id2lang.get(eid, 'hi')\n",
        "    lam = LAM_HI if lang=='hi' else LAM_TA\n",
        "    tgt = TGT_HI if lang=='hi' else TGT_TA\n",
        "    cap = CAP_HI if lang=='hi' else CAP_TA\n",
        "    off_i = ensure_offset_array(off[i])\n",
        "    if off_i is None:\n",
        "        continue\n",
        "    s_log = S[i].copy(); e_log = E[i].copy()\n",
        "    valid = (off_i[:,0] + off_i[:,1]) > 0\n",
        "    if valid.shape[0] > 0:\n",
        "        valid[0] = False  # drop CLS\n",
        "    s_log[~valid] = -1e30; e_log[~valid] = -1e30\n",
        "    k_eff = int(min(K, int(valid.sum()))) if valid.ndim==1 else K\n",
        "    if k_eff <= 0:\n",
        "        continue\n",
        "    try:\n",
        "        s_idx = np.argpartition(-s_log, k_eff)[:k_eff]\n",
        "        e_idx = np.argpartition(-e_log, k_eff)[:k_eff]\n",
        "    except ValueError:\n",
        "        s_idx = np.argsort(-s_log)[:k_eff]\n",
        "        e_idx = np.argsort(-e_log)[:k_eff]\n",
        "    local_best = None\n",
        "    for si in s_idx:\n",
        "        if si < 0 or si >= valid.shape[0] or not valid[si]:\n",
        "            continue\n",
        "        for ei in e_idx:\n",
        "            if ei < 0 or ei >= valid.shape[0] or not valid[ei] or ei < si:\n",
        "                continue\n",
        "            st_char = int(off_i[si,0]); ed_char = int(off_i[ei,1])\n",
        "            if ed_char <= st_char:\n",
        "                continue\n",
        "            span_len = ed_char - st_char\n",
        "            if span_len <= 0 or span_len > cap:\n",
        "                continue\n",
        "            score = float(s_log[si] + e_log[ei] - lam * abs(span_len - tgt))\n",
        "            if isinstance(ctx, str) and ctx:\n",
        "                Lb = (st_char == 0) or is_space_or_punct(ctx[st_char-1])\n",
        "                Rb = (ed_char >= len(ctx)) or is_space_or_punct(ctx[ed_char:ed_char+1])\n",
        "                if Lb and Rb:\n",
        "                    score += BOUNDARY_BONUS\n",
        "            if (local_best is None) or (score > local_best[0]):\n",
        "                local_best = (score, st_char, ed_char, span_len)\n",
        "    if local_best is None:\n",
        "        continue\n",
        "    if (eid not in best_by_eid) or (local_best[0] > best_by_eid[eid][0]):\n",
        "        best_by_eid[eid] = local_best\n",
        "\n",
        "# Build submission\n",
        "pred_rows = []\n",
        "for eid in test_df['id']:\n",
        "    ctx = id2ctx.get(eid, '')\n",
        "    if eid in best_by_eid and isinstance(ctx, str) and ctx:\n",
        "        _, st, ed, _ = best_by_eid[eid]\n",
        "        st = max(0, min(len(ctx), int(st))); ed = max(st, min(len(ctx), int(ed)))\n",
        "        pred_rows.append((eid, ctx[st:ed]))\n",
        "    else:\n",
        "        pred_rows.append((eid, ''))\n",
        "sub = pd.DataFrame(pred_rows, columns=['id','PredictionString'])\n",
        "sub['len'] = sub['PredictionString'].astype(str).str.len()\n",
        "df_lang = sub.merge(test_df[['id','language']], on='id', how='left')\n",
        "print(f\"mean_len overall: {sub['len'].mean():.2f} | HI: {df_lang.loc[df_lang['language']=='hi','len'].mean():.2f} | TA: {df_lang.loc[df_lang['language']=='ta','len'].mean():.2f}\")\n",
        "out_fp = 'submission_logitblend_xlmr_muril_targetprior.csv'\n",
        "sub[['id','PredictionString']].to_csv(out_fp, index=False)\n",
        "print('Wrote', out_fp, 'Elapsed:', f'{time.time()-t:.1f}s')\n",
        "print('Note: Not updating submission.csv; primary remains majority-fallback.')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blending logits: N=1921, L=384\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 1513 is out of bounds for axis 0 with size 1513",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     81\u001b[39m S = np.empty_like(sx); E = np.empty_like(ex)\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     S[i] = \u001b[32m0.5\u001b[39m*znorm_row(sx[i]) + \u001b[32m0.5\u001b[39m*znorm_row(\u001b[43msm\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m     84\u001b[39m     E[i] = \u001b[32m0.5\u001b[39m*znorm_row(ex[i]) + \u001b[32m0.5\u001b[39m*znorm_row(em[i])\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Decode best span per example id across all feature windows\u001b[39;00m\n",
            "\u001b[31mIndexError\u001b[39m: index 1513 is out of bounds for axis 0 with size 1513"
          ]
        }
      ]
    },
    {
      "id": "0a8aa7b4-ea36-4655-b087-94098b2cede4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Point submission.csv to 4-stream no-expansion ensemble and write\n",
        "import pandas as pd, os\n",
        "fp = 'submission_4stream_noexp_TS_LD_TD_CF.csv'\n",
        "assert os.path.exists(fp), f'Missing {fp}'\n",
        "sub = pd.read_csv(fp)\n",
        "sub[['id','PredictionString']].to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', fp)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv updated -> submission_4stream_noexp_TS_LD_TD_CF.csv\n"
          ]
        }
      ]
    },
    {
      "id": "02aafd79-709d-46d6-aa7e-dba4944efeb6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# LD-majority ensemble: LD1 (hi1.40_ta1.80_caps19_17_scan), LD2 (hi1.60_ta2.00_caps19_17_scan), TD; TS backup; CF last resort\n",
        "import pandas as pd, numpy as np, unicodedata as ud, re, os, time, sys\n",
        "\n",
        "t5 = time.time()\n",
        "\n",
        "fp_ld1 = 'submission_xlmr_logitdecoder_lenprior_caps_hi1.40_ta1.80_caps19_17_scan.csv'\n",
        "fp_ld2 = 'submission_xlmr_logitdecoder_lenprior_caps_hi1.60_ta2.00_caps19_17_scan.csv'\n",
        "fp_td  = 'submission_tokendp_512.csv'\n",
        "fp_ts  = 'submission_tokenselect_512single_or_384_lambda012.csv'\n",
        "fp_cf  = 'submission_charfusion_512_single_seed_alignsafe.csv'\n",
        "\n",
        "avail = {\n",
        "    'LD1': os.path.exists(fp_ld1),\n",
        "    'LD2': os.path.exists(fp_ld2),\n",
        "    'TD': os.path.exists(fp_td),\n",
        "    'TS': os.path.exists(fp_ts),\n",
        "    'CF': os.path.exists(fp_cf),\n",
        "}\n",
        "assert avail['LD1'] or avail['LD2'], 'Missing LD files; run Cell 7 first.'\n",
        "assert avail['TD'], 'Missing TD file.'\n",
        "\n",
        "sub_ld1 = pd.read_csv(fp_ld1) if avail['LD1'] else None\n",
        "sub_ld2 = pd.read_csv(fp_ld2) if avail['LD2'] else None\n",
        "sub_td  = pd.read_csv(fp_td)\n",
        "sub_ts  = pd.read_csv(fp_ts) if avail['TS'] else None\n",
        "sub_cf  = pd.read_csv(fp_cf) if avail['CF'] else None\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "for df in [d for d in (sub_ld1, sub_ld2, sub_td, sub_ts, sub_cf) if d is not None]:\n",
        "    assert 'id' in df.columns and 'PredictionString' in df.columns\n",
        "    df['id'] = df['id'].astype(str)\n",
        "test_df['id'] = test_df['id'].astype(str)\n",
        "\n",
        "def detect_lang(text):\n",
        "    if not isinstance(text, str): return 'hi'\n",
        "    return 'ta' if any('\\u0B80' <= c <= '\\u0BFF' for c in text) else 'hi'\n",
        "col = 'question' if 'question' in test_df.columns else ('question_text' if 'question_text' in test_df.columns else ('context' if 'context' in test_df.columns else None))\n",
        "if col: test_df['language'] = test_df[col].apply(detect_lang)\n",
        "else: test_df['language'] = 'hi'\n",
        "\n",
        "base = test_df[['id']]\n",
        "if avail['LD1']: base = base.merge(sub_ld1.rename(columns={'PredictionString':'LD1'}), on='id', how='left')\n",
        "if avail['LD2']: base = base.merge(sub_ld2.rename(columns={'PredictionString':'LD2'}), on='id', how='left')\n",
        "base = base.merge(sub_td.rename(columns={'PredictionString':'TD'}), on='id', how='left')\n",
        "if avail['TS']: base = base.merge(sub_ts.rename(columns={'PredictionString':'TS'}), on='id', how='left')\n",
        "if avail['CF']: base = base.merge(sub_cf.rename(columns={'PredictionString':'CF'}), on='id', how='left')\n",
        "\n",
        "ctx_col = 'context' if 'context' in test_df.columns else None\n",
        "id2ctx = dict(zip(test_df['id'], test_df[ctx_col])) if ctx_col else {}\n",
        "id2lang = dict(zip(test_df['id'], test_df['language']))\n",
        "\n",
        "PUNCT_CLASS = ''.join(chr(i) for i in range(sys.maxunicode) if ud.category(chr(i)).startswith('P'))\n",
        "PUNCT_RE = re.compile(f\"[\\s{re.escape(PUNCT_CLASS)}]+\")\n",
        "DANDA='\\u0964'; TA_PULLI='\\u0bcd'; DEV_VIRAMA='\\u094d'\n",
        "\n",
        "def nfc(s):\n",
        "    try: return ud.normalize('NFC', s)\n",
        "    except Exception: return s\n",
        "def norm_basic(s):\n",
        "    if not isinstance(s, str): s = ''\n",
        "    s = nfc(s.strip())\n",
        "    s = s.replace('\\u200c','').replace('\\u200d','')\n",
        "    s = PUNCT_RE.sub(' ', s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "def norm_spaces_only(s):\n",
        "    s = nfc(s if isinstance(s, str) else '')\n",
        "    return re.sub(r'\\s+', ' ', s).strip()\n",
        "def is_space_or_punct(c):\n",
        "    return c.isspace() or ud.category(c).startswith('P')\n",
        "def is_present_boundary(ans, ctx):\n",
        "    if not (isinstance(ans,str) and ans and isinstance(ctx,str) and ctx): return False\n",
        "    st = ctx.find(ans)\n",
        "    if st == -1: return False\n",
        "    ed = st + len(ans)\n",
        "    left = (st==0) or is_space_or_punct(ctx[st-1])\n",
        "    right = (ed==len(ctx)) or is_space_or_punct(ctx[ed])\n",
        "    return left and right\n",
        "HI_DIGITS = {ord(c): ord('0')+i for i, c in enumerate('\\u0966\\u0967\\u0968\\u0969\\u096a\\u096b\\u096c\\u096d\\u096e\\u096f')}\n",
        "TA_DIGITS = {ord(c): ord('0')+i for i, c in enumerate('\\u0be6\\u0be7\\u0be8\\u0be9\\u0bea\\u0beb\\u0bec\\u0bed\\u0bee\\u0bef')}\n",
        "def ascii_digits(s):\n",
        "    if not isinstance(s, str): return ''\n",
        "    return s.translate(HI_DIGITS).translate(TA_DIGITS)\n",
        "SEP_RE = re.compile(r'[\\./\\-,:\\s]+')\n",
        "NUM_CHARS_RE = re.compile(r'^[0-9\\-]+$')\n",
        "def is_numeric_or_date_like(s, lang, cap):\n",
        "    if not isinstance(s, str) or not s: return False\n",
        "    s2 = ascii_digits(nfc(s)); s2 = SEP_RE.sub('-', s2).strip('-')\n",
        "    if not s2 or len(s2) > cap: return False\n",
        "    return bool(NUM_CHARS_RE.match(s2))\n",
        "\n",
        "PRIORITY = {'LD1':4,'LD2':3,'TD':2,'TS':1,'CF':0}\n",
        "\n",
        "out = []\n",
        "routes = {'majority':0,'present_priority':0,'fallback_td':0,'numeric_override':0,'substr_gate':0,'boundary_snap':0,'ta_zwj_snap':0,'ta_boundary_extend':0}\n",
        "\n",
        "for i, row in base.iterrows():\n",
        "    if (i % 500 == 0) and i:\n",
        "        print(f'Processed {i}/{len(base)} in {time.time()-t5:.1f}s', flush=True)\n",
        "    _id = row['id']\n",
        "    lang = id2lang.get(_id, 'hi')\n",
        "    # minimally-relaxed caps for overrides/micros per expert set\n",
        "    cap_override = 18 if lang == 'hi' else 20\n",
        "    ctx = id2ctx.get(_id, '') if ctx_col else ''\n",
        "    cand = {}\n",
        "    for k in ('LD1','LD2','TD','TS','CF'):\n",
        "        if k in row.index:\n",
        "            v = row[k] if isinstance(row[k], str) else ''\n",
        "            if v is not None: cand[k] = v\n",
        "    nrm = {k: norm_basic(v) for k,v in cand.items()}\n",
        "\n",
        "    # Majority over LD1, LD2, TD (priority-only TB to allow micros to act later)\n",
        "    counts = {}\n",
        "    for k in ('LD1','LD2','TD'):\n",
        "        if k in nrm:\n",
        "            counts.setdefault(nrm[k], []).append(k)\n",
        "    chosen=None; chosen_src=None; majority_key=None\n",
        "    for n, srcs in counts.items():\n",
        "        if n and len(srcs)>=2:\n",
        "            top = sorted(srcs, key=lambda x: -PRIORITY.get(x, 0))[0]\n",
        "            chosen, chosen_src = cand[top], top\n",
        "            majority_key = n\n",
        "            routes['majority'] += 1\n",
        "            break\n",
        "\n",
        "    # Presence gate among LD1/LD2/TD; TS if none present; CF last\n",
        "    if chosen is None:\n",
        "        present = []\n",
        "        if isinstance(ctx,str) and ctx:\n",
        "            for key in ('LD1','LD2','TD'):\n",
        "                if key in cand and cand[key] and ctx.find(cand[key])!=-1:\n",
        "                    present.append(key)\n",
        "        if present:\n",
        "            pref = [(key, cand[key], is_present_boundary(cand[key], ctx)) for key in present]\n",
        "            robust = [(key,v) for key,v,rob in pref if rob]\n",
        "            pool = robust if robust else [(key,v) for key,v,_rob in pref]\n",
        "            best = sorted(pool, key=lambda kv: -PRIORITY[kv[0]])[0]\n",
        "            chosen, chosen_src = best[1], best[0]\n",
        "            routes['present_priority'] += 1\n",
        "        else:\n",
        "            if 'TS' in cand and cand['TS'] and isinstance(ctx,str) and ctx and ctx.find(cand['TS'])!=-1:\n",
        "                chosen, chosen_src = cand['TS'], 'TS'\n",
        "                routes['present_priority'] += 1\n",
        "            elif 'TD' in cand:\n",
        "                chosen, chosen_src = cand['TD'], 'TD'\n",
        "                routes['fallback_td'] += 1\n",
        "            elif 'CF' in cand and cand['CF'] and isinstance(ctx,str) and ctx and ctx.find(cand['CF'])!=-1:\n",
        "                chosen, chosen_src = cand['CF'], 'CF'\n",
        "                routes['present_priority'] += 1\n",
        "            else:\n",
        "                chosen, chosen_src = cand.get('TS',''), 'TS'\n",
        "\n",
        "    # Tight numeric/date override (minimally-relaxed: allow 2-of-3 numeric-like with TD numeric-like)\n",
        "    cap_num = cap_override\n",
        "    num_flags = {k: (k in cand and is_numeric_or_date_like(cand[k], lang, cap_num)) for k in ('LD1','LD2','TD')}\n",
        "    if sum(1 for v in num_flags.values() if v) >= 2 and ('TD' in cand and num_flags['TD']):\n",
        "        if isinstance(ctx,str) and ctx:\n",
        "            td_ascii = ascii_digits(cand['TD'])\n",
        "            st = ctx.find(td_ascii)\n",
        "            if st != -1:\n",
        "                ed = st + len(td_ascii)\n",
        "                left = (st==0) or is_space_or_punct(ctx[st-1])\n",
        "                right = (ed==len(ctx)) or is_space_or_punct(ctx[ed])\n",
        "                if left and right and len(cand['TD']) <= cap_num and ctx.count(td_ascii) <= 2:\n",
        "                    chosen, chosen_src = cand['TD'], 'TD'\n",
        "                    routes['numeric_override'] += 1\n",
        "\n",
        "    # === Micro-rules: tighten to reduce boundary_snap firing to ~1\u20132% and nudge TA length ===\n",
        "    def _tight_norm(x):\n",
        "        x = nfc(x if isinstance(x, str) else '')\n",
        "        x = re.sub(r'\\s+', ' ', x).strip()\n",
        "        return x\n",
        "\n",
        "    TERMINAL_MARKS = {DANDA, DEV_VIRAMA, TA_PULLI}\n",
        "    def _is_terminal(c):\n",
        "        return (c in TERMINAL_MARKS) or ud.category(c).startswith('P')\n",
        "    def _strip_one_terminal(s):\n",
        "        if isinstance(s, str) and s and _is_terminal(s[-1]):\n",
        "            return s[:-1]\n",
        "        return s\n",
        "\n",
        "    # 1) Boundary Snap (only when chosen is boundary-bad; no absent-case; no shortening)\n",
        "    if isinstance(ctx, str) and ctx and chosen:\n",
        "        chosen_present = (ctx.find(chosen) != -1)\n",
        "        chosen_bad = (chosen_present and (not is_present_boundary(chosen, ctx)))\n",
        "        if chosen_bad:\n",
        "            Lc = len(chosen)\n",
        "            chosen_tight = _tight_norm(chosen)\n",
        "            chosen_tight_trim = _tight_norm(_strip_one_terminal(chosen)) if Lc > 1 else chosen_tight\n",
        "            chosen_digits = ascii_digits(chosen)\n",
        "            chosen_is_num = is_numeric_or_date_like(chosen, lang, cap_override)\n",
        "            for k in ('LD1', 'LD2', 'TD'):\n",
        "                alt = cand.get(k, '')\n",
        "                if not alt:\n",
        "                    continue\n",
        "                La = len(alt)\n",
        "                # Allow equal or longer by +1 always; allow +2 only with containment; disallow shorter to avoid length drift\n",
        "                if not ((La == Lc) or (La == Lc + 1) or (La == Lc + 2)):\n",
        "                    continue\n",
        "                if La > cap_override:\n",
        "                    continue\n",
        "                if ctx.count(alt) > 3:\n",
        "                    continue\n",
        "                alt_tight = _tight_norm(alt)\n",
        "                alt_tight_trim = _tight_norm(_strip_one_terminal(alt)) if La > 1 else alt_tight\n",
        "                # normalization: equality or trimmed equality; containment only if alt is longer\n",
        "                norm_eq = (alt_tight == chosen_tight) or (alt_tight == chosen_tight_trim) or (alt_tight_trim == chosen_tight)\n",
        "                norm_contain = (La > Lc) and ((chosen_tight in alt_tight) or (chosen_tight_trim in alt_tight))\n",
        "                if not (norm_eq or norm_contain):\n",
        "                    continue\n",
        "                # digits constraint: equal OR both non-numeric-like\n",
        "                alt_digits = ascii_digits(alt)\n",
        "                alt_is_num = is_numeric_or_date_like(alt, lang, cap_override)\n",
        "                digits_ok = (alt_digits == chosen_digits) or (not chosen_is_num and not alt_is_num)\n",
        "                if not digits_ok:\n",
        "                    continue\n",
        "                if is_present_boundary(alt, ctx):\n",
        "                    chosen, chosen_src = alt, k\n",
        "                    routes['boundary_snap'] += 1\n",
        "                    break\n",
        "\n",
        "    # 2) Tamil ZWJ/ZWNJ Snap (very low-fire; slight rarity relax)\n",
        "    if lang == 'ta' and isinstance(chosen, str) and isinstance(ctx, str) and ctx and chosen:\n",
        "        if ('\\u200c' in chosen or '\\u200d' in chosen):\n",
        "            stripped = chosen.replace('\\u200c', '').replace('\\u200d', '')\n",
        "            if stripped != chosen and len(stripped) <= cap_override:\n",
        "                need = (ctx.find(chosen) == -1) or (not is_present_boundary(chosen, ctx))\n",
        "                if need and ctx.count(stripped) <= 3 and is_present_boundary(stripped, ctx):\n",
        "                    chosen = stripped\n",
        "                    routes['ta_zwj_snap'] += 1\n",
        "\n",
        "    # 3) Tamil Boundary-Extend (guarded; encourage slightly longer clean spans)\n",
        "    if lang == 'ta' and isinstance(chosen, str) and isinstance(ctx, str) and ctx and chosen and len(chosen) < cap_override:\n",
        "        guard_short = len(chosen) <= 12\n",
        "        if guard_short:\n",
        "            ct = _tight_norm(chosen)\n",
        "            ct_trim = _tight_norm(_strip_one_terminal(chosen)) if len(chosen) > 1 else ct\n",
        "            choices = []\n",
        "            for k in ('LD1', 'LD2', 'TD', 'TS'):\n",
        "                v = cand.get(k, '')\n",
        "                if not v or len(v) <= len(chosen) or len(v) > cap_override:\n",
        "                    continue\n",
        "                if not is_present_boundary(v, ctx):\n",
        "                    continue\n",
        "                vt = _tight_norm(v)\n",
        "                # relaxed containment: allow tight-norm containment or literal containment\n",
        "                contains_ok = (chosen in v) or (ct in vt) or (ct_trim in vt)\n",
        "                if not contains_ok:\n",
        "                    continue\n",
        "                equal_or_off_by1 = (vt == ct) or (len(v) > 1 and _tight_norm(v[:-1]) == ct) or (len(chosen) > 1 and vt == ct_trim)\n",
        "                if equal_or_off_by1 and ctx.count(v) <= 6:\n",
        "                    choices.append((len(v), PRIORITY.get(k, 0), k, v))\n",
        "            if choices:\n",
        "                choices.sort()\n",
        "                _lv, _pr, k_best, v_best = choices[-1]\n",
        "                chosen, chosen_src = v_best, k_best\n",
        "                routes['ta_boundary_extend'] += 1\n",
        "    # === end micro-rules ===\n",
        "\n",
        "    # Substring safety: if chosen not present but others are, snap to highest-priority present among LD1/LD2/TD, else TS, else CF\n",
        "    if isinstance(ctx,str) and ctx and chosen:\n",
        "        if ctx.find(chosen) == -1:\n",
        "            present = []\n",
        "            for key in ('LD1','LD2','TD','TS'):\n",
        "                if key in cand and cand[key] and ctx.find(cand[key])!=-1:\n",
        "                    present.append(key)\n",
        "            if not present and 'CF' in cand and cand['CF'] and ctx.find(cand['CF'])!=-1:\n",
        "                present = ['CF']\n",
        "            if present:\n",
        "                best = sorted(present, key=lambda x: -PRIORITY[x])[0]\n",
        "                chosen, chosen_src = cand[best], best\n",
        "                routes['substr_gate'] += 1\n",
        "\n",
        "    out.append((_id, chosen))\n",
        "\n",
        "sub = pd.DataFrame(out, columns=['id','PredictionString'])\n",
        "sub['len'] = sub['PredictionString'].astype(str).str.len()\n",
        "df_lang = sub.merge(test_df[['id','language']], on='id', how='left')\n",
        "print(f\"mean_len overall: {sub['len'].mean():.2f} | HI: {df_lang.loc[df_lang['language']=='hi','len'].mean():.2f} | TA: {df_lang.loc[df_lang['language']=='ta','len'].mean():.2f}\")\n",
        "total = len(sub)\n",
        "print('Routes %: ' + ', '.join(f\"{k}={v/total*100:.1f}%\" for k,v in routes.items()), flush=True)\n",
        "\n",
        "out_fp = 'submission_ld_majority_LD1_LD2_TD.csv'\n",
        "sub[['id','PredictionString']].to_csv(out_fp, index=False)\n",
        "sub[['id','PredictionString']].to_csv('submission.csv', index=False)\n",
        "print('Wrote', out_fp, 'and updated submission.csv in', f'{time.time()-t5:.1f}s')"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_len overall: 10.58 | HI: 11.02 | TA: 9.25\nRoutes %: majority=92.9%, present_priority=7.1%, fallback_td=0.0%, numeric_override=0.9%, substr_gate=0.0%, boundary_snap=0.9%, ta_zwj_snap=0.0%, ta_boundary_extend=0.0%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_ld_majority_LD1_LD2_TD.csv and updated submission.csv in 0.3s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}