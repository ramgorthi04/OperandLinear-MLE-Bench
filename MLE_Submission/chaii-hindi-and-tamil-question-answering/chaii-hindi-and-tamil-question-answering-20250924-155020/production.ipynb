{
  "cells": [
    {
      "id": "b3b5be7f-11d5-44be-a2e1-8dd54d45280d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal production decoder setup: inspect logits artifact and compute priors\n",
        "import os, json, math, re, sys, gc, time, unicodedata as ud\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print('CWD:', os.getcwd())\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "print('train/test shapes:', train.shape, test.shape, flush=True)\n",
        "\n",
        "# Compute per-language char-length log-normal params from train\n",
        "def clean_text(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return ''\n",
        "    return s.strip()\n",
        "\n",
        "train['answer_text'] = train['answer_text'].astype(str).map(clean_text)\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    return float(x.mean()), float(x.std() if x.std() > 1e-6 else 1e-6)\n",
        "\n",
        "priors = {}\n",
        "for lang, g in train.groupby('language'):\n",
        "    mu, sigma = fit_log_normal_params(g)\n",
        "    priors[lang] = {'mu': mu, 'sigma': sigma, 'n': int(len(g))}\n",
        "print('Priors (log-space) by language:', priors, flush=True)\n",
        "\n",
        "# Load strongest logits artifact\n",
        "npz_path = 'xlmr_large_512_3seeds_avg.npz'\n",
        "assert os.path.exists(npz_path), f'Missing {npz_path}'\n",
        "npz = np.load(npz_path, allow_pickle=True)\n",
        "print('NPZ keys:', list(npz.keys()))\n",
        "\n",
        "# Try to infer shapes\n",
        "shapes = {k: (npz[k].shape if hasattr(npz[k], 'shape') else type(npz[k]).__name__) for k in npz.keys()}\n",
        "print('Shapes:', shapes, flush=True)\n",
        "\n",
        "# Peek a few entries for mapping keys commonly used: example_id(s), offset_mapping, start_logits, end_logits\n",
        "def safe_len(x):\n",
        "    try:\n",
        "        return len(x)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "for k in ['example_id', 'example_ids', 'example_id_list']:\n",
        "    if k in npz:\n",
        "        ex_ids = npz[k]\n",
        "        print(k, 'len=', safe_len(ex_ids))\n",
        "        break\n",
        "\n",
        "for k in ['offset_mapping', 'offset_mappings', 'test_offset_mapping']:\n",
        "    if k in npz:\n",
        "        off = npz[k]\n",
        "        print(k, 'dtype:', getattr(off, 'dtype', None), 'shape0:', off.shape[0] if hasattr(off,'shape') else None)\n",
        "        break\n",
        "\n",
        "for k in ['start_logits', 'test_start_logits', 'start_logits_avg']:\n",
        "    if k in npz:\n",
        "        sl = npz[k]\n",
        "        print(k, 'shape:', getattr(sl, 'shape', None))\n",
        "        break\n",
        "\n",
        "for k in ['end_logits', 'test_end_logits', 'end_logits_avg']:\n",
        "    if k in npz:\n",
        "        el = npz[k]\n",
        "        print(k, 'shape:', getattr(el, 'shape', None))\n",
        "        break\n",
        "\n",
        "print('Ready to implement decoding in next cell.', flush=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CWD: /app/agent_run_states/chaii-hindi-and-tamil-question-answering-20250924-155020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train/test shapes: (1002, 6) (112, 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priors (log-space) by language: {'hindi': {'mu': 2.27352915467594, 'sigma': 0.7057322733289972, 'n': 662}, 'tamil': {'mu': 2.240137765846579, 'sigma': 0.8165516610717193, 'n': 340}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NPZ keys: ['start', 'end']\nShapes: {'start': (1401, 512), 'end': (1401, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ready to implement decoding in next cell.\n"
          ]
        }
      ]
    },
    {
      "id": "74152509-753f-472a-b6ee-1cfba5c10348",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Single-stream decoder with char-length prior (lambda sweep) using xlmr_large_512_3seeds_avg.npz\n",
        "import os, json, math, re, sys, gc, time, unicodedata as ud\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "npz_path = 'xlmr_large_512_3seeds_avg.npz'\n",
        "npz = np.load(npz_path, allow_pickle=True)\n",
        "start_logits = npz['start']  # (Nfeat, L)\n",
        "end_logits = npz['end']      # (Nfeat, L)\n",
        "Nfeat, L = start_logits.shape\n",
        "print('Loaded logits:', start_logits.shape, end_logits.shape, flush=True)\n",
        "\n",
        "# Load mapping from one seed's saved artifacts (same featureization/order)\n",
        "map_dir = 'xlmr_large_512_test_logits'\n",
        "eid_path = Path(map_dir) / 'test_example_id.json'\n",
        "off_path = Path(map_dir) / 'test_offset_mapping.npy'\n",
        "assert eid_path.exists() and off_path.exists(), 'Missing mapping files'\n",
        "example_id_list = json.loads(Path(eid_path).read_text())  # list length Nfeat\n",
        "offset_mapping = np.load(off_path, allow_pickle=True)     # often object array (Nfeat, L) of tuples\n",
        "print('Mapping loaded:', len(example_id_list), getattr(offset_mapping, 'shape', None), 'dtype:', getattr(offset_mapping, 'dtype', None), flush=True)\n",
        "assert len(example_id_list) == Nfeat\n",
        "\n",
        "# Load test meta\n",
        "test = pd.read_csv('test.csv')\n",
        "id2lang = dict(zip(test['id'].tolist(), test['language'].tolist()))\n",
        "id2context = dict(zip(test['id'].tolist(), test['context'].astype(str).tolist()))\n",
        "\n",
        "# Priors computed in cell 0 (recompute here if needed)\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {}\n",
        "for lang, g in train.groupby('language'):\n",
        "    priors[lang] = dict(zip(['mu','sigma'], fit_log_normal_params(g)))\n",
        "print('Priors:', priors, flush=True)\n",
        "\n",
        "# Utility: unicode hygiene (minimal and safe)\n",
        "ZW_CHARS = {\n",
        "    '\\u200B', '\\u200C', '\\u200D', '\\u2060', '\\ufeff',  # ZWSP, ZWNJ, ZWJ, WJ, BOM\n",
        "}\n",
        "NBSP_SET = {'\\u00A0', '\\u2002', '\\u2003', '\\u2004', '\\u2005', '\\u2006', '\\u2007', '\\u2008', '\\u2009', '\\u200A'}\n",
        "HI_VIRAMA = '\\u094D'\n",
        "TA_PULLI = '\\u0BCD'\n",
        "\n",
        "def clean_span_text(s: str, lang: str) -> str:\n",
        "    if not s:\n",
        "        return ''\n",
        "    # remove zero-width\n",
        "    s = ''.join(ch for ch in s if ch not in ZW_CHARS)\n",
        "    # collapse NBSP/thin spaces to space\n",
        "    s = ''.join(' ' if ch in NBSP_SET else ch for ch in s)\n",
        "    # collapse spaces\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    # numeric glue inside digit runs\n",
        "    s = re.sub(r'(?<=\\d)[\\s,._-](?=\\d)', '', s)\n",
        "    # drop one trailing combining mark or virama/pulli\n",
        "    if s:\n",
        "        last = s[-1]\n",
        "        if ud.category(last) == 'Mn' or last in (HI_VIRAMA, TA_PULLI):\n",
        "            s = s[:-1]\n",
        "    # Hindi specific: collapse multiple dandas and strip final danda\n",
        "    if lang == 'hindi':\n",
        "        s = s.replace('\\u0964\\u0964', '\\u0964')\n",
        "        if s.endswith('\\u0964'):\n",
        "            s = s[:-1].rstrip()\n",
        "    return s\n",
        "\n",
        "# Scoring helpers\n",
        "def log_normal_logpdf_len(char_len: int, mu: float, sigma: float) -> float:\n",
        "    Lc = max(1, int(char_len))\n",
        "    x = math.log(Lc)\n",
        "    val = -0.5 * ((x - mu)/sigma)**2 - math.log(max(Lc,1e-6)) - math.log(sigma) - 0.5*math.log(2*math.pi)\n",
        "    return val\n",
        "\n",
        "def to_pair(t):\n",
        "    try:\n",
        "        if t is None:\n",
        "            return (0, 0)\n",
        "        if isinstance(t, (list, tuple, np.ndarray)):\n",
        "            if len(t) >= 2:\n",
        "                a = int(t[0]) if t[0] is not None else 0\n",
        "                b = int(t[1]) if t[1] is not None else 0\n",
        "                return (a, b)\n",
        "            else:\n",
        "                return (0, 0)\n",
        "        # unexpected type\n",
        "        return (0, 0)\n",
        "    except Exception:\n",
        "        return (0, 0)\n",
        "\n",
        "def to_offs_matrix(offs_raw):\n",
        "    # Convert a per-feature offset entry into an int32 (M,2) array; supports ragged/object formats\n",
        "    if isinstance(offs_raw, np.ndarray) and offs_raw.ndim == 2 and offs_raw.shape[1] == 2 and offs_raw.dtype != object:\n",
        "        return offs_raw.astype(np.int32, copy=False)\n",
        "    try:\n",
        "        pairs = [to_pair(t) for t in list(offs_raw)]\n",
        "        arr = np.asarray(pairs, dtype=np.int32)\n",
        "        if arr.ndim == 2 and arr.shape[1] == 2:\n",
        "            return arr\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "def decode_single(lambda_len: float, beta_freq: float = 0.0, temps=None, nbest_hi=180, nbest_ta=220, Lmax_hi=50, Lmax_ta=60, clip_prior=(-0.7, 0.0)):\n",
        "    t0 = time.time()\n",
        "    preds = {}  # id -> (score, text)\n",
        "    counts = {'total_feats': Nfeat, 'skipped_feats': 0}\n",
        "    # optional temperatures (unused by default)\n",
        "    if temps is None: temps = {'hindi': (1.0, 1.0), 'tamil': (1.0, 1.0)}\n",
        "    for fi in range(Nfeat):\n",
        "        qid = example_id_list[fi]\n",
        "        lang = id2lang[qid]\n",
        "        context = id2context[qid]\n",
        "        mu = priors.get(lang, {}).get('mu', 2.3 if lang=='hindi' else 2.1)\n",
        "        sigma = priors.get(lang, {}).get('sigma', 0.8 if lang=='hindi' else 0.7)\n",
        "        Lmax = Lmax_hi if lang == 'hindi' else Lmax_ta\n",
        "        nbest = nbest_hi if lang == 'hindi' else nbest_ta\n",
        "        Ts, Te = temps.get(lang, (1.0,1.0))\n",
        "        s_logits_full = start_logits[fi] / Ts\n",
        "        e_logits_full = end_logits[fi] / Te\n",
        "        # Offsets for this feature\n",
        "        offs = to_offs_matrix(offset_mapping[fi])\n",
        "        if offs is None:\n",
        "            counts['skipped_feats'] += 1\n",
        "            continue\n",
        "        M = offs.shape[0]\n",
        "        # Align logits to available offsets length\n",
        "        s_logits = s_logits_full[:M]\n",
        "        e_logits = e_logits_full[:M]\n",
        "        # Build valid mask: end > start (positive span within context); disallow specials where end==start\n",
        "        valid = (offs[:,1] > offs[:,0])\n",
        "        if not valid.any():\n",
        "            counts['skipped_feats'] += 1\n",
        "            continue\n",
        "        # Top-N start indices among valid\n",
        "        valid_idx = np.where(valid)[0]\n",
        "        s_candidates = valid_idx[np.argsort(s_logits[valid_idx])[::-1][:nbest]]\n",
        "        best_score_f = -1e18\n",
        "        best_span_f = None  # (si, ei, score)\n",
        "        for si in s_candidates:\n",
        "            s_off = offs[si]\n",
        "            if s_off[1] <= s_off[0]:\n",
        "                continue\n",
        "            # Bound ends: >= si and within Lmax chars\n",
        "            end_range = np.arange(si, M, dtype=np.int32)\n",
        "            e_offs = offs[end_range]\n",
        "            clen = e_offs[:,1] - s_off[0]\n",
        "            mask = (e_offs[:,1] > e_offs[:,0]) & (clen > 0) & (clen <= Lmax)\n",
        "            if not mask.any():\n",
        "                continue\n",
        "            cand_e_idx = end_range[mask]\n",
        "            raw_scores = s_logits[si] + e_logits[cand_e_idx]\n",
        "            # length prior\n",
        "            if lambda_len > 0.0:\n",
        "                clen_valid = (offs[cand_e_idx][:,1] - s_off[0]).astype(int)\n",
        "                lp = np.array([log_normal_logpdf_len(int(c), mu, sigma) for c in clen_valid], dtype=np.float64)\n",
        "                lp = np.clip(lp, clip_prior[0], clip_prior[1])\n",
        "                raw_scores = raw_scores + lambda_len * lp\n",
        "            # optional frequency prior (compute only on top few)\n",
        "            if beta_freq > 0.0:\n",
        "                order = np.argsort(raw_scores)[::-1][:8]\n",
        "                for idx in order:\n",
        "                    ei = int(cand_e_idx[idx])\n",
        "                    a = int(offs[si][0]); b = int(offs[ei][1])\n",
        "                    if not (0 <= a < len(context)) or not (0 < b <= len(context)) or a >= b:\n",
        "                        continue\n",
        "                    span_text = context[a:b]\n",
        "                    freq = context.count(span_text)\n",
        "                    raw_scores[idx] += beta_freq * math.log(1 + freq)\n",
        "            # take best end\n",
        "            best_idx = int(np.argmax(raw_scores))\n",
        "            ei = int(cand_e_idx[best_idx])\n",
        "            score = float(np.max(raw_scores))\n",
        "            if score > best_score_f:\n",
        "                best_score_f = score\n",
        "                best_span_f = (si, ei, score)\n",
        "        if best_span_f is None:\n",
        "            continue\n",
        "        si, ei, sc = best_span_f\n",
        "        a = int(offs[si][0]); b = int(offs[ei][1])\n",
        "        # Sanity to ensure offsets within context\n",
        "        if not (0 <= a < len(context)) or not (0 < b <= len(context)) or a >= b:\n",
        "            # Fallback: pick maximal logit end within cap ignoring offsets (should rarely happen)\n",
        "            ei = int(np.argmax(e_logits))\n",
        "            si = int(np.argmax(s_logits[:ei+1]))\n",
        "            a = int(offs[si][0]); b = int(offs[ei][1])\n",
        "            a = max(0, min(a, len(context)-1))\n",
        "            b = max(a+1, min(b, len(context)))\n",
        "        text = context[a:b]\n",
        "        text = clean_span_text(text, lang)\n",
        "        if not text:\n",
        "            # force non-empty by expanding one char to the right within cap\n",
        "            b = min(len(context), a + 1)\n",
        "            text = clean_span_text(context[a:b], lang) or context[a:b]\n",
        "        # Keep best across features for this example id\n",
        "        prev = preds.get(qid)\n",
        "        if (prev is None) or (sc > prev[0]):\n",
        "            preds[qid] = (sc, text)\n",
        "        if (fi+1) % 200 == 0:\n",
        "            print(f'Processed feat {fi+1}/{Nfeat} (elapsed {time.time()-t0:.1f}s)')\n",
        "    # Build submission DataFrame\n",
        "    out = []\n",
        "    for qid in test['id'].tolist():\n",
        "        if qid in preds:\n",
        "            out.append((qid, preds[qid][1]))\n",
        "        else:\n",
        "            # extremely rare: fallback blank -> use first Lmax chars of context\n",
        "            lang = id2lang[qid]; ctx = id2context[qid]; cap = Lmax_hi if lang=='hindi' else Lmax_ta\n",
        "            fallback = clean_span_text(ctx[:max(1, min(cap, len(ctx)))], lang)\n",
        "            out.append((qid, fallback if fallback else ctx[:1]))\n",
        "    sub = pd.DataFrame(out, columns=['id','PredictionString'])\n",
        "    # Diagnostics\n",
        "    empties = (sub['PredictionString'].astype(str).str.len() == 0).sum()\n",
        "    mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "    print('Decode done. Empties:', empties, 'Mean char len:', round(mean_len,2))\n",
        "    return sub\n",
        "\n",
        "# Run variants\n",
        "variants = [\n",
        "    ('A_lenprior015', dict(lambda_len=0.15, beta_freq=0.0, nbest_hi=180, nbest_ta=220, Lmax_hi=50, Lmax_ta=60)),\n",
        "    ('B_lenprior030', dict(lambda_len=0.30, beta_freq=0.0, nbest_hi=180, nbest_ta=220, Lmax_hi=50, Lmax_ta=60)),\n",
        "    ('C_lenprior000', dict(lambda_len=0.00, beta_freq=0.0, nbest_hi=180, nbest_ta=220, Lmax_hi=50, Lmax_ta=60)),\n",
        "]\n",
        "\n",
        "subs = {}\n",
        "for tag, kwargs in variants:\n",
        "    print(f'Running variant {tag} with kwargs:', kwargs, flush=True)\n",
        "    sub = decode_single(**kwargs)\n",
        "    out_name = f'submission_512only_lenprior_{tag}.csv'\n",
        "    sub.to_csv(out_name, index=False)\n",
        "    subs[tag] = out_name\n",
        "    print('Wrote', out_name, flush=True)\n",
        "\n",
        "# Set primary submission.csv to Variant A\n",
        "primary = subs['A_lenprior015']\n",
        "pd.read_csv(primary).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', primary, flush=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded logits: (1401, 512) (1401, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapping loaded: 1401 (1401, 512) dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Priors: {'hindi': {'mu': 2.27352915467594, 'sigma': 0.7057322733289972}, 'tamil': {'mu': 2.240137765846579, 'sigma': 0.8165516610717193}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running variant A_lenprior015 with kwargs: {'lambda_len': 0.15, 'beta_freq': 0.0, 'nbest_hi': 180, 'nbest_ta': 220, 'Lmax_hi': 50, 'Lmax_ta': 60}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed feat 200/1401 (elapsed 2.7s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed feat 400/1401 (elapsed 5.3s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed feat 600/1401 (elapsed 7.9s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed feat 800/1401 (elapsed 10.5s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed feat 1000/1401 (elapsed 13.1s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed feat 1200/1401 (elapsed 15.8s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed feat 1400/1401 (elapsed 18.6s)\nDecode done. Empties: 0 Mean char len: 10.67\nWrote submission_512only_lenprior_A_lenprior015.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running variant B_lenprior030 with kwargs: {'lambda_len': 0.3, 'beta_freq': 0.0, 'nbest_hi': 180, 'nbest_ta': 220, 'Lmax_hi': 50, 'Lmax_ta': 60}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed feat 200/1401 (elapsed 2.8s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed feat 400/1401 (elapsed 5.6s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed feat 600/1401 (elapsed 8.4s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed feat 800/1401 (elapsed 11.2s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed feat 1000/1401 (elapsed 13.9s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed feat 1200/1401 (elapsed 16.5s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed feat 1400/1401 (elapsed 19.4s)\nDecode done. Empties: 0 Mean char len: 10.67\nWrote submission_512only_lenprior_B_lenprior030.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running variant C_lenprior000 with kwargs: {'lambda_len': 0.0, 'beta_freq': 0.0, 'nbest_hi': 180, 'nbest_ta': 220, 'Lmax_hi': 50, 'Lmax_ta': 60}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed feat 200/1401 (elapsed 1.3s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed feat 400/1401 (elapsed 2.5s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed feat 600/1401 (elapsed 3.9s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed feat 800/1401 (elapsed 5.2s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed feat 1000/1401 (elapsed 6.5s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed feat 1200/1401 (elapsed 7.8s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed feat 1400/1401 (elapsed 9.1s)\nDecode done. Empties: 0 Mean char len: 10.67\nWrote submission_512only_lenprior_C_lenprior000.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv updated -> submission_512only_lenprior_A_lenprior015.csv\n"
          ]
        }
      ]
    },
    {
      "id": "38461b91-4eed-4320-baaf-2ab8721ef793",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Char-level fusion decoder (single-stream 512 3-seed), lambda=0.15 primary\n",
        "import numpy as np, pandas as pd, time, math, json, re, unicodedata as ud\n",
        "from pathlib import Path\n",
        "\n",
        "npz_path = 'xlmr_large_512_3seeds_avg.npz'\n",
        "npz = np.load(npz_path, allow_pickle=True)\n",
        "start_logits = npz['start']\n",
        "end_logits = npz['end']\n",
        "Nfeat, Ltok = start_logits.shape\n",
        "\n",
        "map_dir = 'xlmr_large_512_test_logits'\n",
        "eid_path = Path(map_dir) / 'test_example_id.json'\n",
        "off_path = Path(map_dir) / 'test_offset_mapping.npy'\n",
        "example_id_list = json.loads(Path(eid_path).read_text())\n",
        "offset_mapping = np.load(off_path, allow_pickle=True)\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id2lang = dict(zip(test['id'].tolist(), test['language'].tolist()))\n",
        "id2context = dict(zip(test['id'].tolist(), test['context'].astype(str).tolist()))\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "ZW_CHARS = {'\\u200B','\\u200C','\\u200D','\\u2060','\\ufeff'}\n",
        "NBSP_SET = {'\\u00A0','\\u2002','\\u2003','\\u2004','\\u2005','\\u2006','\\u2007','\\u2008','\\u2009','\\u200A'}\n",
        "HI_VIRAMA = '\\u094D'\n",
        "TA_PULLI = '\\u0BCD'\n",
        "def clean_span_text(s: str, lang: str) -> str:\n",
        "    if not s: return ''\n",
        "    s = ''.join(ch for ch in s if ch not in ZW_CHARS)\n",
        "    s = ''.join(' ' if ch in NBSP_SET else ch for ch in s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    s = re.sub(r'(?<=\\d)[\\s,._-](?=\\d)', '', s)\n",
        "    if s:\n",
        "        last = s[-1]\n",
        "        if ud.category(last) == 'Mn' or last in (HI_VIRAMA, TA_PULLI):\n",
        "            s = s[:-1]\n",
        "    if lang == 'hindi':\n",
        "        s = s.replace('\\u0964\\u0964','\\u0964')\n",
        "        if s.endswith('\\u0964'): s = s[:-1].rstrip()\n",
        "    return s\n",
        "\n",
        "def log_normal_logpdf_len(char_len: int, mu: float, sigma: float) -> float:\n",
        "    Lc = max(1, int(char_len))\n",
        "    x = math.log(Lc)\n",
        "    return -0.5*((x-mu)/sigma)**2 - math.log(max(Lc,1e-6)) - math.log(sigma) - 0.5*math.log(2*math.pi)\n",
        "\n",
        "def to_pair(t):\n",
        "    try:\n",
        "        if t is None: return (0,0)\n",
        "        if isinstance(t,(list,tuple,np.ndarray)) and len(t)>=2:\n",
        "            a = int(t[0]) if t[0] is not None else 0\n",
        "            b = int(t[1]) if t[1] is not None else 0\n",
        "            return (a,b)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return (0,0)\n",
        "\n",
        "# Build index lists per example\n",
        "qid_to_feat_idx = {}\n",
        "for i, qid in enumerate(example_id_list):\n",
        "    qid_to_feat_idx.setdefault(qid, []).append(i)\n",
        "\n",
        "def maxpool1d(x):\n",
        "    if len(x) == 0: return x\n",
        "    y = x.copy()\n",
        "    if len(x) == 1: return y\n",
        "    # 1D max-pool with kernel=3, stride=1, padding=1\n",
        "    y[0] = max(x[0], x[1])\n",
        "    for i in range(1, len(x)-1):\n",
        "        y[i] = max(x[i-1], x[i], x[i+1])\n",
        "    y[-1] = max(x[-2], x[-1])\n",
        "    return y\n",
        "\n",
        "def decode_charfusion(lambda_len=0.15, clip_prior=(-0.7,0.0), nbest_hi=200, nbest_ta=240, Lmax_hi=50, Lmax_ta=60, do_pool=True):\n",
        "    t0 = time.time()\n",
        "    out_rows = []\n",
        "    for qid in test['id'].tolist():\n",
        "        lang = id2lang[qid]\n",
        "        ctx = id2context[qid]\n",
        "        mu = priors.get(lang, {}).get('mu', 2.3 if lang=='hindi' else 2.1)\n",
        "        sigma = priors.get(lang, {}).get('sigma', 0.8 if lang=='hindi' else 0.7)\n",
        "        Lmax = Lmax_hi if lang=='hindi' else Lmax_ta\n",
        "        K = nbest_hi if lang=='hindi' else nbest_ta\n",
        "        S = np.zeros(len(ctx), dtype=np.float32)\n",
        "        E = np.zeros(len(ctx), dtype=np.float32)\n",
        "        idxs = qid_to_feat_idx.get(qid, [])\n",
        "        for fi in idxs:\n",
        "            offs_raw = offset_mapping[fi]\n",
        "            # offs_raw is (Ltok,) of pairs\n",
        "            # Align token logits length to offs length\n",
        "            M = len(offs_raw) if hasattr(offs_raw,'__len__') else Ltok\n",
        "            s_log = start_logits[fi][:M]\n",
        "            e_log = end_logits[fi][:M]\n",
        "            # accumulate\n",
        "            for ti in range(M):\n",
        "                a, b = to_pair(offs_raw[ti])\n",
        "                if b > a and 0 <= a < len(ctx) and 1 <= b <= len(ctx):\n",
        "                    S[a] += s_log[ti]\n",
        "                    E[b-1] += e_log[ti]\n",
        "        if do_pool:\n",
        "            S = maxpool1d(S)\n",
        "            E = maxpool1d(E)\n",
        "        # Top-K starts\n",
        "        if len(S) == 0:\n",
        "            out_rows.append((qid, ctx[:1]))\n",
        "            continue\n",
        "        starts = np.argsort(S)[::-1][:K]\n",
        "        best_score = -1e18\n",
        "        best_span = (0, max(0, min(Lmax, len(ctx))-1))\n",
        "        for si in starts:\n",
        "            end_lo = si\n",
        "            end_hi = min(len(ctx)-1, si + Lmax - 1)\n",
        "            if end_hi < end_lo: continue\n",
        "            # choose best end by E\n",
        "            seg = E[end_lo:end_hi+1]\n",
        "            ej_rel = int(np.argmax(seg))\n",
        "            ej = end_lo + ej_rel\n",
        "            raw = float(S[si] + E[ej])\n",
        "            if lambda_len > 0.0:\n",
        "                clen = ej - si + 1\n",
        "                lp = log_normal_logpdf_len(clen, mu, sigma)\n",
        "                lp = max(clip_prior[0], min(clip_prior[1], lp))\n",
        "                raw += lambda_len * lp\n",
        "            if raw > best_score:\n",
        "                best_score = raw\n",
        "                best_span = (si, ej)\n",
        "        a, b = best_span\n",
        "        text = ctx[a:b+1]\n",
        "        text = clean_span_text(text, lang)\n",
        "        if not text:\n",
        "            b2 = min(len(ctx), a+1)\n",
        "            text = clean_span_text(ctx[a:b2], lang) or ctx[a:b2]\n",
        "        out_rows.append((qid, text))\n",
        "    sub = pd.DataFrame(out_rows, columns=['id','PredictionString'])\n",
        "    empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "    mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "    print(f'Char-fusion decode done in {time.time()-t0:.1f}s. Empties={empties}, mean_len={mean_len:.2f}')\n",
        "    return sub\n",
        "\n",
        "# Run char-fusion primary and write submission\n",
        "sub_char = decode_charfusion(lambda_len=0.15, nbest_hi=200, nbest_ta=240, Lmax_hi=50, Lmax_ta=60, do_pool=True)\n",
        "out_name = 'submission_charfusion_512_lambda015.csv'\n",
        "sub_char.to_csv(out_name, index=False)\n",
        "pd.read_csv(out_name).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_name)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char-fusion decode done in 2.2s. Empties=0, mean_len=10.67\nsubmission.csv updated -> submission_charfusion_512_lambda015.csv\n"
          ]
        }
      ]
    },
    {
      "id": "9aa1759b-f0cb-4bbf-a631-d016690a366d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Multi-stream char-level fusion: 512(3seeds)+384(+MuRIL for Hindi) with per-language weights, lambda=0.15\n",
        "import json, math, time, re, unicodedata as ud\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Common resources\n",
        "test = pd.read_csv('test.csv')\n",
        "id2lang = dict(zip(test['id'].tolist(), test['language'].tolist()))\n",
        "id2context = dict(zip(test['id'].tolist(), test['context'].astype(str).tolist()))\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "ZW_CHARS = {'\\u200B','\\u200C','\\u200D','\\u2060','\\ufeff'}\n",
        "NBSP_SET = {'\\u00A0','\\u2002','\\u2003','\\u2004','\\u2005','\\u2006','\\u2007','\\u2008','\\u2009','\\u200A'}\n",
        "HI_VIRAMA = '\\u094D'\n",
        "TA_PULLI = '\\u0BCD'\n",
        "def clean_span_text(s: str, lang: str) -> str:\n",
        "    if not s: return ''\n",
        "    s = ''.join(ch for ch in s if ch not in ZW_CHARS)\n",
        "    s = ''.join(' ' if ch in NBSP_SET else ch for ch in s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    s = re.sub(r'(?<=\\d)[\\s,._-](?=\\d)', '', s)\n",
        "    if s:\n",
        "        last = s[-1]\n",
        "        if ud.category(last) == 'Mn' or last in (HI_VIRAMA, TA_PULLI): s = s[:-1]\n",
        "    if lang == 'hindi':\n",
        "        s = s.replace('\\u0964\\u0964','\\u0964')\n",
        "        if s.endswith('\\u0964'): s = s[:-1].rstrip()\n",
        "    return s\n",
        "\n",
        "def log_normal_logpdf_len(char_len: int, mu: float, sigma: float) -> float:\n",
        "    Lc = max(1, int(char_len))\n",
        "    x = math.log(Lc)\n",
        "    return -0.5*((x-mu)/sigma)**2 - math.log(max(Lc,1e-6)) - math.log(sigma) - 0.5*math.log(2*math.pi)\n",
        "\n",
        "def to_pair(t):\n",
        "    try:\n",
        "        if t is None: return (0,0)\n",
        "        if isinstance(t,(list,tuple,np.ndarray)) and len(t)>=2:\n",
        "            a = int(t[0]) if t[0] is not None else 0\n",
        "            b = int(t[1]) if t[1] is not None else 0\n",
        "            return (a,b)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return (0,0)\n",
        "\n",
        "def load_npz_logits(npz_path):\n",
        "    arr = np.load(npz_path, allow_pickle=True)\n",
        "    keys = list(arr.keys())\n",
        "    if 'start' in keys and 'end' in keys:\n",
        "        return arr['start'], arr['end']\n",
        "    # fallback key names\n",
        "    for sk in ['start_logits','test_start_logits','start_logits_avg']:\n",
        "        for ek in ['end_logits','test_end_logits','end_logits_avg']:\n",
        "            if sk in keys and ek in keys:\n",
        "                return arr[sk], arr[ek]\n",
        "    raise ValueError(f'Unknown keys in {npz_path}: {keys}')\n",
        "\n",
        "def maxpool1d(x):\n",
        "    if len(x) == 0: return x\n",
        "    y = x.copy()\n",
        "    if len(x) == 1: return y\n",
        "    y[0] = max(x[0], x[1])\n",
        "    for i in range(1, len(x)-1): y[i] = max(x[i-1], x[i], x[i+1])\n",
        "    y[-1] = max(x[-2], x[-1])\n",
        "    return y\n",
        "\n",
        "# Streams config: paths and map dirs\n",
        "streams = [\n",
        "    dict(name='xlmr512', npz='xlmr_large_512_3seeds_avg.npz', map_dir='xlmr_large_512_test_logits'),\n",
        "    dict(name='xlmr384', npz='xlmr_large_test_avg.npz', map_dir='xlmr_large_test_logits'),\n",
        "    dict(name='muril',   npz='muril_large_test_avg.npz',   map_dir='muril_large_test_logits'),\n",
        "]\n",
        "\n",
        "# Per-language weights\n",
        "weights_hi = {'xlmr512': 0.80, 'xlmr384': 0.15, 'muril': 0.05}\n",
        "weights_ta = {'xlmr512': 0.95, 'xlmr384': 0.05, 'muril': 0.00}\n",
        "\n",
        "# Load all streams\n",
        "loaded = []\n",
        "for s in streams:\n",
        "    try:\n",
        "        s_start, s_end = load_npz_logits(s['npz'])\n",
        "        eid_path = Path(s['map_dir']) / 'test_example_id.json'\n",
        "        offs_path = Path(s['map_dir']) / 'test_offset_mapping.npy'\n",
        "        eid = json.loads(eid_path.read_text())\n",
        "        offs = np.load(offs_path, allow_pickle=True)\n",
        "        loaded.append((s['name'], s_start, s_end, eid, offs))\n",
        "        print('Loaded stream', s['name'], s_start.shape, 'features and offsets shape', getattr(offs,'shape',None))\n",
        "    except Exception as e:\n",
        "        print('Skip stream due to load error:', s['name'], e)\n",
        "\n",
        "def decode_charfusion_multistream(lambda_len=0.15, clip_prior=(-0.7,0.0), nbest_hi=200, nbest_ta=240, Lmax_hi=50, Lmax_ta=60, do_pool=True):\n",
        "    t0 = time.time()\n",
        "    # Build index per stream: qid -> feat idx list\n",
        "    idx_maps = []\n",
        "    for name, s_start, s_end, eid, offs in loaded:\n",
        "        m = {}\n",
        "        for i, qid in enumerate(eid):\n",
        "            m.setdefault(qid, []).append(i)\n",
        "        idx_maps.append(m)\n",
        "    out_rows = []\n",
        "    for qid in test['id'].tolist():\n",
        "        lang = id2lang[qid]\n",
        "        ctx = id2context[qid]\n",
        "        mu = priors.get(lang, {}).get('mu', 2.3 if lang=='hindi' else 2.1)\n",
        "        sigma = priors.get(lang, {}).get('sigma', 0.8 if lang=='hindi' else 0.7)\n",
        "        Lmax = Lmax_hi if lang=='hindi' else Lmax_ta\n",
        "        K = nbest_hi if lang=='hindi' else nbest_ta\n",
        "        S = np.zeros(len(ctx), dtype=np.float32)\n",
        "        E = np.zeros(len(ctx), dtype=np.float32)\n",
        "        for (name, s_start, s_end, eid, offs), m in zip(loaded, idx_maps):\n",
        "            w = (weights_hi if lang=='hindi' else weights_ta).get(name, 0.0)\n",
        "            if w <= 0.0: continue\n",
        "            feat_idx = m.get(qid, [])\n",
        "            for fi in feat_idx:\n",
        "                offs_raw = offs[fi]\n",
        "                M = len(offs_raw) if hasattr(offs_raw,'__len__') else s_start.shape[1]\n",
        "                s_log = s_start[fi][:M] * w\n",
        "                e_log = s_end[fi][:M] * w\n",
        "                for ti in range(M):\n",
        "                    a, b = to_pair(offs_raw[ti])\n",
        "                    if b > a and 0 <= a < len(ctx) and 1 <= b <= len(ctx):\n",
        "                        S[a] += s_log[ti]\n",
        "                        E[b-1] += e_log[ti]\n",
        "        if do_pool:\n",
        "            S = maxpool1d(S); E = maxpool1d(E)\n",
        "        # Decode\n",
        "        starts = np.argsort(S)[::-1][:K]\n",
        "        best_score = -1e18\n",
        "        best_span = (0, max(0, min(Lmax, len(ctx))-1))\n",
        "        for si in starts:\n",
        "            end_lo = si; end_hi = min(len(ctx)-1, si + Lmax - 1)\n",
        "            if end_hi < end_lo: continue\n",
        "            seg = E[end_lo:end_hi+1]\n",
        "            ej = end_lo + int(np.argmax(seg))\n",
        "            raw = float(S[si] + E[ej])\n",
        "            if lambda_len > 0.0:\n",
        "                clen = ej - si + 1\n",
        "                lp = log_normal_logpdf_len(clen, mu, sigma)\n",
        "                lp = max(clip_prior[0], min(clip_prior[1], lp))\n",
        "                raw += lambda_len * lp\n",
        "            if raw > best_score:\n",
        "                best_score = raw; best_span = (si, ej)\n",
        "        a, b = best_span\n",
        "        text = clean_span_text(ctx[a:b+1], lang)\n",
        "        if not text:\n",
        "            b2 = min(len(ctx), a+1)\n",
        "            text = clean_span_text(ctx[a:b2], lang) or ctx[a:b2]\n",
        "        out_rows.append((qid, text))\n",
        "    sub = pd.DataFrame(out_rows, columns=['id','PredictionString'])\n",
        "    empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "    mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "    print(f'Multi-stream char-fusion done in {time.time()-t0:.1f}s. Empties={empties}, mean_len={mean_len:.2f}')\n",
        "    return sub\n",
        "\n",
        "# Build submission\n",
        "sub_ms = decode_charfusion_multistream(lambda_len=0.15, nbest_hi=200, nbest_ta=240, Lmax_hi=50, Lmax_ta=60, do_pool=True)\n",
        "out_ms = 'submission_charfusion_multistream_512_384_muril_hi80_15_5_ta95_5.csv'\n",
        "sub_ms.to_csv(out_ms, index=False)\n",
        "pd.read_csv(out_ms).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_ms)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded stream xlmr512 (1401, 512) features and offsets shape (1401, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded stream xlmr384 (1921, 384) features and offsets shape (1921, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded stream muril (1513, 384) features and offsets shape (1513, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-stream char-fusion done in 3.8s. Empties=0, mean_len=10.03\nsubmission.csv updated -> submission_charfusion_multistream_512_384_muril_hi80_15_5_ta95_5.csv\n"
          ]
        }
      ]
    },
    {
      "id": "6413adb2-63a8-471e-a400-853d5f08e049",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Expert-recommended final variants: Primary (multistream+snap) and Safety (512-only+freq)\n",
        "import json, math, time, re, unicodedata as ud\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Common data\n",
        "test = pd.read_csv('test.csv')\n",
        "id2lang = dict(zip(test['id'].tolist(), test['language'].tolist()))\n",
        "id2context = dict(zip(test['id'].tolist(), test['context'].astype(str).tolist()))\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "ZW_CHARS = {'\\u200B','\\u200C','\\u200D','\\u2060','\\ufeff'}\n",
        "NBSP_SET = {'\\u00A0','\\u2002','\\u2003','\\u2004','\\u2005','\\u2006','\\u2007','\\u2008','\\u2009','\\u200A'}\n",
        "HI_VIRAMA = '\\u094D'\n",
        "TA_PULLI = '\\u0BCD'\n",
        "DANDA = '\\u0964'\n",
        "def clean_span_text(s: str, lang: str) -> str:\n",
        "    if not s: return ''\n",
        "    s = ''.join(ch for ch in s if ch not in ZW_CHARS)\n",
        "    s = ''.join(' ' if ch in NBSP_SET else ch for ch in s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    s = re.sub(r'(?<=\\d)[\\s,._-](?=\\d)', '', s)\n",
        "    if s:\n",
        "        last = s[-1]\n",
        "        if ud.category(last) == 'Mn' or last in (HI_VIRAMA, TA_PULLI): s = s[:-1]\n",
        "    if lang == 'hindi':\n",
        "        s = s.replace(DANDA+DANDA, DANDA)\n",
        "        if s.endswith(DANDA): s = s[:-1].rstrip()\n",
        "    return s\n",
        "\n",
        "def log_normal_logpdf_len(char_len: int, mu: float, sigma: float) -> float:\n",
        "    Lc = max(1, int(char_len))\n",
        "    x = math.log(Lc)\n",
        "    return -0.5*((x-mu)/sigma)**2 - math.log(max(Lc,1e-6)) - math.log(sigma) - 0.5*math.log(2*math.pi)\n",
        "\n",
        "def to_pair(t):\n",
        "    try:\n",
        "        if t is None: return (0,0)\n",
        "        if isinstance(t,(list,tuple,np.ndarray)) and len(t)>=2:\n",
        "            a = int(t[0]) if t[0] is not None else 0\n",
        "            b = int(t[1]) if t[1] is not None else 0\n",
        "            return (a,b)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return (0,0)\n",
        "\n",
        "def load_npz_logits(npz_path):\n",
        "    arr = np.load(npz_path, allow_pickle=True)\n",
        "    keys = list(arr.keys())\n",
        "    if 'start' in keys and 'end' in keys:\n",
        "        return arr['start'], arr['end']\n",
        "    for sk in ['start_logits','test_start_logits','start_logits_avg']:\n",
        "        for ek in ['end_logits','test_end_logits','end_logits_avg']:\n",
        "            if sk in keys and ek in keys:\n",
        "                return arr[sk], arr[ek]\n",
        "    raise ValueError(f'Unknown keys in {npz_path}: {keys}')\n",
        "\n",
        "def maxpool1d(x):\n",
        "    if len(x) == 0: return x\n",
        "    y = x.copy()\n",
        "    if len(x) == 1: return y\n",
        "    y[0] = max(x[0], x[1])\n",
        "    for i in range(1, len(x)-1): y[i] = max(x[i-1], x[i], x[i+1])\n",
        "    y[-1] = max(x[-2], x[-1])\n",
        "    return y\n",
        "\n",
        "def is_boundary_char(ch: str) -> bool:\n",
        "    return ch.isspace() or ch in {'.', ',', '!', '?', DANDA}\n",
        "\n",
        "def snap_span(ctx: str, S: np.ndarray, E: np.ndarray, a: int, b: int, delta: float = 0.04):\n",
        "    base = float(S[a] + E[b])\n",
        "    # find previous boundary for left\n",
        "    a2 = a\n",
        "    for i in range(a-1, -1, -1):\n",
        "        if is_boundary_char(ctx[i]):\n",
        "            a2 = i+1\n",
        "            break\n",
        "    # find next boundary for right (b inclusive index for E)\n",
        "    b2 = b\n",
        "    for j in range(b, len(ctx)):\n",
        "        if is_boundary_char(ctx[j]):\n",
        "            b2 = max(a2, j-1)\n",
        "            break\n",
        "    # evaluate snapped vs original\n",
        "    cand = float(S[a2] + E[b2])\n",
        "    if cand >= base - delta:\n",
        "        return a2, b2\n",
        "    return a, b\n",
        "\n",
        "# Load streams\n",
        "streams = [\n",
        "    dict(name='xlmr512', npz='xlmr_large_512_3seeds_avg.npz', map_dir='xlmr_large_512_test_logits'),\n",
        "    dict(name='xlmr384', npz='xlmr_large_test_avg.npz', map_dir='xlmr_large_test_logits'),\n",
        "    dict(name='muril',   npz='muril_large_test_avg.npz',   map_dir='muril_large_test_logits'),\n",
        "]\n",
        "loaded = []\n",
        "for s in streams:\n",
        "    try:\n",
        "        s_start, s_end = load_npz_logits(s['npz'])\n",
        "        eid_path = Path(s['map_dir']) / 'test_example_id.json'\n",
        "        offs_path = Path(s['map_dir']) / 'test_offset_mapping.npy'\n",
        "        eid = json.loads(eid_path.read_text())\n",
        "        offs = np.load(offs_path, allow_pickle=True)\n",
        "        loaded.append((s['name'], s_start, s_end, eid, offs))\n",
        "        print('Loaded', s['name'], s_start.shape, 'offsets', getattr(offs,'shape',None))\n",
        "    except Exception as e:\n",
        "        print('Skip stream', s['name'], '->', e)\n",
        "\n",
        "def decode_primary_multistream():\n",
        "    # Weights per language\n",
        "    weights_hi = {'xlmr512': 0.85, 'xlmr384': 0.10, 'muril': 0.05}\n",
        "    weights_ta = {'xlmr512': 0.97, 'xlmr384': 0.03, 'muril': 0.00}\n",
        "    lambda_len = 0.15\n",
        "    K_hi, K_ta = 210, 250\n",
        "    Lmax_hi, Lmax_ta = 52, 62\n",
        "    t0 = time.time()\n",
        "    # Build per-stream index maps\n",
        "    idx_maps = []\n",
        "    for name, s_start, s_end, eid, offs in loaded:\n",
        "        m = {}\n",
        "        for i, qid in enumerate(eid): m.setdefault(qid, []).append(i)\n",
        "        idx_maps.append(m)\n",
        "    out_rows = []\n",
        "    for qid in test['id'].tolist():\n",
        "        lang = id2lang[qid]; ctx = id2context[qid]\n",
        "        mu = priors.get(lang, {}).get('mu', 2.3 if lang=='hindi' else 2.1)\n",
        "        sigma = priors.get(lang, {}).get('sigma', 0.8 if lang=='hindi' else 0.7)\n",
        "        Lmax = Lmax_hi if lang=='hindi' else Lmax_ta\n",
        "        K = K_hi if lang=='hindi' else K_ta\n",
        "        S = np.zeros(len(ctx), dtype=np.float32)\n",
        "        E = np.zeros(len(ctx), dtype=np.float32)\n",
        "        for (name, s_start, s_end, eid, offs), m in zip(loaded, idx_maps):\n",
        "            w = (weights_hi if lang=='hindi' else weights_ta).get(name, 0.0)\n",
        "            if w <= 0.0: continue\n",
        "            feat_idx = m.get(qid, [])\n",
        "            for fi in feat_idx:\n",
        "                offs_raw = offs[fi]\n",
        "                M = len(offs_raw) if hasattr(offs_raw,'__len__') else s_start.shape[1]\n",
        "                s_log = s_start[fi][:M] * w\n",
        "                e_log = s_end[fi][:M] * w\n",
        "                for ti in range(M):\n",
        "                    a, b = to_pair(offs_raw[ti])\n",
        "                    if b > a and 0 <= a < len(ctx) and 1 <= b <= len(ctx):\n",
        "                        S[a] += s_log[ti]\n",
        "                        E[b-1] += e_log[ti]\n",
        "        S = maxpool1d(S); E = maxpool1d(E)\n",
        "        starts = np.argsort(S)[::-1][:K]\n",
        "        best_score = -1e18\n",
        "        best_span = (0, max(0, min(Lmax, len(ctx))-1))\n",
        "        for si in starts:\n",
        "            end_lo = si; end_hi = min(len(ctx)-1, si + Lmax - 1)\n",
        "            if end_hi < end_lo: continue\n",
        "            seg = E[end_lo:end_hi+1]\n",
        "            ej = end_lo + int(np.argmax(seg))\n",
        "            raw = float(S[si] + E[ej])\n",
        "            if lambda_len > 0.0:\n",
        "                clen = ej - si + 1\n",
        "                lp = log_normal_logpdf_len(clen, mu, sigma)\n",
        "                lp = max(-0.7, min(0.0, lp))\n",
        "                raw += lambda_len * lp\n",
        "            if raw > best_score:\n",
        "                best_score = raw; best_span = (si, ej)\n",
        "        a, b = best_span\n",
        "        a, b = snap_span(ctx, S, E, a, b, delta=0.04)\n",
        "        text = clean_span_text(ctx[a:b+1], lang)\n",
        "        if not text:\n",
        "            b2 = min(len(ctx), a+1)\n",
        "            text = clean_span_text(ctx[a:b2], lang) or ctx[a:b2]\n",
        "        out_rows.append((qid, text))\n",
        "    sub = pd.DataFrame(out_rows, columns=['id','PredictionString'])\n",
        "    empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "    mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "    print(f'Primary multistream+snap done in {time.time()-t0:.1f}s. Empties={empties}, mean_len={mean_len:.2f}')\n",
        "    return sub\n",
        "\n",
        "def decode_safety_512only():\n",
        "    # Use only xlmr512\n",
        "    lambda_len = 0.10; beta = 0.10\n",
        "    K_hi, K_ta = 210, 250\n",
        "    Lmax_hi, Lmax_ta = 48, 58\n",
        "    t0 = time.time()\n",
        "    # find 512 stream\n",
        "    s512 = None\n",
        "    for tpl in loaded:\n",
        "        if tpl[0] == 'xlmr512': s512 = tpl\n",
        "    assert s512 is not None, 'xlmr512 stream missing'\n",
        "    name, s_start, s_end, eid, offs_all = s512\n",
        "    # index map\n",
        "    m = {}\n",
        "    for i, qid in enumerate(eid): m.setdefault(qid, []).append(i)\n",
        "    out_rows = []\n",
        "    for qid in test['id'].tolist():\n",
        "        lang = id2lang[qid]; ctx = id2context[qid]\n",
        "        mu = priors.get(lang, {}).get('mu', 2.3 if lang=='hindi' else 2.1)\n",
        "        sigma = priors.get(lang, {}).get('sigma', 0.8 if lang=='hindi' else 0.7)\n",
        "        Lmax = Lmax_hi if lang=='hindi' else Lmax_ta\n",
        "        K = K_hi if lang=='hindi' else K_ta\n",
        "        S = np.zeros(len(ctx), dtype=np.float32)\n",
        "        E = np.zeros(len(ctx), dtype=np.float32)\n",
        "        for fi in m.get(qid, []):\n",
        "            offs_raw = offs_all[fi]\n",
        "            M = len(offs_raw) if hasattr(offs_raw,'__len__') else s_start.shape[1]\n",
        "            s_log = s_start[fi][:M]\n",
        "            e_log = s_end[fi][:M]\n",
        "            for ti in range(M):\n",
        "                a, b = to_pair(offs_raw[ti])\n",
        "                if b > a and 0 <= a < len(ctx) and 1 <= b <= len(ctx):\n",
        "                    S[a] += s_log[ti]\n",
        "                    E[b-1] += e_log[ti]\n",
        "        S = maxpool1d(S); E = maxpool1d(E)\n",
        "        starts = np.argsort(S)[::-1][:K]\n",
        "        best_score = -1e18\n",
        "        best_span = (0, max(0, min(Lmax, len(ctx))-1))\n",
        "        for si in starts:\n",
        "            end_lo = si; end_hi = min(len(ctx)-1, si + Lmax - 1)\n",
        "            if end_hi < end_lo: continue\n",
        "            seg = E[end_lo:end_hi+1]\n",
        "            # shortlist top ends (8-10)\n",
        "            top = np.argsort(seg)[::-1][:10]\n",
        "            for rel in top:\n",
        "                ej = end_lo + int(rel)\n",
        "                raw = float(S[si] + E[ej])\n",
        "                clen = ej - si + 1\n",
        "                if lambda_len > 0.0:\n",
        "                    lp = log_normal_logpdf_len(clen, mu, sigma)\n",
        "                    lp = max(-0.7, min(0.0, lp))\n",
        "                    raw += lambda_len * lp\n",
        "                # frequency prior on shortlist\n",
        "                span_text = ctx[si:ej+1]\n",
        "                if span_text:\n",
        "                    raw += beta * math.log(1 + ctx.count(span_text))\n",
        "                if raw > best_score:\n",
        "                    best_score = raw; best_span = (si, ej)\n",
        "        a, b = best_span\n",
        "        text = clean_span_text(ctx[a:b+1], lang)\n",
        "        if not text:\n",
        "            b2 = min(len(ctx), a+1)\n",
        "            text = clean_span_text(ctx[a:b2], lang) or ctx[a:b2]\n",
        "        out_rows.append((qid, text))\n",
        "    sub = pd.DataFrame(out_rows, columns=['id','PredictionString'])\n",
        "    empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "    mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "    print(f'Safety 512-only+freq done in {time.time()-t0:.1f}s. Empties={empties}, mean_len={mean_len:.2f}')\n",
        "    return sub\n",
        "\n",
        "# Build both submissions\n",
        "sub_primary = decode_primary_multistream()\n",
        "primary_path = 'submission_primary_multistream_snap_hi85_10_5_ta97_3_0_lambda015.csv'\n",
        "sub_primary.to_csv(primary_path, index=False)\n",
        "sub_safety = decode_safety_512only()\n",
        "safety_path = 'submission_safety_512only_lambda010_freq010.csv'\n",
        "sub_safety.to_csv(safety_path, index=False)\n",
        "# Set primary as submission.csv\n",
        "pd.read_csv(primary_path).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', primary_path)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded xlmr512 (1401, 512) offsets (1401, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded xlmr384 (1921, 384) offsets (1921, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded muril (1513, 384) offsets (1513, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primary multistream+snap done in 3.8s. Empties=0, mean_len=10.55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Safety 512-only+freq done in 4.6s. Empties=0, mean_len=13.18\nsubmission.csv updated -> submission_primary_multistream_snap_hi85_10_5_ta97_3_0_lambda015.csv\n"
          ]
        }
      ]
    },
    {
      "id": "79810dc7-cc4f-49d9-8c98-5063c5341f18",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Switch submission.csv to Safety variant\n",
        "import pandas as pd\n",
        "safety_path = 'submission_safety_512only_lambda010_freq010.csv'\n",
        "pd.read_csv(safety_path).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', safety_path)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv updated -> submission_safety_512only_lambda010_freq010.csv\n"
          ]
        }
      ]
    },
    {
      "id": "9fa919bd-ab5c-476e-9a55-17e72bf2fa24",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional third variant: Multi-stream, drop MuRIL entirely + snap (lambda=0.10)\n",
        "import json, math, time, re, unicodedata as ud\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id2lang = dict(zip(test['id'].tolist(), test['language'].tolist()))\n",
        "id2context = dict(zip(test['id'].tolist(), test['context'].astype(str).tolist()))\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "ZW_CHARS = {'\\u200B','\\u200C','\\u200D','\\u2060','\\ufeff'}\n",
        "NBSP_SET = {'\\u00A0','\\u2002','\\u2003','\\u2004','\\u2005','\\u2006','\\u2007','\\u2008','\\u2009','\\u200A'}\n",
        "HI_VIRAMA = '\\u094D'\n",
        "TA_PULLI = '\\u0BCD'\n",
        "DANDA = '\\u0964'\n",
        "def clean_span_text(s: str, lang: str) -> str:\n",
        "    if not s: return ''\n",
        "    s = ''.join(ch for ch in s if ch not in ZW_CHARS)\n",
        "    s = ''.join(' ' if ch in NBSP_SET else ch for ch in s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    s = re.sub(r'(?<=\\d)[\\s,._-](?=\\d)', '', s)\n",
        "    if s:\n",
        "        last = s[-1]\n",
        "        if ud.category(last) == 'Mn' or last in (HI_VIRAMA, TA_PULLI): s = s[:-1]\n",
        "    if lang == 'hindi':\n",
        "        s = s.replace(DANDA+DANDA, DANDA)\n",
        "        if s.endswith(DANDA): s = s[:-1].rstrip()\n",
        "    return s\n",
        "\n",
        "def log_normal_logpdf_len(char_len: int, mu: float, sigma: float) -> float:\n",
        "    Lc = max(1, int(char_len))\n",
        "    x = math.log(Lc)\n",
        "    return -0.5*((x-mu)/sigma)**2 - math.log(max(Lc,1e-6)) - math.log(sigma) - 0.5*math.log(2*math.pi)\n",
        "\n",
        "def to_pair(t):\n",
        "    try:\n",
        "        if t is None: return (0,0)\n",
        "        if isinstance(t,(list,tuple,np.ndarray)) and len(t)>=2:\n",
        "            a = int(t[0]) if t[0] is not None else 0\n",
        "            b = int(t[1]) if t[1] is not None else 0\n",
        "            return (a,b)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return (0,0)\n",
        "\n",
        "def load_npz_logits(npz_path):\n",
        "    arr = np.load(npz_path, allow_pickle=True)\n",
        "    keys = list(arr.keys())\n",
        "    if 'start' in keys and 'end' in keys:\n",
        "        return arr['start'], arr['end']\n",
        "    for sk in ['start_logits','test_start_logits','start_logits_avg']:\n",
        "        for ek in ['end_logits','test_end_logits','end_logits_avg']:\n",
        "            if sk in keys and ek in keys:\n",
        "                return arr[sk], arr[ek]\n",
        "    raise ValueError(f'Unknown keys in {npz_path}: {keys}')\n",
        "\n",
        "def maxpool1d(x):\n",
        "    if len(x) == 0: return x\n",
        "    y = x.copy()\n",
        "    if len(x) == 1: return y\n",
        "    y[0] = max(x[0], x[1])\n",
        "    for i in range(1, len(x)-1): y[i] = max(x[i-1], x[i], x[i+1])\n",
        "    y[-1] = max(x[-2], x[-1])\n",
        "    return y\n",
        "\n",
        "def is_boundary_char(ch: str) -> bool:\n",
        "    return ch.isspace() or ch in {'.', ',', '!', '?', DANDA}\n",
        "\n",
        "def snap_span(ctx: str, S: np.ndarray, E: np.ndarray, a: int, b: int, delta: float = 0.04):\n",
        "    base = float(S[a] + E[b])\n",
        "    a2 = a\n",
        "    for i in range(a-1, -1, -1):\n",
        "        if is_boundary_char(ctx[i]):\n",
        "            a2 = i+1\n",
        "            break\n",
        "    b2 = b\n",
        "    for j in range(b, len(ctx)):\n",
        "        if is_boundary_char(ctx[j]):\n",
        "            b2 = max(a2, j-1)\n",
        "            break\n",
        "    cand = float(S[a2] + E[b2])\n",
        "    if cand >= base - delta:\n",
        "        return a2, b2\n",
        "    return a, b\n",
        "\n",
        "# Load only required streams (drop MuRIL)\n",
        "streams = [\n",
        "    dict(name='xlmr512', npz='xlmr_large_512_3seeds_avg.npz', map_dir='xlmr_large_512_test_logits'),\n",
        "    dict(name='xlmr384', npz='xlmr_large_test_avg.npz', map_dir='xlmr_large_test_logits'),\n",
        "]\n",
        "loaded = []\n",
        "for s in streams:\n",
        "    try:\n",
        "        s_start, s_end = load_npz_logits(s['npz'])\n",
        "        eid_path = Path(s['map_dir']) / 'test_example_id.json'\n",
        "        offs_path = Path(s['map_dir']) / 'test_offset_mapping.npy'\n",
        "        eid = json.loads(eid_path.read_text())\n",
        "        offs = np.load(offs_path, allow_pickle=True)\n",
        "        loaded.append((s['name'], s_start, s_end, eid, offs))\n",
        "        print('Loaded', s['name'], s_start.shape, 'offsets', getattr(offs,'shape',None))\n",
        "    except Exception as e:\n",
        "        print('Skip stream', s['name'], '->', e)\n",
        "\n",
        "def decode_third_variant():\n",
        "    # Weights (drop MuRIL entirely)\n",
        "    weights_hi = {'xlmr512': 0.90, 'xlmr384': 0.10}\n",
        "    weights_ta = {'xlmr512': 0.99, 'xlmr384': 0.01}\n",
        "    lambda_len = 0.10\n",
        "    K_hi, K_ta = 200, 240\n",
        "    Lmax_hi, Lmax_ta = 50, 58\n",
        "    t0 = time.time()\n",
        "    idx_maps = []\n",
        "    for name, s_start, s_end, eid, offs in loaded:\n",
        "        m = {}\n",
        "        for i, qid in enumerate(eid): m.setdefault(qid, []).append(i)\n",
        "        idx_maps.append(m)\n",
        "    out_rows = []\n",
        "    for qid in test['id'].tolist():\n",
        "        lang = id2lang[qid]; ctx = id2context[qid]\n",
        "        mu = priors.get(lang, {}).get('mu', 2.3 if lang=='hindi' else 2.1)\n",
        "        sigma = priors.get(lang, {}).get('sigma', 0.8 if lang=='hindi' else 0.7)\n",
        "        Lmax = Lmax_hi if lang=='hindi' else Lmax_ta\n",
        "        K = K_hi if lang=='hindi' else K_ta\n",
        "        S = np.zeros(len(ctx), dtype=np.float32)\n",
        "        E = np.zeros(len(ctx), dtype=np.float32)\n",
        "        for (name, s_start, s_end, eid, offs), m in zip(loaded, idx_maps):\n",
        "            w = (weights_hi if lang=='hindi' else weights_ta).get(name, 0.0)\n",
        "            if w <= 0.0: continue\n",
        "            feat_idx = m.get(qid, [])\n",
        "            for fi in feat_idx:\n",
        "                offs_raw = offs[fi]\n",
        "                M = len(offs_raw) if hasattr(offs_raw,'__len__') else s_start.shape[1]\n",
        "                s_log = s_start[fi][:M] * w\n",
        "                e_log = s_end[fi][:M] * w\n",
        "                for ti in range(M):\n",
        "                    a, b = to_pair(offs_raw[ti])\n",
        "                    if b > a and 0 <= a < len(ctx) and 1 <= b <= len(ctx):\n",
        "                        S[a] += s_log[ti]\n",
        "                        E[b-1] += e_log[ti]\n",
        "        S = maxpool1d(S); E = maxpool1d(E)\n",
        "        starts = np.argsort(S)[::-1][:K]\n",
        "        best_score = -1e18\n",
        "        best_span = (0, max(0, min(Lmax, len(ctx))-1))\n",
        "        for si in starts:\n",
        "            end_lo = si; end_hi = min(len(ctx)-1, si + Lmax - 1)\n",
        "            if end_hi < end_lo: continue\n",
        "            seg = E[end_lo:end_hi+1]\n",
        "            ej = end_lo + int(np.argmax(seg))\n",
        "            raw = float(S[si] + E[ej])\n",
        "            if lambda_len > 0.0:\n",
        "                clen = ej - si + 1\n",
        "                lp = log_normal_logpdf_len(clen, mu, sigma)\n",
        "                lp = max(-0.7, min(0.0, lp))\n",
        "                raw += lambda_len * lp\n",
        "            if raw > best_score:\n",
        "                best_score = raw; best_span = (si, ej)\n",
        "        a, b = best_span\n",
        "        a, b = snap_span(ctx, S, E, a, b, delta=0.04)\n",
        "        text = clean_span_text(ctx[a:b+1], lang)\n",
        "        if not text:\n",
        "            b2 = min(len(ctx), a+1)\n",
        "            text = clean_span_text(ctx[a:b2], lang) or ctx[a:b2]\n",
        "        out_rows.append((qid, text))\n",
        "    sub = pd.DataFrame(out_rows, columns=['id','PredictionString'])\n",
        "    empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "    mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "    print(f'Third variant (drop MuRIL) done in {time.time()-t0:.1f}s. Empties={empties}, mean_len={mean_len:.2f}')\n",
        "    return sub\n",
        "\n",
        "sub_third = decode_third_variant()\n",
        "third_path = 'submission_third_multistream_drop_muril_snap_lambda010.csv'\n",
        "sub_third.to_csv(third_path, index=False)\n",
        "pd.read_csv(third_path).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', third_path)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded xlmr512 (1401, 512) offsets (1401, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded xlmr384 (1921, 384) offsets (1921, 384)\n"
          ]
        }
      ]
    },
    {
      "id": "b36d5f18-d3aa-437d-ad88-51840a21ad83",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Force submission.csv to the third-variant file and print quick diagnostics\n",
        "import pandas as pd, os, time\n",
        "third_path = 'submission_third_multistream_drop_muril_snap_lambda010.csv'\n",
        "assert os.path.exists(third_path), f\"Missing {third_path}\"\n",
        "sub = pd.read_csv(third_path)\n",
        "empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', third_path)\n",
        "print('Diagnostics: empties=', int(empties), 'mean_len=', round(float(mean_len), 2))\n",
        "print('mtime(submission.csv)=', time.ctime(os.path.getmtime('submission.csv')))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv updated -> submission_third_multistream_drop_muril_snap_lambda010.csv\nDiagnostics: empties= 0 mean_len= 10.79\nmtime(submission.csv)= Thu Sep 25 10:55:33 2025\n"
          ]
        }
      ]
    },
    {
      "id": "5043bc09-136c-4fcc-9cbe-27369bea4711",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Medal-push multistream (xlmr512 + tiny xlmr384), no MuRIL, tight snap, tiny freq prior\n",
        "import json, math, time, re, unicodedata as ud\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id2lang = dict(zip(test['id'].tolist(), test['language'].tolist()))\n",
        "id2context = dict(zip(test['id'].tolist(), test['context'].astype(str).tolist()))\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "ZW_CHARS = {'\\u200B','\\u200C','\\u200D','\\u2060','\\ufeff'}\n",
        "NBSP_SET = {'\\u00A0','\\u2002','\\u2003','\\u2004','\\u2005','\\u2006','\\u2007','\\u2008','\\u2009','\\u200A'}\n",
        "HI_VIRAMA = '\\u094D'; TA_PULLI = '\\u0BCD'; DANDA = '\\u0964'\n",
        "def clean_span_text(s: str, lang: str) -> str:\n",
        "    if not s: return ''\n",
        "    s = ''.join(ch for ch in s if ch not in ZW_CHARS)\n",
        "    s = ''.join(' ' if ch in NBSP_SET else ch for ch in s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    s = re.sub(r'(?<=\\d)[\\s,._-](?=\\d)', '', s)\n",
        "    if s and (ud.category(s[-1]) == 'Mn' or s[-1] in (HI_VIRAMA, TA_PULLI)):\n",
        "        s = s[:-1]\n",
        "    if lang == 'hindi':\n",
        "        s = s.replace(DANDA+DANDA, DANDA)\n",
        "        if s.endswith(DANDA): s = s[:-1].rstrip()\n",
        "    return s\n",
        "\n",
        "def log_normal_logpdf_len(char_len: int, mu: float, sigma: float) -> float:\n",
        "    Lc = max(1, int(char_len))\n",
        "    x = math.log(Lc)\n",
        "    return -0.5*((x-mu)/sigma)**2 - math.log(max(Lc,1e-6)) - math.log(sigma) - 0.5*math.log(2*math.pi)\n",
        "\n",
        "def to_pair(t):\n",
        "    try:\n",
        "        if t is None: return (0,0)\n",
        "        if isinstance(t,(list,tuple,np.ndarray)) and len(t)>=2:\n",
        "            a = int(t[0]) if t[0] is not None else 0\n",
        "            b = int(t[1]) if t[1] is not None else 0\n",
        "            return (a,b)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return (0,0)\n",
        "\n",
        "def load_npz_logits(npz_path):\n",
        "    arr = np.load(npz_path, allow_pickle=True)\n",
        "    keys = list(arr.keys())\n",
        "    if 'start' in keys and 'end' in keys:\n",
        "        return arr['start'], arr['end']\n",
        "    for sk in ['start_logits','test_start_logits','start_logits_avg']:\n",
        "        for ek in ['end_logits','test_end_logits','end_logits_avg']:\n",
        "            if sk in keys and ek in keys: return arr[sk], arr[ek]\n",
        "    raise ValueError(f'Unknown keys in {npz_path}: {keys}')\n",
        "\n",
        "def maxpool1d(x):\n",
        "    if len(x) == 0: return x\n",
        "    y = x.copy()\n",
        "    if len(x) == 1: return y\n",
        "    y[0] = max(x[0], x[1])\n",
        "    for i in range(1, len(x)-1): y[i] = max(x[i-1], x[i], x[i+1])\n",
        "    y[-1] = max(x[-2], x[-1])\n",
        "    return y\n",
        "\n",
        "def is_boundary_char(ch: str) -> bool:\n",
        "    return ch.isspace() or ch in {'.', ',', '!', '?', DANDA, '\"', \"'\", '(', ')', '[', ']', '{', '}'}\n",
        "\n",
        "def snap_span(ctx: str, S: np.ndarray, E: np.ndarray, a: int, b: int, delta: float = 0.03):\n",
        "    base = float(S[a] + E[b])\n",
        "    a2 = a\n",
        "    for i in range(a-1, -1, -1):\n",
        "        if is_boundary_char(ctx[i]):\n",
        "            a2 = i+1; break\n",
        "    b2 = b\n",
        "    for j in range(b, len(ctx)):\n",
        "        if is_boundary_char(ctx[j]):\n",
        "            b2 = max(a2, j-1); break\n",
        "    cand = float(S[a2] + E[b2])\n",
        "    if cand >= base - delta:\n",
        "        return a2, b2\n",
        "    if a > 0 and b < len(ctx)-1 and ctx[a-1] in {'(', '\"', \"'\"} and ctx[b+1] in {')', '\"', \"'\"}:\n",
        "        cand2 = float(S[a-1] + E[b+1])\n",
        "        if cand2 >= base - delta and (b+1 - (a-1) <= (b - a) + 2):\n",
        "            return a-1, b+1\n",
        "    return a, b\n",
        "\n",
        "# Load only xlmr512 and xlmr384\n",
        "streams = [\n",
        "    dict(name='xlmr512', npz='xlmr_large_512_3seeds_avg.npz', map_dir='xlmr_large_512_test_logits'),\n",
        "    dict(name='xlmr384', npz='xlmr_large_test_avg.npz',       map_dir='xlmr_large_test_logits'),\n",
        "]\n",
        "loaded = []\n",
        "for s in streams:\n",
        "    try:\n",
        "        s_start, s_end = load_npz_logits(s['npz'])\n",
        "        eid_path = Path(s['map_dir']) / 'test_example_id.json'\n",
        "        offs_path = Path(s['map_dir']) / 'test_offset_mapping.npy'\n",
        "        eid = json.loads(eid_path.read_text())\n",
        "        offs = np.load(offs_path, allow_pickle=True)\n",
        "        loaded.append((s['name'], s_start, s_end, eid, offs))\n",
        "        print('Loaded', s['name'], s_start.shape, 'offsets', getattr(offs,'shape',None))\n",
        "    except Exception as e:\n",
        "        print('Skip stream', s['name'], '->', e)\n",
        "\n",
        "def decode_medal_push():\n",
        "    # Weights\n",
        "    weights_hi = {'xlmr512': 0.92,  'xlmr384': 0.08}\n",
        "    weights_ta = {'xlmr512': 0.995, 'xlmr384': 0.005}\n",
        "    # Hyperparams\n",
        "    lambda_len = 0.12\n",
        "    K_hi, K_ta = 240, 280\n",
        "    Lmax_hi, Lmax_ta = 52, 62\n",
        "    clip_prior = (-0.8, 0.0)\n",
        "    beta = 0.06    # tiny frequency prior\n",
        "    shortlist = 6  # ends per start\n",
        "\n",
        "    # Build per-stream index maps\n",
        "    idx_maps = []\n",
        "    for name, s_start, s_end, eid, offs in loaded:\n",
        "        m = {}\n",
        "        for i, qid in enumerate(eid): m.setdefault(qid, []).append(i)\n",
        "        idx_maps.append(m)\n",
        "\n",
        "    out_rows = []\n",
        "    t0 = time.time()\n",
        "    for qid in test['id'].tolist():\n",
        "        lang = id2lang[qid]; ctx = id2context[qid]\n",
        "        mu = priors[lang]['mu']; sigma = priors[lang]['sigma']\n",
        "        Lmax = Lmax_hi if lang=='hindi' else Lmax_ta\n",
        "        K = K_hi if lang=='hindi' else K_ta\n",
        "        weights = weights_hi if lang=='hindi' else weights_ta\n",
        "\n",
        "        S = np.zeros(len(ctx), dtype=np.float32)\n",
        "        E = np.zeros(len(ctx), dtype=np.float32)\n",
        "\n",
        "        # Accumulate streams\n",
        "        for (name, s_start, s_end, eid, offs), m in zip(loaded, idx_maps):\n",
        "            w = weights.get(name, 0.0)\n",
        "            if w <= 0.0: continue\n",
        "            for fi in m.get(qid, []):\n",
        "                offs_raw = offs[fi]\n",
        "                M = len(offs_raw) if hasattr(offs_raw,'__len__') else s_start.shape[1]\n",
        "                s_log = s_start[fi][:M] * w\n",
        "                e_log = s_end[fi][:M] * w\n",
        "                for ti in range(M):\n",
        "                    a, b = to_pair(offs_raw[ti])\n",
        "                    if b > a and 0 <= a < len(ctx) and 1 <= b <= len(ctx):\n",
        "                        S[a] += s_log[ti]\n",
        "                        E[b-1] += e_log[ti]\n",
        "\n",
        "        # Smooth\n",
        "        S = maxpool1d(S); E = maxpool1d(E)\n",
        "\n",
        "        # Decode\n",
        "        starts = np.argsort(S)[::-1][:K]\n",
        "        best_score = -1e18\n",
        "        best_span = (0, max(0, min(Lmax, len(ctx))-1))\n",
        "        for si in starts:\n",
        "            end_lo = si; end_hi = min(len(ctx)-1, si + Lmax - 1)\n",
        "            if end_hi < end_lo: continue\n",
        "            seg = E[end_lo:end_hi+1]\n",
        "            top_rel = np.argsort(seg)[::-1][:shortlist]\n",
        "            for rel in top_rel:\n",
        "                ej = end_lo + int(rel)\n",
        "                raw = float(S[si] + E[ej])\n",
        "                clen = ej - si + 1\n",
        "                lp = log_normal_logpdf_len(clen, mu, sigma)\n",
        "                lp = max(clip_prior[0], min(clip_prior[1], lp))\n",
        "                raw += lambda_len * lp\n",
        "                if beta > 0.0:\n",
        "                    span_text = ctx[si:ej+1]\n",
        "                    if span_text and ctx.count(span_text) > 1:\n",
        "                        raw += beta * math.log(1 + ctx.count(span_text))\n",
        "                if raw > best_score:\n",
        "                    best_score = raw; best_span = (si, ej)\n",
        "\n",
        "        a, b = best_span\n",
        "        a, b = snap_span(ctx, S, E, a, b, delta=0.03)\n",
        "        text = clean_span_text(ctx[a:b+1], lang)\n",
        "        if not text:\n",
        "            b2 = min(len(ctx), a+1)\n",
        "            text = clean_span_text(ctx[a:b2], lang) or ctx[a:b2]\n",
        "        out_rows.append((qid, text))\n",
        "\n",
        "    sub = pd.DataFrame(out_rows, columns=['id','PredictionString'])\n",
        "    empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "    mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "    print(f'Medal-push multistream done in {time.time()-t0:.1f}s. Empties={empties}, mean_len={mean_len:.2f}')\n",
        "    return sub\n",
        "\n",
        "sub = decode_medal_push()\n",
        "out_path = 'submission_medalpush_2stream_lambda012_K240_280_Lmax52_62_beta006_delta003.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "pd.read_csv(out_path).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_path)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded xlmr512 (1401, 512) offsets (1401, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded xlmr384 (1921, 384) offsets (1921, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Medal-push multistream done in 4.5s. Empties=0, mean_len=13.77\nsubmission.csv updated -> submission_medalpush_2stream_lambda012_K240_280_Lmax52_62_beta006_delta003.csv\n"
          ]
        }
      ]
    },
    {
      "id": "9e63e4ca-d2c9-48cd-b3d3-687f9a14fcb7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 384-only sanity decode (alignment check): char-fusion single-stream with its own mapping\n",
        "import numpy as np, pandas as pd, time, math, json, re, unicodedata as ud\n",
        "from pathlib import Path\n",
        "\n",
        "def load_npz_logits(npz_path):\n",
        "    arr = np.load(npz_path, allow_pickle=True)\n",
        "    keys = list(arr.keys())\n",
        "    if 'start' in keys and 'end' in keys:\n",
        "        return arr['start'], arr['end']\n",
        "    for sk in ['start_logits','test_start_logits','start_logits_avg']:\n",
        "        for ek in ['end_logits','test_end_logits','end_logits_avg']:\n",
        "            if sk in keys and ek in keys: return arr[sk], arr[ek]\n",
        "    raise ValueError(f'Unknown keys in {npz_path}: {keys}')\n",
        "\n",
        "npz_path = 'xlmr_large_test_avg.npz'  # 384\n",
        "start_logits, end_logits = load_npz_logits(npz_path)\n",
        "Nfeat, Ltok = start_logits.shape\n",
        "print('Loaded 384 logits:', start_logits.shape, end_logits.shape, flush=True)\n",
        "\n",
        "map_dir = 'xlmr_large_test_logits'  # 384 mapping (must match)\n",
        "eid_path = Path(map_dir) / 'test_example_id.json'\n",
        "off_path = Path(map_dir) / 'test_offset_mapping.npy'\n",
        "example_id_list = json.loads(Path(eid_path).read_text())\n",
        "offset_mapping = np.load(off_path, allow_pickle=True)\n",
        "print('Loaded 384 mapping:', len(example_id_list), getattr(offset_mapping,'shape',None), flush=True)\n",
        "assert len(example_id_list) == Nfeat\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id2lang = dict(zip(test['id'].tolist(), test['language'].tolist()))\n",
        "id2context = dict(zip(test['id'].tolist(), test['context'].astype(str).tolist()))\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "ZW_CHARS = {'\\u200B','\\u200C','\\u200D','\\u2060','\\ufeff'}\n",
        "NBSP_SET = {'\\u00A0','\\u2002','\\u2003','\\u2004','\\u2005','\\u2006','\\u2007','\\u2008','\\u2009','\\u200A'}\n",
        "HI_VIRAMA = '\\u094D'; TA_PULLI='\\u0BCD'; DANDA='\\u0964'\n",
        "def clean_span_text(s: str, lang: str) -> str:\n",
        "    if not s: return ''\n",
        "    s = ''.join(ch for ch in s if ch not in ZW_CHARS)\n",
        "    s = ''.join(' ' if ch in NBSP_SET else ch for ch in s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    s = re.sub(r'(?<=\\d)[\\s,._-](?=\\d)', '', s)\n",
        "    if s:\n",
        "        last = s[-1]\n",
        "        if ud.category(last) == 'Mn' or last in (HI_VIRAMA, TA_PULLI): s = s[:-1]\n",
        "    if lang == 'hindi':\n",
        "        s = s.replace(DANDA+DANDA, DANDA)\n",
        "        if s.endswith(DANDA): s = s[:-1].rstrip()\n",
        "    return s\n",
        "\n",
        "def log_normal_logpdf_len(char_len: int, mu: float, sigma: float) -> float:\n",
        "    Lc = max(1, int(char_len))\n",
        "    x = math.log(Lc)\n",
        "    return -0.5*((x-mu)/sigma)**2 - math.log(max(Lc,1e-6)) - math.log(sigma) - 0.5*math.log(2*math.pi)\n",
        "\n",
        "def to_pair(t):\n",
        "    try:\n",
        "        if t is None: return (0,0)\n",
        "        if isinstance(t,(list,tuple,np.ndarray)) and len(t)>=2:\n",
        "            a = int(t[0]) if t[0] is not None else 0\n",
        "            b = int(t[1]) if t[1] is not None else 0\n",
        "            return (a,b)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return (0,0)\n",
        "\n",
        "qid_to_feat_idx = {}\n",
        "for i, qid in enumerate(example_id_list):\n",
        "    qid_to_feat_idx.setdefault(qid, []).append(i)\n",
        "\n",
        "def maxpool1d(x):\n",
        "    if len(x) == 0: return x\n",
        "    y = x.copy()\n",
        "    if len(x) == 1: return y\n",
        "    y[0] = max(x[0], x[1])\n",
        "    for i in range(1, len(x)-1): y[i] = max(x[i-1], x[i], x[i+1])\n",
        "    y[-1] = max(x[-2], x[-1])\n",
        "    return y\n",
        "\n",
        "def decode_charfusion_384(lambda_len=0.15, clip_prior=(-0.7,0.0), nbest_hi=220, nbest_ta=260, Lmax_hi=50, Lmax_ta=60, do_pool=True):\n",
        "    t0 = time.time()\n",
        "    out_rows = []\n",
        "    for qid in test['id'].tolist():\n",
        "        lang = id2lang[qid]; ctx = id2context[qid]\n",
        "        mu = priors[lang]['mu']; sigma = priors[lang]['sigma']\n",
        "        Lmax = Lmax_hi if lang=='hindi' else Lmax_ta\n",
        "        K = nbest_hi if lang=='hindi' else nbest_ta\n",
        "        S = np.zeros(len(ctx), dtype=np.float32)\n",
        "        E = np.zeros(len(ctx), dtype=np.float32)\n",
        "        for fi in qid_to_feat_idx.get(qid, []):\n",
        "            offs_raw = offset_mapping[fi]\n",
        "            M = len(offs_raw) if hasattr(offs_raw,'__len__') else Ltok\n",
        "            s_log = start_logits[fi][:M]\n",
        "            e_log = end_logits[fi][:M]\n",
        "            for ti in range(M):\n",
        "                a,b = to_pair(offs_raw[ti])\n",
        "                if b > a and 0 <= a < len(ctx) and 1 <= b <= len(ctx):\n",
        "                    S[a] += s_log[ti]\n",
        "                    E[b-1] += e_log[ti]\n",
        "        if do_pool:\n",
        "            S = maxpool1d(S); E = maxpool1d(E)\n",
        "        if len(S) == 0:\n",
        "            out_rows.append((qid, ctx[:1] if len(ctx)>0 else ''))\n",
        "            continue\n",
        "        starts = np.argsort(S)[::-1][:K]\n",
        "        best_score = -1e18; best_span = (0, max(0, min(Lmax, len(ctx))-1))\n",
        "        for si in starts:\n",
        "            end_lo = si; end_hi = min(len(ctx)-1, si + Lmax - 1)\n",
        "            if end_hi < end_lo: continue\n",
        "            seg = E[end_lo:end_hi+1]\n",
        "            ej = end_lo + int(np.argmax(seg))\n",
        "            raw = float(S[si] + E[ej])\n",
        "            clen = ej - si + 1\n",
        "            lp = log_normal_logpdf_len(clen, mu, sigma)\n",
        "            lp = max(clip_prior[0], min(clip_prior[1], lp))\n",
        "            raw += lambda_len * lp\n",
        "            if raw > best_score:\n",
        "                best_score = raw; best_span = (si, ej)\n",
        "        a,b = best_span\n",
        "        text = clean_span_text(ctx[a:b+1], lang)\n",
        "        if not text:\n",
        "            b2 = min(len(ctx), a+1); text = clean_span_text(ctx[a:b2], lang) or ctx[a:b2]\n",
        "        out_rows.append((qid, text))\n",
        "    sub = pd.DataFrame(out_rows, columns=['id','PredictionString'])\n",
        "    empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "    mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "    print(f'384-only char-fusion done in {time.time()-t0:.1f}s. Empties={empties}, mean_len={mean_len:.2f}')\n",
        "    return sub\n",
        "\n",
        "sub384 = decode_charfusion_384(lambda_len=0.15, nbest_hi=220, nbest_ta=260, Lmax_hi=50, Lmax_ta=60, do_pool=True)\n",
        "out384 = 'submission_384only_charfusion_lambda015.csv'\n",
        "sub384.to_csv(out384, index=False)\n",
        "pd.read_csv(out384).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out384)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 384 logits: (1921, 384) (1921, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 384 mapping: 1921 (1921, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "384-only char-fusion done in 2.3s. Empties=0, mean_len=9.81\nsubmission.csv updated -> submission_384only_charfusion_lambda015.csv\n"
          ]
        }
      ]
    },
    {
      "id": "6cff00fc-443b-4cc3-b128-9a8e7d1ad2be",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Medal-push (alignment-safe): use 512 single-seed test_avg + 384; no MuRIL\n",
        "import json, math, time, re, unicodedata as ud\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id2lang = dict(zip(test['id'].tolist(), test['language'].tolist()))\n",
        "id2context = dict(zip(test['id'].tolist(), test['context'].astype(str).tolist()))\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "ZW_CHARS = {'\\u200B','\\u200C','\\u200D','\\u2060','\\ufeff'}\n",
        "NBSP_SET = {'\\u00A0','\\u2002','\\u2003','\\u2004','\\u2005','\\u2006','\\u2007','\\u2008','\\u2009','\\u200A'}\n",
        "HI_VIRAMA = '\\u094D'; TA_PULLI = '\\u0BCD'; DANDA = '\\u0964'\n",
        "def clean_span_text(s: str, lang: str) -> str:\n",
        "    if not s: return ''\n",
        "    s = ''.join(ch for ch in s if ch not in ZW_CHARS)\n",
        "    s = ''.join(' ' if ch in NBSP_SET else ch for ch in s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    s = re.sub(r'(?<=\\d)[\\s,._-](?=\\d)', '', s)\n",
        "    if s and (ud.category(s[-1]) == 'Mn' or s[-1] in (HI_VIRAMA, TA_PULLI)):\n",
        "        s = s[:-1]\n",
        "    if lang == 'hindi':\n",
        "        s = s.replace(DANDA+DANDA, DANDA)\n",
        "        if s.endswith(DANDA): s = s[:-1].rstrip()\n",
        "    return s\n",
        "\n",
        "def log_normal_logpdf_len(char_len: int, mu: float, sigma: float) -> float:\n",
        "    Lc = max(1, int(char_len))\n",
        "    x = math.log(Lc)\n",
        "    return -0.5*((x-mu)/sigma)**2 - math.log(max(Lc,1e-6)) - math.log(sigma) - 0.5*math.log(2*math.pi)\n",
        "\n",
        "def to_pair(t):\n",
        "    try:\n",
        "        if t is None: return (0,0)\n",
        "        if isinstance(t,(list,tuple,np.ndarray)) and len(t)>=2:\n",
        "            a = int(t[0]) if t[0] is not None else 0\n",
        "            b = int(t[1]) if t[1] is not None else 0\n",
        "            return (a,b)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return (0,0)\n",
        "\n",
        "def load_npz_logits(npz_path):\n",
        "    arr = np.load(npz_path, allow_pickle=True)\n",
        "    keys = list(arr.keys())\n",
        "    if 'start' in keys and 'end' in keys:\n",
        "        return arr['start'], arr['end']\n",
        "    for sk in ['start_logits','test_start_logits','start_logits_avg']:\n",
        "        for ek in ['end_logits','test_end_logits','end_logits_avg']:\n",
        "            if sk in keys and ek in keys: return arr[sk], arr[ek]\n",
        "    raise ValueError(f'Unknown keys in {npz_path}: {keys}')\n",
        "\n",
        "def maxpool1d(x):\n",
        "    if len(x) == 0: return x\n",
        "    y = x.copy()\n",
        "    if len(x) == 1: return y\n",
        "    y[0] = max(x[0], x[1])\n",
        "    for i in range(1, len(x)-1): y[i] = max(x[i-1], x[i], x[i+1])\n",
        "    y[-1] = max(x[-2], x[-1])\n",
        "    return y\n",
        "\n",
        "def is_boundary_char(ch: str) -> bool:\n",
        "    return ch.isspace() or ch in {'.', ',', '!', '?', DANDA, '\"', \"'\", '(', ')', '[', ']', '{', '}'}\n",
        "\n",
        "def snap_span(ctx: str, S: np.ndarray, E: np.ndarray, a: int, b: int, delta: float = 0.03):\n",
        "    base = float(S[a] + E[b])\n",
        "    a2 = a\n",
        "    for i in range(a-1, -1, -1):\n",
        "        if is_boundary_char(ctx[i]):\n",
        "            a2 = i+1; break\n",
        "    b2 = b\n",
        "    for j in range(b, len(ctx)):\n",
        "        if is_boundary_char(ctx[j]):\n",
        "            b2 = max(a2, j-1); break\n",
        "    cand = float(S[a2] + E[b2])\n",
        "    if cand >= base - delta:\n",
        "        return a2, b2\n",
        "    if a > 0 and b < len(ctx)-1 and ctx[a-1] in {'(', '\"', \"'\"} and ctx[b+1] in {')', '\"', \"'\"}:\n",
        "        cand2 = float(S[a-1] + E[b+1])\n",
        "        if cand2 >= base - delta and (b+1 - (a-1) <= (b - a) + 2):\n",
        "            return a-1, b+1\n",
        "    return a, b\n",
        "\n",
        "# Streams: 512 single-seed test_avg (with its own mapping) + 384\n",
        "streams = [\n",
        "    dict(name='xlmr512s', npz='xlmr_large_512_test_avg.npz', map_dir='xlmr_large_512_test_logits'),\n",
        "    dict(name='xlmr384',  npz='xlmr_large_test_avg.npz',     map_dir='xlmr_large_test_logits'),\n",
        "]\n",
        "loaded = []\n",
        "for s in streams:\n",
        "    try:\n",
        "        s_start, s_end = load_npz_logits(s['npz'])\n",
        "        eid_path = Path(s['map_dir']) / 'test_example_id.json'\n",
        "        offs_path = Path(s['map_dir']) / 'test_offset_mapping.npy'\n",
        "        eid = json.loads(eid_path.read_text())\n",
        "        offs = np.load(offs_path, allow_pickle=True)\n",
        "        loaded.append((s['name'], s_start, s_end, eid, offs))\n",
        "        print('Loaded', s['name'], s_start.shape, 'offsets', getattr(offs,'shape',None))\n",
        "    except Exception as e:\n",
        "        print('Skip stream', s['name'], '->', e)\n",
        "\n",
        "def decode_medal_push_alignsafe():\n",
        "    # Per-language weights (no MuRIL)\n",
        "    weights_hi = {'xlmr512s': 0.92,  'xlmr384': 0.08}\n",
        "    weights_ta = {'xlmr512s': 0.995, 'xlmr384': 0.005}\n",
        "    # Hyperparams\n",
        "    lambda_len = 0.12\n",
        "    K_hi, K_ta = 240, 280\n",
        "    Lmax_hi, Lmax_ta = 52, 62\n",
        "    clip_prior = (-0.8, 0.0)\n",
        "    beta = 0.06\n",
        "    shortlist = 6\n",
        "\n",
        "    # index maps per stream\n",
        "    idx_maps = []\n",
        "    for name, s_start, s_end, eid, offs in loaded:\n",
        "        m = {}\n",
        "        for i, qid in enumerate(eid): m.setdefault(qid, []).append(i)\n",
        "        idx_maps.append(m)\n",
        "\n",
        "    out_rows = []\n",
        "    t0 = time.time()\n",
        "    for qid in test['id'].tolist():\n",
        "        lang = id2lang[qid]; ctx = id2context[qid]\n",
        "        mu = priors[lang]['mu']; sigma = priors[lang]['sigma']\n",
        "        Lmax = Lmax_hi if lang=='hindi' else Lmax_ta\n",
        "        K = K_hi if lang=='hindi' else K_ta\n",
        "        weights = weights_hi if lang=='hindi' else weights_ta\n",
        "\n",
        "        S = np.zeros(len(ctx), dtype=np.float32)\n",
        "        E = np.zeros(len(ctx), dtype=np.float32)\n",
        "\n",
        "        for (name, s_start, s_end, eid, offs), m in zip(loaded, idx_maps):\n",
        "            w = weights.get(name, 0.0)\n",
        "            if w <= 0.0: continue\n",
        "            for fi in m.get(qid, []):\n",
        "                offs_raw = offs[fi]\n",
        "                M = len(offs_raw) if hasattr(offs_raw,'__len__') else s_start.shape[1]\n",
        "                s_log = s_start[fi][:M] * w\n",
        "                e_log = s_end[fi][:M] * w\n",
        "                for ti in range(M):\n",
        "                    a, b = to_pair(offs_raw[ti])\n",
        "                    if b > a and 0 <= a < len(ctx) and 1 <= b <= len(ctx):\n",
        "                        S[a] += s_log[ti]\n",
        "                        E[b-1] += e_log[ti]\n",
        "\n",
        "        S = maxpool1d(S); E = maxpool1d(E)\n",
        "\n",
        "        starts = np.argsort(S)[::-1][:K]\n",
        "        best_score = -1e18\n",
        "        best_span = (0, max(0, min(Lmax, len(ctx))-1))\n",
        "        for si in starts:\n",
        "            end_lo = si; end_hi = min(len(ctx)-1, si + Lmax - 1)\n",
        "            if end_hi < end_lo: continue\n",
        "            seg = E[end_lo:end_hi+1]\n",
        "            top_rel = np.argsort(seg)[::-1][:shortlist]\n",
        "            for rel in top_rel:\n",
        "                ej = end_lo + int(rel)\n",
        "                raw = float(S[si] + E[ej])\n",
        "                clen = ej - si + 1\n",
        "                lp = log_normal_logpdf_len(clen, mu, sigma)\n",
        "                lp = max(clip_prior[0], min(clip_prior[1], lp))\n",
        "                raw += lambda_len * lp\n",
        "                if beta > 0.0:\n",
        "                    span_text = ctx[si:ej+1]\n",
        "                    if span_text and ctx.count(span_text) > 1:\n",
        "                        raw += beta * math.log(1 + ctx.count(span_text))\n",
        "                if raw > best_score:\n",
        "                    best_score = raw; best_span = (si, ej)\n",
        "\n",
        "        a, b = best_span\n",
        "        a, b = snap_span(ctx, S, E, a, b, delta=0.03)\n",
        "        text = clean_span_text(ctx[a:b+1], lang)\n",
        "        if not text:\n",
        "            b2 = min(len(ctx), a+1)\n",
        "            text = clean_span_text(ctx[a:b2], lang) or ctx[a:b2]\n",
        "        out_rows.append((qid, text))\n",
        "\n",
        "    sub = pd.DataFrame(out_rows, columns=['id','PredictionString'])\n",
        "    empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "    mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "    print(f'Align-safe 2-stream decode done in {time.time()-t0:.1f}s. Empties={empties}, mean_len={mean_len:.2f}')\n",
        "    return sub\n",
        "\n",
        "sub_as = decode_medal_push_alignsafe()\n",
        "out_path_as = 'submission_alignsafe_512single_384_lambda012_K240_280_Lmax52_62_beta006_delta003.csv'\n",
        "sub_as.to_csv(out_path_as, index=False)\n",
        "pd.read_csv(out_path_as).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_path_as)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded xlmr512s (1401, 512) offsets (1401, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded xlmr384 (1921, 384) offsets (1921, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Align-safe 2-stream decode done in 4.6s. Empties=0, mean_len=13.80\nsubmission.csv updated -> submission_alignsafe_512single_384_lambda012_K240_280_Lmax52_62_beta006_delta003.csv\n"
          ]
        }
      ]
    },
    {
      "id": "280b90c0-e625-4f73-8ef2-d77e68184861",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Hedge variant: alignment-safe 2-stream (512 single-seed + 384), beta=0.0, snap delta=0.02\n",
        "import json, math, time, re, unicodedata as ud\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id2lang = dict(zip(test['id'].tolist(), test['language'].tolist()))\n",
        "id2context = dict(zip(test['id'].tolist(), test['context'].astype(str).tolist()))\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "ZW_CHARS = {'\\u200B','\\u200C','\\u200D','\\u2060','\\ufeff'}\n",
        "NBSP_SET = {'\\u00A0','\\u2002','\\u2003','\\u2004','\\u2005','\\u2006','\\u2007','\\u2008','\\u2009','\\u200A'}\n",
        "HI_VIRAMA = '\\u094D'; TA_PULLI = '\\u0BCD'; DANDA = '\\u0964'\n",
        "def clean_span_text(s: str, lang: str) -> str:\n",
        "    if not s: return ''\n",
        "    s = ''.join(ch for ch in s if ch not in ZW_CHARS)\n",
        "    s = ''.join(' ' if ch in NBSP_SET else ch for ch in s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    s = re.sub(r'(?<=\\d)[\\s,._-](?=\\d)', '', s)\n",
        "    if s and (ud.category(s[-1]) == 'Mn' or s[-1] in (HI_VIRAMA, TA_PULLI)):\n",
        "        s = s[:-1]\n",
        "    if lang == 'hindi':\n",
        "        s = s.replace(DANDA+DANDA, DANDA)\n",
        "        if s.endswith(DANDA): s = s[:-1].rstrip()\n",
        "    return s\n",
        "\n",
        "def log_normal_logpdf_len(char_len: int, mu: float, sigma: float) -> float:\n",
        "    Lc = max(1, int(char_len))\n",
        "    x = math.log(Lc)\n",
        "    return -0.5*((x-mu)/sigma)**2 - math.log(max(Lc,1e-6)) - math.log(sigma) - 0.5*math.log(2*math.pi)\n",
        "\n",
        "def to_pair(t):\n",
        "    try:\n",
        "        if t is None: return (0,0)\n",
        "        if isinstance(t,(list,tuple,np.ndarray)) and len(t)>=2:\n",
        "            a = int(t[0]) if t[0] is not None else 0\n",
        "            b = int(t[1]) if t[1] is not None else 0\n",
        "            return (a,b)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return (0,0)\n",
        "\n",
        "def load_npz_logits(npz_path):\n",
        "    arr = np.load(npz_path, allow_pickle=True)\n",
        "    keys = list(arr.keys())\n",
        "    if 'start' in keys and 'end' in keys:\n",
        "        return arr['start'], arr['end']\n",
        "    for sk in ['start_logits','test_start_logits','start_logits_avg']:\n",
        "        for ek in ['end_logits','test_end_logits','end_logits_avg']:\n",
        "            if sk in keys and ek in keys: return arr[sk], arr[ek]\n",
        "    raise ValueError(f'Unknown keys in {npz_path}: {keys}')\n",
        "\n",
        "def maxpool1d(x):\n",
        "    if len(x) == 0: return x\n",
        "    y = x.copy()\n",
        "    if len(x) == 1: return y\n",
        "    y[0] = max(x[0], x[1])\n",
        "    for i in range(1, len(x)-1): y[i] = max(x[i-1], x[i], x[i+1])\n",
        "    y[-1] = max(x[-2], x[-1])\n",
        "    return y\n",
        "\n",
        "def is_boundary_char(ch: str) -> bool:\n",
        "    return ch.isspace() or ch in {'.', ',', '!', '?', DANDA, '\"', \"'\", '(', ')', '[', ']', '{', '}'}\n",
        "\n",
        "def snap_span(ctx: str, S: np.ndarray, E: np.ndarray, a: int, b: int, delta: float = 0.02):\n",
        "    base = float(S[a] + E[b])\n",
        "    a2 = a\n",
        "    for i in range(a-1, -1, -1):\n",
        "        if is_boundary_char(ctx[i]):\n",
        "            a2 = i+1; break\n",
        "    b2 = b\n",
        "    for j in range(b, len(ctx)):\n",
        "        if is_boundary_char(ctx[j]):\n",
        "            b2 = max(a2, j-1); break\n",
        "    cand = float(S[a2] + E[b2])\n",
        "    if cand >= base - delta:\n",
        "        return a2, b2\n",
        "    if a > 0 and b < len(ctx)-1 and ctx[a-1] in {'(', '\"', \"'\"} and ctx[b+1] in {')', '\"', \"'\"}:\n",
        "        cand2 = float(S[a-1] + E[b+1])\n",
        "        if cand2 >= base - delta and (b+1 - (a-1) <= (b - a) + 2):\n",
        "            return a-1, b+1\n",
        "    return a, b\n",
        "\n",
        "# Streams: 512 single-seed test_avg (with its own mapping) + 384\n",
        "streams = [\n",
        "    dict(name='xlmr512s', npz='xlmr_large_512_test_avg.npz', map_dir='xlmr_large_512_test_logits'),\n",
        "    dict(name='xlmr384',  npz='xlmr_large_test_avg.npz',     map_dir='xlmr_large_test_logits'),\n",
        "]\n",
        "loaded = []\n",
        "for s in streams:\n",
        "    try:\n",
        "        s_start, s_end = load_npz_logits(s['npz'])\n",
        "        eid_path = Path(s['map_dir']) / 'test_example_id.json'\n",
        "        offs_path = Path(s['map_dir']) / 'test_offset_mapping.npy'\n",
        "        eid = json.loads(eid_path.read_text())\n",
        "        offs = np.load(offs_path, allow_pickle=True)\n",
        "        loaded.append((s['name'], s_start, s_end, eid, offs))\n",
        "        print('Loaded', s['name'], s_start.shape, 'offsets', getattr(offs,'shape',None))\n",
        "    except Exception as e:\n",
        "        print('Skip stream', s['name'], '->', e)\n",
        "\n",
        "def decode_alignsafe_beta0():\n",
        "    weights_hi = {'xlmr512s': 0.92,  'xlmr384': 0.08}\n",
        "    weights_ta = {'xlmr512s': 0.995, 'xlmr384': 0.005}\n",
        "    lambda_len = 0.12\n",
        "    K_hi, K_ta = 240, 280\n",
        "    Lmax_hi, Lmax_ta = 52, 62\n",
        "    clip_prior = (-0.8, 0.0)\n",
        "    beta = 0.0\n",
        "    shortlist = 6\n",
        "\n",
        "    idx_maps = []\n",
        "    for name, s_start, s_end, eid, offs in loaded:\n",
        "        m = {}\n",
        "        for i, qid in enumerate(eid): m.setdefault(qid, []).append(i)\n",
        "        idx_maps.append(m)\n",
        "\n",
        "    out_rows = []\n",
        "    t0 = time.time()\n",
        "    for qid in test['id'].tolist():\n",
        "        lang = id2lang[qid]; ctx = id2context[qid]\n",
        "        mu = priors[lang]['mu']; sigma = priors[lang]['sigma']\n",
        "        Lmax = Lmax_hi if lang=='hindi' else Lmax_ta\n",
        "        K = K_hi if lang=='hindi' else K_ta\n",
        "        weights = weights_hi if lang=='hindi' else weights_ta\n",
        "\n",
        "        S = np.zeros(len(ctx), dtype=np.float32)\n",
        "        E = np.zeros(len(ctx), dtype=np.float32)\n",
        "\n",
        "        for (name, s_start, s_end, eid, offs), m in zip(loaded, idx_maps):\n",
        "            w = weights.get(name, 0.0)\n",
        "            if w <= 0.0: continue\n",
        "            for fi in m.get(qid, []):\n",
        "                offs_raw = offs[fi]\n",
        "                M = len(offs_raw) if hasattr(offs_raw,'__len__') else s_start.shape[1]\n",
        "                s_log = s_start[fi][:M] * w\n",
        "                e_log = s_end[fi][:M] * w\n",
        "                for ti in range(M):\n",
        "                    a, b = to_pair(offs_raw[ti])\n",
        "                    if b > a and 0 <= a < len(ctx) and 1 <= b <= len(ctx):\n",
        "                        S[a] += s_log[ti]\n",
        "                        E[b-1] += e_log[ti]\n",
        "\n",
        "        S = maxpool1d(S); E = maxpool1d(E)\n",
        "\n",
        "        starts = np.argsort(S)[::-1][:K]\n",
        "        best_score = -1e18\n",
        "        best_span = (0, max(0, min(Lmax, len(ctx))-1))\n",
        "        for si in starts:\n",
        "            end_lo = si; end_hi = min(len(ctx)-1, si + Lmax - 1)\n",
        "            if end_hi < end_lo: continue\n",
        "            seg = E[end_lo:end_hi+1]\n",
        "            top_rel = np.argsort(seg)[::-1][:shortlist]\n",
        "            for rel in top_rel:\n",
        "                ej = end_lo + int(rel)\n",
        "                raw = float(S[si] + E[ej])\n",
        "                clen = ej - si + 1\n",
        "                lp = log_normal_logpdf_len(clen, mu, sigma)\n",
        "                lp = max(clip_prior[0], min(clip_prior[1], lp))\n",
        "                raw += lambda_len * lp\n",
        "                if beta > 0.0:\n",
        "                    span_text = ctx[si:ej+1]\n",
        "                    if span_text and ctx.count(span_text) > 1:\n",
        "                        raw += beta * math.log(1 + ctx.count(span_text))\n",
        "                if raw > best_score:\n",
        "                    best_score = raw; best_span = (si, ej)\n",
        "\n",
        "        a, b = best_span\n",
        "        a, b = snap_span(ctx, S, E, a, b, delta=0.02)\n",
        "        text = clean_span_text(ctx[a:b+1], lang)\n",
        "        if not text:\n",
        "            b2 = min(len(ctx), a+1)\n",
        "            text = clean_span_text(ctx[a:b2], lang) or ctx[a:b2]\n",
        "        out_rows.append((qid, text))\n",
        "\n",
        "    sub = pd.DataFrame(out_rows, columns=['id','PredictionString'])\n",
        "    empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "    mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "    print(f'Align-safe beta0 snap002 done in {time.time()-t0:.1f}s. Empties={empties}, mean_len={mean_len:.2f}')\n",
        "    return sub\n",
        "\n",
        "sub_beta0 = decode_alignsafe_beta0()\n",
        "out_path = 'submission_alignsafe_beta0_snap002_lambda012_K240_280_Lmax52_62.csv'\n",
        "sub_beta0.to_csv(out_path, index=False)\n",
        "pd.read_csv(out_path).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_path)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded xlmr512s (1401, 512) offsets (1401, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded xlmr384 (1921, 384) offsets (1921, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Align-safe beta0 snap002 done in 3.5s. Empties=0, mean_len=14.01\nsubmission.csv updated -> submission_alignsafe_beta0_snap002_lambda012_K240_280_Lmax52_62.csv\n"
          ]
        }
      ]
    },
    {
      "id": "1b426aca-6a4a-4a11-8571-443549ecc045",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Token-level per-stream decode and per-id selection (512 single-seed vs 384), no MuRIL\n",
        "import json, math, time, re, unicodedata as ud\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def load_npz_logits(npz_path):\n",
        "    arr = np.load(npz_path, allow_pickle=True)\n",
        "    keys = list(arr.keys())\n",
        "    if 'start' in keys and 'end' in keys:\n",
        "        return arr['start'], arr['end']\n",
        "    for sk in ['start_logits','test_start_logits','start_logits_avg']:\n",
        "        for ek in ['end_logits','test_end_logits','end_logits_avg']:\n",
        "            if sk in keys and ek in keys: return arr[sk], arr[ek]\n",
        "    raise ValueError(f'Unknown keys in {npz_path}: {keys}')\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id2lang = dict(zip(test['id'].tolist(), test['language'].tolist()))\n",
        "id2context = dict(zip(test['id'].tolist(), test['context'].astype(str).tolist()))\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "ZW_CHARS = {'\\u200B','\\u200C','\\u200D','\\u2060','\\ufeff'}\n",
        "NBSP_SET = {'\\u00A0','\\u2002','\\u2003','\\u2004','\\u2005','\\u2006','\\u2007','\\u2008','\\u2009','\\u200A'}\n",
        "HI_VIRAMA = '\\u094D'; TA_PULLI='\\u0BCD'; DANDA='\\u0964'\n",
        "def clean_span_text(s: str, lang: str) -> str:\n",
        "    if not s: return ''\n",
        "    s = ''.join(ch for ch in s if ch not in ZW_CHARS)\n",
        "    s = ''.join(' ' if ch in NBSP_SET else ch for ch in s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    s = re.sub(r'(?<=\\d)[\\s,._-](?=\\d)', '', s)\n",
        "    if s:\n",
        "        last = s[-1]\n",
        "        if ud.category(last) == 'Mn' or last in (HI_VIRAMA, TA_PULLI): s = s[:-1]\n",
        "    if lang == 'hindi':\n",
        "        s = s.replace(DANDA+DANDA, DANDA)\n",
        "        if s.endswith(DANDA): s = s[:-1].rstrip()\n",
        "    return s\n",
        "\n",
        "def log_normal_logpdf_len(char_len: int, mu: float, sigma: float) -> float:\n",
        "    Lc = max(1, int(char_len))\n",
        "    x = math.log(Lc)\n",
        "    return -0.5*((x-mu)/sigma)**2 - math.log(max(Lc,1e-6)) - math.log(sigma) - 0.5*math.log(2*math.pi)\n",
        "\n",
        "def to_pair(t):\n",
        "    try:\n",
        "        if t is None: return (0,0)\n",
        "        if isinstance(t,(list,tuple,np.ndarray)) and len(t)>=2:\n",
        "            a = int(t[0]) if t[0] is not None else 0\n",
        "            b = int(t[1]) if t[1] is not None else 0\n",
        "            return (a,b)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return (0,0)\n",
        "\n",
        "def build_qid_index(example_id_list):\n",
        "    m = {}\n",
        "    for i, qid in enumerate(example_id_list):\n",
        "        m.setdefault(qid, []).append(i)\n",
        "    return m\n",
        "\n",
        "def decode_stream_token_level(start_logits, end_logits, example_id_list, offset_mapping, lambda_len=0.12, clip_prior=(-0.8,0.0),\n",
        "                               K_hi=240, K_ta=280, Lmax_hi=52, Lmax_ta=62, shortlist=6):\n",
        "    qid_index = build_qid_index(example_id_list)\n",
        "    preds = {}  # qid -> (score, text)\n",
        "    for qid in test['id'].tolist():\n",
        "        lang = id2lang[qid]; ctx = id2context[qid]\n",
        "        mu = priors[lang]['mu']; sigma = priors[lang]['sigma']\n",
        "        Lmax = Lmax_hi if lang=='hindi' else Lmax_ta\n",
        "        K = K_hi if lang=='hindi' else K_ta\n",
        "        best_score = -1e18; best_text = None\n",
        "        for fi in qid_index.get(qid, []):\n",
        "            offs = offset_mapping[fi]\n",
        "            M = len(offs) if hasattr(offs,'__len__') else start_logits.shape[1]\n",
        "            s_log = start_logits[fi][:M]\n",
        "            e_log = end_logits[fi][:M]\n",
        "            # candidate starts: top-K among tokens that begin a valid span\n",
        "            valid = []\n",
        "            for ti in range(M):\n",
        "                a,b = to_pair(offs[ti])\n",
        "                if b > a and 0 <= a < len(ctx) and 1 <= b <= len(ctx):\n",
        "                    valid.append(ti)\n",
        "            if not valid: continue\n",
        "            valid = np.array(valid, dtype=np.int32)\n",
        "            s_top = valid[np.argsort(s_log[valid])[::-1][:K]]\n",
        "            for si in s_top:\n",
        "                a0, _ = to_pair(offs[si])\n",
        "                end_lo = si; end_hi = M-1\n",
        "                # restrict ends to within Lmax chars and valid offsets\n",
        "                cand = []\n",
        "                for ei in range(end_lo, M):\n",
        "                    a2, b2 = to_pair(offs[ei])\n",
        "                    if not (b2 > a2 and 0 <= a2 < len(ctx) and 1 <= b2 <= len(ctx)):\n",
        "                        continue\n",
        "                    clen = b2 - a0\n",
        "                    if clen <= 0 or clen > Lmax:\n",
        "                        continue\n",
        "                    cand.append(ei)\n",
        "                if not cand: continue\n",
        "                cand = np.array(cand, dtype=np.int32)\n",
        "                seg = e_log[cand]\n",
        "                top_idx = cand[np.argsort(seg)[::-1][:shortlist]]\n",
        "                for ei in top_idx:\n",
        "                    a, b = to_pair(offs[si])[0], to_pair(offs[ei])[1]\n",
        "                    if not (0 <= a < len(ctx) and 1 <= b <= len(ctx) and b > a):\n",
        "                        continue\n",
        "                    raw = float(s_log[si] + e_log[ei])\n",
        "                    clen = b - a\n",
        "                    lp = log_normal_logpdf_len(clen, mu, sigma)\n",
        "                    lp = max(clip_prior[0], min(clip_prior[1], lp))\n",
        "                    raw += lambda_len * lp\n",
        "                    if raw > best_score:\n",
        "                        best_score = raw\n",
        "                        best_text = clean_span_text(ctx[a:b], lang) or ctx[a:b]\n",
        "        if best_text is None:\n",
        "            best_text = clean_span_text(ctx[:1], lang) or ctx[:1]\n",
        "        preds[qid] = best_text\n",
        "    return preds\n",
        "\n",
        "# Load 512 single-seed (alignment-safe) stream\n",
        "s512, e512 = load_npz_logits('xlmr_large_512_test_avg.npz')\n",
        "eid_512 = json.loads(Path('xlmr_large_512_test_logits/test_example_id.json').read_text())\n",
        "off_512 = np.load('xlmr_large_512_test_logits/test_offset_mapping.npy', allow_pickle=True)\n",
        "print('512 single-seed:', s512.shape, 'mapping', len(eid_512))\n",
        "# Load 384 stream\n",
        "s384, e384 = load_npz_logits('xlmr_large_test_avg.npz')\n",
        "eid_384 = json.loads(Path('xlmr_large_test_logits/test_example_id.json').read_text())\n",
        "off_384 = np.load('xlmr_large_test_logits/test_offset_mapping.npy', allow_pickle=True)\n",
        "print('384:', s384.shape, 'mapping', len(eid_384))\n",
        "\n",
        "t0 = time.time()\n",
        "preds_512 = decode_stream_token_level(s512, e512, eid_512, off_512, lambda_len=0.12, clip_prior=(-0.8,0.0),\n",
        "                                      K_hi=240, K_ta=280, Lmax_hi=52, Lmax_ta=62, shortlist=6)\n",
        "preds_384 = decode_stream_token_level(s384, e384, eid_384, off_384, lambda_len=0.12, clip_prior=(-0.8,0.0),\n",
        "                                      K_hi=240, K_ta=280, Lmax_hi=52, Lmax_ta=62, shortlist=6)\n",
        "print(f'Token-level per-stream decodes done in {time.time()-t0:.1f}s')\n",
        "\n",
        "# Simple per-id selection: prefer 512 unless 384 gives shorter clean text within 20% length\n",
        "rows = []\n",
        "for qid in test['id'].tolist():\n",
        "    t512 = preds_512.get(qid, '')\n",
        "    t384 = preds_384.get(qid, '')\n",
        "    if not t512:\n",
        "        best = t384\n",
        "    elif not t384:\n",
        "        best = t512\n",
        "    else:\n",
        "        # Heuristic: pick the one closer to per-lang median length in log space\n",
        "        lang = id2lang[qid]\n",
        "        mu = priors[lang]['mu']\n",
        "        def neg_ll(s):\n",
        "            Lc = max(1, len(s)); return abs(math.log(Lc) - mu)\n",
        "        d512, d384 = neg_ll(t512), neg_ll(t384)\n",
        "        best = t512 if d512 <= d384 else t384\n",
        "    rows.append((qid, best))\n",
        "sub_tok = pd.DataFrame(rows, columns=['id','PredictionString'])\n",
        "empties = (sub_tok['PredictionString'].astype(str).str.len()==0).sum()\n",
        "mean_len = sub_tok['PredictionString'].astype(str).str.len().mean()\n",
        "print('Token-level selection diagnostics: empties=', int(empties), 'mean_len=', round(float(mean_len),2))\n",
        "out_tok = 'submission_tokenselect_512single_or_384_lambda012.csv'\n",
        "sub_tok.to_csv(out_tok, index=False)\n",
        "pd.read_csv(out_tok).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_tok)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512 single-seed: (1401, 512) mapping 1401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "384: (1921, 384) mapping 1921\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token-level per-stream decodes done in 117.6s\nToken-level selection diagnostics: empties= 0 mean_len= 10.9\nsubmission.csv updated -> submission_tokenselect_512single_or_384_lambda012.csv\n"
          ]
        }
      ]
    },
    {
      "id": "7e36972c-7642-46ce-8317-608ad7a1001f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Align-safe 2-stream char-fusion with tight length control (per expert). 512 single-seed + tiny 384; no MuRIL.\n",
        "import json, math, time, re, unicodedata as ud\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id2lang = dict(zip(test['id'].tolist(), test['language'].tolist()))\n",
        "id2context = dict(zip(test['id'].tolist(), test['context'].astype(str).tolist()))\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "ZW_CHARS = {'\\u200B','\\u200C','\\u200D','\\u2060','\\ufeff'}\n",
        "NBSP_SET = {'\\u00A0','\\u2002','\\u2003','\\u2004','\\u2005','\\u2006','\\u2007','\\u2008','\\u2009','\\u200A'}\n",
        "HI_VIRAMA = '\\u094D'; TA_PULLI='\\u0BCD'; DANDA='\\u0964'\n",
        "def clean_span_text(s: str, lang: str) -> str:\n",
        "    if not s: return ''\n",
        "    s = ''.join(ch for ch in s if ch not in ZW_CHARS)\n",
        "    s = ''.join(' ' if ch in NBSP_SET else ch for ch in s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    s = re.sub(r'(?<=\\d)[\\s,._-](?=\\d)', '', s)\n",
        "    if s:\n",
        "        last = s[-1]\n",
        "        if ud.category(last) == 'Mn' or last in (HI_VIRAMA, TA_PULLI): s = s[:-1]\n",
        "    if lang == 'hindi':\n",
        "        s = s.replace(DANDA+DANDA, DANDA)\n",
        "        if s.endswith(DANDA): s = s[:-1].rstrip()\n",
        "    return s\n",
        "\n",
        "def log_normal_logpdf_len(char_len: int, mu: float, sigma: float) -> float:\n",
        "    Lc = max(1, int(char_len))\n",
        "    x = math.log(Lc)\n",
        "    return -0.5*((x-mu)/sigma)**2 - math.log(max(Lc,1e-6)) - math.log(sigma) - 0.5*math.log(2*math.pi)\n",
        "\n",
        "def to_pair(t):\n",
        "    try:\n",
        "        if t is None: return (0,0)\n",
        "        if isinstance(t,(list,tuple,np.ndarray)) and len(t)>=2:\n",
        "            a = int(t[0]) if t[0] is not None else 0\n",
        "            b = int(t[1]) if t[1] is not None else 0\n",
        "            return (a,b)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return (0,0)\n",
        "\n",
        "def load_npz_logits(npz_path):\n",
        "    arr = np.load(npz_path, allow_pickle=True)\n",
        "    keys = list(arr.keys())\n",
        "    if 'start' in keys and 'end' in keys:\n",
        "        return arr['start'], arr['end']\n",
        "    for sk in ['start_logits','test_start_logits','start_logits_avg']:\n",
        "        for ek in ['end_logits','test_end_logits','end_logits_avg']:\n",
        "            if sk in keys and ek in keys: return arr[sk], arr[ek]\n",
        "    raise ValueError(f'Unknown keys in {npz_path}: {keys}')\n",
        "\n",
        "def maxpool1d(x):\n",
        "    if len(x) == 0: return x\n",
        "    y = x.copy()\n",
        "    if len(x) == 1: return y\n",
        "    y[0] = max(x[0], x[1])\n",
        "    for i in range(1, len(x)-1): y[i] = max(x[i-1], x[i], x[i+1])\n",
        "    y[-1] = max(x[-2], x[-1])\n",
        "    return y\n",
        "\n",
        "def is_boundary_char(ch: str) -> bool:\n",
        "    return ch.isspace() or ch in {'.', ',', '!', '?', DANDA, '\"', \"'\", '(', ')', '[', ']', '{', '}', '\u201c', '\u201d', '\u2018', '\u2019', '\u00ab', '\u00bb'}\n",
        "\n",
        "def snap_span(ctx: str, S: np.ndarray, E: np.ndarray, a: int, b: int, delta: float = 0.02):\n",
        "    base = float(S[a] + E[b])\n",
        "    a2 = a\n",
        "    for i in range(a-1, -1, -1):\n",
        "        if is_boundary_char(ctx[i]):\n",
        "            a2 = i+1; break\n",
        "    b2 = b\n",
        "    for j in range(b, len(ctx)):\n",
        "        if is_boundary_char(ctx[j]):\n",
        "            b2 = max(a2, j-1); break\n",
        "    cand = float(S[a2] + E[b2])\n",
        "    if cand >= base - delta:\n",
        "        return a2, b2\n",
        "    if a > 0 and b < len(ctx)-1 and ctx[a-1] in {'(', '\"', \"'\"} and ctx[b+1] in {')', '\"', \"'\"}:\n",
        "        cand2 = float(S[a-1] + E[b+1])\n",
        "        if cand2 >= base - delta and (b+1 - (a-1) <= (b - a) + 2):\n",
        "            return a-1, b+1\n",
        "    return a, b\n",
        "\n",
        "# Streams: 512 single-seed test_avg (with its mapping) + 384\n",
        "streams = [\n",
        "    dict(name='xlmr512s', npz='xlmr_large_512_test_avg.npz', map_dir='xlmr_large_512_test_logits'),\n",
        "    dict(name='xlmr384',  npz='xlmr_large_test_avg.npz',     map_dir='xlmr_large_test_logits'),\n",
        "]\n",
        "loaded = []\n",
        "for s in streams:\n",
        "    try:\n",
        "        s_start, s_end = load_npz_logits(s['npz'])\n",
        "        eid_path = Path(s['map_dir']) / 'test_example_id.json'\n",
        "        offs_path = Path(s['map_dir']) / 'test_offset_mapping.npy'\n",
        "        eid = json.loads(eid_path.read_text())\n",
        "        offs = np.load(offs_path, allow_pickle=True)\n",
        "        loaded.append((s['name'], s_start, s_end, eid, offs))\n",
        "        print('Loaded', s['name'], s_start.shape, 'offsets', getattr(offs,'shape',None))\n",
        "    except Exception as e:\n",
        "        print('Skip stream', s['name'], '->', e)\n",
        "\n",
        "def decode_alignsafe_tight():\n",
        "    # Weights per language\n",
        "    weights_hi = {'xlmr512s': 0.90, 'xlmr384': 0.10}\n",
        "    weights_ta = {'xlmr512s': 1.00, 'xlmr384': 0.00}\n",
        "    # Hyperparams\n",
        "    lambda_len = 0.12\n",
        "    clip_prior = (-0.8, 0.0)\n",
        "    beta = 0.0\n",
        "    shortlist = 6\n",
        "    K_hi, K_ta = 200, 240\n",
        "    Lmax_hi, Lmax_ta = 48, 58\n",
        "\n",
        "    # index maps\n",
        "    idx_maps = []\n",
        "    for name, s_start, s_end, eid, offs in loaded:\n",
        "        m = {}\n",
        "        for i, qid in enumerate(eid): m.setdefault(qid, []).append(i)\n",
        "        idx_maps.append(m)\n",
        "\n",
        "    out_rows = []\n",
        "    t0 = time.time()\n",
        "    for qid in test['id'].tolist():\n",
        "        lang = id2lang[qid]; ctx = id2context[qid]\n",
        "        mu = priors[lang]['mu']; sigma = priors[lang]['sigma']\n",
        "        Lmax = Lmax_hi if lang=='hindi' else Lmax_ta\n",
        "        K = K_hi if lang=='hindi' else K_ta\n",
        "        weights = weights_hi if lang=='hindi' else weights_ta\n",
        "\n",
        "        S = np.zeros(len(ctx), dtype=np.float32)\n",
        "        E = np.zeros(len(ctx), dtype=np.float32)\n",
        "\n",
        "        for (name, s_start, s_end, eid, offs), m in zip(loaded, idx_maps):\n",
        "            w = weights.get(name, 0.0)\n",
        "            if w <= 0.0: continue\n",
        "            for fi in m.get(qid, []):\n",
        "                offs_raw = offs[fi]\n",
        "                M = len(offs_raw) if hasattr(offs_raw,'__len__') else s_start.shape[1]\n",
        "                s_log = s_start[fi][:M] * w\n",
        "                e_log = s_end[fi][:M] * w\n",
        "                for ti in range(M):\n",
        "                    a, b = to_pair(offs_raw[ti])\n",
        "                    if b > a and 0 <= a < len(ctx) and 1 <= b <= len(ctx):\n",
        "                        S[a] += s_log[ti]\n",
        "                        E[b-1] += e_log[ti]\n",
        "\n",
        "        S = maxpool1d(S); E = maxpool1d(E)\n",
        "\n",
        "        starts = np.argsort(S)[::-1][:K]\n",
        "        best_score = -1e18\n",
        "        best_span = (0, max(0, min(Lmax, len(ctx))-1))\n",
        "        for si in starts:\n",
        "            end_lo = si; end_hi = min(len(ctx)-1, si + Lmax - 1)\n",
        "            if end_hi < end_lo: continue\n",
        "            seg = E[end_lo:end_hi+1]\n",
        "            top_rel = np.argsort(seg)[::-1][:shortlist]\n",
        "            for rel in top_rel:\n",
        "                ej = end_lo + int(rel)\n",
        "                raw = float(S[si] + E[ej])\n",
        "                clen = ej - si + 1\n",
        "                lp = log_normal_logpdf_len(clen, mu, sigma)\n",
        "                lp = max(clip_prior[0], min(clip_prior[1], lp))\n",
        "                raw += lambda_len * lp\n",
        "                if raw > best_score:\n",
        "                    best_score = raw; best_span = (si, ej)\n",
        "\n",
        "        a, b = best_span\n",
        "        a, b = snap_span(ctx, S, E, a, b, delta=0.02)\n",
        "        text = clean_span_text(ctx[a:b+1], lang)\n",
        "        if not text:\n",
        "            b2 = min(len(ctx), a+1)\n",
        "            text = clean_span_text(ctx[a:b2], lang) or ctx[a:b2]\n",
        "        out_rows.append((qid, text))\n",
        "\n",
        "    sub = pd.DataFrame(out_rows, columns=['id','PredictionString'])\n",
        "    empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "    mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "    print(f'Align-safe tight decode done in {time.time()-t0:.1f}s. Empties={empties}, mean_len={mean_len:.2f}')\n",
        "    return sub\n",
        "\n",
        "sub_tight = decode_alignsafe_tight()\n",
        "out_tight = 'submission_alignsafe_tight_512single_384_wHI90_10_wTA100_0_lambda012_K200_240_Lmax48_58_delta002.csv'\n",
        "sub_tight.to_csv(out_tight, index=False)\n",
        "pd.read_csv(out_tight).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_tight)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded xlmr512s (1401, 512) offsets (1401, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded xlmr384 (1921, 384) offsets (1921, 384)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Align-safe tight decode done in 3.2s. Empties=0, mean_len=13.80\nsubmission.csv updated -> submission_alignsafe_tight_512single_384_wHI90_10_wTA100_0_lambda012_K200_240_Lmax48_58_delta002.csv\n"
          ]
        }
      ]
    },
    {
      "id": "16d82dd8-951a-412d-ba01-bb8d3daea02b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Align-safe 512-only char-fusion (tight). Tamil 512-only; Hindi also 512-only. No 384, no MuRIL.\n",
        "import json, math, time, re, unicodedata as ud\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id2lang = dict(zip(test['id'].tolist(), test['language'].tolist()))\n",
        "id2context = dict(zip(test['id'].tolist(), test['context'].astype(str).tolist()))\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "ZW_CHARS = {'\\u200B','\\u200C','\\u200D','\\u2060','\\ufeff'}\n",
        "NBSP_SET = {'\\u00A0','\\u2002','\\u2003','\\u2004','\\u2005','\\u2006','\\u2007','\\u2008','\\u2009','\\u200A'}\n",
        "HI_VIRAMA='\\u094D'; TA_PULLI='\\u0BCD'; DANDA='\\u0964'\n",
        "def clean_span_text(s: str, lang: str) -> str:\n",
        "    if not s: return ''\n",
        "    s = ''.join(ch for ch in s if ch not in ZW_CHARS)\n",
        "    s = ''.join(' ' if ch in NBSP_SET else ch for ch in s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    s = re.sub(r'(?<=\\d)[\\s,._-](?=\\d)', '', s)\n",
        "    if s:\n",
        "        last = s[-1]\n",
        "        if ud.category(last) == 'Mn' or last in (HI_VIRAMA, TA_PULLI): s = s[:-1]\n",
        "    if lang == 'hindi':\n",
        "        s = s.replace(DANDA+DANDA, DANDA)\n",
        "        if s.endswith(DANDA): s = s[:-1].rstrip()\n",
        "    return s\n",
        "\n",
        "def log_normal_logpdf_len(char_len: int, mu: float, sigma: float) -> float:\n",
        "    Lc = max(1, int(char_len))\n",
        "    x = math.log(Lc)\n",
        "    return -0.5*((x-mu)/sigma)**2 - math.log(max(Lc,1e-6)) - math.log(sigma) - 0.5*math.log(2*math.pi)\n",
        "\n",
        "def to_pair(t):\n",
        "    try:\n",
        "        if t is None: return (0,0)\n",
        "        if isinstance(t,(list,tuple,np.ndarray)) and len(t)>=2:\n",
        "            a = int(t[0]) if t[0] is not None else 0\n",
        "            b = int(t[1]) if t[1] is not None else 0\n",
        "            return (a,b)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return (0,0)\n",
        "\n",
        "def load_npz_logits(npz_path):\n",
        "    arr = np.load(npz_path, allow_pickle=True)\n",
        "    keys = list(arr.keys())\n",
        "    if 'start' in keys and 'end' in keys:\n",
        "        return arr['start'], arr['end']\n",
        "    for sk in ['start_logits','test_start_logits','start_logits_avg']:\n",
        "        for ek in ['end_logits','test_end_logits','end_logits_avg']:\n",
        "            if sk in keys and ek in keys: return arr[sk], arr[ek]\n",
        "    raise ValueError(f'Unknown keys in {npz_path}: {keys}')\n",
        "\n",
        "def maxpool1d(x):\n",
        "    if len(x) == 0: return x\n",
        "    y = x.copy()\n",
        "    if len(x) == 1: return y\n",
        "    y[0] = max(x[0], x[1])\n",
        "    for i in range(1, len(x)-1): y[i] = max(x[i-1], x[i], x[i+1])\n",
        "    y[-1] = max(x[-2], x[-1])\n",
        "    return y\n",
        "\n",
        "def is_boundary_char(ch: str) -> bool:\n",
        "    return ch.isspace() or ch in {'.', ',', '!', '?', DANDA, '\"', \"'\", '(', ')', '[', ']', '{', '}', '\\u201c', '\\u201d', '\\u2018', '\\u2019', '\\u00ab', '\\u00bb'}\n",
        "\n",
        "def snap_span(ctx: str, S: np.ndarray, E: np.ndarray, a: int, b: int, delta: float = 0.02):\n",
        "    base = float(S[a] + E[b])\n",
        "    a2 = a\n",
        "    for i in range(a-1, -1, -1):\n",
        "        if is_boundary_char(ctx[i]):\n",
        "            a2 = i+1; break\n",
        "    b2 = b\n",
        "    for j in range(b, len(ctx)):\n",
        "        if is_boundary_char(ctx[j]):\n",
        "            b2 = max(a2, j-1); break\n",
        "    cand = float(S[a2] + E[b2])\n",
        "    if cand >= base - delta:\n",
        "        return a2, b2\n",
        "    if a > 0 and b < len(ctx)-1 and ctx[a-1] in {'(', '\"', \"'\"} and ctx[b+1] in {')', '\"', \"'\"}:\n",
        "        cand2 = float(S[a-1] + E[b+1])\n",
        "        if cand2 >= base - delta and (b+1 - (a-1) <= (b - a) + 2):\n",
        "            return a-1, b+1\n",
        "    return a, b\n",
        "\n",
        "# Load 512 single-seed stream only\n",
        "s_start, s_end = load_npz_logits('xlmr_large_512_test_avg.npz')\n",
        "eid = json.loads(Path('xlmr_large_512_test_logits/test_example_id.json').read_text())\n",
        "offs_all = np.load('xlmr_large_512_test_logits/test_offset_mapping.npy', allow_pickle=True)\n",
        "print('Loaded 512 single-seed:', s_start.shape, 'offsets', getattr(offs_all,'shape',None))\n",
        "\n",
        "def decode_512only_tight():\n",
        "    lambda_len = 0.10\n",
        "    clip_prior = (-0.8, 0.0)\n",
        "    K_hi, K_ta = 200, 240\n",
        "    Lmax_hi, Lmax_ta = 48, 58\n",
        "    shortlist = 6\n",
        "    # index map\n",
        "    m = {}\n",
        "    for i, qid in enumerate(eid): m.setdefault(qid, []).append(i)\n",
        "    out_rows = []\n",
        "    t0 = time.time()\n",
        "    for qid in test['id'].tolist():\n",
        "        lang = id2lang[qid]; ctx = id2context[qid]\n",
        "        mu = priors[lang]['mu']; sigma = priors[lang]['sigma']\n",
        "        Lmax = Lmax_hi if lang=='hindi' else Lmax_ta\n",
        "        K = K_hi if lang=='hindi' else K_ta\n",
        "        S = np.zeros(len(ctx), dtype=np.float32)\n",
        "        E = np.zeros(len(ctx), dtype=np.float32)\n",
        "        for fi in m.get(qid, []):\n",
        "            offs_raw = offs_all[fi]\n",
        "            M = len(offs_raw) if hasattr(offs_raw,'__len__') else s_start.shape[1]\n",
        "            s_log = s_start[fi][:M]\n",
        "            e_log = s_end[fi][:M]\n",
        "            for ti in range(M):\n",
        "                a, b = to_pair(offs_raw[ti])\n",
        "                if b > a and 0 <= a < len(ctx) and 1 <= b <= len(ctx):\n",
        "                    S[a] += s_log[ti]\n",
        "                    E[b-1] += e_log[ti]\n",
        "        S = maxpool1d(S); E = maxpool1d(E)\n",
        "        starts = np.argsort(S)[::-1][:K]\n",
        "        best_score = -1e18; best_span = (0, max(0, min(Lmax, len(ctx))-1))\n",
        "        for si in starts:\n",
        "            end_lo = si; end_hi = min(len(ctx)-1, si + Lmax - 1)\n",
        "            if end_hi < end_lo: continue\n",
        "            seg = E[end_lo:end_hi+1]\n",
        "            top_rel = np.argsort(seg)[::-1][:shortlist]\n",
        "            for rel in top_rel:\n",
        "                ej = end_lo + int(rel)\n",
        "                raw = float(S[si] + E[ej])\n",
        "                clen = ej - si + 1\n",
        "                lp = log_normal_logpdf_len(clen, mu, sigma)\n",
        "                lp = max(clip_prior[0], min(clip_prior[1], lp))\n",
        "                raw += lambda_len * lp\n",
        "                if raw > best_score:\n",
        "                    best_score = raw; best_span = (si, ej)\n",
        "        a, b = best_span\n",
        "        a, b = snap_span(ctx, S, E, a, b, delta=0.02)\n",
        "        text = clean_span_text(ctx[a:b+1], lang)\n",
        "        if not text:\n",
        "            b2 = min(len(ctx), a+1)\n",
        "            text = clean_span_text(ctx[a:b2], lang) or ctx[a:b2]\n",
        "        out_rows.append((qid, text))\n",
        "    sub = pd.DataFrame(out_rows, columns=['id','PredictionString'])\n",
        "    empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "    mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "    print(f'512-only tight decode done in {time.time()-t0:.1f}s. Empties={empties}, mean_len={mean_len:.2f}')\n",
        "    return sub\n",
        "\n",
        "sub_512tight = decode_512only_tight()\n",
        "out_512tight = 'submission_512only_alignsafe_tight_lambda010_K200_240_Lmax48_58_delta002.csv'\n",
        "sub_512tight.to_csv(out_512tight, index=False)\n",
        "pd.read_csv(out_512tight).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_512tight)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 512 single-seed: (1401, 512) offsets (1401, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512-only tight decode done in 2.5s. Empties=0, mean_len=14.40\nsubmission.csv updated -> submission_512only_alignsafe_tight_lambda010_K200_240_Lmax48_58_delta002.csv\n"
          ]
        }
      ]
    },
    {
      "id": "23f7d35a-ddc3-4615-b757-b9a723088334",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3-seed 512 + 384 (no MuRIL), tight settings per expert (beta=0, K/Lmax tight, delta=0.02, expanded boundaries)\n",
        "import json, math, time, re, unicodedata as ud\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id2lang = dict(zip(test['id'].tolist(), test['language'].tolist()))\n",
        "id2context = dict(zip(test['id'].tolist(), test['context'].astype(str).tolist()))\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "ZW_CHARS = {'\\u200B','\\u200C','\\u200D','\\u2060','\\ufeff'}\n",
        "NBSP_SET = {'\\u00A0','\\u2002','\\u2003','\\u2004','\\u2005','\\u2006','\\u2007','\\u2008','\\u2009','\\u200A'}\n",
        "HI_VIRAMA='\\u094D'; TA_PULLI='\\u0BCD'; DANDA='\\u0964'\n",
        "def clean_span_text(s: str, lang: str) -> str:\n",
        "    if not s: return ''\n",
        "    s = ''.join(ch for ch in s if ch not in ZW_CHARS)\n",
        "    s = ''.join(' ' if ch in NBSP_SET else ch for ch in s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    s = re.sub(r'(?<=\\d)[\\s,._-](?=\\d)', '', s)\n",
        "    if s:\n",
        "        last = s[-1]\n",
        "        if ud.category(last) == 'Mn' or last in (HI_VIRAMA, TA_PULLI): s = s[:-1]\n",
        "    if lang == 'hindi':\n",
        "        s = s.replace(DANDA+DANDA, DANDA)\n",
        "        if s.endswith(DANDA): s = s[:-1].rstrip()\n",
        "    return s\n",
        "\n",
        "def log_normal_logpdf_len(char_len: int, mu: float, sigma: float) -> float:\n",
        "    Lc = max(1, int(char_len))\n",
        "    x = math.log(Lc)\n",
        "    return -0.5*((x-mu)/sigma)**2 - math.log(max(Lc,1e-6)) - math.log(sigma) - 0.5*math.log(2*math.pi)\n",
        "\n",
        "def to_pair(t):\n",
        "    try:\n",
        "        if t is None: return (0,0)\n",
        "        if isinstance(t,(list,tuple,np.ndarray)) and len(t)>=2:\n",
        "            a = int(t[0]) if t[0] is not None else 0\n",
        "            b = int(t[1]) if t[1] is not None else 0\n",
        "            return (a,b)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return (0,0)\n",
        "\n",
        "def load_npz_logits(npz_path):\n",
        "    arr = np.load(npz_path, allow_pickle=True)\n",
        "    keys = list(arr.keys())\n",
        "    if 'start' in keys and 'end' in keys:\n",
        "        return arr['start'], arr['end']\n",
        "    for sk in ['start_logits','test_start_logits','start_logits_avg']:\n",
        "        for ek in ['end_logits','test_end_logits','end_logits_avg']:\n",
        "            if sk in keys and ek in keys: return arr[sk], arr[ek]\n",
        "    raise ValueError(f'Unknown keys in {npz_path}: {keys}')\n",
        "\n",
        "def maxpool1d(x):\n",
        "    if len(x) == 0: return x\n",
        "    y = x.copy()\n",
        "    if len(x) == 1: return y\n",
        "    y[0] = max(x[0], x[1])\n",
        "    for i in range(1, len(x)-1): y[i] = max(x[i-1], x[i], x[i+1])\n",
        "    y[-1] = max(x[-2], x[-1])\n",
        "    return y\n",
        "\n",
        "def is_boundary_char(ch: str) -> bool:\n",
        "    return ch.isspace() or ch in {'.', ',', '!', '?', DANDA, '\"', \"'\", '(', ')', '[', ']', '{', '}', '\\u201c', '\\u201d', '\\u2018', '\\u2019', '\\u00ab', '\\u00bb'}\n",
        "\n",
        "def snap_span(ctx: str, S: np.ndarray, E: np.ndarray, a: int, b: int, delta: float = 0.02):\n",
        "    base = float(S[a] + E[b])\n",
        "    a2 = a\n",
        "    for i in range(a-1, -1, -1):\n",
        "        if is_boundary_char(ctx[i]):\n",
        "            a2 = i+1; break\n",
        "    b2 = b\n",
        "    for j in range(b, len(ctx)):\n",
        "        if is_boundary_char(ctx[j]):\n",
        "            b2 = max(a2, j-1); break\n",
        "    cand = float(S[a2] + E[b2])\n",
        "    if cand >= base - delta:\n",
        "        return a2, b2\n",
        "    if a > 0 and b < len(ctx)-1 and ctx[a-1] in {'(', '\"', \"'\"} and ctx[b+1] in {')', '\"', \"'\"}:\n",
        "        cand2 = float(S[a-1] + E[b+1])\n",
        "        if cand2 >= base - delta and (b+1 - (a-1) <= (b - a) + 2):\n",
        "            return a-1, b+1\n",
        "    return a, b\n",
        "\n",
        "# Streams: 512 (3-seed avg) + 384, no MuRIL\n",
        "streams = [\n",
        "    dict(name='xlmr512', npz='xlmr_large_512_3seeds_avg.npz', map_dir='xlmr_large_512_test_logits'),\n",
        "    dict(name='xlmr384', npz='xlmr_large_test_avg.npz',       map_dir='xlmr_large_test_logits'),\n",
        "]\n",
        "loaded = []\n",
        "for s in streams:\n",
        "    try:\n",
        "        s_start, s_end = load_npz_logits(s['npz'])\n",
        "        eid_path = Path(s['map_dir']) / 'test_example_id.json'\n",
        "        offs_path = Path(s['map_dir']) / 'test_offset_mapping.npy'\n",
        "        eid = json.loads(eid_path.read_text())\n",
        "        offs = np.load(offs_path, allow_pickle=True)\n",
        "        loaded.append((s['name'], s_start, s_end, eid, offs))\n",
        "        print('Loaded', s['name'], s_start.shape, 'offsets', getattr(offs,'shape',None))\n",
        "    except Exception as e:\n",
        "        print('Skip stream', s['name'], '->', e)\n",
        "\n",
        "def decode_3seed_tight():\n",
        "    # Weights per language\n",
        "    weights_hi = {'xlmr512': 0.90, 'xlmr384': 0.10}\n",
        "    weights_ta = {'xlmr512': 1.00, 'xlmr384': 0.00}\n",
        "    # Hyperparams\n",
        "    lambda_len = 0.12\n",
        "    clip_prior = (-0.8, 0.0)\n",
        "    beta = 0.0\n",
        "    shortlist = 6\n",
        "    K_hi, K_ta = 200, 240\n",
        "    Lmax_hi, Lmax_ta = 48, 58\n",
        "\n",
        "    # index maps per stream\n",
        "    idx_maps = []\n",
        "    for name, s_start, s_end, eid, offs in loaded:\n",
        "        m = {}\n",
        "        for i, qid in enumerate(eid): m.setdefault(qid, []).append(i)\n",
        "        idx_maps.append(m)\n",
        "\n",
        "    out_rows = []\n",
        "    t0 = time.time()\n",
        "    for qid in test['id'].tolist():\n",
        "        lang = id2lang[qid]; ctx = id2context[qid]\n",
        "        mu = priors[lang]['mu']; sigma = priors[lang]['sigma']\n",
        "        Lmax = Lmax_hi if lang=='hindi' else Lmax_ta\n",
        "        K = K_hi if lang=='hindi' else K_ta\n",
        "        weights = weights_hi if lang=='hindi' else weights_ta\n",
        "\n",
        "        S = np.zeros(len(ctx), dtype=np.float32)\n",
        "        E = np.zeros(len(ctx), dtype=np.float32)\n",
        "\n",
        "        for (name, s_start, s_end, eid, offs), m in zip(loaded, idx_maps):\n",
        "            w = weights.get(name, 0.0)\n",
        "            if w <= 0.0: continue\n",
        "            for fi in m.get(qid, []):\n",
        "                offs_raw = offs[fi]\n",
        "                M = len(offs_raw) if hasattr(offs_raw,'__len__') else s_start.shape[1]\n",
        "                s_log = s_start[fi][:M] * w\n",
        "                e_log = s_end[fi][:M] * w\n",
        "                for ti in range(M):\n",
        "                    a, b = to_pair(offs_raw[ti])\n",
        "                    if b > a and 0 <= a < len(ctx) and 1 <= b <= len(ctx):\n",
        "                        S[a] += s_log[ti]\n",
        "                        E[b-1] += e_log[ti]\n",
        "\n",
        "        S = maxpool1d(S); E = maxpool1d(E)\n",
        "\n",
        "        starts = np.argsort(S)[::-1][:K]\n",
        "        best_score = -1e18\n",
        "        best_span = (0, max(0, min(Lmax, len(ctx))-1))\n",
        "        for si in starts:\n",
        "            end_lo = si; end_hi = min(len(ctx)-1, si + Lmax - 1)\n",
        "            if end_hi < end_lo: continue\n",
        "            seg = E[end_lo:end_hi+1]\n",
        "            top_rel = np.argsort(seg)[::-1][:shortlist]\n",
        "            for rel in top_rel:\n",
        "                ej = end_lo + int(rel)\n",
        "                raw = float(S[si] + E[ej])\n",
        "                clen = ej - si + 1\n",
        "                lp = log_normal_logpdf_len(clen, mu, sigma)\n",
        "                lp = max(clip_prior[0], min(clip_prior[1], lp))\n",
        "                raw += lambda_len * lp\n",
        "                if raw > best_score:\n",
        "                    best_score = raw; best_span = (si, ej)\n",
        "\n",
        "        a, b = best_span\n",
        "        a, b = snap_span(ctx, S, E, a, b, delta=0.02)\n",
        "        text = clean_span_text(ctx[a:b+1], lang)\n",
        "        if not text:\n",
        "            b2 = min(len(ctx), a+1)\n",
        "            text = clean_span_text(ctx[a:b2], lang) or ctx[a:b2]\n",
        "        out_rows.append((qid, text))\n",
        "\n",
        "    sub = pd.DataFrame(out_rows, columns=['id','PredictionString'])\n",
        "    empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "    mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "    print(f'3-seed+384 tight decode done in {time.time()-t0:.1f}s. Empties={empties}, mean_len={mean_len:.2f}')\n",
        "    return sub\n",
        "\n",
        "sub_3tight = decode_3seed_tight()\n",
        "out_3tight = 'submission_3seed_512_plus_384_tight_wHI90_10_wTA100_0_lambda012_K200_240_Lmax48_58_delta002.csv'\n",
        "sub_3tight.to_csv(out_3tight, index=False)\n",
        "pd.read_csv(out_3tight).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_3tight)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded xlmr512 (1401, 512) offsets (1401, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded xlmr384 (1921, 384) offsets (1921, 384)\n"
          ]
        }
      ]
    },
    {
      "id": "469b856a-0f0b-4889-9eb9-20b541eafdb7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 512 single-seed char-fusion with question-aware Lmax cap, tighter search, expanded boundaries, beta=0\n",
        "import json, math, time, re, unicodedata as ud\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id2lang = dict(zip(test['id'].tolist(), test['language'].tolist()))\n",
        "id2context = dict(zip(test['id'].tolist(), test['context'].astype(str).tolist()))\n",
        "id2question = dict(zip(test['id'].tolist(), test.get('question', pd.Series(['']*len(test))).astype(str).tolist()))\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "ZW_CHARS = {'\\u200B','\\u200C','\\u200D','\\u2060','\\ufeff'}\n",
        "NBSP_SET = {'\\u00A0','\\u2002','\\u2003','\\u2004','\\u2005','\\u2006','\\u2007','\\u2008','\\u2009','\\u200A'}\n",
        "HI_VIRAMA='\\u094D'; TA_PULLI='\\u0BCD'; DANDA='\\u0964'\n",
        "def clean_span_text(s: str, lang: str) -> str:\n",
        "    if not s: return ''\n",
        "    s = ''.join(ch for ch in s if ch not in ZW_CHARS)\n",
        "    s = ''.join(' ' if ch in NBSP_SET else ch for ch in s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    s = re.sub(r'(?<=\\d)[\\s,._-](?=\\d)', '', s)\n",
        "    if s:\n",
        "        last = s[-1]\n",
        "        if ud.category(last) == 'Mn' or last in (HI_VIRAMA, TA_PULLI): s = s[:-1]\n",
        "    if lang == 'hindi':\n",
        "        s = s.replace(DANDA+DANDA, DANDA)\n",
        "        if s.endswith(DANDA): s = s[:-1].rstrip()\n",
        "    return s\n",
        "\n",
        "def log_normal_logpdf_len(char_len: int, mu: float, sigma: float) -> float:\n",
        "    Lc = max(1, int(char_len))\n",
        "    x = math.log(Lc)\n",
        "    return -0.5*((x-mu)/sigma)**2 - math.log(max(Lc,1e-6)) - math.log(sigma) - 0.5*math.log(2*math.pi)\n",
        "\n",
        "def to_pair(t):\n",
        "    try:\n",
        "        if t is None: return (0,0)\n",
        "        if isinstance(t,(list,tuple,np.ndarray)) and len(t)>=2:\n",
        "            a = int(t[0]) if t[0] is not None else 0\n",
        "            b = int(t[1]) if t[1] is not None else 0\n",
        "            return (a,b)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return (0,0)\n",
        "\n",
        "def load_npz_logits(npz_path):\n",
        "    arr = np.load(npz_path, allow_pickle=True)\n",
        "    keys = list(arr.keys())\n",
        "    if 'start' in keys and 'end' in keys:\n",
        "        return arr['start'], arr['end']\n",
        "    for sk in ['start_logits','test_start_logits','start_logits_avg']:\n",
        "        for ek in ['end_logits','test_end_logits','end_logits_avg']:\n",
        "            if sk in keys and ek in keys: return arr[sk], arr[ek]\n",
        "    raise ValueError(f'Unknown keys in {npz_path}: {keys}')\n",
        "\n",
        "def maxpool1d(x):\n",
        "    if len(x) == 0: return x\n",
        "    y = x.copy()\n",
        "    if len(x) == 1: return y\n",
        "    y[0] = max(x[0], x[1])\n",
        "    for i in range(1, len(x)-1): y[i] = max(x[i-1], x[i], x[i+1])\n",
        "    y[-1] = max(x[-2], x[-1])\n",
        "    return y\n",
        "\n",
        "def is_boundary_char(ch: str) -> bool:\n",
        "    return ch.isspace() or ch in {'.', ',', '!', '?', DANDA, '\"', \"'\", '(', ')', '[', ']', '{', '}', '\\u201c', '\\u201d', '\\u2018', '\\u2019', '\\u00ab', '\\u00bb'}\n",
        "\n",
        "def snap_span(ctx: str, S: np.ndarray, E: np.ndarray, a: int, b: int, delta: float = 0.02):\n",
        "    base = float(S[a] + E[b])\n",
        "    a2 = a\n",
        "    for i in range(a-1, -1, -1):\n",
        "        if is_boundary_char(ctx[i]):\n",
        "            a2 = i+1; break\n",
        "    b2 = b\n",
        "    for j in range(b, len(ctx)):\n",
        "        if is_boundary_char(ctx[j]):\n",
        "            b2 = max(a2, j-1); break\n",
        "    cand = float(S[a2] + E[b2])\n",
        "    if cand >= base - delta:\n",
        "        return a2, b2\n",
        "    if a > 0 and b < len(ctx)-1 and ctx[a-1] in {'(', '\"', \"'\"} and ctx[b+1] in {')', '\"', \"'\"}:\n",
        "        cand2 = float(S[a-1] + E[b+1])\n",
        "        if cand2 >= base - delta and (b+1 - (a-1) <= (b - a) + 2):\n",
        "            return a-1, b+1\n",
        "    return a, b\n",
        "\n",
        "# Question-aware cap detection\n",
        "NUM_PAT = re.compile(r'\\d')\n",
        "DATE_PAT = re.compile(r'(\\d{1,2}[\\-/\\.]\\d{1,2}|\\d{4})')\n",
        "HI_NUM_WORDS = {'\u0915\u092c','\u0915\u093f\u0924\u0928\u0947','\u0915\u093f\u0924\u0928\u093e','\u0938\u0902\u0916\u094d\u092f\u093e','\u0926\u093f\u0928','\u0938\u093e\u0932','\u0924\u093e\u0930\u0940\u0916'}\n",
        "TA_NUM_WORDS = {'\u0b8e\u0baa\u0bcd\u0baa\u0bcb\u0ba4\u0bc1','\u0b8e\u0ba4\u0bcd\u0ba4\u0ba9\u0bc8','\u0b8e\u0bb5\u0bcd\u0bb5\u0bb3\u0bb5\u0bc1','\u0b8e\u0ba3\u0bcd','\u0b86\u0ba3\u0bcd\u0b9f\u0bc1','\u0ba4\u0bc7\u0ba4\u0bbf'}\n",
        "\n",
        "def cap_Lmax_by_question(qid: str, base_Lmax: int) -> int:\n",
        "    q = id2question.get(qid, '')\n",
        "    lang = id2lang[qid]\n",
        "    has_num = bool(NUM_PAT.search(q)) or bool(DATE_PAT.search(q))\n",
        "    if lang == 'hindi':\n",
        "        for w in HI_NUM_WORDS:\n",
        "            if w in q: has_num = True; break\n",
        "    else:\n",
        "        for w in TA_NUM_WORDS:\n",
        "            if w in q: has_num = True; break\n",
        "    if has_num:\n",
        "        return min(base_Lmax, 20)\n",
        "    return base_Lmax\n",
        "\n",
        "# Load 512 single-seed only (alignment-safe)\n",
        "s_start, s_end = load_npz_logits('xlmr_large_512_test_avg.npz')\n",
        "eid = json.loads(Path('xlmr_large_512_test_logits/test_example_id.json').read_text())\n",
        "offs_all = np.load('xlmr_large_512_test_logits/test_offset_mapping.npy', allow_pickle=True)\n",
        "print('Loaded 512 single-seed:', s_start.shape, 'offsets', getattr(offs_all,'shape',None))\n",
        "\n",
        "def decode_512_alignsafe_qcap():\n",
        "    lambda_len = 0.12\n",
        "    clip_prior = (-0.8, 0.0)\n",
        "    K_hi, K_ta = 180, 200\n",
        "    Lmax_hi, Lmax_ta = 46, 56\n",
        "    shortlist = 5\n",
        "    # index map\n",
        "    m = {}\n",
        "    for i, qid in enumerate(eid): m.setdefault(qid, []).append(i)\n",
        "    out_rows = []\n",
        "    t0 = time.time()\n",
        "    for qid in test['id'].tolist():\n",
        "        lang = id2lang[qid]; ctx = id2context[qid]\n",
        "        mu = priors[lang]['mu']; sigma = priors[lang]['sigma']\n",
        "        Lmax_base = Lmax_hi if lang=='hindi' else Lmax_ta\n",
        "        Lmax = cap_Lmax_by_question(qid, Lmax_base)\n",
        "        K = K_hi if lang=='hindi' else K_ta\n",
        "        S = np.zeros(len(ctx), dtype=np.float32)\n",
        "        E = np.zeros(len(ctx), dtype=np.float32)\n",
        "        for fi in m.get(qid, []):\n",
        "            offs_raw = offs_all[fi]\n",
        "            M = len(offs_raw) if hasattr(offs_raw,'__len__') else s_start.shape[1]\n",
        "            s_log = s_start[fi][:M]\n",
        "            e_log = s_end[fi][:M]\n",
        "            for ti in range(M):\n",
        "                a, b = to_pair(offs_raw[ti])\n",
        "                if b > a and 0 <= a < len(ctx) and 1 <= b <= len(ctx):\n",
        "                    S[a] += s_log[ti]\n",
        "                    E[b-1] += e_log[ti]\n",
        "        S = maxpool1d(S); E = maxpool1d(E)\n",
        "        starts = np.argsort(S)[::-1][:K]\n",
        "        best_score = -1e18; best_span = (0, max(0, min(Lmax, len(ctx))-1))\n",
        "        for si in starts:\n",
        "            end_lo = si; end_hi = min(len(ctx)-1, si + Lmax - 1)\n",
        "            if end_hi < end_lo: continue\n",
        "            seg = E[end_lo:end_hi+1]\n",
        "            top_rel = np.argsort(seg)[::-1][:shortlist]\n",
        "            for rel in top_rel:\n",
        "                ej = end_lo + int(rel)\n",
        "                raw = float(S[si] + E[ej])\n",
        "                clen = ej - si + 1\n",
        "                lp = log_normal_logpdf_len(clen, mu, sigma)\n",
        "                lp = max(clip_prior[0], min(clip_prior[1], lp))\n",
        "                raw += lambda_len * lp\n",
        "                if raw > best_score:\n",
        "                    best_score = raw; best_span = (si, ej)\n",
        "        a, b = best_span\n",
        "        a, b = snap_span(ctx, S, E, a, b, delta=0.02)\n",
        "        text = clean_span_text(ctx[a:b+1], lang)\n",
        "        if not text:\n",
        "            b2 = min(len(ctx), a+1)\n",
        "            text = clean_span_text(ctx[a:b2], lang) or ctx[a:b2]\n",
        "        out_rows.append((qid, text))\n",
        "    sub = pd.DataFrame(out_rows, columns=['id','PredictionString'])\n",
        "    empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "    mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "    print(f'512 align-safe q-cap decode done in {time.time()-t0:.1f}s. Empties={empties}, mean_len={mean_len:.2f}')\n",
        "    return sub\n",
        "\n",
        "sub_qcap = decode_512_alignsafe_qcap()\n",
        "out_qcap = 'submission_512_alignsafe_qcap_lambda012_K180_200_Lmax46_56_short5_delta002.csv'\n",
        "sub_qcap.to_csv(out_qcap, index=False)\n",
        "pd.read_csv(out_qcap).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_qcap)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 512 single-seed: (1401, 512) offsets (1401, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512 align-safe q-cap decode done in 2.5s. Empties=0, mean_len=13.91\nsubmission.csv updated -> submission_512_alignsafe_qcap_lambda012_K180_200_Lmax46_56_short5_delta002.csv\n"
          ]
        }
      ]
    },
    {
      "id": "082c2e90-9930-401d-b43d-e7817c606563",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Single-stream 3-seed 512 char-fusion with boundary snap (alignment as before), no MuRIL/384\n",
        "import json, math, time, re, unicodedata as ud\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id2lang = dict(zip(test['id'].tolist(), test['language'].tolist()))\n",
        "id2context = dict(zip(test['id'].tolist(), test['context'].astype(str).tolist()))\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "ZW_CHARS = {'\\u200B','\\u200C','\\u200D','\\u2060','\\ufeff'}\n",
        "NBSP_SET = {'\\u00A0','\\u2002','\\u2003','\\u2004','\\u2005','\\u2006','\\u2007','\\u2008','\\u2009','\\u200A'}\n",
        "HI_VIRAMA='\\u094D'; TA_PULLI='\\u0BCD'; DANDA='\\u0964'\n",
        "def clean_span_text(s: str, lang: str) -> str:\n",
        "    if not s: return ''\n",
        "    s = ''.join(ch for ch in s if ch not in ZW_CHARS)\n",
        "    s = ''.join(' ' if ch in NBSP_SET else ch for ch in s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    s = re.sub(r'(?<=\\d)[\\s,._-](?=\\d)', '', s)\n",
        "    if s:\n",
        "        last = s[-1]\n",
        "        if ud.category(last) == 'Mn' or last in (HI_VIRAMA, TA_PULLI): s = s[:-1]\n",
        "    if lang == 'hindi':\n",
        "        s = s.replace(DANDA+DANDA, DANDA)\n",
        "        if s.endswith(DANDA): s = s[:-1].rstrip()\n",
        "    return s\n",
        "\n",
        "def log_normal_logpdf_len(char_len: int, mu: float, sigma: float) -> float:\n",
        "    Lc = max(1, int(char_len))\n",
        "    x = math.log(Lc)\n",
        "    return -0.5*((x-mu)/sigma)**2 - math.log(max(Lc,1e-6)) - math.log(sigma) - 0.5*math.log(2*math.pi)\n",
        "\n",
        "def to_pair(t):\n",
        "    try:\n",
        "        if t is None: return (0,0)\n",
        "        if isinstance(t,(list,tuple,np.ndarray)) and len(t)>=2:\n",
        "            a = int(t[0]) if t[0] is not None else 0\n",
        "            b = int(t[1]) if t[1] is not None else 0\n",
        "            return (a,b)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return (0,0)\n",
        "\n",
        "def maxpool1d(x):\n",
        "    if len(x) == 0: return x\n",
        "    y = x.copy()\n",
        "    if len(x) == 1: return y\n",
        "    y[0] = max(x[0], x[1])\n",
        "    for i in range(1, len(x)-1): y[i] = max(x[i-1], x[i], x[i+1])\n",
        "    y[-1] = max(x[-2], x[-1])\n",
        "    return y\n",
        "\n",
        "def is_boundary_char(ch: str) -> bool:\n",
        "    return ch.isspace() or ch in {'.', ',', '!', '?', DANDA, '\"', \"'\", '(', ')', '[', ']', '{', '}', '\\u201c', '\\u201d', '\\u2018', '\\u2019', '\\u00ab', '\\u00bb'}\n",
        "\n",
        "def snap_span(ctx: str, S: np.ndarray, E: np.ndarray, a: int, b: int, delta: float = 0.02):\n",
        "    base = float(S[a] + E[b])\n",
        "    a2 = a\n",
        "    for i in range(a-1, -1, -1):\n",
        "        if is_boundary_char(ctx[i]):\n",
        "            a2 = i+1; break\n",
        "    b2 = b\n",
        "    for j in range(b, len(ctx)):\n",
        "        if is_boundary_char(ctx[j]):\n",
        "            b2 = max(a2, j-1); break\n",
        "    cand = float(S[a2] + E[b2])\n",
        "    if cand >= base - delta:\n",
        "        return a2, b2\n",
        "    if a > 0 and b < len(ctx)-1 and ctx[a-1] in {'(', '\"', \"'\"} and ctx[b+1] in {')', '\"', \"'\"}:\n",
        "        cand2 = float(S[a-1] + E[b+1])\n",
        "        if cand2 >= base - delta and (b+1 - (a-1) <= (b - a) + 2):\n",
        "            return a-1, b+1\n",
        "    return a, b\n",
        "\n",
        "# Load 3-seed 512 logits and 512 mapping\n",
        "s512, e512 = np.load('xlmr_large_512_3seeds_avg.npz', allow_pickle=True)['start'], np.load('xlmr_large_512_3seeds_avg.npz', allow_pickle=True)['end']\n",
        "eid_512 = json.loads(Path('xlmr_large_512_test_logits/test_example_id.json').read_text())\n",
        "off_512 = np.load('xlmr_large_512_test_logits/test_offset_mapping.npy', allow_pickle=True)\n",
        "print('Loaded 3-seed 512:', s512.shape, 'mapping', len(eid_512))\n",
        "\n",
        "# Build qid -> feat indices\n",
        "qid_to_feat_idx = {}\n",
        "for i, qid in enumerate(eid_512):\n",
        "    qid_to_feat_idx.setdefault(qid, []).append(i)\n",
        "\n",
        "def decode_3seed_single(lambda_len=0.12, clip_prior=(-0.8,0.0), nbest_hi=200, nbest_ta=240, Lmax_hi=50, Lmax_ta=60):\n",
        "    t0 = time.time()\n",
        "    out_rows = []\n",
        "    for qid in test['id'].tolist():\n",
        "        lang = id2lang[qid]; ctx = id2context[qid]\n",
        "        mu = priors[lang]['mu']; sigma = priors[lang]['sigma']\n",
        "        Lmax = Lmax_hi if lang=='hindi' else Lmax_ta\n",
        "        K = nbest_hi if lang=='hindi' else nbest_ta\n",
        "        S = np.zeros(len(ctx), dtype=np.float32)\n",
        "        E = np.zeros(len(ctx), dtype=np.float32)\n",
        "        for fi in qid_to_feat_idx.get(qid, []):\n",
        "            offs_raw = off_512[fi]\n",
        "            M = len(offs_raw) if hasattr(offs_raw,'__len__') else s512.shape[1]\n",
        "            s_log = s512[fi][:M]\n",
        "            e_log = e512[fi][:M]\n",
        "            for ti in range(M):\n",
        "                a,b = to_pair(offs_raw[ti])\n",
        "                if b > a and 0 <= a < len(ctx) and 1 <= b <= len(ctx):\n",
        "                    S[a] += s_log[ti]\n",
        "                    E[b-1] += e_log[ti]\n",
        "        S = maxpool1d(S); E = maxpool1d(E)\n",
        "        if len(S) == 0:\n",
        "            out_rows.append((qid, ctx[:1] if len(ctx)>0 else '')); continue\n",
        "        starts = np.argsort(S)[::-1][:K]\n",
        "        best_score = -1e18; best_span = (0, max(0, min(Lmax, len(ctx))-1))\n",
        "        for si in starts:\n",
        "            end_lo = si; end_hi = min(len(ctx)-1, si + Lmax - 1)\n",
        "            if end_hi < end_lo: continue\n",
        "            seg = E[end_lo:end_hi+1]\n",
        "            ej = end_lo + int(np.argmax(seg))\n",
        "            raw = float(S[si] + E[ej])\n",
        "            clen = ej - si + 1\n",
        "            lp = log_normal_logpdf_len(clen, mu, sigma)\n",
        "            lp = max(clip_prior[0], min(clip_prior[1], lp))\n",
        "            raw += lambda_len * lp\n",
        "            if raw > best_score:\n",
        "                best_score = raw; best_span = (si, ej)\n",
        "        a,b = best_span\n",
        "        a,b = snap_span(ctx, S, E, a, b, delta=0.02)\n",
        "        text = clean_span_text(ctx[a:b+1], lang)\n",
        "        if not text:\n",
        "            b2 = min(len(ctx), a+1); text = clean_span_text(ctx[a:b2], lang) or ctx[a:b2]\n",
        "        out_rows.append((qid, text))\n",
        "    sub = pd.DataFrame(out_rows, columns=['id','PredictionString'])\n",
        "    empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "    mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "    print(f'3-seed single-stream snap decode done in {time.time()-t0:.1f}s. Empties={empties}, mean_len={mean_len:.2f}')\n",
        "    return sub\n",
        "\n",
        "sub3s = decode_3seed_single(lambda_len=0.12, nbest_hi=200, nbest_ta=240, Lmax_hi=50, Lmax_ta=60)\n",
        "out3s = 'submission_3seed512_single_charfusion_snap_lambda012_K200_240_Lmax50_60_delta002.csv'\n",
        "sub3s.to_csv(out3s, index=False)\n",
        "pd.read_csv(out3s).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out3s)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3-seed 512: (1401, 512) mapping 1401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3-seed single-stream snap decode done in 2.3s. Empties=0, mean_len=11.25\nsubmission.csv updated -> submission_3seed512_single_charfusion_snap_lambda012_K200_240_Lmax50_60_delta002.csv\n"
          ]
        }
      ]
    },
    {
      "id": "6884f6c5-7e79-4aeb-9af2-9e7689f1f941",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Consensus ensemble over multiple healthy submissions via average word-level Jaccard\n",
        "import pandas as pd, numpy as np, math, json, time\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id_list = test['id'].tolist()\n",
        "lang_map = dict(zip(test['id'], test['language']))\n",
        "\n",
        "# Candidate submissions (only healthy diagnostics, no long-drift variants)\n",
        "cand_files = [\n",
        "    'submission_charfusion_512_lambda015.csv',\n",
        "    'submission_384only_charfusion_lambda015.csv',\n",
        "    'submission_primary_multistream_snap_hi85_10_5_ta97_3_0_lambda015.csv',\n",
        "    'submission_third_multistream_drop_muril_snap_lambda010.csv',\n",
        "    'submission_tokenselect_512single_or_384_lambda012.csv',\n",
        "    'submission_3seed512_single_charfusion_snap_lambda012_K200_240_Lmax50_60_delta002.csv',\n",
        "]\n",
        "\n",
        "# Load all candidates that exist\n",
        "cands = []\n",
        "for f in cand_files:\n",
        "    try:\n",
        "        df = pd.read_csv(f)\n",
        "        if set(df.columns) >= {'id','PredictionString'} and len(df)==len(test):\n",
        "            df = df[['id','PredictionString']].copy()\n",
        "            df['PredictionString'] = df['PredictionString'].astype(str)\n",
        "            cands.append((f, df.set_index('id')['PredictionString'].to_dict()))\n",
        "            print('Loaded candidate:', f)\n",
        "        else:\n",
        "            print('Skip (shape/cols mismatch):', f)\n",
        "    except Exception as e:\n",
        "        print('Skip (read error):', f, '->', e)\n",
        "\n",
        "assert len(cands) >= 3, 'Not enough candidate submissions loaded for consensus'\n",
        "\n",
        "def words(s):\n",
        "    return [w for w in str(s).strip().split() if len(w)>0]\n",
        "\n",
        "def jaccard(a, b):\n",
        "    wa, wb = set(words(a)), set(words(b))\n",
        "    if not wa and not wb: return 1.0\n",
        "    if not wa or not wb: return 0.0\n",
        "    inter = len(wa & wb); uni = len(wa | wb)\n",
        "    return inter / uni if uni>0 else 0.0\n",
        "\n",
        "# Per-language log-normal priors\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "def len_prior_penalty(s, lang):\n",
        "    L = max(1, len(str(s)))\n",
        "    mu = priors.get(lang, {}).get('mu', 2.3)\n",
        "    return -abs(math.log(L) - mu)  # higher is better (closer to mean)\n",
        "\n",
        "# Consensus: for each id, score each candidate by average Jaccard vs others; tie-break by len prior proximity\n",
        "out_rows = []\n",
        "t0 = time.time()\n",
        "for qid in id_list:\n",
        "    lang = lang_map[qid]\n",
        "    texts = [d[qid] for _, d in cands]\n",
        "    best_idx, best_score, best_lp = -1, -1e18, -1e18\n",
        "    for i, ti in enumerate(texts):\n",
        "        # avg Jaccard vs others\n",
        "        if len(texts) == 1:\n",
        "            avg_j = 0.0\n",
        "        else:\n",
        "            s = 0.0; cnt = 0\n",
        "            for j, tj in enumerate(texts):\n",
        "                if i==j: continue\n",
        "                s += jaccard(ti, tj); cnt += 1\n",
        "            avg_j = s / max(1, cnt)\n",
        "        lp = len_prior_penalty(ti, lang)\n",
        "        score = avg_j + 0.02 * lp  # small regularization toward prior\n",
        "        if score > best_score or (abs(score-best_score) < 1e-9 and lp > best_lp):\n",
        "            best_score, best_idx, best_lp = score, i, lp\n",
        "    out_rows.append((qid, texts[best_idx]))\n",
        "\n",
        "sub = pd.DataFrame(out_rows, columns=['id','PredictionString'])\n",
        "empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "print(f'Consensus built from {len(cands)} submissions in {time.time()-t0:.2f}s. Empties={empties}, mean_len={mean_len:.2f}')\n",
        "\n",
        "out_path = 'submission_consensus_avgjacc_lenreg002.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "pd.read_csv(out_path).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_path)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded candidate: submission_charfusion_512_lambda015.csv\nLoaded candidate: submission_384only_charfusion_lambda015.csv\nLoaded candidate: submission_primary_multistream_snap_hi85_10_5_ta97_3_0_lambda015.csv\nLoaded candidate: submission_third_multistream_drop_muril_snap_lambda010.csv\nLoaded candidate: submission_tokenselect_512single_or_384_lambda012.csv\nLoaded candidate: submission_3seed512_single_charfusion_snap_lambda012_K200_240_Lmax50_60_delta002.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consensus built from 6 submissions in 0.01s. Empties=0, mean_len=10.78\nsubmission.csv updated -> submission_consensus_avgjacc_lenreg002.csv\n"
          ]
        }
      ]
    },
    {
      "id": "bc034bd1-1b6e-4241-a4c7-0442a8ba8f23",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Per-language consensus with majority override and length regularization\n",
        "import pandas as pd, numpy as np, math, time\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id_list = test['id'].tolist()\n",
        "lang_map = dict(zip(test['id'], test['language']))\n",
        "\n",
        "# Global candidate pool (healthy only)\n",
        "all_cands = [\n",
        "    'submission_charfusion_512_lambda015.csv',\n",
        "    'submission_384only_charfusion_lambda015.csv',\n",
        "    'submission_primary_multistream_snap_hi85_10_5_ta97_3_0_lambda015.csv',\n",
        "    'submission_third_multistream_drop_muril_snap_lambda010.csv',\n",
        "    'submission_tokenselect_512single_or_384_lambda012.csv',\n",
        "    'submission_3seed512_single_charfusion_snap_lambda012_K200_240_Lmax50_60_delta002.csv',\n",
        "]\n",
        "\n",
        "# Tamil-restricted candidate set (avoid 384 pull); include third_variant optionally if exists\n",
        "ta_pref = [\n",
        "    'submission_charfusion_512_lambda015.csv',\n",
        "    'submission_3seed512_single_charfusion_snap_lambda012_K200_240_Lmax50_60_delta002.csv',\n",
        "    'submission_tokenselect_512single_or_384_lambda012.csv',\n",
        "    'submission_third_multistream_drop_muril_snap_lambda010.csv',\n",
        "]\n",
        "\n",
        "def load_cands(paths):\n",
        "    loaded = []\n",
        "    for f in paths:\n",
        "        try:\n",
        "            df = pd.read_csv(f)\n",
        "            if set(df.columns) >= {'id','PredictionString'} and len(df)==len(test):\n",
        "                ser = df.set_index('id')['PredictionString'].astype(str)\n",
        "                loaded.append((f, ser.to_dict()))\n",
        "                print('Loaded:', f)\n",
        "            else:\n",
        "                print('Skip (shape/cols):', f)\n",
        "        except Exception as e:\n",
        "            print('Skip:', f, '->', e)\n",
        "    return loaded\n",
        "\n",
        "# Build per-language candidate lists\n",
        "cands_hi = load_cands(all_cands)\n",
        "cands_ta = load_cands([f for f in ta_pref if f in all_cands])\n",
        "if len(cands_ta) < 3:\n",
        "    # Fallback: use all_cands if restriction too small\n",
        "    cands_ta = cands_hi.copy()\n",
        "\n",
        "def words(s):\n",
        "    return [w for w in str(s).strip().split() if len(w)>0]\n",
        "\n",
        "def jaccard(a, b):\n",
        "    wa, wb = set(words(a)), set(words(b))\n",
        "    if not wa and not wb: return 1.0\n",
        "    if not wa or not wb: return 0.0\n",
        "    inter = len(wa & wb); uni = len(wa | wb)\n",
        "    return inter/uni if uni>0 else 0.0\n",
        "\n",
        "# Fit per-language log-normal priors from train\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "def len_prior_score(s, lang):\n",
        "    L = max(1, len(str(s)))\n",
        "    mu = priors.get(lang, {}).get('mu', 2.3)\n",
        "    return -abs(math.log(L) - mu)  # higher is better (closer to mean)\n",
        "\n",
        "def majority_override(texts, min_votes=3):\n",
        "    from collections import Counter\n",
        "    cnt = Counter(texts)\n",
        "    text, votes = cnt.most_common(1)[0]\n",
        "    return (text if votes >= min_votes else None), votes\n",
        "\n",
        "# Per-language lambda for length regularization\n",
        "lambda_hi = 0.02\n",
        "lambda_ta = 0.03\n",
        "\n",
        "t0 = time.time()\n",
        "rows = []\n",
        "for qid in id_list:\n",
        "    lang = lang_map[qid]\n",
        "    cands = cands_hi if lang=='hindi' else cands_ta\n",
        "    texts = [d[qid] for _, d in cands]\n",
        "    # Majority override\n",
        "    maj_text, votes = majority_override(texts, min_votes=3)\n",
        "    if maj_text is not None:\n",
        "        rows.append((qid, maj_text))\n",
        "        continue\n",
        "    # Score by avg word-Jaccard + len prior\n",
        "    best_idx, best_score = -1, -1e18\n",
        "    best_len = 10**9\n",
        "    lam = lambda_hi if lang=='hindi' else lambda_ta\n",
        "    for i, ti in enumerate(texts):\n",
        "        if len(texts) == 1:\n",
        "            avg_j = 0.0\n",
        "        else:\n",
        "            s = 0.0; cnt = 0\n",
        "            for j, tj in enumerate(texts):\n",
        "                if i==j: continue\n",
        "                s += jaccard(ti, tj); cnt += 1\n",
        "            avg_j = s / max(1, cnt)\n",
        "        lp = len_prior_score(ti, lang)\n",
        "        score = avg_j + lam * lp\n",
        "        # Tie-breaker within 0.01: prefer shorter string\n",
        "        L = len(str(ti))\n",
        "        if (score > best_score + 1e-12) or (abs(score - best_score) <= 0.01 and L < best_len):\n",
        "            best_score = score; best_idx = i; best_len = L\n",
        "    rows.append((qid, texts[best_idx]))\n",
        "\n",
        "sub = pd.DataFrame(rows, columns=['id','PredictionString'])\n",
        "empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "print(f'Per-language consensus built. Empties={int(empties)}, mean_len={mean_len:.2f}, time={time.time()-t0:.2f}s')\n",
        "\n",
        "out_path = 'submission_consensus_perlang_majority_lenhi002_lenta003.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "pd.read_csv(out_path).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_path)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: submission_charfusion_512_lambda015.csv\nLoaded: submission_384only_charfusion_lambda015.csv\nLoaded: submission_primary_multistream_snap_hi85_10_5_ta97_3_0_lambda015.csv\nLoaded: submission_third_multistream_drop_muril_snap_lambda010.csv\nLoaded: submission_tokenselect_512single_or_384_lambda012.csv\nLoaded: submission_3seed512_single_charfusion_snap_lambda012_K200_240_Lmax50_60_delta002.csv\nLoaded: submission_charfusion_512_lambda015.csv\nLoaded: submission_3seed512_single_charfusion_snap_lambda012_K200_240_Lmax50_60_delta002.csv\nLoaded: submission_tokenselect_512single_or_384_lambda012.csv\nLoaded: submission_third_multistream_drop_muril_snap_lambda010.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-language consensus built. Empties=0, mean_len=10.71, time=0.01s\nsubmission.csv updated -> submission_consensus_perlang_majority_lenhi002_lenta003.csv\n"
          ]
        }
      ]
    },
    {
      "id": "c9c18bb4-6015-4a8d-a6a9-df5f0893e66b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Global stronger length-regularized consensus (lambda_len=0.05), same healthy candidates\n",
        "import pandas as pd, numpy as np, math, time\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id_list = test['id'].tolist()\n",
        "lang_map = dict(zip(test['id'], test['language']))\n",
        "\n",
        "cand_files = [\n",
        "    'submission_charfusion_512_lambda015.csv',\n",
        "    'submission_384only_charfusion_lambda015.csv',\n",
        "    'submission_primary_multistream_snap_hi85_10_5_ta97_3_0_lambda015.csv',\n",
        "    'submission_third_multistream_drop_muril_snap_lambda010.csv',\n",
        "    'submission_tokenselect_512single_or_384_lambda012.csv',\n",
        "    'submission_3seed512_single_charfusion_snap_lambda012_K200_240_Lmax50_60_delta002.csv',\n",
        "]\n",
        "\n",
        "cands = []\n",
        "for f in cand_files:\n",
        "    try:\n",
        "        df = pd.read_csv(f)\n",
        "        if set(df.columns) >= {'id','PredictionString'} and len(df)==len(test):\n",
        "            cands.append((f, df.set_index('id')['PredictionString'].astype(str).to_dict()))\n",
        "            print('Loaded candidate:', f)\n",
        "        else:\n",
        "            print('Skip (shape/cols mismatch):', f)\n",
        "    except Exception as e:\n",
        "        print('Skip (read error):', f, '->', e)\n",
        "assert len(cands) >= 3, 'Need at least 3 candidates'\n",
        "\n",
        "def words(s):\n",
        "    return [w for w in str(s).strip().split() if len(w)>0]\n",
        "\n",
        "def jaccard(a, b):\n",
        "    wa, wb = set(words(a)), set(words(b))\n",
        "    if not wa and not wb: return 1.0\n",
        "    if not wa or not wb: return 0.0\n",
        "    inter = len(wa & wb); uni = len(wa | wb)\n",
        "    return inter/uni if uni>0 else 0.0\n",
        "\n",
        "# Per-language log-normal prior (for length regularization)\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "def len_prior_score(s, lang):\n",
        "    L = max(1, len(str(s)))\n",
        "    mu = priors.get(lang, {}).get('mu', 2.3)\n",
        "    return -abs(math.log(L) - mu)  # higher is better when closer to lang mean\n",
        "\n",
        "lambda_len = 0.05\n",
        "rows = []\n",
        "t0 = time.time()\n",
        "for qid in id_list:\n",
        "    lang = lang_map[qid]\n",
        "    texts = [d[qid] for _, d in cands]\n",
        "    best_idx, best_score, best_len = -1, -1e18, 10**9\n",
        "    for i, ti in enumerate(texts):\n",
        "        if len(texts) == 1:\n",
        "            avg_j = 0.0\n",
        "        else:\n",
        "            s = 0.0; cnt = 0\n",
        "            for j, tj in enumerate(texts):\n",
        "                if i==j: continue\n",
        "                s += jaccard(ti, tj); cnt += 1\n",
        "            avg_j = s / max(1, cnt)\n",
        "        lp = len_prior_score(ti, lang)\n",
        "        score = avg_j + lambda_len * lp\n",
        "        L = len(str(ti))\n",
        "        if (score > best_score + 1e-12) or (abs(score - best_score) <= 0.01 and L < best_len):\n",
        "            best_score = score; best_idx = i; best_len = L\n",
        "    rows.append((qid, texts[best_idx]))\n",
        "\n",
        "sub = pd.DataFrame(rows, columns=['id','PredictionString'])\n",
        "empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "print(f'Stronger len-reg consensus built from {len(cands)} in {time.time()-t0:.2f}s. Empties={int(empties)}, mean_len={mean_len:.2f}')\n",
        "\n",
        "out_path = 'submission_consensus_lenreg005.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "pd.read_csv(out_path).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_path)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded candidate: submission_charfusion_512_lambda015.csv\nLoaded candidate: submission_384only_charfusion_lambda015.csv\nLoaded candidate: submission_primary_multistream_snap_hi85_10_5_ta97_3_0_lambda015.csv\nLoaded candidate: submission_third_multistream_drop_muril_snap_lambda010.csv\nLoaded candidate: submission_tokenselect_512single_or_384_lambda012.csv\nLoaded candidate: submission_3seed512_single_charfusion_snap_lambda012_K200_240_Lmax50_60_delta002.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stronger len-reg consensus built from 6 in 0.01s. Empties=0, mean_len=10.77\nsubmission.csv updated -> submission_consensus_lenreg005.csv\n"
          ]
        }
      ]
    },
    {
      "id": "e7a6fd99-223c-4abb-b376-9acc24daa8b2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Coverage-normalized decoders (per expert) + per-id consensus over A/B/C\n",
        "import numpy as np, pandas as pd, math, json, time, re, unicodedata as ud\n",
        "from pathlib import Path\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id_list = test['id'].tolist()\n",
        "id2lang = dict(zip(test['id'], test['language']))\n",
        "id2ctx = dict(zip(test['id'], test['context'].astype(str)))\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "def to_pair(t):\n",
        "    try:\n",
        "        if t is None: return (0,0)\n",
        "        if isinstance(t,(list,tuple,np.ndarray)) and len(t)>=2:\n",
        "            a = int(t[0]) if t[0] is not None else 0\n",
        "            b = int(t[1]) if t[1] is not None else 0\n",
        "            return (a,b)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return (0,0)\n",
        "\n",
        "def softmax(x):\n",
        "    x = np.asarray(x, dtype=np.float64)\n",
        "    m = np.max(x) if x.size else 0.0\n",
        "    e = np.exp(x - m)\n",
        "    s = e.sum()\n",
        "    return e / s if s > 0 else np.zeros_like(x, dtype=np.float64)\n",
        "\n",
        "ZW_CHARS = {'\\u200B','\\u200C','\\u200D','\\u2060','\\ufeff'}\n",
        "NBSP_SET = {'\\u00A0','\\u2002','\\u2003','\\u2004','\\u2005','\\u2006','\\u2007','\\u2008','\\u2009','\\u200A'}\n",
        "HI_VIRAMA='\\u094D'; TA_PULLI='\\u0BCD'; DANDA='\\u0964'\n",
        "def clean_span_text(s: str, lang: str) -> str:\n",
        "    if not s: return ''\n",
        "    s = ''.join(ch for ch in s if ch not in ZW_CHARS)\n",
        "    s = ''.join(' ' if ch in NBSP_SET else ch for ch in s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    s = re.sub(r'(?<=\\d)[\\s,._-](?=\\d)', '', s)\n",
        "    if s:\n",
        "        last = s[-1]\n",
        "        if ud.category(last) == 'Mn' or last in (HI_VIRAMA, TA_PULLI): s = s[:-1]\n",
        "    if lang == 'hindi':\n",
        "        s = s.replace(DANDA+DANDA, DANDA)\n",
        "        if s.endswith(DANDA): s = s[:-1].rstrip()\n",
        "    return s\n",
        "\n",
        "# Load 512 single-seed (alignment-safe) artifacts\n",
        "def load_npz_logits(npz_path):\n",
        "    arr = np.load(npz_path, allow_pickle=True)\n",
        "    keys = list(arr.keys())\n",
        "    if 'start' in keys and 'end' in keys: return arr['start'], arr['end']\n",
        "    for sk in ['start_logits','test_start_logits','start_logits_avg']:\n",
        "        for ek in ['end_logits','test_end_logits','end_logits_avg']:\n",
        "            if sk in keys and ek in keys: return arr[sk], arr[ek]\n",
        "    raise ValueError(f'Unknown keys in {npz_path}: {keys}')\n",
        "\n",
        "s512, e512 = load_npz_logits('xlmr_large_512_test_avg.npz')\n",
        "eid_512 = json.loads(Path('xlmr_large_512_test_logits/test_example_id.json').read_text())\n",
        "off_512 = np.load('xlmr_large_512_test_logits/test_offset_mapping.npy', allow_pickle=True)\n",
        "print('512 single-seed logits:', s512.shape, 'mapping', len(eid_512))\n",
        "\n",
        "# Load 384 artifacts\n",
        "s384, e384 = load_npz_logits('xlmr_large_test_avg.npz')\n",
        "eid_384 = json.loads(Path('xlmr_large_test_logits/test_example_id.json').read_text())\n",
        "off_384 = np.load('xlmr_large_test_logits/test_offset_mapping.npy', allow_pickle=True)\n",
        "print('384 logits:', s384.shape, 'mapping', len(eid_384))\n",
        "\n",
        "def build_index(eids):\n",
        "    m = {}\n",
        "    for i, q in enumerate(eids): m.setdefault(q, []).append(i)\n",
        "    return m\n",
        "\n",
        "idx512 = build_index(eid_512)\n",
        "idx384 = build_index(eid_384)\n",
        "\n",
        "def decode_charfusion_covnorm(start_logits, end_logits, eids, offs_all, idx_map, tag: str):\n",
        "    rows = []\n",
        "    t0 = time.time()\n",
        "    for qid in id_list:\n",
        "        ctx = id2ctx[qid]; lang = id2lang[qid]\n",
        "        Lmax = 50 if lang=='hindi' else 60\n",
        "        K = 200 if lang=='hindi' else 240\n",
        "        S = np.zeros(len(ctx), dtype=np.float64)\n",
        "        E = np.zeros(len(ctx), dtype=np.float64)\n",
        "        CS = np.zeros(len(ctx), dtype=np.float64)  # coverage counters\n",
        "        CE = np.zeros(len(ctx), dtype=np.float64)\n",
        "        for fi in idx_map.get(qid, []):\n",
        "            offs = offs_all[fi]\n",
        "            M = len(offs) if hasattr(offs,'__len__') else start_logits.shape[1]\n",
        "            ps = softmax(start_logits[fi][:M])\n",
        "            pe = softmax(end_logits[fi][:M])\n",
        "            for ti in range(M):\n",
        "                a,b = to_pair(offs[ti])\n",
        "                if b > a and 0 <= a < len(ctx) and 1 <= b <= len(ctx):\n",
        "                    # distribute uniformly over covered chars\n",
        "                    S[a:b] += ps[ti]\n",
        "                    CS[a:b] += 1.0\n",
        "                    E[a:b] += pe[ti]\n",
        "                    CE[a:b] += 1.0\n",
        "        # coverage-normalize\n",
        "        CS = np.maximum(CS, 1.0); CE = np.maximum(CE, 1.0)\n",
        "        S /= CS; E /= CE\n",
        "        # decode with tight caps, no priors\n",
        "        if len(S) == 0:\n",
        "            rows.append((qid, ctx[:1] if len(ctx)>0 else '')); continue\n",
        "        starts = np.argsort(S)[::-1][:K]\n",
        "        best = (-1e18, 0, max(0, min(Lmax, len(ctx))-1))\n",
        "        for si in starts:\n",
        "            end_lo = si; end_hi = min(len(ctx)-1, si + Lmax - 1)\n",
        "            if end_hi < end_lo: continue\n",
        "            seg = E[end_lo:end_hi+1]\n",
        "            ej = end_lo + int(np.argmax(seg))\n",
        "            score = float(S[si] + E[ej])\n",
        "            if score > best[0]: best = (score, si, ej)\n",
        "        _, a, b = best\n",
        "        text = clean_span_text(ctx[a:b+1], lang)\n",
        "        if not text:\n",
        "            b2 = min(len(ctx), a+1)\n",
        "            text = clean_span_text(ctx[a:b2], lang) or ctx[a:b2]\n",
        "        rows.append((qid, text))\n",
        "    sub = pd.DataFrame(rows, columns=['id','PredictionString'])\n",
        "    empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "    mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "    print(f'Cov-norm char-fusion {tag}: Empties={int(empties)}, mean_len={mean_len:.2f}, time={time.time()-t0:.2f}s')\n",
        "    return sub\n",
        "\n",
        "def decode_token_dp_512():\n",
        "    rows = []\n",
        "    t0 = time.time()\n",
        "    for qid in id_list:\n",
        "        ctx = id2ctx[qid]; lang = id2lang[qid]\n",
        "        Lmax = 50 if lang=='hindi' else 60\n",
        "        K = 200 if lang=='hindi' else 240\n",
        "        best = (-1e18, '')\n",
        "        for fi in idx512.get(qid, []):\n",
        "            offs = off_512[fi]\n",
        "            M = len(offs) if hasattr(offs,'__len__') else s512.shape[1]\n",
        "            ps = softmax(s512[fi][:M]); pe = softmax(e512[fi][:M])\n",
        "            # valid token indices\n",
        "            valid = []\n",
        "            for ti in range(M):\n",
        "                a,b = to_pair(offs[ti])\n",
        "                if b > a and 0 <= a < len(ctx) and 1 <= b <= len(ctx): valid.append(ti)\n",
        "            if not valid: continue\n",
        "            valid = np.array(valid, dtype=np.int32)\n",
        "            s_top = valid[np.argsort(ps[valid])[::-1][:K]]\n",
        "            for si in s_top:\n",
        "                a0, _ = to_pair(offs[si])\n",
        "                cand_e = []\n",
        "                for ei in range(si, M):\n",
        "                    a2,b2 = to_pair(offs[ei])\n",
        "                    if not (b2 > a2 and 0 <= a2 < len(ctx) and 1 <= b2 <= len(ctx)): continue\n",
        "                    clen = b2 - a0\n",
        "                    if clen <= 0 or clen > Lmax: continue\n",
        "                    cand_e.append(ei)\n",
        "                if not cand_e: continue\n",
        "                cand_e = np.array(cand_e, dtype=np.int32)\n",
        "                # score = log ps + log pe\n",
        "                score_vec = np.log(np.maximum(ps[si], 1e-15)) + np.log(np.maximum(pe[cand_e], 1e-15))\n",
        "                ei = int(cand_e[int(np.argmax(score_vec))])\n",
        "                a,b = to_pair(offs[si])[0], to_pair(offs[ei])[1]\n",
        "                if not (0 <= a < len(ctx) and 1 <= b <= len(ctx) and b > a): continue\n",
        "                text = clean_span_text(ctx[a:b], lang) or ctx[a:b]\n",
        "                sc = float(np.max(score_vec))\n",
        "                if sc > best[0]: best = (sc, text)\n",
        "        if best[1] == '':\n",
        "            best = (best[0], clean_span_text(ctx[:1], lang) or ctx[:1])\n",
        "        rows.append((qid, best[1]))\n",
        "    sub = pd.DataFrame(rows, columns=['id','PredictionString'])\n",
        "    empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "    mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "    print(f'Token-DP 512: Empties={int(empties)}, mean_len={mean_len:.2f}, time={time.time()-t0:.2f}s')\n",
        "    return sub\n",
        "\n",
        "# Run three decoders\n",
        "subA = decode_charfusion_covnorm(s512, e512, eid_512, off_512, idx512, tag='A(512)')\n",
        "pathA = 'submission_covnorm_charfusion_512.csv'; subA.to_csv(pathA, index=False)\n",
        "subB = decode_charfusion_covnorm(s384, e384, eid_384, off_384, idx384, tag='B(384)')\n",
        "pathB = 'submission_covnorm_charfusion_384.csv'; subB.to_csv(pathB, index=False)\n",
        "subC = decode_token_dp_512()\n",
        "pathC = 'submission_tokendp_512.csv'; subC.to_csv(pathC, index=False)\n",
        "\n",
        "# Per-id consensus over A/B/C: avg word-Jaccard; tie-break by length proximity to lang prior\n",
        "def words(s):\n",
        "    return [w for w in str(s).strip().split() if len(w)>0]\n",
        "def jaccard(a, b):\n",
        "    wa, wb = set(words(a)), set(words(b))\n",
        "    if not wa and not wb: return 1.0\n",
        "    if not wa or not wb: return 0.0\n",
        "    inter = len(wa & wb); uni = len(wa | wb)\n",
        "    return inter/uni if uni>0 else 0.0\n",
        "def len_prior_dist(s, lang):\n",
        "    L = max(1, len(str(s))); mu = priors.get(lang, {}).get('mu', 2.3)\n",
        "    return abs(math.log(L) - mu)\n",
        "\n",
        "mA = subA.set_index('id')['PredictionString'].to_dict()\n",
        "mB = subB.set_index('id')['PredictionString'].to_dict()\n",
        "mC = subC.set_index('id')['PredictionString'].to_dict()\n",
        "rows = []\n",
        "t0 = time.time()\n",
        "for qid in id_list:\n",
        "    lang = id2lang[qid]\n",
        "    cand = [mA[qid], mB[qid], mC[qid]]\n",
        "    best_i, best_s, best_d = 0, -1e18, 1e9\n",
        "    for i, ti in enumerate(cand):\n",
        "        s = 0.0; cnt = 0\n",
        "        for j, tj in enumerate(cand):\n",
        "            if i==j: continue\n",
        "            s += jaccard(ti, tj); cnt += 1\n",
        "        avg_j = s / max(1, cnt)\n",
        "        d = len_prior_dist(ti, lang)\n",
        "        # tie-break: prefer closer to length prior if within 0.01\n",
        "        if (avg_j > best_s + 1e-12) or (abs(avg_j - best_s) <= 0.01 and d < best_d):\n",
        "            best_s, best_i, best_d = avg_j, i, d\n",
        "    rows.append((qid, cand[best_i]))\n",
        "sub = pd.DataFrame(rows, columns=['id','PredictionString'])\n",
        "empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "print(f'Consensus A/B/C built. Empties={int(empties)}, mean_len={mean_len:.2f}')\n",
        "out_path = 'submission_consensus_covnorm_tokendp.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "pd.read_csv(out_path).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_path)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512 single-seed logits: (1401, 512) mapping 1401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "384 logits: (1921, 384) mapping 1921\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cov-norm char-fusion A(512): Empties=0, mean_len=7.57, time=5.25s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cov-norm char-fusion B(384): Empties=0, mean_len=7.62, time=5.39s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token-DP 512: Empties=0, mean_len=10.71, time=50.22s\nConsensus A/B/C built. Empties=0, mean_len=8.65\nsubmission.csv updated -> submission_consensus_covnorm_tokendp.csv\n"
          ]
        }
      ]
    },
    {
      "id": "fa35e8c3-090f-4994-a159-acbedf697b46",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Coverage-normalized decoders with Lmax+4 hedge and new consensus\n",
        "import numpy as np, pandas as pd, math, json, time, re, unicodedata as ud\n",
        "from pathlib import Path\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id_list = test['id'].tolist()\n",
        "id2lang = dict(zip(test['id'], test['language']))\n",
        "id2ctx = dict(zip(test['id'], test['context'].astype(str)))\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "def to_pair(t):\n",
        "    try:\n",
        "        if t is None: return (0,0)\n",
        "        if isinstance(t,(list,tuple,np.ndarray)) and len(t)>=2:\n",
        "            a = int(t[0]) if t[0] is not None else 0\n",
        "            b = int(t[1]) if t[1] is not None else 0\n",
        "            return (a,b)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return (0,0)\n",
        "\n",
        "def softmax(x):\n",
        "    x = np.asarray(x, dtype=np.float64)\n",
        "    m = np.max(x) if x.size else 0.0\n",
        "    e = np.exp(x - m)\n",
        "    s = e.sum()\n",
        "    return e / s if s > 0 else np.zeros_like(x, dtype=np.float64)\n",
        "\n",
        "ZW_CHARS = {'\\u200B','\\u200C','\\u200D','\\u2060','\\ufeff'}\n",
        "NBSP_SET = {'\\u00A0','\\u2002','\\u2003','\\u2004','\\u2005','\\u2006','\\u2007','\\u2008','\\u2009','\\u200A'}\n",
        "HI_VIRAMA='\\u094D'; TA_PULLI='\\u0BCD'; DANDA='\\u0964'\n",
        "def clean_span_text(s: str, lang: str) -> str:\n",
        "    if not s: return ''\n",
        "    s = ''.join(ch for ch in s if ch not in ZW_CHARS)\n",
        "    s = ''.join(' ' if ch in NBSP_SET else ch for ch in s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    s = re.sub(r'(?<=\\d)[\\s,._-](?=\\d)', '', s)\n",
        "    if s:\n",
        "        last = s[-1]\n",
        "        if ud.category(last) == 'Mn' or last in (HI_VIRAMA, TA_PULLI): s = s[:-1]\n",
        "    if lang == 'hindi':\n",
        "        s = s.replace(DANDA+DANDA, DANDA)\n",
        "        if s.endswith(DANDA): s = s[:-1].rstrip()\n",
        "    return s\n",
        "\n",
        "def load_npz_logits(npz_path):\n",
        "    arr = np.load(npz_path, allow_pickle=True)\n",
        "    keys = list(arr.keys())\n",
        "    if 'start' in keys and 'end' in keys: return arr['start'], arr['end']\n",
        "    for sk in ['start_logits','test_start_logits','start_logits_avg']:\n",
        "        for ek in ['end_logits','test_end_logits','end_logits_avg']:\n",
        "            if sk in keys and ek in keys: return arr[sk], arr[ek]\n",
        "    raise ValueError(f'Unknown keys in {npz_path}: {keys}')\n",
        "\n",
        "# Load 512 single-seed and 384 artifacts\n",
        "s512, e512 = load_npz_logits('xlmr_large_512_test_avg.npz')\n",
        "eid_512 = json.loads(Path('xlmr_large_512_test_logits/test_example_id.json').read_text())\n",
        "off_512 = np.load('xlmr_large_512_test_logits/test_offset_mapping.npy', allow_pickle=True)\n",
        "s384, e384 = load_npz_logits('xlmr_large_test_avg.npz')\n",
        "eid_384 = json.loads(Path('xlmr_large_test_logits/test_example_id.json').read_text())\n",
        "off_384 = np.load('xlmr_large_test_logits/test_offset_mapping.npy', allow_pickle=True)\n",
        "\n",
        "def build_index(eids):\n",
        "    m = {}\n",
        "    for i, q in enumerate(eids): m.setdefault(q, []).append(i)\n",
        "    return m\n",
        "idx512 = build_index(eid_512)\n",
        "idx384 = build_index(eid_384)\n",
        "\n",
        "def decode_charfusion_covnorm_plus4(start_logits, end_logits, offs_all, idx_map, tag: str):\n",
        "    rows = []; t0 = time.time()\n",
        "    for qid in id_list:\n",
        "        ctx = id2ctx[qid]; lang = id2lang[qid]\n",
        "        Lmax = (50 if lang=='hindi' else 60) + 4\n",
        "        K = (200 if lang=='hindi' else 240) + 20\n",
        "        S = np.zeros(len(ctx), dtype=np.float64)\n",
        "        E = np.zeros(len(ctx), dtype=np.float64)\n",
        "        CS = np.zeros(len(ctx), dtype=np.float64)\n",
        "        CE = np.zeros(len(ctx), dtype=np.float64)\n",
        "        for fi in idx_map.get(qid, []):\n",
        "            offs = offs_all[fi]\n",
        "            M = len(offs) if hasattr(offs,'__len__') else start_logits.shape[1]\n",
        "            ps = softmax(start_logits[fi][:M])\n",
        "            pe = softmax(end_logits[fi][:M])\n",
        "            for ti in range(M):\n",
        "                a,b = to_pair(offs[ti])\n",
        "                if b > a and 0 <= a < len(ctx) and 1 <= b <= len(ctx):\n",
        "                    S[a:b] += ps[ti]; CS[a:b] += 1.0\n",
        "                    E[a:b] += pe[ti]; CE[a:b] += 1.0\n",
        "        CS = np.maximum(CS, 1.0); CE = np.maximum(CE, 1.0)\n",
        "        S /= CS; E /= CE\n",
        "        if len(S) == 0:\n",
        "            rows.append((qid, ctx[:1] if len(ctx)>0 else '')); continue\n",
        "        starts = np.argsort(S)[::-1][:K]\n",
        "        best = (-1e18, 0, max(0, min(Lmax, len(ctx))-1))\n",
        "        for si in starts:\n",
        "            end_lo = si; end_hi = min(len(ctx)-1, si + Lmax - 1)\n",
        "            if end_hi < end_lo: continue\n",
        "            seg = E[end_lo:end_hi+1]\n",
        "            ej = end_lo + int(np.argmax(seg))\n",
        "            score = float(S[si] + E[ej])\n",
        "            if score > best[0]: best = (score, si, ej)\n",
        "        _, a, b = best\n",
        "        text = clean_span_text(ctx[a:b+1], lang)\n",
        "        if not text:\n",
        "            b2 = min(len(ctx), a+1)\n",
        "            text = clean_span_text(ctx[a:b2], lang) or ctx[a:b2]\n",
        "        rows.append((qid, text))\n",
        "    sub = pd.DataFrame(rows, columns=['id','PredictionString'])\n",
        "    empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "    mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "    print(f'Cov-norm+4 {tag}: Empties={int(empties)}, mean_len={mean_len:.2f}, time={time.time()-t0:.2f}s')\n",
        "    return sub\n",
        "\n",
        "def decode_tokendp_512_plus4():\n",
        "    rows = []; t0 = time.time()\n",
        "    for qid in id_list:\n",
        "        ctx = id2ctx[qid]; lang = id2lang[qid]\n",
        "        Lmax = (50 if lang=='hindi' else 60) + 4\n",
        "        K = (200 if lang=='hindi' else 240) + 20\n",
        "        best = (-1e18, '')\n",
        "        for fi in idx512.get(qid, []):\n",
        "            offs = off_512[fi]\n",
        "            M = len(offs) if hasattr(offs,'__len__') else s512.shape[1]\n",
        "            ps = softmax(s512[fi][:M]); pe = softmax(e512[fi][:M])\n",
        "            valid = []\n",
        "            for ti in range(M):\n",
        "                a,b = to_pair(offs[ti])\n",
        "                if b > a and 0 <= a < len(ctx) and 1 <= b <= len(ctx): valid.append(ti)\n",
        "            if not valid: continue\n",
        "            valid = np.array(valid, dtype=np.int32)\n",
        "            s_top = valid[np.argsort(ps[valid])[::-1][:K]]\n",
        "            for si in s_top:\n",
        "                a0, _ = to_pair(offs[si])\n",
        "                cand_e = []\n",
        "                for ei in range(si, M):\n",
        "                    a2,b2 = to_pair(offs[ei])\n",
        "                    if not (b2 > a2 and 0 <= a2 < len(ctx) and 1 <= b2 <= len(ctx)): continue\n",
        "                    clen = b2 - a0\n",
        "                    if clen <= 0 or clen > Lmax: continue\n",
        "                    cand_e.append(ei)\n",
        "                if not cand_e: continue\n",
        "                cand_e = np.array(cand_e, dtype=np.int32)\n",
        "                score_vec = np.log(np.maximum(ps[si], 1e-15)) + np.log(np.maximum(pe[cand_e], 1e-15))\n",
        "                ei = int(cand_e[int(np.argmax(score_vec))])\n",
        "                a,b = to_pair(offs[si])[0], to_pair(offs[ei])[1]\n",
        "                if not (0 <= a < len(ctx) and 1 <= b <= len(ctx) and b > a): continue\n",
        "                text = clean_span_text(ctx[a:b], lang) or ctx[a:b]\n",
        "                sc = float(np.max(score_vec))\n",
        "                if sc > best[0]: best = (sc, text)\n",
        "        if best[1] == '':\n",
        "            best = (best[0], clean_span_text(ctx[:1], lang) or ctx[:1])\n",
        "        rows.append((qid, best[1]))\n",
        "    sub = pd.DataFrame(rows, columns=['id','PredictionString'])\n",
        "    empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "    mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "    print(f'Token-DP+4 512: Empties={int(empties)}, mean_len={mean_len:.2f}, time={time.time()-t0:.2f}s')\n",
        "    return sub\n",
        "\n",
        "# Run decoders with Lmax+4\n",
        "subA = decode_charfusion_covnorm_plus4(s512, e512, off_512, idx512, tag='A(512)')\n",
        "pathA = 'submission_covnorm_charfusion_512_LmaxPlus4.csv'; subA.to_csv(pathA, index=False)\n",
        "subB = decode_charfusion_covnorm_plus4(s384, e384, off_384, idx384, tag='B(384)')\n",
        "pathB = 'submission_covnorm_charfusion_384_LmaxPlus4.csv'; subB.to_csv(pathB, index=False)\n",
        "subC = decode_tokendp_512_plus4()\n",
        "pathC = 'submission_tokendp_512_LmaxPlus4.csv'; subC.to_csv(pathC, index=False)\n",
        "\n",
        "# Consensus A/B/C (avg word Jaccard, tie-break by length prior proximity)\n",
        "def words(s):\n",
        "    return [w for w in str(s).strip().split() if len(w)>0]\n",
        "def jaccard(a, b):\n",
        "    wa, wb = set(words(a)), set(words(b))\n",
        "    if not wa and not wb: return 1.0\n",
        "    if not wa or not wb: return 0.0\n",
        "    inter = len(wa & wb); uni = len(wa | wb)\n",
        "    return inter/uni if uni>0 else 0.0\n",
        "def len_prior_dist(s, lang):\n",
        "    L = max(1, len(str(s))); mu = priors.get(lang, {}).get('mu', 2.3)\n",
        "    return abs(math.log(L) - mu)\n",
        "\n",
        "mA = subA.set_index('id')['PredictionString'].to_dict()\n",
        "mB = subB.set_index('id')['PredictionString'].to_dict()\n",
        "mC = subC.set_index('id')['PredictionString'].to_dict()\n",
        "rows = []\n",
        "for qid in id_list:\n",
        "    lang = id2lang[qid]\n",
        "    cand = [mA[qid], mB[qid], mC[qid]]\n",
        "    best_i, best_s, best_d = 0, -1e18, 1e9\n",
        "    for i, ti in enumerate(cand):\n",
        "        s = 0.0; cnt = 0\n",
        "        for j, tj in enumerate(cand):\n",
        "            if i==j: continue\n",
        "            s += jaccard(ti, tj); cnt += 1\n",
        "        avg_j = s / max(1, cnt)\n",
        "        d = len_prior_dist(ti, lang)\n",
        "        if (avg_j > best_s + 1e-12) or (abs(avg_j - best_s) <= 0.01 and d < best_d):\n",
        "            best_s, best_i, best_d = avg_j, i, d\n",
        "    rows.append((qid, cand[best_i]))\n",
        "sub = pd.DataFrame(rows, columns=['id','PredictionString'])\n",
        "empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "print(f'Consensus A/B/C (Lmax+4) built. Empties={int(empties)}, mean_len={mean_len:.2f}')\n",
        "out_path = 'submission_consensus_covnorm_tokendp_LmaxPlus4.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "pd.read_csv(out_path).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_path)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cov-norm+4 A(512): Empties=0, mean_len=7.57, time=5.12s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cov-norm+4 B(384): Empties=0, mean_len=7.62, time=5.40s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token-DP+4 512: Empties=0, mean_len=10.71, time=55.48s\nConsensus A/B/C (Lmax+4) built. Empties=0, mean_len=8.65\nsubmission.csv updated -> submission_consensus_covnorm_tokendp_LmaxPlus4.csv\n"
          ]
        }
      ]
    },
    {
      "id": "14f4a400-eacf-476a-a658-cb1e98c67d16",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Meta-consensus per expert: per-language candidate pools, majority override, mean Jaccard + len prior\n",
        "import pandas as pd, numpy as np, math, time\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id_list = test['id'].tolist()\n",
        "lang_map = dict(zip(test['id'], test['language']))\n",
        "\n",
        "# Candidates (healthy only). Hindi uses all 6; Tamil restrict to 4 (no 384-only).\n",
        "cand_hi_files = [\n",
        "    'submission_3seed512_single_charfusion_snap_lambda012_K200_240_Lmax50_60_delta002.csv',\n",
        "    'submission_charfusion_512_lambda015.csv',\n",
        "    'submission_tokenselect_512single_or_384_lambda012.csv',\n",
        "    'submission_384only_charfusion_lambda015.csv',\n",
        "    'submission_primary_multistream_snap_hi85_10_5_ta97_3_0_lambda015.csv',\n",
        "    'submission_third_multistream_drop_muril_snap_lambda010.csv',\n",
        "]\n",
        "cand_ta_files = [\n",
        "    'submission_3seed512_single_charfusion_snap_lambda012_K200_240_Lmax50_60_delta002.csv',\n",
        "    'submission_charfusion_512_lambda015.csv',\n",
        "    'submission_tokenselect_512single_or_384_lambda012.csv',\n",
        "    'submission_third_multistream_drop_muril_snap_lambda010.csv',\n",
        "]\n",
        "# Optional: include token-DP 512 for Hindi only if present\n",
        "optional_hi = ['submission_tokendp_512.csv']\n",
        "\n",
        "def load_cands(paths):\n",
        "    out = []\n",
        "    for f in paths:\n",
        "        try:\n",
        "            df = pd.read_csv(f)\n",
        "            if set(df.columns) >= {'id','PredictionString'} and len(df)==len(test):\n",
        "                out.append((f, df.set_index('id')['PredictionString'].astype(str).to_dict()))\n",
        "                print('Loaded:', f)\n",
        "            else:\n",
        "                print('Skip (shape/cols):', f)\n",
        "        except Exception as e:\n",
        "            print('Skip:', f, '->', e)\n",
        "    return out\n",
        "\n",
        "cands_hi = load_cands(cand_hi_files + [f for f in optional_hi if pd.Series([f]).apply(lambda x: os.path.exists(x) if 'os' in globals() else __import__('os').path.exists(x)).iloc[0]])\n",
        "cands_ta = load_cands(cand_ta_files)\n",
        "if len(cands_hi) < 4: cands_hi = load_cands(cand_hi_files)  # fallback if optional missing\n",
        "\n",
        "def words(s):\n",
        "    return [w for w in str(s).strip().split() if len(w)>0]\n",
        "def jaccard(a, b):\n",
        "    wa, wb = set(words(a)), set(words(b))\n",
        "    if not wa and not wb: return 1.0\n",
        "    if not wa or not wb: return 0.0\n",
        "    inter = len(wa & wb); uni = len(wa | wb)\n",
        "    return inter/uni if uni>0 else 0.0\n",
        "\n",
        "# Per-language log-normal priors\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "def len_prior_dist(s, lang):\n",
        "    L = max(1, len(str(s))); mu = priors.get(lang, {}).get('mu', 2.3)\n",
        "    return abs(math.log(L) - mu)\n",
        "\n",
        "from collections import Counter\n",
        "def majority_override(texts, min_votes=3):\n",
        "    cnt = Counter(texts)\n",
        "    text, votes = cnt.most_common(1)[0]\n",
        "    return (text if votes >= min_votes else None), votes\n",
        "\n",
        "# Length regularization weights\n",
        "lambda_hi = 0.02\n",
        "lambda_ta = 0.03\n",
        "\n",
        "rows = []\n",
        "t0 = time.time()\n",
        "for qid in id_list:\n",
        "    lang = lang_map[qid]\n",
        "    pool = cands_hi if lang=='hindi' else cands_ta\n",
        "    texts = [d[qid] for _, d in pool]\n",
        "    # Majority override first\n",
        "    maj, votes = majority_override(texts, min_votes=3)\n",
        "    if maj is not None:\n",
        "        rows.append((qid, maj));\n",
        "        continue\n",
        "    # Score by mean Jaccard; add small len prior; tie-break by closer to prior, then shorter within 0.01\n",
        "    best_i, best_s, best_d, best_L = -1, -1e18, 1e9, 10**9\n",
        "    lam = lambda_hi if lang=='hindi' else lambda_ta\n",
        "    for i, ti in enumerate(texts):\n",
        "        s = 0.0; cnt = 0\n",
        "        for j, tj in enumerate(texts):\n",
        "            if i==j: continue\n",
        "            s += jaccard(ti, tj); cnt += 1\n",
        "        avg_j = s / max(1, cnt)\n",
        "        dlen = len_prior_dist(ti, lang)\n",
        "        score = avg_j - lam * dlen  # equivalent to + lam * (-|logL-mu|)\n",
        "        L = len(str(ti))\n",
        "        if (score > best_s + 1e-12) or (abs(score - best_s) <= 0.01 and (dlen < best_d or (abs(dlen-best_d) <= 1e-6 and L < best_L))):\n",
        "            best_s, best_i, best_d, best_L = score, i, dlen, L\n",
        "    rows.append((qid, texts[best_i]))\n",
        "\n",
        "sub = pd.DataFrame(rows, columns=['id','PredictionString'])\n",
        "empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "print(f'Meta-consensus built. Empties={int(empties)}, mean_len={mean_len:.2f}, time={time.time()-t0:.2f}s')\n",
        "\n",
        "out_path = 'submission_meta_consensus_perlang_majority_lenhi002_lenta003_plusjacc.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "pd.read_csv(out_path).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_path)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: submission_3seed512_single_charfusion_snap_lambda012_K200_240_Lmax50_60_delta002.csv\nLoaded: submission_charfusion_512_lambda015.csv\nLoaded: submission_tokenselect_512single_or_384_lambda012.csv\nLoaded: submission_384only_charfusion_lambda015.csv\nLoaded: submission_primary_multistream_snap_hi85_10_5_ta97_3_0_lambda015.csv\nLoaded: submission_third_multistream_drop_muril_snap_lambda010.csv\nLoaded: submission_tokendp_512.csv\nLoaded: submission_3seed512_single_charfusion_snap_lambda012_K200_240_Lmax50_60_delta002.csv\nLoaded: submission_charfusion_512_lambda015.csv\nLoaded: submission_tokenselect_512single_or_384_lambda012.csv\nLoaded: submission_third_multistream_drop_muril_snap_lambda010.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-consensus built. Empties=0, mean_len=10.95, time=0.00s\nsubmission.csv updated -> submission_meta_consensus_perlang_majority_lenhi002_lenta003_plusjacc.csv\n"
          ]
        }
      ]
    },
    {
      "id": "ce57399a-987a-40e1-85a3-688c06e0f31a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Improved 512 single-seed token-DP (K=280, Lmax=54/64, ends shortlist=10); overwrite submission_tokendp_512.csv\n",
        "import numpy as np, pandas as pd, json, math, time, re, unicodedata as ud\n",
        "from pathlib import Path\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id_list = test['id'].tolist()\n",
        "id2lang = dict(zip(test['id'], test['language']))\n",
        "id2ctx = dict(zip(test['id'], test['context'].astype(str)))\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "def to_pair(t):\n",
        "    try:\n",
        "        if t is None: return (0,0)\n",
        "        if isinstance(t,(list,tuple,np.ndarray)) and len(t)>=2:\n",
        "            a = int(t[0]) if t[0] is not None else 0\n",
        "            b = int(t[1]) if t[1] is not None else 0\n",
        "            return (a,b)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return (0,0)\n",
        "\n",
        "def softmax(x):\n",
        "    x = np.asarray(x, dtype=np.float64)\n",
        "    m = np.max(x) if x.size else 0.0\n",
        "    e = np.exp(x - m)\n",
        "    s = e.sum()\n",
        "    return e / s if s > 0 else np.zeros_like(x, dtype=np.float64)\n",
        "\n",
        "ZW_CHARS = {'\\u200B','\\u200C','\\u200D','\\u2060','\\ufeff'}\n",
        "NBSP_SET = {'\\u00A0','\\u2002','\\u2003','\\u2004','\\u2005','\\u2006','\\u2007','\\u2008','\\u2009','\\u200A'}\n",
        "HI_VIRAMA='\\u094D'; TA_PULLI='\\u0BCD'; DANDA='\\u0964'\n",
        "def clean_span_text(s: str, lang: str) -> str:\n",
        "    if not s: return ''\n",
        "    s = ''.join(ch for ch in s if ch not in ZW_CHARS)\n",
        "    s = ''.join(' ' if ch in NBSP_SET else ch for ch in s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    s = re.sub(r'(?<=\\d)[\\s,._-](?=\\d)', '', s)\n",
        "    if s:\n",
        "        last = s[-1]\n",
        "        if ud.category(last) == 'Mn' or last in (HI_VIRAMA, TA_PULLI): s = s[:-1]\n",
        "    if lang == 'hindi':\n",
        "        s = s.replace(DANDA+DANDA, DANDA)\n",
        "        if s.endswith(DANDA): s = s[:-1].rstrip()\n",
        "    return s\n",
        "\n",
        "def log_normal_logpdf_len(char_len: int, mu: float, sigma: float) -> float:\n",
        "    Lc = max(1, int(char_len))\n",
        "    x = math.log(Lc)\n",
        "    return -0.5*((x-mu)/sigma)**2 - math.log(max(Lc,1e-6)) - math.log(sigma) - 0.5*math.log(2*math.pi)\n",
        "\n",
        "def load_npz_logits(npz_path):\n",
        "    arr = np.load(npz_path, allow_pickle=True)\n",
        "    keys = list(arr.keys())\n",
        "    if 'start' in keys and 'end' in keys: return arr['start'], arr['end']\n",
        "    for sk in ['start_logits','test_start_logits','start_logits_avg']:\n",
        "        for ek in ['end_logits','test_end_logits','end_logits_avg']:\n",
        "            if sk in keys and ek in keys: return arr[sk], arr[ek]\n",
        "    raise ValueError(f'Unknown keys in {npz_path}: {keys}')\n",
        "\n",
        "# Load 512 single-seed (alignment-safe)\n",
        "s512, e512 = load_npz_logits('xlmr_large_512_test_avg.npz')\n",
        "eid_512 = json.loads(Path('xlmr_large_512_test_logits/test_example_id.json').read_text())\n",
        "off_512 = np.load('xlmr_large_512_test_logits/test_offset_mapping.npy', allow_pickle=True)\n",
        "print('512 single-seed logits:', s512.shape, 'mapping', len(eid_512))\n",
        "\n",
        "def build_index(eids):\n",
        "    m = {}\n",
        "    for i, q in enumerate(eids): m.setdefault(q, []).append(i)\n",
        "    return m\n",
        "idx512 = build_index(eid_512)\n",
        "\n",
        "def decode_tokendp_512_fixed():\n",
        "    rows = []; t0 = time.time()\n",
        "    # Expert tweak: ends shortlist 12 (was 10); K bumped by +20 (hi 260, ta 300); keep Lmax 54/64 and lambda_len 0.12\n",
        "    K_hi, K_ta = 260, 300\n",
        "    Lmax_hi, Lmax_ta = 54, 64\n",
        "    shortlist = 12\n",
        "    lam = 0.12; clip_prior = (-0.8, 0.0)\n",
        "    for qid in id_list:\n",
        "        ctx = id2ctx[qid]; lang = id2lang[qid]\n",
        "        mu = priors[lang]['mu']; sigma = priors[lang]['sigma']\n",
        "        Lmax = Lmax_hi if lang=='hindi' else Lmax_ta\n",
        "        K = K_hi if lang=='hindi' else K_ta\n",
        "        best = (-1e18, '')\n",
        "        for fi in idx512.get(qid, []):\n",
        "            offs = off_512[fi]\n",
        "            M = len(offs) if hasattr(offs,'__len__') else s512.shape[1]\n",
        "            ps = softmax(s512[fi][:M]); pe = softmax(e512[fi][:M])\n",
        "            valid = []\n",
        "            for ti in range(M):\n",
        "                a,b = to_pair(offs[ti])\n",
        "                if b > a and 0 <= a < len(ctx) and 1 <= b <= len(ctx): valid.append(ti)\n",
        "            if not valid: continue\n",
        "            valid = np.array(valid, dtype=np.int32)\n",
        "            s_top = valid[np.argsort(ps[valid])[::-1][:K]]\n",
        "            for si in s_top:\n",
        "                a0, _ = to_pair(offs[si])\n",
        "                cand_e = []\n",
        "                for ei in range(si, M):\n",
        "                    a2,b2 = to_pair(offs[ei])\n",
        "                    if not (b2 > a2 and 0 <= a2 < len(ctx) and 1 <= b2 <= len(ctx)): continue\n",
        "                    clen = b2 - a0\n",
        "                    if clen <= 0 or clen > Lmax: continue\n",
        "                    cand_e.append(ei)\n",
        "                if not cand_e: continue\n",
        "                cand_e = np.array(cand_e, dtype=np.int32)\n",
        "                seg = pe[cand_e]\n",
        "                top_e = cand_e[np.argsort(seg)[::-1][:shortlist]]\n",
        "                for ei in top_e:\n",
        "                    a,b = to_pair(offs[si])[0], to_pair(offs[ei])[1]\n",
        "                    if not (0 <= a < len(ctx) and 1 <= b <= len(ctx) and b > a): continue\n",
        "                    raw = float(np.log(max(ps[si],1e-15)) + np.log(max(pe[ei],1e-15)))\n",
        "                    clen = b - a\n",
        "                    lp = log_normal_logpdf_len(clen, mu, sigma)\n",
        "                    lp = max(clip_prior[0], min(clip_prior[1], lp))\n",
        "                    raw += lam * lp\n",
        "                    if raw > best[0]:\n",
        "                        best = (raw, clean_span_text(ctx[a:b], lang) or ctx[a:b])\n",
        "        if best[1] == '':\n",
        "            best = (best[0], clean_span_text(ctx[:1], lang) or ctx[:1])\n",
        "        rows.append((qid, best[1]))\n",
        "    sub = pd.DataFrame(rows, columns=['id','PredictionString'])\n",
        "    empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "    mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "    print(f'Token-DP 512 fixed: Empties={int(empties)}, mean_len={mean_len:.2f}, time={time.time()-t0:.2f}s')\n",
        "    return sub\n",
        "\n",
        "sub_fix = decode_tokendp_512_fixed()\n",
        "out_path = 'submission_tokendp_512.csv'  # overwrite existing to be picked by meta-consensus\n",
        "sub_fix.to_csv(out_path, index=False)\n",
        "pd.read_csv(out_path).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_path)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512 single-seed logits: (1401, 512) mapping 1401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token-DP 512 fixed: Empties=0, mean_len=10.71, time=77.24s\nsubmission.csv updated -> submission_tokendp_512.csv\n"
          ]
        }
      ]
    },
    {
      "id": "5f615919-b9b2-48e7-9a47-60f73d0d5e3c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Meta-consensus with normalized scoring (punct/quote-insensitive) and per-language pools\n",
        "import pandas as pd, numpy as np, math, time, re\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id_list = test['id'].tolist()\n",
        "lang_map = dict(zip(test['id'], test['language']))\n",
        "\n",
        "# Candidate pools (same as expert meta-consensus):\n",
        "cand_hi_files = [\n",
        "    'submission_3seed512_single_charfusion_snap_lambda012_K200_240_Lmax50_60_delta002.csv',\n",
        "    'submission_charfusion_512_lambda015.csv',\n",
        "    'submission_tokenselect_512single_or_384_lambda012.csv',\n",
        "    'submission_384only_charfusion_lambda015.csv',\n",
        "    'submission_primary_multistream_snap_hi85_10_5_ta97_3_0_lambda015.csv',\n",
        "    'submission_third_multistream_drop_muril_snap_lambda010.csv',\n",
        "]\n",
        "cand_ta_files = [\n",
        "    'submission_3seed512_single_charfusion_snap_lambda012_K200_240_Lmax50_60_delta002.csv',\n",
        "    'submission_charfusion_512_lambda015.csv',\n",
        "    'submission_tokenselect_512single_or_384_lambda012.csv',\n",
        "    'submission_third_multistream_drop_muril_snap_lambda010.csv',\n",
        "]\n",
        "optional_hi = ['submission_tokendp_512.csv']\n",
        "\n",
        "def load_cands(paths):\n",
        "    out = []\n",
        "    for f in paths:\n",
        "        try:\n",
        "            df = pd.read_csv(f)\n",
        "            if set(df.columns) >= {'id','PredictionString'} and len(df)==len(test):\n",
        "                out.append((f, df.set_index('id')['PredictionString'].astype(str).to_dict()))\n",
        "                print('Loaded:', f)\n",
        "            else:\n",
        "                print('Skip (shape/cols):', f)\n",
        "        except Exception as e:\n",
        "            print('Skip:', f, '->', e)\n",
        "    return out\n",
        "\n",
        "import os\n",
        "cands_hi = load_cands(cand_hi_files + [f for f in optional_hi if os.path.exists(f)])\n",
        "cands_ta = load_cands(cand_ta_files)\n",
        "if len(cands_hi) < 4: cands_hi = load_cands(cand_hi_files)\n",
        "\n",
        "# Normalization for scoring only (do not change final output text)\n",
        "PUNCT = set(list('.,!?;:\"\\'()[]{}\u201c\u201d\u2018\u2019\u00ab\u00bb'))\n",
        "def norm_text(s: str) -> str:\n",
        "    s = str(s).strip()\n",
        "    # trim surrounding quotes/brackets\n",
        "    while len(s) >= 2 and ((s[0], s[-1]) in [('\"','\"'),(\"'\",\"'\"),('\u201c','\u201d'),('\u2018','\u2019'),('(',')'),('[',']'),('{','}'),('\u00ab','\u00bb')]):\n",
        "        s = s[1:-1].strip()\n",
        "    # collapse spaces\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    # strip leading/trailing punctuation\n",
        "    s = s.strip(''.join(PUNCT))\n",
        "    return s.strip()\n",
        "\n",
        "def words_norm(s):\n",
        "    s = norm_text(s)\n",
        "    # split on whitespace; strip punctuation from tokens\n",
        "    toks = []\n",
        "    for w in s.split():\n",
        "        w = w.strip(''.join(PUNCT))\n",
        "        if w: toks.append(w)\n",
        "    return toks\n",
        "\n",
        "def jaccard_norm(a, b):\n",
        "    wa, wb = set(words_norm(a)), set(words_norm(b))\n",
        "    if not wa and not wb: return 1.0\n",
        "    if not wa or not wb: return 0.0\n",
        "    inter = len(wa & wb); uni = len(wa | wb)\n",
        "    return inter/uni if uni>0 else 0.0\n",
        "\n",
        "# Per-language log-normal priors (for small length regularization in log-space)\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "def len_prior_dist(s, lang):\n",
        "    L = max(1, len(str(s))); mu = priors.get(lang, {}).get('mu', 2.3)\n",
        "    return abs(math.log(L) - mu)\n",
        "\n",
        "from collections import Counter\n",
        "def majority_override_norm(texts, min_votes=3):\n",
        "    # vote on normalized form\n",
        "    norm_map = [norm_text(t) for t in texts]\n",
        "    cnt = Counter(norm_map)\n",
        "    top_text, votes = cnt.most_common(1)[0]\n",
        "    if votes >= min_votes:\n",
        "        # return the first candidate whose normalized form matches the winner\n",
        "        for orig, nt in zip(texts, norm_map):\n",
        "            if nt == top_text:\n",
        "                return orig, votes\n",
        "    return None, votes\n",
        "\n",
        "# Length regularization weights\n",
        "lambda_hi = 0.02\n",
        "lambda_ta = 0.03\n",
        "\n",
        "rows = []\n",
        "t0 = time.time()\n",
        "for qid in id_list:\n",
        "    lang = lang_map[qid]\n",
        "    pool = cands_hi if lang=='hindi' else cands_ta\n",
        "    texts = [d[qid] for _, d in pool]\n",
        "    # Majority override on normalized strings\n",
        "    maj, votes = majority_override_norm(texts, min_votes=3)\n",
        "    if maj is not None:\n",
        "        rows.append((qid, maj))\n",
        "        continue\n",
        "    # Score by mean normalized Jaccard; add small per-language len prior; tie-break by closer to prior, then shorter within 0.01\n",
        "    best_i, best_s, best_d, best_L = -1, -1e18, 1e9, 10**9\n",
        "    lam = lambda_hi if lang=='hindi' else lambda_ta\n",
        "    for i, ti in enumerate(texts):\n",
        "        s = 0.0; cnt = 0\n",
        "        for j, tj in enumerate(texts):\n",
        "            if i==j: continue\n",
        "            s += jaccard_norm(ti, tj); cnt += 1\n",
        "        avg_j = s / max(1, cnt)\n",
        "        dlen = len_prior_dist(ti, lang)\n",
        "        score = avg_j - lam * dlen\n",
        "        L = len(str(ti))\n",
        "        if (score > best_s + 1e-12) or (abs(score - best_s) <= 0.01 and (dlen < best_d or (abs(dlen-best_d) <= 1e-6 and L < best_L))):\n",
        "            best_s, best_i, best_d, best_L = score, i, dlen, L\n",
        "    rows.append((qid, texts[best_i]))\n",
        "\n",
        "sub = pd.DataFrame(rows, columns=['id','PredictionString'])\n",
        "empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "print(f'Meta-consensus (normalized scoring) built. Empties={int(empties)}, mean_len={mean_len:.2f}, time={time.time()-t0:.2f}s')\n",
        "\n",
        "out_path = 'submission_meta_consensus_normscore_perlang_majority_lenhi002_lenta003.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "pd.read_csv(out_path).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_path)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: submission_3seed512_single_charfusion_snap_lambda012_K200_240_Lmax50_60_delta002.csv\nLoaded: submission_charfusion_512_lambda015.csv\nLoaded: submission_tokenselect_512single_or_384_lambda012.csv\nLoaded: submission_384only_charfusion_lambda015.csv\nLoaded: submission_primary_multistream_snap_hi85_10_5_ta97_3_0_lambda015.csv\nLoaded: submission_third_multistream_drop_muril_snap_lambda010.csv\nLoaded: submission_tokendp_512.csv\nLoaded: submission_3seed512_single_charfusion_snap_lambda012_K200_240_Lmax50_60_delta002.csv\nLoaded: submission_charfusion_512_lambda015.csv\nLoaded: submission_tokenselect_512single_or_384_lambda012.csv\nLoaded: submission_third_multistream_drop_muril_snap_lambda010.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-consensus (normalized scoring) built. Empties=0, mean_len=10.96, time=0.01s\nsubmission.csv updated -> submission_meta_consensus_normscore_perlang_majority_lenhi002_lenta003.csv\n"
          ]
        }
      ]
    },
    {
      "id": "e9f9f6a5-8d65-4638-bf12-dcb67eceb17c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Switch submission.csv to 512 single-seed token-DP variant and print diagnostics\n",
        "import pandas as pd, os, time\n",
        "path = 'submission_tokendp_512.csv'\n",
        "assert os.path.exists(path), f'Missing {path}'\n",
        "sub = pd.read_csv(path)\n",
        "empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', path)\n",
        "print('Diagnostics: empties=', int(empties), 'mean_len=', round(float(mean_len), 2))\n",
        "print('mtime(submission.csv)=', time.ctime(os.path.getmtime('submission.csv')))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv updated -> submission_tokendp_512.csv\nDiagnostics: empties= 0 mean_len= 10.71\nmtime(submission.csv)= Thu Sep 25 12:42:02 2025\n"
          ]
        }
      ]
    },
    {
      "id": "adf74419-4ecc-41c3-ac19-bf6db9c2c50a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Alignment-safe 512 single-seed char-fusion + meta-consensus over alignment-safe candidates\n",
        "import numpy as np, pandas as pd, json, time, math, re, unicodedata as ud\n",
        "from pathlib import Path\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id_list = test['id'].tolist()\n",
        "id2lang = dict(zip(test['id'], test['language']))\n",
        "id2ctx = dict(zip(test['id'], test['context'].astype(str)))\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "ZW_CHARS = {'\\u200B','\\u200C','\\u200D','\\u2060','\\ufeff'}\n",
        "NBSP_SET = {'\\u00A0','\\u2002','\\u2003','\\u2004','\\u2005','\\u2006','\\u2007','\\u2008','\\u2009','\\u200A'}\n",
        "HI_VIRAMA='\\u094D'; TA_PULLI='\\u0BCD'; DANDA='\\u0964'\n",
        "def clean_span_text(s: str, lang: str) -> str:\n",
        "    if not s: return ''\n",
        "    s = ''.join(ch for ch in s if ch not in ZW_CHARS)\n",
        "    s = ''.join(' ' if ch in NBSP_SET else ch for ch in s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    s = re.sub(r'(?<=\\d)[\\s,._-](?=\\d)', '', s)\n",
        "    if s:\n",
        "        last = s[-1]\n",
        "        if ud.category(last) == 'Mn' or last in (HI_VIRAMA, TA_PULLI): s = s[:-1]\n",
        "    if lang == 'hindi':\n",
        "        s = s.replace(DANDA+DANDA, DANDA)\n",
        "        if s.endswith(DANDA): s = s[:-1].rstrip()\n",
        "    return s\n",
        "\n",
        "def to_pair(t):\n",
        "    try:\n",
        "        if t is None: return (0,0)\n",
        "        if isinstance(t,(list,tuple,np.ndarray)) and len(t)>=2:\n",
        "            a = int(t[0]) if t[0] is not None else 0\n",
        "            b = int(t[1]) if t[1] is not None else 0\n",
        "            return (a,b)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return (0,0)\n",
        "\n",
        "def load_npz_logits(npz_path):\n",
        "    arr = np.load(npz_path, allow_pickle=True)\n",
        "    keys = list(arr.keys())\n",
        "    if 'start' in keys and 'end' in keys: return arr['start'], arr['end']\n",
        "    for sk in ['start_logits','test_start_logits','start_logits_avg']:\n",
        "        for ek in ['end_logits','test_end_logits','end_logits_avg']:\n",
        "            if sk in keys and ek in keys: return arr[sk], arr[ek]\n",
        "    raise ValueError(f'Unknown keys in {npz_path}: {keys}')\n",
        "\n",
        "# Load alignment-safe 512 single-seed logits + mapping\n",
        "s512, e512 = load_npz_logits('xlmr_large_512_test_avg.npz')\n",
        "eid_512 = json.loads(Path('xlmr_large_512_test_logits/test_example_id.json').read_text())\n",
        "off_512 = np.load('xlmr_large_512_test_logits/test_offset_mapping.npy', allow_pickle=True)\n",
        "print('512 single-seed:', s512.shape, 'mapping', len(eid_512))\n",
        "\n",
        "def build_index(eids):\n",
        "    m = {}\n",
        "    for i, q in enumerate(eids): m.setdefault(q, []).append(i)\n",
        "    return m\n",
        "idx512 = build_index(eid_512)\n",
        "\n",
        "def maxpool1d(x):\n",
        "    if len(x) == 0: return x\n",
        "    y = x.copy()\n",
        "    if len(x) == 1: return y\n",
        "    y[0] = max(x[0], x[1])\n",
        "    for i in range(1, len(x)-1): y[i] = max(x[i-1], x[i], x[i+1])\n",
        "    y[-1] = max(x[-2], x[-1])\n",
        "    return y\n",
        "\n",
        "def log_normal_logpdf_len(char_len: int, mu: float, sigma: float) -> float:\n",
        "    Lc = max(1, int(char_len))\n",
        "    x = math.log(Lc)\n",
        "    return -0.5*((x-mu)/sigma)**2 - math.log(max(Lc,1e-6)) - math.log(sigma) - 0.5*math.log(2*math.pi)\n",
        "\n",
        "def decode_charfusion_512_single(lambda_len=0.12, clip_prior=(-0.8,0.0), K_hi=200, K_ta=240, Lmax_hi=50, Lmax_ta=60):\n",
        "    rows = []; t0 = time.time()\n",
        "    for qid in id_list:\n",
        "        lang = id2lang[qid]; ctx = id2ctx[qid]\n",
        "        mu = priors[lang]['mu']; sigma = priors[lang]['sigma']\n",
        "        Lmax = Lmax_hi if lang=='hindi' else Lmax_ta\n",
        "        K = K_hi if lang=='hindi' else K_ta\n",
        "        S = np.zeros(len(ctx), dtype=np.float32)\n",
        "        E = np.zeros(len(ctx), dtype=np.float32)\n",
        "        for fi in idx512.get(qid, []):\n",
        "            offs_raw = off_512[fi]\n",
        "            M = len(offs_raw) if hasattr(offs_raw,'__len__') else s512.shape[1]\n",
        "            s_log = s512[fi][:M]; e_log = e512[fi][:M]\n",
        "            for ti in range(M):\n",
        "                a,b = to_pair(offs_raw[ti])\n",
        "                if b > a and 0 <= a < len(ctx) and 1 <= b <= len(ctx):\n",
        "                    S[a] += s_log[ti]\n",
        "                    E[b-1] += e_log[ti]\n",
        "        S = maxpool1d(S); E = maxpool1d(E)\n",
        "        if len(S) == 0:\n",
        "            rows.append((qid, ctx[:1] if len(ctx)>0 else '')); continue\n",
        "        starts = np.argsort(S)[::-1][:K]\n",
        "        best_score = -1e18; best_span = (0, max(0, min(Lmax, len(ctx))-1))\n",
        "        for si in starts:\n",
        "            end_lo = si; end_hi = min(len(ctx)-1, si + Lmax - 1)\n",
        "            if end_hi < end_lo: continue\n",
        "            seg = E[end_lo:end_hi+1]\n",
        "            ej = end_lo + int(np.argmax(seg))\n",
        "            raw = float(S[si] + E[ej])\n",
        "            clen = ej - si + 1\n",
        "            lp = log_normal_logpdf_len(clen, mu, sigma)\n",
        "            lp = max(clip_prior[0], min(clip_prior[1], lp))\n",
        "            raw += lambda_len * lp\n",
        "            if raw > best_score:\n",
        "                best_score = raw; best_span = (si, ej)\n",
        "        a,b = best_span\n",
        "        text = clean_span_text(ctx[a:b+1], lang)\n",
        "        if not text:\n",
        "            b2 = min(len(ctx), a+1)\n",
        "            text = clean_span_text(ctx[a:b2], lang) or ctx[a:b2]\n",
        "        rows.append((qid, text))\n",
        "    sub = pd.DataFrame(rows, columns=['id','PredictionString'])\n",
        "    empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "    mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "    print(f'512 single-seed char-fusion: Empties={int(empties)}, mean_len={mean_len:.2f}, time={time.time()-t0:.2f}s')\n",
        "    return sub\n",
        "\n",
        "# Build 512 single-seed char-fusion submission\n",
        "sub_512cf = decode_charfusion_512_single(lambda_len=0.12, K_hi=200, K_ta=240, Lmax_hi=50, Lmax_ta=60)\n",
        "path_512cf = 'submission_charfusion_512_single_seed_alignsafe.csv'\n",
        "sub_512cf.to_csv(path_512cf, index=False)\n",
        "\n",
        "# Meta-consensus over alignment-safe candidates only\n",
        "def words(s):\n",
        "    return [w for w in str(s).strip().split() if len(w)>0]\n",
        "def jaccard(a, b):\n",
        "    wa, wb = set(words(a)), set(words(b))\n",
        "    if not wa and not wb: return 1.0\n",
        "    if not wa or not wb: return 0.0\n",
        "    inter = len(wa & wb); uni = len(wa | wb)\n",
        "    return inter/uni if uni>0 else 0.0\n",
        "def len_prior_dist(s, lang):\n",
        "    L = max(1, len(str(s))); mu = priors.get(lang, {}).get('mu', 2.3)\n",
        "    return abs(math.log(L) - mu)\n",
        "from collections import Counter\n",
        "def majority_override(texts, min_votes=3):\n",
        "    cnt = Counter(texts); text, votes = cnt.most_common(1)[0]\n",
        "    return (text if votes >= min_votes else None), votes\n",
        "\n",
        "# Pools: Hindi uses {512 single-seed CF, tokendp_512, tokenselect}; Tamil uses {512 single-seed CF, tokendp_512, tokenselect}\n",
        "cand_hi_files = [\n",
        "    path_512cf,\n",
        "    'submission_tokendp_512.csv',\n",
        "    'submission_tokenselect_512single_or_384_lambda012.csv'\n",
        "]\n",
        "cand_ta_files = [\n",
        "    path_512cf,\n",
        "    'submission_tokendp_512.csv',\n",
        "    'submission_tokenselect_512single_or_384_lambda012.csv',\n",
        "]\n",
        "def load_cands(paths):\n",
        "    out = []\n",
        "    for f in paths:\n",
        "        try:\n",
        "            df = pd.read_csv(f)\n",
        "            if set(df.columns) >= {'id','PredictionString'} and len(df)==len(test):\n",
        "                out.append((f, df.set_index('id')['PredictionString'].astype(str).to_dict()))\n",
        "                print('Loaded:', f)\n",
        "            else:\n",
        "                print('Skip (shape/cols):', f)\n",
        "        except Exception as e:\n",
        "            print('Skip:', f, '->', e)\n",
        "    return out\n",
        "cands_hi = load_cands(cand_hi_files)\n",
        "cands_ta = load_cands(cand_ta_files)\n",
        "\n",
        "lambda_hi = 0.02; lambda_ta = 0.045\n",
        "rows = []; t0 = time.time()\n",
        "for qid in id_list:\n",
        "    lang = id2lang[qid]\n",
        "    pool = cands_hi if lang=='hindi' else cands_ta\n",
        "    texts = [d[qid] for _, d in pool]\n",
        "    maj, votes = majority_override(texts, min_votes=3)\n",
        "    if maj is not None:\n",
        "        rows.append((qid, maj)); continue\n",
        "    best_i, best_s, best_d, best_L = -1, -1e18, 1e9, 10**9\n",
        "    lam = lambda_hi if lang=='hindi' else lambda_ta\n",
        "    for i, ti in enumerate(texts):\n",
        "        s = 0.0; cnt = 0\n",
        "        for j, tj in enumerate(texts):\n",
        "            if i==j: continue\n",
        "            s += jaccard(ti, tj); cnt += 1\n",
        "        avg_j = s / max(1, cnt)\n",
        "        dlen = len_prior_dist(ti, lang)\n",
        "        score = avg_j - lam * dlen\n",
        "        L = len(str(ti))\n",
        "        if (score > best_s + 1e-12) or (abs(score - best_s) <= 0.01 and (dlen < best_d or (abs(dlen-best_d) <= 1e-6 and L < best_L))):\n",
        "            best_s, best_i, best_d, best_L = score, i, dlen, L\n",
        "    rows.append((qid, texts[best_i]))\n",
        "\n",
        "sub = pd.DataFrame(rows, columns=['id','PredictionString'])\n",
        "empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "print(f'Alignment-safe meta-consensus built. Empties={int(empties)}, mean_len={mean_len:.2f}, time={time.time()-t0:.2f}s')\n",
        "\n",
        "out_path = 'submission_meta_consensus_alignsafe_512singlecf_tokendp_tokenselect.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "pd.read_csv(out_path).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_path)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512 single-seed: (1401, 512) mapping 1401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512 single-seed char-fusion: Empties=0, mean_len=10.79, time=2.26s\nLoaded: submission_charfusion_512_single_seed_alignsafe.csv\nLoaded: submission_tokendp_512.csv\nLoaded: submission_tokenselect_512single_or_384_lambda012.csv\nLoaded: submission_charfusion_512_single_seed_alignsafe.csv\nLoaded: submission_tokendp_512.csv\nLoaded: submission_tokenselect_512single_or_384_lambda012.csv\nAlignment-safe meta-consensus built. Empties=0, mean_len=10.86, time=0.00s\nsubmission.csv updated -> submission_meta_consensus_alignsafe_512singlecf_tokendp_tokenselect.csv\n"
          ]
        }
      ]
    },
    {
      "id": "4ec57977-7de0-467f-8f00-166409a14132",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Alignment-safe meta-consensus with normalized scoring (no 3-seed; Tamil excludes 384-only, Hindi also drops 384-only)\n",
        "import pandas as pd, numpy as np, math, time, re, os, unicodedata as ud\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id_list = test['id'].tolist()\n",
        "lang_map = dict(zip(test['id'], test['language']))\n",
        "\n",
        "# Alignment-safe candidate pools\n",
        "# Hindi: 512 single-seed char-fusion, 512 token-DP, 512 vs 384 token-select (DROP 384-only CF)\n",
        "# Tamil: 512 single-seed char-fusion, 512 token-DP, 512 vs 384 token-select\n",
        "cand_hi_files = [\n",
        "    'submission_charfusion_512_single_seed_alignsafe.csv',\n",
        "    'submission_tokendp_512.csv',\n",
        "    'submission_tokenselect_512single_or_384_lambda012.csv',\n",
        "]\n",
        "cand_ta_files = [\n",
        "    'submission_charfusion_512_single_seed_alignsafe.csv',\n",
        "    'submission_tokendp_512.csv',\n",
        "    'submission_tokenselect_512single_or_384_lambda012.csv',\n",
        "]\n",
        "\n",
        "def load_cands(paths):\n",
        "    out = []\n",
        "    for f in paths:\n",
        "        try:\n",
        "            if not os.path.exists(f):\n",
        "                print('Missing:', f); continue\n",
        "            df = pd.read_csv(f)\n",
        "            if set(df.columns) >= {'id','PredictionString'} and len(df)==len(test):\n",
        "                out.append((f, df.set_index('id')['PredictionString'].astype(str).to_dict()))\n",
        "                print('Loaded:', f)\n",
        "            else:\n",
        "                print('Skip (shape/cols):', f)\n",
        "        except Exception as e:\n",
        "            print('Skip:', f, '->', e)\n",
        "    return out\n",
        "\n",
        "cands_hi = load_cands(cand_hi_files)\n",
        "cands_ta = load_cands(cand_ta_files)\n",
        "assert len(cands_hi) >= 3 and len(cands_ta) >= 3, 'Insufficient alignment-safe candidates'\n",
        "\n",
        "# Normalization for scoring (do not change final output text) + digit/sep normalization\n",
        "PUNCT = set(list('.,!?;:\\\"\\'()[]{}\\u201c\\u201d\\u2018\\u2019\\u00ab\\u00bb'))\n",
        "DIG_MAP = str.maketrans({\n",
        "    '\\u0966':'0','\\u0967':'1','\\u0968':'2','\\u0969':'3','\\u096a':'4','\\u096b':'5','\\u096c':'6','\\u096d':'7','\\u096e':'8','\\u096f':'9',\n",
        "    '\\u0be6':'0','\\u0be7':'1','\\u0be8':'2','\\u0be9':'3','\\u0bea':'4','\\u0beb':'5','\\u0bec':'6','\\u0bed':'7','\\u0bee':'8','\\u0bef':'9'\n",
        "})\n",
        "SEP_PAT = re.compile(r'[\\u2010\\u2011\\u2012\\u2013\\u2014\\u2212]+')  # various dashes\n",
        "DANDA = '\\u0964'\n",
        "def norm_text(s: str) -> str:\n",
        "    s = str(s).strip()\n",
        "    s = s.translate(DIG_MAP)\n",
        "    s = SEP_PAT.sub('-', s)\n",
        "    s = s.replace('\\u00A0',' ').replace('\\u2009',' ').replace('\\u200A',' ')\n",
        "    while len(s) >= 2 and ((s[0], s[-1]) in [(\"\\\"\",\"\\\"\"),(\"'\",\"'\"),('\\u201c','\\u201d'),('\\u2018','\\u2019'),('(',')'),('[',']'),('{','}'),('\\u00ab','\\u00bb')]):\n",
        "        s = s[1:-1].strip()\n",
        "    # drop standalone danda at ends for matching robustness\n",
        "    s = s.strip(DANDA)\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    s = s.strip(''.join(PUNCT))\n",
        "    return s.strip()\n",
        "\n",
        "def words_norm(s):\n",
        "    s = norm_text(s)\n",
        "    toks = []\n",
        "    for w in s.split():\n",
        "        w = w.strip(''.join(PUNCT))\n",
        "        if w: toks.append(w)\n",
        "    return toks\n",
        "\n",
        "def jaccard_norm(a, b):\n",
        "    wa, wb = set(words_norm(a)), set(words_norm(b))\n",
        "    if not wa and not wb: return 1.0\n",
        "    if not wa or not wb: return 0.0\n",
        "    inter = len(wa & wb); uni = len(wa | wb)\n",
        "    return inter/uni if uni>0 else 0.0\n",
        "\n",
        "# Per-language log-normal priors\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "\n",
        "def len_prior_dist(s, lang):\n",
        "    L = max(1, len(str(s))); mu = priors.get(lang, {}).get('mu', 2.3)\n",
        "    return abs(math.log(L) - mu)\n",
        "\n",
        "from collections import Counter\n",
        "def majority_override_norm(texts, min_votes=3):\n",
        "    norm_map = [norm_text(t) for t in texts]\n",
        "    cnt = Counter(norm_map)\n",
        "    top_text, votes = cnt.most_common(1)[0]\n",
        "    if votes >= min_votes:\n",
        "        for orig, nt in zip(texts, norm_map):\n",
        "            if nt == top_text:\n",
        "                return orig, votes\n",
        "    return None, votes\n",
        "\n",
        "# Length regularization weights\n",
        "lambda_hi = 0.02\n",
        "lambda_ta = 0.045\n",
        "\n",
        "# Numeric/date micro-rule: match on normalized TD text; allow digits/separators; len <= 16; output raw TD\n",
        "num_re = re.compile(r'^[\\d\\u0966-\\u096f\\u0be6-\\u0bef\\s\\-/\\.]+$')\n",
        "\n",
        "def tokendp_index(pool):\n",
        "    for i, (fname, _) in enumerate(pool):\n",
        "        if fname.endswith('submission_tokendp_512.csv'):\n",
        "            return i\n",
        "    return None\n",
        "\n",
        "def tokenselect_index(pool):\n",
        "    for i, (fname, _) in enumerate(pool):\n",
        "        if fname.endswith('submission_tokenselect_512single_or_384_lambda012.csv'):\n",
        "            return i\n",
        "    return None\n",
        "\n",
        "# Ultra-safe micro-trim (apply only if trimmed normalized form exists among candidates)\n",
        "TA_PULLI = '\\u0BCD'\n",
        "def apply_micro_trim(chosen: str, lang: str, cand_norms: set, norm_text_fn):\n",
        "    if not chosen:\n",
        "        return chosen\n",
        "    # Hindi: final danda\n",
        "    if lang == 'hindi' and chosen.endswith(DANDA):\n",
        "        trimmed = chosen[:-1].rstrip()\n",
        "        if norm_text_fn(trimmed) in cand_norms:\n",
        "            return trimmed\n",
        "    # Any trailing combining mark (incl. virama/pulli)\n",
        "    if len(chosen) >= 2 and ud.category(chosen[-1]) == 'Mn':\n",
        "        trimmed = chosen[:-1]\n",
        "        if norm_text_fn(trimmed) in cand_norms:\n",
        "            return trimmed\n",
        "    return chosen\n",
        "\n",
        "rows = []\n",
        "t0 = time.time()\n",
        "for qid in id_list:\n",
        "    lang = lang_map[qid]\n",
        "    pool = cands_hi if lang=='hindi' else cands_ta\n",
        "    texts = [d[qid] for _, d in pool]\n",
        "    cand_norms = {norm_text(t) for t in texts}\n",
        "    # Micro-override: prefer token-DP if numeric/date-like and short (per-language cap) on normalized form\n",
        "    tdp_idx = tokendp_index(pool)\n",
        "    if tdp_idx is not None:\n",
        "        tdp_raw = texts[tdp_idx]\n",
        "        tdp_norm = norm_text(tdp_raw).replace('\\u0964','')\n",
        "        if tdp_raw and (((lang == 'hindi') and (len(tdp_raw) <= 18)) or ((lang != 'hindi') and (len(tdp_raw) <= 16))) and num_re.match(tdp_norm):\n",
        "            pick = apply_micro_trim(tdp_raw, lang, cand_norms, norm_text)\n",
        "            rows.append((qid, pick))\n",
        "            continue\n",
        "    # Majority override on normalized strings\n",
        "    maj, votes = majority_override_norm(texts, min_votes=3)\n",
        "    if maj is not None:\n",
        "        pick = apply_micro_trim(maj, lang, cand_norms, norm_text)\n",
        "        # Tie-break: if token-select normalized equals chosen normalized, prefer token-select raw\n",
        "        ts_idx = tokenselect_index(pool)\n",
        "        if ts_idx is not None:\n",
        "            TS = texts[ts_idx]\n",
        "            if norm_text(TS) == norm_text(pick) and TS != pick:\n",
        "                pick = apply_micro_trim(TS, lang, cand_norms, norm_text)\n",
        "        rows.append((qid, pick))\n",
        "        continue\n",
        "    # Removed contained-shorter rule to avoid over-shortening\n",
        "    # Score by mean normalized Jaccard; add small per-language len prior; tie-break by closer to prior, then shorter within 0.01\n",
        "    best_i, best_s, best_d, best_L = -1, -1e18, 1e9, 10**9\n",
        "    lam = lambda_hi if lang=='hindi' else lambda_ta\n",
        "    for i, ti in enumerate(texts):\n",
        "        s = 0.0; cnt = 0\n",
        "        for j, tj in enumerate(texts):\n",
        "            if i==j: continue\n",
        "            s += jaccard_norm(ti, tj); cnt += 1\n",
        "        avg_j = s / max(1, cnt)\n",
        "        dlen = len_prior_dist(ti, lang)\n",
        "        score = avg_j - lam * dlen\n",
        "        L = len(str(ti))\n",
        "        if (score > best_s + 1e-12) or (abs(score - best_s) <= 0.01 and (dlen < best_d or (abs(dlen-best_d) <= 1e-6 and L < best_L))):\n",
        "            best_s, best_i, best_d, best_L = score, i, dlen, L\n",
        "    pick = apply_micro_trim(texts[best_i], lang, cand_norms, norm_text)\n",
        "    # Tie-break after scoring: if token-select normalized equals chosen normalized, prefer token-select raw\n",
        "    ts_idx = tokenselect_index(pool)\n",
        "    if ts_idx is not None:\n",
        "        TS = texts[ts_idx]\n",
        "        if norm_text(TS) == norm_text(pick) and TS != pick:\n",
        "            pick = apply_micro_trim(TS, lang, cand_norms, norm_text)\n",
        "    rows.append((qid, pick))\n",
        "\n",
        "sub = pd.DataFrame(rows, columns=['id','PredictionString'])\n",
        "empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "print(f'Align-safe meta-consensus (norm scoring, +numeric<=16 +digit/sep norm +micro-trim, TS-norm tie-break, no-contained-shorter, HI no 384-only) built. Empties={int(empties)}, mean_len={mean_len:.2f}, time={time.time()-t0:.2f}s')\n",
        "\n",
        "out_path = 'submission_meta_consensus_alignsafe_normscore_512singlecf_tokendp_tokenselect.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "pd.read_csv(out_path).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_path)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: submission_charfusion_512_single_seed_alignsafe.csv\nLoaded: submission_tokendp_512.csv\nLoaded: submission_tokenselect_512single_or_384_lambda012.csv\nLoaded: submission_charfusion_512_single_seed_alignsafe.csv\nLoaded: submission_tokendp_512.csv\nLoaded: submission_tokenselect_512single_or_384_lambda012.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Align-safe meta-consensus (norm scoring, +numeric<=16 +digit/sep norm +micro-trim, TS-norm tie-break, no-contained-shorter, HI no 384-only) built. Empties=0, mean_len=10.70, time=0.02s\nsubmission.csv updated -> submission_meta_consensus_alignsafe_normscore_512singlecf_tokendp_tokenselect.csv\n"
          ]
        }
      ]
    },
    {
      "id": "bb6504e1-b514-4994-a4a4-18476f646ba0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Hedge 2 (per-id confidence gate): fallback to token-DP on low-consensus; trailing-punct diff prefers token-DP; numeric/date override <=16\n",
        "import pandas as pd, numpy as np, re, time, math, os, unicodedata as ud\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id_list = test['id'].tolist()\n",
        "lang_map = dict(zip(test['id'], test['language']))\n",
        "\n",
        "# Inputs:\n",
        "# - Cell 28 normalized meta-consensus (primary) WITH numeric/date micro-override\n",
        "sub28_path = 'submission_meta_consensus_alignsafe_normscore_512singlecf_tokendp_tokenselect.csv'\n",
        "# - Cell 27 non-normalized meta-consensus (older pool; approximate use for trailing-punct check if present)\n",
        "sub27_path = 'submission_meta_consensus_alignsafe_512singlecf_tokendp_tokenselect.csv'\n",
        "# - Alignment-safe three candidates for confidence computation: 512 CF single-seed, token-DP 512, token-select\n",
        "cf_path  = 'submission_charfusion_512_single_seed_alignsafe.csv'\n",
        "tdp_path = 'submission_tokendp_512.csv'\n",
        "tsel_path= 'submission_tokenselect_512single_or_384_lambda012.csv'\n",
        "\n",
        "def must_load(path):\n",
        "    assert os.path.exists(path), f'Missing {path}'\n",
        "    df = pd.read_csv(path)\n",
        "    assert set(df.columns) >= {'id','PredictionString'} and len(df)==len(test), f'Bad shape/cols for {path}'\n",
        "    return df.set_index('id')['PredictionString'].astype(str).to_dict()\n",
        "\n",
        "m28 = must_load(sub28_path)\n",
        "mCF = must_load(cf_path)\n",
        "mTD = must_load(tdp_path)\n",
        "mTS = must_load(tsel_path)\n",
        "m27 = None\n",
        "if os.path.exists(sub27_path):\n",
        "    try:\n",
        "        m27 = must_load(sub27_path)\n",
        "    except Exception:\n",
        "        m27 = None\n",
        "\n",
        "# Normalized scoring utils (match Cell 28 digit/sep + punctuation handling)\n",
        "PUNCT = set(list('.,!?;:\\\"\\'()[]{}\u201c\u201d\u2018\u2019\u00ab\u00bb'))\n",
        "DIG_MAP = str.maketrans({\n",
        "    '\u0966':'0','\u0967':'1','\u0968':'2','\u0969':'3','\u096a':'4','\u096b':'5','\u096c':'6','\u096d':'7','\u096e':'8','\u096f':'9',\n",
        "    '\u0be6':'0','\u0be7':'1','\u0be8':'2','\u0be9':'3','\u0bea':'4','\u0beb':'5','\u0bec':'6','\u0bed':'7','\u0bee':'8','\u0bef':'9'\n",
        "})\n",
        "SEP_PAT = re.compile(r'[\\u2010\\u2011\\u2012\\u2013\\u2014\\u2212]+')\n",
        "DANDA = '\\u0964'\n",
        "def norm_text(s: str) -> str:\n",
        "    s = str(s).strip()\n",
        "    s = s.translate(DIG_MAP)\n",
        "    s = SEP_PAT.sub('-', s)\n",
        "    s = s.replace('\\u00A0',' ').replace('\\u2009',' ').replace('\\u200A',' ')\n",
        "    while len(s) >= 2 and ((s[0], s[-1]) in [(\"\\\"\",\"\\\"\"),(\"'\",\"'\"),('\u201c','\u201d'),('\u2018','\u2019'),('(',')'),('[',']'),('{','}'),('\u00ab','\u00bb')]):\n",
        "        s = s[1:-1].strip()\n",
        "    s = s.strip(DANDA)\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    s = s.strip(''.join(PUNCT))\n",
        "    return s.strip()\n",
        "\n",
        "def words_norm(s):\n",
        "    s = norm_text(s)\n",
        "    toks = []\n",
        "    for w in s.split():\n",
        "        w = w.strip(''.join(PUNCT))\n",
        "        if w: toks.append(w)\n",
        "    return toks\n",
        "\n",
        "def jaccard_norm(a, b):\n",
        "    wa, wb = set(words_norm(a)), set(words_norm(b))\n",
        "    if not wa and not wb: return 1.0\n",
        "    if not wa or not wb: return 0.0\n",
        "    inter = len(wa & wb); uni = len(wa | wb)\n",
        "    return inter/uni if uni>0 else 0.0\n",
        "\n",
        "# Numeric/date micro-override (HI len <= 18, TA len <= 16; match on normalized TD); output TD raw when fired\n",
        "num_re = re.compile(r'^[\\d\\u0966-\\u096f\\u0be6-\\u0bef\\s\\-/\\.]+$')\n",
        "\n",
        "# Ultra-safe micro-trim (same as Cell 28): trim final danda or combining mark if another candidate's normalized form matches the trimmed)\n",
        "TA_PULLI = '\\u0BCD'\n",
        "def apply_micro_trim(chosen: str, lang: str, cand_norms: set, norm_text_fn):\n",
        "    if not chosen:\n",
        "        return chosen\n",
        "    # Hindi: final danda\n",
        "    if lang == 'hindi' and chosen.endswith(DANDA):\n",
        "        trimmed = chosen[:-1].rstrip()\n",
        "        if norm_text_fn(trimmed) in cand_norms:\n",
        "            return trimmed\n",
        "    # Any trailing combining mark\n",
        "    if len(chosen) >= 2 and ud.category(chosen[-1]) == 'Mn':\n",
        "        trimmed = chosen[:-1]\n",
        "        if norm_text_fn(trimmed) in cand_norms:\n",
        "            return trimmed\n",
        "    return chosen\n",
        "\n",
        "rows = []\n",
        "t0 = time.time()\n",
        "for qid in id_list:\n",
        "    lang = lang_map[qid]\n",
        "    # Numeric/date override first (per-language len cap)\n",
        "    TD = mTD[qid]\n",
        "    if TD:\n",
        "        td_norm = norm_text(TD).replace(DANDA,'')\n",
        "        len_cap = 18 if lang == 'hindi' else 16\n",
        "        if len(TD) <= len_cap and num_re.match(td_norm):\n",
        "            rows.append((qid, TD))\n",
        "            continue\n",
        "    # Three alignment-safe candidates for confidence computation\n",
        "    CF = mCF[qid]; TS = mTS[qid]\n",
        "    cands = [CF, TD, TS]\n",
        "    cand_norms = {norm_text(t) for t in cands}\n",
        "    # Preference 1: if token-select normalized equals char-fusion normalized, prefer token-select raw\n",
        "    if norm_text(TS) == norm_text(CF) and TS != CF:\n",
        "        pick = apply_micro_trim(TS, lang, cand_norms, norm_text)\n",
        "        rows.append((qid, pick))\n",
        "        continue\n",
        "    # Preference 2 (new): if token-select normalized equals token-DP normalized, prefer token-select raw\n",
        "    if norm_text(TS) == norm_text(TD) and TS != TD:\n",
        "        pick = apply_micro_trim(TS, lang, cand_norms, norm_text)\n",
        "        rows.append((qid, pick))\n",
        "        continue\n",
        "    # Compute mean normalized Jaccard for each candidate vs the other two\n",
        "    conf_scores = []\n",
        "    for i in range(3):\n",
        "        s = 0.0; cnt = 0\n",
        "        for j in range(3):\n",
        "            if i==j: continue\n",
        "            s += jaccard_norm(cands[i], cands[j]); cnt += 1\n",
        "        conf_scores.append(s / max(1, cnt))\n",
        "    conf = max(conf_scores) if conf_scores else 0.0\n",
        "    pick = m28[qid]  # default to Cell 28 primary\n",
        "    # Optional trailing-punctuation/quotes diff rule using Cell 27 (if available)\n",
        "    if m27 is not None:\n",
        "        t27n = norm_text(m27[qid]); t28n = norm_text(m28[qid])\n",
        "        if t27n == t28n and m27[qid] != m28[qid]:\n",
        "            pick = mTD[qid]\n",
        "            pick = apply_micro_trim(pick, lang, cand_norms, norm_text)\n",
        "            rows.append((qid, pick)); continue\n",
        "    # Per-language low-confidence gate\n",
        "    thresh = 0.37 if lang == 'tamil' else 0.33\n",
        "    if conf < thresh:\n",
        "        pick = mTD[qid]\n",
        "    pick = apply_micro_trim(pick, lang, cand_norms, norm_text)\n",
        "    rows.append((qid, pick))\n",
        "\n",
        "sub = pd.DataFrame(rows, columns=['id','PredictionString'])\n",
        "empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "print(f'Per-id hedge built (TA conf<0.37/HI conf<0.33, +HI numeric<=18/TA<=16, +TS=CF|TD norm prefer TS, +micro-trim). Empties={int(empties)}, mean_len={mean_len:.2f}, time={time.time()-t0:.2f}s')\n",
        "\n",
        "out_path = 'submission_perid_hedge_tokendp_lowconf.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "pd.read_csv(out_path).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_path)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-id hedge built (TA conf<0.37/HI conf<0.33, +HI numeric<=18/TA<=16, +TS=CF|TD norm prefer TS, +micro-trim). Empties=0, mean_len=10.47, time=0.02s\nsubmission.csv updated -> submission_perid_hedge_tokendp_lowconf.csv\n"
          ]
        }
      ]
    },
    {
      "id": "73ace88f-a1bd-49e9-ad3d-42d47fe1c164",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Hedge 1: Non-normalized meta-consensus (standard Jaccard) + numeric/date micro-override; strict 3-candidate pool\n",
        "import pandas as pd, numpy as np, math, time, re, os\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id_list = test['id'].tolist()\n",
        "lang_map = dict(zip(test['id'], test['language']))\n",
        "\n",
        "# Strict alignment-safe pools (exactly 3 candidates for both HI and TA, as in Cell 27 before 384 addition)\n",
        "cand_hi_files = [\n",
        "    'submission_charfusion_512_single_seed_alignsafe.csv',\n",
        "    'submission_tokendp_512.csv',\n",
        "    'submission_tokenselect_512single_or_384_lambda012.csv',\n",
        "]\n",
        "cand_ta_files = [\n",
        "    'submission_charfusion_512_single_seed_alignsafe.csv',\n",
        "    'submission_tokendp_512.csv',\n",
        "    'submission_tokenselect_512single_or_384_lambda012.csv',\n",
        "]\n",
        "\n",
        "def load_cands(paths):\n",
        "    out = []\n",
        "    for f in paths:\n",
        "        try:\n",
        "            if not os.path.exists(f):\n",
        "                print('Missing:', f); continue\n",
        "            df = pd.read_csv(f)\n",
        "            if set(df.columns) >= {'id','PredictionString'} and len(df)==len(test):\n",
        "                out.append((f, df.set_index('id')['PredictionString'].astype(str).to_dict()))\n",
        "                print('Loaded:', f)\n",
        "            else:\n",
        "                print('Skip (shape/cols):', f)\n",
        "        except Exception as e:\n",
        "            print('Skip:', f, '->', e)\n",
        "    return out\n",
        "\n",
        "cands_hi = load_cands(cand_hi_files)\n",
        "cands_ta = load_cands(cand_ta_files)\n",
        "assert len(cands_hi) == 3 and len(cands_ta) == 3, 'Expected exactly 3 candidates per language'\n",
        "\n",
        "# Standard word-level Jaccard\n",
        "def words(s):\n",
        "    return [w for w in str(s).strip().split() if len(w)>0]\n",
        "def jaccard(a, b):\n",
        "    wa, wb = set(words(a)), set(words(b))\n",
        "    if not wa and not wb: return 1.0\n",
        "    if not wa or not wb: return 0.0\n",
        "    inter = len(wa & wb); uni = len(wa | wb)\n",
        "    return inter/uni if uni>0 else 0.0\n",
        "\n",
        "# Per-language log-normal priors\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "def len_prior_dist(s, lang):\n",
        "    L = max(1, len(str(s))); mu = priors.get(lang, {}).get('mu', 2.3)\n",
        "    return abs(math.log(L) - mu)\n",
        "\n",
        "from collections import Counter\n",
        "def majority_override(texts, min_votes=3):\n",
        "    cnt = Counter(texts)\n",
        "    text, votes = cnt.most_common(1)[0]\n",
        "    return (text if votes >= min_votes else None), votes\n",
        "\n",
        "# Length regularization weights (keep as advised)\n",
        "lambda_hi = 0.02\n",
        "lambda_ta = 0.045\n",
        "\n",
        "# Numeric/date micro-rule regex per expert: prefer token-DP if matches and <=12 chars\n",
        "num_re = re.compile(r'^[\\d\\u0966-\\u096f\\u0be6-\\u0bef\\s\\-/\\.]+$')\n",
        "def tokendp_index(pool):\n",
        "    for i, (fname, _) in enumerate(pool):\n",
        "        if fname.endswith('submission_tokendp_512.csv'):\n",
        "            return i\n",
        "    return None\n",
        "\n",
        "rows = []\n",
        "t0 = time.time()\n",
        "for qid in id_list:\n",
        "    lang = lang_map[qid]\n",
        "    pool = cands_hi if lang=='hindi' else cands_ta\n",
        "    texts = [d[qid] for _, d in pool]\n",
        "    # Micro-override (numeric/date-like and short) -> token-DP\n",
        "    tdp_idx = tokendp_index(pool)\n",
        "    if tdp_idx is not None:\n",
        "        tdp_txt = texts[tdp_idx]\n",
        "        if tdp_txt and len(tdp_txt) <= 12 and num_re.match(tdp_txt):\n",
        "            rows.append((qid, tdp_txt))\n",
        "            continue\n",
        "    # Majority override on raw strings\n",
        "    maj, votes = majority_override(texts, min_votes=3)\n",
        "    if maj is not None:\n",
        "        rows.append((qid, maj))\n",
        "        continue\n",
        "    # Score by mean word-level Jaccard + small len prior; tie-break by closer to prior, then shorter within 0.01\n",
        "    best_i, best_s, best_d, best_L = -1, -1e18, 1e9, 10**9\n",
        "    lam = lambda_hi if lang=='hindi' else lambda_ta\n",
        "    for i, ti in enumerate(texts):\n",
        "        s = 0.0; cnt = 0\n",
        "        for j, tj in enumerate(texts):\n",
        "            if i==j: continue\n",
        "            s += jaccard(ti, tj); cnt += 1\n",
        "        avg_j = s / max(1, cnt)\n",
        "        dlen = len_prior_dist(ti, lang)\n",
        "        score = avg_j - lam * dlen\n",
        "        L = len(str(ti))\n",
        "        if (score > best_s + 1e-12) or (abs(score - best_s) <= 0.01 and (dlen < best_d or (abs(dlen-best_d) <= 1e-6 and L < best_L))):\n",
        "            best_s, best_i, best_d, best_L = score, i, dlen, L\n",
        "    rows.append((qid, texts[best_i]))\n",
        "\n",
        "sub = pd.DataFrame(rows, columns=['id','PredictionString'])\n",
        "empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "print(f'Hedge1 non-normalized (std Jaccard, +numeric override) built. Empties={int(empties)}, mean_len={mean_len:.2f}, time={time.time()-t0:.2f}s')\n",
        "\n",
        "out_path = 'submission_meta_consensus_alignsafe_nonorm_numeric_3cand.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "pd.read_csv(out_path).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_path)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: submission_charfusion_512_single_seed_alignsafe.csv\nLoaded: submission_tokendp_512.csv\nLoaded: submission_tokenselect_512single_or_384_lambda012.csv\nLoaded: submission_charfusion_512_single_seed_alignsafe.csv\nLoaded: submission_tokendp_512.csv\nLoaded: submission_tokenselect_512single_or_384_lambda012.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hedge1 non-normalized (std Jaccard, +numeric override) built. Empties=0, mean_len=10.71, time=0.00s\nsubmission.csv updated -> submission_meta_consensus_alignsafe_nonorm_numeric_3cand.csv\n"
          ]
        }
      ]
    },
    {
      "id": "a2616243-c329-4625-a5a5-5a02f6c66fc5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Meta-hedge: combine current norm+containment (Cell 28), older norm (384hi variant if present), and non-norm (Cell 30), with TD low-conf gate\n",
        "import pandas as pd, numpy as np, re, math, time, os\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id_list = test['id'].tolist()\n",
        "lang_map = dict(zip(test['id'], test['language']))\n",
        "\n",
        "# Candidates\n",
        "path_curr = 'submission_meta_consensus_alignsafe_normscore_512singlecf_tokendp_tokenselect.csv'  # Cell 28 current (digit/sep+containment)\n",
        "path_old  = 'submission_meta_consensus_alignsafe_normscore_512singlecf_tokendp_tokenselect_384hi.csv'  # older norm-safe (no containment)\n",
        "path_non  = 'submission_meta_consensus_alignsafe_nonorm_numeric_3cand.csv'  # Cell 30 non-normalized numeric override\n",
        "path_tdp  = 'submission_tokendp_512.csv'\n",
        "\n",
        "def must_load(path):\n",
        "    assert os.path.exists(path), f'Missing {path}'\n",
        "    df = pd.read_csv(path)\n",
        "    assert set(df.columns) >= {'id','PredictionString'} and len(df)==len(test), f'Bad shape for {path}'\n",
        "    return df.set_index('id')['PredictionString'].astype(str).to_dict()\n",
        "\n",
        "m_curr = must_load(path_curr)\n",
        "m_non  = must_load(path_non)\n",
        "m_tdp  = must_load(path_tdp)\n",
        "m_old  = None\n",
        "if os.path.exists(path_old):\n",
        "    try:\n",
        "        m_old = must_load(path_old)\n",
        "    except Exception:\n",
        "        m_old = None\n",
        "\n",
        "# Normalized scoring helpers\n",
        "PUNCT = set(list('.,!?;:\"\\'()[]{}\u201c\u201d\u2018\u2019\u00ab\u00bb'))\n",
        "def norm_text(s: str) -> str:\n",
        "    s = str(s).strip()\n",
        "    while len(s) >= 2 and ((s[0], s[-1]) in [(\"\\\"\",\"\\\"\"),(\"'\",\"'\"),('\u201c','\u201d'),('\u2018','\u2019'),('(',')'),('[',']'),('{','}'),('\u00ab','\u00bb')]):\n",
        "        s = s[1:-1].strip()\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    s = s.strip(''.join(PUNCT))\n",
        "    return s.strip()\n",
        "def words_norm(s):\n",
        "    s = norm_text(s)\n",
        "    toks = []\n",
        "    for w in s.split():\n",
        "        w = w.strip(''.join(PUNCT))\n",
        "        if w: toks.append(w)\n",
        "    return toks\n",
        "def jaccard_norm(a, b):\n",
        "    wa, wb = set(words_norm(a)), set(words_norm(b))\n",
        "    if not wa and not wb: return 1.0\n",
        "    if not wa or not wb: return 0.0\n",
        "    inter = len(wa & wb); uni = len(wa | wb)\n",
        "    return inter/uni if uni>0 else 0.0\n",
        "\n",
        "# Length prior from train\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "def len_prior_dist(s, lang):\n",
        "    L = max(1, len(str(s))); mu = priors.get(lang, {}).get('mu', 2.3)\n",
        "    return abs(math.log(L) - mu)\n",
        "\n",
        "# Numeric/date micro-rule\n",
        "num_re = re.compile(r'^[\\d\\u0966-\\u096f\\u0be6-\\u0bef\\s\\-/\\.]+$')\n",
        "\n",
        "lambda_hi = 0.02\n",
        "lambda_ta = 0.045\n",
        "CONF_THRESH = 0.35\n",
        "\n",
        "rows = []\n",
        "t0 = time.time()\n",
        "for qid in id_list:\n",
        "    lang = lang_map[qid]\n",
        "    # build trio: current norm, old norm (fallback to current if missing), non-norm\n",
        "    A = m_curr[qid]\n",
        "    B = m_old[qid] if m_old is not None else A\n",
        "    C = m_non[qid]\n",
        "    TD = m_tdp[qid]\n",
        "    # Numeric/date override: prefer TD if short numeric/date\n",
        "    if TD and len(TD) <= 12 and num_re.match(TD):\n",
        "        rows.append((qid, TD))\n",
        "        continue\n",
        "    # Confidence over A/B/C\n",
        "    texts = [A, B, C]\n",
        "    confs = []\n",
        "    best_idx, best_score, best_d, best_L = -1, -1e18, 1e9, 10**9\n",
        "    lam = lambda_hi if lang=='hindi' else lambda_ta\n",
        "    for i, ti in enumerate(texts):\n",
        "        s = 0.0; cnt = 0\n",
        "        for j, tj in enumerate(texts):\n",
        "            if i==j: continue\n",
        "            s += jaccard_norm(ti, tj); cnt += 1\n",
        "        avg_j = s / max(1, cnt)\n",
        "        confs.append(avg_j)\n",
        "        dlen = len_prior_dist(ti, lang)\n",
        "        score = avg_j - lam * dlen\n",
        "        L = len(str(ti))\n",
        "        if (score > best_score + 1e-12) or (abs(score - best_score) <= 0.01 and (dlen < best_d or (abs(dlen-best_d) <= 1e-6 and L < best_L))):\n",
        "            best_score, best_idx, best_d, best_L = score, i, dlen, L\n",
        "    conf = max(confs) if confs else 0.0\n",
        "    pick = texts[best_idx]\n",
        "    if conf < CONF_THRESH:\n",
        "        pick = TD\n",
        "    rows.append((qid, pick))\n",
        "\n",
        "sub = pd.DataFrame(rows, columns=['id','PredictionString'])\n",
        "empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "print(f'Meta-hedge (A=current norm, B=old norm 384hi, C=non-norm, TD low-conf) Empties={int(empties)}, mean_len={mean_len:.2f}, time={time.time()-t0:.2f}s')\n",
        "\n",
        "out_path = 'submission_metahedge_curr_old_non_tdp.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "pd.read_csv(out_path).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_path)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-hedge (A=current norm, B=old norm 384hi, C=non-norm, TD low-conf) Empties=0, mean_len=10.47, time=0.01s\nsubmission.csv updated -> submission_metahedge_curr_old_non_tdp.csv\n"
          ]
        }
      ]
    },
    {
      "id": "f802ebab-5f70-47d2-ab4c-14d0ecd1abae",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Switch submission.csv to older normalized 384hi variant and print diagnostics\n",
        "import pandas as pd, os, time\n",
        "path = 'submission_meta_consensus_alignsafe_normscore_512singlecf_tokendp_tokenselect_384hi.csv'\n",
        "assert os.path.exists(path), f'Missing {path}'\n",
        "sub = pd.read_csv(path)\n",
        "empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', path)\n",
        "print('Diagnostics: empties=', int(empties), 'mean_len=', round(float(mean_len), 2))\n",
        "print('mtime(submission.csv)=', time.ctime(os.path.getmtime('submission.csv')))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv updated -> submission_meta_consensus_alignsafe_normscore_512singlecf_tokendp_tokenselect_384hi.csv\nDiagnostics: empties= 0 mean_len= 10.65\nmtime(submission.csv)= Thu Sep 25 12:33:08 2025\n"
          ]
        }
      ]
    },
    {
      "id": "bfb149b6-1e62-4e46-8038-f0cfdbbb9eba",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Hedge 1b: Non-normalized consensus (std Jaccard) + numeric/date override <=16; add 384-only to Hindi pool\n",
        "import pandas as pd, numpy as np, math, time, re, os\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id_list = test['id'].tolist()\n",
        "lang_map = dict(zip(test['id'], test['language']))\n",
        "\n",
        "# Pools: Hindi uses 4 candidates (add 384-only CF for diversity); Tamil stays at 3\n",
        "cand_hi_files = [\n",
        "    'submission_charfusion_512_single_seed_alignsafe.csv',\n",
        "    'submission_tokendp_512.csv',\n",
        "    'submission_tokenselect_512single_or_384_lambda012.csv',\n",
        "    'submission_384only_charfusion_lambda015.csv',\n",
        "]\n",
        "cand_ta_files = [\n",
        "    'submission_charfusion_512_single_seed_alignsafe.csv',\n",
        "    'submission_tokendp_512.csv',\n",
        "    'submission_tokenselect_512single_or_384_lambda012.csv',\n",
        "]\n",
        "\n",
        "def load_cands(paths):\n",
        "    out = []\n",
        "    for f in paths:\n",
        "        try:\n",
        "            if not os.path.exists(f):\n",
        "                print('Missing:', f); continue\n",
        "            df = pd.read_csv(f)\n",
        "            if set(df.columns) >= {'id','PredictionString'} and len(df)==len(test):\n",
        "                out.append((f, df.set_index('id')['PredictionString'].astype(str).to_dict()))\n",
        "                print('Loaded:', f)\n",
        "            else:\n",
        "                print('Skip (shape/cols):', f)\n",
        "        except Exception as e:\n",
        "            print('Skip:', f, '->', e)\n",
        "    return out\n",
        "\n",
        "cands_hi = load_cands(cand_hi_files)\n",
        "cands_ta = load_cands(cand_ta_files)\n",
        "assert len(cands_hi) >= 3 and len(cands_ta) == 3, 'Unexpected candidate counts'\n",
        "\n",
        "# Standard word-level Jaccard\n",
        "def words(s):\n",
        "    return [w for w in str(s).strip().split() if len(w)>0]\n",
        "def jaccard(a, b):\n",
        "    wa, wb = set(words(a)), set(words(b))\n",
        "    if not wa and not wb: return 1.0\n",
        "    if not wa or not wb: return 0.0\n",
        "    inter = len(wa & wb); uni = len(wa | wb)\n",
        "    return inter/uni if uni>0 else 0.0\n",
        "\n",
        "# Per-language log-normal priors\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "def len_prior_dist(s, lang):\n",
        "    L = max(1, len(str(s))); mu = priors.get(lang, {}).get('mu', 2.3)\n",
        "    return abs(math.log(L) - mu)\n",
        "\n",
        "from collections import Counter\n",
        "def majority_override(texts, min_votes=3):\n",
        "    cnt = Counter(texts)\n",
        "    text, votes = cnt.most_common(1)[0]\n",
        "    return (text if votes >= min_votes else None), votes\n",
        "\n",
        "# Length regularization weights\n",
        "lambda_hi = 0.02\n",
        "lambda_ta = 0.045\n",
        "\n",
        "# Numeric/date micro-rule: prefer token-DP if normalized numeric/date-like and len<=16; output TD raw\n",
        "num_re = re.compile(r'^[\\d\\u0966-\\u096f\\u0be6-\\u0bef\\s\\-/\\.]+$')\n",
        "PUNCT = set(list('.,!?;:\\\"\\'()[]{}\u201c\u201d\u2018\u2019\u00ab\u00bb'))\n",
        "def norm_text(s: str) -> str:\n",
        "    s = str(s).strip()\n",
        "    while len(s) >= 2 and ((s[0], s[-1]) in [(\"\\\"\",\"\\\"\"),(\"'\",\"'\"),('\u201c','\u201d'),('\u2018','\u2019'),('(',')'),('[',']'),('{','}'),('\u00ab','\u00bb')]):\n",
        "        s = s[1:-1].strip()\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    s = s.strip(''.join(PUNCT))\n",
        "    return s.strip()\n",
        "def tokendp_index(pool):\n",
        "    for i, (fname, _) in enumerate(pool):\n",
        "        if fname.endswith('submission_tokendp_512.csv'):\n",
        "            return i\n",
        "    return None\n",
        "\n",
        "rows = []\n",
        "t0 = time.time()\n",
        "for qid in id_list:\n",
        "    lang = lang_map[qid]\n",
        "    pool = cands_hi if lang=='hindi' else cands_ta\n",
        "    texts = [d[qid] for _, d in pool]\n",
        "    # Numeric/date override first (normalized match), len<=16 -> pick TD raw\n",
        "    tdp_idx = tokendp_index(pool)\n",
        "    if tdp_idx is not None:\n",
        "        td_raw = texts[tdp_idx]\n",
        "        if td_raw:\n",
        "            td_norm = norm_text(td_raw).replace('\\u0964','')\n",
        "            if len(td_raw) <= 16 and num_re.match(td_norm):\n",
        "                rows.append((qid, td_raw)); continue\n",
        "    # Majority override on raw strings\n",
        "    maj, votes = majority_override(texts, min_votes=3)\n",
        "    if maj is not None:\n",
        "        rows.append((qid, maj)); continue\n",
        "    # Score by mean word-Jaccard + small len prior; tie-break by closer to prior, then shorter within 0.01\n",
        "    best_i, best_s, best_d, best_L = -1, -1e18, 1e9, 10**9\n",
        "    lam = lambda_hi if lang=='hindi' else lambda_ta\n",
        "    for i, ti in enumerate(texts):\n",
        "        s = 0.0; cnt = 0\n",
        "        for j, tj in enumerate(texts):\n",
        "            if i==j: continue\n",
        "            s += jaccard(ti, tj); cnt += 1\n",
        "        avg_j = s / max(1, cnt)\n",
        "        dlen = len_prior_dist(ti, lang)\n",
        "        score = avg_j - lam * dlen\n",
        "        L = len(str(ti))\n",
        "        if (score > best_s + 1e-12) or (abs(score - best_s) <= 0.01 and (dlen < best_d or (abs(dlen-best_d) <= 1e-6 and L < best_L))):\n",
        "            best_s, best_i, best_d, best_L = score, i, dlen, L\n",
        "    rows.append((qid, texts[best_i]))\n",
        "\n",
        "sub = pd.DataFrame(rows, columns=['id','PredictionString'])\n",
        "empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "print(f'Hedge1b non-normalized (std Jaccard, +numeric<=16, HI+384-only) built. Empties={int(empties)}, mean_len={mean_len:.2f}, time={time.time()-t0:.2f}s')\n",
        "\n",
        "out_path = 'submission_meta_consensus_alignsafe_nonorm_numeric_hi384.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "pd.read_csv(out_path).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_path)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: submission_charfusion_512_single_seed_alignsafe.csv\nLoaded: submission_tokendp_512.csv\nLoaded: submission_tokenselect_512single_or_384_lambda012.csv\nLoaded: submission_384only_charfusion_lambda015.csv\nLoaded: submission_charfusion_512_single_seed_alignsafe.csv\nLoaded: submission_tokendp_512.csv\nLoaded: submission_tokenselect_512single_or_384_lambda012.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hedge1b non-normalized (std Jaccard, +numeric<=16, HI+384-only) built. Empties=0, mean_len=10.52, time=0.01s\nsubmission.csv updated -> submission_meta_consensus_alignsafe_nonorm_numeric_hi384.csv\n"
          ]
        }
      ]
    },
    {
      "id": "d55ef598-4b4a-44f9-a3d6-5107de1b6446",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Final meta-hedge: combine Cell 28 (norm meta-consensus), Cell 29 (per-id hedge), and token-DP\n",
        "import pandas as pd, numpy as np, re, math, time, os, unicodedata as ud\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id_list = test['id'].tolist()\n",
        "lang_map = dict(zip(test['id'], test['language']))\n",
        "\n",
        "path_28 = 'submission_meta_consensus_alignsafe_normscore_512singlecf_tokendp_tokenselect.csv'\n",
        "path_29 = 'submission_perid_hedge_tokendp_lowconf.csv'\n",
        "path_td = 'submission_tokendp_512.csv'\n",
        "\n",
        "def must_load(path):\n",
        "    assert os.path.exists(path), f'Missing {path}'\n",
        "    df = pd.read_csv(path)\n",
        "    assert set(df.columns) >= {'id','PredictionString'} and len(df)==len(test), f'Bad shape/cols for {path}'\n",
        "    return df.set_index('id')['PredictionString'].astype(str).to_dict()\n",
        "\n",
        "m28 = must_load(path_28)\n",
        "m29 = must_load(path_29)\n",
        "mTD = must_load(path_td)\n",
        "\n",
        "# Normalization for scoring only (digits/dashes/quotes-insensitive, matches Cell 28)\n",
        "PUNCT = set(list('.,!?;:\\\"\\'()[]{}\u201c\u201d\u2018\u2019\u00ab\u00bb'))\n",
        "DIG_MAP = str.maketrans({\n",
        "    '\u0966':'0','\u0967':'1','\u0968':'2','\u0969':'3','\u096a':'4','\u096b':'5','\u096c':'6','\u096d':'7','\u096e':'8','\u096f':'9',\n",
        "    '\u0be6':'0','\u0be7':'1','\u0be8':'2','\u0be9':'3','\u0bea':'4','\u0beb':'5','\u0bec':'6','\u0bed':'7','\u0bee':'8','\u0bef':'9'\n",
        "})\n",
        "SEP_PAT = re.compile(r'[\\u2010\\u2011\\u2012\\u2013\\u2014\\u2212]+')\n",
        "DANDA = '\\u0964'\n",
        "def norm_text(s: str) -> str:\n",
        "    s = str(s).strip()\n",
        "    s = s.translate(DIG_MAP)\n",
        "    s = SEP_PAT.sub('-', s)\n",
        "    s = s.replace('\\u00A0',' ').replace('\\u2009',' ').replace('\\u200A',' ')\n",
        "    while len(s) >= 2 and ((s[0], s[-1]) in [(\"\\\"\",\"\\\"\"),(\"'\",\"'\"),('\u201c','\u201d'),('\u2018','\u2019'),('(',')'),('[',']'),('{','}'),('\u00ab','\u00bb')]):\n",
        "        s = s[1:-1].strip()\n",
        "    s = s.strip(DANDA)\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    s = s.strip(''.join(PUNCT))\n",
        "    return s.strip()\n",
        "\n",
        "def words_norm(s):\n",
        "    s = norm_text(s)\n",
        "    toks = []\n",
        "    for w in s.split():\n",
        "        w = w.strip(''.join(PUNCT))\n",
        "        if w: toks.append(w)\n",
        "    return toks\n",
        "\n",
        "def jaccard_norm(a, b):\n",
        "    wa, wb = set(words_norm(a)), set(words_norm(b))\n",
        "    if not wa and not wb: return 1.0\n",
        "    if not wa or not wb: return 0.0\n",
        "    inter = len(wa & wb); uni = len(wa | wb)\n",
        "    return inter/uni if uni>0 else 0.0\n",
        "\n",
        "# Per-language length prior from train\n",
        "train = pd.read_csv('train.csv')\n",
        "train['answer_text'] = train['answer_text'].astype(str).str.strip()\n",
        "train['char_len'] = train['answer_text'].str.len().clip(lower=1)\n",
        "def fit_log_normal_params(df):\n",
        "    x = np.log(df['char_len'].values.astype(float))\n",
        "    mu = float(x.mean()); sd = float(x.std());\n",
        "    if sd <= 1e-6: sd = 1e-6\n",
        "    return mu, sd\n",
        "priors = {lang: dict(zip(['mu','sigma'], fit_log_normal_params(g))) for lang, g in train.groupby('language')}\n",
        "def len_prior_dist(s, lang):\n",
        "    L = max(1, len(str(s))); mu = priors.get(lang, {}).get('mu', 2.3)\n",
        "    return abs(math.log(L) - mu)\n",
        "\n",
        "# Numeric/date micro-override: HI len<=18, TA len<=16; normalized numeric/date-like -> pick TD raw\n",
        "num_re = re.compile(r'^[\\d\\u0966-\\u096f\\u0be6-\\u0bef\\s\\-/\\.]+$')\n",
        "\n",
        "# Ultra-safe micro-trim: trim final danda or combining mark if another candidate's normalized form matches the trimmed\n",
        "def apply_micro_trim(chosen: str, lang: str, cand_norms: set):\n",
        "    if not chosen:\n",
        "        return chosen\n",
        "    if lang == 'hindi' and chosen.endswith(DANDA):\n",
        "        trimmed = chosen[:-1].rstrip()\n",
        "        if norm_text(trimmed) in cand_norms:\n",
        "            return trimmed\n",
        "    if len(chosen) >= 2 and ud.category(chosen[-1]) == 'Mn':\n",
        "        trimmed = chosen[:-1]\n",
        "        if norm_text(trimmed) in cand_norms:\n",
        "            return trimmed\n",
        "    return chosen\n",
        "\n",
        "lambda_hi = 0.02\n",
        "lambda_ta = 0.045\n",
        "\n",
        "rows = []\n",
        "t0 = time.time()\n",
        "for qid in id_list:\n",
        "    lang = lang_map[qid]\n",
        "    A = m28[qid]; B = m29[qid]; TD = mTD[qid]\n",
        "    # One-line tweak: if A and B are norm-equivalent but differ raw, prefer B\n",
        "    if A != B and norm_text(A) == norm_text(B): A = B\n",
        "    # Numeric/date override first\n",
        "    if TD:\n",
        "        td_norm = norm_text(TD).replace(DANDA,'')\n",
        "        len_cap = 18 if lang == 'hindi' else 16\n",
        "        if len(TD) <= len_cap and num_re.match(td_norm):\n",
        "            rows.append((qid, TD))\n",
        "            continue\n",
        "    texts = [A, B, TD]\n",
        "    cand_norms = {norm_text(t) for t in texts}\n",
        "    # Majority on normalized strings\n",
        "    from collections import Counter\n",
        "    cnt = Counter([norm_text(t) for t in texts])\n",
        "    top_norm, votes = cnt.most_common(1)[0]\n",
        "    if votes >= 2 and top_norm != '':\n",
        "        for t in texts:\n",
        "            if norm_text(t) == top_norm:\n",
        "                pick = apply_micro_trim(t, lang, cand_norms)\n",
        "                rows.append((qid, pick))\n",
        "                break\n",
        "        continue\n",
        "    # Score by mean normalized Jaccard + small len prior (per-language)\n",
        "    best_i, best_s, best_d, best_L = -1, -1e18, 1e9, 10**9\n",
        "    lam = lambda_hi if lang == 'hindi' else lambda_ta\n",
        "    for i, ti in enumerate(texts):\n",
        "        s = 0.0; cntp = 0\n",
        "        for j, tj in enumerate(texts):\n",
        "            if i==j: continue\n",
        "            s += jaccard_norm(ti, tj); cntp += 1\n",
        "        avg_j = s / max(1, cntp)\n",
        "        dlen = len_prior_dist(ti, lang)\n",
        "        score = avg_j - lam * dlen\n",
        "        L = len(str(ti))\n",
        "        if (score > best_s + 1e-12) or (abs(score - best_s) <= 0.01 and (dlen < best_d or (abs(dlen-best_d) <= 1e-6 and L < best_L))):\n",
        "            best_s, best_i, best_d, best_L = score, i, dlen, L\n",
        "    pick = apply_micro_trim(texts[best_i], lang, cand_norms)\n",
        "    rows.append((qid, pick))\n",
        "\n",
        "sub = pd.DataFrame(rows, columns=['id','PredictionString'])\n",
        "empties = (sub['PredictionString'].astype(str).str.len()==0).sum()\n",
        "mean_len = sub['PredictionString'].astype(str).str.len().mean()\n",
        "print(f'Final meta-hedge (Cell28 vs Cell29 vs TD) built. Empties={int(empties)}, mean_len={mean_len:.2f}, time={time.time()-t0:.2f}s')\n",
        "\n",
        "out_path = 'submission_metahedge_28_29_tokendp.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "pd.read_csv(out_path).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_path)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final meta-hedge (Cell28 vs Cell29 vs TD) built. Empties=0, mean_len=10.71, time=0.01s\nsubmission.csv updated -> submission_metahedge_28_29_tokendp.csv\n"
          ]
        }
      ]
    },
    {
      "id": "ef1dfdad-d93d-4c2b-833d-1d37b62f6b47",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Zero-shot QA (deepset/xlm-roberta-large-squad2) + ultra-safe meta with Cell 34 pick\n",
        "import pandas as pd, numpy as np, time, re, os, unicodedata as ud\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "ids = test['id'].tolist()\n",
        "questions = test['question'].astype(str).tolist()\n",
        "contexts = test['context'].astype(str).tolist()\n",
        "\n",
        "# Load zero-shot QA pipeline (prefer GPU if available)\n",
        "model_name = 'deepset/xlm-roberta-large-squad2'\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "qa = pipeline('question-answering', model=model_name, tokenizer=model_name, device=device)\n",
        "\n",
        "rows = []\n",
        "t0 = time.time()\n",
        "for i, (qid, q, ctx) in enumerate(zip(ids, questions, contexts)):\n",
        "    try:\n",
        "        out = qa({'question': q, 'context': ctx}, handle_impossible_answer=True)\n",
        "        ans = str(out.get('answer', '')).strip()\n",
        "    except Exception:\n",
        "        ans = ''\n",
        "    if not ans:\n",
        "        ans = ctx[:1] if len(ctx) > 0 else ''\n",
        "    rows.append((qid, ans))\n",
        "    if (i+1) % 20 == 0:\n",
        "        print(f'Zero-shot decoded {i+1}/{len(ids)} in {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "sub_zz = pd.DataFrame(rows, columns=['id','PredictionString'])\n",
        "out_path_zz = 'submission_zeroshot_xlmr_squad2.csv'\n",
        "sub_zz.to_csv(out_path_zz, index=False)\n",
        "print('Wrote', out_path_zz, 'empties=', int((sub_zz['PredictionString'].astype(str).str.len()==0).sum()),\n",
        "      'mean_len=', float(sub_zz['PredictionString'].astype(str).str.len().mean()))\n",
        "\n",
        "# Ultra-safe merge: prefer zero-shot raw ONLY if its normalized form equals current meta-hedge pick\n",
        "curr_path = 'submission_metahedge_28_29_tokendp.csv'\n",
        "assert os.path.exists(curr_path), f'Missing {curr_path}'\n",
        "curr = pd.read_csv(curr_path).set_index('id')['PredictionString'].astype(str).to_dict()\n",
        "zz = sub_zz.set_index('id')['PredictionString'].astype(str).to_dict()\n",
        "\n",
        "PUNCT = set(list('.,!?;:\\\"\\'()[]{}\\u201c\\u201d\\u2018\\u2019\\u00ab\\u00bb'))\n",
        "DIG_MAP = str.maketrans({\n",
        "    '\\u0966':'0','\\u0967':'1','\\u0968':'2','\\u0969':'3','\\u096a':'4','\\u096b':'5','\\u096c':'6','\\u096d':'7','\\u096e':'8','\\u096f':'9',\n",
        "    '\\u0be6':'0','\\u0be7':'1','\\u0be8':'2','\\u0be9':'3','\\u0bea':'4','\\u0beb':'5','\\u0bec':'6','\\u0bed':'7','\\u0bee':'8','\\u0bef':'9'\n",
        "})\n",
        "SEP_PAT = re.compile(r'[\\u2010\\u2011\\u2012\\u2013\\u2014\\u2212]+')\n",
        "DANDA = '\\u0964'\n",
        "def norm_text(s: str) -> str:\n",
        "    s = str(s).strip()\n",
        "    s = s.translate(DIG_MAP)\n",
        "    s = SEP_PAT.sub('-', s)\n",
        "    s = s.replace('\\u00A0',' ').replace('\\u2009',' ').replace('\\u200A',' ')\n",
        "    while len(s) >= 2 and ((s[0], s[-1]) in [(\"\\\"\",\"\\\"\"),(\"'\",\"'\"),('\\u201c','\\u201d'),('\\u2018','\\u2019'),('(',')'),('[',']'),('{','}'),('\\u00ab','\\u00bb')]):\n",
        "        s = s[1:-1].strip()\n",
        "    s = s.strip(DANDA)\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "    s = s.strip(''.join(PUNCT))\n",
        "    return s.strip()\n",
        "\n",
        "def apply_micro_trim(chosen: str, lang: str, cand_norms: set):\n",
        "    if not chosen:\n",
        "        return chosen\n",
        "    if lang == 'hindi' and chosen.endswith(DANDA):\n",
        "        trimmed = chosen[:-1].rstrip()\n",
        "        if norm_text(trimmed) in cand_norms:\n",
        "            return trimmed\n",
        "    if len(chosen) >= 2 and ud.category(chosen[-1]) == 'Mn':\n",
        "        trimmed = chosen[:-1]\n",
        "        if norm_text(trimmed) in cand_norms:\n",
        "            return trimmed\n",
        "    return chosen\n",
        "\n",
        "lang_map = dict(zip(test['id'], test['language']))\n",
        "rows2 = []\n",
        "for qid in ids:\n",
        "    base = curr[qid]\n",
        "    z = zz[qid]\n",
        "    lang = lang_map[qid]\n",
        "    cand_norms = {norm_text(base), norm_text(z)}\n",
        "    pick = base\n",
        "    if norm_text(z) == norm_text(base) and z != base:\n",
        "        pick = apply_micro_trim(z, lang, cand_norms)\n",
        "    rows2.append((qid, pick))\n",
        "\n",
        "sub_final = pd.DataFrame(rows2, columns=['id','PredictionString'])\n",
        "empties = (sub_final['PredictionString'].astype(str).str.len()==0).sum()\n",
        "mean_len = sub_final['PredictionString'].astype(str).str.len().mean()\n",
        "print(f'Zero-shot ultra-safe merge done. Empties={int(empties)}, mean_len={mean_len:.2f}')\n",
        "\n",
        "out_merge = 'submission_metahedge_28_29_tokendp_plus_zeroshot.csv'\n",
        "sub_final.to_csv(out_merge, index=False)\n",
        "pd.read_csv(out_merge).to_csv('submission.csv', index=False)\n",
        "print('submission.csv updated ->', out_merge)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-shot decoded 20/112 in 16.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-shot decoded 40/112 in 36.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-shot decoded 60/112 in 55.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-shot decoded 80/112 in 67.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-shot decoded 100/112 in 79.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_zeroshot_xlmr_squad2.csv empties= 0 mean_len= 9.375\nZero-shot ultra-safe merge done. Empties=0, mean_len=10.82\nsubmission.csv updated -> submission_metahedge_28_29_tokendp_plus_zeroshot.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}