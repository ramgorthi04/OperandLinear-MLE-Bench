{
  "cells": [
    {
      "id": "a7cfd273-d537-454e-abfe-9ed5ee850bea",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan: chaii-hindi-and-tamil-question-answering\n",
        "\n",
        "Objectives:\n",
        "- Build a robust extractive QA pipeline for Hindi/Tamil; target word-level Jaccard for medal.\n",
        "- Establish trustworthy CV mirroring test; iterate from a fast baseline to strong transformer fine-tuning; ensemble if time.\n",
        "\n",
        "Process checklist:\n",
        "1) Environment\n",
        "- Verify GPU (nvidia-smi). Install PyTorch CUDA 12.1 stack, transformers, accelerate, datasets, sentencepiece.\n",
        "- Add progress logging and elapsed-time prints in all loops.\n",
        "- Note: Current GPU V100 16GB \u2192 adjust batch sizes accordingly.\n",
        "\n",
        "2) Data & EDA\n",
        "- Load train.csv/test.csv; inspect cols (id, context, question, language, answer_text, answer_start if present).\n",
        "- Basic stats: language distribution, lengths, answer coverage in context, Unicode ranges, punctuation/diacritics.\n",
        "- Validate that answers are substrings of context; handle edge cases (multiple occurrences, whitespace normalization).\n",
        "\n",
        "3) Metric & CV\n",
        "- Implement competition word-level Jaccard scorer; use it (not HF EM/F1) for model selection/OOF.\n",
        "- Build normalized_context_hash: NFKC, remove zero-width (ZWJ/ZWNJ/BOM), collapse spaces, strip.\n",
        "- CV: 5-fold StratifiedGroupKFold with stratify=language and groups=normalized_context_hash (optional extra strat by question length bins).\n",
        "- Save and reuse folds.csv with fixed seed; no leakage (fit transforms per fold).\n",
        "\n",
        "4) Baselines\n",
        "- Rule baseline: exact substring span finder; quick CV to sanity check pipeline.\n",
        "- HF baseline: xlm-roberta-base for quick iteration; target xlm-roberta-large as workhorse; add MuRIL (google/muril-*-cased) for diversity.\n",
        "- Convert data to SQuAD format; use_fast=True tokenizer with return_offsets_mapping=True.\n",
        "\n",
        "5) Training\n",
        "- Mixed precision (fp16), gradient accumulation as needed; V100 16GB typical: seq_len=384, stride=128; bs ~8 (xlm-r-large) / 16\u201332 (base).\n",
        "- Optim: AdamW, lr 2e-5\u20133e-5, warmup ~10%, weight_decay 0.01, max_grad_norm 1.0; optional LLRD ~0.95.\n",
        "- Epochs: 2\u20133 (large), 3\u20134 (base). Save per-fold best by eval Jaccard; store OOF start/end logits and predictions.\n",
        "\n",
        "6) Inference\n",
        "- Sliding windows with return_overflowing_tokens=True; aggregate spans across features per example.\n",
        "- n_best_size 20\u201330; max_answer_length 40\u201364.\n",
        "- Post-process: normalization + realignment to original context; fallback heuristic if needed.\n",
        "\n",
        "7) Ensembling\n",
        "- Average start/end logits across folds/seeds/models; for mixed tokenizers, optionally aggregate to char-level evidence before decoding.\n",
        "- Diverse models: xlm-roberta-large/base + muril-base/large; optional mdeberta-v3-base, IndicBERT v2.\n",
        "\n",
        "8) Normalization & Error analysis\n",
        "- normalize(text): NFKC; remove ZWJ/ZWNJ/BOM; collapse spaces; strip; gentle trim of outer punctuation/quotes/danda; keep diacritics.\n",
        "- Apply normalize() for scoring and predictions (not for training offsets).\n",
        "- Bucket errors by language, lengths; adjust stride/max_answer_len; ensure Unicode hygiene.\n",
        "\n",
        "Milestones with Expert Review:\n",
        "- After this plan.\n",
        "- After EDA + CV definition and folds.csv creation.\n",
        "- After baseline heuristic.\n",
        "- After first transformer OOF.\n",
        "- After any ensemble/tuning.\n",
        "\n",
        "Next:\n",
        "- Install torch cu121 + transformers stack; verify CUDA in torch.\n",
        "- Load data; implement normalize(), Jaccard scorer, and StratifiedGroupKFold by context hash; save folds.csv.\n",
        "- Quick rule baseline; then train xlm-roberta-base (384/128) to establish OOF; iterate to xlm-roberta-large.\n",
        "- Request expert review on CV/OOF before heavier training/ensembles."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "8d44c626-4664-45e7-be7e-5ace9f6e5454",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess, sys, os, time, platform\n",
        "from pathlib import Path\n",
        "\n",
        "print('Python:', sys.version)\n",
        "print('Platform:', platform.platform())\n",
        "print('Checking GPU with nvidia-smi...')\n",
        "t0 = time.time()\n",
        "subprocess.run(['bash','-lc','nvidia-smi || true'], check=False)\n",
        "print('Elapsed: %.2fs' % (time.time()-t0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "d541b40b-5ffd-47ef-8def-f7b2a1e030a0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys, subprocess, os, shutil, time\n",
        "from pathlib import Path\n",
        "\n",
        "def pip(*args):\n",
        "    print('>', *args, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "print('Starting PyTorch cu121 + NLP stack installation...')\n",
        "t0 = time.time()\n",
        "\n",
        "# 0) Uninstall any existing torch stack to avoid conflicts\n",
        "for pkg in ('torch','torchvision','torchaudio'):\n",
        "    try:\n",
        "        subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', pkg], check=False)\n",
        "    except Exception as e:\n",
        "        print('Uninstall error for', pkg, e)\n",
        "\n",
        "# Clean possible shadow dirs (idempotent)\n",
        "for d in (\n",
        "    '/app/.pip-target/torch',\n",
        "    '/app/.pip-target/torchvision',\n",
        "    '/app/.pip-target/torchaudio',\n",
        "    '/app/.pip-target/torch-2.8.0.dist-info',\n",
        "    '/app/.pip-target/torch-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torchvision-0.23.0.dist-info',\n",
        "    '/app/.pip-target/torchvision-0.19.1.dist-info',\n",
        "    '/app/.pip-target/torchaudio-2.8.0.dist-info',\n",
        "    '/app/.pip-target/torchaudio-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torchgen',\n",
        "    '/app/.pip-target/functorch',\n",
        "):\n",
        "    if os.path.exists(d):\n",
        "        print('Removing', d)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "# 1) Install the EXACT cu121 torch stack\n",
        "pip('install',\n",
        "    '--index-url', 'https://download.pytorch.org/whl/cu121',\n",
        "    '--extra-index-url', 'https://pypi.org/simple',\n",
        "    'torch==2.4.1', 'torchvision==0.19.1', 'torchaudio==2.4.1')\n",
        "\n",
        "# 2) Freeze torch versions\n",
        "Path('constraints.txt').write_text('torch==2.4.1\\ntorchvision==0.19.1\\ntorchaudio==2.4.1\\n')\n",
        "\n",
        "# 3) Install NLP deps honoring constraints (avoid upgrading torch)\n",
        "pip('install', '-c', 'constraints.txt',\n",
        "    'transformers==4.44.2', 'accelerate==0.34.2',\n",
        "    'datasets==2.21.0', 'evaluate==0.4.2',\n",
        "    'sentencepiece', 'scikit-learn',\n",
        "    '--upgrade-strategy', 'only-if-needed')\n",
        "\n",
        "# 4) Sanity gate\n",
        "import torch\n",
        "print('torch:', torch.__version__, 'built CUDA:', getattr(torch.version, 'cuda', None))\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "assert str(getattr(torch.version,'cuda','')).startswith('12.1'), f'Wrong CUDA build: {torch.version.cuda}'\n",
        "assert torch.cuda.is_available(), 'CUDA not available after install'\n",
        "print('Install complete in %.1fs' % (time.time()-t0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "6de39b34-c8b8-4888-991f-043280a6220e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd, numpy as np, unicodedata, re, hashlib, os, time\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "\n",
        "# Normalization helpers per expert guidance\n",
        "ZW_CHARS = ''.join([\n",
        "    '\\u200c',  # ZWNJ\n",
        "    '\\u200d',  # ZWJ\n",
        "    '\\ufeff',  # BOM\n",
        "])\n",
        "ZW_RE = re.compile(f'[{re.escape(ZW_CHARS)}]')\n",
        "WS_RE = re.compile(r'\\s+')\n",
        "OUTER_PUNCT_RE = re.compile(r'^[\\s\\\"\\'\\\u201c\\\u201d\\\u2018\\\u2019\\(\\)\\[\\]\\{\\}\\|\u0964,:;.!?-]+|[\\s\\\"\\'\\\u201c\\\u201d\\\u2018\\\u2019\\(\\)\\[\\]\\{\\}\\|\u0964,:;.!?-]+$')\n",
        "\n",
        "def normalize_text(s: str, trim_outer_punct: bool = True) -> str:\n",
        "    if s is None:\n",
        "        return ''\n",
        "    s = unicodedata.normalize('NFKC', str(s))\n",
        "    s = ZW_RE.sub('', s)\n",
        "    s = WS_RE.sub(' ', s).strip()\n",
        "    if trim_outer_punct and s:\n",
        "        s = OUTER_PUNCT_RE.sub('', s).strip()\n",
        "    return s\n",
        "\n",
        "def context_group_hash(s: str) -> str:\n",
        "    norm = normalize_text(s, trim_outer_punct=False)\n",
        "    return hashlib.md5(norm.encode('utf-8')).hexdigest()\n",
        "\n",
        "def jaccard_word_level(a: str, b: str) -> float:\n",
        "    a_n = normalize_text(a)\n",
        "    b_n = normalize_text(b)\n",
        "    if not a_n and not b_n:\n",
        "        return 1.0\n",
        "    a_set = set(a_n.split())\n",
        "    b_set = set(b_n.split())\n",
        "    if not a_set and not b_set:\n",
        "        return 1.0\n",
        "    inter = len(a_set & b_set)\n",
        "    union = len(a_set | b_set)\n",
        "    return inter / union if union else 0.0\n",
        "\n",
        "# Load data\n",
        "t0 = time.time()\n",
        "train_path = 'train.csv'\n",
        "assert os.path.exists(train_path), 'train.csv not found'\n",
        "df = pd.read_csv(train_path)\n",
        "print('Loaded train:', df.shape, 'cols:', list(df.columns))\n",
        "\n",
        "# Basic sanity checks\n",
        "required_cols = ['id','context','question','answer_text']\n",
        "for c in required_cols:\n",
        "    assert c in df.columns, f'Missing column: {c}'\n",
        "lang_col = 'language' if 'language' in df.columns else None\n",
        "if lang_col is None:\n",
        "    # Fallback heuristic: mark unknown\n",
        "    df['language'] = 'unknown'\n",
        "    lang_col = 'language'\n",
        "\n",
        "# Build normalized context hash (group key)\n",
        "df['norm_ctx_hash'] = df['context'].astype(str).map(context_group_hash)\n",
        "\n",
        "# Quick EDA: answer substring coverage\n",
        "ans_in_ctx = (df.apply(lambda r: normalize_text(str(r['answer_text'])) in normalize_text(str(r['context'])), axis=1)).mean()\n",
        "print(f'Answer in normalized context coverage: {ans_in_ctx:.3f}')\n",
        "\n",
        "# Create StratifiedGroupKFold by language with groups = norm_ctx_hash\n",
        "n_splits = 5\n",
        "skf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=2025)\n",
        "y_strat = df[lang_col].astype(str).values\n",
        "groups = df['norm_ctx_hash'].values\n",
        "folds = np.full(len(df), -1, dtype=int)\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(df, y_strat, groups)):\n",
        "    folds[val_idx] = fold\n",
        "assert (folds >= 0).all(), 'Unassigned fold indices present'\n",
        "df['fold'] = folds\n",
        "\n",
        "# Save folds.csv\n",
        "folds_df = df[['id','fold']].copy()\n",
        "folds_df.to_csv('folds.csv', index=False)\n",
        "print('Saved folds.csv with shape:', folds_df.shape)\n",
        "\n",
        "# Report per-fold counts and language distribution\n",
        "for f in range(n_splits):\n",
        "    sub = df[df['fold']==f]\n",
        "    print(f'Fold {f}: n={len(sub)} | lang dist:', sub[lang_col].value_counts(normalize=True).to_dict())\n",
        "\n",
        "print('Done in %.2fs' % (time.time()-t0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "8ce3ce6d-62d7-49de-8e21-5ce46cb861b5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, math, numpy as np, pandas as pd, time, collections, json, random\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Constants for QA preprocessing (retokenize to match model checkpoint)\n",
        "MODEL_NAME = 'deepset/xlm-roberta-large-squad2'\n",
        "MAX_LENGTH = 384\n",
        "DOC_STRIDE = 192\n",
        "MAX_ANSWER_LEN = 64\n",
        "NEG_RATIO = 2  # keep up to 2x negatives per example (after mapping); keep all positives\n",
        "SEED = 2025\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "\n",
        "print('Loading tokenizer:', MODEL_NAME)\n",
        "tok_t0 = time.time()\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "pad_on_right = tokenizer.padding_side == 'right'\n",
        "print('Tokenizer loaded in %.2fs; pad_on_right=%s' % (time.time()-tok_t0, pad_on_right))\n",
        "\n",
        "# Load data and folds\n",
        "df = pd.read_csv('train.csv')\n",
        "folds_df = pd.read_csv('folds.csv')\n",
        "df = df.merge(folds_df, on='id', how='left')\n",
        "assert df['fold'].notna().all(), 'Missing fold assignments'\n",
        "\n",
        "# Select a fold for smoke-prep (fold 0 val) \u2014 full 5-fold will reuse same pipeline\n",
        "FOLD = 0\n",
        "train_df = df[df['fold'] != FOLD].reset_index(drop=True)\n",
        "valid_df = df[df['fold'] == FOLD].reset_index(drop=True)\n",
        "print(f'Fold {FOLD} split: train {len(train_df)} | valid {len(valid_df)}')\n",
        "\n",
        "# Build HF Datasets\n",
        "train_ds = Dataset.from_pandas(train_df[['id','question','context','answer_text','answer_start','language']])\n",
        "valid_ds = Dataset.from_pandas(valid_df[['id','question','context','answer_text','answer_start','language']])\n",
        "\n",
        "# Coverage-based labeling per expert guidance\n",
        "def prepare_train_features(examples):\n",
        "    questions = [q.strip() for q in examples['question']]\n",
        "    contexts = examples['context']\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        questions if pad_on_right else contexts,\n",
        "        contexts if pad_on_right else questions,\n",
        "        truncation='only_second' if pad_on_right else 'only_first',\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "\n",
        "    sample_mapping = tokenized.pop('overflow_to_sample_mapping')\n",
        "    offset_mapping = tokenized.pop('offset_mapping')\n",
        "    input_ids = tokenized['input_ids']\n",
        "\n",
        "    start_positions, end_positions = [], []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        sample_idx = sample_mapping[i]\n",
        "        context_text = contexts[sample_idx]\n",
        "        ans_text = str(examples['answer_text'][sample_idx])\n",
        "        ans_start = int(examples['answer_start'][sample_idx])\n",
        "        ans_end = ans_start + len(ans_text)  # exclusive\n",
        "\n",
        "        # Trim whitespace at boundaries to stabilize mapping\n",
        "        while ans_start < ans_end and ans_start < len(context_text) and context_text[ans_start].isspace():\n",
        "            ans_start += 1\n",
        "        while ans_end > ans_start and ans_end-1 < len(context_text) and context_text[ans_end-1].isspace():\n",
        "            ans_end -= 1\n",
        "        if ans_start >= ans_end:\n",
        "            ans_end = ans_start  # degenerate, will force CLS below if no coverage\n",
        "\n",
        "        seq_ids = tokenized.sequence_ids(i)\n",
        "        try:\n",
        "            cls_index = input_ids[i].index(tokenizer.cls_token_id) if tokenizer.cls_token_id is not None else 0\n",
        "        except ValueError:\n",
        "            cls_index = 0\n",
        "\n",
        "        # Find context token span [ctx_start_tok, ctx_end_tok]\n",
        "        context_id = 1 if pad_on_right else 0\n",
        "        try:\n",
        "            ctx_start_tok = seq_ids.index(context_id)\n",
        "            ctx_end_tok = len(seq_ids) - 1 - seq_ids[::-1].index(context_id)\n",
        "        except ValueError:\n",
        "            start_positions.append(cls_index); end_positions.append(cls_index); continue\n",
        "\n",
        "        # Find first token covering ans_start and last token covering ans_end-1 (inclusive)\n",
        "        start_tok, end_tok = None, None\n",
        "        target_end_inclusive = ans_end - 1\n",
        "\n",
        "        for t in range(ctx_start_tok, ctx_end_tok + 1):\n",
        "            o = offsets[t]\n",
        "            if o is None:\n",
        "                continue\n",
        "            tok_s, tok_e = o  # [tok_s, tok_e)\n",
        "            if tok_s is None or tok_e is None:\n",
        "                continue\n",
        "            if start_tok is None and tok_s <= ans_start < tok_e:\n",
        "                start_tok = t\n",
        "            if tok_s <= target_end_inclusive < tok_e:\n",
        "                end_tok = t\n",
        "\n",
        "        if start_tok is not None and end_tok is not None and start_tok <= end_tok:\n",
        "            start_positions.append(start_tok)\n",
        "            end_positions.append(end_tok)\n",
        "        else:\n",
        "            start_positions.append(cls_index)\n",
        "            end_positions.append(cls_index)\n",
        "\n",
        "    # Keep mapping for downstream sampling\n",
        "    tokenized['start_positions'] = start_positions\n",
        "    tokenized['end_positions'] = end_positions\n",
        "    tokenized['overflow_to_sample_mapping'] = sample_mapping\n",
        "    return tokenized\n",
        "\n",
        "def prepare_validation_features(examples):\n",
        "    questions = [q.strip() for q in examples['question']]\n",
        "    contexts = examples['context']\n",
        "    tokenized = tokenizer(\n",
        "        questions if pad_on_right else contexts,\n",
        "        contexts if pad_on_right else questions,\n",
        "        truncation='only_second' if pad_on_right else 'only_first',\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    sample_mapping = tokenized.pop('overflow_to_sample_mapping')\n",
        "    tokenized['example_id'] = []\n",
        "    new_offsets = []\n",
        "    for i, offsets in enumerate(tokenized['offset_mapping']):\n",
        "        sample_idx = sample_mapping[i]\n",
        "        tokenized['example_id'].append(str(examples['id'][sample_idx]))\n",
        "        sequence_ids = tokenized.sequence_ids(i)\n",
        "        # Set offsets to None for non-context tokens\n",
        "        context_id = 1 if pad_on_right else 0\n",
        "        new_offsets.append([o if sequence_ids[k] == context_id else None for k, o in enumerate(offsets)])\n",
        "    tokenized['offset_mapping'] = new_offsets\n",
        "    return tokenized\n",
        "\n",
        "print('Tokenizing train with sliding windows...')\n",
        "t0 = time.time()\n",
        "train_tokenized = train_ds.map(prepare_train_features, batched=True, remove_columns=train_ds.column_names, desc='train_tokenize')\n",
        "print('Train features (pre-sample):', train_tokenized.num_rows, 'in %.2fs' % (time.time()-t0))\n",
        "\n",
        "# Negative window sampling: keep all positives; for negatives per example, keep up to NEG_RATIO x positives\n",
        "print('Applying negative window sampling ...')\n",
        "smap = np.array(train_tokenized['overflow_to_sample_mapping'])\n",
        "sp = np.array(train_tokenized['start_positions'])\n",
        "is_pos = (sp != 0)  # CLS assumed at position 0 for XLM-R\n",
        "keep_indices = []\n",
        "by_example = collections.defaultdict(list)\n",
        "for idx, ex in enumerate(smap):\n",
        "    by_example[ex].append(idx)\n",
        "for ex, idxs in by_example.items():\n",
        "    idxs = np.array(idxs)\n",
        "    pos_idxs = idxs[is_pos[idxs]]\n",
        "    neg_idxs = idxs[~is_pos[idxs]]\n",
        "    keep_indices.extend(pos_idxs.tolist())\n",
        "    if len(pos_idxs) == 0:\n",
        "        # Fallback: keep a small cap of negatives to avoid dropping the example entirely\n",
        "        sel = neg_idxs[:min(4, len(neg_idxs))]\n",
        "        keep_indices.extend(sel.tolist())\n",
        "    else:\n",
        "        cap = min(len(neg_idxs), NEG_RATIO * len(pos_idxs))\n",
        "        if cap > 0:\n",
        "            sel = neg_idxs[np.random.permutation(len(neg_idxs))[:cap]]\n",
        "            keep_indices.extend(sel.tolist())\n",
        "keep_indices = np.array(sorted(set(keep_indices)))\n",
        "before_n = train_tokenized.num_rows\n",
        "train_tokenized = train_tokenized.select(keep_indices.tolist())\n",
        "after_n = train_tokenized.num_rows\n",
        "print(f'Negative sampling reduced features: {before_n} -> {after_n} ({after_n/before_n:.2%})')\n",
        "\n",
        "print('Tokenizing valid with sliding windows...')\n",
        "t0 = time.time()\n",
        "valid_tokenized = valid_ds.map(prepare_validation_features, batched=True, remove_columns=valid_ds.column_names, desc='valid_tokenize')\n",
        "print('Valid features:', valid_tokenized.num_rows, 'in %.2fs' % (time.time()-t0))\n",
        "\n",
        "# Quick sanity: ensure at least some start/end positions not equal to CLS\n",
        "if 'start_positions' in train_tokenized.features:\n",
        "    sp2 = np.array(train_tokenized['start_positions'])\n",
        "    cls_pos = 0  # typically CLS is position 0 in XLM-R\n",
        "    ratio_cls = float((sp2 == cls_pos).mean())\n",
        "    print('Train start_positions CLS ratio (post-sample):', round(ratio_cls, 4))\n",
        "\n",
        "print('Prep complete. Ready for model training (sanity next).')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "3161423c-2a93-45ae-9f81-8efb17b6b05a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import math, numpy as np, torch, time, os\n",
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "from collections import defaultdict\n",
        "from torch.nn.functional import log_softmax\n",
        "\n",
        "# Environment tweaks to reduce CUDA OOM\n",
        "os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')\n",
        "os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True,max_split_size_mb:64')\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "try:\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Sanity-check training on fold 0 with QA-pretrained LARGE checkpoint\n",
        "MODEL_NAME = 'deepset/xlm-roberta-large-squad2'\n",
        "EPOCHS = 2\n",
        "LR = 2e-5\n",
        "TRAIN_BS = 6  # per advice for V100 16GB\n",
        "EVAL_BS = 16\n",
        "GRAD_ACCUM = 3  # effective batch 18\n",
        "SEED = 2025\n",
        "N_BEST = 25\n",
        "MAX_ANSWER_LEN = 64\n",
        "\n",
        "def set_seed_all(seed: int):\n",
        "    import random, numpy as _np, torch as _torch\n",
        "    random.seed(seed); _np.random.seed(seed); _torch.manual_seed(seed);\n",
        "    if _torch.cuda.is_available():\n",
        "        _torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed_all(SEED)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Reuse tokenizer, train_tokenized, valid_tokenized from previous cell\n",
        "assert 'train_tokenized' in globals() and 'valid_tokenized' in globals(), 'Run preprocessing cell first'\n",
        "\n",
        "# Prepare torch datasets\n",
        "train_tok = train_tokenized.remove_columns([c for c in train_tokenized.column_names if c not in ['input_ids','attention_mask','start_positions','end_positions']])\n",
        "valid_tok_for_pred = valid_tokenized  # keep example_id and offset_mapping in this copy for decoding\n",
        "valid_tok = valid_tokenized.remove_columns([c for c in valid_tokenized.column_names if c not in ['input_ids','attention_mask']])\n",
        "train_tok.set_format(type='torch')\n",
        "valid_tok.set_format(type='torch')\n",
        "\n",
        "print('Loading model:', MODEL_NAME)\n",
        "m_t0 = time.time()\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "try:\n",
        "    model.gradient_checkpointing_enable()\n",
        "except Exception:\n",
        "    pass\n",
        "print('Model loaded in %.2fs' % (time.time()-m_t0))\n",
        "\n",
        "collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='oof_xlmr_large_fold0_sanity',\n",
        "    num_train_epochs=EPOCHS,\n",
        "    learning_rate=LR,\n",
        "    per_device_train_batch_size=TRAIN_BS,\n",
        "    per_device_eval_batch_size=EVAL_BS,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    fp16=True,\n",
        "    logging_steps=100,\n",
        "    save_strategy='no',\n",
        "    evaluation_strategy='no',\n",
        "    seed=SEED,\n",
        "    dataloader_num_workers=2,\n",
        "    report_to=[],\n",
        "    gradient_checkpointing=True,\n",
        "    warmup_ratio=0.1,\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=1.0\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=None,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n",
        "print('Starting training ...')\n",
        "t0 = time.time()\n",
        "trainer.train()\n",
        "print('Train done in %.1fs' % (time.time()-t0))\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Predict on validation features\n",
        "print('Predicting on validation features ...')\n",
        "pred = trainer.predict(valid_tok)\n",
        "start_logits, end_logits = pred.predictions\n",
        "\n",
        "# Decoding with log-softmax, mask non-context, aggregate best per example\n",
        "s_logp = log_softmax(torch.tensor(start_logits), dim=-1).numpy()\n",
        "e_logp = log_softmax(torch.tensor(end_logits), dim=-1).numpy()\n",
        "\n",
        "example_to_best = {}  # example_id (str) -> (score, text)\n",
        "for i in range(len(valid_tok_for_pred)):\n",
        "    example_id = valid_tok_for_pred[i]['example_id']\n",
        "    offsets = valid_tok_for_pred[i]['offset_mapping']\n",
        "    context_text = valid_df.loc[valid_df['id'].astype(str)==example_id, 'context'].values[0]\n",
        "\n",
        "    s = s_logp[i]; e = e_logp[i]\n",
        "    mask = np.array([o is not None for o in offsets], dtype=bool)\n",
        "    s_masked = np.where(mask, s, -1e9)\n",
        "    e_masked = np.where(mask, e, -1e9)\n",
        "\n",
        "    top_starts = np.argsort(s_masked)[-N_BEST:]\n",
        "    top_ends = np.argsort(e_masked)[-N_BEST:]\n",
        "\n",
        "    best = (float('-inf'), '')\n",
        "    for si in top_starts:\n",
        "        for ei in top_ends:\n",
        "            if ei < si:\n",
        "                continue\n",
        "            if (ei - si + 1) > MAX_ANSWER_LEN:\n",
        "                continue\n",
        "            if offsets[si] is None or offsets[ei] is None:\n",
        "                continue\n",
        "            cs, _ = offsets[si]; _, ce = offsets[ei]\n",
        "            if cs is None or ce is None or ce <= cs:\n",
        "                continue\n",
        "            cand_text = context_text[cs:ce]\n",
        "            score = float(s_masked[si] + e_masked[ei])\n",
        "            if score > best[0]:\n",
        "                best = (score, cand_text)\n",
        "\n",
        "    if best[0] == float('-inf'):\n",
        "        best = (-1e9, '')\n",
        "\n",
        "    prev = example_to_best.get(example_id, (float('-inf'), ''))\n",
        "    if best[0] > prev[0]:\n",
        "        example_to_best[example_id] = best\n",
        "\n",
        "# Build predictions and score Jaccard\n",
        "pred_texts = {k: v[1] for k, v in example_to_best.items()}\n",
        "y_true = valid_df.assign(id_str=valid_df['id'].astype(str)).set_index('id_str')['answer_text'].to_dict()\n",
        "\n",
        "assert 'jaccard_word_level' in globals() and 'normalize_text' in globals(), 'Run CV/normalization cell first'\n",
        "scores = []\n",
        "for ex_id, gt in y_true.items():\n",
        "    pr = pred_texts.get(ex_id, '')\n",
        "    scores.append(jaccard_word_level(gt, pr))\n",
        "oof_jaccard = float(np.mean(scores)) if scores else 0.0\n",
        "print(f'Fold 0 OOF Jaccard: {oof_jaccard:.4f} (N={len(scores)})')\n",
        "\n",
        "for k in list(pred_texts.keys())[:5]:\n",
        "    print('ID', k, '| pred:', normalize_text(pred_texts[k])[:80], '| gt:', normalize_text(y_true.get(k, ''))[:80])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "f35b3124-25e6-4f6d-8bc2-879d4e81a1c9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 5-fold training with deepset/xlm-roberta-large-squad2, negative sampling, OOF Jaccard logging + TEST LOGIT SAVING\n",
        "import os, time, numpy as np, pandas as pd, collections, random, math, json, sys\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "from torch.nn.functional import log_softmax\n",
        "\n",
        "SEED = 2025\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "MODEL_NAME = 'deepset/xlm-roberta-large-squad2'\n",
        "MAX_LENGTH = 384\n",
        "DOC_STRIDE = 192\n",
        "MAX_ANSWER_LEN = 64\n",
        "NEG_RATIO = 2\n",
        "EPOCHS = 3\n",
        "LR = 2e-5\n",
        "TRAIN_BS = 4  # reduced to avoid OOM\n",
        "EVAL_BS = 12  # slight reduction\n",
        "GRAD_ACCUM = 5  # keep effective batch reasonable\n",
        "N_BEST = 25\n",
        "\n",
        "print('Retokenizing with', MODEL_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "pad_on_right = tokenizer.padding_side == 'right'\n",
        "\n",
        "full_df = pd.read_csv('train.csv').merge(pd.read_csv('folds.csv'), on='id', how='left')\n",
        "assert full_df['fold'].notna().all()\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "def prepare_train_features_fn(examples):\n",
        "    questions = [q.strip() for q in examples['question']]; contexts = examples['context']\n",
        "    tok = tokenizer(\n",
        "        questions if pad_on_right else contexts,\n",
        "        contexts if pad_on_right else questions,\n",
        "        truncation='only_second' if pad_on_right else 'only_first',\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    smap = tok.pop('overflow_to_sample_mapping')\n",
        "    offsets = tok.pop('offset_mapping')\n",
        "    input_ids = tok['input_ids']\n",
        "    start_positions, end_positions = [], []\n",
        "    for i, off in enumerate(offsets):\n",
        "        si = smap[i]\n",
        "        ctx = contexts[si]\n",
        "        a_text = str(examples['answer_text'][si])\n",
        "        a_start = int(examples['answer_start'][si])\n",
        "        a_end = a_start + len(a_text)\n",
        "        while a_start < a_end and a_start < len(ctx) and ctx[a_start].isspace():\n",
        "            a_start += 1\n",
        "        while a_end > a_start and a_end-1 < len(ctx) and ctx[a_end-1].isspace():\n",
        "            a_end -= 1\n",
        "        if a_start >= a_end:\n",
        "            a_end = a_start\n",
        "        seq_ids = tok.sequence_ids(i)\n",
        "        try:\n",
        "            cls_index = input_ids[i].index(tokenizer.cls_token_id) if tokenizer.cls_token_id is not None else 0\n",
        "        except ValueError:\n",
        "            cls_index = 0\n",
        "        context_id = 1 if pad_on_right else 0\n",
        "        try:\n",
        "            ctx_s = seq_ids.index(context_id)\n",
        "            ctx_e = len(seq_ids) - 1 - seq_ids[::-1].index(context_id)\n",
        "        except ValueError:\n",
        "            start_positions.append(cls_index); end_positions.append(cls_index); continue\n",
        "        st_tok = en_tok = None\n",
        "        tgt_end_incl = a_end - 1\n",
        "        for t in range(ctx_s, ctx_e + 1):\n",
        "            o = off[t]\n",
        "            if o is None:\n",
        "                continue\n",
        "            ts, te = o\n",
        "            if ts is None or te is None:\n",
        "                continue\n",
        "            if st_tok is None and ts <= a_start < te:\n",
        "                st_tok = t\n",
        "            if ts <= tgt_end_incl < te:\n",
        "                en_tok = t\n",
        "        if st_tok is not None and en_tok is not None and st_tok <= en_tok:\n",
        "            start_positions.append(st_tok); end_positions.append(en_tok)\n",
        "        else:\n",
        "            start_positions.append(cls_index); end_positions.append(cls_index)\n",
        "    tok['start_positions'] = start_positions; tok['end_positions'] = end_positions; tok['overflow_to_sample_mapping'] = smap\n",
        "    return tok\n",
        "\n",
        "def prepare_pred_features_fn(examples):\n",
        "    questions = [q.strip() for q in examples['question']]; contexts = examples['context']\n",
        "    tok = tokenizer(\n",
        "        questions if pad_on_right else contexts,\n",
        "        contexts if pad_on_right else questions,\n",
        "        truncation='only_second' if pad_on_right else 'only_first',\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    smap = tok.pop('overflow_to_sample_mapping')\n",
        "    tok['example_id'] = []\n",
        "    new_offsets = []\n",
        "    for i, off in enumerate(tok['offset_mapping']):\n",
        "        si = smap[i]\n",
        "        tok['example_id'].append(str(examples['id'][si]))\n",
        "        seq_ids = tok.sequence_ids(i)\n",
        "        context_id = 1 if pad_on_right else 0\n",
        "        new_offsets.append([o if seq_ids[k] == context_id else None for k, o in enumerate(off)])\n",
        "    tok['offset_mapping'] = new_offsets\n",
        "    return tok\n",
        "\n",
        "# Pre-tokenize TEST once; feature order is deterministic\n",
        "test_ds = Dataset.from_pandas(test_df[['id','question','context','language']])\n",
        "test_tok_all = test_ds.map(prepare_pred_features_fn, batched=True, remove_columns=test_ds.column_names, desc='test_tokenize_allfolds')\n",
        "te_ds_pred = test_tok_all.remove_columns([c for c in test_tok_all.column_names if c not in ['input_ids','attention_mask']])\n",
        "te_ds_pred.set_format(type='torch')\n",
        "\n",
        "# Persist example_id and offsets once for downstream char-level aggregation\n",
        "os.makedirs('xlmr_large_test_logits', exist_ok=True)\n",
        "with open('xlmr_large_test_logits/test_example_id.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(list(test_tok_all['example_id']), f, ensure_ascii=False)\n",
        "np.save('xlmr_large_test_logits/test_offset_mapping.npy', np.array(test_tok_all['offset_mapping'], dtype=object), allow_pickle=True)\n",
        "\n",
        "all_oof_scores = []; oof_rows = []; test_start_list = []; test_end_list = [];\n",
        "t_global = time.time()\n",
        "for FOLD in range(5):\n",
        "    t_fold = time.time()\n",
        "    print(f'\\n===== Fold {FOLD} =====')\n",
        "    tr_df = full_df[full_df['fold'] != FOLD].reset_index(drop=True)\n",
        "    va_df = full_df[full_df['fold'] == FOLD].reset_index(drop=True)\n",
        "    tr_ds = Dataset.from_pandas(tr_df[['id','question','context','answer_text','answer_start','language']])\n",
        "    va_ds = Dataset.from_pandas(va_df[['id','question','context','answer_text','answer_start','language']])\n",
        "    t0 = time.time()\n",
        "    tr_tok = tr_ds.map(prepare_train_features_fn, batched=True, remove_columns=tr_ds.column_names, desc=f'train_tokenize_f{FOLD}')\n",
        "    va_tok_all = va_ds.map(prepare_pred_features_fn, batched=True, remove_columns=va_ds.column_names, desc=f'valid_tokenize_f{FOLD}')\n",
        "    print(f'Fold {FOLD}: train feats pre-sample={tr_tok.num_rows} | valid feats={va_tok_all.num_rows} | tok_time={time.time()-t0:.1f}s')\n",
        "    # Negative sampling\n",
        "    smap = np.array(tr_tok['overflow_to_sample_mapping']); sp = np.array(tr_tok['start_positions']); is_pos = (sp != 0)\n",
        "    keep = []; by_ex = collections.defaultdict(list)\n",
        "    for idx, ex in enumerate(smap):\n",
        "        by_ex[ex].append(idx)\n",
        "    for ex, idxs in by_ex.items():\n",
        "        idxs = np.array(idxs); pos = idxs[is_pos[idxs]]; neg = idxs[~is_pos[idxs]]\n",
        "        keep.extend(pos.tolist())\n",
        "        if len(pos) == 0:\n",
        "            sel = neg[:min(4, len(neg))]; keep.extend(sel.tolist())\n",
        "        else:\n",
        "            cap = min(len(neg), NEG_RATIO*len(pos))\n",
        "            if cap > 0:\n",
        "                sel = neg[np.random.permutation(len(neg))[:cap]]; keep.extend(sel.tolist())\n",
        "    keep = np.array(sorted(set(keep))); before = tr_tok.num_rows\n",
        "    tr_tok = tr_tok.select(keep.tolist()); after = tr_tok.num_rows\n",
        "    print(f'Fold {FOLD}: neg-sample {before}->{after} ({after/before:.2%})')\n",
        "    # Format for Trainer\n",
        "    tr_ds_torch = tr_tok.remove_columns([c for c in tr_tok.column_names if c not in ['input_ids','attention_mask','start_positions','end_positions']])\n",
        "    tr_ds_torch.set_format(type='torch')\n",
        "    va_ds_pred = va_tok_all.remove_columns([c for c in va_tok_all.column_names if c not in ['input_ids','attention_mask']])\n",
        "    va_ds_pred.set_format(type='torch')\n",
        "    # Model/Trainer\n",
        "    torch.cuda.empty_cache()\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "    try:\n",
        "        model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'xlmr_large_f{FOLD}',\n",
        "        num_train_epochs=EPOCHS,\n",
        "        learning_rate=LR,\n",
        "        per_device_train_batch_size=TRAIN_BS,\n",
        "        per_device_eval_batch_size=EVAL_BS,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM,\n",
        "        fp16=True,\n",
        "        logging_steps=100,\n",
        "        save_strategy='no',\n",
        "        evaluation_strategy='no',\n",
        "        seed=SEED,\n",
        "        dataloader_num_workers=2,\n",
        "        report_to=[],\n",
        "        gradient_checkpointing=True,\n",
        "        warmup_ratio=0.1,\n",
        "        weight_decay=0.01,\n",
        "        max_grad_norm=1.0\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tr_ds_torch,\n",
        "        eval_dataset=None,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer),\n",
        "    )\n",
        "    print(f'Fold {FOLD}: training...')\n",
        "    torch.cuda.empty_cache()\n",
        "    t1 = time.time()\n",
        "    trainer.train()\n",
        "    print(f'Fold {FOLD}: train_done in {time.time()-t1:.1f}s | total_elapsed={time.time()-t_fold:.1f}s')\n",
        "    # Predict on valid\n",
        "    print(f'Fold {FOLD}: predicting valid...')\n",
        "    pred = trainer.predict(va_ds_pred)\n",
        "    s_logp = log_softmax(torch.tensor(pred.predictions[0]), dim=-1).numpy()\n",
        "    e_logp = log_softmax(torch.tensor(pred.predictions[1]), dim=-1).numpy()\n",
        "    # Decode\n",
        "    ex_best = {}\n",
        "    for i in range(len(va_tok_all)):\n",
        "        ex_id = va_tok_all[i]['example_id']\n",
        "        offsets = va_tok_all[i]['offset_mapping']\n",
        "        ctx = va_df.loc[va_df['id'].astype(str)==ex_id, 'context'].values[0]\n",
        "        s = s_logp[i]; e = e_logp[i]\n",
        "        mask = np.array([o is not None for o in offsets], dtype=bool)\n",
        "        s_m = np.where(mask, s, -1e9); e_m = np.where(mask, e, -1e9)\n",
        "        top_s = np.argsort(s_m)[-N_BEST:]; top_e = np.argsort(e_m)[-N_BEST:]\n",
        "        best = (float('-inf'), '')\n",
        "        for si in top_s:\n",
        "            for ei in top_e:\n",
        "                if ei < si: continue\n",
        "                if (ei - si + 1) > MAX_ANSWER_LEN: continue\n",
        "                if offsets[si] is None or offsets[ei] is None: continue\n",
        "                cs,_ = offsets[si]; _,ce = offsets[ei]\n",
        "                if cs is None or ce is None or ce <= cs: continue\n",
        "                cand = ctx[cs:ce]; score = float(s_m[si] + e_m[ei])\n",
        "                if score > best[0]: best = (score, cand)\n",
        "        if best[0] == float('-inf'): best = (-1e9, '')\n",
        "        prev = ex_best.get(ex_id, (float('-inf'), ''))\n",
        "        if best[0] > prev[0]: ex_best[ex_id] = best\n",
        "    preds = {k:v[1] for k,v in ex_best.items()}\n",
        "    y_true = va_df.assign(id_str=va_df['id'].astype(str)).set_index('id_str')['answer_text'].to_dict()\n",
        "    fold_scores = [jaccard_word_level(y_true[k], preds.get(k, '')) for k in y_true.keys()]\n",
        "    f_j = float(np.mean(fold_scores)) if fold_scores else 0.0\n",
        "    all_oof_scores.append(f_j)\n",
        "    print(f'Fold {FOLD} Jaccard: {f_j:.4f} | elapsed_fold={time.time()-t_fold:.1f}s')\n",
        "    for k,v in preds.items():\n",
        "        oof_rows.append({'id': k, 'pred_text': v, 'gt': y_true.get(k, ''), 'fold': FOLD})\n",
        "\n",
        "    # Predict TEST and save per-fold logits\n",
        "    print(f'Fold {FOLD}: predicting TEST and saving logits...')\n",
        "    te_pred = trainer.predict(te_ds_pred)\n",
        "    te_start = te_pred.predictions[0]\n",
        "    te_end = te_pred.predictions[1]\n",
        "    # Save raw logits per fold\n",
        "    np.save(f'xlmr_large_test_logits/test_start_logits_f{FOLD}.npy', te_start)\n",
        "    np.save(f'xlmr_large_test_logits/test_end_logits_f{FOLD}.npy', te_end)\n",
        "    # Keep in memory for averaging, use logit space (not softmax)\n",
        "    test_start_list.append(te_start)\n",
        "    test_end_list.append(te_end)\n",
        "\n",
        "    # free\n",
        "    del model, trainer, tr_ds_torch, va_ds_pred, tr_tok, va_tok_all, pred, s_logp, e_logp, te_pred\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Save OOF preds\n",
        "oof_df = pd.DataFrame(oof_rows)\n",
        "oof_df.to_csv('oof_preds_xlmr_large.csv', index=False)\n",
        "mean_oof = float(np.mean(all_oof_scores)) if all_oof_scores else 0.0\n",
        "print('5-fold OOF Jaccard (mean across folds):', round(mean_oof, 4))\n",
        "\n",
        "# Average TEST logits across folds and persist for downstream ensembling\n",
        "print('Averaging TEST logits across folds...')\n",
        "start_stack = np.stack(test_start_list, axis=0)  # [fold, n_feat, L]\n",
        "end_stack = np.stack(test_end_list, axis=0)\n",
        "start_avg = start_stack.mean(axis=0)\n",
        "end_avg = end_stack.mean(axis=0)\n",
        "np.savez_compressed('xlmr_large_test_avg.npz', start=start_avg, end=end_avg)\n",
        "print('Saved averaged TEST logits to xlmr_large_test_avg.npz; example_id/offsets saved under xlmr_large_test_logits/')\n",
        "print('Done 5-fold in %.1fs' % (time.time()-t_global))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "fc988f8f-e0e5-4ca6-9cb1-0992768dd7b2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train on full training set and predict test to create submission.csv\n",
        "import os, time, numpy as np, pandas as pd, random, collections, torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "from torch.nn.functional import log_softmax\n",
        "\n",
        "SEED = 2025\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "MODEL_NAME = 'deepset/xlm-roberta-large-squad2'\n",
        "MAX_LENGTH = 384\n",
        "DOC_STRIDE = 192\n",
        "MAX_ANSWER_LEN = 64\n",
        "NEG_RATIO = 2\n",
        "EPOCHS = 3\n",
        "LR = 2e-5\n",
        "TRAIN_BS = 6\n",
        "EVAL_BS = 16\n",
        "GRAD_ACCUM = 3\n",
        "N_BEST = 25\n",
        "\n",
        "os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')\n",
        "os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True,max_split_size_mb:64')\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "try: torch.set_float32_matmul_precision('high')\n",
        "except Exception: pass\n",
        "\n",
        "print('Loading data ...')\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "print('Train:', train_df.shape, '| Test:', test_df.shape)\n",
        "\n",
        "print('Loading tokenizer/model:', MODEL_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "pad_on_right = tokenizer.padding_side == 'right'\n",
        "\n",
        "def prepare_train_features_full(examples):\n",
        "    questions = [q.strip() for q in examples['question']]; contexts = examples['context']\n",
        "    tok = tokenizer(\n",
        "        questions if pad_on_right else contexts,\n",
        "        contexts if pad_on_right else questions,\n",
        "        truncation='only_second' if pad_on_right else 'only_first',\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    smap = tok.pop('overflow_to_sample_mapping')\n",
        "    offsets = tok.pop('offset_mapping')\n",
        "    input_ids = tok['input_ids']\n",
        "    start_positions, end_positions = [], []\n",
        "    for i, off in enumerate(offsets):\n",
        "        si = smap[i]\n",
        "        ctx = contexts[si]\n",
        "        a_text = str(examples['answer_text'][si])\n",
        "        a_start = int(examples['answer_start'][si])\n",
        "        a_end = a_start + len(a_text)\n",
        "        while a_start < a_end and a_start < len(ctx) and ctx[a_start].isspace(): a_start += 1\n",
        "        while a_end > a_start and a_end-1 < len(ctx) and ctx[a_end-1].isspace(): a_end -= 1\n",
        "        if a_start >= a_end: a_end = a_start\n",
        "        seq_ids = tok.sequence_ids(i)\n",
        "        try: cls_index = input_ids[i].index(tokenizer.cls_token_id) if tokenizer.cls_token_id is not None else 0\n",
        "        except ValueError: cls_index = 0\n",
        "        context_id = 1 if pad_on_right else 0\n",
        "        try:\n",
        "            ctx_s = seq_ids.index(context_id)\n",
        "            ctx_e = len(seq_ids) - 1 - seq_ids[::-1].index(context_id)\n",
        "        except ValueError:\n",
        "            start_positions.append(cls_index); end_positions.append(cls_index); continue\n",
        "        st_tok = en_tok = None\n",
        "        tgt_end_incl = a_end - 1\n",
        "        for t in range(ctx_s, ctx_e + 1):\n",
        "            o = off[t]\n",
        "            if o is None: continue\n",
        "            ts, te = o\n",
        "            if ts is None or te is None: continue\n",
        "            if st_tok is None and ts <= a_start < te: st_tok = t\n",
        "            if ts <= tgt_end_incl < te: en_tok = t\n",
        "        if st_tok is not None and en_tok is not None and st_tok <= en_tok:\n",
        "            start_positions.append(st_tok); end_positions.append(en_tok)\n",
        "        else:\n",
        "            start_positions.append(cls_index); end_positions.append(cls_index)\n",
        "    tok['start_positions'] = start_positions; tok['end_positions'] = end_positions; tok['overflow_to_sample_mapping'] = smap\n",
        "    return tok\n",
        "\n",
        "def prepare_test_features(examples):\n",
        "    questions = [q.strip() for q in examples['question']]; contexts = examples['context']\n",
        "    tok = tokenizer(\n",
        "        questions if pad_on_right else contexts,\n",
        "        contexts if pad_on_right else questions,\n",
        "        truncation='only_second' if pad_on_right else 'only_first',\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    smap = tok.pop('overflow_to_sample_mapping')\n",
        "    tok['example_id'] = []\n",
        "    new_offsets = []\n",
        "    for i, off in enumerate(tok['offset_mapping']):\n",
        "        si = smap[i]\n",
        "        tok['example_id'].append(str(examples['id'][si]))\n",
        "        seq_ids = tok.sequence_ids(i)\n",
        "        context_id = 1 if pad_on_right else 0\n",
        "        new_offsets.append([o if seq_ids[k] == context_id else None for k, o in enumerate(off)])\n",
        "    tok['offset_mapping'] = new_offsets\n",
        "    return tok\n",
        "\n",
        "print('Building HF datasets ...')\n",
        "full_tr_ds = Dataset.from_pandas(train_df[['id','question','context','answer_text','answer_start','language']])\n",
        "test_ds = Dataset.from_pandas(test_df[['id','question','context','language']])\n",
        "\n",
        "t0 = time.time()\n",
        "full_tr_tok = full_tr_ds.map(prepare_train_features_full, batched=True, remove_columns=full_tr_ds.column_names, desc='train_tokenize_full')\n",
        "print('Full train feats pre-sample=', full_tr_tok.num_rows, 'tok_time=%.1fs' % (time.time()-t0))\n",
        "\n",
        "# Negative sampling as in CV\n",
        "smap = np.array(full_tr_tok['overflow_to_sample_mapping']); sp = np.array(full_tr_tok['start_positions']); is_pos = (sp != 0)\n",
        "keep = []; by_ex = collections.defaultdict(list)\n",
        "for idx, ex in enumerate(smap): by_ex[ex].append(idx)\n",
        "for ex, idxs in by_ex.items():\n",
        "    idxs = np.array(idxs); pos = idxs[is_pos[idxs]]; neg = idxs[~is_pos[idxs]]\n",
        "    keep.extend(pos.tolist())\n",
        "    if len(pos) == 0:\n",
        "        sel = neg[:min(4, len(neg))]; keep.extend(sel.tolist())\n",
        "    else:\n",
        "        cap = min(len(neg), NEG_RATIO*len(pos))\n",
        "        if cap > 0:\n",
        "            sel = neg[np.random.permutation(len(neg))[:cap]]; keep.extend(sel.tolist())\n",
        "keep = np.array(sorted(set(keep))); before = full_tr_tok.num_rows\n",
        "full_tr_tok = full_tr_tok.select(keep.tolist()); after = full_tr_tok.num_rows\n",
        "print(f'Neg-sample {before}->{after} ({after/before:.2%})')\n",
        "\n",
        "print('Tokenizing test ...')\n",
        "t1 = time.time()\n",
        "test_tok_all = test_ds.map(prepare_test_features, batched=True, remove_columns=test_ds.column_names, desc='test_tokenize')\n",
        "print('Test feats:', test_tok_all.num_rows, 'tok_time=%.1fs' % (time.time()-t1))\n",
        "\n",
        "tr_ds_torch = full_tr_tok.remove_columns([c for c in full_tr_tok.column_names if c not in ['input_ids','attention_mask','start_positions','end_positions']])\n",
        "tr_ds_torch.set_format(type='torch')\n",
        "te_ds_pred = test_tok_all.remove_columns([c for c in test_tok_all.column_names if c not in ['input_ids','attention_mask']])\n",
        "te_ds_pred.set_format(type='torch')\n",
        "\n",
        "print('Loading model ...')\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "try: model.gradient_checkpointing_enable()\n",
        "except Exception: pass\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='xlmr_large_full',\n",
        "    num_train_epochs=EPOCHS,\n",
        "    learning_rate=LR,\n",
        "    per_device_train_batch_size=TRAIN_BS,\n",
        "    per_device_eval_batch_size=EVAL_BS,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    fp16=True,\n",
        "    logging_steps=200,\n",
        "    save_strategy='no',\n",
        "    evaluation_strategy='no',\n",
        "    seed=SEED,\n",
        "    dataloader_num_workers=2,\n",
        "    report_to=[],\n",
        "    gradient_checkpointing=True,\n",
        "    warmup_ratio=0.1,\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=1.0\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tr_ds_torch,\n",
        "    eval_dataset=None,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer),\n",
        ")\n",
        "\n",
        "print('Training on full data ...')\n",
        "t2 = time.time()\n",
        "trainer.train()\n",
        "print('Full training done in %.1fs' % (time.time()-t2))\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print('Predicting test ...')\n",
        "pred = trainer.predict(te_ds_pred)\n",
        "s_logp = log_softmax(torch.tensor(pred.predictions[0]), dim=-1).numpy()\n",
        "e_logp = log_softmax(torch.tensor(pred.predictions[1]), dim=-1).numpy()\n",
        "\n",
        "# Decode per example\n",
        "ex_best = {}\n",
        "for i in range(len(test_tok_all)):\n",
        "    ex_id = test_tok_all[i]['example_id']\n",
        "    offsets = test_tok_all[i]['offset_mapping']\n",
        "    ctx = test_df.loc[test_df['id'].astype(str)==ex_id, 'context'].values[0]\n",
        "    s = s_logp[i]; e = e_logp[i]\n",
        "    mask = np.array([o is not None for o in offsets], dtype=bool)\n",
        "    s_m = np.where(mask, s, -1e9); e_m = np.where(mask, e, -1e9)\n",
        "    top_s = np.argsort(s_m)[-N_BEST:]; top_e = np.argsort(e_m)[-N_BEST:]\n",
        "    best = (float('-inf'), '')\n",
        "    for si in top_s:\n",
        "        for ei in top_e:\n",
        "            if ei < si: continue\n",
        "            if (ei - si + 1) > MAX_ANSWER_LEN: continue\n",
        "            if offsets[si] is None or offsets[ei] is None: continue\n",
        "            cs,_ = offsets[si]; _,ce = offsets[ei]\n",
        "            if cs is None or ce is None or ce <= cs: continue\n",
        "            cand = ctx[cs:ce]\n",
        "            score = float(s_m[si] + e_m[ei])\n",
        "            if score > best[0]: best = (score, cand)\n",
        "    if best[0] == float('-inf'): best = (-1e9, '')\n",
        "    prev = ex_best.get(ex_id, (float('-inf'), ''))\n",
        "    if best[0] > prev[0]: ex_best[ex_id] = best\n",
        "\n",
        "test_preds = {k:v[1] for k,v in ex_best.items()}\n",
        "sub = pd.DataFrame({'id': test_df['id'].astype(str)})\n",
        "sub['PredictionString'] = sub['id'].map(test_preds).fillna('')\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with shape', sub.shape)\n",
        "sub.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "87bf9142-cdbf-4ec5-b377-9b031880b295",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 5-fold training with google/muril-large-cased + TEST LOGIT SAVING (mirror of XLM-R large)\n",
        "import os, time, numpy as np, pandas as pd, collections, random, json, torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "from torch.nn.functional import log_softmax\n",
        "\n",
        "SEED = 2025\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "MODEL_NAME = 'google/muril-large-cased'\n",
        "MAX_LENGTH = 384\n",
        "DOC_STRIDE = 192\n",
        "MAX_ANSWER_LEN = 64\n",
        "NEG_RATIO = 2\n",
        "EPOCHS = 3\n",
        "LR = 2e-5\n",
        "TRAIN_BS = 6\n",
        "EVAL_BS = 16\n",
        "GRAD_ACCUM = 3\n",
        "N_BEST = 30\n",
        "\n",
        "print('Retokenizing with', MODEL_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "pad_on_right = tokenizer.padding_side == 'right'\n",
        "\n",
        "full_df = pd.read_csv('train.csv').merge(pd.read_csv('folds.csv'), on='id', how='left')\n",
        "assert full_df['fold'].notna().all()\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "def prepare_train_features_fn(examples):\n",
        "    questions = [q.strip() for q in examples['question']]; contexts = examples['context']\n",
        "    tok = tokenizer(\n",
        "        questions if pad_on_right else contexts,\n",
        "        contexts if pad_on_right else questions,\n",
        "        truncation='only_second' if pad_on_right else 'only_first',\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    smap = tok.pop('overflow_to_sample_mapping')\n",
        "    offsets = tok.pop('offset_mapping')\n",
        "    input_ids = tok['input_ids']\n",
        "    start_positions, end_positions = [], []\n",
        "    cls_positions = []  # track per-feature CLS index for robust negative sampling\n",
        "    for i, off in enumerate(offsets):\n",
        "        si = smap[i]\n",
        "        ctx = contexts[si]\n",
        "        a_text = str(examples['answer_text'][si])\n",
        "        a_start = int(examples['answer_start'][si])\n",
        "        a_end = a_start + len(a_text)\n",
        "        while a_start < a_end and a_start < len(ctx) and ctx[a_start].isspace(): a_start += 1\n",
        "        while a_end > a_start and a_end-1 < len(ctx) and ctx[a_end-1].isspace(): a_end -= 1\n",
        "        if a_start >= a_end: a_end = a_start\n",
        "        seq_ids = tok.sequence_ids(i)\n",
        "        try:\n",
        "            cls_index = input_ids[i].index(tokenizer.cls_token_id) if tokenizer.cls_token_id is not None else 0\n",
        "        except ValueError:\n",
        "            cls_index = 0\n",
        "        cls_positions.append(cls_index)\n",
        "        context_id = 1 if pad_on_right else 0\n",
        "        try:\n",
        "            ctx_s = seq_ids.index(context_id)\n",
        "            ctx_e = len(seq_ids) - 1 - seq_ids[::-1].index(context_id)\n",
        "        except ValueError:\n",
        "            start_positions.append(cls_index); end_positions.append(cls_index); continue\n",
        "        st_tok = en_tok = None\n",
        "        tgt_end_incl = a_end - 1\n",
        "        for t in range(ctx_s, ctx_e + 1):\n",
        "            o = off[t]\n",
        "            if o is None: continue\n",
        "            ts, te = o\n",
        "            if ts is None or te is None: continue\n",
        "            if st_tok is None and ts <= a_start < te: st_tok = t\n",
        "            if ts <= tgt_end_incl < te: en_tok = t\n",
        "        if st_tok is not None and en_tok is not None and st_tok <= en_tok:\n",
        "            start_positions.append(st_tok); end_positions.append(en_tok)\n",
        "        else:\n",
        "            start_positions.append(cls_index); end_positions.append(cls_index)\n",
        "    tok['start_positions'] = start_positions; tok['end_positions'] = end_positions; tok['overflow_to_sample_mapping'] = smap; tok['cls_positions'] = cls_positions\n",
        "    return tok\n",
        "\n",
        "def prepare_pred_features_fn(examples):\n",
        "    questions = [q.strip() for q in examples['question']]; contexts = examples['context']\n",
        "    tok = tokenizer(\n",
        "        questions if pad_on_right else contexts,\n",
        "        contexts if pad_on_right else questions,\n",
        "        truncation='only_second' if pad_on_right else 'only_first',\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    smap = tok.pop('overflow_to_sample_mapping')\n",
        "    tok['example_id'] = []\n",
        "    new_offsets = []\n",
        "    for i, off in enumerate(tok['offset_mapping']):\n",
        "        si = smap[i]\n",
        "        tok['example_id'].append(str(examples['id'][si]))\n",
        "        seq_ids = tok.sequence_ids(i)\n",
        "        context_id = 1 if pad_on_right else 0\n",
        "        new_offsets.append([o if seq_ids[k] == context_id else None for k, o in enumerate(off)])\n",
        "    tok['offset_mapping'] = new_offsets\n",
        "    return tok\n",
        "\n",
        "# Pre-tokenize TEST once\n",
        "test_ds = Dataset.from_pandas(test_df[['id','question','context','language']])\n",
        "test_tok_all = test_ds.map(prepare_pred_features_fn, batched=True, remove_columns=test_ds.column_names, desc='muril_test_tokenize_allfolds')\n",
        "te_ds_pred = test_tok_all.remove_columns([c for c in test_tok_all.column_names if c not in ['input_ids','attention_mask']])\n",
        "te_ds_pred.set_format(type='torch')\n",
        "\n",
        "os.makedirs('muril_large_test_logits', exist_ok=True)\n",
        "with open('muril_large_test_logits/test_example_id.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(list(test_tok_all['example_id']), f, ensure_ascii=False)\n",
        "np.save('muril_large_test_logits/test_offset_mapping.npy', np.array(test_tok_all['offset_mapping'], dtype=object), allow_pickle=True)\n",
        "\n",
        "all_oof_scores = []; oof_rows = []; test_start_list = []; test_end_list = [];\n",
        "t_global = time.time()\n",
        "for FOLD in range(5):\n",
        "    t_fold = time.time()\n",
        "    print(f'\\n===== MuRIL Fold {FOLD} =====')\n",
        "    tr_df = full_df[full_df['fold'] != FOLD].reset_index(drop=True)\n",
        "    va_df = full_df[full_df['fold'] == FOLD].reset_index(drop=True)\n",
        "    tr_ds = Dataset.from_pandas(tr_df[['id','question','context','answer_text','answer_start','language']])\n",
        "    va_ds = Dataset.from_pandas(va_df[['id','question','context','answer_text','answer_start','language']])\n",
        "    t0 = time.time()\n",
        "    tr_tok = tr_ds.map(prepare_train_features_fn, batched=True, remove_columns=tr_ds.column_names, desc=f'muril_train_tokenize_f{FOLD}')\n",
        "    va_tok_all = va_ds.map(prepare_pred_features_fn, batched=True, remove_columns=va_ds.column_names, desc=f'muril_valid_tokenize_f{FOLD}')\n",
        "    print(f'MuRIL Fold {FOLD}: train feats pre-sample={tr_tok.num_rows} | valid feats={va_tok_all.num_rows} | tok_time={time.time()-t0:.1f}s')\n",
        "    # Negative sampling (robust to CLS index)\n",
        "    smap = np.array(tr_tok['overflow_to_sample_mapping'])\n",
        "    sp = np.array(tr_tok['start_positions'])\n",
        "    cls_arr = np.array(tr_tok['cls_positions'])\n",
        "    is_pos = (sp != cls_arr)\n",
        "    keep = []; by_ex = collections.defaultdict(list)\n",
        "    for idx, ex in enumerate(smap): by_ex[ex].append(idx)\n",
        "    for ex, idxs in by_ex.items():\n",
        "        idxs = np.array(idxs); pos = idxs[is_pos[idxs]]; neg = idxs[~is_pos[idxs]]\n",
        "        keep.extend(pos.tolist())\n",
        "        if len(pos) == 0:\n",
        "            sel = neg[:min(4, len(neg))]; keep.extend(sel.tolist())\n",
        "        else:\n",
        "            cap = min(len(neg), NEG_RATIO*len(pos))\n",
        "            if cap > 0:\n",
        "                sel = neg[np.random.permutation(len(neg))[:cap]]; keep.extend(sel.tolist())\n",
        "    keep = np.array(sorted(set(keep))); before = tr_tok.num_rows\n",
        "    tr_tok = tr_tok.select(keep.tolist()); after = tr_tok.num_rows\n",
        "    print(f'MuRIL Fold {FOLD}: neg-sample {before}->{after} ({after/before:.2%})')\n",
        "    # Datasets\n",
        "    tr_ds_torch = tr_tok.remove_columns([c for c in tr_tok.column_names if c not in ['input_ids','attention_mask','start_positions','end_positions']])\n",
        "    tr_ds_torch.set_format(type='torch')\n",
        "    va_ds_pred = va_tok_all.remove_columns([c for c in va_tok_all.column_names if c not in ['input_ids','attention_mask']])\n",
        "    va_ds_pred.set_format(type='torch')\n",
        "    # Model\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "    try: model.gradient_checkpointing_enable()\n",
        "    except Exception: pass\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'muril_large_f{FOLD}',\n",
        "        num_train_epochs=EPOCHS,\n",
        "        learning_rate=LR,\n",
        "        per_device_train_batch_size=TRAIN_BS,\n",
        "        per_device_eval_batch_size=EVAL_BS,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM,\n",
        "        fp16=True,\n",
        "        logging_steps=250,\n",
        "        save_strategy='no',\n",
        "        evaluation_strategy='no',\n",
        "        seed=SEED,\n",
        "        dataloader_num_workers=2,\n",
        "        report_to=[],\n",
        "        gradient_checkpointing=True,\n",
        "        warmup_ratio=0.1,\n",
        "        weight_decay=0.01,\n",
        "        max_grad_norm=1.0\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tr_ds_torch,\n",
        "        eval_dataset=None,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer),\n",
        "    )\n",
        "    print(f'MuRIL Fold {FOLD}: training...')\n",
        "    t1 = time.time()\n",
        "    trainer.train()\n",
        "    print(f'MuRIL Fold {FOLD}: train_done in {time.time()-t1:.1f}s | total_elapsed={time.time()-t_fold:.1f}s')\n",
        "    # Predict valid\n",
        "    print(f'MuRIL Fold {FOLD}: predicting valid...')\n",
        "    pred = trainer.predict(va_ds_pred)\n",
        "    s_logp = log_softmax(torch.tensor(pred.predictions[0]), dim=-1).numpy()\n",
        "    e_logp = log_softmax(torch.tensor(pred.predictions[1]), dim=-1).numpy()\n",
        "    ex_best = {}\n",
        "    for i in range(len(va_tok_all)):\n",
        "        ex_id = va_tok_all[i]['example_id']\n",
        "        offsets = va_tok_all[i]['offset_mapping']\n",
        "        ctx = va_df.loc[va_df['id'].astype(str)==ex_id, 'context'].values[0]\n",
        "        s = s_logp[i]; e = e_logp[i]\n",
        "        mask = np.array([o is not None for o in offsets], dtype=bool)\n",
        "        s_m = np.where(mask, s, -1e9); e_m = np.where(mask, e, -1e9)\n",
        "        top_s = np.argsort(s_m)[-N_BEST:]; top_e = np.argsort(e_m)[-N_BEST:]\n",
        "        best = (float('-inf'), '')\n",
        "        for si in top_s:\n",
        "            for ei in top_e:\n",
        "                if ei < si: continue\n",
        "                if (ei - si + 1) > MAX_ANSWER_LEN: continue\n",
        "                if offsets[si] is None or offsets[ei] is None: continue\n",
        "                cs,_ = offsets[si]; _,ce = offsets[ei]\n",
        "                if cs is None or ce is None or ce <= cs: continue\n",
        "                cand = ctx[cs:ce]; score = float(s_m[si] + e_m[ei])\n",
        "                if score > best[0]: best = (score, cand)\n",
        "        if best[0] == float('-inf'): best = (-1e9, '')\n",
        "        prev = ex_best.get(ex_id, (float('-inf'), ''))\n",
        "        if best[0] > prev[0]: ex_best[ex_id] = best\n",
        "    preds = {k:v[1] for k,v in ex_best.items()}\n",
        "    y_true = va_df.assign(id_str=va_df['id'].astype(str)).set_index('id_str')['answer_text'].to_dict()\n",
        "    fold_scores = [jaccard_word_level(y_true[k], preds.get(k, '')) for k in y_true.keys()]\n",
        "    f_j = float(np.mean(fold_scores)) if fold_scores else 0.0\n",
        "    all_oof_scores.append(f_j)\n",
        "    print(f'MuRIL Fold {FOLD} Jaccard: {f_j:.4f} | elapsed_fold={time.time()-t_fold:.1f}s')\n",
        "    for k,v in preds.items():\n",
        "        oof_rows.append({'id': k, 'pred_text': v, 'gt': y_true.get(k, ''), 'fold': FOLD})\n",
        "\n",
        "    # Predict TEST and save logits\n",
        "    print(f'MuRIL Fold {FOLD}: predicting TEST and saving logits...')\n",
        "    te_pred = trainer.predict(te_ds_pred)\n",
        "    te_start = te_pred.predictions[0]\n",
        "    te_end = te_pred.predictions[1]\n",
        "    np.save(f'muril_large_test_logits/test_start_logits_f{FOLD}.npy', te_start)\n",
        "    np.save(f'muril_large_test_logits/test_end_logits_f{FOLD}.npy', te_end)\n",
        "    test_start_list.append(te_start)\n",
        "    test_end_list.append(te_end)\n",
        "\n",
        "    del model, trainer, tr_ds_torch, va_ds_pred, tr_tok, va_tok_all, pred, s_logp, e_logp, te_pred\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Save OOF, mean\n",
        "oof_df = pd.DataFrame(oof_rows)\n",
        "oof_df.to_csv('oof_preds_muril_large.csv', index=False)\n",
        "mean_oof = float(np.mean(all_oof_scores)) if all_oof_scores else 0.0\n",
        "print('MuRIL 5-fold OOF Jaccard (mean):', round(mean_oof, 4))\n",
        "\n",
        "# Average TEST logits across folds\n",
        "print('MuRIL: Averaging TEST logits across folds...')\n",
        "start_stack = np.stack(test_start_list, axis=0)\n",
        "end_stack = np.stack(test_end_list, axis=0)\n",
        "start_avg = start_stack.mean(axis=0)\n",
        "end_avg = end_stack.mean(axis=0)\n",
        "np.savez_compressed('muril_large_test_avg.npz', start=start_avg, end=end_avg)\n",
        "print('Saved muril_large_test_avg.npz; example_id/offsets saved under muril_large_test_logits/')\n",
        "print('MuRIL 5-fold done in %.1fs' % (time.time()-t_global))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "061ebbb1-3710-437b-a917-74a6f109e883",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Char-level blending and decoding (prob-space aggregation, banded decode, robust fallback with per-lang caps and optional smoothing)\n",
        "import os, json, numpy as np, pandas as pd, time, torch, re, unicodedata\n",
        "from torch.nn.functional import log_softmax\n",
        "\n",
        "# Safe normalization fallback (in case kernel was restarted)\n",
        "ZW_CHARS = ''.join([\n",
        "    '\\u200c',  # ZWNJ\n",
        "    '\\u200d',  # ZWJ\n",
        "    '\\ufeff',  # BOM\n",
        "])\n",
        "ZW_RE = re.compile(f'[{re.escape(ZW_CHARS)}]')\n",
        "WS_RE = re.compile(r'\\s+')\n",
        "OUTER_PUNCT_RE = re.compile(r'^[\\s\\\"\\'\\\u201c\\\u201d\\\u2018\\\u2019\\(\\)\\[\\]\\{\\}\\|\u0964,:;.!?-]+|[\\s\\\"\\'\\\u201c\\\u201d\\\u2018\\\u2019\\(\\)\\[\\]\\{\\}\\|\u0964,:;.!?-]+$')\n",
        "def _normalize_text_local(s: str, trim_outer_punct: bool = True) -> str:\n",
        "    if s is None:\n",
        "        return ''\n",
        "    s = unicodedata.normalize('NFKC', str(s))\n",
        "    s = ZW_RE.sub('', s)\n",
        "    s = WS_RE.sub(' ', s).strip()\n",
        "    if trim_outer_punct and s:\n",
        "        s = OUTER_PUNCT_RE.sub('', s).strip()\n",
        "    return s\n",
        "try:\n",
        "    normalize_text\n",
        "except NameError:\n",
        "    normalize_text = _normalize_text_local\n",
        "\n",
        "# Globals and per-stream temperature\n",
        "N_BEST_CHAR = 150\n",
        "MAX_ANSWER_LEN = 64\n",
        "LENGTH_PENALTY = 0.0048\n",
        "LOGIT_TEMPERATURE = 0.90\n",
        "STREAM_TEMP = {\n",
        "    'xlmr': 0.95,\n",
        "    'muril': {'hindi': 1.15, 'tamil': 1.10, 'default': 1.10},\n",
        "    'xlmr512': 0.88,\n",
        "    'mdeberta': 1.00,\n",
        "}\n",
        "APPLY_SMOOTHING = False  # set True for the smoothing probe (global; additional Tamil end-only smoothing applied inside build loop)\n",
        "EPS = 1e-12\n",
        "\n",
        "PUNCT_STRIP = '\\u0964\\u0965\u0964,:;.!?\\\"\\'\\\u201c\\\u201d\\\u2018\\\u2019\\)\\]\\}\\|\\-\\s'\n",
        "\n",
        "def strip_trailing_punct(s: str) -> str:\n",
        "    if not s: return s\n",
        "    return s.strip().strip(PUNCT_STRIP)\n",
        "\n",
        "def trim_boundary_combining(s: str) -> str:\n",
        "    if not s: return s\n",
        "    if len(s) > 0 and unicodedata.combining(s[0]):\n",
        "        s = s[1:]\n",
        "    if len(s) > 0 and unicodedata.combining(s[-1]):\n",
        "        s = s[:-1]\n",
        "    return s\n",
        "\n",
        "def remove_unmatched_quotes(s: str) -> str:\n",
        "    if not s: return s\n",
        "    quotes = [(\"\\\"\",\"\\\"\"), (\"'\",\"'\"), ('\u201c','\u201d'), ('\u2018','\u2019')]\n",
        "    for lq, rq in quotes:\n",
        "        if s.startswith(lq) and not s.endswith(rq): s = s[len(lq):]\n",
        "        if s.endswith(rq) and not s.startswith(lq): s = s[:-len(rq)]\n",
        "    return s\n",
        "\n",
        "# Load model evidence: masked log-softmax -> prob -> accumulate -> mean-by-coverage -> log\n",
        "def load_model_char_evidence(avg_npz_path, exid_json, offsets_npy, test_df, stream_key):\n",
        "    if not (os.path.exists(avg_npz_path) and os.path.exists(exid_json) and os.path.exists(offsets_npy)):\n",
        "        return None\n",
        "    data = np.load(avg_npz_path)\n",
        "    start_logits = data['start']\n",
        "    end_logits = data['end']\n",
        "    with open(exid_json, 'r', encoding='utf-8') as f:\n",
        "        example_ids = json.load(f)\n",
        "    offsets_list = np.load(offsets_npy, allow_pickle=True)\n",
        "    ctx_texts = dict(zip(test_df['id'].astype(str), test_df['context'].astype(str)))\n",
        "    lang_map = dict(zip(test_df['id'].astype(str), test_df['language'].astype(str) if 'language' in test_df.columns else ['unknown']*len(test_df)))\n",
        "\n",
        "    start_char_map, end_char_map = {}, {}\n",
        "    start_cov, end_cov = {}, {}\n",
        "\n",
        "    for i in range(len(example_ids)):\n",
        "        ex_id = example_ids[i]\n",
        "        ctx = ctx_texts.get(ex_id, '')\n",
        "        ex_lang = str(lang_map.get(ex_id, 'unknown')).lower().strip()\n",
        "        # per-language temperature (supports dict for muril) or scalar\n",
        "        T_cfg = STREAM_TEMP.get(stream_key, LOGIT_TEMPERATURE)\n",
        "        if isinstance(T_cfg, dict):\n",
        "            T = T_cfg.get(ex_lang, T_cfg.get('default', LOGIT_TEMPERATURE))\n",
        "        else:\n",
        "            T = T_cfg\n",
        "\n",
        "        n = len(ctx)\n",
        "        if ex_id not in start_char_map:\n",
        "            start_char_map[ex_id] = np.zeros(n, dtype=np.float32)\n",
        "            end_char_map[ex_id] = np.zeros(n, dtype=np.float32)\n",
        "            start_cov[ex_id] = np.zeros(n, dtype=np.float32)\n",
        "            end_cov[ex_id] = np.zeros(n, dtype=np.float32)\n",
        "\n",
        "        offs = offsets_list[i]\n",
        "        s_logit = torch.tensor(start_logits[i], dtype=torch.float32) / max(T, 1e-6)\n",
        "        e_logit = torch.tensor(end_logits[i], dtype=torch.float32) / max(T, 1e-6)\n",
        "        mask_bool = np.array([o is not None and o[0] is not None and o[1] is not None and (o[1] > o[0]) for o in offs], dtype=bool)\n",
        "        s_masked = torch.where(torch.tensor(mask_bool), s_logit, torch.full_like(s_logit, -1e9))\n",
        "        e_masked = torch.where(torch.tensor(mask_bool), e_logit, torch.full_like(e_logit, -1e9))\n",
        "\n",
        "        s_logp = torch.nn.functional.log_softmax(s_masked, dim=-1).numpy()\n",
        "        e_logp = torch.nn.functional.log_softmax(e_masked, dim=-1).numpy()\n",
        "        s_prob = np.exp(s_logp)\n",
        "        e_prob = np.exp(e_logp)\n",
        "\n",
        "        for t, o in enumerate(offs):\n",
        "            if o is None: continue\n",
        "            cs, ce = o\n",
        "            if cs is None or ce is None or ce <= cs: continue\n",
        "            if 0 <= cs < n:\n",
        "                start_char_map[ex_id][cs] += float(s_prob[t])\n",
        "                start_cov[ex_id][cs] += 1.0\n",
        "            end_idx = ce - 1\n",
        "            if 0 <= end_idx < n:\n",
        "                end_char_map[ex_id][end_idx] += float(e_prob[t])\n",
        "                end_cov[ex_id][end_idx] += 1.0\n",
        "\n",
        "    for ex_id in start_char_map.keys():\n",
        "        sc = start_cov[ex_id]; ec = end_cov[ex_id]\n",
        "        s = start_char_map[ex_id]; e = end_char_map[ex_id]\n",
        "        sc_safe = np.where(sc > 0, sc, 1.0)\n",
        "        ec_safe = np.where(ec > 0, ec, 1.0)\n",
        "        s_mean = s / sc_safe\n",
        "        e_mean = e / ec_safe\n",
        "        start_char_map[ex_id] = np.log(np.clip(s_mean, EPS, 1.0)).astype(np.float32)\n",
        "        end_char_map[ex_id] = np.log(np.clip(e_mean, EPS, 1.0)).astype(np.float32)\n",
        "\n",
        "    return start_char_map, end_char_map\n",
        "\n",
        "# Banded outer-product decode that returns (score, start, end, text)\n",
        "def decode_best_span(ctx, s_arr, e_arr, n_best=N_BEST_CHAR, max_len=MAX_ANSWER_LEN, length_penalty=LENGTH_PENALTY):\n",
        "    if len(ctx) == 0:\n",
        "        return (float('-inf'), -1, -1, '')\n",
        "    n = len(s_arr)\n",
        "    top_s = np.argsort(s_arr)[-n_best:]\n",
        "    best = (float('-inf'), 0, -1)\n",
        "    for si in top_s:\n",
        "        ei_max = min(n - 1, si + max_len - 1)\n",
        "        band = e_arr[si:ei_max+1]\n",
        "        if band.size == 0: continue\n",
        "        off = int(np.argmax(band))\n",
        "        ei = si + off\n",
        "        L = ei - si + 1\n",
        "        score = float(s_arr[si] + e_arr[ei] - length_penalty * (L - 1))\n",
        "        if score > best[0]:\n",
        "            best = (score, si, ei)\n",
        "    if best[2] < best[1]:\n",
        "        return (float('-inf'), -1, -1, '')\n",
        "    return (best[0], best[1], best[2], ctx[best[1]:best[2]+1])\n",
        "\n",
        "def _conv1d_reflect(arr: np.ndarray, kernel=(0.2, 0.6, 0.2)) -> np.ndarray:\n",
        "    if arr.size == 0: return arr\n",
        "    k = np.array(kernel, dtype=np.float32)\n",
        "    pad = len(k)//2\n",
        "    # reflect padding\n",
        "    left = arr[1:pad+1][::-1] if arr.size > 1 else arr\n",
        "    right = arr[-pad-1:-1][::-1] if arr.size > 1 else arr\n",
        "    padded = np.concatenate([left, arr, right]) if pad > 0 else arr\n",
        "    out = np.convolve(padded, k, mode='valid')\n",
        "    return out.astype(np.float32)\n",
        "\n",
        "def robust_decode(ctx, s_char_log, e_char_log, lang='unknown'):\n",
        "    lang_l = str(lang).lower().strip()\n",
        "    # per-language max_len caps, decode knobs, and no-answer thresholds\n",
        "    if lang_l == 'hindi':\n",
        "        max_len = 50; nb = 160; len_pen = 0.0048; na_threshold = -5.0\n",
        "    elif lang_l == 'tamil':\n",
        "        max_len = 64; nb = 200; len_pen = 0.0040; na_threshold = -4.5\n",
        "    else:\n",
        "        max_len = MAX_ANSWER_LEN; nb = N_BEST_CHAR; len_pen = LENGTH_PENALTY; na_threshold = -5.0\n",
        "\n",
        "    score, si, ei, ans = decode_best_span(ctx, s_char_log, e_char_log, n_best=nb, max_len=max_len, length_penalty=len_pen)\n",
        "    # No-answer threshold: if below, return empty (disable risky fallbacks)\n",
        "    if score < na_threshold:\n",
        "        return ''\n",
        "\n",
        "    # Minimal hygiene\n",
        "    if ans:\n",
        "        if len(ans) and (unicodedata.category(ans[-1]) == 'Mn' or ans[-1] == '\\u094D'):\n",
        "            ans = ans[:-1]\n",
        "        if len(ans) and unicodedata.category(ans[0]) == 'Mn':\n",
        "            ans = ans[1:]\n",
        "        # Tamil-specific: drop isolated trailing vowel sign once\n",
        "        if lang_l == 'tamil' and len(ans) and unicodedata.category(ans[-1]) == 'Mn':\n",
        "            ans = ans[:-1]\n",
        "    return ans or ''\n",
        "\n",
        "def build_submission(blend_weights=None, per_language_weights=None):\n",
        "    t0 = time.time()\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    # Load models' char evidence\n",
        "    xlmr = load_model_char_evidence(\n",
        "        'xlmr_large_test_avg.npz',\n",
        "        'xlmr_large_test_logits/test_example_id.json',\n",
        "        'xlmr_large_test_logits/test_offset_mapping.npy',\n",
        "        test_df, 'xlmr'\n",
        "    )\n",
        "    muril = load_model_char_evidence(\n",
        "        'muril_large_test_avg.npz',\n",
        "        'muril_large_test_logits/test_example_id.json',\n",
        "        'muril_large_test_logits/test_offset_mapping.npy',\n",
        "        test_df, 'muril'\n",
        "    )\n",
        "    mdb = load_model_char_evidence(\n",
        "        'mdeberta_base_test_avg.npz',\n",
        "        'mdeberta_base_test_logits/test_example_id.json',\n",
        "        'mdeberta_base_test_logits/test_offset_mapping.npy',\n",
        "        test_df, 'mdeberta'\n",
        "    )\n",
        "    xlmr512 = load_model_char_evidence(\n",
        "        'xlmr_large_512_test_avg.npz',\n",
        "        'xlmr_large_512_test_logits/test_example_id.json',\n",
        "        'xlmr_large_512_test_logits/test_offset_mapping.npy',\n",
        "        test_df, 'xlmr512'\n",
        "    )\n",
        "    have_xlmr = xlmr is not None\n",
        "    have_muril = muril is not None\n",
        "    have_mdb = mdb is not None\n",
        "    have_xlmr512 = xlmr512 is not None\n",
        "    assert have_xlmr or have_muril or have_mdb or have_xlmr512, 'No averaged logits found. Run folds first.'\n",
        "\n",
        "    if blend_weights is None:\n",
        "        if have_xlmr and have_muril and have_mdb and have_xlmr512:\n",
        "            w_x, w_m, w_d, w_x512 = 0.45, 0.25, 0.0, 0.30\n",
        "        elif have_xlmr and have_muril and have_xlmr512:\n",
        "            w_x, w_m, w_d, w_x512 = 0.30, 0.10, 0.0, 0.60\n",
        "        elif have_xlmr and have_xlmr512:\n",
        "            w_x, w_m, w_d, w_x512 = 0.35, 0.0, 0.0, 0.65\n",
        "        elif have_xlmr:\n",
        "            w_x, w_m, w_d, w_x512 = 1.0, 0.0, 0.0, 0.0\n",
        "        elif have_muril:\n",
        "            w_x, w_m, w_d, w_x512 = 0.0, 1.0, 0.0, 0.0\n",
        "        else:\n",
        "            w_x, w_m, w_d, w_x512 = 0.0, 0.0, 0.0, 1.0\n",
        "    else:\n",
        "        if len(blend_weights) == 2:\n",
        "            w_x, w_m = blend_weights; w_d = 0.0; w_x512 = 0.0\n",
        "        elif len(blend_weights) == 3:\n",
        "            w_x, w_m, w_d = blend_weights; w_x512 = 0.0\n",
        "        else:\n",
        "            w_x, w_m, w_d, w_x512 = blend_weights\n",
        "\n",
        "    preds = []\n",
        "    ids = test_df['id'].astype(str).values\n",
        "    langs = test_df['language'].astype(str).values if 'language' in test_df.columns else np.array(['unknown']*len(ids))\n",
        "    for ex_id, lang in zip(ids, langs):\n",
        "        ctx = str(test_df.loc[test_df['id'].astype(str)==ex_id, 'context'].values[0])\n",
        "        wx, wm, wd, wx512 = w_x, w_m, w_d, w_x512\n",
        "        if per_language_weights is not None and isinstance(per_language_weights, dict):\n",
        "            if lang in per_language_weights:\n",
        "                pl = per_language_weights[lang]\n",
        "                if len(pl) == 2:\n",
        "                    wx, wm = pl; wd, wx512 = 0.0, 0.0\n",
        "                elif len(pl) == 3:\n",
        "                    wx, wm, wd = pl; wx512 = 0.0\n",
        "                else:\n",
        "                    wx, wm, wd, wx512 = pl\n",
        "        weight_items = []\n",
        "        if have_xlmr: weight_items.append(('xlmr', wx))\n",
        "        if have_muril: weight_items.append(('muril', wm))\n",
        "        if have_mdb: weight_items.append(('mdeberta', wd))\n",
        "        if have_xlmr512: weight_items.append(('xlmr512', wx512))\n",
        "        sumw = sum(max(0.0, w) for _, w in weight_items)\n",
        "        if sumw <= 0:\n",
        "            norm_weights = {k: 1.0/len(weight_items) for k,_ in weight_items} if weight_items else {}\n",
        "        else:\n",
        "            norm_weights = {k: max(0.0, w)/sumw for k,w in weight_items}\n",
        "\n",
        "        s_char = np.zeros(len(ctx), dtype=np.float32)\n",
        "        e_char = np.zeros(len(ctx), dtype=np.float32)\n",
        "        if have_xlmr:\n",
        "            s_map_x, e_map_x = xlmr\n",
        "            sx = s_map_x.get(ex_id); exm = e_map_x.get(ex_id)\n",
        "            if sx is not None: s_char += norm_weights.get('xlmr', 0.0) * sx\n",
        "            if exm is not None: e_char += norm_weights.get('xlmr', 0.0) * exm\n",
        "        if have_muril:\n",
        "            s_map_m, e_map_m = muril\n",
        "            sm = s_map_m.get(ex_id); em = e_map_m.get(ex_id)\n",
        "            if sm is not None: s_char += norm_weights.get('muril', 0.0) * sm\n",
        "            if em is not None: e_char += norm_weights.get('muril', 0.0) * em\n",
        "        if have_mdb:\n",
        "            s_map_d, e_map_d = mdb\n",
        "            sd = s_map_d.get(ex_id); ed = e_map_d.get(ex_id)\n",
        "            if sd is not None: s_char += norm_weights.get('mdeberta', 0.0) * sd\n",
        "            if ed is not None: e_char += norm_weights.get('mdeberta', 0.0) * ed\n",
        "        if have_xlmr512:\n",
        "            s_map_x512, e_map_x512 = xlmr512\n",
        "            sx512 = s_map_x512.get(ex_id); ex512 = e_map_x512.get(ex_id)\n",
        "            if sx512 is not None: s_char += norm_weights.get('xlmr512', 0.0) * sx512\n",
        "            if ex512 is not None: e_char += norm_weights.get('xlmr512', 0.0) * ex512\n",
        "\n",
        "        # Optional global light smoothing (for probe only)\n",
        "        if APPLY_SMOOTHING:\n",
        "            s_char = _conv1d_reflect(s_char)\n",
        "            e_char = _conv1d_reflect(e_char)\n",
        "\n",
        "        # Tamil-only end smoothing as recommended\n",
        "        if str(lang).lower().strip() == 'tamil':\n",
        "            e_char = _conv1d_reflect(e_char)\n",
        "\n",
        "        ans = robust_decode(ctx, s_char, e_char, lang)\n",
        "        try:\n",
        "            # Numeric punctuation collapse inside digit runs\n",
        "            ans = re.sub(r'(?<=\\d)[ ,._-](?=\\d)', '', ans)\n",
        "            # Danda/virama hygiene: replace trailing '\\u094D\\u0964' with '\u0964', collapse multiple dandas, then strip trailing danda once\n",
        "            ans = ans.replace('\\u094D\\u0964', '\\u0964').replace('\\u094D\u0964', '\u0964')\n",
        "            ans = re.sub(r'[\u0964]+', '\u0964', ans)\n",
        "            if ans.endswith('\u0964'): ans = ans[:-1]\n",
        "            ans = strip_trailing_punct(ans)\n",
        "            ans = trim_boundary_combining(ans)\n",
        "            if len(ans) > 0 and ans[0].isspace(): ans = ans[0:].lstrip()\n",
        "            if len(ans) > 0 and ans[-1].isspace(): ans = ans.rstrip()\n",
        "            ans = remove_unmatched_quotes(ans)\n",
        "            ans = normalize_text(ans, trim_outer_punct=True)\n",
        "        except Exception:\n",
        "            ans = (ans or '').strip()\n",
        "        preds.append(ans if ans is not None else '')\n",
        "    sub = pd.DataFrame({'id': ids, 'PredictionString': preds})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv', sub.shape, 'in %.1fs' % (time.time()-t0))\n",
        "    return sub\n",
        "\n",
        "print('Blend/Decode cell ready. Use build_submission() with per-lang nb/lp, Tamil end smoothing, per-lang temps, and per-language weights.')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blend/Decode cell ready. Use build_submission() with per-lang nb/lp, Tamil end smoothing, per-lang temps, and per-language weights.\n"
          ]
        }
      ]
    },
    {
      "id": "b34a095b-9ff8-4c93-a9d7-050e3445e71f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build blended submission with 3-model weights [XLM-R, MuRIL, mDeBERTa] = [0.50, 0.30, 0.20]\n",
        "sub = build_submission(blend_weights=(0.50, 0.30, 0.20))\n",
        "sub.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "c64f163d-2931-49c5-a6ff-74377d096de6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 5-fold training with microsoft/mdeberta-v3-base (MAX_LEN=512, STRIDE=256) + TEST LOGIT SAVING\n",
        "import os, time, numpy as np, pandas as pd, collections, random, json, torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "from torch.nn.functional import log_softmax\n",
        "\n",
        "SEED = 2025\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "MODEL_NAME = 'microsoft/mdeberta-v3-base'\n",
        "MAX_LENGTH = 512\n",
        "DOC_STRIDE = 256\n",
        "MAX_ANSWER_LEN = 64\n",
        "NEG_RATIO = 1\n",
        "EPOCHS = 3\n",
        "LR = 3e-5\n",
        "TRAIN_BS = 16  # fallback to 12 if OOM\n",
        "EVAL_BS = 32\n",
        "GRAD_ACCUM = 1\n",
        "N_BEST = 30\n",
        "\n",
        "print('Retokenizing with', MODEL_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "pad_on_right = tokenizer.padding_side == 'right'\n",
        "\n",
        "full_df = pd.read_csv('train.csv').merge(pd.read_csv('folds.csv'), on='id', how='left')\n",
        "assert full_df['fold'].notna().all()\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "def prepare_train_features_fn(examples):\n",
        "    questions = [q.strip() for q in examples['question']]; contexts = examples['context']\n",
        "    tok = tokenizer(\n",
        "        questions if pad_on_right else contexts,\n",
        "        contexts if pad_on_right else questions,\n",
        "        truncation='only_second' if pad_on_right else 'only_first',\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    smap = tok.pop('overflow_to_sample_mapping')\n",
        "    offsets = tok.pop('offset_mapping')\n",
        "    input_ids = tok['input_ids']\n",
        "    start_positions, end_positions = [], []\n",
        "    cls_positions = []\n",
        "    for i, off in enumerate(offsets):\n",
        "        si = smap[i]\n",
        "        ctx = contexts[si]\n",
        "        a_text = str(examples['answer_text'][si])\n",
        "        a_start = int(examples['answer_start'][si])\n",
        "        a_end = a_start + len(a_text)\n",
        "        while a_start < a_end and a_start < len(ctx) and ctx[a_start].isspace(): a_start += 1\n",
        "        while a_end > a_start and a_end-1 < len(ctx) and ctx[a_end-1].isspace(): a_end -= 1\n",
        "        if a_start >= a_end: a_end = a_start\n",
        "        seq_ids = tok.sequence_ids(i)\n",
        "        try:\n",
        "            cls_index = input_ids[i].index(tokenizer.cls_token_id) if tokenizer.cls_token_id is not None else 0\n",
        "        except ValueError:\n",
        "            cls_index = 0\n",
        "        cls_positions.append(cls_index)\n",
        "        context_id = 1 if pad_on_right else 0\n",
        "        try:\n",
        "            ctx_s = seq_ids.index(context_id)\n",
        "            ctx_e = len(seq_ids) - 1 - seq_ids[::-1].index(context_id)\n",
        "        except ValueError:\n",
        "            start_positions.append(cls_index); end_positions.append(cls_index); continue\n",
        "        st_tok = en_tok = None\n",
        "        tgt_end_incl = a_end - 1\n",
        "        for t in range(ctx_s, ctx_e + 1):\n",
        "            o = off[t]\n",
        "            if o is None: continue\n",
        "            ts, te = o\n",
        "            if ts is None or te is None: continue\n",
        "            if st_tok is None and ts <= a_start < te: st_tok = t\n",
        "            if ts <= tgt_end_incl < te: en_tok = t\n",
        "        if st_tok is not None and en_tok is not None and st_tok <= en_tok:\n",
        "            start_positions.append(st_tok); end_positions.append(en_tok)\n",
        "        else:\n",
        "            start_positions.append(cls_index); end_positions.append(cls_index)\n",
        "    tok['start_positions'] = start_positions; tok['end_positions'] = end_positions; tok['overflow_to_sample_mapping'] = smap; tok['cls_positions'] = cls_positions\n",
        "    return tok\n",
        "\n",
        "def prepare_pred_features_fn(examples):\n",
        "    questions = [q.strip() for q in examples['question']]; contexts = examples['context']\n",
        "    tok = tokenizer(\n",
        "        questions if pad_on_right else contexts,\n",
        "        contexts if pad_on_right else questions,\n",
        "        truncation='only_second' if pad_on_right else 'only_first',\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    smap = tok.pop('overflow_to_sample_mapping')\n",
        "    tok['example_id'] = []\n",
        "    new_offsets = []\n",
        "    for i, off in enumerate(tok['offset_mapping']):\n",
        "        si = smap[i]\n",
        "        tok['example_id'].append(str(examples['id'][si]))\n",
        "        seq_ids = tok.sequence_ids(i)\n",
        "        context_id = 1 if pad_on_right else 0\n",
        "        new_offsets.append([o if seq_ids[k] == context_id else None for k, o in enumerate(off)])\n",
        "    tok['offset_mapping'] = new_offsets\n",
        "    return tok\n",
        "\n",
        "# Pre-tokenize TEST once\n",
        "test_ds = Dataset.from_pandas(test_df[['id','question','context','language']])\n",
        "test_tok_all = test_ds.map(prepare_pred_features_fn, batched=True, remove_columns=test_ds.column_names, desc='mdeberta_test_tokenize_allfolds')\n",
        "te_ds_pred = test_tok_all.remove_columns([c for c in test_tok_all.column_names if c not in ['input_ids','attention_mask']])\n",
        "te_ds_pred.set_format(type='torch')\n",
        "\n",
        "os.makedirs('mdeberta_base_test_logits', exist_ok=True)\n",
        "with open('mdeberta_base_test_logits/test_example_id.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(list(test_tok_all['example_id']), f, ensure_ascii=False)\n",
        "np.save('mdeberta_base_test_logits/test_offset_mapping.npy', np.array(test_tok_all['offset_mapping'], dtype=object), allow_pickle=True)\n",
        "\n",
        "all_oof_scores = []; oof_rows = []; test_start_list = []; test_end_list = [];\n",
        "t_global = time.time()\n",
        "for FOLD in range(5):\n",
        "    t_fold = time.time()\n",
        "    print(f'\\n===== mDeBERTa Fold {FOLD} =====')\n",
        "    tr_df = full_df[full_df['fold'] != FOLD].reset_index(drop=True)\n",
        "    va_df = full_df[full_df['fold'] == FOLD].reset_index(drop=True)\n",
        "    tr_ds = Dataset.from_pandas(tr_df[['id','question','context','answer_text','answer_start','language']])\n",
        "    va_ds = Dataset.from_pandas(va_df[['id','question','context','answer_text','answer_start','language']])\n",
        "    t0 = time.time()\n",
        "    tr_tok = tr_ds.map(prepare_train_features_fn, batched=True, remove_columns=tr_ds.column_names, desc=f'mdeberta_train_tokenize_f{FOLD}')\n",
        "    va_tok_all = va_ds.map(prepare_pred_features_fn, batched=True, remove_columns=va_ds.column_names, desc=f'mdeberta_valid_tokenize_f{FOLD}')\n",
        "    print(f'mDeBERTa Fold {FOLD}: train feats pre-sample={tr_tok.num_rows} | valid feats={va_tok_all.num_rows} | tok_time={time.time()-t0:.1f}s')\n",
        "    # Negative sampling (robust via cls_positions)\n",
        "    smap = np.array(tr_tok['overflow_to_sample_mapping'])\n",
        "    sp = np.array(tr_tok['start_positions'])\n",
        "    cls_arr = np.array(tr_tok['cls_positions'])\n",
        "    is_pos = (sp != cls_arr)\n",
        "    keep = []; by_ex = collections.defaultdict(list)\n",
        "    for idx, ex in enumerate(smap): by_ex[ex].append(idx)\n",
        "    for ex, idxs in by_ex.items():\n",
        "        idxs = np.array(idxs); pos = idxs[is_pos[idxs]]; neg = idxs[~is_pos[idxs]]\n",
        "        keep.extend(pos.tolist())\n",
        "        if len(pos) == 0:\n",
        "            sel = neg[:min(4, len(neg))]; keep.extend(sel.tolist())\n",
        "        else:\n",
        "            cap = min(len(neg), NEG_RATIO*len(pos))\n",
        "            if cap > 0:\n",
        "                sel = neg[np.random.permutation(len(neg))[:cap]]; keep.extend(sel.tolist())\n",
        "    keep = np.array(sorted(set(keep))); before = tr_tok.num_rows\n",
        "    tr_tok = tr_tok.select(keep.tolist()); after = tr_tok.num_rows\n",
        "    print(f'mDeBERTa Fold {FOLD}: neg-sample {before}->{after} ({after/before:.2%})')\n",
        "    # Datasets\n",
        "    tr_ds_torch = tr_tok.remove_columns([c for c in tr_tok.column_names if c not in ['input_ids','attention_mask','start_positions','end_positions']])\n",
        "    tr_ds_torch.set_format(type='torch')\n",
        "    va_ds_pred = va_tok_all.remove_columns([c for c in va_tok_all.column_names if c not in ['input_ids','attention_mask']])\n",
        "    va_ds_pred.set_format(type='torch')\n",
        "    # Model\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "    try: model.gradient_checkpointing_enable()\n",
        "    except Exception: pass\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'mdeberta_base_f{FOLD}',\n",
        "        num_train_epochs=EPOCHS,\n",
        "        learning_rate=LR,\n",
        "        per_device_train_batch_size=TRAIN_BS,\n",
        "        per_device_eval_batch_size=EVAL_BS,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM,\n",
        "        fp16=True,\n",
        "        logging_steps=250,\n",
        "        save_strategy='no',\n",
        "        evaluation_strategy='no',\n",
        "        seed=SEED,\n",
        "        dataloader_num_workers=2,\n",
        "        report_to=[],\n",
        "        gradient_checkpointing=True,\n",
        "        warmup_ratio=0.1,\n",
        "        weight_decay=0.01,\n",
        "        max_grad_norm=1.0\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tr_ds_torch,\n",
        "        eval_dataset=None,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer),\n",
        "    )\n",
        "    print(f'mDeBERTa Fold {FOLD}: training...')\n",
        "    t1 = time.time()\n",
        "    try:\n",
        "        trainer.train()\n",
        "    except RuntimeError as e:\n",
        "        if 'CUDA out of memory' in str(e):\n",
        "            print('OOM encountered; reducing TRAIN_BS to 12 and retrying this fold...')\n",
        "            del trainer, model\n",
        "            torch.cuda.empty_cache()\n",
        "            model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "            try: model.gradient_checkpointing_enable()\n",
        "            except Exception: pass\n",
        "            args = TrainingArguments(\n",
        "                output_dir=f'mdeberta_base_f{FOLD}',\n",
        "                num_train_epochs=EPOCHS,\n",
        "                learning_rate=LR,\n",
        "                per_device_train_batch_size=12,\n",
        "                per_device_eval_batch_size=EVAL_BS,\n",
        "                gradient_accumulation_steps=GRAD_ACCUM,\n",
        "                fp16=True,\n",
        "                logging_steps=250,\n",
        "                save_strategy='no',\n",
        "                evaluation_strategy='no',\n",
        "                seed=SEED,\n",
        "                dataloader_num_workers=2,\n",
        "                report_to=[],\n",
        "                gradient_checkpointing=True,\n",
        "                warmup_ratio=0.1,\n",
        "                weight_decay=0.01,\n",
        "                max_grad_norm=1.0,\n",
        "            )\n",
        "            trainer = Trainer(\n",
        "                model=model,\n",
        "                args=args,\n",
        "                train_dataset=tr_ds_torch,\n",
        "                eval_dataset=None,\n",
        "                tokenizer=tokenizer,\n",
        "                data_collator=DataCollatorWithPadding(tokenizer),\n",
        "            )\n",
        "            trainer.train()\n",
        "        else:\n",
        "            raise\n",
        "    print(f'mDeBERTa Fold {FOLD}: train_done in {time.time()-t1:.1f}s | total_elapsed={time.time()-t_fold:.1f}s')\n",
        "    # Predict valid\n",
        "    print(f'mDeBERTa Fold {FOLD}: predicting valid...')\n",
        "    pred = trainer.predict(va_ds_pred)\n",
        "    s_logp = log_softmax(torch.tensor(pred.predictions[0]), dim=-1).numpy()\n",
        "    e_logp = log_softmax(torch.tensor(pred.predictions[1]), dim=-1).numpy()\n",
        "    ex_best = {}\n",
        "    for i in range(len(va_tok_all)):\n",
        "        ex_id = va_tok_all[i]['example_id']\n",
        "        offsets = va_tok_all[i]['offset_mapping']\n",
        "        ctx = va_df.loc[va_df['id'].astype(str)==ex_id, 'context'].values[0]\n",
        "        s = s_logp[i]; e = e_logp[i]\n",
        "        mask = np.array([o is not None for o in offsets], dtype=bool)\n",
        "        s_m = np.where(mask, s, -1e9); e_m = np.where(mask, e, -1e9)\n",
        "        top_s = np.argsort(s_m)[-N_BEST:]; top_e = np.argsort(e_m)[-N_BEST:]\n",
        "        best = (float('-inf'), '')\n",
        "        for si in top_s:\n",
        "            for ei in top_e:\n",
        "                if ei < si: continue\n",
        "                if (ei - si + 1) > MAX_ANSWER_LEN: continue\n",
        "                if offsets[si] is None or offsets[ei] is None: continue\n",
        "                cs,_ = offsets[si]; _,ce = offsets[ei]\n",
        "                if cs is None or ce is None or ce <= cs: continue\n",
        "                cand = ctx[cs:ce]; score = float(s_m[si] + e_m[ei])\n",
        "                if score > best[0]: best = (score, cand)\n",
        "        if best[0] == float('-inf'): best = (-1e9, '')\n",
        "        prev = ex_best.get(ex_id, (float('-inf'), ''))\n",
        "        if best[0] > prev[0]: ex_best[ex_id] = best\n",
        "    preds = {k:v[1] for k,v in ex_best.items()}\n",
        "    y_true = va_df.assign(id_str=va_df['id'].astype(str)).set_index('id_str')['answer_text'].to_dict()\n",
        "    fold_scores = [jaccard_word_level(y_true[k], preds.get(k, '')) for k in y_true.keys()]\n",
        "    f_j = float(np.mean(fold_scores)) if fold_scores else 0.0\n",
        "    all_oof_scores.append(f_j)\n",
        "    print(f'mDeBERTa Fold {FOLD} Jaccard: {f_j:.4f} | elapsed_fold={time.time()-t_fold:.1f}s')\n",
        "    for k,v in preds.items():\n",
        "        oof_rows.append({'id': k, 'pred_text': v, 'gt': y_true.get(k, ''), 'fold': FOLD})\n",
        "\n",
        "    # Predict TEST and save logits\n",
        "    print(f'mDeBERTa Fold {FOLD}: predicting TEST and saving logits...')\n",
        "    te_pred = trainer.predict(te_ds_pred)\n",
        "    te_start = te_pred.predictions[0]\n",
        "    te_end = te_pred.predictions[1]\n",
        "    np.save(f'mdeberta_base_test_logits/test_start_logits_f{FOLD}.npy', te_start)\n",
        "    np.save(f'mdeberta_base_test_logits/test_end_logits_f{FOLD}.npy', te_end)\n",
        "    test_start_list.append(te_start)\n",
        "    test_end_list.append(te_end)\n",
        "\n",
        "    del model, trainer, tr_ds_torch, va_ds_pred, tr_tok, va_tok_all, pred, s_logp, e_logp, te_pred\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Save OOF, mean\n",
        "oof_df = pd.DataFrame(oof_rows)\n",
        "oof_df.to_csv('oof_preds_mdeberta_base.csv', index=False)\n",
        "mean_oof = float(np.mean(all_oof_scores)) if all_oof_scores else 0.0\n",
        "print('mDeBERTa 5-fold OOF Jaccard (mean):', round(mean_oof, 4))\n",
        "\n",
        "# Average TEST logits across folds\n",
        "print('mDeBERTa: Averaging TEST logits across folds...')\n",
        "start_stack = np.stack(test_start_list, axis=0)\n",
        "end_stack = np.stack(test_end_list, axis=0)\n",
        "start_avg = start_stack.mean(axis=0)\n",
        "end_avg = end_stack.mean(axis=0)\n",
        "np.savez_compressed('mdeberta_base_test_avg.npz', start=start_avg, end=end_avg)\n",
        "print('Saved mdeberta_base_test_avg.npz; example_id/offsets saved under mdeberta_base_test_logits/')\n",
        "print('mDeBERTa 5-fold done in %.1fs' % (time.time()-t_global))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "a507dbfc-3c23-4727-985f-38926e3f8d15",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 5-fold training with deepset/xlm-roberta-large-squad2 at MAX_LEN=512, STRIDE=256 + TEST LOGIT SAVING (TTA-style retrain)\n",
        "import os, time, numpy as np, pandas as pd, collections, random, json, torch, re, unicodedata, hashlib\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "from torch.nn.functional import log_softmax\n",
        "\n",
        "# --- Inline metric/normalization to avoid missing globals after restarts ---\n",
        "ZW_CHARS = ''.join([\n",
        "    '\\u200c',  # ZWNJ\n",
        "    '\\u200d',  # ZWJ\n",
        "    '\\ufeff',  # BOM\n",
        "])\n",
        "ZW_RE = re.compile(f'[{re.escape(ZW_CHARS)}]')\n",
        "WS_RE = re.compile(r'\\s+')\n",
        "OUTER_PUNCT_RE = re.compile(r'^[\\s\\\"\\'\\\u201c\\\u201d\\\u2018\\\u2019\\(\\)\\[\\]\\{\\}\\|\u0964,:;.!?-]+|[\\s\\\"\\'\\\u201c\\\u201d\\\u2018\\\u2019\\(\\)\\[\\]\\{\\}\\|\u0964,:;.!?-]+$')\n",
        "def normalize_text(s: str, trim_outer_punct: bool = True) -> str:\n",
        "    if s is None:\n",
        "        return ''\n",
        "    s = unicodedata.normalize('NFKC', str(s))\n",
        "    s = ZW_RE.sub('', s)\n",
        "    s = WS_RE.sub(' ', s).strip()\n",
        "    if trim_outer_punct and s:\n",
        "        s = OUTER_PUNCT_RE.sub('', s).strip()\n",
        "    return s\n",
        "def jaccard_word_level(a: str, b: str) -> float:\n",
        "    a_n = normalize_text(a)\n",
        "    b_n = normalize_text(b)\n",
        "    if not a_n and not b_n:\n",
        "        return 1.0\n",
        "    a_set = set(a_n.split())\n",
        "    b_set = set(b_n.split())\n",
        "    if not a_set and not b_set:\n",
        "        return 1.0\n",
        "    inter = len(a_set & b_set)\n",
        "    union = len(a_set | b_set)\n",
        "    return inter / union if union else 0.0\n",
        "\n",
        "# Memory-friendly env settings\n",
        "os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')\n",
        "os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True,max_split_size_mb:64')\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "try: torch.set_float32_matmul_precision('high')\n",
        "except Exception: pass\n",
        "\n",
        "SEED = 2025\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "MODEL_NAME = 'deepset/xlm-roberta-large-squad2'\n",
        "MAX_LENGTH = 512\n",
        "DOC_STRIDE = 256\n",
        "MAX_ANSWER_LEN = 64\n",
        "NEG_RATIO = 2\n",
        "EPOCHS = 3\n",
        "LR = 2e-5\n",
        "TRAIN_BS = 1  # ultra-conservative for 24GB A10/V100; rely on GRAD_ACCUM\n",
        "EVAL_BS = 4   # reduce eval bs to avoid OOM during prediction\n",
        "GRAD_ACCUM = 16  # effective batch size kept reasonable\n",
        "N_BEST = 25\n",
        "\n",
        "print('Retokenizing with', MODEL_NAME, 'len', MAX_LENGTH, 'stride', DOC_STRIDE)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "pad_on_right = tokenizer.padding_side == 'right'\n",
        "\n",
        "full_df = pd.read_csv('train.csv').merge(pd.read_csv('folds.csv'), on='id', how='left')\n",
        "assert full_df['fold'].notna().all()\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "def prepare_train_features_fn(examples):\n",
        "    questions = [q.strip() for q in examples['question']]; contexts = examples['context']\n",
        "    tok = tokenizer(\n",
        "        questions if pad_on_right else contexts,\n",
        "        contexts if pad_on_right else questions,\n",
        "        truncation='only_second' if pad_on_right else 'only_first',\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    smap = tok.pop('overflow_to_sample_mapping')\n",
        "    offsets = tok.pop('offset_mapping')\n",
        "    input_ids = tok['input_ids']\n",
        "    start_positions, end_positions = [], []\n",
        "    cls_positions = []  # track per-feature CLS index for robust negative sampling\n",
        "    for i, off in enumerate(offsets):\n",
        "        si = smap[i]\n",
        "        ctx = contexts[si]\n",
        "        a_text = str(examples['answer_text'][si])\n",
        "        a_start = int(examples['answer_start'][si])\n",
        "        a_end = a_start + len(a_text)\n",
        "        while a_start < a_end and a_start < len(ctx) and ctx[a_start].isspace(): a_start += 1\n",
        "        while a_end > a_start and a_end-1 < len(ctx) and ctx[a_end-1].isspace(): a_end -= 1\n",
        "        if a_start >= a_end: a_end = a_start\n",
        "        seq_ids = tok.sequence_ids(i)\n",
        "        try:\n",
        "            cls_index = input_ids[i].index(tokenizer.cls_token_id) if tokenizer.cls_token_id is not None else 0\n",
        "        except ValueError:\n",
        "            cls_index = 0\n",
        "        cls_positions.append(cls_index)\n",
        "        context_id = 1 if pad_on_right else 0\n",
        "        try:\n",
        "            ctx_s = seq_ids.index(context_id)\n",
        "            ctx_e = len(seq_ids) - 1 - seq_ids[::-1].index(context_id)\n",
        "        except ValueError:\n",
        "            start_positions.append(cls_index); end_positions.append(cls_index); continue\n",
        "        st_tok = en_tok = None\n",
        "        tgt_end_incl = a_end - 1\n",
        "        for t in range(ctx_s, ctx_e + 1):\n",
        "            o = off[t]\n",
        "            if o is None: continue\n",
        "            ts, te = o\n",
        "            if ts is None or te is None: continue\n",
        "            if st_tok is None and ts <= a_start < te: st_tok = t\n",
        "            if ts <= tgt_end_incl < te: en_tok = t\n",
        "        if st_tok is not None and en_tok is not None and st_tok <= en_tok:\n",
        "            start_positions.append(st_tok); end_positions.append(en_tok)\n",
        "        else:\n",
        "            start_positions.append(cls_index); end_positions.append(cls_index)\n",
        "    tok['start_positions'] = start_positions; tok['end_positions'] = end_positions; tok['overflow_to_sample_mapping'] = smap; tok['cls_positions'] = cls_positions\n",
        "    return tok\n",
        "\n",
        "def prepare_pred_features_fn(examples):\n",
        "    questions = [q.strip() for q in examples['question']]; contexts = examples['context']\n",
        "    tok = tokenizer(\n",
        "        questions if pad_on_right else contexts,\n",
        "        contexts if pad_on_right else questions,\n",
        "        truncation='only_second' if pad_on_right else 'only_first',\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    smap = tok.pop('overflow_to_sample_mapping')\n",
        "    tok['example_id'] = []\n",
        "    new_offsets = []\n",
        "    for i, off in enumerate(tok['offset_mapping']):\n",
        "        si = smap[i]\n",
        "        tok['example_id'].append(str(examples['id'][si]))\n",
        "        seq_ids = tok.sequence_ids(i)\n",
        "        context_id = 1 if pad_on_right else 0\n",
        "        new_offsets.append([o if seq_ids[k] == context_id else None for k, o in enumerate(off)])\n",
        "    tok['offset_mapping'] = new_offsets\n",
        "    return tok\n",
        "\n",
        "# Pre-tokenize TEST once for 512 setting\n",
        "test_ds = Dataset.from_pandas(test_df[['id','question','context','language']])\n",
        "test_tok_all = test_ds.map(prepare_pred_features_fn, batched=True, remove_columns=test_ds.column_names, desc='xlmr512_test_tokenize_allfolds')\n",
        "te_ds_pred = test_tok_all.remove_columns([c for c in test_tok_all.column_names if c not in ['input_ids','attention_mask']])\n",
        "te_ds_pred.set_format(type='torch')\n",
        "\n",
        "os.makedirs('xlmr_large_512_test_logits', exist_ok=True)\n",
        "with open('xlmr_large_512_test_logits/test_example_id.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(list(test_tok_all['example_id']), f, ensure_ascii=False)\n",
        "np.save('xlmr_large_512_test_logits/test_offset_mapping.npy', np.array(test_tok_all['offset_mapping'], dtype=object), allow_pickle=True)\n",
        "\n",
        "all_oof_scores = []; oof_rows = []; test_start_list = []; test_end_list = [];\n",
        "t_global = time.time()\n",
        "for FOLD in range(5):\n",
        "    t_fold = time.time()\n",
        "    print(f'\\n===== XLM-R Large 512 Fold {FOLD} =====')\n",
        "    tr_df = full_df[full_df['fold'] != FOLD].reset_index(drop=True)\n",
        "    va_df = full_df[full_df['fold'] == FOLD].reset_index(drop=True)\n",
        "    tr_ds = Dataset.from_pandas(tr_df[['id','question','context','answer_text','answer_start','language']])\n",
        "    va_ds = Dataset.from_pandas(va_df[['id','question','context','answer_text','answer_start','language']])\n",
        "    t0 = time.time()\n",
        "    tr_tok = tr_ds.map(prepare_train_features_fn, batched=True, remove_columns=tr_ds.column_names, desc=f'xlmr512_train_tokenize_f{FOLD}')\n",
        "    va_tok_all = va_ds.map(prepare_pred_features_fn, batched=True, remove_columns=va_ds.column_names, desc=f'xlmr512_valid_tokenize_f{FOLD}')\n",
        "    print(f'Fold {FOLD}: train feats pre-sample={tr_tok.num_rows} | valid feats={va_tok_all.num_rows} | tok_time={time.time()-t0:.1f}s')\n",
        "    # Negative sampling (robust via cls_positions)\n",
        "    smap = np.array(tr_tok['overflow_to_sample_mapping']); sp = np.array(tr_tok['start_positions']); cls_arr = np.array(tr_tok['cls_positions'])\n",
        "    is_pos = (sp != cls_arr)\n",
        "    keep = []; by_ex = collections.defaultdict(list)\n",
        "    for idx, ex in enumerate(smap): by_ex[ex].append(idx)\n",
        "    for ex, idxs in by_ex.items():\n",
        "        idxs = np.array(idxs); pos = idxs[is_pos[idxs]]; neg = idxs[~is_pos[idxs]]\n",
        "        keep.extend(pos.tolist())\n",
        "        if len(pos) == 0:\n",
        "            sel = neg[:min(4, len(neg))]; keep.extend(sel.tolist())\n",
        "        else:\n",
        "            cap = min(len(neg), NEG_RATIO*len(pos))\n",
        "            if cap > 0:\n",
        "                sel = neg[np.random.permutation(len(neg))[:cap]]; keep.extend(sel.tolist())\n",
        "    keep = np.array(sorted(set(keep))); before = tr_tok.num_rows\n",
        "    tr_tok = tr_tok.select(keep.tolist()); after = tr_tok.num_rows\n",
        "    print(f'Fold {FOLD}: neg-sample {before}->{after} ({after/before:.2%})')\n",
        "    # Torch datasets\n",
        "    tr_ds_torch = tr_tok.remove_columns([c for c in tr_tok.column_names if c not in ['input_ids','attention_mask','start_positions','end_positions']])\n",
        "    tr_ds_torch.set_format(type='torch')\n",
        "    va_ds_pred = va_tok_all.remove_columns([c for c in va_tok_all.column_names if c not in ['input_ids','attention_mask']])\n",
        "    va_ds_pred.set_format(type='torch')\n",
        "    # Model\n",
        "    torch.cuda.empty_cache()\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
        "    try: model.gradient_checkpointing_enable()\n",
        "    except Exception: pass\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'xlmr_large_512_f{FOLD}',\n",
        "        num_train_epochs=EPOCHS,\n",
        "        learning_rate=LR,\n",
        "        per_device_train_batch_size=TRAIN_BS,\n",
        "        per_device_eval_batch_size=EVAL_BS,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM,\n",
        "        fp16=False,\n",
        "        bf16=True,\n",
        "        logging_steps=200,\n",
        "        save_strategy='no',\n",
        "        evaluation_strategy='no',\n",
        "        seed=SEED,\n",
        "        dataloader_num_workers=2,\n",
        "        report_to=[],\n",
        "        gradient_checkpointing=True,\n",
        "        warmup_ratio=0.1,\n",
        "        weight_decay=0.01,\n",
        "        max_grad_norm=1.0\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tr_ds_torch,\n",
        "        eval_dataset=None,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer),\n",
        "    )\n",
        "    print(f'Fold {FOLD}: training...')\n",
        "    t1 = time.time()\n",
        "    try:\n",
        "        trainer.train()\n",
        "    except RuntimeError as e:\n",
        "        if 'out of memory' in str(e).lower():\n",
        "            print('OOM encountered; retrying with TRAIN_BS=1, EVAL_BS=2, GRAD_ACCUM=24, and fresh model...')\n",
        "            del trainer, model\n",
        "            torch.cuda.empty_cache()\n",
        "            model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
        "            try: model.gradient_checkpointing_enable()\n",
        "            except Exception: pass\n",
        "            args = TrainingArguments(\n",
        "                output_dir=f'xlmr_large_512_f{FOLD}',\n",
        "                num_train_epochs=EPOCHS,\n",
        "                learning_rate=LR,\n",
        "                per_device_train_batch_size=1,\n",
        "                per_device_eval_batch_size=2,\n",
        "                gradient_accumulation_steps=24,\n",
        "                fp16=False,\n",
        "                bf16=True,\n",
        "                logging_steps=200,\n",
        "                save_strategy='no',\n",
        "                evaluation_strategy='no',\n",
        "                seed=SEED,\n",
        "                dataloader_num_workers=2,\n",
        "                report_to=[],\n",
        "                gradient_checkpointing=True,\n",
        "                warmup_ratio=0.1,\n",
        "                weight_decay=0.01,\n",
        "                max_grad_norm=1.0,\n",
        "            )\n",
        "            trainer = Trainer(\n",
        "                model=model,\n",
        "                args=args,\n",
        "                train_dataset=tr_ds_torch,\n",
        "                eval_dataset=None,\n",
        "                tokenizer=tokenizer,\n",
        "                data_collator=DataCollatorWithPadding(tokenizer),\n",
        "            )\n",
        "            trainer.train()\n",
        "        else:\n",
        "            raise\n",
        "    print(f'Fold {FOLD}: train_done in {time.time()-t1:.1f}s | total_elapsed={time.time()-t_fold:.1f}s')\n",
        "    # Predict valid\n",
        "    print(f'Fold {FOLD}: predicting valid...')\n",
        "    pred = trainer.predict(va_ds_pred)\n",
        "    s_logp = log_softmax(torch.tensor(pred.predictions[0]), dim=-1).numpy()\n",
        "    e_logp = log_softmax(torch.tensor(pred.predictions[1]), dim=-1).numpy()\n",
        "    ex_best = {}\n",
        "    for i in range(len(va_tok_all)):\n",
        "        ex_id = va_tok_all[i]['example_id']\n",
        "        offsets = va_tok_all[i]['offset_mapping']\n",
        "        ctx = va_df.loc[va_df['id'].astype(str)==ex_id, 'context'].values[0]\n",
        "        s = s_logp[i]; e = e_logp[i]\n",
        "        mask = np.array([o is not None for o in offsets], dtype=bool)\n",
        "        s_m = np.where(mask, s, -1e9); e_m = np.where(mask, e, -1e9)\n",
        "        top_s = np.argsort(s_m)[-N_BEST:]; top_e = np.argsort(e_m)[-N_BEST:]\n",
        "        best = (float('-inf'), '')\n",
        "        for si in top_s:\n",
        "            for ei in top_e:\n",
        "                if ei < si: continue\n",
        "                if (ei - si + 1) > MAX_ANSWER_LEN: continue\n",
        "                if offsets[si] is None or offsets[ei] is None: continue\n",
        "                cs,_ = offsets[si]; _,ce = offsets[ei]\n",
        "                if cs is None or ce is None or ce <= cs: continue\n",
        "                cand = ctx[cs:ce]; score = float(s_m[si] + e_m[ei])\n",
        "                if score > best[0]: best = (score, cand)\n",
        "        if best[0] == float('-inf'): best = (-1e9, '')\n",
        "        prev = ex_best.get(ex_id, (float('-inf'), ''))\n",
        "        if best[0] > prev[0]: ex_best[ex_id] = best\n",
        "    preds = {k:v[1] for k,v in ex_best.items()}\n",
        "    y_true = va_df.assign(id_str=va_df['id'].astype(str)).set_index('id_str')['answer_text'].to_dict()\n",
        "    fold_scores = [jaccard_word_level(y_true[k], preds.get(k, '')) for k in y_true.keys()]\n",
        "    f_j = float(np.mean(fold_scores)) if fold_scores else 0.0\n",
        "    all_oof_scores.append(f_j)\n",
        "    print(f'Fold {FOLD} Jaccard: {f_j:.4f} | elapsed_fold={time.time()-t_fold:.1f}s')\n",
        "    for k,v in preds.items():\n",
        "        oof_rows.append({'id': k, 'pred_text': v, 'gt': y_true.get(k, ''), 'fold': FOLD})\n",
        "\n",
        "    # Predict TEST and save logits\n",
        "    print(f'Fold {FOLD}: predicting TEST and saving logits (512 setting)...')\n",
        "    te_pred = trainer.predict(te_ds_pred)\n",
        "    te_start = te_pred.predictions[0]\n",
        "    te_end = te_pred.predictions[1]\n",
        "    np.save(f'xlmr_large_512_test_logits/test_start_logits_f{FOLD}.npy', te_start)\n",
        "    np.save(f'xlmr_large_512_test_logits/test_end_logits_f{FOLD}.npy', te_end)\n",
        "    test_start_list.append(te_start)\n",
        "    test_end_list.append(te_end)\n",
        "\n",
        "    del model, trainer, tr_ds_torch, va_ds_pred, tr_tok, va_tok_all, pred, s_logp, e_logp, te_pred\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Save OOF, mean\n",
        "oof_df = pd.DataFrame(oof_rows)\n",
        "oof_df.to_csv('oof_preds_xlmr_large_512.csv', index=False)\n",
        "mean_oof = float(np.mean(all_oof_scores)) if all_oof_scores else 0.0\n",
        "print('XLM-R Large 512 5-fold OOF Jaccard (mean):', round(mean_oof, 4))\n",
        "\n",
        "# Average TEST logits across folds\n",
        "print('XLM-R Large 512: Averaging TEST logits across folds...')\n",
        "start_stack = np.stack(test_start_list, axis=0)\n",
        "end_stack = np.stack(test_end_list, axis=0)\n",
        "start_avg = start_stack.mean(axis=0)\n",
        "end_avg = end_stack.mean(axis=0)\n",
        "np.savez_compressed('xlmr_large_512_test_avg.npz', start=start_avg, end=end_avg)\n",
        "print('Saved xlmr_large_512_test_avg.npz; example_id/offsets saved under xlmr_large_512_test_logits/')\n",
        "print('XLM-R Large 512 5-fold done in %.1fs' % (time.time()-t_global))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retokenizing with deepset/xlm-roberta-large-squad2 len 512 stride 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_test_tokenize_allfolds:   0%|          | 0/112 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_test_tokenize_allfolds: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112/112 [00:02<00:00, 46.93 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_test_tokenize_allfolds: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112/112 [00:02<00:00, 46.21 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== XLM-R Large 512 Fold 0 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_train_tokenize_f0:   0%|          | 0/817 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_train_tokenize_f0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 817/817 [00:18<00:00, 43.87 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_train_tokenize_f0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 817/817 [00:18<00:00, 43.63 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_valid_tokenize_f0:   0%|          | 0/185 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_valid_tokenize_f0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 185/185 [00:04<00:00, 38.89 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_valid_tokenize_f0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 185/185 [00:04<00:00, 38.07 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: train feats pre-sample=10484 | valid feats=2301 | tok_time=23.7s\nFold 0: neg-sample 10484->2995 (28.57%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='561' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/561 : < :, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: train_done in 1960.8s | total_elapsed=1985.8s\nFold 0: predicting valid...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='576' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/576 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0 Jaccard: 0.7057 | elapsed_fold=2097.2s\nFold 0: predicting TEST and saving logits (512 setting)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/351 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== XLM-R Large 512 Fold 1 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_train_tokenize_f1:   0%|          | 0/807 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_train_tokenize_f1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 807/807 [00:16<00:00, 48.98 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_train_tokenize_f1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 807/807 [00:16<00:00, 48.70 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_valid_tokenize_f1:   0%|          | 0/195 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_valid_tokenize_f1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 195/195 [00:04<00:00, 45.91 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_valid_tokenize_f1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 195/195 [00:04<00:00, 44.95 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: train feats pre-sample=10396 | valid feats=2389 | tok_time=21.0s\nFold 1: neg-sample 10396->2932 (28.20%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='549' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/549 : < :, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: train_done in 2076.0s | total_elapsed=2098.7s\nFold 1: predicting valid...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='598' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/598 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 Jaccard: 0.6167 | elapsed_fold=2305.8s\nFold 1: predicting TEST and saving logits (512 setting)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/351 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== XLM-R Large 512 Fold 2 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_train_tokenize_f2:   0%|          | 0/784 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_train_tokenize_f2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 784/784 [00:16<00:00, 48.92 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_train_tokenize_f2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 784/784 [00:16<00:00, 48.62 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_valid_tokenize_f2:   0%|          | 0/218 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_valid_tokenize_f2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 218/218 [00:05<00:00, 37.41 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_valid_tokenize_f2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 218/218 [00:05<00:00, 36.64 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: train feats pre-sample=9842 | valid feats=2943 | tok_time=22.2s\nFold 2: neg-sample 9842->2810 (28.55%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/525 : < :, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: train_done in 1852.3s | total_elapsed=1877.2s\nFold 2: predicting valid...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='736' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/736 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 Jaccard: 0.6377 | elapsed_fold=2027.6s\nFold 2: predicting TEST and saving logits (512 setting)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/351 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== XLM-R Large 512 Fold 3 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_train_tokenize_f3:   0%|          | 0/795 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_train_tokenize_f3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 795/795 [00:15<00:00, 49.88 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_train_tokenize_f3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 795/795 [00:16<00:00, 49.57 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_valid_tokenize_f3:   0%|          | 0/207 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_valid_tokenize_f3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 207/207 [00:05<00:00, 37.82 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_valid_tokenize_f3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 207/207 [00:05<00:00, 37.10 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: train feats pre-sample=9960 | valid feats=2825 | tok_time=21.7s\nFold 3: neg-sample 9960->2857 (28.68%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOM encountered; retrying with TRAIN_BS=1, EVAL_BS=2, GRAD_ACCUM=24, and fresh model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 7.00 MiB is free. Process 1299294 has 1.68 GiB memory in use. Process 1514573 has 3.27 GiB memory in use. Process 1609549 has 5.15 GiB memory in use. Process 1639108 has 1.15 GiB memory in use. Process 1685557 has 4.50 GiB memory in use. Of the allocated memory 4.77 GiB is allocated by PyTorch, and 8.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 229\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/transformers/trainer.py:1938\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1937\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1938\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/transformers/trainer.py:2341\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2339\u001b[39m         grad_norm = _grad_norm\n\u001b[32m-> \u001b[39m\u001b[32m2341\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_optimizer_step(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/accelerate/optimizer.py:172\u001b[39m, in \u001b[36mAcceleratedOptimizer.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator_state.distributed_type == DistributedType.XLA:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/optim/lr_scheduler.py:130\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/optim/optimizer.py:484\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    481\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    482\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/optim/optimizer.py:89\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     88\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/optim/adamw.py:227\u001b[39m, in \u001b[36mAdamW.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    216\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    217\u001b[39m         group,\n\u001b[32m    218\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    224\u001b[39m         state_steps,\n\u001b[32m    225\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/optim/optimizer.py:161\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/optim/adamw.py:767\u001b[39m, in \u001b[36madamw\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    765\u001b[39m     func = _single_tensor_adamw\n\u001b[32m--> \u001b[39m\u001b[32m767\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    772\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/optim/adamw.py:600\u001b[39m, in \u001b[36m_multi_tensor_adamw\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[39m\n\u001b[32m    599\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     exp_avg_sq_sqrt = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    602\u001b[39m torch._foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n",
            "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 7.00 MiB is free. Process 1299294 has 1.68 GiB memory in use. Process 1514573 has 3.27 GiB memory in use. Process 1609549 has 5.15 GiB memory in use. Process 1639108 has 1.15 GiB memory in use. Process 1685557 has 4.50 GiB memory in use. Of the allocated memory 4.77 GiB is allocated by PyTorch, and 8.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 258\u001b[39m\n\u001b[32m    237\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m: \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    238\u001b[39m     args = TrainingArguments(\n\u001b[32m    239\u001b[39m         output_dir=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mxlmr_large_512_f\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFOLD\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m    240\u001b[39m         num_train_epochs=EPOCHS,\n\u001b[32m   (...)\u001b[39m\u001b[32m    256\u001b[39m         max_grad_norm=\u001b[32m1.0\u001b[39m,\n\u001b[32m    257\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     trainer = \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtr_ds_torch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDataCollatorWithPadding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m     trainer.train()\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/transformers/trainer.py:535\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[39m\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    532\u001b[39m     \u001b[38;5;28mself\u001b[39m.place_model_on_device\n\u001b[32m    533\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33mquantization_method\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == QuantizationMethod.BITS_AND_BYTES\n\u001b[32m    534\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_model_parallel:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/transformers/trainer.py:782\u001b[39m, in \u001b[36mTrainer._move_model_to_device\u001b[39m\u001b[34m(self, model, device)\u001b[39m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m     model = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    783\u001b[39m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[32m    784\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.parallel_mode == ParallelMode.TPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33mtie_weights\u001b[39m\u001b[33m\"\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/transformers/modeling_utils.py:2905\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   2900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   2901\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2902\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2903\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2904\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2905\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:1174\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1171\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1172\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:780\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    778\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    779\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m780\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    784\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    785\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    790\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    791\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:780\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    778\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    779\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m780\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    784\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    785\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    790\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    791\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "    \u001b[31m[... skipping similar frames: Module._apply at line 780 (3 times)]\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:780\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    778\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    779\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m780\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    784\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    785\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    790\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    791\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:805\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    803\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    804\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m805\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    806\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    808\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/nn/modules/module.py:1160\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1153\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1154\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1155\u001b[39m             device,\n\u001b[32m   1156\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1157\u001b[39m             non_blocking,\n\u001b[32m   1158\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1159\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
            "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 7.00 MiB is free. Process 1299294 has 1.68 GiB memory in use. Process 1514573 has 3.27 GiB memory in use. Process 1609549 has 5.15 GiB memory in use. Process 1639108 has 1.15 GiB memory in use. Process 1685557 has 4.50 GiB memory in use. Of the allocated memory 4.77 GiB is allocated by PyTorch, and 8.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "id": "ae6618dd-9639-405b-94e7-4f0837e97c29",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build blended submissions per expert advice (conditional on availability of xlmr512):\n",
        "import shutil, os\n",
        "\n",
        "xlmr512_ready = os.path.exists('xlmr_large_512_test_avg.npz') and os.path.exists('xlmr_large_512_test_logits/test_example_id.json') and os.path.exists('xlmr_large_512_test_logits/test_offset_mapping.npy')\n",
        "\n",
        "if xlmr512_ready:\n",
        "    # Primary weights (xlmr384, muril, mdeberta, xlmr512) = (0.15, 0.25, 0.00, 0.60)\n",
        "    sub = build_submission(blend_weights=(0.15, 0.25, 0.00, 0.60))\n",
        "    if os.path.exists('submission.csv'):\n",
        "        shutil.copy('submission.csv', 'submission_primary_015_025_000_060.csv')\n",
        "\n",
        "    # Safety blends (no MuRIL, no mDeBERTa), 512-heavy\n",
        "    sub_b = build_submission(blend_weights=(0.35, 0.00, 0.00, 0.65))\n",
        "    if os.path.exists('submission.csv'):\n",
        "        shutil.copy('submission.csv', 'submission_safety_035_000_000_065.csv')\n",
        "\n",
        "    sub_c = build_submission(blend_weights=(0.30, 0.00, 0.00, 0.70))\n",
        "    if os.path.exists('submission.csv'):\n",
        "        shutil.copy('submission.csv', 'submission_safety_030_000_000_070.csv')\n",
        "\n",
        "    # Extra safety: 512-only\n",
        "    sub_d = build_submission(blend_weights=(0.00, 0.00, 0.00, 1.00))\n",
        "    if os.path.exists('submission.csv'):\n",
        "        shutil.copy('submission.csv', 'submission_safety_000_000_000_100.csv')\n",
        "\n",
        "    # Optional alt decode variant: same primary weights but LENGTH_PENALTY=0.005 and TEMP=0.90\n",
        "    try:\n",
        "        _old_lp = LENGTH_PENALTY\n",
        "        _old_temp = LOGIT_TEMPERATURE\n",
        "    except NameError:\n",
        "        _old_lp = 0.0045\n",
        "        _old_temp = 1.00\n",
        "    LENGTH_PENALTY = 0.0050\n",
        "    LOGIT_TEMPERATURE = 0.90\n",
        "    sub_alt = build_submission(blend_weights=(0.15, 0.25, 0.00, 0.60))\n",
        "    if os.path.exists('submission.csv'):\n",
        "        shutil.copy('submission.csv', 'submission_primary_altdec_lp0.0050_t0.90_015_025_000_060.csv')\n",
        "    # Restore defaults\n",
        "    LENGTH_PENALTY = _old_lp\n",
        "    LOGIT_TEMPERATURE = _old_temp\n",
        "\n",
        "    # Restore primary as the final submission.csv\n",
        "    sub = build_submission(blend_weights=(0.15, 0.25, 0.00, 0.60))\n",
        "else:\n",
        "    # Fallback: 2-model blends without xlmr512 (order: xlmr384, muril)\n",
        "    sub_2a = build_submission(blend_weights=(0.70, 0.30))\n",
        "    if os.path.exists('submission.csv'):\n",
        "        shutil.copy('submission.csv', 'submission_twomodel_070_030.csv')\n",
        "\n",
        "    sub_2b = build_submission(blend_weights=(0.60, 0.40))\n",
        "    if os.path.exists('submission.csv'):\n",
        "        shutil.copy('submission.csv', 'submission_twomodel_060_040.csv')\n",
        "\n",
        "    # Keep 0.70/0.30 as current default submission.csv\n",
        "    sub = build_submission(blend_weights=(0.70, 0.30))\n",
        "\n",
        "sub.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 30.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 30.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 31.1s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m     shutil.copy(\u001b[33m'\u001b[39m\u001b[33msubmission.csv\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msubmission_safety_030_000_000_070.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Extra safety: 512-only\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m sub_d = \u001b[43mbuild_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblend_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.00\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.00\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.00\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.00\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(\u001b[33m'\u001b[39m\u001b[33msubmission.csv\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     24\u001b[39m     shutil.copy(\u001b[33m'\u001b[39m\u001b[33msubmission.csv\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msubmission_safety_000_000_000_100.csv\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 147\u001b[39m, in \u001b[36mbuild_submission\u001b[39m\u001b[34m(blend_weights, per_language_weights)\u001b[39m\n\u001b[32m    135\u001b[39m xlmr = load_model_char_evidence(\n\u001b[32m    136\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mxlmr_large_test_avg.npz\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    137\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mxlmr_large_test_logits/test_example_id.json\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    138\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mxlmr_large_test_logits/test_offset_mapping.npy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    139\u001b[39m     test_df\n\u001b[32m    140\u001b[39m )\n\u001b[32m    141\u001b[39m muril = load_model_char_evidence(\n\u001b[32m    142\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmuril_large_test_avg.npz\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    143\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmuril_large_test_logits/test_example_id.json\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    144\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmuril_large_test_logits/test_offset_mapping.npy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    145\u001b[39m     test_df\n\u001b[32m    146\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m mdb = \u001b[43mload_model_char_evidence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmdeberta_base_test_avg.npz\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmdeberta_base_test_logits/test_example_id.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmdeberta_base_test_logits/test_offset_mapping.npy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_df\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m xlmr512 = load_model_char_evidence(\n\u001b[32m    154\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mxlmr_large_512_test_avg.npz\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    155\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mxlmr_large_512_test_logits/test_example_id.json\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    156\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mxlmr_large_512_test_logits/test_offset_mapping.npy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    157\u001b[39m     test_df\n\u001b[32m    158\u001b[39m )\n\u001b[32m    159\u001b[39m have_xlmr = xlmr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mload_model_char_evidence\u001b[39m\u001b[34m(avg_npz_path, exid_json, offsets_npy, test_df)\u001b[39m\n\u001b[32m     98\u001b[39m         end_idx = ce - \u001b[32m1\u001b[39m\n\u001b[32m     99\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m0\u001b[39m <= end_idx < \u001b[38;5;28mlen\u001b[39m(ctx):\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m             end_char_map[ex_id][end_idx] += \u001b[38;5;28mfloat\u001b[39m(e_logp[t])\n\u001b[32m    101\u001b[39m             end_cov[ex_id][end_idx] += \u001b[32m1.0\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# normalize by coverage counts to avoid bias from models with more windows\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "c770d805-3532-43d0-bcec-808321e11083",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Utility: free GPU by killing stray Python processes (use with caution)\n",
        "import os, subprocess, signal, time, sys\n",
        "\n",
        "def list_gpu_procs():\n",
        "    try:\n",
        "        out = subprocess.check_output([\n",
        "            'bash','-lc',\n",
        "            \"nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv,noheader,nounits || true\"\n",
        "        ], stderr=subprocess.STDOUT).decode('utf-8', errors='ignore')\n",
        "        procs = []\n",
        "        for line in out.strip().splitlines():\n",
        "            parts = [p.strip() for p in line.split(',')]\n",
        "            if len(parts) >= 3:\n",
        "                try:\n",
        "                    procs.append({'pid': int(parts[0]), 'name': parts[1], 'mem_mb': int(parts[2])})\n",
        "                except Exception:\n",
        "                    pass\n",
        "        return procs\n",
        "    except Exception as e:\n",
        "        print('nvidia-smi query failed:', e)\n",
        "        return []\n",
        "\n",
        "me = os.getpid()\n",
        "print('Current PID:', me)\n",
        "procs = list_gpu_procs()\n",
        "print('GPU processes before:', procs)\n",
        "\n",
        "# Kill other python processes holding GPU (not this PID)\n",
        "killed = []\n",
        "for p in procs:\n",
        "    if p['pid'] != me and ('python' in (p['name'] or '').lower() or (p['mem_mb'] > 0)):\n",
        "        try:\n",
        "            print('Sending SIGTERM to PID', p['pid'], p)\n",
        "            os.kill(p['pid'], signal.SIGTERM)\n",
        "            killed.append(p['pid'])\n",
        "        except Exception as e:\n",
        "            print('SIGTERM failed for', p['pid'], e)\n",
        "\n",
        "time.sleep(2.0)\n",
        "procs_after = list_gpu_procs()\n",
        "print('GPU processes after SIGTERM:', procs_after)\n",
        "\n",
        "# Force kill remaining heavy holders if needed\n",
        "for p in procs_after:\n",
        "    if p['pid'] != me and p['pid'] in killed:\n",
        "        try:\n",
        "            print('Sending SIGKILL to PID', p['pid'])\n",
        "            os.kill(p['pid'], signal.SIGKILL)\n",
        "        except Exception as e:\n",
        "            print('SIGKILL failed for', p['pid'], e)\n",
        "\n",
        "time.sleep(1.0)\n",
        "print('Final GPU processes:', list_gpu_procs())\n",
        "print('GPU cleanup done. If training still OOMs at Trainer init, restart kernel and rerun cell 12.')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current PID: 2977\nGPU processes before: [{'pid': 1299294, 'name': '[Not Found]', 'mem_mb': 1164}, {'pid': 1514573, 'name': '[Not Found]', 'mem_mb': 3352}, {'pid': 1609549, 'name': '[Not Found]', 'mem_mb': 5278}, {'pid': 1639108, 'name': '[Not Found]', 'mem_mb': 1174}, {'pid': 1685557, 'name': '[Not Found]', 'mem_mb': 4608}]\nSending SIGTERM to PID 1299294 {'pid': 1299294, 'name': '[Not Found]', 'mem_mb': 1164}\nSIGTERM failed for 1299294 [Errno 3] No such process\nSending SIGTERM to PID 1514573 {'pid': 1514573, 'name': '[Not Found]', 'mem_mb': 3352}\nSIGTERM failed for 1514573 [Errno 3] No such process\nSending SIGTERM to PID 1609549 {'pid': 1609549, 'name': '[Not Found]', 'mem_mb': 5278}\nSIGTERM failed for 1609549 [Errno 3] No such process\nSending SIGTERM to PID 1639108 {'pid': 1639108, 'name': '[Not Found]', 'mem_mb': 1174}\nSIGTERM failed for 1639108 [Errno 3] No such process\nSending SIGTERM to PID 1685557 {'pid': 1685557, 'name': '[Not Found]', 'mem_mb': 4608}\nSIGTERM failed for 1685557 [Errno 3] No such process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU processes after SIGTERM: [{'pid': 1299294, 'name': '[Not Found]', 'mem_mb': 1164}, {'pid': 1514573, 'name': '[Not Found]', 'mem_mb': 3352}, {'pid': 1609549, 'name': '[Not Found]', 'mem_mb': 5278}, {'pid': 1639108, 'name': '[Not Found]', 'mem_mb': 1174}, {'pid': 1685557, 'name': '[Not Found]', 'mem_mb': 4608}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final GPU processes: [{'pid': 1299294, 'name': '[Not Found]', 'mem_mb': 1164}, {'pid': 1514573, 'name': '[Not Found]', 'mem_mb': 3352}, {'pid': 1609549, 'name': '[Not Found]', 'mem_mb': 5278}, {'pid': 1639108, 'name': '[Not Found]', 'mem_mb': 1174}, {'pid': 1685557, 'name': '[Not Found]', 'mem_mb': 4608}]\nGPU cleanup done. If training still OOMs at Trainer init, restart kernel and rerun cell 12.\n"
          ]
        }
      ]
    },
    {
      "id": "b7a1eedc-f636-456b-b82b-e4aaac5d3adf",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build per-language blended submission (renormalizes by available streams)\n",
        "import os, shutil\n",
        "\n",
        "per_lang = {\n",
        "    'hindi':  (0.35, 0.40, 0.00, 0.25),\n",
        "    'tamil':  (0.45, 0.20, 0.00, 0.35),\n",
        "}\n",
        "\n",
        "sub_lang = build_submission(per_language_weights=per_lang)\n",
        "if os.path.exists('submission.csv'):\n",
        "    shutil.copy('submission.csv', 'submission_per_language.csv')\n",
        "sub_lang.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 28.9s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "          id                                   PredictionString\n0  be799d365                                                   \n1  26f356026                                                   \n2  57a56c43f                                                   \n3  da062fdbb                                                 \u0930\u0915\n4  72fc0d5b5  \u094b\u091c\u093c\u0947\u092b \u0917\u094b\u092f\u092c\u0932\u094d\u0938 \u0915\u094b \u0905\u092a\u0928\u093e \u092a\u094d\u0930\u091a\u093e\u0930\u092e\u0902\u0924\u094d\u0930\u0940 \u0928\u093f\u092f\u0941\u0915\u094d\u0924 \u0915\u093f\u092f...",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>PredictionString</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>be799d365</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26f356026</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>57a56c43f</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>da062fdbb</td>\n      <td>\u0930\u0915</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>72fc0d5b5</td>\n      <td>\u094b\u091c\u093c\u0947\u092b \u0917\u094b\u092f\u092c\u0932\u094d\u0938 \u0915\u094b \u0905\u092a\u0928\u093e \u092a\u094d\u0930\u091a\u093e\u0930\u092e\u0902\u0924\u094d\u0930\u0940 \u0928\u093f\u092f\u0941\u0915\u094d\u0924 \u0915\u093f\u092f...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "733fe11b-b145-4e29-b3e6-d77c2ad3c543",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Tiny decode grid for 2-model blend (xlmr384, muril) and empty-answer fallback; writes multiple CSVs and final submission.csv\n",
        "import os, shutil, pandas as pd\n",
        "\n",
        "def run_blend_with_params(wx=0.75, wm=0.25, n_best=80, len_pen=0.004, temp=0.90, fallback=False, fb_len_pen=0.0, fb_max_len=32, tag=''):\n",
        "    global N_BEST_CHAR, LENGTH_PENALTY, LOGIT_TEMPERATURE, MAX_ANSWER_LEN\n",
        "    prev_max_len = MAX_ANSWER_LEN\n",
        "    N_BEST_CHAR = n_best\n",
        "    LENGTH_PENALTY = len_pen\n",
        "    LOGIT_TEMPERATURE = temp\n",
        "    if not fallback:\n",
        "        sub = build_submission(blend_weights=(wx, wm))\n",
        "        out_name = f'sub_2m_nb{n_best}_lp{len_pen:.4f}_t{temp:.2f}_{wx:0.2f}_{wm:0.2f}{tag}.csv'\n",
        "        if os.path.exists('submission.csv'): shutil.copy('submission.csv', out_name)\n",
        "        return out_name\n",
        "    else:\n",
        "        # fallback re-decode with zero length penalty and tighter max answer length\n",
        "        LENGTH_PENALTY = fb_len_pen\n",
        "        MAX_ANSWER_LEN = fb_max_len\n",
        "        sub_fb = build_submission(blend_weights=(wx, wm))\n",
        "        fb_name = f'sub_2m_fallback_nb{n_best}_lp{fb_len_pen:.4f}_max{fb_max_len}_t{temp:.2f}_{wx:0.2f}_{wm:0.2f}{tag}.csv'\n",
        "        if os.path.exists('submission.csv'): shutil.copy('submission.csv', fb_name)\n",
        "        MAX_ANSWER_LEN = prev_max_len\n",
        "        return fb_name\n",
        "\n",
        "# Grid params per expert advice\n",
        "grid = [\n",
        "    dict(n_best=80, len_pen=0.004, temp=0.90),\n",
        "    dict(n_best=80, len_pen=0.004, temp=1.00),\n",
        "    dict(n_best=80, len_pen=0.005, temp=0.90),\n",
        "    dict(n_best=80, len_pen=0.005, temp=1.00),\n",
        "]\n",
        "\n",
        "wx, wm = 0.75, 0.25\n",
        "generated = []\n",
        "for g in grid:\n",
        "    tag = f\"_nb{g['n_best']}_lp{g['len_pen']:.4f}_t{g['temp']:.2f}\"\n",
        "    base_file = run_blend_with_params(wx=wx, wm=wm, n_best=g['n_best'], len_pen=g['len_pen'], temp=g['temp'], fallback=False, tag=tag)\n",
        "    fb_file = run_blend_with_params(wx=wx, wm=wm, n_best=g['n_best'], len_pen=g['len_pen'], temp=g['temp'], fallback=True, fb_len_pen=0.0, fb_max_len=32, tag=tag)\n",
        "    # Combine: if base PredictionString is empty, take from fallback\n",
        "    try:\n",
        "        b = pd.read_csv(base_file)\n",
        "        f = pd.read_csv(fb_file)\n",
        "        assert (b['id'] == f['id']).all()\n",
        "        comb = b.copy()\n",
        "        mask_empty = comb['PredictionString'].fillna('') == ''\n",
        "        comb.loc[mask_empty, 'PredictionString'] = f.loc[mask_empty, 'PredictionString']\n",
        "        comb_name = base_file.replace('.csv', '_with_fallback.csv')\n",
        "        comb.to_csv(comb_name, index=False)\n",
        "        generated.append(comb_name)\n",
        "    except Exception as e:\n",
        "        print('Combine failed for', base_file, 'and', fb_file, e)\n",
        "\n",
        "# Choose a strong default to write as submission.csv: prefer len_pen=0.004, temp=0.90 with fallback\n",
        "preferred = [p for p in generated if '_lp0.0040_' in p and '_t0.90_' in p]\n",
        "final_file = preferred[0] if preferred else (generated[0] if generated else None)\n",
        "if final_file and os.path.exists(final_file):\n",
        "    shutil.copy(final_file, 'submission.csv')\n",
        "    print('Final submission.csv copied from', final_file)\n",
        "else:\n",
        "    print('No generated submission from grid; keeping existing submission.csv if present.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "1d85b54f-9a8c-4db6-b013-d9668b062c88",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Utility: create interim xlmr_large_512_test_avg.npz from available per-fold logits\n",
        "import os, re, numpy as np, json\n",
        "\n",
        "logdir = 'xlmr_large_512_test_logits'\n",
        "start_files = []; end_files = []\n",
        "if os.path.isdir(logdir):\n",
        "    for fn in os.listdir(logdir):\n",
        "        m = re.match(r'test_start_logits_f(\\d+)\\.npy$', fn)\n",
        "        if m:\n",
        "            f_end = f'test_end_logits_f{m.group(1)}.npy'\n",
        "            if os.path.exists(os.path.join(logdir, f_end)):\n",
        "                start_files.append(os.path.join(logdir, fn))\n",
        "                end_files.append(os.path.join(logdir, f_end))\n",
        "\n",
        "if start_files:\n",
        "    start_stack = np.stack([np.load(fp) for fp in sorted(start_files)], axis=0)\n",
        "    end_stack = np.stack([np.load(fp) for fp in sorted(end_files)], axis=0)\n",
        "    start_avg = start_stack.mean(axis=0)\n",
        "    end_avg = end_stack.mean(axis=0)\n",
        "    np.savez_compressed('xlmr_large_512_test_avg.npz', start=start_avg, end=end_avg)\n",
        "    print(f'Interim xlmr_large_512_test_avg.npz saved using {start_stack.shape[0]} folds.')\n",
        "    # ensure example_id and offsets exist\n",
        "    exid_json = os.path.join(logdir, 'test_example_id.json')\n",
        "    offs_npy = os.path.join(logdir, 'test_offset_mapping.npy')\n",
        "    assert os.path.exists(exid_json) and os.path.exists(offs_npy), 'Missing example_id/offset files for 512 stream'\n",
        "else:\n",
        "    print('No 512 per-fold logits found to average.')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interim xlmr_large_512_test_avg.npz saved using 3 folds.\n"
          ]
        }
      ]
    },
    {
      "id": "fee1bb40-476c-4a51-8dd2-1d8f5a8fba3b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Replace submission.csv with safety blend (global 512-heavy) and ensure no NaNs\n",
        "import os, shutil, pandas as pd\n",
        "src = 'submission_safety_global_015_005_000_080_nb200_lp0.0044.csv'\n",
        "assert os.path.exists(src), f'Missing safety CSV: {src}'\n",
        "df = pd.read_csv(src)\n",
        "if 'PredictionString' in df.columns:\n",
        "    df['PredictionString'] = df['PredictionString'].fillna('')\n",
        "df.to_csv('submission.csv', index=False)\n",
        "print('submission.csv replaced with', src)\n",
        "print('Head:')\n",
        "print(pd.read_csv('submission.csv').head())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv replaced with submission_safety_global_015_005_000_080_nb200_lp0.0044.csv\nHead:\n          id               PredictionString\n0  be799d365                          \u092e\u0941\u0902\u092c\u0908\n1  26f356026  \u0909\u0926\u093e\u0938\u093f\u0928\u093e\u091a\u093e\u0930\u094d\u092f \u0938\u0941\u092e\u0947\u0930\u0941\u0926\u093e\u0938 \u092e\u0939\u093e\u0930\u093e\u091c\n2  57a56c43f          \u0baa\u0bc6\u0bb0\u0bc1\u0bae\u0bc2\u0bb3\u0bc8 \u0b85\u0bb0\u0bc8\u0b95\u0bcd\u0b95\u0bcb\u0bb3\u0b99\u0bcd\u0b95\u0bb3\n3  da062fdbb                       \u092c\u093f\u0902\u092c\u093f\u0938\u093e\u0930\n4  72fc0d5b5                           1889\n"
          ]
        }
      ]
    },
    {
      "id": "b6c3b5ef-a661-48d6-9581-555ef9ea30a1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build 512-only blend and ensure no NaNs in submission.csv\n",
        "import pandas as pd\n",
        "sub_512 = build_submission(blend_weights=(0.00, 0.00, 0.00, 1.00))\n",
        "df = pd.read_csv('submission.csv')\n",
        "if 'PredictionString' in df.columns:\n",
        "    df['PredictionString'] = df['PredictionString'].fillna('')\n",
        "df.to_csv('submission.csv', index=False)\n",
        "print('submission.csv written (512-only weights) with NaNs filled to empty strings. Head:')\n",
        "print(df.head())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 29.0s\nsubmission.csv written (512-only weights) with NaNs filled to empty strings. Head:\n          id PredictionString\n0  be799d365   \u0928\u0947 \u0916\u0930\u0940\u0926\u093e \u0925\u093e\u0964 \u091a\n1  26f356026                 \n2  57a56c43f                 \n3  da062fdbb                 \n4  72fc0d5b5                \u0939\n"
          ]
        }
      ]
    },
    {
      "id": "1e7064ce-e79f-4324-8dd9-d94981ad5c3f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Switch to per-language blend as submission and ensure no NaNs\n",
        "import os, shutil, pandas as pd\n",
        "src = 'submission_per_language.csv'\n",
        "assert os.path.exists(src), f'Missing per-language CSV: {src}'\n",
        "df = pd.read_csv(src)\n",
        "if 'PredictionString' in df.columns:\n",
        "    df['PredictionString'] = df['PredictionString'].fillna('')\n",
        "df.to_csv('submission.csv', index=False)\n",
        "print('submission.csv replaced with per-language blend and NaNs filled. Head:')\n",
        "print(df.head())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv replaced with per-language blend and NaNs filled. Head:\n          id                                   PredictionString\n0  be799d365                                                   \n1  26f356026                                                   \n2  57a56c43f                                                   \n3  da062fdbb                                                 \u0930\u0915\n4  72fc0d5b5  \u094b\u091c\u093c\u0947\u092b \u0917\u094b\u092f\u092c\u0932\u094d\u0938 \u0915\u094b \u0905\u092a\u0928\u093e \u092a\u094d\u0930\u091a\u093e\u0930\u092e\u0902\u0924\u094d\u0930\u0940 \u0928\u093f\u092f\u0941\u0915\u094d\u0924 \u0915\u093f\u092f...\n"
          ]
        }
      ]
    },
    {
      "id": "0003fc33-9a00-4e69-a92b-ab1d91697b31",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Final blends per expert guidance: Primary and Safety\n",
        "import os, shutil, pandas as pd\n",
        "\n",
        "assert 'build_submission' in globals(), 'Run the decoding cell that defines build_submission() first (Cell 9).'\n",
        "\n",
        "# Primary submission: weights (xlmr384, muril, mdeberta, xlmr512) = (0.15, 0.05, 0.00, 0.80)\n",
        "sub_primary = build_submission(blend_weights=(0.15, 0.05, 0.00, 0.80))\n",
        "if os.path.exists('submission.csv'):\n",
        "    # Save a copy and ensure no NaNs\n",
        "    df_p = pd.read_csv('submission.csv')\n",
        "    if 'PredictionString' in df_p.columns:\n",
        "        df_p['PredictionString'] = df_p['PredictionString'].fillna('')\n",
        "    df_p.to_csv('submission.csv', index=False)\n",
        "    shutil.copy('submission.csv', 'submission_primary_final.csv')\n",
        "    print('Primary blend saved to submission.csv and submission_primary_final.csv')\n",
        "\n",
        "# Safety submission: weights (0.25, 0.00, 0.00, 0.75)\n",
        "sub_safety = build_submission(blend_weights=(0.25, 0.00, 0.00, 0.75))\n",
        "if os.path.exists('submission.csv'):\n",
        "    df_s = pd.read_csv('submission.csv')\n",
        "    if 'PredictionString' in df_s.columns:\n",
        "        df_s['PredictionString'] = df_s['PredictionString'].fillna('')\n",
        "    df_s.to_csv('submission_safety_final.csv', index=False)\n",
        "    print('Safety blend saved to submission_safety_final.csv')\n",
        "\n",
        "# Restore primary as the active submission.csv\n",
        "if os.path.exists('submission_primary_final.csv'):\n",
        "    shutil.copy('submission_primary_final.csv', 'submission.csv')\n",
        "    print('submission.csv restored from submission_primary_final.csv')\n",
        "\n",
        "print('Head of submission.csv:')\n",
        "print(pd.read_csv('submission.csv').head())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 28.0s\nPrimary blend saved to submission.csv and submission_primary_final.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 28.3s\nSafety blend saved to submission_safety_final.csv\nsubmission.csv restored from submission_primary_final.csv\nHead of submission.csv:\n          id               PredictionString\n0  be799d365                          \u092e\u0941\u0902\u092c\u0908\n1  26f356026  \u0909\u0926\u093e\u0938\u093f\u0928\u093e\u091a\u093e\u0930\u094d\u092f \u0938\u0941\u092e\u0947\u0930\u0941\u0926\u093e\u0938 \u092e\u0939\u093e\u0930\u093e\u091c\n2  57a56c43f          \u0baa\u0bc6\u0bb0\u0bc1\u0bae\u0bc2\u0bb3\u0bc8 \u0b85\u0bb0\u0bc8\u0b95\u0bcd\u0b95\u0bcb\u0bb3\u0b99\u0bcd\u0b95\u0bb3\n3  da062fdbb                       \u092c\u093f\u0902\u092c\u093f\u0938\u093e\u0930\n4  72fc0d5b5                           1889\n"
          ]
        }
      ]
    },
    {
      "id": "ec7afe1c-a3aa-4b89-89fe-c86852e0b3eb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sanitize final submission.csv: ensure no NaNs and strings only\n",
        "import pandas as pd, os\n",
        "assert os.path.exists('submission.csv'), 'submission.csv not found'\n",
        "df = pd.read_csv('submission.csv')\n",
        "if 'PredictionString' in df.columns:\n",
        "    df['PredictionString'] = df['PredictionString'].astype(str).replace('nan', '').fillna('')\n",
        "else:\n",
        "    raise AssertionError('PredictionString column missing in submission.csv')\n",
        "df.to_csv('submission.csv', index=False)\n",
        "print('submission.csv sanitized. Null count:', df['PredictionString'].isna().sum(), '| Empty count:', (df['PredictionString']=='').sum())\n",
        "print(df.head())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv sanitized. Null count: 0 | Empty count: 71\n          id          PredictionString\n0  be799d365                     \u092e\u0941\u0902\u092c\u0908\n1  26f356026               \u0938\u0941\u0927\u093e\u0902\u0936\u0941\u092c\u093e\u0932\u093e\n2  57a56c43f  \u0baa\u0bc1\u0bb1\u0ba3\u0bbf\u0baa\u0bcd (cerebral cortex\n3  da062fdbb                          \n4  72fc0d5b5            \u0968\u0966 \u0905\u092a\u094d\u0930\u0948\u0932 \u0967\u096e\u096e\u096f\n"
          ]
        }
      ]
    },
    {
      "id": "2554bcb3-fe35-4d55-bea0-427e0c3b27ac",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Resume XLM-R Large 512 training for missing folds only; then rebuild 512 avg npz\n",
        "import os, time, numpy as np, pandas as pd, collections, random, json, torch, re, gc\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "from torch.nn.functional import log_softmax\n",
        "\n",
        "SEED = 2025\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')\n",
        "os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True,max_split_size_mb:64')\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "try: torch.set_float32_matmul_precision('high')\n",
        "except Exception: pass\n",
        "\n",
        "MODEL_NAME = 'deepset/xlm-roberta-large-squad2'\n",
        "MAX_LENGTH = 512\n",
        "DOC_STRIDE = 256\n",
        "MAX_ANSWER_LEN = 64\n",
        "NEG_RATIO = 2\n",
        "EPOCHS = 3\n",
        "LR = 2e-5\n",
        "TRAIN_BS = 1\n",
        "EVAL_BS = 1\n",
        "GRAD_ACCUM = 16\n",
        "N_BEST = 25\n",
        "\n",
        "print('Tokenizer for resume with', MODEL_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "pad_on_right = tokenizer.padding_side == 'right'\n",
        "\n",
        "full_df = pd.read_csv('train.csv').merge(pd.read_csv('folds.csv'), on='id', how='left')\n",
        "assert full_df['fold'].notna().all()\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "def prepare_train_features_fn(examples):\n",
        "    questions = [q.strip() for q in examples['question']]; contexts = examples['context']\n",
        "    tok = tokenizer(\n",
        "        questions if pad_on_right else contexts,\n",
        "        contexts if pad_on_right else questions,\n",
        "        truncation='only_second' if pad_on_right else 'only_first',\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    smap = tok.pop('overflow_to_sample_mapping')\n",
        "    offsets = tok.pop('offset_mapping')\n",
        "    input_ids = tok['input_ids']\n",
        "    start_positions, end_positions, cls_positions = [], [], []\n",
        "    for i, off in enumerate(offsets):\n",
        "        si = smap[i]\n",
        "        ctx = contexts[si]\n",
        "        a_text = str(examples['answer_text'][si])\n",
        "        a_start = int(examples['answer_start'][si])\n",
        "        a_end = a_start + len(a_text)\n",
        "        while a_start < a_end and a_start < len(ctx) and ctx[a_start].isspace(): a_start += 1\n",
        "        while a_end > a_start and a_end-1 < len(ctx) and ctx[a_end-1].isspace(): a_end -= 1\n",
        "        if a_start >= a_end: a_end = a_start\n",
        "        seq_ids = tok.sequence_ids(i)\n",
        "        try:\n",
        "            cls_index = input_ids[i].index(tokenizer.cls_token_id) if tokenizer.cls_token_id is not None else 0\n",
        "        except ValueError:\n",
        "            cls_index = 0\n",
        "        cls_positions.append(cls_index)\n",
        "        context_id = 1 if pad_on_right else 0\n",
        "        try:\n",
        "            ctx_s = seq_ids.index(context_id)\n",
        "            ctx_e = len(seq_ids) - 1 - seq_ids[::-1].index(context_id)\n",
        "        except ValueError:\n",
        "            start_positions.append(cls_index); end_positions.append(cls_index); continue\n",
        "        st_tok = en_tok = None\n",
        "        tgt_end_incl = a_end - 1\n",
        "        for t in range(ctx_s, ctx_e + 1):\n",
        "            o = off[t]\n",
        "            if o is None: continue\n",
        "            ts, te = o\n",
        "            if ts is None or te is None: continue\n",
        "            if st_tok is None and ts <= a_start < te: st_tok = t\n",
        "            if ts <= tgt_end_incl < te: en_tok = t\n",
        "        if st_tok is not None and en_tok is not None and st_tok <= en_tok:\n",
        "            start_positions.append(st_tok); end_positions.append(en_tok)\n",
        "        else:\n",
        "            start_positions.append(cls_index); end_positions.append(cls_index)\n",
        "    tok['start_positions'] = start_positions; tok['end_positions'] = end_positions; tok['overflow_to_sample_mapping'] = smap; tok['cls_positions'] = cls_positions\n",
        "    return tok\n",
        "\n",
        "def prepare_pred_features_fn(examples):\n",
        "    questions = [q.strip() for q in examples['question']]; contexts = examples['context']\n",
        "    tok = tokenizer(\n",
        "        questions if pad_on_right else contexts,\n",
        "        contexts if pad_on_right else questions,\n",
        "        truncation='only_second' if pad_on_right else 'only_first',\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    smap = tok.pop('overflow_to_sample_mapping')\n",
        "    tok['example_id'] = []\n",
        "    new_offsets = []\n",
        "    for i, off in enumerate(tok['offset_mapping']):\n",
        "        si = smap[i]\n",
        "        tok['example_id'].append(str(examples['id'][si]))\n",
        "        seq_ids = tok.sequence_ids(i)\n",
        "        context_id = 1 if pad_on_right else 0\n",
        "        new_offsets.append([o if seq_ids[k] == context_id else None for k, o in enumerate(off)])\n",
        "    tok['offset_mapping'] = new_offsets\n",
        "    return tok\n",
        "\n",
        "# Pre-tokenize TEST once for 512 setting (if not already)\n",
        "logdir = 'xlmr_large_512_test_logits'\n",
        "os.makedirs(logdir, exist_ok=True)\n",
        "exid_json = os.path.join(logdir, 'test_example_id.json')\n",
        "offs_npy = os.path.join(logdir, 'test_offset_mapping.npy')\n",
        "need_tokenize_test = not (os.path.exists(exid_json) and os.path.exists(offs_npy))\n",
        "if need_tokenize_test:\n",
        "    test_ds = Dataset.from_pandas(test_df[['id','question','context','language']])\n",
        "    test_tok_all = test_ds.map(prepare_pred_features_fn, batched=True, remove_columns=test_ds.column_names, desc='xlmr512_test_tokenize_resume')\n",
        "    te_ds_pred = test_tok_all.remove_columns([c for c in test_tok_all.column_names if c not in ['input_ids','attention_mask']])\n",
        "    te_ds_pred.set_format(type='torch')\n",
        "    with open(exid_json, 'w', encoding='utf-8') as f:\n",
        "        json.dump(list(test_tok_all['example_id']), f, ensure_ascii=False)\n",
        "    np.save(offs_npy, np.array(test_tok_all['offset_mapping'], dtype=object), allow_pickle=True)\n",
        "else:\n",
        "    # Build te_ds_pred for prediction\n",
        "    test_ds = Dataset.from_pandas(test_df[['id','question','context','language']])\n",
        "    test_tok_all = test_ds.map(prepare_pred_features_fn, batched=True, remove_columns=test_ds.column_names, desc='xlmr512_test_tokenize_resume_cached')\n",
        "    te_ds_pred = test_tok_all.remove_columns([c for c in test_tok_all.column_names if c not in ['input_ids','attention_mask']])\n",
        "    te_ds_pred.set_format(type='torch')\n",
        "\n",
        "# Determine missing folds\n",
        "existing = set()\n",
        "for f in range(5):\n",
        "    s_fp = os.path.join(logdir, f'test_start_logits_f{f}.npy')\n",
        "    e_fp = os.path.join(logdir, f'test_end_logits_f{f}.npy')\n",
        "    if os.path.exists(s_fp) and os.path.exists(e_fp):\n",
        "        existing.add(f)\n",
        "missing = [f for f in range(5) if f not in existing]\n",
        "print('Existing 512 folds:', sorted(existing), '| Missing:', missing)\n",
        "\n",
        "test_start_list = []; test_end_list = []\n",
        "# Also collect already-saved folds for averaging\n",
        "for f in sorted(existing):\n",
        "    test_start_list.append(np.load(os.path.join(logdir, f'test_start_logits_f{f}.npy')))\n",
        "    test_end_list.append(np.load(os.path.join(logdir, f'test_end_logits_f{f}.npy')))\n",
        "\n",
        "for FOLD in missing:\n",
        "    t_fold = time.time()\n",
        "    print(f'===== Resume XLM-R Large 512 Fold {FOLD} =====', flush=True)\n",
        "    tr_df = full_df[full_df['fold'] != FOLD].reset_index(drop=True)\n",
        "    va_df = full_df[full_df['fold'] == FOLD].reset_index(drop=True)\n",
        "    tr_ds = Dataset.from_pandas(tr_df[['id','question','context','answer_text','answer_start','language']])\n",
        "    va_ds = Dataset.from_pandas(va_df[['id','question','context','answer_text','answer_start','language']])\n",
        "    t0 = time.time()\n",
        "    tr_tok = tr_ds.map(prepare_train_features_fn, batched=True, remove_columns=tr_ds.column_names, desc=f'xlmr512_train_tokenize_f{FOLD}')\n",
        "    print(f'Fold {FOLD}: train feats pre-sample={tr_tok.num_rows} | tok_time={time.time()-t0:.1f}s', flush=True)\n",
        "    # Negative sampling\n",
        "    smap = np.array(tr_tok['overflow_to_sample_mapping']); sp = np.array(tr_tok['start_positions']); cls_arr = np.array(tr_tok['cls_positions'])\n",
        "    is_pos = (sp != cls_arr)\n",
        "    keep = []; by_ex = collections.defaultdict(list)\n",
        "    for idx, ex in enumerate(smap): by_ex[ex].append(idx)\n",
        "    for ex, idxs in by_ex.items():\n",
        "        idxs = np.array(idxs); pos = idxs[is_pos[idxs]]; neg = idxs[~is_pos[idxs]]\n",
        "        keep.extend(pos.tolist())\n",
        "        if len(pos) == 0:\n",
        "            sel = neg[:min(4, len(neg))]; keep.extend(sel.tolist())\n",
        "        else:\n",
        "            cap = min(len(neg), NEG_RATIO*len(pos))\n",
        "            if cap > 0:\n",
        "                sel = neg[np.random.permutation(len(neg))[:cap]]; keep.extend(sel.tolist())\n",
        "    keep = np.array(sorted(set(keep))); before = tr_tok.num_rows\n",
        "    tr_tok = tr_tok.select(keep.tolist()); after = tr_tok.num_rows\n",
        "    print(f'Fold {FOLD}: neg-sample {before}->{after} ({after/before:.2%})', flush=True)\n",
        "    # Torch datasets\n",
        "    tr_ds_torch = tr_tok.remove_columns([c for c in tr_tok.column_names if c not in ['input_ids','attention_mask','start_positions','end_positions']])\n",
        "    tr_ds_torch.set_format(type='torch')\n",
        "    # Model\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
        "    try:\n",
        "        model.config.use_cache = False\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
        "    except Exception:\n",
        "        try: model.gradient_checkpointing_enable()\n",
        "        except Exception: pass\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'xlmr_large_512_f{FOLD}',\n",
        "        num_train_epochs=EPOCHS,\n",
        "        learning_rate=LR,\n",
        "        per_device_train_batch_size=TRAIN_BS,\n",
        "        per_device_eval_batch_size=EVAL_BS,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM,\n",
        "        fp16=False,\n",
        "        bf16=True,\n",
        "        logging_steps=200,\n",
        "        save_strategy='no',\n",
        "        evaluation_strategy='no',\n",
        "        seed=SEED,\n",
        "        dataloader_num_workers=0,\n",
        "        dataloader_pin_memory=False,\n",
        "        eval_accumulation_steps=1,\n",
        "        report_to=[],\n",
        "        gradient_checkpointing=True,\n",
        "        warmup_ratio=0.1,\n",
        "        weight_decay=0.01,\n",
        "        max_grad_norm=1.0\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tr_ds_torch,\n",
        "        eval_dataset=None,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer),\n",
        "    )\n",
        "    print(f'Fold {FOLD}: training...', flush=True)\n",
        "    try:\n",
        "        trainer.train()\n",
        "    except RuntimeError as e:\n",
        "        if 'out of memory' in str(e).lower():\n",
        "            print('OOM; retry with GRAD_ACCUM=24, EVAL_BS=1', flush=True)\n",
        "            del trainer, model\n",
        "            gc.collect(); torch.cuda.empty_cache()\n",
        "            model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
        "            try:\n",
        "                model.config.use_cache = False\n",
        "            except Exception:\n",
        "                pass\n",
        "            try:\n",
        "                model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
        "            except Exception:\n",
        "                try: model.gradient_checkpointing_enable()\n",
        "                except Exception: pass\n",
        "            args = TrainingArguments(\n",
        "                output_dir=f'xlmr_large_512_f{FOLD}',\n",
        "                num_train_epochs=EPOCHS,\n",
        "                learning_rate=LR,\n",
        "                per_device_train_batch_size=1,\n",
        "                per_device_eval_batch_size=1,\n",
        "                gradient_accumulation_steps=24,\n",
        "                fp16=False,\n",
        "                bf16=True,\n",
        "                logging_steps=200,\n",
        "                save_strategy='no',\n",
        "                evaluation_strategy='no',\n",
        "                seed=SEED,\n",
        "                dataloader_num_workers=0,\n",
        "                dataloader_pin_memory=False,\n",
        "                eval_accumulation_steps=1,\n",
        "                report_to=[],\n",
        "                gradient_checkpointing=True,\n",
        "                warmup_ratio=0.1,\n",
        "                weight_decay=0.01,\n",
        "                max_grad_norm=1.0,\n",
        "            )\n",
        "            trainer = Trainer(\n",
        "                model=model,\n",
        "                args=args,\n",
        "                train_dataset=tr_ds_torch,\n",
        "                eval_dataset=None,\n",
        "                tokenizer=tokenizer,\n",
        "                data_collator=DataCollatorWithPadding(tokenizer),\n",
        "            )\n",
        "            try:\n",
        "                trainer.train()\n",
        "            except RuntimeError as e2:\n",
        "                if 'out of memory' in str(e2).lower():\n",
        "                    print('OOM again; retry with GRAD_ACCUM=32, EVAL_BS=1', flush=True)\n",
        "                    del trainer, model\n",
        "                    gc.collect(); torch.cuda.empty_cache()\n",
        "                    model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
        "                    try:\n",
        "                        model.config.use_cache = False\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                    try:\n",
        "                        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
        "                    except Exception:\n",
        "                        try: model.gradient_checkpointing_enable()\n",
        "                        except Exception: pass\n",
        "                    args = TrainingArguments(\n",
        "                        output_dir=f'xlmr_large_512_f{FOLD}',\n",
        "                        num_train_epochs=EPOCHS,\n",
        "                        learning_rate=LR,\n",
        "                        per_device_train_batch_size=1,\n",
        "                        per_device_eval_batch_size=1,\n",
        "                        gradient_accumulation_steps=32,\n",
        "                        fp16=False,\n",
        "                        bf16=True,\n",
        "                        logging_steps=200,\n",
        "                        save_strategy='no',\n",
        "                        evaluation_strategy='no',\n",
        "                        seed=SEED,\n",
        "                        dataloader_num_workers=0,\n",
        "                        dataloader_pin_memory=False,\n",
        "                        eval_accumulation_steps=1,\n",
        "                        report_to=[],\n",
        "                        gradient_checkpointing=True,\n",
        "                        warmup_ratio=0.1,\n",
        "                        weight_decay=0.01,\n",
        "                        max_grad_norm=1.0,\n",
        "                    )\n",
        "                    trainer = Trainer(\n",
        "                        model=model,\n",
        "                        args=args,\n",
        "                        train_dataset=tr_ds_torch,\n",
        "                        eval_dataset=None,\n",
        "                        tokenizer=tokenizer,\n",
        "                        data_collator=DataCollatorWithPadding(tokenizer),\n",
        "                    )\n",
        "                    trainer.train()\n",
        "                else:\n",
        "                    raise\n",
        "        else:\n",
        "            raise\n",
        "    print(f'Fold {FOLD}: train_done in {time.time()-t_fold:.1f}s', flush=True)\n",
        "    # Predict TEST and save logits for this fold\n",
        "    print(f'Fold {FOLD}: predicting TEST and saving logits...', flush=True)\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        te_pred = trainer.predict(te_ds_pred)\n",
        "    te_start = te_pred.predictions[0]\n",
        "    te_end = te_pred.predictions[1]\n",
        "    np.save(os.path.join(logdir, f'test_start_logits_f{FOLD}.npy'), te_start)\n",
        "    np.save(os.path.join(logdir, f'test_end_logits_f{FOLD}.npy'), te_end)\n",
        "    test_start_list.append(te_start)\n",
        "    test_end_list.append(te_end)\n",
        "\n",
        "    del model, trainer, tr_ds_torch, tr_tok, te_pred\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "# Rebuild averaged TEST logits for 512 stream\n",
        "start_files = sorted([os.path.join(logdir, f) for f in os.listdir(logdir) if re.match(r'test_start_logits_f\\d+\\.npy$', f)])\n",
        "end_files = sorted([os.path.join(logdir, f) for f in os.listdir(logdir) if re.match(r'test_end_logits_f\\d+\\.npy$', f)])\n",
        "assert len(start_files) == len(end_files) and len(start_files) >= 3, 'Insufficient 512 folds to average'\n",
        "start_stack = np.stack([np.load(fp) for fp in start_files], axis=0)\n",
        "end_stack = np.stack([np.load(fp) for fp in end_files], axis=0)\n",
        "start_avg = start_stack.mean(axis=0)\n",
        "end_avg = end_stack.mean(axis=0)\n",
        "np.savez_compressed('xlmr_large_512_test_avg.npz', start=start_avg, end=end_avg)\n",
        "print(f'xlmr_large_512_test_avg.npz saved with {start_stack.shape[0]} folds averaged.')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer for resume with deepset/xlm-roberta-large-squad2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_test_tokenize_resume_cached:   0%|          | 0/112 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_test_tokenize_resume_cached: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112/112 [00:03<00:00, 36.57 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_test_tokenize_resume_cached: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112/112 [00:03<00:00, 36.09 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing 512 folds: [0, 1, 2] | Missing: [3, 4]\n===== Resume XLM-R Large 512 Fold 3 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_train_tokenize_f3:   0%|          | 0/795 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_train_tokenize_f3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 795/795 [00:18<00:00, 42.65 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_train_tokenize_f3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 795/795 [00:18<00:00, 42.43 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: train feats pre-sample=9960 | tok_time=18.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: neg-sample 9960->2857 (28.68%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/534 : < :, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: train_done in 2515.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: predicting TEST and saving logits...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='1401' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   1/1401 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Resume XLM-R Large 512 Fold 4 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_train_tokenize_f4:   0%|          | 0/805 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_train_tokenize_f4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 805/805 [00:20<00:00, 39.76 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_train_tokenize_f4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 805/805 [00:20<00:00, 39.52 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: train feats pre-sample=10458 | tok_time=20.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: neg-sample 10458->2894 (27.67%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/540 : < :, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: train_done in 2071.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: predicting TEST and saving logits...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='1401' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   1/1401 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xlmr_large_512_test_avg.npz saved with 5 folds averaged.\n"
          ]
        }
      ]
    },
    {
      "id": "9c3658e5-551d-4b97-b7ad-361fb6816124",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build per-language blend with expert-recommended weights and keep as active submission\n",
        "import os, shutil, pandas as pd\n",
        "assert 'build_submission' in globals(), 'Run Cell 9 first to define build_submission()'\n",
        "per_lang_weights = {\n",
        "    'hindi': (0.30, 0.20, 0.00, 0.50),\n",
        "    'tamil': (0.20, 0.00, 0.00, 0.80),\n",
        "}\n",
        "sub_pl = build_submission(per_language_weights=per_lang_weights)\n",
        "if os.path.exists('submission.csv'):\n",
        "    # Save a copy\n",
        "    shutil.copy('submission.csv', 'submission_per_language_final.csv')\n",
        "    df = pd.read_csv('submission.csv')\n",
        "    if 'PredictionString' in df.columns:\n",
        "        df['PredictionString'] = df['PredictionString'].fillna('')\n",
        "        df.to_csv('submission.csv', index=False)\n",
        "    print('Per-language blend saved to submission_per_language_final.csv and active in submission.csv')\n",
        "print(pd.read_csv('submission.csv').head())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 27.8s\nPer-language blend saved to submission_per_language_final.csv and active in submission.csv\n          id PredictionString\n0  be799d365            \u092e\u0941\u0902\u092c\u0908\n1  26f356026      \u0938\u0941\u0927\u093e\u0902\u0936\u0941\u092c\u093e\u0932\u093e\n2  57a56c43f            \u0baa\u0bc1\u0bb1\u0ba3\u0bbf\n3  da062fdbb         \u092c\u093f\u0902\u092c\u093f\u0938\u093e\u0930\n4  72fc0d5b5   \u0968\u0966 \u0905\u092a\u094d\u0930\u0948\u0932 \u0967\u096e\u096e\u096f\n"
          ]
        }
      ]
    },
    {
      "id": "cfc5d687-cb15-43be-afea-547e3cea0b27",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Decode/weight sweep using existing artifacts to generate multiple candidate submissions\n",
        "import os, shutil, pandas as pd, time\n",
        "assert 'build_submission' in globals(), 'Run Cell 9 first to define build_submission()'\n",
        "\n",
        "def set_decode_params(n_best=None, len_pen=None):\n",
        "    global N_BEST_CHAR, LENGTH_PENALTY\n",
        "    if n_best is not None:\n",
        "        N_BEST_CHAR = int(n_best)\n",
        "    if len_pen is not None:\n",
        "        LENGTH_PENALTY = float(len_pen)\n",
        "\n",
        "out_files = []\n",
        "combos = [\n",
        "    # (weights tuple, tag, per_language)\n",
        "    ((0.15, 0.05, 0.00, 0.80), 'w015_005_000_080', None),\n",
        "    ((0.20, 0.05, 0.00, 0.75), 'w020_005_000_075', None),\n",
        "    ((0.10, 0.05, 0.00, 0.85), 'w010_005_000_085', None),\n",
        "    ((0.00, 0.00, 0.00, 1.00), 'w000_000_000_100', None),\n",
        "]\n",
        "\n",
        "nbests = [120, 150, 200]\n",
        "len_pens = [0.0044, 0.0048, 0.0052]\n",
        "\n",
        "t0 = time.time()\n",
        "for wts, wtag, plw in combos:\n",
        "    for nb in nbests:\n",
        "        for lp in len_pens:\n",
        "            set_decode_params(n_best=nb, len_pen=lp)\n",
        "            sub = build_submission(blend_weights=wts, per_language_weights=plw)\n",
        "            out_name = f'sub_grid_{wtag}_nb{nb}_lp{lp:.4f}.csv'\n",
        "            if os.path.exists('submission.csv'):\n",
        "                df = pd.read_csv('submission.csv')\n",
        "                if 'PredictionString' in df.columns:\n",
        "                    df['PredictionString'] = df['PredictionString'].fillna('')\n",
        "                df.to_csv(out_name, index=False)\n",
        "                out_files.append(out_name)\n",
        "                print('Wrote', out_name)\n",
        "\n",
        "# Per-language variants (fixed params)\n",
        "set_decode_params(n_best=150, len_pen=0.0048)\n",
        "per_lang_variants = [\n",
        "    ({'hindi': (0.30,0.20,0.00,0.50), 'tamil': (0.20,0.00,0.00,0.80)}, 'pl_hi030_020_000_050_ta020_000_000_080'),\n",
        "    ({'hindi': (0.25,0.15,0.00,0.60), 'tamil': (0.15,0.00,0.00,0.85)}, 'pl_hi025_015_000_060_ta015_000_000_085'),\n",
        "]\n",
        "for plw, tag in per_lang_variants:\n",
        "    sub = build_submission(per_language_weights=plw)\n",
        "    out_name = f'sub_grid_{tag}_nb150_lp0.0048.csv'\n",
        "    if os.path.exists('submission.csv'):\n",
        "        df = pd.read_csv('submission.csv')\n",
        "        if 'PredictionString' in df.columns:\n",
        "            df['PredictionString'] = df['PredictionString'].fillna('')\n",
        "        df.to_csv(out_name, index=False)\n",
        "        out_files.append(out_name)\n",
        "        print('Wrote', out_name)\n",
        "\n",
        "print(f'Decode/weight sweep complete: {len(out_files)} files in {time.time()-t0:.1f}s')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 28.5s\nWrote sub_grid_w015_005_000_080_nb120_lp0.0044.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 28.4s\nWrote sub_grid_w015_005_000_080_nb120_lp0.0048.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 27.7s\nWrote sub_grid_w015_005_000_080_nb120_lp0.0052.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 28.1s\nWrote sub_grid_w015_005_000_080_nb150_lp0.0044.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 29.0s\nWrote sub_grid_w015_005_000_080_nb150_lp0.0048.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 28.1s\nWrote sub_grid_w015_005_000_080_nb150_lp0.0052.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 29.7s\nWrote sub_grid_w015_005_000_080_nb200_lp0.0044.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 30.1s\nWrote sub_grid_w015_005_000_080_nb200_lp0.0048.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 28.5s\nWrote sub_grid_w015_005_000_080_nb200_lp0.0052.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 28.5s\nWrote sub_grid_w020_005_000_075_nb120_lp0.0044.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 28.2s\nWrote sub_grid_w020_005_000_075_nb120_lp0.0048.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 28.9s\nWrote sub_grid_w020_005_000_075_nb120_lp0.0052.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 28.5s\nWrote sub_grid_w020_005_000_075_nb150_lp0.0044.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 27.2s\nWrote sub_grid_w020_005_000_075_nb150_lp0.0048.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 27.8s\nWrote sub_grid_w020_005_000_075_nb150_lp0.0052.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 28.0s\nWrote sub_grid_w020_005_000_075_nb200_lp0.0044.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 27.5s\nWrote sub_grid_w020_005_000_075_nb200_lp0.0048.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 27.7s\nWrote sub_grid_w020_005_000_075_nb200_lp0.0052.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 28.0s\nWrote sub_grid_w010_005_000_085_nb120_lp0.0044.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 28.2s\nWrote sub_grid_w010_005_000_085_nb120_lp0.0048.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 28.2s\nWrote sub_grid_w010_005_000_085_nb120_lp0.0052.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 27.9s\nWrote sub_grid_w010_005_000_085_nb150_lp0.0044.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 29.6s\nWrote sub_grid_w010_005_000_085_nb150_lp0.0048.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 29.9s\nWrote sub_grid_w010_005_000_085_nb150_lp0.0052.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 29.9s\nWrote sub_grid_w010_005_000_085_nb200_lp0.0044.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 28.2s\nWrote sub_grid_w010_005_000_085_nb200_lp0.0048.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 28.7s\nWrote sub_grid_w010_005_000_085_nb200_lp0.0052.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 29.5s\nWrote sub_grid_w000_000_000_100_nb120_lp0.0044.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 28.5s\nWrote sub_grid_w000_000_000_100_nb120_lp0.0048.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 29.2s\nWrote sub_grid_w000_000_000_100_nb120_lp0.0052.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 27.8s\nWrote sub_grid_w000_000_000_100_nb150_lp0.0044.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 27.7s\nWrote sub_grid_w000_000_000_100_nb150_lp0.0048.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 27.9s\nWrote sub_grid_w000_000_000_100_nb150_lp0.0052.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 28.7s\nWrote sub_grid_w000_000_000_100_nb200_lp0.0044.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 29.1s\nWrote sub_grid_w000_000_000_100_nb200_lp0.0048.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 27.6s\nWrote sub_grid_w000_000_000_100_nb200_lp0.0052.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 29.1s\nWrote sub_grid_pl_hi030_020_000_050_ta020_000_000_080_nb150_lp0.0048.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 28.9s\nWrote sub_grid_pl_hi025_015_000_060_ta015_000_000_085_nb150_lp0.0048.csv\nDecode/weight sweep complete: 38 files in 1083.2s\n"
          ]
        }
      ]
    },
    {
      "id": "a56353c2-1cb2-4f1f-a066-60df7af5aa2d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Final submissions per expert advice: Primary per-language and one Safety variant\n",
        "import os, shutil, pandas as pd\n",
        "assert 'build_submission' in globals(), 'Run Cell 9 first to define build_submission()'\n",
        "\n",
        "# Set decode params\n",
        "N_BEST_CHAR = 150\n",
        "LENGTH_PENALTY = 0.0048\n",
        "APPLY_SMOOTHING = False  # primary: no smoothing\n",
        "\n",
        "# Per-language weights (xlmr384, muril, mdeberta, xlmr512)\n",
        "per_lang_weights = {\n",
        "    'hindi': (0.25, 0.15, 0.00, 0.60),\n",
        "    'tamil': (0.15, 0.00, 0.00, 0.85),\n",
        "}\n",
        "\n",
        "print('Building PRIMARY per-language submission...')\n",
        "sub_primary = build_submission(per_language_weights=per_lang_weights)\n",
        "if os.path.exists('submission.csv'):\n",
        "    df = pd.read_csv('submission.csv')\n",
        "    if 'PredictionString' in df.columns:\n",
        "        df['PredictionString'] = df['PredictionString'].fillna('')\n",
        "        df.to_csv('submission.csv', index=False)\n",
        "    shutil.copy('submission.csv', 'submission_primary_perlang.csv')\n",
        "    print('Saved submission_primary_perlang.csv and kept as active submission.csv')\n",
        "\n",
        "# Safety variant (choose ONE). We generate both but keep PRIMARY active.\n",
        "# A) Global 512-heavy, no smoothing, nb=200, lp=0.0044\n",
        "print('Building SAFETY (global 512-heavy) submission...')\n",
        "N_BEST_CHAR = 200\n",
        "LENGTH_PENALTY = 0.0044\n",
        "APPLY_SMOOTHING = False\n",
        "sub_safety_global = build_submission(blend_weights=(0.15, 0.05, 0.00, 0.80))\n",
        "if os.path.exists('submission.csv'):\n",
        "    df_g = pd.read_csv('submission.csv')\n",
        "    if 'PredictionString' in df_g.columns:\n",
        "        df_g['PredictionString'] = df_g['PredictionString'].fillna('')\n",
        "    df_g.to_csv('submission_safety_global_015_005_000_080_nb200_lp0.0044.csv', index=False)\n",
        "    print('Saved submission_safety_global_015_005_000_080_nb200_lp0.0044.csv')\n",
        "\n",
        "# B) Smoothing probe: same per-language weights, smoothing on, revert decode params to primary\n",
        "print('Building SECONDARY (smoothed per-language) submission...')\n",
        "N_BEST_CHAR = 150\n",
        "LENGTH_PENALTY = 0.0048\n",
        "APPLY_SMOOTHING = True\n",
        "sub_smoothed = build_submission(per_language_weights=per_lang_weights)\n",
        "if os.path.exists('submission.csv'):\n",
        "    df_s = pd.read_csv('submission.csv')\n",
        "    if 'PredictionString' in df_s.columns:\n",
        "        df_s['PredictionString'] = df_s['PredictionString'].fillna('')\n",
        "    df_s.to_csv('submission_secondary_smoothed.csv', index=False)\n",
        "    print('Saved submission_secondary_smoothed.csv')\n",
        "\n",
        "# Restore PRIMARY as active submission.csv\n",
        "if os.path.exists('submission_primary_perlang.csv'):\n",
        "    shutil.copy('submission_primary_perlang.csv', 'submission.csv')\n",
        "    print('submission.csv restored to PRIMARY per-language')\n",
        "\n",
        "# Sanitize final active submission\n",
        "df_final = pd.read_csv('submission.csv')\n",
        "if 'PredictionString' in df_final.columns:\n",
        "    df_final['PredictionString'] = df_final['PredictionString'].astype(str).replace('nan', '').fillna('')\n",
        "df_final.to_csv('submission.csv', index=False)\n",
        "print('Final submission.csv sanitized. Head:')\n",
        "print(df_final.head())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building PRIMARY per-language submission...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 27.6s\nSaved submission_primary_perlang.csv and kept as active submission.csv\nBuilding SAFETY (global 512-heavy) submission...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 27.5s\nSaved submission_safety_global_015_005_000_080_nb200_lp0.0044.csv\nBuilding SECONDARY (smoothed per-language) submission...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 27.7s\nSaved submission_secondary_smoothed.csv\nsubmission.csv restored to PRIMARY per-language\nFinal submission.csv sanitized. Head:\n          id PredictionString\n0  be799d365            \u092e\u0941\u0902\u092c\u0908\n1  26f356026      \u0938\u0941\u0927\u093e\u0902\u0936\u0941\u092c\u093e\u0932\u093e\n2  57a56c43f            \u0baa\u0bc1\u0bb1\u0ba3\u0bbf\n3  da062fdbb         \u092c\u093f\u0902\u092c\u093f\u0938\u093e\u0930\n4  72fc0d5b5   \u0968\u0966 \u0905\u092a\u094d\u0930\u0948\u0932 \u0967\u096e\u096e\u096f\n"
          ]
        }
      ]
    },
    {
      "id": "b3a65da3-673a-4e34-91c9-cad27a01aa7b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Activate smoothing variant as submission and sanitize\n",
        "import os, pandas as pd, shutil\n",
        "src = 'submission_secondary_smoothed.csv'\n",
        "assert os.path.exists(src), f'Missing smoothed CSV: {src}'\n",
        "df = pd.read_csv(src)\n",
        "if 'PredictionString' in df.columns:\n",
        "    df['PredictionString'] = df['PredictionString'].astype(str).replace('nan', '').fillna('')\n",
        "df.to_csv('submission.csv', index=False)\n",
        "print('submission.csv replaced with', src)\n",
        "print('Head:')\n",
        "print(pd.read_csv('submission.csv').head())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv replaced with submission_secondary_smoothed.csv\nHead:\n          id                                   PredictionString\n0  be799d365                                                  \u096d\n1  26f356026                 \u0938\u0926\u094d\u0917\u0941\u0930\u0941 \u0936\u094d\u0930\u0947\u0923\u0940:\u092a\u0930\u092e\u0939\u0902\u0938 \u0936\u094d\u0930\u0947\u0923\u0940:\u0939\u093f\u0928\u094d\u0926\n2  57a56c43f  \u0baa\u0bc6\u0bb0\u0bc1\u0bae\u0bc2\u0bb3\u0bc8\u0baa\u0bcd \u0baa\u0bc1\u0bb1\u0ba3\u0bbf\u0baf\u0bbf\u0bb2\u0bcd \u0b89\u0bb3\u0bcd\u0bb3 \u0baa\u0bbe\u0bb0\u0bcd\u0bb5\u0bc8\u0baa\u0bcd \u0baa\u0bc1\u0bb1\u0ba3\u0bbf\u0baf\u0bbf\u0bb2\u0bcd (...\n3  da062fdbb      \u092d \u0928\u0947 \u091b\u0940\u0928 \u0932\u093f\u092f\u093e \u0925\u093e\u0964 \u0909\u0938\u0915\u0947 \u0930\u093e\u091c\u0924\u094d\u0935\u0915\u093e\u0932 \u092e\u0947\u0902 \u0939\u0940 \u0935\u093f\u0921\u0942\u0921\n4  72fc0d5b5                                              \u092b \u0917\u094b\u092f\n"
          ]
        }
      ]
    },
    {
      "id": "429a8f82-8061-44f5-b0a8-dd03091489db",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Aggressive probe: 512-only with N_BEST_CHAR=250, LENGTH_PENALTY=0.0040\n",
        "import os, pandas as pd, shutil\n",
        "assert 'build_submission' in globals(), 'Run Cell 9 first to define build_submission()'\n",
        "N_BEST_CHAR = 250\n",
        "LENGTH_PENALTY = 0.0040\n",
        "APPLY_SMOOTHING = False\n",
        "sub_aggr = build_submission(blend_weights=(0.00, 0.00, 0.00, 1.00))\n",
        "out = 'submission_aggressive_512only_nb250_lp0.0040.csv'\n",
        "if os.path.exists('submission.csv'):\n",
        "    df = pd.read_csv('submission.csv')\n",
        "    if 'PredictionString' in df.columns:\n",
        "        df['PredictionString'] = df['PredictionString'].astype(str).replace('nan', '').fillna('')\n",
        "        df.to_csv(out, index=False)\n",
        "        shutil.copy(out, 'submission.csv')\n",
        "        print('Saved and activated', out)\n",
        "    else:\n",
        "        print('Warning: PredictionString column missing in generated submission.csv')\n",
        "else:\n",
        "    print('No submission.csv found after build_submission()')\n",
        "print(pd.read_csv('submission.csv').head())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 27.7s\nSaved and activated submission_aggressive_512only_nb250_lp0.0040.csv\n          id               PredictionString\n0  be799d365              \u092e\u0941\u0902\u092c\u0908, \u092e\u0939\u093e\u0930\u093e\u0937\u094d\u091f\u094d\u0930\n1  26f356026  \u0909\u0926\u093e\u0938\u093f\u0928\u093e\u091a\u093e\u0930\u094d\u092f \u0938\u0941\u092e\u0947\u0930\u0941\u0926\u093e\u0938 \u092e\u0939\u093e\u0930\u093e\u091c\n2  57a56c43f                          \u0baa\u0bc1\u0bb1\u0ba3\u0bbf\n3  da062fdbb                       \u092c\u093f\u0902\u092c\u093f\u0938\u093e\u0930\n4  72fc0d5b5                           1889\n"
          ]
        }
      ]
    },
    {
      "id": "6a63fcbb-aff9-4812-994e-b1406c99767b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build per-language submission with expert one-shot settings\n",
        "import os, shutil, pandas as pd\n",
        "assert 'build_submission' in globals(), 'Run Cell 9 first to define build_submission()'\n",
        "\n",
        "APPLY_SMOOTHING = False  # global off; Tamil end-only smoothing is handled inside build loop\n",
        "\n",
        "per_lang_weights = {\n",
        "    'hindi': (0.25, 0.20, 0.00, 0.55),\n",
        "    'tamil': (0.18, 0.00, 0.00, 0.82),\n",
        "}\n",
        "print('Building per-language submission (hi: 0.25/0.20/0/0.55, ta: 0.18/0/0/0.82) ...')\n",
        "sub = build_submission(per_language_weights=per_lang_weights)\n",
        "if os.path.exists('submission.csv'):\n",
        "    df = pd.read_csv('submission.csv')\n",
        "    if 'PredictionString' in df.columns:\n",
        "        df['PredictionString'] = df['PredictionString'].astype(str).replace('nan', '').fillna('')\n",
        "        df.to_csv('submission.csv', index=False)\n",
        "    out_name = 'submission_perlang_expert_nb_langlp_temps.csv'\n",
        "    shutil.copy('submission.csv', out_name)\n",
        "    print('Saved', out_name, 'and kept as active submission.csv')\n",
        "print(pd.read_csv('submission.csv').head())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building per-language submission (hi: 0.25/0.20/0/0.55, ta: 0.18/0/0/0.82) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (112, 2) in 29.1s\nSaved submission_perlang_expert_nb_langlp_temps.csv and kept as active submission.csv\n          id          PredictionString\n0  be799d365                     \u092e\u0941\u0902\u092c\u0908\n1  26f356026               \u0938\u0941\u0927\u093e\u0902\u0936\u0941\u092c\u093e\u0932\u093e\n2  57a56c43f  \u0baa\u0bc1\u0bb1\u0ba3\u0bbf\u0baa\u0bcd (cerebral cortex\n3  da062fdbb                       NaN\n4  72fc0d5b5            \u0968\u0966 \u0905\u092a\u094d\u0930\u0948\u0932 \u0967\u096e\u096e\u096f\n"
          ]
        }
      ]
    },
    {
      "id": "b887df1d-c779-4ac0-9ecb-2fda498f0b9c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Span-level decode: XLM-R large 512 only, per-language n_best/len_pen/max_len (aggressive knobs) + extra hygiene\n",
        "import numpy as np, pandas as pd, json, os, time, re, unicodedata, shutil\n",
        "\n",
        "def span_level_decode_xlmr512():\n",
        "    t0 = time.time()\n",
        "    # Prefer 3-seed averaged logits for safety submission; else 2-seed; else single\n",
        "    if os.path.exists('xlmr_large_512_3seeds_avg.npz'):\n",
        "        npz = 'xlmr_large_512_3seeds_avg.npz'\n",
        "    elif os.path.exists('xlmr_large_512_2seeds_avg.npz'):\n",
        "        npz = 'xlmr_large_512_2seeds_avg.npz'\n",
        "    else:\n",
        "        npz = 'xlmr_large_512_test_avg.npz'\n",
        "    exid_json = 'xlmr_large_512_test_logits/test_example_id.json'\n",
        "    offs_npy = 'xlmr_large_512_test_logits/test_offset_mapping.npy'\n",
        "    assert os.path.exists(npz) and os.path.exists(exid_json) and os.path.exists(offs_npy), 'Missing 512 artifacts'\n",
        "    data = np.load(npz)\n",
        "    start_logits = data['start']  # [n_feat, L]\n",
        "    end_logits = data['end']\n",
        "    with open(exid_json, 'r', encoding='utf-8') as f:\n",
        "        example_ids = json.load(f)\n",
        "    offsets_list = np.load(offs_npy, allow_pickle=True)\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    id2ctx = dict(zip(test_df['id'].astype(str), test_df['context'].astype(str)))\n",
        "    id2lang = dict(zip(test_df['id'].astype(str), test_df['language'].astype(str) if 'language' in test_df.columns else ['unknown']*len(test_df)))\n",
        "\n",
        "    def _stable_logsoftmax(arr):\n",
        "        m = np.max(arr)\n",
        "        return arr - (m + np.log(np.sum(np.exp(arr - m))))\n",
        "\n",
        "    def best_span_for_feature(ex_id, lang, offsets, s_logit, e_logit):\n",
        "        # mask\n",
        "        mask = np.array([o is not None and o[0] is not None and o[1] is not None and (o[1] > o[0]) for o in offsets], dtype=bool)\n",
        "        s = np.where(mask, s_logit, -1e9)\n",
        "        e = np.where(mask, e_logit, -1e9)\n",
        "        # per-lang knobs (primary-safe settings)\n",
        "        lang_l = str(lang).lower().strip()\n",
        "        if lang_l == 'hindi':\n",
        "            n_best = 180; max_len = 52; len_pen = 0.0050\n",
        "        elif lang_l == 'tamil':\n",
        "            n_best = 220; max_len = 60; len_pen = 0.0040  # tightened per expert advice\n",
        "        else:\n",
        "            n_best = 150; max_len = 64; len_pen = 0.0048\n",
        "        # log-softmax\n",
        "        s_logp = _stable_logsoftmax(s)\n",
        "        e_logp = _stable_logsoftmax(e)\n",
        "        top_s = np.argsort(s_logp)[-n_best:]\n",
        "        best = (float('-inf'), None, None)\n",
        "        for si in top_s:\n",
        "            ei_max = min(len(e_logp)-1, si + max_len - 1)\n",
        "            band = e_logp[si:ei_max+1]\n",
        "            if band.size == 0: continue\n",
        "            off = int(np.argmax(band))\n",
        "            ei = si + off\n",
        "            L = ei - si + 1\n",
        "            score = float(s_logp[si] + e_logp[ei] - len_pen * (L - 1))\n",
        "            if score > best[0]:\n",
        "                best = (score, si, ei)\n",
        "        if best[1] is None:\n",
        "            return (-1e9, '')\n",
        "        cs, _ = offsets[best[1]]; _, ce = offsets[best[2]]\n",
        "        if cs is None or ce is None or ce <= cs:\n",
        "            return (-1e9, '')\n",
        "        ctx = id2ctx.get(ex_id, '')\n",
        "        return (best[0], ctx[cs:ce])\n",
        "\n",
        "    preds = {}  # ex_id -> (score, text)\n",
        "    n_feat = start_logits.shape[0]\n",
        "    for i in range(n_feat):\n",
        "        ex_id = example_ids[i]\n",
        "        lang = id2lang.get(ex_id, 'unknown')\n",
        "        score, text = best_span_for_feature(ex_id, lang, offsets_list[i], start_logits[i], end_logits[i])\n",
        "        prev = preds.get(ex_id, (float('-inf'), ''))\n",
        "        if score > prev[0]:\n",
        "            preds[ex_id] = (score, text)\n",
        "\n",
        "    # Post-processing: safe trims + collapse spaces + remove ZWNJ/ZWJ and BOM/LRM/RLM, hyphen collapse, ZWSP/soft hyphen removal\n",
        "    def strip_trailing_punct(s: str) -> str:\n",
        "        PUNCT_STRIP = '\\u0964\\u0965\u0964,:;.!?\\\"\\'\\\u201c\\\u201d\\\u2018\\\u2019\\)\\]\\}\\|\\-\\s'\n",
        "        return s.strip().strip(PUNCT_STRIP) if s else s\n",
        "    def trim_boundary_combining(s: str) -> str:\n",
        "        if not s: return s\n",
        "        if len(s) > 0 and unicodedata.combining(s[0]): s = s[1:]\n",
        "        if len(s) > 0 and unicodedata.combining(s[-1]): s = s[:-1]\n",
        "        return s\n",
        "    def remove_unmatched_quotes(s: str) -> str:\n",
        "        if not s: return s\n",
        "        for lq, rq in [(\"\\\"\",\"\\\"\"), (\"'\",\"'\"), ('\\u201c','\\u201d'), ('\\u2018','\\u2019')]:\n",
        "            if s.startswith(lq) and not s.endswith(rq): s = s[len(lq):]\n",
        "            if s.endswith(rq) and not s.startswith(lq): s = s[:-len(rq)]\n",
        "        return s\n",
        "\n",
        "    out_ids = test_df['id'].astype(str).values\n",
        "    out_pred = []\n",
        "    for ex_id in out_ids:\n",
        "        lang = str(id2lang.get(ex_id, 'unknown')).lower().strip()\n",
        "        ans = preds.get(ex_id, (float('-inf'), ''))[1]\n",
        "        ans = re.sub(r'(?<=\\d)[ ,._-](?=\\d)', '', ans or '')\n",
        "        ans = ans.replace('\\u094D\\u0964', '\\u0964').replace('\\u094D\u0964', '\u0964')\n",
        "        ans = re.sub(r'[\u0964]+', '\u0964', ans)\n",
        "        if ans.endswith('\u0964'): ans = ans[:-1]\n",
        "        # trailing virama/combining guard\n",
        "        if ans and (unicodedata.category(ans[-1]) == 'Mn' or ans[-1] == '\\u094D'): ans = ans[:-1]\n",
        "        # Hindi-specific: drop isolated trailing virama after space/punct\n",
        "        if lang == 'hindi' and len(ans) >= 2 and ans[-1] == '\\u094D' and (ans[-2].isspace() or ans[-2] in '\u0964,:;.!?\"\\'\\u0964\\u0965-'):\n",
        "            ans = ans[:-1]\n",
        "        # Tamil-specific: drop trailing combining mark once\n",
        "        if lang == 'tamil' and len(ans) and unicodedata.category(ans[-1]) == 'Mn':\n",
        "            ans = ans[:-1]\n",
        "        if ans and unicodedata.category(ans[0]) == 'Mn': ans = ans[1:]\n",
        "        # collapse repeated hyphens/dashes\n",
        "        ans = re.sub(r'(?<=\\S)[\\-\u2013\u2014]{2,}(?=\\S)', '-', ans)\n",
        "        ans = strip_trailing_punct(ans)\n",
        "        ans = trim_boundary_combining(ans)\n",
        "        ans = remove_unmatched_quotes(ans)\n",
        "        # remove BOM/LRM/RLM and ZWNJ/ZWJ; collapse NBSP/thin space; remove ZWSP/soft hyphen\n",
        "        ans = ans.replace('\\ufeff','').replace('\\u200e','').replace('\\u200f','')\n",
        "        ans = ans.replace('\\u200B','').replace('\\u00AD','')\n",
        "        ans = ans.replace('\\u00A0',' ').replace('\\u2009',' ')\n",
        "        ans = ' '.join(ans.split())\n",
        "        ans = ans.replace('\\u200C','').replace('\\u200D','')\n",
        "        out_pred.append(ans or '')\n",
        "\n",
        "    sub = pd.DataFrame({'id': out_ids, 'PredictionString': out_pred})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    # Also save a named backup (3-seed 512-only safety)\n",
        "    backup_name = 'submission_safety_512only_3seeds.csv'\n",
        "    sub.to_csv(backup_name, index=False)\n",
        "    print('Span-level 512-only submission.csv saved', sub.shape, 'in %.1fs' % (time.time()-t0), '| backup:', backup_name, '| source npz:', os.path.basename(npz))\n",
        "    return sub\n",
        "\n",
        "# Run span-level decode and write submission\n",
        "sub_span512 = span_level_decode_xlmr512()\n",
        "print(sub_span512.head())"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Span-level 512-only submission.csv saved (112, 2) in 2.7s | backup: submission_safety_512only_3seeds.csv | source npz: xlmr_large_512_3seeds_avg.npz\n          id               PredictionString\n0  be799d365              \u092e\u0941\u0902\u092c\u0908, \u092e\u0939\u093e\u0930\u093e\u0937\u094d\u091f\u094d\u0930\n1  26f356026  \u0909\u0926\u093e\u0938\u093f\u0928\u093e\u091a\u093e\u0930\u094d\u092f \u0938\u0941\u092e\u0947\u0930\u0941\u0926\u093e\u0938 \u092e\u0939\u093e\u0930\u093e\u091c\n2  57a56c43f               \u0baa\u0bc6\u0bb0\u0bc1\u0bae\u0bc2\u0bb3\u0bc8\u0baa\u0bcd \u0baa\u0bc1\u0bb1\u0ba3\u0bbf\n3  da062fdbb                       \u092c\u093f\u0902\u092c\u093f\u0938\u093e\u0930\n4  72fc0d5b5                           1889\n"
          ]
        }
      ]
    },
    {
      "id": "41f494a6-8ef6-48b7-b8a6-549880fde964",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Span-level ensemble: pick best span per model, then choose by weighted score per language\n",
        "import numpy as np, pandas as pd, json, os, re, unicodedata, time\n",
        "\n",
        "def _stable_logsoftmax(arr):\n",
        "    m = np.max(arr)\n",
        "    return arr - (m + np.log(np.sum(np.exp(arr - m))))\n",
        "\n",
        "def _decode_best_for_feature(offsets, s_logit, e_logit, lang):\n",
        "    mask = np.array([o is not None and o[0] is not None and o[1] is not None and (o[1] > o[0]) for o in offsets], dtype=bool)\n",
        "    s = np.where(mask, s_logit, -1e9)\n",
        "    e = np.where(mask, e_logit, -1e9)\n",
        "    lang_l = str(lang).lower().strip()\n",
        "    if lang_l == 'hindi':\n",
        "        n_best = 180; max_len = 52; len_pen = 0.0050; fallback_cap = 36\n",
        "    elif lang_l == 'tamil':\n",
        "        n_best = 220; max_len = 60; len_pen = 0.0040; fallback_cap = 42\n",
        "    else:\n",
        "        n_best = 150; max_len = 64; len_pen = 0.0048; fallback_cap = 42\n",
        "    s_logp = _stable_logsoftmax(s)\n",
        "    e_logp = _stable_logsoftmax(e)\n",
        "    top_s = np.argsort(s_logp)[-n_best:]\n",
        "    best = (float('-inf'), None, None)\n",
        "    for si in top_s:\n",
        "        ei_max = min(len(e_logp)-1, si + max_len - 1)\n",
        "        band = e_logp[si:ei_max+1]\n",
        "        if band.size == 0: continue\n",
        "        off = int(np.argmax(band))\n",
        "        ei = si + off\n",
        "        L = ei - si + 1\n",
        "        score = float(s_logp[si] + e_logp[ei] - len_pen * (L - 1))\n",
        "        if score > best[0]:\n",
        "            best = (score, si, ei)\n",
        "    # Length-based fallback if char span > 40: re-decode with tighter max_len and overwrite if within 0.5 of original or higher\n",
        "    if best[1] is not None and best[2] is not None:\n",
        "        cs, _ = offsets[best[1]]; _, ce = offsets[best[2]]\n",
        "        if cs is not None and ce is not None and ce > cs:\n",
        "            char_len = ce - cs\n",
        "            if char_len > 40:\n",
        "                best_fb = (float('-inf'), None, None)\n",
        "                for si in top_s:\n",
        "                    ei_max = min(len(e_logp)-1, si + fallback_cap - 1)\n",
        "                    band = e_logp[si:ei_max+1]\n",
        "                    if band.size == 0: continue\n",
        "                    off = int(np.argmax(band))\n",
        "                    ei = si + off\n",
        "                    L = ei - si + 1\n",
        "                    score = float(s_logp[si] + e_logp[ei] - len_pen * (L - 1))\n",
        "                    if score > best_fb[0]:\n",
        "                        best_fb = (score, si, ei)\n",
        "                if best_fb[1] is not None and best_fb[2] is not None:\n",
        "                    # Accept fallback if new score within 0.5 of original or higher\n",
        "                    if best_fb[0] >= best[0] - 0.5:\n",
        "                        best = best_fb\n",
        "    return best  # (score, si, ei) or ( -inf, None, None )\n",
        "\n",
        "def _postprocess_ans(ans: str, lang: str = None) -> str:\n",
        "    if not ans:\n",
        "        return ''\n",
        "    # Nukta normalization (safe map) before other trims\n",
        "    nukta_map = {'\\u095c':'\u0921\u093c','\\u095d':'\u0922\u093c','\\u095e':'\u092b\u093c','\\u095f':'\u092f\u093c'}\n",
        "    ans = ''.join(nukta_map.get(c, c) for c in ans)\n",
        "    # Chandrabindu collapse\n",
        "    ans = re.sub(r'\u0901+', '\u0901', ans)\n",
        "    # Numeric punctuation collapse inside digit runs\n",
        "    ans = re.sub(r'(?<=\\d)[ ,._-](?=\\d)', '', ans)\n",
        "    # Danda/virama hygiene\n",
        "    ans = ans.replace('\\u094D\\u0964', '\\u0964').replace('\\u094D\u0964', '\u0964')\n",
        "    ans = re.sub(r'[\u0964]+', '\u0964', ans)\n",
        "    if ans.endswith('\u0964'): ans = ans[:-1]\n",
        "    # Trim trailing virama/combining marks once\n",
        "    if ans and (unicodedata.category(ans[-1]) == 'Mn' or ans[-1] == '\\u094D'):\n",
        "        ans = ans[:-1]\n",
        "    # Hindi-specific: drop isolated trailing virama if preceded by space/punct\n",
        "    if str(lang).lower().strip() == 'hindi':\n",
        "        if len(ans) >= 2 and ans[-1] == '\\u094D' and (ans[-2].isspace() or ans[-2] in '\u0964,:;.!?\"\\'\\u0964\\u0965-'):\n",
        "            ans = ans[:-1]\n",
        "    # Tamil-specific: if last char is a lone combining mark/vowel sign, drop once\n",
        "    if str(lang).lower().strip() == 'tamil':\n",
        "        if len(ans) and unicodedata.category(ans[-1]) == 'Mn':\n",
        "            ans = ans[:-1]\n",
        "    # Strip leading combining mark once\n",
        "    if ans and unicodedata.category(ans[0]) == 'Mn':\n",
        "        ans = ans[1:]\n",
        "    # Collapse repeated hyphens/dashes inside words (safe)\n",
        "    ans = re.sub(r'(?<=\\S)[\\-\u2013\u2014]{2,}(?=\\S)', '-', ans)\n",
        "    # Trim outer punct/spaces\n",
        "    punct_strip = '\\u0964\\u0965\u0964,:;.!?\\\"\\'\\\u201c\\\u201d\\\u2018\\\u2019\\)\\]\\}\\|\\-\\s'\n",
        "    ans = ans.strip().strip(punct_strip)\n",
        "    # Unmatched quotes\n",
        "    for lq, rq in [(\"\\\"\",\"\\\"\"), (\"'\",\"'\"), ('\u201c','\u201d'), ('\u2018','\u2019')]:\n",
        "        if ans.startswith(lq) and not ans.endswith(rq): ans = ans[len(lq):]\n",
        "        if ans.endswith(rq) and not ans.startswith(lq): ans = ans[:-len(rq)]\n",
        "    # Remove BOM/LRM/RLM; drop ZWNJ/ZWJ; remove zero-width space/soft hyphen; normalize spaces incl. NBSP/thin space\n",
        "    ans = ans.replace('\\ufeff','').replace('\\u200e','').replace('\\u200f','')\n",
        "    ans = ans.replace('\\u200B','').replace('\\u00AD','')\n",
        "    ans = ans.replace('\\u00A0',' ').replace('\\u2009',' ')\n",
        "    ans = ' '.join(ans.split())\n",
        "    ans = ans.replace('\\u200C','').replace('\\u200D','')\n",
        "    # Final danda guard\n",
        "    if ans.endswith('\u0964'): ans = ans[:-1]\n",
        "    return ans or ''\n",
        "\n",
        "def build_span_level_ensemble_submission():\n",
        "    t0 = time.time()\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    id2ctx = dict(zip(test_df['id'].astype(str), test_df['context'].astype(str)))\n",
        "    id2lang = dict(zip(test_df['id'].astype(str), test_df['language'].astype(str) if 'language' in test_df.columns else ['unknown']*len(test_df)))\n",
        "\n",
        "    # Load averaged logits + mapping per model (if available)\n",
        "    def load_stream(npz_path, dir_prefix):\n",
        "        if not os.path.exists(npz_path):\n",
        "            return None\n",
        "        exid_json = os.path.join(dir_prefix, 'test_example_id.json')\n",
        "        offs_npy = os.path.join(dir_prefix, 'test_offset_mapping.npy')\n",
        "        if not (os.path.exists(exid_json) and os.path.exists(offs_npy)):\n",
        "            return None\n",
        "        data = np.load(npz_path)\n",
        "        with open(exid_json, 'r', encoding='utf-8') as f:\n",
        "            exids = json.load(f)\n",
        "        offs = np.load(offs_npy, allow_pickle=True)\n",
        "        return {'start': data['start'], 'end': data['end'], 'exids': exids, 'offs': offs}\n",
        "\n",
        "    # Prefer 3-seed 512 npz if available; else 2-seed; else single-seed\n",
        "    if os.path.exists('xlmr_large_512_3seeds_avg.npz'):\n",
        "        xlmr512_npz = 'xlmr_large_512_3seeds_avg.npz'\n",
        "    elif os.path.exists('xlmr_large_512_2seeds_avg.npz'):\n",
        "        xlmr512_npz = 'xlmr_large_512_2seeds_avg.npz'\n",
        "    else:\n",
        "        xlmr512_npz = 'xlmr_large_512_test_avg.npz'\n",
        "\n",
        "    streams = {\n",
        "        'xlmr384': load_stream('xlmr_large_test_avg.npz', 'xlmr_large_test_logits'),\n",
        "        'muril':   load_stream('muril_large_test_avg.npz', 'muril_large_test_logits'),\n",
        "        'xlmr512': load_stream(xlmr512_npz, 'xlmr_large_512_test_logits'),\n",
        "    }\n",
        "    # Per-language weights (xlmr384, muril, xlmr512) updated for 3-seed-dominant primary\n",
        "    per_lang_weights = {\n",
        "        'hindi': {'xlmr384': 0.05, 'muril': 0.10, 'xlmr512': 0.85},\n",
        "        'tamil': {'xlmr384': 0.02, 'muril': 0.00, 'xlmr512': 0.98},\n",
        "    }\n",
        "\n",
        "    # Collect best span and score per example per model (store text for final selection)\n",
        "    best_by_model = {k: {} for k in streams.keys()}\n",
        "    for key, pack in streams.items():\n",
        "        if pack is None:\n",
        "            continue\n",
        "        S, E, exids, offs_all = pack['start'], pack['end'], pack['exids'], pack['offs']\n",
        "        n_feat = S.shape[0]\n",
        "        for i in range(n_feat):\n",
        "            ex_id = exids[i]\n",
        "            lang = id2lang.get(ex_id, 'unknown')\n",
        "            ctx = id2ctx.get(ex_id, '')\n",
        "            score, si, ei = _decode_best_for_feature(offs_all[i], S[i], E[i], lang)\n",
        "            if si is not None and ei is not None:\n",
        "                cs, _ = offs_all[i][si]; _, ce = offs_all[i][ei]\n",
        "                if cs is not None and ce is not None and ce > cs:\n",
        "                    text = ctx[cs:ce]\n",
        "                else:\n",
        "                    text = ''\n",
        "            else:\n",
        "                text = ''\n",
        "            prev = best_by_model[key].get(ex_id, (float('-inf'), ''))\n",
        "            if score > prev[0]:\n",
        "                best_by_model[key][ex_id] = (score, text)\n",
        "\n",
        "    out_ids = test_df['id'].astype(str).values\n",
        "    out_pred = []\n",
        "    for ex_id in out_ids:\n",
        "        lang = str(id2lang.get(ex_id, 'unknown')).lower().strip()\n",
        "        wmap = per_lang_weights.get(lang, per_lang_weights.get('hindi', {'xlmr384':0.05,'muril':0.10,'xlmr512':0.85}))\n",
        "        best = (float('-inf'), '')\n",
        "        for key, w in wmap.items():\n",
        "            if w <= 0: continue\n",
        "            tup = best_by_model.get(key, {}).get(ex_id)\n",
        "            if not tup: continue\n",
        "            score, text = tup\n",
        "            wscore = w * score\n",
        "            if wscore > best[0]:\n",
        "                best = (wscore, text)\n",
        "        ans = _postprocess_ans(best[1], lang) if best[1] else ''\n",
        "        out_pred.append(ans)\n",
        "\n",
        "    sub = pd.DataFrame({'id': out_ids, 'PredictionString': out_pred})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Span-level 3-stream weighted submission.csv saved', sub.shape, 'in %.1fs' % (time.time()-t0))\n",
        "    return sub\n",
        "\n",
        "sub_span_blend = build_span_level_ensemble_submission()\n",
        "print(sub_span_blend.head())"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Span-level 3-stream weighted submission.csv saved (112, 2) in 9.5s\n          id   PredictionString\n0  be799d365  \u092e\u0941\u0902\u092c\u0908, \u092e\u0939\u093e\u0930\u093e\u0937\u094d\u091f\u094d\u0930\n1  26f356026        \u0938\u0941\u0927\u093e\u0902\u0936\u0941\u092c\u093e\u0932\u093e\n2  57a56c43f   \u0baa\u0bc6\u0bb0\u0bc1\u0bae\u0bc2\u0bb3\u0bc8\u0baa\u0bcd \u0baa\u0bc1\u0bb1\u0ba3\u0bbf\n3  da062fdbb           \u092c\u093f\u0902\u092c\u093f\u0938\u093e\u0930\n4  72fc0d5b5               1889\n"
          ]
        }
      ]
    },
    {
      "id": "72488448-ecad-49d9-967b-9978953634e8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Diagnostics: submission health check (empties, per-language breakdown, length stats)\n",
        "import pandas as pd, numpy as np\n",
        "sub = pd.read_csv('submission.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "assert len(sub)==112 and sub['id'].nunique()==112, f\"Bad row/id count: rows={len(sub)}, unique={sub['id'].nunique()}\"\n",
        "sub = sub.merge(test_df[['id','language']], on='id', how='left')\n",
        "sub['PredictionString'] = sub['PredictionString'].astype(str).replace('nan','').fillna('')\n",
        "sub['len'] = sub['PredictionString'].str.len()\n",
        "sub['empty'] = (sub['PredictionString']=='')\n",
        "print('Overall: empties', int(sub['empty'].sum()), '/', len(sub), '| mean len', round(sub['len'].mean(),2), '| median len', int(sub['len'].median()))\n",
        "print('Per-language empties:')\n",
        "print(sub.groupby('language')['empty'].sum().to_dict())\n",
        "print('Per-language mean length:')\n",
        "print(sub.groupby('language')['len'].mean().round(2).to_dict())\n",
        "print('Head (id, lang, len, empty, PredictionString[:60]):')\n",
        "print(sub[['id','language','len','empty','PredictionString']].assign(PredictionString=sub['PredictionString'].str[:60]).head(10))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall: empties 0 / 112 | mean len 9.71 | median len 9\nPer-language empties:\n{'hindi': 0, 'tamil': 0}\nPer-language mean length:\n{'hindi': 9.57, 'tamil': 10.14}\nHead (id, lang, len, empty, PredictionString[:60]):\n          id language  len  empty   PredictionString\n0  be799d365    hindi   17  False  \u092e\u0941\u0902\u092c\u0908, \u092e\u0939\u093e\u0930\u093e\u0937\u094d\u091f\u094d\u0930\n1  26f356026    hindi   11  False        \u0938\u0941\u0927\u093e\u0902\u0936\u0941\u092c\u093e\u0932\u093e\n2  57a56c43f    tamil   16  False   \u0baa\u0bc6\u0bb0\u0bc1\u0bae\u0bc2\u0bb3\u0bc8\u0baa\u0bcd \u0baa\u0bc1\u0bb1\u0ba3\u0bbf\n3  da062fdbb    hindi    8  False           \u092c\u093f\u0902\u092c\u093f\u0938\u093e\u0930\n4  72fc0d5b5    hindi    4  False               1889\n5  6108306f5    hindi   10  False         \u091c\u093f\u092e\u0940 \u0935\u0947\u0932\u094d\u0938\n6  850c30cde    hindi    3  False                \u0939\u0947\u0917\n7  af061a7a4    hindi   13  False      2330 \u0935\u0930\u094d\u0917 \u092e\u0940\u0932\n8  64b643feb    hindi   14  False     \u092e\u093e\u0930\u094d\u091a 14, 1965\n9  4301ae869    tamil    9  False          \u0b85\u0bb0\u0bbf\u0baf\u0bb2\u0bc2\u0bb0\u0bbf\u0bb2\n"
          ]
        }
      ]
    },
    {
      "id": "7cbcb538-51e8-4daf-ab11-fc785d8a3b3a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train new XLM-R large 512 seed (SEED=2026) and save test logits; settings per expert with safe stride for logit-averaging\n",
        "import os, time, numpy as np, pandas as pd, collections, random, json, torch, re, gc\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "from torch.nn.functional import log_softmax\n",
        "\n",
        "SEED = 2026\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')\n",
        "os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True,max_split_size_mb:64')\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "try: torch.set_float32_matmul_precision('high')\n",
        "except Exception: pass\n",
        "\n",
        "MODEL_NAME = 'deepset/xlm-roberta-large-squad2'\n",
        "MAX_LENGTH = 512\n",
        "# Note: keep DOC_STRIDE=256 to ensure identical test featureization vs prior seed for direct logit averaging\n",
        "DOC_STRIDE = 256  # expert suggested 224 for diversity; we keep 256 so shapes match for averaging\n",
        "MAX_ANSWER_LEN = 64\n",
        "NEG_RATIO = 1  # increased diversity vs previous seed\n",
        "EPOCHS = 3\n",
        "LR = 2e-5\n",
        "TRAIN_BS = 1\n",
        "EVAL_BS = 2\n",
        "GRAD_ACCUM = 20\n",
        "N_BEST = 25\n",
        "WARMUP_RATIO = 0.15\n",
        "WEIGHT_DECAY = 0.02\n",
        "\n",
        "print('Tokenizer for new seed with', MODEL_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "pad_on_right = tokenizer.padding_side == 'right'\n",
        "\n",
        "full_df = pd.read_csv('train.csv').merge(pd.read_csv('folds.csv'), on='id', how='left')\n",
        "assert full_df['fold'].notna().all()\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "def prepare_train_features_fn(examples):\n",
        "    questions = [q.strip() for q in examples['question']]; contexts = examples['context']\n",
        "    tok = tokenizer(\n",
        "        questions if pad_on_right else contexts,\n",
        "        contexts if pad_on_right else questions,\n",
        "        truncation='only_second' if pad_on_right else 'only_first',\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    smap = tok.pop('overflow_to_sample_mapping')\n",
        "    offsets = tok.pop('offset_mapping')\n",
        "    input_ids = tok['input_ids']\n",
        "    start_positions, end_positions, cls_positions = [], [], []\n",
        "    for i, off in enumerate(offsets):\n",
        "        si = smap[i]\n",
        "        ctx = contexts[si]\n",
        "        a_text = str(examples['answer_text'][si])\n",
        "        a_start = int(examples['answer_start'][si])\n",
        "        a_end = a_start + len(a_text)\n",
        "        while a_start < a_end and a_start < len(ctx) and ctx[a_start].isspace(): a_start += 1\n",
        "        while a_end > a_start and a_end-1 < len(ctx) and ctx[a_end-1].isspace(): a_end -= 1\n",
        "        if a_start >= a_end: a_end = a_start\n",
        "        seq_ids = tok.sequence_ids(i)\n",
        "        try:\n",
        "            cls_index = input_ids[i].index(tokenizer.cls_token_id) if tokenizer.cls_token_id is not None else 0\n",
        "        except ValueError:\n",
        "            cls_index = 0\n",
        "        cls_positions.append(cls_index)\n",
        "        context_id = 1 if pad_on_right else 0\n",
        "        try:\n",
        "            ctx_s = seq_ids.index(context_id)\n",
        "            ctx_e = len(seq_ids) - 1 - seq_ids[::-1].index(context_id)\n",
        "        except ValueError:\n",
        "            start_positions.append(cls_index); end_positions.append(cls_index); continue\n",
        "        st_tok = en_tok = None\n",
        "        tgt_end_incl = a_end - 1\n",
        "        for t in range(ctx_s, ctx_e + 1):\n",
        "            o = off[t]\n",
        "            if o is None: continue\n",
        "            ts, te = o\n",
        "            if ts is None or te is None: continue\n",
        "            if st_tok is None and ts <= a_start < te: st_tok = t\n",
        "            if ts <= tgt_end_incl < te: en_tok = t\n",
        "        if st_tok is not None and en_tok is not None and st_tok <= en_tok:\n",
        "            start_positions.append(st_tok); end_positions.append(en_tok)\n",
        "        else:\n",
        "            start_positions.append(cls_index); end_positions.append(cls_index)\n",
        "    tok['start_positions'] = start_positions; tok['end_positions'] = end_positions; tok['overflow_to_sample_mapping'] = smap; tok['cls_positions'] = cls_positions\n",
        "    return tok\n",
        "\n",
        "def prepare_pred_features_fn(examples):\n",
        "    questions = [q.strip() for q in examples['question']]; contexts = examples['context']\n",
        "    tok = tokenizer(\n",
        "        questions if pad_on_right else contexts,\n",
        "        contexts if pad_on_right else questions,\n",
        "        truncation='only_second' if pad_on_right else 'only_first',\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    smap = tok.pop('overflow_to_sample_mapping')\n",
        "    tok['example_id'] = []\n",
        "    new_offsets = []\n",
        "    for i, off in enumerate(tok['offset_mapping']):\n",
        "        si = smap[i]\n",
        "        tok['example_id'].append(str(examples['id'][si]))\n",
        "        seq_ids = tok.sequence_ids(i)\n",
        "        context_id = 1 if pad_on_right else 0\n",
        "        new_offsets.append([o if seq_ids[k] == context_id else None for k, o in enumerate(off)])\n",
        "    tok['offset_mapping'] = new_offsets\n",
        "    return tok\n",
        "\n",
        "# Pre-tokenize TEST once for this seed\n",
        "logdir = 'xlmr_large_512_seed2026_test_logits'\n",
        "os.makedirs(logdir, exist_ok=True)\n",
        "print('Tokenizing TEST for seed 2026 ...')\n",
        "test_ds = Dataset.from_pandas(test_df[['id','question','context','language']])\n",
        "test_tok_all = test_ds.map(prepare_pred_features_fn, batched=True, remove_columns=test_ds.column_names, desc='xlmr512_seed2026_test_tokenize')\n",
        "te_ds_pred = test_tok_all.remove_columns([c for c in test_tok_all.column_names if c not in ['input_ids','attention_mask']])\n",
        "te_ds_pred.set_format(type='torch')\n",
        "with open(os.path.join(logdir, 'test_example_id.json'), 'w', encoding='utf-8') as f:\n",
        "    json.dump(list(test_tok_all['example_id']), f, ensure_ascii=False)\n",
        "np.save(os.path.join(logdir, 'test_offset_mapping.npy'), np.array(test_tok_all['offset_mapping'], dtype=object), allow_pickle=True)\n",
        "\n",
        "all_oof_scores = []; oof_rows = []; test_start_list = []; test_end_list = []; t_global = time.time()\n",
        "for FOLD in range(5):\n",
        "    t_fold = time.time()\n",
        "    print(f'\\n===== XLM-R Large 512 SEED2026 Fold {FOLD} =====', flush=True)\n",
        "    tr_df = full_df[full_df['fold'] != FOLD].reset_index(drop=True)\n",
        "    va_df = full_df[full_df['fold'] == FOLD].reset_index(drop=True)\n",
        "    tr_ds = Dataset.from_pandas(tr_df[['id','question','context','answer_text','answer_start','language']])\n",
        "    va_ds = Dataset.from_pandas(va_df[['id','question','context','answer_text','answer_start','language']])\n",
        "    t0 = time.time()\n",
        "    tr_tok = tr_ds.map(prepare_train_features_fn, batched=True, remove_columns=tr_ds.column_names, desc=f'seed2026_train_tokenize_f{FOLD}')\n",
        "    print(f'Fold {FOLD}: train feats pre-sample={tr_tok.num_rows} | tok_time={time.time()-t0:.1f}s', flush=True)\n",
        "    # Negative sampling (NEG_RATIO=1) robust via cls_positions\n",
        "    smap = np.array(tr_tok['overflow_to_sample_mapping']); sp = np.array(tr_tok['start_positions']); cls_arr = np.array(tr_tok['cls_positions'])\n",
        "    is_pos = (sp != cls_arr)\n",
        "    keep = []; by_ex = collections.defaultdict(list)\n",
        "    for idx, ex in enumerate(smap): by_ex[ex].append(idx)\n",
        "    for ex, idxs in by_ex.items():\n",
        "        idxs = np.array(idxs); pos = idxs[is_pos[idxs]]; neg = idxs[~is_pos[idxs]]\n",
        "        keep.extend(pos.tolist())\n",
        "        if len(pos) == 0:\n",
        "            sel = neg[:min(4, len(neg))]; keep.extend(sel.tolist())\n",
        "        else:\n",
        "            cap = min(len(neg), 1 * len(pos))\n",
        "            if cap > 0:\n",
        "                sel = neg[np.random.permutation(len(neg))[:cap]]; keep.extend(sel.tolist())\n",
        "    keep = np.array(sorted(set(keep))); before = tr_tok.num_rows\n",
        "    tr_tok = tr_tok.select(keep.tolist()); after = tr_tok.num_rows\n",
        "    print(f'Fold {FOLD}: neg-sample {before}->{after} ({after/before:.2%})', flush=True)\n",
        "    # Torch datasets\n",
        "    tr_ds_torch = tr_tok.remove_columns([c for c in tr_tok.column_names if c not in ['input_ids','attention_mask','start_positions','end_positions']])\n",
        "    tr_ds_torch.set_format(type='torch')\n",
        "    va_tok_all = va_ds.map(prepare_pred_features_fn, batched=True, remove_columns=va_ds.column_names, desc=f'seed2026_valid_tokenize_f{FOLD}')\n",
        "    va_ds_pred = va_tok_all.remove_columns([c for c in va_tok_all.column_names if c not in ['input_ids','attention_mask']])\n",
        "    va_ds_pred.set_format(type='torch')\n",
        "    # Model\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
        "    try:\n",
        "        model.config.use_cache = False\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
        "    except Exception:\n",
        "        try: model.gradient_checkpointing_enable()\n",
        "        except Exception: pass\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'xlmr_large_512_seed2026_f{FOLD}',\n",
        "        num_train_epochs=EPOCHS,\n",
        "        learning_rate=LR,\n",
        "        per_device_train_batch_size=TRAIN_BS,\n",
        "        per_device_eval_batch_size=EVAL_BS,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM,\n",
        "        fp16=False,\n",
        "        bf16=True,\n",
        "        logging_steps=200,\n",
        "        save_strategy='no',\n",
        "        evaluation_strategy='no',\n",
        "        seed=SEED,\n",
        "        dataloader_num_workers=0,\n",
        "        dataloader_pin_memory=False,\n",
        "        eval_accumulation_steps=1,\n",
        "        report_to=[],\n",
        "        gradient_checkpointing=True,\n",
        "        warmup_ratio=WARMUP_RATIO,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        max_grad_norm=1.0\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tr_ds_torch,\n",
        "        eval_dataset=None,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer),\n",
        "    )\n",
        "    print(f'Fold {FOLD}: training...')\n",
        "    try:\n",
        "        trainer.train()\n",
        "    except RuntimeError as e:\n",
        "        if 'out of memory' in str(e).lower():\n",
        "            print('OOM; retry with GRAD_ACCUM=24, EVAL_BS=1')\n",
        "            del trainer, model\n",
        "            gc.collect(); torch.cuda.empty_cache()\n",
        "            model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
        "            try: model.config.use_cache = False\n",
        "            except Exception: pass\n",
        "            try: model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
        "            except Exception:\n",
        "                try: model.gradient_checkpointing_enable()\n",
        "                except Exception: pass\n",
        "            args = TrainingArguments(\n",
        "                output_dir=f'xlmr_large_512_seed2026_f{FOLD}',\n",
        "                num_train_epochs=EPOCHS,\n",
        "                learning_rate=LR,\n",
        "                per_device_train_batch_size=1,\n",
        "                per_device_eval_batch_size=1,\n",
        "                gradient_accumulation_steps=24,\n",
        "                fp16=False,\n",
        "                bf16=True,\n",
        "                logging_steps=200,\n",
        "                save_strategy='no',\n",
        "                evaluation_strategy='no',\n",
        "                seed=SEED,\n",
        "                dataloader_num_workers=0,\n",
        "                dataloader_pin_memory=False,\n",
        "                eval_accumulation_steps=1,\n",
        "                report_to=[],\n",
        "                gradient_checkpointing=True,\n",
        "                warmup_ratio=WARMUP_RATIO,\n",
        "                weight_decay=WEIGHT_DECAY,\n",
        "                max_grad_norm=1.0,\n",
        "            )\n",
        "            trainer = Trainer(\n",
        "                model=model,\n",
        "                args=args,\n",
        "                train_dataset=tr_ds_torch,\n",
        "                eval_dataset=None,\n",
        "                tokenizer=tokenizer,\n",
        "                data_collator=DataCollatorWithPadding(tokenizer),\n",
        "            )\n",
        "            trainer.train()\n",
        "        else:\n",
        "            raise\n",
        "    print(f'Fold {FOLD}: train_done in {time.time()-t_fold:.1f}s')\n",
        "    # Predict TEST and save logits for this fold\n",
        "    print(f'Fold {FOLD}: predicting TEST and saving logits (seed2026) ...')\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        te_pred = trainer.predict(te_ds_pred)\n",
        "    te_start = te_pred.predictions[0]\n",
        "    te_end = te_pred.predictions[1]\n",
        "    np.save(os.path.join(logdir, f'test_start_logits_f{FOLD}.npy'), te_start)\n",
        "    np.save(os.path.join(logdir, f'test_end_logits_f{FOLD}.npy'), te_end)\n",
        "    test_start_list.append(te_start)\n",
        "    test_end_list.append(te_end)\n",
        "\n",
        "    del model, trainer, tr_ds_torch, tr_tok, va_ds_pred, va_tok_all, te_pred\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "# Average TEST logits across folds for this seed\n",
        "print('Averaging TEST logits across folds for seed 2026 ...')\n",
        "start_stack = np.stack(test_start_list, axis=0)\n",
        "end_stack = np.stack(test_end_list, axis=0)\n",
        "start_avg = start_stack.mean(axis=0)\n",
        "end_avg = end_stack.mean(axis=0)\n",
        "np.savez_compressed('xlmr_large_512_seed2026_test_avg.npz', start=start_avg, end=end_avg)\n",
        "print('Saved xlmr_large_512_seed2026_test_avg.npz; example_id/offsets saved under', logdir)\n",
        "print('Seed 2026 5-fold done in %.1fs' % (time.time()-t_global))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer for new seed with deepset/xlm-roberta-large-squad2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing TEST for seed 2026 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_seed2026_test_tokenize:   0%|          | 0/112 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_seed2026_test_tokenize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112/112 [00:03<00:00, 32.11 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_seed2026_test_tokenize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112/112 [00:03<00:00, 31.73 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== XLM-R Large 512 SEED2026 Fold 0 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_train_tokenize_f0:   0%|          | 0/817 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_train_tokenize_f0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 817/817 [00:21<00:00, 38.73 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_train_tokenize_f0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 817/817 [00:21<00:00, 38.52 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: train feats pre-sample=10484 | tok_time=21.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: neg-sample 10484->2085 (19.89%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_valid_tokenize_f0:   0%|          | 0/185 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_valid_tokenize_f0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 185/185 [00:05<00:00, 31.43 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_valid_tokenize_f0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 185/185 [00:06<00:00, 30.79 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/312 : < :, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: train_done in 1665.5s\nFold 0: predicting TEST and saving logits (seed2026) ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='701' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/701 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== XLM-R Large 512 SEED2026 Fold 1 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_train_tokenize_f1:   0%|          | 0/807 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_train_tokenize_f1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 807/807 [00:32<00:00, 24.54 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_train_tokenize_f1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 807/807 [00:33<00:00, 24.41 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: train feats pre-sample=10396 | tok_time=33.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: neg-sample 10396->2047 (19.69%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_valid_tokenize_f1:   0%|          | 0/195 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_valid_tokenize_f1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 195/195 [00:09<00:00, 21.17 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_valid_tokenize_f1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 195/195 [00:09<00:00, 20.79 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/306 : < :, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: train_done in 1837.8s\nFold 1: predicting TEST and saving logits (seed2026) ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='701' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/701 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== XLM-R Large 512 SEED2026 Fold 2 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_train_tokenize_f2:   0%|          | 0/784 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_train_tokenize_f2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 784/784 [00:33<00:00, 23.48 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_train_tokenize_f2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 784/784 [00:33<00:00, 23.36 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: train feats pre-sample=9842 | tok_time=33.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: neg-sample 9842->1962 (19.93%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_valid_tokenize_f2:   0%|          | 0/218 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_valid_tokenize_f2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 218/218 [00:16<00:00, 13.18 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_valid_tokenize_f2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 218/218 [00:16<00:00, 13.08 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='294' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/294 : < :, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: train_done in 1902.8s\nFold 2: predicting TEST and saving logits (seed2026) ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='701' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/701 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== XLM-R Large 512 SEED2026 Fold 3 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_train_tokenize_f3:   0%|          | 0/795 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_train_tokenize_f3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 795/795 [00:14<00:00, 53.72 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_train_tokenize_f3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 795/795 [00:14<00:00, 53.38 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: train feats pre-sample=9960 | tok_time=14.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: neg-sample 9960->1994 (20.02%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_valid_tokenize_f3:   0%|          | 0/207 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_valid_tokenize_f3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 207/207 [00:05<00:00, 40.17 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_valid_tokenize_f3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 207/207 [00:05<00:00, 39.38 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/297 : < :, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: train_done in 1822.6s\nFold 3: predicting TEST and saving logits (seed2026) ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='701' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/701 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== XLM-R Large 512 SEED2026 Fold 4 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_train_tokenize_f4:   0%|          | 0/805 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_train_tokenize_f4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 805/805 [00:15<00:00, 51.32 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_train_tokenize_f4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 805/805 [00:15<00:00, 51.00 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: train feats pre-sample=10458 | tok_time=15.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: neg-sample 10458->2024 (19.35%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_valid_tokenize_f4:   0%|          | 0/197 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_valid_tokenize_f4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 197/197 [00:04<00:00, 48.13 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2026_valid_tokenize_f4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 197/197 [00:04<00:00, 47.04 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='303' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/303 : < :, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: train_done in 2550.9s\nFold 4: predicting TEST and saving logits (seed2026) ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='701' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/701 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Averaging TEST logits across folds for seed 2026 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved xlmr_large_512_seed2026_test_avg.npz; example_id/offsets saved under xlmr_large_512_seed2026_test_logits\nSeed 2026 5-fold done in 10308.1s\n"
          ]
        }
      ]
    },
    {
      "id": "b814e64a-ed91-4a4f-8531-474357f3f2e4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Average 512 seeds at logit level, rebuild primary span-level ensemble, and run diagnostics\n",
        "import os, numpy as np, pandas as pd, time\n",
        "\n",
        "def average_512_seeds_and_rebuild():\n",
        "    t0 = time.time()\n",
        "    npz1 = 'xlmr_large_512_test_avg.npz'\n",
        "    npz2 = 'xlmr_large_512_seed2026_test_avg.npz'\n",
        "    assert os.path.exists(npz1), f'Missing {npz1}'\n",
        "    assert os.path.exists(npz2), f'Missing {npz2} (wait for training to finish)'\n",
        "    d1 = np.load(npz1)\n",
        "    d2 = np.load(npz2)\n",
        "    start = (d1['start'] + d2['start']) / 2.0\n",
        "    end = (d1['end'] + d2['end']) / 2.0\n",
        "    out_npz = 'xlmr_large_512_2seeds_avg.npz'\n",
        "    np.savez_compressed(out_npz, start=start, end=end)\n",
        "    print('Saved', out_npz, 'with shape', start.shape, 'in %.1fs' % (time.time()-t0))\n",
        "\n",
        "    # Rebuild primary span-level 3-stream submission using Cell 31's builder (it prefers 2seeds file if present)\n",
        "    assert 'build_span_level_ensemble_submission' in globals(), 'Run Cell 31 once to define the span-level builder.'\n",
        "    sub = build_span_level_ensemble_submission()\n",
        "    print('Primary span-level 3-stream rebuilt. Head:')\n",
        "    print(sub.head())\n",
        "\n",
        "    # Diagnostics (same as Cell 32)\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    sub_diag = pd.read_csv('submission.csv').merge(test_df[['id','language']], on='id', how='left')\n",
        "    sub_diag['PredictionString'] = sub_diag['PredictionString'].astype(str).replace('nan','').fillna('')\n",
        "    sub_diag['len'] = sub_diag['PredictionString'].str.len()\n",
        "    sub_diag['empty'] = (sub_diag['PredictionString']=='')\n",
        "    print('Diag: empties', int(sub_diag['empty'].sum()), '/ 112 | mean len', round(sub_diag['len'].mean(),2))\n",
        "    print(sub_diag.head())\n",
        "    return out_npz\n",
        "\n",
        "# Execute averaging and rebuild now\n",
        "avg_npz_path = average_512_seeds_and_rebuild()\n",
        "print('Averaged NPZ:', avg_npz_path)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved xlmr_large_512_2seeds_avg.npz with shape (1401, 512) in 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Span-level 3-stream weighted submission.csv saved (112, 2) in 15.0s\nPrimary span-level 3-stream rebuilt. Head:\n          id   PredictionString\n0  be799d365  \u092e\u0941\u0902\u092c\u0908, \u092e\u0939\u093e\u0930\u093e\u0937\u094d\u091f\u094d\u0930\n1  26f356026        \u0938\u0941\u0927\u093e\u0902\u0936\u0941\u092c\u093e\u0932\u093e\n2  57a56c43f   \u0baa\u0bc6\u0bb0\u0bc1\u0bae\u0bc2\u0bb3\u0bc8\u0baa\u0bcd \u0baa\u0bc1\u0bb1\u0ba3\u0bbf\n3  da062fdbb           \u092c\u093f\u0902\u092c\u093f\u0938\u093e\u0930\n4  72fc0d5b5               1889\nDiag: empties 0 / 112 | mean len 9.79\n          id   PredictionString language  len  empty\n0  be799d365  \u092e\u0941\u0902\u092c\u0908, \u092e\u0939\u093e\u0930\u093e\u0937\u094d\u091f\u094d\u0930    hindi   17  False\n1  26f356026        \u0938\u0941\u0927\u093e\u0902\u0936\u0941\u092c\u093e\u0932\u093e    hindi   11  False\n2  57a56c43f   \u0baa\u0bc6\u0bb0\u0bc1\u0bae\u0bc2\u0bb3\u0bc8\u0baa\u0bcd \u0baa\u0bc1\u0bb1\u0ba3\u0bbf    tamil   16  False\n3  da062fdbb           \u092c\u093f\u0902\u092c\u093f\u0938\u093e\u0930    hindi    8  False\n4  72fc0d5b5               1889    hindi    4  False\nAveraged NPZ: xlmr_large_512_2seeds_avg.npz\n"
          ]
        }
      ]
    },
    {
      "id": "c35fefcc-4c71-44e2-9ea5-6dffd2a5c1a0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build alt per-language weight submission (hi 0.20/0.10/0.70, ta 0.15/0.00/0.85) without disturbing current primary\n",
        "import os, json, numpy as np, pandas as pd, time\n",
        "\n",
        "# Save current primary submission as backup\n",
        "if os.path.exists('submission.csv'):\n",
        "    pd.read_csv('submission.csv').to_csv('submission_primary_3stream_2seeds.csv', index=False)\n",
        "\n",
        "def _load_stream_alt(npz_path, dir_prefix):\n",
        "    if not os.path.exists(npz_path):\n",
        "        return None\n",
        "    exid_json = os.path.join(dir_prefix, 'test_example_id.json')\n",
        "    offs_npy = os.path.join(dir_prefix, 'test_offset_mapping.npy')\n",
        "    if not (os.path.exists(exid_json) and os.path.exists(offs_npy)):\n",
        "        return None\n",
        "    data = np.load(npz_path)\n",
        "    with open(exid_json, 'r', encoding='utf-8') as f:\n",
        "        exids = json.load(f)\n",
        "    offs = np.load(offs_npy, allow_pickle=True)\n",
        "    return {'start': data['start'], 'end': data['end'], 'exids': exids, 'offs': offs}\n",
        "\n",
        "def build_span_level_ensemble_submission_alt():\n",
        "    t0 = time.time()\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    id2ctx = dict(zip(test_df['id'].astype(str), test_df['context'].astype(str)))\n",
        "    id2lang = dict(zip(test_df['id'].astype(str), test_df['language'].astype(str) if 'language' in test_df.columns else ['unknown']*len(test_df)))\n",
        "    xlmr512_npz = 'xlmr_large_512_2seeds_avg.npz' if os.path.exists('xlmr_large_512_2seeds_avg.npz') else 'xlmr_large_512_test_avg.npz'\n",
        "    streams = {\n",
        "        'xlmr384': _load_stream_alt('xlmr_large_test_avg.npz', 'xlmr_large_test_logits'),\n",
        "        'muril':   _load_stream_alt('muril_large_test_avg.npz', 'muril_large_test_logits'),\n",
        "        'xlmr512': _load_stream_alt(xlmr512_npz, 'xlmr_large_512_test_logits'),\n",
        "    }\n",
        "    # Alt per-language weights (xlmr384, muril, xlmr512)\n",
        "    per_lang_weights = {\n",
        "        'hindi': {'xlmr384': 0.20, 'muril': 0.10, 'xlmr512': 0.70},\n",
        "        'tamil': {'xlmr384': 0.15, 'muril': 0.00, 'xlmr512': 0.85},\n",
        "    }\n",
        "    # Collect best per model\n",
        "    best_by_model = {k: {} for k in streams.keys()}\n",
        "    for key, pack in streams.items():\n",
        "        if pack is None:\n",
        "            continue\n",
        "        S, E, exids, offs_all = pack['start'], pack['end'], pack['exids'], pack['offs']\n",
        "        for i in range(S.shape[0]):\n",
        "            ex_id = exids[i]\n",
        "            lang = id2lang.get(ex_id, 'unknown')\n",
        "            ctx = id2ctx.get(ex_id, '')\n",
        "            score, si, ei = _decode_best_for_feature(offs_all[i], S[i], E[i], lang)\n",
        "            if si is not None and ei is not None:\n",
        "                cs, _ = offs_all[i][si]; _, ce = offs_all[i][ei]\n",
        "                text = ctx[cs:ce] if (cs is not None and ce is not None and ce > cs) else ''\n",
        "            else:\n",
        "                text = ''\n",
        "            prev = best_by_model[key].get(ex_id, (float('-inf'), ''))\n",
        "            if score > prev[0]:\n",
        "                best_by_model[key][ex_id] = (score, text)\n",
        "    out_ids = test_df['id'].astype(str).values\n",
        "    out_pred = []\n",
        "    for ex_id in out_ids:\n",
        "        lang = str(id2lang.get(ex_id, 'unknown')).lower().strip()\n",
        "        wmap = per_lang_weights.get(lang, per_lang_weights['hindi'])\n",
        "        best = (float('-inf'), '')\n",
        "        for key, w in wmap.items():\n",
        "            if w <= 0: continue\n",
        "            tup = best_by_model.get(key, {}).get(ex_id)\n",
        "            if not tup: continue\n",
        "            score, text = tup\n",
        "            wscore = w * score\n",
        "            if wscore > best[0]:\n",
        "                best = (wscore, text)\n",
        "        ans = _postprocess_ans(best[1], lang) if best[1] else ''\n",
        "        out_pred.append(ans)\n",
        "    sub = pd.DataFrame({'id': out_ids, 'PredictionString': out_pred})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Alt weights span-level submission.csv saved', sub.shape, 'in %.1fs' % (time.time()-t0))\n",
        "    return sub\n",
        "\n",
        "# Build ALT and save under distinct name; then restore primary to submission.csv\n",
        "sub_alt = build_span_level_ensemble_submission_alt()\n",
        "alt_name = 'submission_alt_perlang_hi020_010_070_ta015_000_085.csv'\n",
        "sub_alt.to_csv(alt_name, index=False)\n",
        "# Diagnostics for ALT\n",
        "test_df = pd.read_csv('test.csv')\n",
        "diag = sub_alt.merge(test_df[['id','language']], on='id', how='left')\n",
        "diag['PredictionString'] = diag['PredictionString'].astype(str).replace('nan','').fillna('')\n",
        "diag['len'] = diag['PredictionString'].str.len()\n",
        "diag['empty'] = (diag['PredictionString']=='')\n",
        "print('ALT Diag: empties', int(diag['empty'].sum()), '/ 112 | mean len', round(diag['len'].mean(),2))\n",
        "print(diag.head())\n",
        "\n",
        "# Restore primary submission.csv if backup exists\n",
        "if os.path.exists('submission_primary_3stream_2seeds.csv'):\n",
        "    pd.read_csv('submission_primary_3stream_2seeds.csv').to_csv('submission.csv', index=False)\n",
        "    print('Restored primary submission.csv from backup; ALT saved as', alt_name)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alt weights span-level submission.csv saved (112, 2) in 7.8s\nALT Diag: empties 0 / 112 | mean len 9.65\n          id   PredictionString language  len  empty\n0  be799d365  \u092e\u0941\u0902\u092c\u0908, \u092e\u0939\u093e\u0930\u093e\u0937\u094d\u091f\u094d\u0930    hindi   17  False\n1  26f356026           \u092c\u093e\u092e\u093e\u0916\u0947\u092a\u093e    hindi    8  False\n2  57a56c43f   \u0baa\u0bc6\u0bb0\u0bc1\u0bae\u0bc2\u0bb3\u0bc8\u0baa\u0bcd \u0baa\u0bc1\u0bb1\u0ba3\u0bbf    tamil   16  False\n3  da062fdbb           \u092c\u093f\u0902\u092c\u093f\u0938\u093e\u0930    hindi    8  False\n4  72fc0d5b5               1889    hindi    4  False\nRestored primary submission.csv from backup; ALT saved as submission_alt_perlang_hi020_010_070_ta015_000_085.csv\n"
          ]
        }
      ]
    },
    {
      "id": "5b82cd39-463b-4ccb-9c95-2eb988603639",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Activate ALT per-language weights submission as submission.csv\n",
        "import pandas as pd, shutil, os\n",
        "alt_fp = 'submission_alt_perlang_hi020_010_070_ta015_000_085.csv'\n",
        "assert os.path.exists(alt_fp), f'Missing ALT file: {alt_fp}'\n",
        "df_alt = pd.read_csv(alt_fp)\n",
        "if 'PredictionString' in df_alt.columns:\n",
        "    df_alt['PredictionString'] = df_alt['PredictionString'].astype(str).replace('nan','').fillna('')\n",
        "df_alt.to_csv('submission.csv', index=False)\n",
        "print('submission.csv replaced with ALT:', alt_fp)\n",
        "print(df_alt.head())"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv replaced with ALT: submission_alt_perlang_hi020_010_070_ta015_000_085.csv\n          id   PredictionString\n0  be799d365  \u092e\u0941\u0902\u092c\u0908, \u092e\u0939\u093e\u0930\u093e\u0937\u094d\u091f\u094d\u0930\n1  26f356026           \u092c\u093e\u092e\u093e\u0916\u0947\u092a\u093e\n2  57a56c43f   \u0baa\u0bc6\u0bb0\u0bc1\u0bae\u0bc2\u0bb3\u0bc8\u0baa\u0bcd \u0baa\u0bc1\u0bb1\u0ba3\u0bbf\n3  da062fdbb           \u092c\u093f\u0902\u092c\u093f\u0938\u093e\u0930\n4  72fc0d5b5               1889\n"
          ]
        }
      ]
    },
    {
      "id": "f0c7917a-f062-42de-88fa-5e7afb50e946",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Span-level tiny knob sweep (per expert ranges) keeping streams and weights fixed\n",
        "import os, json, numpy as np, pandas as pd, time, re, unicodedata\n",
        "\n",
        "def _stable_logsoftmax(arr):\n",
        "    m = np.max(arr)\n",
        "    return arr - (m + np.log(np.sum(np.exp(arr - m))))\n",
        "\n",
        "def _decode_best_for_feature_params(offsets, s_logit, e_logit, n_best, max_len, len_pen):\n",
        "    mask = np.array([o is not None and o[0] is not None and o[1] is not None and (o[1] > o[0]) for o in offsets], dtype=bool)\n",
        "    s = np.where(mask, s_logit, -1e9)\n",
        "    e = np.where(mask, e_logit, -1e9)\n",
        "    s_logp = _stable_logsoftmax(s)\n",
        "    e_logp = _stable_logsoftmax(e)\n",
        "    top_s = np.argsort(s_logp)[-int(n_best):]\n",
        "    best = (float('-inf'), None, None)\n",
        "    for si in top_s:\n",
        "        ei_max = min(len(e_logp)-1, si + int(max_len) - 1)\n",
        "        band = e_logp[si:ei_max+1]\n",
        "        if band.size == 0: continue\n",
        "        off = int(np.argmax(band))\n",
        "        ei = si + off\n",
        "        L = ei - si + 1\n",
        "        score = float(s_logp[si] + e_logp[ei] - float(len_pen) * (L - 1))\n",
        "        if score > best[0]:\n",
        "            best = (score, si, ei)\n",
        "    return best\n",
        "\n",
        "def _postprocess_ans_sweep(ans: str, lang: str = None) -> str:\n",
        "    if not ans: return ''\n",
        "    ans = re.sub(r'(?<=\\d)[ ,._-](?=\\d)', '', ans)\n",
        "    ans = ans.replace('\\u094D\\u0964', '\\u0964').replace('\\u094D\u0964', '\u0964')\n",
        "    ans = re.sub(r'[\u0964]+', '\u0964', ans)\n",
        "    if ans.endswith('\u0964'): ans = ans[:-1]\n",
        "    if ans and (unicodedata.category(ans[-1]) == 'Mn' or ans[-1] == '\\u094D'): ans = ans[:-1]\n",
        "    lang_l = str(lang).lower().strip()\n",
        "    if lang_l == 'hindi' and len(ans) >= 2 and ans[-1] == '\\u094D' and (ans[-2].isspace() or ans[-2] in '\u0964,:;.!?\"\\'\\u0964\\u0965-'):\n",
        "        ans = ans[:-1]\n",
        "    if lang_l == 'tamil' and len(ans) and unicodedata.category(ans[-1]) == 'Mn':\n",
        "        ans = ans[:-1]\n",
        "    if ans and unicodedata.category(ans[0]) == 'Mn': ans = ans[1:]\n",
        "    punct_strip = '\\u0964\\u0965\u0964,:;.!?\\\"\\'\\\u201c\\\u201d\\\u2018\\\u2019\\)\\]\\}\\|\\-\\s'\n",
        "    ans = ans.strip().strip(punct_strip)\n",
        "    for lq, rq in [(\"\\\"\",\"\\\"\"), (\"'\",\"'\"), ('\u201c','\u201d'), ('\u2018','\u2019')]:\n",
        "        if ans.startswith(lq) and not ans.endswith(rq): ans = ans[len(lq):]\n",
        "        if ans.endswith(rq) and not ans.startswith(lq): ans = ans[:-len(rq)]\n",
        "    ans = ans.replace('\\ufeff','').replace('\\u200e','').replace('\\u200f','')\n",
        "    ans = ans.replace('\\u00A0',' ').replace('\\u2009',' ')\n",
        "    ans = ' '.join(ans.split())\n",
        "    ans = ans.replace('\\u200C','').replace('\\u200D','')\n",
        "    return ans or ''\n",
        "\n",
        "def build_span_level_ensemble_with_params(hi_params, ta_params, per_lang_weights=None, out_name=None):\n",
        "    t0 = time.time()\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    id2ctx = dict(zip(test_df['id'].astype(str), test_df['context'].astype(str)))\n",
        "    id2lang = dict(zip(test_df['id'].astype(str), test_df['language'].astype(str) if 'language' in test_df.columns else ['unknown']*len(test_df)))\n",
        "    # Streams\n",
        "    xlmr512_npz = 'xlmr_large_512_2seeds_avg.npz' if os.path.exists('xlmr_large_512_2seeds_avg.npz') else 'xlmr_large_512_test_avg.npz'\n",
        "    def load_stream(npz_path, dir_prefix):\n",
        "        if not os.path.exists(npz_path): return None\n",
        "        exid_json = os.path.join(dir_prefix, 'test_example_id.json')\n",
        "        offs_npy = os.path.join(dir_prefix, 'test_offset_mapping.npy')\n",
        "        if not (os.path.exists(exid_json) and os.path.exists(offs_npy)): return None\n",
        "        data = np.load(npz_path)\n",
        "        with open(exid_json, 'r', encoding='utf-8') as f:\n",
        "            exids = json.load(f)\n",
        "        offs = np.load(offs_npy, allow_pickle=True)\n",
        "        return {'start': data['start'], 'end': data['end'], 'exids': exids, 'offs': offs}\n",
        "    streams = {\n",
        "        'xlmr384': load_stream('xlmr_large_test_avg.npz', 'xlmr_large_test_logits'),\n",
        "        'muril':   load_stream('muril_large_test_avg.npz', 'muril_large_test_logits'),\n",
        "        'xlmr512': load_stream(xlmr512_npz, 'xlmr_large_512_test_logits'),\n",
        "    }\n",
        "    if per_lang_weights is None:\n",
        "        per_lang_weights = {\n",
        "            'hindi': {'xlmr384': 0.15, 'muril': 0.10, 'xlmr512': 0.75},\n",
        "            'tamil': {'xlmr384': 0.10, 'muril': 0.00, 'xlmr512': 0.90},\n",
        "        }\n",
        "    # Collect best span text per model using custom params\n",
        "    best_by_model = {k: {} for k in streams.keys()}\n",
        "    for key, pack in streams.items():\n",
        "        if pack is None: continue\n",
        "        S, E, exids, offs_all = pack['start'], pack['end'], pack['exids'], pack['offs']\n",
        "        for i in range(S.shape[0]):\n",
        "            ex_id = exids[i]\n",
        "            lang = str(id2lang.get(ex_id, 'unknown')).lower().strip()\n",
        "            ctx = id2ctx.get(ex_id, '')\n",
        "            if lang == 'hindi':\n",
        "                params = hi_params\n",
        "            elif lang == 'tamil':\n",
        "                params = ta_params\n",
        "            else:\n",
        "                params = {'n_best': 150, 'max_len': 64, 'len_pen': 0.0048}\n",
        "            score, si, ei = _decode_best_for_feature_params(offs_all[i], S[i], E[i], params['n_best'], params['max_len'], params['len_pen'])\n",
        "            if si is not None and ei is not None:\n",
        "                cs, _ = offs_all[i][si]; _, ce = offs_all[i][ei]\n",
        "                text = ctx[cs:ce] if (cs is not None and ce is not None and ce > cs) else ''\n",
        "            else:\n",
        "                text = ''\n",
        "            prev = best_by_model[key].get(ex_id, (float('-inf'), ''))\n",
        "            if score > prev[0]:\n",
        "                best_by_model[key][ex_id] = (score, text)\n",
        "    # Choose per-language by weights\n",
        "    out_ids = test_df['id'].astype(str).values\n",
        "    out_pred = []\n",
        "    for ex_id in out_ids:\n",
        "        lang = str(id2lang.get(ex_id, 'unknown')).lower().strip()\n",
        "        wmap = per_lang_weights.get(lang, per_lang_weights.get('hindi'))\n",
        "        best = (float('-inf'), '')\n",
        "        for key, w in wmap.items():\n",
        "            if w <= 0: continue\n",
        "            tup = best_by_model.get(key, {}).get(ex_id)\n",
        "            if not tup: continue\n",
        "            score, text = tup\n",
        "            wscore = float(w) * score\n",
        "            if wscore > best[0]:\n",
        "                best = (wscore, text)\n",
        "        ans = _postprocess_ans_sweep(best[1], lang) if best[1] else ''\n",
        "        out_pred.append(ans)\n",
        "    sub = pd.DataFrame({'id': out_ids, 'PredictionString': out_pred})\n",
        "    if out_name:\n",
        "        sub.to_csv(out_name, index=False)\n",
        "    return sub\n",
        "\n",
        "# Define two tiny-probe configs per expert ranges\n",
        "hi_def = {'n_best': 180, 'max_len': 52, 'len_pen': 0.0050}\n",
        "ta_def = {'n_best': 220, 'max_len': 62, 'len_pen': 0.0040}\n",
        "hi_alt = {'n_best': 170, 'max_len': 50, 'len_pen': 0.0051}\n",
        "ta_alt = {'n_best': 210, 'max_len': 60, 'len_pen': 0.0039}\n",
        "\n",
        "weights_pl = {\n",
        "    'hindi': {'xlmr384': 0.15, 'muril': 0.10, 'xlmr512': 0.75},\n",
        "    'tamil': {'xlmr384': 0.10, 'muril': 0.00, 'xlmr512': 0.90},\n",
        "}\n",
        "\n",
        "print('Building span-level DEF config...')\n",
        "sub_def = build_span_level_ensemble_with_params(hi_def, ta_def, per_lang_weights=weights_pl, out_name='submission_spanlvl_def_weights_defknobs.csv')\n",
        "print('Building span-level ALT config...')\n",
        "sub_alt = build_span_level_ensemble_with_params(hi_alt, ta_alt, per_lang_weights=weights_pl, out_name='submission_spanlvl_def_weights_altknobs.csv')\n",
        "\n",
        "# Diagnostics for both\n",
        "test_df = pd.read_csv('test.csv')\n",
        "for name in ['submission_spanlvl_def_weights_defknobs.csv', 'submission_spanlvl_def_weights_altknobs.csv']:\n",
        "    df = pd.read_csv(name).merge(test_df[['id','language']], on='id', how='left')\n",
        "    df['PredictionString'] = df['PredictionString'].astype(str).replace('nan','').fillna('')\n",
        "    df['len'] = df['PredictionString'].str.len()\n",
        "    df['empty'] = (df['PredictionString']=='')\n",
        "    print(name, '| empties:', int(df['empty'].sum()), '/ 112 | mean len:', round(df['len'].mean(),2), '| per-lang empties:', df.groupby('language')['empty'].sum().to_dict())\n",
        "print('Sweep done. Choose which to activate and submit.')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building span-level DEF config...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building span-level ALT config...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission_spanlvl_def_weights_defknobs.csv | empties: 0 / 112 | mean len: 9.79 | per-lang empties: {'hindi': 0, 'tamil': 0}\nsubmission_spanlvl_def_weights_altknobs.csv | empties: 0 / 112 | mean len: 9.79 | per-lang empties: {'hindi': 0, 'tamil': 0}\nSweep done. Choose which to activate and submit.\n"
          ]
        }
      ]
    },
    {
      "id": "66d87a9b-4453-40ec-a071-9cf40a15d1b0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train another XLM-R large 512 seed (SEED=2027) for 3-seed averaging\n",
        "import os, time, numpy as np, pandas as pd, collections, random, json, torch, re, gc\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "\n",
        "SEED = 2027\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')\n",
        "os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True,max_split_size_mb:64')\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "try: torch.set_float32_matmul_precision('high')\n",
        "except Exception: pass\n",
        "\n",
        "MODEL_NAME = 'deepset/xlm-roberta-large-squad2'\n",
        "MAX_LENGTH = 512\n",
        "DOC_STRIDE = 256  # keep identical featureization for direct logit averaging\n",
        "NEG_RATIO = 1\n",
        "EPOCHS = 3\n",
        "LR = 2e-5\n",
        "TRAIN_BS = 1\n",
        "EVAL_BS = 2\n",
        "GRAD_ACCUM = 20\n",
        "WARMUP_RATIO = 0.15\n",
        "WEIGHT_DECAY = 0.02\n",
        "\n",
        "print('Tokenizer for new seed with', MODEL_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "pad_on_right = tokenizer.padding_side == 'right'\n",
        "\n",
        "full_df = pd.read_csv('train.csv').merge(pd.read_csv('folds.csv'), on='id', how='left')\n",
        "assert full_df['fold'].notna().all()\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "def prepare_train_features_fn(examples):\n",
        "    questions = [q.strip() for q in examples['question']]; contexts = examples['context']\n",
        "    tok = tokenizer(\n",
        "        questions if pad_on_right else contexts,\n",
        "        contexts if pad_on_right else questions,\n",
        "        truncation='only_second' if pad_on_right else 'only_first',\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    smap = tok.pop('overflow_to_sample_mapping')\n",
        "    offsets = tok.pop('offset_mapping')\n",
        "    input_ids = tok['input_ids']\n",
        "    start_positions, end_positions, cls_positions = [], [], []\n",
        "    for i, off in enumerate(offsets):\n",
        "        si = smap[i]\n",
        "        ctx = contexts[si]\n",
        "        a_text = str(examples['answer_text'][si])\n",
        "        a_start = int(examples['answer_start'][si])\n",
        "        a_end = a_start + len(a_text)\n",
        "        while a_start < a_end and a_start < len(ctx) and ctx[a_start].isspace(): a_start += 1\n",
        "        while a_end > a_start and a_end-1 < len(ctx) and ctx[a_end-1].isspace(): a_end -= 1\n",
        "        if a_start >= a_end: a_end = a_start\n",
        "        seq_ids = tok.sequence_ids(i)\n",
        "        try:\n",
        "            cls_index = input_ids[i].index(tokenizer.cls_token_id) if tokenizer.cls_token_id is not None else 0\n",
        "        except ValueError:\n",
        "            cls_index = 0\n",
        "        cls_positions.append(cls_index)\n",
        "        context_id = 1 if pad_on_right else 0\n",
        "        try:\n",
        "            ctx_s = seq_ids.index(context_id)\n",
        "            ctx_e = len(seq_ids) - 1 - seq_ids[::-1].index(context_id)\n",
        "        except ValueError:\n",
        "            start_positions.append(cls_index); end_positions.append(cls_index); continue\n",
        "        st_tok = en_tok = None\n",
        "        tgt_end_incl = a_end - 1\n",
        "        for t in range(ctx_s, ctx_e + 1):\n",
        "            o = off[t]\n",
        "            if o is None: continue\n",
        "            ts, te = o\n",
        "            if ts is None or te is None: continue\n",
        "            if st_tok is None and ts <= a_start < te: st_tok = t\n",
        "            if ts <= tgt_end_incl < te: en_tok = t\n",
        "        if st_tok is not None and en_tok is not None and st_tok <= en_tok:\n",
        "            start_positions.append(st_tok); end_positions.append(en_tok)\n",
        "        else:\n",
        "            start_positions.append(cls_index); end_positions.append(cls_index)\n",
        "    tok['start_positions'] = start_positions; tok['end_positions'] = end_positions; tok['overflow_to_sample_mapping'] = smap; tok['cls_positions'] = cls_positions\n",
        "    return tok\n",
        "\n",
        "def prepare_pred_features_fn(examples):\n",
        "    questions = [q.strip() for q in examples['question']]; contexts = examples['context']\n",
        "    tok = tokenizer(\n",
        "        questions if pad_on_right else contexts,\n",
        "        contexts if pad_on_right else questions,\n",
        "        truncation='only_second' if pad_on_right else 'only_first',\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=DOC_STRIDE,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    smap = tok.pop('overflow_to_sample_mapping')\n",
        "    tok['example_id'] = []\n",
        "    new_offsets = []\n",
        "    for i, off in enumerate(tok['offset_mapping']):\n",
        "        si = smap[i]\n",
        "        tok['example_id'].append(str(examples['id'][si]))\n",
        "        seq_ids = tok.sequence_ids(i)\n",
        "        context_id = 1 if pad_on_right else 0\n",
        "        new_offsets.append([o if seq_ids[k] == context_id else None for k, o in enumerate(off)])\n",
        "    tok['offset_mapping'] = new_offsets\n",
        "    return tok\n",
        "\n",
        "# Pre-tokenize TEST once for this seed\n",
        "logdir = 'xlmr_large_512_seed2027_test_logits'\n",
        "os.makedirs(logdir, exist_ok=True)\n",
        "print('Tokenizing TEST for seed 2027 ...')\n",
        "test_ds = Dataset.from_pandas(test_df[['id','question','context','language']])\n",
        "test_tok_all = test_ds.map(prepare_pred_features_fn, batched=True, remove_columns=test_ds.column_names, desc='xlmr512_seed2027_test_tokenize')\n",
        "te_ds_pred = test_tok_all.remove_columns([c for c in test_tok_all.column_names if c not in ['input_ids','attention_mask']])\n",
        "te_ds_pred.set_format(type='torch')\n",
        "with open(os.path.join(logdir, 'test_example_id.json'), 'w', encoding='utf-8') as f:\n",
        "    json.dump(list(test_tok_all['example_id']), f, ensure_ascii=False)\n",
        "np.save(os.path.join(logdir, 'test_offset_mapping.npy'), np.array(test_tok_all['offset_mapping'], dtype=object), allow_pickle=True)\n",
        "\n",
        "test_start_list = []; test_end_list = []; t_global = time.time()\n",
        "for FOLD in range(5):\n",
        "    t_fold = time.time()\n",
        "    print(f'\\n===== XLM-R Large 512 SEED2027 Fold {FOLD} =====', flush=True)\n",
        "    tr_df = full_df[full_df['fold'] != FOLD].reset_index(drop=True)\n",
        "    va_df = full_df[full_df['fold'] == FOLD].reset_index(drop=True)\n",
        "    tr_ds = Dataset.from_pandas(tr_df[['id','question','context','answer_text','answer_start','language']])\n",
        "    va_ds = Dataset.from_pandas(va_df[['id','question','context','answer_text','answer_start','language']])\n",
        "    t0 = time.time()\n",
        "    tr_tok = tr_ds.map(prepare_train_features_fn, batched=True, remove_columns=tr_ds.column_names, desc=f'seed2027_train_tokenize_f{FOLD}')\n",
        "    print(f'Fold {FOLD}: train feats pre-sample={tr_tok.num_rows} | tok_time={time.time()-t0:.1f}s', flush=True)\n",
        "    # Negative sampling (NEG_RATIO=1) robust via cls_positions\n",
        "    smap = np.array(tr_tok['overflow_to_sample_mapping']); sp = np.array(tr_tok['start_positions']); cls_arr = np.array(tr_tok['cls_positions'])\n",
        "    is_pos = (sp != cls_arr)\n",
        "    keep = []; by_ex = collections.defaultdict(list)\n",
        "    for idx, ex in enumerate(smap): by_ex[ex].append(idx)\n",
        "    for ex, idxs in by_ex.items():\n",
        "        idxs = np.array(idxs); pos = idxs[is_pos[idxs]]; neg = idxs[~is_pos[idxs]]\n",
        "        keep.extend(pos.tolist())\n",
        "        if len(pos) == 0:\n",
        "            sel = neg[:min(4, len(neg))]; keep.extend(sel.tolist())\n",
        "        else:\n",
        "            cap = min(len(neg), 1 * len(pos))\n",
        "            if cap > 0:\n",
        "                sel = neg[np.random.permutation(len(neg))[:cap]]; keep.extend(sel.tolist())\n",
        "    keep = np.array(sorted(set(keep))); before = tr_tok.num_rows\n",
        "    tr_tok = tr_tok.select(keep.tolist()); after = tr_tok.num_rows\n",
        "    print(f'Fold {FOLD}: neg-sample {before}->{after} ({after/before:.2%})', flush=True)\n",
        "    # Torch datasets\n",
        "    tr_ds_torch = tr_tok.remove_columns([c for c in tr_tok.column_names if c not in ['input_ids','attention_mask','start_positions','end_positions']])\n",
        "    tr_ds_torch.set_format(type='torch')\n",
        "    va_tok_all = va_ds.map(prepare_pred_features_fn, batched=True, remove_columns=va_ds.column_names, desc=f'seed2027_valid_tokenize_f{FOLD}')\n",
        "    va_ds_pred = va_tok_all.remove_columns([c for c in va_tok_all.column_names if c not in ['input_ids','attention_mask']])\n",
        "    va_ds_pred.set_format(type='torch')\n",
        "    # Model\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
        "    try:\n",
        "        model.config.use_cache = False\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={'use_reentrant': False})\n",
        "    except Exception:\n",
        "        try: model.gradient_checkpointing_enable()\n",
        "        except Exception: pass\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'xlmr_large_512_seed2027_f{FOLD}',\n",
        "        num_train_epochs=EPOCHS,\n",
        "        learning_rate=LR,\n",
        "        per_device_train_batch_size=TRAIN_BS,\n",
        "        per_device_eval_batch_size=EVAL_BS,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM,\n",
        "        fp16=False,\n",
        "        bf16=True,\n",
        "        logging_steps=200,\n",
        "        save_strategy='no',\n",
        "        evaluation_strategy='no',\n",
        "        seed=SEED,\n",
        "        dataloader_num_workers=0,\n",
        "        dataloader_pin_memory=False,\n",
        "        eval_accumulation_steps=1,\n",
        "        report_to=[],\n",
        "        gradient_checkpointing=True,\n",
        "        warmup_ratio=WARMUP_RATIO,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        max_grad_norm=1.0\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tr_ds_torch,\n",
        "        eval_dataset=None,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer),\n",
        "    )\n",
        "    print(f'Fold {FOLD}: training...')\n",
        "    try:\n",
        "        trainer.train()\n",
        "    except RuntimeError as e:\n",
        "        if 'out of memory' in str(e).lower():\n",
        "            print('OOM; retry with GRAD_ACCUM=24, EVAL_BS=1')\n",
        "            del trainer, model\n",
        "            gc.collect(); torch.cuda.empty_cache()\n",
        "            model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
        "            try: model.config.use_cache = False\n",
        "            except Exception: pass\n",
        "            try: model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={'use_reentrant': False})\n",
        "            except Exception:\n",
        "                try: model.gradient_checkpointing_enable()\n",
        "                except Exception: pass\n",
        "            args = TrainingArguments(\n",
        "                output_dir=f'xlmr_large_512_seed2027_f{FOLD}',\n",
        "                num_train_epochs=EPOCHS,\n",
        "                learning_rate=LR,\n",
        "                per_device_train_batch_size=1,\n",
        "                per_device_eval_batch_size=1,\n",
        "                gradient_accumulation_steps=24,\n",
        "                fp16=False,\n",
        "                bf16=True,\n",
        "                logging_steps=200,\n",
        "                save_strategy='no',\n",
        "                evaluation_strategy='no',\n",
        "                seed=SEED,\n",
        "                dataloader_num_workers=0,\n",
        "                dataloader_pin_memory=False,\n",
        "                eval_accumulation_steps=1,\n",
        "                report_to=[],\n",
        "                gradient_checkpointing=True,\n",
        "                warmup_ratio=WARMUP_RATIO,\n",
        "                weight_decay=WEIGHT_DECAY,\n",
        "                max_grad_norm=1.0,\n",
        "            )\n",
        "            trainer = Trainer(\n",
        "                model=model,\n",
        "                args=args,\n",
        "                train_dataset=tr_ds_torch,\n",
        "                eval_dataset=None,\n",
        "                tokenizer=tokenizer,\n",
        "                data_collator=DataCollatorWithPadding(tokenizer),\n",
        "            )\n",
        "            trainer.train()\n",
        "        else:\n",
        "            raise\n",
        "    print(f'Fold {FOLD}: train_done in {time.time()-t_fold:.1f}s')\n",
        "    # Predict TEST and save logits for this fold\n",
        "    print(f'Fold {FOLD}: predicting TEST and saving logits (seed2027) ...')\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        te_pred = trainer.predict(te_ds_pred)\n",
        "    te_start = te_pred.predictions[0]\n",
        "    te_end = te_pred.predictions[1]\n",
        "    np.save(os.path.join(logdir, f'test_start_logits_f{FOLD}.npy'), te_start)\n",
        "    np.save(os.path.join(logdir, f'test_end_logits_f{FOLD}.npy'), te_end)\n",
        "    test_start_list.append(te_start)\n",
        "    test_end_list.append(te_end)\n",
        "\n",
        "    del model, trainer, tr_ds_torch, tr_tok, va_ds_pred, va_tok_all, te_pred\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "# Average TEST logits across folds for this seed\n",
        "print('Averaging TEST logits across folds for seed 2027 ...')\n",
        "start_stack = np.stack(test_start_list, axis=0)\n",
        "end_stack = np.stack(test_end_list, axis=0)\n",
        "start_avg = start_stack.mean(axis=0)\n",
        "end_avg = end_stack.mean(axis=0)\n",
        "np.savez_compressed('xlmr_large_512_seed2027_test_avg.npz', start=start_avg, end=end_avg)\n",
        "print('Saved xlmr_large_512_seed2027_test_avg.npz; example_id/offsets saved under', logdir)\n",
        "print('Seed 2027 5-fold done in %.1fs' % (time.time()-t_global))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer for new seed with deepset/xlm-roberta-large-squad2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing TEST for seed 2027 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_seed2027_test_tokenize:   0%|          | 0/112 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_seed2027_test_tokenize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112/112 [00:02<00:00, 39.66 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rxlmr512_seed2027_test_tokenize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112/112 [00:02<00:00, 39.14 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== XLM-R Large 512 SEED2027 Fold 0 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_train_tokenize_f0:   0%|          | 0/817 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_train_tokenize_f0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 817/817 [00:17<00:00, 46.69 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_train_tokenize_f0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 817/817 [00:17<00:00, 46.42 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: train feats pre-sample=10484 | tok_time=17.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: neg-sample 10484->2085 (19.89%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_valid_tokenize_f0:   0%|          | 0/185 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_valid_tokenize_f0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 185/185 [00:04<00:00, 41.86 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_valid_tokenize_f0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 185/185 [00:04<00:00, 40.94 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/312 : < :, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: train_done in 2598.1s\nFold 0: predicting TEST and saving logits (seed2027) ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='701' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/701 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== XLM-R Large 512 SEED2027 Fold 1 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_train_tokenize_f1:   0%|          | 0/807 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_train_tokenize_f1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 807/807 [00:13<00:00, 57.71 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_train_tokenize_f1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 807/807 [00:14<00:00, 57.32 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: train feats pre-sample=10396 | tok_time=14.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: neg-sample 10396->2047 (19.69%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_valid_tokenize_f1:   0%|          | 0/195 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_valid_tokenize_f1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 195/195 [00:03<00:00, 51.41 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_valid_tokenize_f1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 195/195 [00:03<00:00, 50.07 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/306 : < :, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: train_done in 2566.3s\nFold 1: predicting TEST and saving logits (seed2027) ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='701' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/701 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== XLM-R Large 512 SEED2027 Fold 2 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_train_tokenize_f2:   0%|          | 0/784 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_train_tokenize_f2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 784/784 [00:13<00:00, 56.42 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_train_tokenize_f2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 784/784 [00:13<00:00, 56.06 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: train feats pre-sample=9842 | tok_time=14.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: neg-sample 9842->1962 (19.93%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_valid_tokenize_f2:   0%|          | 0/218 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_valid_tokenize_f2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 218/218 [00:04<00:00, 43.95 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_valid_tokenize_f2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 218/218 [00:05<00:00, 43.04 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='294' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/294 : < :, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: train_done in 2408.6s\nFold 2: predicting TEST and saving logits (seed2027) ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='701' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/701 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== XLM-R Large 512 SEED2027 Fold 3 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_train_tokenize_f3:   0%|          | 0/795 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_train_tokenize_f3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 795/795 [00:15<00:00, 51.33 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_train_tokenize_f3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 795/795 [00:15<00:00, 51.01 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: train feats pre-sample=9960 | tok_time=15.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: neg-sample 9960->1994 (20.02%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_valid_tokenize_f3:   0%|          | 0/207 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_valid_tokenize_f3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 207/207 [00:05<00:00, 39.22 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_valid_tokenize_f3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 207/207 [00:05<00:00, 38.45 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/297 : < :, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: train_done in 2471.8s\nFold 3: predicting TEST and saving logits (seed2027) ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='701' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/701 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== XLM-R Large 512 SEED2027 Fold 4 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_train_tokenize_f4:   0%|          | 0/805 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_train_tokenize_f4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 805/805 [00:13<00:00, 58.23 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_train_tokenize_f4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 805/805 [00:13<00:00, 57.84 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: train feats pre-sample=10458 | tok_time=14.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: neg-sample 10458->2024 (19.35%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_valid_tokenize_f4:   0%|          | 0/197 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_valid_tokenize_f4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 197/197 [00:03<00:00, 51.09 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rseed2027_valid_tokenize_f4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 197/197 [00:03<00:00, 49.82 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='303' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/303 : < :, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: train_done in 2548.0s\nFold 4: predicting TEST and saving logits (seed2027) ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='701' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/701 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Averaging TEST logits across folds for seed 2027 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved xlmr_large_512_seed2027_test_avg.npz; example_id/offsets saved under xlmr_large_512_seed2027_test_logits\nSeed 2027 5-fold done in 13207.9s\n"
          ]
        }
      ]
    },
    {
      "id": "37d7b3bc-d884-4ef7-9539-6268e3122842",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Average 3 seeds (512) at logit level and rebuild span-level ensemble + diagnostics (no file swapping)\n",
        "import os, time, numpy as np, pandas as pd\n",
        "\n",
        "def average_3seeds_512_and_rebuild():\n",
        "    t0 = time.time()\n",
        "    npz_main = 'xlmr_large_512_test_avg.npz'\n",
        "    npz_s2 = 'xlmr_large_512_seed2026_test_avg.npz'\n",
        "    npz_s3 = 'xlmr_large_512_seed2027_test_avg.npz'\n",
        "    assert os.path.exists(npz_main), f'Missing {npz_main}'\n",
        "    assert os.path.exists(npz_s2), f'Missing {npz_s2}'\n",
        "    assert os.path.exists(npz_s3), f'Missing {npz_s3} (wait for seed2027 training to finish)'\n",
        "    d0 = np.load(npz_main); d1 = np.load(npz_s2); d2 = np.load(npz_s3)\n",
        "    start = (d0['start'] + d1['start'] + d2['start']) / 3.0\n",
        "    end = (d0['end'] + d1['end'] + d2['end']) / 3.0\n",
        "    out_npz = 'xlmr_large_512_3seeds_avg.npz'\n",
        "    np.savez_compressed(out_npz, start=start, end=end)\n",
        "    print('Saved', out_npz, 'with shape', start.shape, 'in %.1fs' % (time.time()-t0))\n",
        "\n",
        "    # Build span-level 3-stream using builder (Cell 31 prefers 3-seed if present)\n",
        "    assert 'build_span_level_ensemble_submission' in globals(), 'Run Cell 31 to define the span-level builder.'\n",
        "    sub = build_span_level_ensemble_submission()\n",
        "    print('3-seed primary span-level rebuilt. Head:')\n",
        "    print(sub.head())\n",
        "\n",
        "    # Diagnostics\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    sub_diag = pd.read_csv('submission.csv').merge(test_df[['id','language']], on='id', how='left')\n",
        "    sub_diag['PredictionString'] = sub_diag['PredictionString'].astype(str).replace('nan','').fillna('')\n",
        "    sub_diag['len'] = sub_diag['PredictionString'].str.len()\n",
        "    sub_diag['empty'] = (sub_diag['PredictionString']=='')\n",
        "    print('Diag: empties', int(sub_diag['empty'].sum()), '/ 112 | mean len', round(sub_diag['len'].mean(),2))\n",
        "    return out_npz\n",
        "\n",
        "# Execute now\n",
        "avg_npz = average_3seeds_512_and_rebuild()\n",
        "print('Averaged NPZ path:', avg_npz)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved xlmr_large_512_3seeds_avg.npz with shape (1401, 512) in 0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Span-level 3-stream weighted submission.csv saved (112, 2) in 8.2s\n3-seed primary span-level rebuilt. Head:\n          id   PredictionString\n0  be799d365  \u092e\u0941\u0902\u092c\u0908, \u092e\u0939\u093e\u0930\u093e\u0937\u094d\u091f\u094d\u0930\n1  26f356026        \u0938\u0941\u0927\u093e\u0902\u0936\u0941\u092c\u093e\u0932\u093e\n2  57a56c43f   \u0baa\u0bc6\u0bb0\u0bc1\u0bae\u0bc2\u0bb3\u0bc8\u0baa\u0bcd \u0baa\u0bc1\u0bb1\u0ba3\u0bbf\n3  da062fdbb           \u092c\u093f\u0902\u092c\u093f\u0938\u093e\u0930\n4  72fc0d5b5               1889\nDiag: empties 0 / 112 | mean len 9.79\nAveraged NPZ path: xlmr_large_512_3seeds_avg.npz\n"
          ]
        }
      ]
    },
    {
      "id": "4858862f-e2da-4490-b476-7ed2fbaaf367",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Activate expert-priority variant #2: span-level def-weights ALT knobs\n",
        "import pandas as pd, shutil, os\n",
        "alt2 = 'submission_spanlvl_def_weights_altknobs.csv'\n",
        "assert os.path.exists(alt2), f'Missing file: {alt2}'\n",
        "df = pd.read_csv(alt2)\n",
        "if 'PredictionString' in df.columns:\n",
        "    df['PredictionString'] = df['PredictionString'].astype(str).replace('nan','').fillna('')\n",
        "df.to_csv('submission.csv', index=False)\n",
        "print('submission.csv replaced with', alt2)\n",
        "print(df.head())"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv replaced with submission_spanlvl_def_weights_altknobs.csv\n          id   PredictionString\n0  be799d365  \u092e\u0941\u0902\u092c\u0908, \u092e\u0939\u093e\u0930\u093e\u0937\u094d\u091f\u094d\u0930\n1  26f356026        \u0938\u0941\u0927\u093e\u0902\u0936\u0941\u092c\u093e\u0932\u093e\n2  57a56c43f   \u0baa\u0bc6\u0bb0\u0bc1\u0bae\u0bc2\u0bb3\u0bc8\u0baa\u0bcd \u0baa\u0bc1\u0bb1\u0ba3\u0bbf\n3  da062fdbb           \u092c\u093f\u0902\u092c\u093f\u0938\u093e\u0930\n4  72fc0d5b5               1889\n"
          ]
        }
      ]
    },
    {
      "id": "a9d22259-ce1c-4de3-a710-7bf005e2b685",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build expert-suggested extra slot: 512-only span-level with tightened max_len and slightly higher len_pen\n",
        "import os, json, numpy as np, pandas as pd, time, re, unicodedata, shutil\n",
        "\n",
        "def _stable_logsoftmax(arr):\n",
        "    m = np.max(arr)\n",
        "    return arr - (m + np.log(np.sum(np.exp(arr - m))))\n",
        "\n",
        "def _decode_best_for_feature_custom(offsets, s_logit, e_logit, lang):\n",
        "    mask = np.array([o is not None and o[0] is not None and o[1] is not None and (o[1] > o[0]) for o in offsets], dtype=bool)\n",
        "    s = np.where(mask, s_logit, -1e9)\n",
        "    e = np.where(mask, e_logit, -1e9)\n",
        "    lang_l = str(lang).lower().strip()\n",
        "    # Expert extra slot knobs: tighten max_len and add +0.0001 len_pen\n",
        "    if lang_l == 'hindi':\n",
        "        n_best = 180; max_len = 48; len_pen = 0.0051\n",
        "    elif lang_l == 'tamil':\n",
        "        n_best = 220; max_len = 58; len_pen = 0.0041\n",
        "    else:\n",
        "        n_best = 150; max_len = 60; len_pen = 0.0049\n",
        "    s_logp = _stable_logsoftmax(s)\n",
        "    e_logp = _stable_logsoftmax(e)\n",
        "    top_s = np.argsort(s_logp)[-n_best:]\n",
        "    best = (float('-inf'), None, None)\n",
        "    for si in top_s:\n",
        "        ei_max = min(len(e_logp)-1, si + max_len - 1)\n",
        "        band = e_logp[si:ei_max+1]\n",
        "        if band.size == 0: continue\n",
        "        off = int(np.argmax(band))\n",
        "        ei = si + off\n",
        "        L = ei - si + 1\n",
        "        score = float(s_logp[si] + e_logp[ei] - len_pen * (L - 1))\n",
        "        if score > best[0]:\n",
        "            best = (score, si, ei)\n",
        "    return best\n",
        "\n",
        "def _postprocess_ans_min(ans: str, lang: str = None) -> str:\n",
        "    if not ans: return ''\n",
        "    # numeric collapse\n",
        "    ans = re.sub(r'(?<=\\d)[ ,._-](?=\\d)', '', ans)\n",
        "    # danda/virama hygiene and collapse\n",
        "    ans = ans.replace('\\u094D\\u0964', '\\u0964').replace('\\u094D\u0964', '\u0964').replace('\\u094D\u0964', '\u0964')\n",
        "    ans = re.sub(r'[\u0964]+', '\u0964', ans)\n",
        "    if ans.endswith('\u0964'): ans = ans[:-1]\n",
        "    # trim trailing combining/virama once\n",
        "    if ans and (unicodedata.category(ans[-1]) == 'Mn' or ans[-1] == '\\u094D'): ans = ans[:-1]\n",
        "    lang_l = str(lang).lower().strip()\n",
        "    if lang_l == 'hindi' and len(ans) >= 2 and ans[-1] == '\\u094D' and (ans[-2].isspace() or ans[-2] in '\u0964,:;.!?\"\\'\\u0964\\u0965-'):\n",
        "        ans = ans[:-1]\n",
        "    if lang_l == 'tamil' and len(ans) and unicodedata.category(ans[-1]) == 'Mn':\n",
        "        ans = ans[:-1]\n",
        "    if ans and unicodedata.category(ans[0]) == 'Mn': ans = ans[1:]\n",
        "    # outer punct/space trim + zero-widths\n",
        "    punct_strip = '\\u0964\\u0965\u0964,:;.!?\\\"\\'\\\u201c\\\u201d\\\u2018\\\u2019\\)\\]\\}\\|\\-\\s'\n",
        "    ans = ans.strip().strip(punct_strip)\n",
        "    for lq, rq in [(\"\\\"\",\"\\\"\"), (\"'\",\"'\"), ('\\u201c','\\u201d'), ('\\u2018','\\u2019')]:\n",
        "        if ans.startswith(lq) and not ans.endswith(rq): ans = ans[len(lq):]\n",
        "        if ans.endswith(rq) and not ans.startswith(lq): ans = ans[:-len(rq)]\n",
        "    ans = ans.replace('\\ufeff','').replace('\\u200e','').replace('\\u200f','')\n",
        "    ans = ans.replace('\\u200B','').replace('\\u00AD','')\n",
        "    ans = ans.replace('\\u00A0',' ').replace('\\u2009',' ')\n",
        "    ans = ' '.join(ans.split())\n",
        "    ans = ans.replace('\\u200C','').replace('\\u200D','')\n",
        "    if ans.endswith('\u0964'): ans = ans[:-1]\n",
        "    return ans or ''\n",
        "\n",
        "def build_512only_spanlvl_tight():\n",
        "    t0 = time.time()\n",
        "    # Choose 3-seed averaged logits if available\n",
        "    if os.path.exists('xlmr_large_512_3seeds_avg.npz'):\n",
        "        npz = 'xlmr_large_512_3seeds_avg.npz'\n",
        "    elif os.path.exists('xlmr_large_512_2seeds_avg.npz'):\n",
        "        npz = 'xlmr_large_512_2seeds_avg.npz'\n",
        "    else:\n",
        "        npz = 'xlmr_large_512_test_avg.npz'\n",
        "    exid_json = 'xlmr_large_512_test_logits/test_example_id.json'\n",
        "    offs_npy = 'xlmr_large_512_test_logits/test_offset_mapping.npy'\n",
        "    assert os.path.exists(npz) and os.path.exists(exid_json) and os.path.exists(offs_npy), 'Missing 512 artifacts'\n",
        "    data = np.load(npz)\n",
        "    S = data['start']; E = data['end']\n",
        "    with open(exid_json, 'r', encoding='utf-8') as f:\n",
        "        exids = json.load(f)\n",
        "    offs_all = np.load(offs_npy, allow_pickle=True)\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    id2ctx = dict(zip(test_df['id'].astype(str), test_df['context'].astype(str)))\n",
        "    id2lang = dict(zip(test_df['id'].astype(str), test_df['language'].astype(str) if 'language' in test_df.columns else ['unknown']*len(test_df)))\n",
        "    # Pick best per example from 512 stream only\n",
        "    best_per_ex = {}  # ex_id -> (score, text)\n",
        "    for i in range(S.shape[0]):\n",
        "        ex_id = exids[i]\n",
        "        lang = id2lang.get(ex_id, 'unknown')\n",
        "        score, si, ei = _decode_best_for_feature_custom(offs_all[i], S[i], E[i], lang)\n",
        "        if si is not None and ei is not None:\n",
        "            cs, _ = offs_all[i][si]; _, ce = offs_all[i][ei]\n",
        "            if cs is not None and ce is not None and ce > cs:\n",
        "                text = id2ctx.get(ex_id, '')[cs:ce]\n",
        "            else:\n",
        "                text = ''\n",
        "        else:\n",
        "            text = ''\n",
        "        prev = best_per_ex.get(ex_id, (float('-inf'), ''))\n",
        "        if score > prev[0]:\n",
        "            best_per_ex[ex_id] = (score, text)\n",
        "    # Post-process and write\n",
        "    out_ids = test_df['id'].astype(str).values\n",
        "    out_pred = []\n",
        "    for ex_id in out_ids:\n",
        "        lang = id2lang.get(ex_id, 'unknown')\n",
        "        ans = best_per_ex.get(ex_id, (float('-inf'), ''))[1]\n",
        "        out_pred.append(_postprocess_ans_min(ans, lang))\n",
        "    sub = pd.DataFrame({'id': out_ids, 'PredictionString': out_pred})\n",
        "    sub.to_csv('submission_512only_spanlvl_hi48_ta58_lenpen_plus0.0001.csv', index=False)\n",
        "    # Activate as submission.csv\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Activated 512-only tight variant as submission.csv | shape', sub.shape, '| wrote backup submission_512only_spanlvl_hi48_ta58_lenpen_plus0.0001.csv in %.1fs' % (time.time()-t0))\n",
        "    # Quick diagnostics\n",
        "    diag = sub.merge(test_df[['id','language']], on='id', how='left')\n",
        "    diag['PredictionString'] = diag['PredictionString'].astype(str).replace('nan','').fillna('')\n",
        "    diag['len'] = diag['PredictionString'].str.len()\n",
        "    diag['empty'] = (diag['PredictionString']=='')\n",
        "    print('Diag: empties', int(diag['empty'].sum()), '/ 112 | mean len', round(diag['len'].mean(),2), '| per-lang empties', diag.groupby('language')['empty'].sum().to_dict())\n",
        "    return sub\n",
        "\n",
        "_ = build_512only_spanlvl_tight()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activated 512-only tight variant as submission.csv | shape (112, 2) | wrote backup submission_512only_spanlvl_hi48_ta58_lenpen_plus0.0001.csv in 3.0s\nDiag: empties 0 / 112 | mean len 10.4 | per-lang empties {'hindi': 0, 'tamil': 0}\n"
          ]
        }
      ]
    },
    {
      "id": "1a83f573-2904-4461-88bd-7c4f418032af",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 512-only span-level with fallback gate (experts' top pick)\n",
        "import os, json, numpy as np, pandas as pd, time, re, unicodedata\n",
        "\n",
        "def _stable_logsoftmax(arr):\n",
        "    m = np.max(arr)\n",
        "    return arr - (m + np.log(np.sum(np.exp(arr - m))))\n",
        "\n",
        "def _decode_best_with_fallback(offsets, s_logit, e_logit, lang):\n",
        "    # Per-language knobs\n",
        "    lang_l = str(lang).lower().strip()\n",
        "    if lang_l == 'hindi':\n",
        "        n_best = 180; max_len = 50; len_pen = 0.00505; fb_cap = 36\n",
        "    elif lang_l == 'tamil':\n",
        "        n_best = 220; max_len = 60; len_pen = 0.00405; fb_cap = 42\n",
        "    else:\n",
        "        n_best = 150; max_len = 60; len_pen = 0.0048; fb_cap = 42\n",
        "    # mask non-context\n",
        "    mask = np.array([o is not None and o[0] is not None and o[1] is not None and (o[1] > o[0]) for o in offsets], dtype=bool)\n",
        "    s = np.where(mask, s_logit, -1e9)\n",
        "    e = np.where(mask, e_logit, -1e9)\n",
        "    s_logp = _stable_logsoftmax(s)\n",
        "    e_logp = _stable_logsoftmax(e)\n",
        "    top_s = np.argsort(s_logp)[-n_best:]\n",
        "    best = (float('-inf'), None, None)\n",
        "    for si in top_s:\n",
        "        ei_max = min(len(e_logp)-1, si + max_len - 1)\n",
        "        band = e_logp[si:ei_max+1]\n",
        "        if band.size == 0: continue\n",
        "        off = int(np.argmax(band))\n",
        "        ei = si + off\n",
        "        L = ei - si + 1\n",
        "        score = float(s_logp[si] + e_logp[ei] - len_pen * (L - 1))\n",
        "        if score > best[0]:\n",
        "            best = (score, si, ei)\n",
        "    # Fallback if chosen char span > 40: re-decode with tighter cap and accept if >= best-0.5\n",
        "    if best[1] is not None and best[2] is not None:\n",
        "        cs, _ = offsets[best[1]]; _, ce = offsets[best[2]]\n",
        "        if cs is not None and ce is not None and ce > cs:\n",
        "            if (ce - cs) > 40:\n",
        "                best_fb = (float('-inf'), None, None)\n",
        "                for si in top_s:\n",
        "                    ei_max = min(len(e_logp)-1, si + fb_cap - 1)\n",
        "                    band = e_logp[si:ei_max+1]\n",
        "                    if band.size == 0: continue\n",
        "                    off = int(np.argmax(band))\n",
        "                    ei = si + off\n",
        "                    L = ei - si + 1\n",
        "                    score = float(s_logp[si] + e_logp[ei] - len_pen * (L - 1))\n",
        "                    if score > best_fb[0]:\n",
        "                        best_fb = (score, si, ei)\n",
        "                if best_fb[1] is not None and best_fb[2] is not None and best_fb[0] >= best[0] - 0.5:\n",
        "                    best = best_fb\n",
        "    return best\n",
        "\n",
        "def _postprocess_ans_fallback(ans: str, lang: str = None) -> str:\n",
        "    if not ans: return ''\n",
        "    # numeric collapse\n",
        "    ans = re.sub(r'(?<=\\d)[ ,._-](?=\\d)', '', ans)\n",
        "    # danda/virama hygiene\n",
        "    ans = ans.replace('\\u094D\\u0964', '\\u0964').replace('\\u094D\u0964', '\u0964')\n",
        "    ans = re.sub(r'[\u0964]+', '\u0964', ans)\n",
        "    if ans.endswith('\u0964'): ans = ans[:-1]\n",
        "    # trailing combining/virama guard\n",
        "    if ans and (unicodedata.category(ans[-1]) == 'Mn' or ans[-1] == '\\u094D'): ans = ans[:-1]\n",
        "    lang_l = str(lang).lower().strip()\n",
        "    if lang_l == 'hindi' and len(ans) >= 2 and ans[-1] == '\\u094D' and (ans[-2].isspace() or ans[-2] in '\u0964,:;.!?\"\\'\\u0964\\u0965-'):\n",
        "        ans = ans[:-1]\n",
        "    if lang_l == 'tamil' and len(ans) and unicodedata.category(ans[-1]) == 'Mn':\n",
        "        ans = ans[:-1]\n",
        "    if ans and unicodedata.category(ans[0]) == 'Mn': ans = ans[1:]\n",
        "    # collapse hyphens inside words\n",
        "    ans = re.sub(r'(?<=\\S)[\\-\u2013\u2014]{2,}(?=\\S)', '-', ans)\n",
        "    # strip outer punct/space\n",
        "    punct_strip = '\\u0964\\u0965\u0964,:;.!?\\\"\\'\\\u201c\\\u201d\\\u2018\\\u2019\\)\\]\\}\\|\\-\\s'\n",
        "    ans = ans.strip().strip(punct_strip)\n",
        "    # unmatched quotes\n",
        "    for lq, rq in [(\"\\\"\",\"\\\"\"), (\"'\",\"'\"), ('\\u201c','\\u201d'), ('\\u2018','\\u2019')]:\n",
        "        if ans.startswith(lq) and not ans.endswith(rq): ans = ans[len(lq):]\n",
        "        if ans.endswith(rq) and not ans.startswith(lq): ans = ans[:-len(rq)]\n",
        "    # zero-widths and spaces\n",
        "    ans = ans.replace('\\ufeff','').replace('\\u200e','').replace('\\u200f','')\n",
        "    ans = ans.replace('\\u200B','').replace('\\u00AD','')\n",
        "    ans = ans.replace('\\u00A0',' ').replace('\\u2009',' ')\n",
        "    ans = ' '.join(ans.split())\n",
        "    ans = ans.replace('\\u200C','').replace('\\u200D','')\n",
        "    if ans.endswith('\u0964'): ans = ans[:-1]\n",
        "    return ans or ''\n",
        "\n",
        "def build_512only_with_fallback_submit():\n",
        "    t0 = time.time()\n",
        "    # Prefer 3-seed avg npz\n",
        "    if os.path.exists('xlmr_large_512_3seeds_avg.npz'):\n",
        "        npz = 'xlmr_large_512_3seeds_avg.npz'\n",
        "    elif os.path.exists('xlmr_large_512_2seeds_avg.npz'):\n",
        "        npz = 'xlmr_large_512_2seeds_avg.npz'\n",
        "    else:\n",
        "        npz = 'xlmr_large_512_test_avg.npz'\n",
        "    exid_json = 'xlmr_large_512_test_logits/test_example_id.json'\n",
        "    offs_npy = 'xlmr_large_512_test_logits/test_offset_mapping.npy'\n",
        "    assert os.path.exists(npz) and os.path.exists(exid_json) and os.path.exists(offs_npy), 'Missing 512 artifacts'\n",
        "    data = np.load(npz)\n",
        "    S = data['start']; E = data['end']\n",
        "    with open(exid_json, 'r', encoding='utf-8') as f:\n",
        "        exids = json.load(f)\n",
        "    offs_all = np.load(offs_npy, allow_pickle=True)\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    id2ctx = dict(zip(test_df['id'].astype(str), test_df['context'].astype(str)))\n",
        "    id2lang = dict(zip(test_df['id'].astype(str), test_df['language'].astype(str) if 'language' in test_df.columns else ['unknown']*len(test_df)))\n",
        "    # Best per example\n",
        "    best_per_ex = {}\n",
        "    for i in range(S.shape[0]):\n",
        "        ex_id = exids[i]\n",
        "        lang = id2lang.get(ex_id, 'unknown')\n",
        "        score, si, ei = _decode_best_with_fallback(offs_all[i], S[i], E[i], lang)\n",
        "        if si is not None and ei is not None:\n",
        "            cs, _ = offs_all[i][si]; _, ce = offs_all[i][ei]\n",
        "            if cs is not None and ce is not None and ce > cs:\n",
        "                text = id2ctx.get(ex_id, '')[cs:ce]\n",
        "            else:\n",
        "                text = ''\n",
        "        else:\n",
        "            text = ''\n",
        "        prev = best_per_ex.get(ex_id, (float('-inf'), ''))\n",
        "        if score > prev[0]:\n",
        "            best_per_ex[ex_id] = (score, text)\n",
        "    # Post-process and save\n",
        "    out_ids = test_df['id'].astype(str).values\n",
        "    preds = []\n",
        "    for ex_id in out_ids:\n",
        "        lang = id2lang.get(ex_id, 'unknown')\n",
        "        ans = best_per_ex.get(ex_id, (float('-inf'), ''))[1]\n",
        "        preds.append(_postprocess_ans_fallback(ans, lang))\n",
        "    sub = pd.DataFrame({'id': out_ids, 'PredictionString': preds})\n",
        "    out_name = 'submission_512only_fallback_hi180_50_00505_ta220_60_00405_fb36_42.csv'\n",
        "    sub.to_csv(out_name, index=False)\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    # Diagnostics\n",
        "    diag = sub.merge(test_df[['id','language']], on='id', how='left')\n",
        "    diag['PredictionString'] = diag['PredictionString'].astype(str).replace('nan','').fillna('')\n",
        "    diag['len'] = diag['PredictionString'].str.len()\n",
        "    diag['empty'] = (diag['PredictionString']=='')\n",
        "    print('512-only with fallback saved as submission.csv and', out_name, '| Empties:', int(diag['empty'].sum()), '/ 112 | Mean len:', round(diag['len'].mean(),2), '| Per-lang empties:', diag.groupby('language')['empty'].sum().to_dict(), '| Took: %.1fs' % (time.time()-t0))\n",
        "    return sub\n",
        "\n",
        "# Build and activate\n",
        "_ = build_512only_with_fallback_submit()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512-only with fallback saved as submission.csv and submission_512only_fallback_hi180_50_00505_ta220_60_00405_fb36_42.csv | Empties: 0 / 112 | Mean len: 10.4 | Per-lang empties: {'hindi': 0, 'tamil': 0} | Took: 2.4s\n"
          ]
        }
      ]
    },
    {
      "id": "a2a6356f-d325-47d6-a800-68acacae9527",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 512-only span-level, no-fallback, tiny knob bump (experts' #2 variant)\n",
        "import os, json, numpy as np, pandas as pd, time, re, unicodedata\n",
        "\n",
        "def _stable_logsoftmax(arr):\n",
        "    m = np.max(arr)\n",
        "    return arr - (m + np.log(np.sum(np.exp(arr - m))))\n",
        "\n",
        "def _decode_best_nofallback(offsets, s_logit, e_logit, lang):\n",
        "    lang_l = str(lang).lower().strip()\n",
        "    if lang_l == 'hindi':\n",
        "        n_best = 190; max_len = 50; len_pen = 0.00505\n",
        "    elif lang_l == 'tamil':\n",
        "        n_best = 230; max_len = 60; len_pen = 0.00405\n",
        "    else:\n",
        "        n_best = 150; max_len = 60; len_pen = 0.0048\n",
        "    mask = np.array([o is not None and o[0] is not None and o[1] is not None and (o[1] > o[0]) for o in offsets], dtype=bool)\n",
        "    s = np.where(mask, s_logit, -1e9)\n",
        "    e = np.where(mask, e_logit, -1e9)\n",
        "    s_logp = _stable_logsoftmax(s)\n",
        "    e_logp = _stable_logsoftmax(e)\n",
        "    top_s = np.argsort(s_logp)[-n_best:]\n",
        "    best = (float('-inf'), None, None)\n",
        "    for si in top_s:\n",
        "        ei_max = min(len(e_logp)-1, si + max_len - 1)\n",
        "        band = e_logp[si:ei_max+1]\n",
        "        if band.size == 0: continue\n",
        "        off = int(np.argmax(band))\n",
        "        ei = si + off\n",
        "        L = ei - si + 1\n",
        "        score = float(s_logp[si] + e_logp[ei] - len_pen * (L - 1))\n",
        "        if score > best[0]:\n",
        "            best = (score, si, ei)\n",
        "    return best\n",
        "\n",
        "def _postprocess_ans_min2(ans: str, lang: str = None) -> str:\n",
        "    if not ans: return ''\n",
        "    ans = re.sub(r'(?<=\\d)[ ,._-](?=\\d)', '', ans)\n",
        "    ans = ans.replace('\\u094D\\u0964', '\\u0964').replace('\\u094D\u0964', '\u0964')\n",
        "    ans = re.sub(r'[\u0964]+', '\u0964', ans)\n",
        "    if ans.endswith('\u0964'): ans = ans[:-1]\n",
        "    if ans and (unicodedata.category(ans[-1]) == 'Mn' or ans[-1] == '\\u094D'): ans = ans[:-1]\n",
        "    lang_l = str(lang).lower().strip()\n",
        "    if lang_l == 'hindi' and len(ans) >= 2 and ans[-1] == '\\u094D' and (ans[-2].isspace() or ans[-2] in '\u0964,:;.!?\"\\'\\u0964\\u0965-'):\n",
        "        ans = ans[:-1]\n",
        "    if lang_l == 'tamil' and len(ans) and unicodedata.category(ans[-1]) == 'Mn':\n",
        "        ans = ans[:-1]\n",
        "    if ans and unicodedata.category(ans[0]) == 'Mn': ans = ans[1:]\n",
        "    punct_strip = '\\u0964\\u0965\u0964,:;.!?\\\"\\'\\\u201c\\\u201d\\\u2018\\\u2019\\)\\]\\}\\|\\-\\s'\n",
        "    ans = ans.strip().strip(punct_strip)\n",
        "    for lq, rq in [(\"\\\"\",\"\\\"\"), (\"'\",\"'\"), ('\\u201c','\\u201d'), ('\\u2018','\\u2019')]:\n",
        "        if ans.startswith(lq) and not ans.endswith(rq): ans = ans[len(lq):]\n",
        "        if ans.endswith(rq) and not ans.startswith(lq): ans = ans[:-len(rq)]\n",
        "    ans = ans.replace('\\ufeff','').replace('\\u200e','').replace('\\u200f','')\n",
        "    ans = ans.replace('\\u200B','').replace('\\u00AD','')\n",
        "    ans = ans.replace('\\u00A0',' ').replace('\\u2009',' ')\n",
        "    ans = ' '.join(ans.split())\n",
        "    ans = ans.replace('\\u200C','').replace('\\u200D','')\n",
        "    if ans.endswith('\u0964'): ans = ans[:-1]\n",
        "    return ans or ''\n",
        "\n",
        "def build_512only_nofallback_submit():\n",
        "    t0 = time.time()\n",
        "    if os.path.exists('xlmr_large_512_3seeds_avg.npz'):\n",
        "        npz = 'xlmr_large_512_3seeds_avg.npz'\n",
        "    elif os.path.exists('xlmr_large_512_2seeds_avg.npz'):\n",
        "        npz = 'xlmr_large_512_2seeds_avg.npz'\n",
        "    else:\n",
        "        npz = 'xlmr_large_512_test_avg.npz'\n",
        "    exid_json = 'xlmr_large_512_test_logits/test_example_id.json'\n",
        "    offs_npy = 'xlmr_large_512_test_logits/test_offset_mapping.npy'\n",
        "    assert os.path.exists(npz) and os.path.exists(exid_json) and os.path.exists(offs_npy), 'Missing 512 artifacts'\n",
        "    data = np.load(npz)\n",
        "    S = data['start']; E = data['end']\n",
        "    with open(exid_json, 'r', encoding='utf-8') as f:\n",
        "        exids = json.load(f)\n",
        "    offs_all = np.load(offs_npy, allow_pickle=True)\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    id2ctx = dict(zip(test_df['id'].astype(str), test_df['context'].astype(str)))\n",
        "    id2lang = dict(zip(test_df['id'].astype(str), test_df['language'].astype(str) if 'language' in test_df.columns else ['unknown']*len(test_df)))\n",
        "    best_per_ex = {}\n",
        "    for i in range(S.shape[0]):\n",
        "        ex_id = exids[i]\n",
        "        lang = id2lang.get(ex_id, 'unknown')\n",
        "        score, si, ei = _decode_best_nofallback(offs_all[i], S[i], E[i], lang)\n",
        "        if si is not None and ei is not None:\n",
        "            cs, _ = offs_all[i][si]; _, ce = offs_all[i][ei]\n",
        "            if cs is not None and ce is not None and ce > cs:\n",
        "                text = id2ctx.get(ex_id, '')[cs:ce]\n",
        "            else:\n",
        "                text = ''\n",
        "        else:\n",
        "            text = ''\n",
        "        prev = best_per_ex.get(ex_id, (float('-inf'), ''))\n",
        "        if score > prev[0]:\n",
        "            best_per_ex[ex_id] = (score, text)\n",
        "    out_ids = test_df['id'].astype(str).values\n",
        "    preds = []\n",
        "    for ex_id in out_ids:\n",
        "        lang = id2lang.get(ex_id, 'unknown')\n",
        "        ans = best_per_ex.get(ex_id, (float('-inf'), ''))[1]\n",
        "        preds.append(_postprocess_ans_min2(ans, lang))\n",
        "    sub = pd.DataFrame({'id': out_ids, 'PredictionString': preds})\n",
        "    out_name = 'submission_512only_nb_hi190_ta230_max50_60_lp0.00505_0.00405.csv'\n",
        "    sub.to_csv(out_name, index=False)\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    # Diagnostics\n",
        "    diag = sub.merge(test_df[['id','language']], on='id', how='left')\n",
        "    diag['PredictionString'] = diag['PredictionString'].astype(str).replace('nan','').fillna('')\n",
        "    diag['len'] = diag['PredictionString'].str.len()\n",
        "    diag['empty'] = (diag['PredictionString']=='')\n",
        "    print('512-only no-fallback saved as submission.csv and', out_name, '| Empties:', int(diag['empty'].sum()), '/ 112 | Mean len:', round(diag['len'].mean(),2), '| Per-lang empties:', diag.groupby('language')['empty'].sum().to_dict(), '| Took: %.1fs' % (time.time()-t0))\n",
        "    return sub\n",
        "\n",
        "_ = build_512only_nofallback_submit()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512-only no-fallback saved as submission.csv and submission_512only_nb_hi190_ta230_max50_60_lp0.00505_0.00405.csv | Empties: 0 / 112 | Mean len: 10.4 | Per-lang empties: {'hindi': 0, 'tamil': 0} | Took: 2.8s\n"
          ]
        }
      ]
    },
    {
      "id": "a348d208-b38e-4d9c-a5de-5b2782c3859d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Hybrid-Lang submission: Hindi from 3-stream span-level (DEF weights), Tamil from 512-only with fallback\n",
        "import pandas as pd, os\n",
        "\n",
        "hi_src = 'submission_spanlvl_def_weights_defknobs.csv'  # fallback to DEF file from Cell 37\n",
        "ta_src = 'submission_512only_fallback_hi180_50_00505_ta220_60_00405_fb36_42.csv'  # Cell 42\n",
        "assert os.path.exists(hi_src), f'Missing Hindi source: {hi_src}'\n",
        "assert os.path.exists(ta_src), f'Missing Tamil source: {ta_src}'\n",
        "\n",
        "test_df = pd.read_csv('test.csv')\n",
        "lang_map = dict(zip(test_df['id'].astype(str), test_df['language'].astype(str)))\n",
        "df_hi = pd.read_csv(hi_src).copy()\n",
        "df_ta = pd.read_csv(ta_src).copy()\n",
        "df_hi['PredictionString'] = df_hi['PredictionString'].astype(str).replace('nan','').fillna('')\n",
        "df_ta['PredictionString'] = df_ta['PredictionString'].astype(str).replace('nan','').fillna('')\n",
        "map_hi = dict(zip(df_hi['id'].astype(str), df_hi['PredictionString']))\n",
        "map_ta = dict(zip(df_ta['id'].astype(str), df_ta['PredictionString']))\n",
        "\n",
        "out_ids = test_df['id'].astype(str).values\n",
        "out_pred = []\n",
        "for _id in out_ids:\n",
        "    lang = str(lang_map.get(_id, 'unknown')).lower().strip()\n",
        "    if lang == 'hindi':\n",
        "        ans = map_hi.get(_id, '')\n",
        "    elif lang == 'tamil':\n",
        "        ans = map_ta.get(_id, '')\n",
        "    else:\n",
        "        ans = map_ta.get(_id, '')  # default to safer 512-fallback\n",
        "    out_pred.append(ans if isinstance(ans, str) else '')\n",
        "\n",
        "hybrid = pd.DataFrame({'id': out_ids, 'PredictionString': out_pred})\n",
        "hybrid.to_csv('submission_hybrid_hi3stream_ta512fb.csv', index=False)\n",
        "hybrid.to_csv('submission.csv', index=False)\n",
        "print('Hybrid submission built and activated as submission.csv; backup saved as submission_hybrid_hi3stream_ta512fb.csv')\n",
        "# Quick diagnostics\n",
        "diag = hybrid.merge(test_df[['id','language']], on='id', how='left')\n",
        "diag['PredictionString'] = diag['PredictionString'].astype(str).replace('nan','').fillna('')\n",
        "diag['len'] = diag['PredictionString'].str.len()\n",
        "diag['empty'] = (diag['PredictionString'] == '')\n",
        "print('Diag: empties', int(diag['empty'].sum()), '/ 112 | mean len', round(diag['len'].mean(),2), '| per-lang empties', diag.groupby('language')['empty'].sum().to_dict())\n",
        "print(diag.head())"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hybrid submission built and activated as submission.csv; backup saved as submission_hybrid_hi3stream_ta512fb.csv\nDiag: empties 0 / 112 | mean len 10.38 | per-lang empties {'hindi': 0, 'tamil': 0}\n          id   PredictionString language  len  empty\n0  be799d365  \u092e\u0941\u0902\u092c\u0908, \u092e\u0939\u093e\u0930\u093e\u0937\u094d\u091f\u094d\u0930    hindi   17  False\n1  26f356026        \u0938\u0941\u0927\u093e\u0902\u0936\u0941\u092c\u093e\u0932\u093e    hindi   11  False\n2  57a56c43f   \u0baa\u0bc6\u0bb0\u0bc1\u0bae\u0bc2\u0bb3\u0bc8\u0baa\u0bcd \u0baa\u0bc1\u0bb1\u0ba3\u0bbf    tamil   16  False\n3  da062fdbb           \u092c\u093f\u0902\u092c\u093f\u0938\u093e\u0930    hindi    8  False\n4  72fc0d5b5               1889    hindi    4  False\n"
          ]
        }
      ]
    },
    {
      "id": "d903803f-c3be-4f14-976c-16876501e3d2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 512-only span-level with fallback and n_best bump (hi=200, ta=240) per expert advice\n",
        "import os, json, numpy as np, pandas as pd, time, re, unicodedata\n",
        "\n",
        "def _stable_logsoftmax(arr):\n",
        "    m = np.max(arr)\n",
        "    return arr - (m + np.log(np.sum(np.exp(arr - m))))\n",
        "\n",
        "def _decode_best_with_fallback_nb_bump(offsets, s_logit, e_logit, lang):\n",
        "    lang_l = str(lang).lower().strip()\n",
        "    if lang_l == 'hindi':\n",
        "        n_best = 200; max_len = 50; len_pen = 0.00505; fb_cap = 36\n",
        "    elif lang_l == 'tamil':\n",
        "        n_best = 240; max_len = 60; len_pen = 0.00405; fb_cap = 42\n",
        "    else:\n",
        "        n_best = 150; max_len = 60; len_pen = 0.0048; fb_cap = 42\n",
        "    mask = np.array([o is not None and o[0] is not None and o[1] is not None and (o[1] > o[0]) for o in offsets], dtype=bool)\n",
        "    s = np.where(mask, s_logit, -1e9)\n",
        "    e = np.where(mask, e_logit, -1e9)\n",
        "    s_logp = _stable_logsoftmax(s)\n",
        "    e_logp = _stable_logsoftmax(e)\n",
        "    top_s = np.argsort(s_logp)[-n_best:]\n",
        "    best = (float('-inf'), None, None)\n",
        "    for si in top_s:\n",
        "        ei_max = min(len(e_logp)-1, si + max_len - 1)\n",
        "        band = e_logp[si:ei_max+1]\n",
        "        if band.size == 0: continue\n",
        "        off = int(np.argmax(band))\n",
        "        ei = si + off\n",
        "        L = ei - si + 1\n",
        "        score = float(s_logp[si] + e_logp[ei] - len_pen * (L - 1))\n",
        "        if score > best[0]:\n",
        "            best = (score, si, ei)\n",
        "    if best[1] is not None and best[2] is not None:\n",
        "        cs, _ = offsets[best[1]]; _, ce = offsets[best[2]]\n",
        "        if cs is not None and ce is not None and ce > cs:\n",
        "            if (ce - cs) > 40:\n",
        "                best_fb = (float('-inf'), None, None)\n",
        "                for si in top_s:\n",
        "                    ei_max = min(len(e_logp)-1, si + fb_cap - 1)\n",
        "                    band = e_logp[si:ei_max+1]\n",
        "                    if band.size == 0: continue\n",
        "                    off = int(np.argmax(band))\n",
        "                    ei = si + off\n",
        "                    L = ei - si + 1\n",
        "                    score = float(s_logp[si] + e_logp[ei] - len_pen * (L - 1))\n",
        "                    if score > best_fb[0]:\n",
        "                        best_fb = (score, si, ei)\n",
        "                if best_fb[1] is not None and best_fb[2] is not None and best_fb[0] >= best[0] - 0.5:\n",
        "                    best = best_fb\n",
        "    return best\n",
        "\n",
        "def _postprocess_ans_fb(ans: str, lang: str = None) -> str:\n",
        "    if not ans: return ''\n",
        "    ans = re.sub(r'(?<=\\d)[ ,._-](?=\\d)', '', ans)\n",
        "    ans = ans.replace('\\u094D\\u0964', '\\u0964').replace('\\u094D\u0964', '\u0964')\n",
        "    ans = re.sub(r'[\u0964]+', '\u0964', ans)\n",
        "    if ans.endswith('\u0964'): ans = ans[:-1]\n",
        "    if ans and (unicodedata.category(ans[-1]) == 'Mn' or ans[-1] == '\\u094D'): ans = ans[:-1]\n",
        "    lang_l = str(lang).lower().strip()\n",
        "    if lang_l == 'hindi' and len(ans) >= 2 and ans[-1] == '\\u094D' and (ans[-2].isspace() or ans[-2] in '\u0964,:;.!?\"\\'\\u0964\\u0965-'): ans = ans[:-1]\n",
        "    if lang_l == 'tamil' and len(ans) and unicodedata.category(ans[-1]) == 'Mn': ans = ans[:-1]\n",
        "    if ans and unicodedata.category(ans[0]) == 'Mn': ans = ans[1:]\n",
        "    ans = re.sub(r'(?<=\\S)[\\-\u2013\u2014]{2,}(?=\\S)', '-', ans)\n",
        "    punct_strip = '\\u0964\\u0965\u0964,:;.!?\\\"\\'\\\u201c\\\u201d\\\u2018\\\u2019\\)\\]\\}\\|\\-\\s'\n",
        "    ans = ans.strip().strip(punct_strip)\n",
        "    for lq, rq in [(\"\\\"\",\"\\\"\"), (\"'\",\"'\"), ('\\u201c','\\u201d'), ('\\u2018','\\u2019')]:\n",
        "        if ans.startswith(lq) and not ans.endswith(rq): ans = ans[len(lq):]\n",
        "        if ans.endswith(rq) and not ans.startswith(lq): ans = ans[:-len(rq)]\n",
        "    ans = ans.replace('\\ufeff','').replace('\\u200e','').replace('\\u200f','')\n",
        "    ans = ans.replace('\\u200B','').replace('\\u00AD','')\n",
        "    ans = ans.replace('\\u00A0',' ').replace('\\u2009',' ')\n",
        "    ans = ' '.join(ans.split())\n",
        "    ans = ans.replace('\\u200C','').replace('\\u200D','')\n",
        "    if ans.endswith('\u0964'): ans = ans[:-1]\n",
        "    return ans or ''\n",
        "\n",
        "def build_512only_with_fallback_nb_bump_submit():\n",
        "    t0 = time.time()\n",
        "    if os.path.exists('xlmr_large_512_3seeds_avg.npz'):\n",
        "        npz = 'xlmr_large_512_3seeds_avg.npz'\n",
        "    elif os.path.exists('xlmr_large_512_2seeds_avg.npz'):\n",
        "        npz = 'xlmr_large_512_2seeds_avg.npz'\n",
        "    else:\n",
        "        npz = 'xlmr_large_512_test_avg.npz'\n",
        "    exid_json = 'xlmr_large_512_test_logits/test_example_id.json'\n",
        "    offs_npy = 'xlmr_large_512_test_logits/test_offset_mapping.npy'\n",
        "    assert os.path.exists(npz) and os.path.exists(exid_json) and os.path.exists(offs_npy), 'Missing 512 artifacts'\n",
        "    data = np.load(npz)\n",
        "    S = data['start']; E = data['end']\n",
        "    with open(exid_json, 'r', encoding='utf-8') as f:\n",
        "        exids = json.load(f)\n",
        "    offs_all = np.load(offs_npy, allow_pickle=True)\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    id2ctx = dict(zip(test_df['id'].astype(str), test_df['context'].astype(str)))\n",
        "    id2lang = dict(zip(test_df['id'].astype(str), test_df['language'].astype(str) if 'language' in test_df.columns else ['unknown']*len(test_df)))\n",
        "    best_per_ex = {}\n",
        "    for i in range(S.shape[0]):\n",
        "        ex_id = exids[i]\n",
        "        lang = id2lang.get(ex_id, 'unknown')\n",
        "        score, si, ei = _decode_best_with_fallback_nb_bump(offs_all[i], S[i], E[i], lang)\n",
        "        if si is not None and ei is not None:\n",
        "            cs, _ = offs_all[i][si]; _, ce = offs_all[i][ei]\n",
        "            if cs is not None and ce is not None and ce > cs:\n",
        "                text = id2ctx.get(ex_id, '')[cs:ce]\n",
        "            else:\n",
        "                text = ''\n",
        "        else:\n",
        "            text = ''\n",
        "        prev = best_per_ex.get(ex_id, (float('-inf'), ''))\n",
        "        if score > prev[0]:\n",
        "            best_per_ex[ex_id] = (score, text)\n",
        "    out_ids = test_df['id'].astype(str).values\n",
        "    preds = []\n",
        "    for ex_id in out_ids:\n",
        "        lang = id2lang.get(ex_id, 'unknown')\n",
        "        ans = best_per_ex.get(ex_id, (float('-inf'), ''))[1]\n",
        "        preds.append(_postprocess_ans_fb(ans, lang))\n",
        "    sub = pd.DataFrame({'id': out_ids, 'PredictionString': preds})\n",
        "    out_name = 'submission_512only_fallback_nb_bump_hi200_ta240.csv'\n",
        "    sub.to_csv(out_name, index=False)\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    diag = sub.merge(test_df[['id','language']], on='id', how='left')\n",
        "    diag['PredictionString'] = diag['PredictionString'].astype(str).replace('nan','').fillna('')\n",
        "    diag['len'] = diag['PredictionString'].str.len()\n",
        "    diag['empty'] = (diag['PredictionString']=='')\n",
        "    print('512-only fallback (nb bump) saved as submission.csv and', out_name, '| Empties:', int(diag['empty'].sum()), '/ 112 | Mean len:', round(diag['len'].mean(),2), '| Per-lang empties:', diag.groupby('language')['empty'].sum().to_dict(), '| Took: %.1fs' % (time.time()-t0))\n",
        "    return sub\n",
        "\n",
        "_ = build_512only_with_fallback_nb_bump_submit()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512-only fallback (nb bump) saved as submission.csv and submission_512only_fallback_nb_bump_hi200_ta240.csv | Empties: 0 / 112 | Mean len: 10.4 | Per-lang empties: {'hindi': 0, 'tamil': 0} | Took: 2.8s\n"
          ]
        }
      ]
    },
    {
      "id": "776c8291-6496-4eb3-a3ad-861656c20da5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Activate 3-stream primary (DEF weights, DEF knobs) as submission.csv\n",
        "import pandas as pd, os\n",
        "src = 'submission_spanlvl_def_weights_defknobs.csv'\n",
        "assert os.path.exists(src), f'Missing file: {src}'\n",
        "df = pd.read_csv(src)\n",
        "if 'PredictionString' in df.columns:\n",
        "    df['PredictionString'] = df['PredictionString'].astype(str).replace('nan','').fillna('')\n",
        "df.to_csv('submission.csv', index=False)\n",
        "print('submission.csv replaced with', src)\n",
        "print(df.head())"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv replaced with submission_spanlvl_def_weights_defknobs.csv\n          id   PredictionString\n0  be799d365  \u092e\u0941\u0902\u092c\u0908, \u092e\u0939\u093e\u0930\u093e\u0937\u094d\u091f\u094d\u0930\n1  26f356026        \u0938\u0941\u0927\u093e\u0902\u0936\u0941\u092c\u093e\u0932\u093e\n2  57a56c43f   \u0baa\u0bc6\u0bb0\u0bc1\u0bae\u0bc2\u0bb3\u0bc8\u0baa\u0bcd \u0baa\u0bc1\u0bb1\u0ba3\u0bbf\n3  da062fdbb           \u092c\u093f\u0902\u092c\u093f\u0938\u093e\u0930\n4  72fc0d5b5               1889\n"
          ]
        }
      ]
    },
    {
      "id": "64cd77f2-a8a4-47d8-a4cd-077002303b97",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 512-only span-level with fallback using DEF knobs (hi: 180/52/0.0050; ta: 220/62/0.0040)\n",
        "import os, json, numpy as np, pandas as pd, time, re, unicodedata\n",
        "\n",
        "def _stable_logsoftmax(arr):\n",
        "    m = np.max(arr)\n",
        "    return arr - (m + np.log(np.sum(np.exp(arr - m))))\n",
        "\n",
        "def _decode_best_with_fallback_def(offsets, s_logit, e_logit, lang):\n",
        "    lang_l = str(lang).lower().strip()\n",
        "    if lang_l == 'hindi':\n",
        "        n_best = 180; max_len = 52; len_pen = 0.0050; fb_cap = 36\n",
        "    elif lang_l == 'tamil':\n",
        "        n_best = 220; max_len = 62; len_pen = 0.0040; fb_cap = 42\n",
        "    else:\n",
        "        n_best = 150; max_len = 60; len_pen = 0.0048; fb_cap = 42\n",
        "    mask = np.array([o is not None and o[0] is not None and o[1] is not None and (o[1] > o[0]) for o in offsets], dtype=bool)\n",
        "    s = np.where(mask, s_logit, -1e9)\n",
        "    e = np.where(mask, e_logit, -1e9)\n",
        "    s_logp = _stable_logsoftmax(s)\n",
        "    e_logp = _stable_logsoftmax(e)\n",
        "    top_s = np.argsort(s_logp)[-n_best:]\n",
        "    best = (float('-inf'), None, None)\n",
        "    for si in top_s:\n",
        "        ei_max = min(len(e_logp)-1, si + max_len - 1)\n",
        "        band = e_logp[si:ei_max+1]\n",
        "        if band.size == 0: continue\n",
        "        off = int(np.argmax(band))\n",
        "        ei = si + off\n",
        "        L = ei - si + 1\n",
        "        score = float(s_logp[si] + e_logp[ei] - len_pen * (L - 1))\n",
        "        if score > best[0]:\n",
        "            best = (score, si, ei)\n",
        "    if best[1] is not None and best[2] is not None:\n",
        "        cs, _ = offsets[best[1]]; _, ce = offsets[best[2]]\n",
        "        if cs is not None and ce is not None and ce > cs:\n",
        "            if (ce - cs) > 40:\n",
        "                best_fb = (float('-inf'), None, None)\n",
        "                for si in top_s:\n",
        "                    ei_max = min(len(e_logp)-1, si + fb_cap - 1)\n",
        "                    band = e_logp[si:ei_max+1]\n",
        "                    if band.size == 0: continue\n",
        "                    off = int(np.argmax(band))\n",
        "                    ei = si + off\n",
        "                    L = ei - si + 1\n",
        "                    score = float(s_logp[si] + e_logp[ei] - len_pen * (L - 1))\n",
        "                    if score > best_fb[0]:\n",
        "                        best_fb = (score, si, ei)\n",
        "                if best_fb[1] is not None and best_fb[2] is not None and best_fb[0] >= best[0] - 0.5:\n",
        "                    best = best_fb\n",
        "    return best\n",
        "\n",
        "def _postprocess_ans_def(ans: str, lang: str = None) -> str:\n",
        "    if not ans: return ''\n",
        "    ans = re.sub(r'(?<=\\d)[ ,._-](?=\\d)', '', ans)\n",
        "    ans = ans.replace('\\u094D\\u0964', '\\u0964').replace('\\u094D\u0964', '\u0964')\n",
        "    ans = re.sub(r'[\u0964]+', '\u0964', ans)\n",
        "    if ans.endswith('\u0964'): ans = ans[:-1]\n",
        "    if ans and (unicodedata.category(ans[-1]) == 'Mn' or ans[-1] == '\\u094D'): ans = ans[:-1]\n",
        "    lang_l = str(lang).lower().strip()\n",
        "    if lang_l == 'hindi' and len(ans) >= 2 and ans[-1] == '\\u094D' and (ans[-2].isspace() or ans[-2] in '\u0964,:;.!?\"\\'\\u0964\\u0965-'): ans = ans[:-1]\n",
        "    if lang_l == 'tamil' and len(ans) and unicodedata.category(ans[-1]) == 'Mn': ans = ans[:-1]\n",
        "    if ans and unicodedata.category(ans[0]) == 'Mn': ans = ans[1:]\n",
        "    ans = re.sub(r'(?<=\\S)[\\-\u2013\u2014]{2,}(?=\\S)', '-', ans)\n",
        "    punct_strip = '\\u0964\\u0965\u0964,:;.!?\\\"\\'\\\u201c\\\u201d\\\u2018\\\u2019\\)\\]\\}\\|\\-\\s'\n",
        "    ans = ans.strip().strip(punct_strip)\n",
        "    for lq, rq in [(\"\\\"\",\"\\\"\"), (\"'\",\"'\"), ('\\u201c','\\u201d'), ('\\u2018','\\u2019')]:\n",
        "        if ans.startswith(lq) and not ans.endswith(rq): ans = ans[len(lq):]\n",
        "        if ans.endswith(rq) and not ans.startswith(lq): ans = ans[:-len(rq)]\n",
        "    ans = ans.replace('\\ufeff','').replace('\\u200e','').replace('\\u200f','')\n",
        "    ans = ans.replace('\\u200B','').replace('\\u00AD','')\n",
        "    ans = ans.replace('\\u00A0',' ').replace('\\u2009',' ')\n",
        "    ans = ' '.join(ans.split())\n",
        "    ans = ans.replace('\\u200C','').replace('\\u200D','')\n",
        "    if ans.endswith('\u0964'): ans = ans[:-1]\n",
        "    return ans or ''\n",
        "\n",
        "def build_512only_with_fallback_def_submit():\n",
        "    t0 = time.time()\n",
        "    if os.path.exists('xlmr_large_512_3seeds_avg.npz'):\n",
        "        npz = 'xlmr_large_512_3seeds_avg.npz'\n",
        "    elif os.path.exists('xlmr_large_512_2seeds_avg.npz'):\n",
        "        npz = 'xlmr_large_512_2seeds_avg.npz'\n",
        "    else:\n",
        "        npz = 'xlmr_large_512_test_avg.npz'\n",
        "    exid_json = 'xlmr_large_512_test_logits/test_example_id.json'\n",
        "    offs_npy = 'xlmr_large_512_test_logits/test_offset_mapping.npy'\n",
        "    assert os.path.exists(npz) and os.path.exists(exid_json) and os.path.exists(offs_npy), 'Missing 512 artifacts'\n",
        "    data = np.load(npz)\n",
        "    S = data['start']; E = data['end']\n",
        "    with open(exid_json, 'r', encoding='utf-8') as f:\n",
        "        exids = json.load(f)\n",
        "    offs_all = np.load(offs_npy, allow_pickle=True)\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    id2ctx = dict(zip(test_df['id'].astype(str), test_df['context'].astype(str)))\n",
        "    id2lang = dict(zip(test_df['id'].astype(str), test_df['language'].astype(str) if 'language' in test_df.columns else ['unknown']*len(test_df)))\n",
        "    best_per_ex = {}\n",
        "    for i in range(S.shape[0]):\n",
        "        ex_id = exids[i]\n",
        "        lang = id2lang.get(ex_id, 'unknown')\n",
        "        score, si, ei = _decode_best_with_fallback_def(offs_all[i], S[i], E[i], lang)\n",
        "        if si is not None and ei is not None:\n",
        "            cs, _ = offs_all[i][si]; _, ce = offs_all[i][ei]\n",
        "            if cs is not None and ce is not None and ce > cs:\n",
        "                text = id2ctx.get(ex_id, '')[cs:ce]\n",
        "            else:\n",
        "                text = ''\n",
        "        else:\n",
        "            text = ''\n",
        "        prev = best_per_ex.get(ex_id, (float('-inf'), ''))\n",
        "        if score > prev[0]:\n",
        "            best_per_ex[ex_id] = (score, text)\n",
        "    out_ids = test_df['id'].astype(str).values\n",
        "    preds = []\n",
        "    for ex_id in out_ids:\n",
        "        lang = id2lang.get(ex_id, 'unknown')\n",
        "        ans = best_per_ex.get(ex_id, (float('-inf'), ''))[1]\n",
        "        preds.append(_postprocess_ans_def(ans, lang))\n",
        "    sub = pd.DataFrame({'id': out_ids, 'PredictionString': preds})\n",
        "    out_name = 'submission_512only_fallback_hi180_52_00500_ta220_62_00400_fb36_42.csv'\n",
        "    sub.to_csv(out_name, index=False)\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    diag = sub.merge(test_df[['id','language']], on='id', how='left')\n",
        "    diag['PredictionString'] = diag['PredictionString'].astype(str).replace('nan','').fillna('')\n",
        "    diag['len'] = diag['PredictionString'].str.len()\n",
        "    diag['empty'] = (diag['PredictionString']=='')\n",
        "    print('512-only fallback (DEF knobs) saved as submission.csv and', out_name, '| Empties:', int(diag['empty'].sum()), '/ 112 | Mean len:', round(diag['len'].mean(),2), '| Per-lang empties:', diag.groupby('language')['empty'].sum().to_dict(), '| Took: %.1fs' % (time.time()-t0))\n",
        "    return sub\n",
        "\n",
        "_ = build_512only_with_fallback_def_submit()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512-only fallback (DEF knobs) saved as submission.csv and submission_512only_fallback_hi180_52_00500_ta220_62_00400_fb36_42.csv | Empties: 0 / 112 | Mean len: 10.4 | Per-lang empties: {'hindi': 0, 'tamil': 0} | Took: 3.0s\n"
          ]
        }
      ]
    },
    {
      "id": "0696f347-1fda-46ca-9f8a-67cfebfe9d4a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Hybrid-Lang v2: Hindi = 3-stream DEF, Tamil = 512-only fallback with n_best bump\n",
        "import pandas as pd, os\n",
        "test_df = pd.read_csv('test.csv')\n",
        "lang = dict(zip(test_df['id'].astype(str), test_df['language'].astype(str)))\n",
        "hi_fp = 'submission_spanlvl_def_weights_defknobs.csv'\n",
        "ta_fp = 'submission_512only_fallback_nb_bump_hi200_ta240.csv'\n",
        "assert os.path.exists(hi_fp), f'Missing Hindi source: {hi_fp}'\n",
        "assert os.path.exists(ta_fp), f'Missing Tamil source: {ta_fp}'\n",
        "hi = pd.read_csv(hi_fp)\n",
        "ta = pd.read_csv(ta_fp)\n",
        "hi_map = dict(zip(hi['id'].astype(str), hi['PredictionString'].astype(str).fillna('')))\n",
        "ta_map = dict(zip(ta['id'].astype(str), ta['PredictionString'].astype(str).fillna('')))\n",
        "out_ids, out_pred = [], []\n",
        "for _id in test_df['id'].astype(str).values:\n",
        "    l = str(lang.get(_id,'')).lower()\n",
        "    out_ids.append(_id)\n",
        "    out_pred.append(hi_map.get(_id,'')) if l=='hindi' else out_pred.append(ta_map.get(_id,''))\n",
        "sub = pd.DataFrame({'id': out_ids, 'PredictionString': out_pred})\n",
        "sub.to_csv('submission_hybrid_v2.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Hybrid v2 built and activated as submission.csv; backup saved as submission_hybrid_v2.csv')\n",
        "print(sub.head())"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hybrid v2 built and activated as submission.csv; backup saved as submission_hybrid_v2.csv\n          id   PredictionString\n0  be799d365  \u092e\u0941\u0902\u092c\u0908, \u092e\u0939\u093e\u0930\u093e\u0937\u094d\u091f\u094d\u0930\n1  26f356026        \u0938\u0941\u0927\u093e\u0902\u0936\u0941\u092c\u093e\u0932\u093e\n2  57a56c43f   \u0baa\u0bc6\u0bb0\u0bc1\u0bae\u0bc2\u0bb3\u0bc8\u0baa\u0bcd \u0baa\u0bc1\u0bb1\u0ba3\u0bbf\n3  da062fdbb           \u092c\u093f\u0902\u092c\u093f\u0938\u093e\u0930\n4  72fc0d5b5               1889\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}