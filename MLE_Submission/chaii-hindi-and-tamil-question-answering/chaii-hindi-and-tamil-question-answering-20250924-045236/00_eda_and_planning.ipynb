{
  "cells": [
    {
      "id": "afba1677-4d14-45c4-a470-0fd5044d0838",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan: chaii - Hindi and Tamil QA (MLE-Benchmark)\n",
        "\n",
        "Objectives:\n",
        "- Establish GPU availability and environment sanity.\n",
        "- Inspect data schema and target (answer_start).\n",
        "- Build a fast, reproducible baseline with proper CV mirroring test.\n",
        "- Iterate to a medal via improved models (mBERT/XLM-R), span extraction heuristics, ensembling.\n",
        "\n",
        "Validation:\n",
        "- Stratify by language if available; otherwise group by article/context id to avoid leakage (same context in multiple rows).\n",
        "- Use KFold with deterministic seed; save folds for reuse.\n",
        "\n",
        "Modeling roadmap:\n",
        "1) Baseline heuristic: character-based match of answer_text in context to verify target and compute a sanity Jaccard.\n",
        "2) Transformer QA head (start/end token classification) using multilingual base (xlm-roberta-base \u2192 large), GPU-required.\n",
        "3) OOF-based error analysis; adjust preprocessing, improve max_length/stride, post-processing.\n",
        "4) Seed averaging and model ensembling.\n",
        "\n",
        "Deliverables:\n",
        "- Reliable CV (OOF Jaccard) and submission.csv.\n",
        "- Logs with timing per fold; cached OOF/test predictions.\n",
        "\n",
        "Next:\n",
        "1) Check GPU\n",
        "2) Load and profile data\n",
        "3) Define CV splits and quick baseline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "d38ce418-cd64-4cb1-a5da-73b1a5a6cf60",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment check: GPU availability\n",
        "import subprocess, sys, time, os\n",
        "print(\"Running nvidia-smi...\", flush=True)\n",
        "subprocess.run(['bash','-lc','nvidia-smi || true'], check=False)\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get('CUDA_VISIBLE_DEVICES'))\n",
        "print(\"Done GPU sanity.\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running nvidia-smi...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Sep 25 03:16:33 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.158.01             Driver Version: 580.65.06      CUDA Version: 13.0     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla V100-SXM2-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   61C    P0            276W /  300W |    8700MiB /  16384MiB |     96%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\nPython: 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]\nCUDA_VISIBLE_DEVICES: \nDone GPU sanity.\n"
          ]
        }
      ]
    },
    {
      "id": "7511e7ab-13ba-4fd8-a5df-b15cf8cecfb8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# EDA: load data, inspect schema, verify labels and submission format\n",
        "import pandas as pd, unicodedata, re, hashlib, time, os\n",
        "t0=time.time()\n",
        "train=pd.read_csv('train.csv')\n",
        "test=pd.read_csv('test.csv')\n",
        "print('Train shape:', train.shape); print('Test shape:', test.shape)\n",
        "print('Train columns:', list(train.columns))\n",
        "print('Test columns:', list(test.columns))\n",
        "print('\\nHead train:')\n",
        "print(train.head(3))\n",
        "print('\\nHead test:')\n",
        "print(test.head(3))\n",
        "\n",
        "# Check sample submission format\n",
        "sub=pd.read_csv('sample_submission.csv')\n",
        "print('\\nSample submission columns:', list(sub.columns))\n",
        "print(sub.head())\n",
        "\n",
        "# Normalization helpers for label verification\n",
        "ZW_CHARS = ''.join([chr(c) for c in [0x200B,0x200C,0x200D,0xFEFF]])\n",
        "ZW_RE = re.compile(f\"[{re.escape(ZW_CHARS)}]\")\n",
        "WS_RE = re.compile(r\"\\s+\")\n",
        "def normalize_text(s:str)->str:\n",
        "    if not isinstance(s,str): return ''\n",
        "    s = unicodedata.normalize('NFKC', s)\n",
        "    s = ZW_RE.sub('', s)\n",
        "    s = WS_RE.sub(' ', s).strip()\n",
        "    return s\n",
        "\n",
        "# Verify answer alignment if columns exist\n",
        "align_checks = {'total':0,'ok':0,'mismatch':0,'nan_els':0}\n",
        "sample_rows = min(2000, len(train))\n",
        "cols = set(train.columns.str.lower())\n",
        "has_answer_start = 'answer_start' in cols\n",
        "has_answer_text = 'answer_text' in cols\n",
        "print(f'Has answer_start: {has_answer_start}, has answer_text: {has_answer_text}')\n",
        "if has_answer_start and has_answer_text:\n",
        "    # map actual column names (case-insensitive)\n",
        "    def col(name):\n",
        "        for c in train.columns:\n",
        "            if c.lower()==name: return c\n",
        "        return name\n",
        "    c_context = col('context')\n",
        "    c_answer_text = col('answer_text')\n",
        "    c_answer_start = col('answer_start')\n",
        "    mism_examples = []\n",
        "    for i,(ctx,ans,st) in enumerate(zip(train[c_context].astype(str), train[c_answer_text].astype(str), train[c_answer_start])):\n",
        "        if i>=sample_rows: break\n",
        "        align_checks['total']+=1\n",
        "        if pd.isna(st):\n",
        "            align_checks['nan_els']+=1\n",
        "            continue\n",
        "        st = int(st)\n",
        "        slice_txt = ctx[st:st+len(ans)] if 0<=st<len(ctx) else ''\n",
        "        if slice_txt==ans:\n",
        "            align_checks['ok']+=1\n",
        "            continue\n",
        "        # try normalized comparison\n",
        "        if normalize_text(slice_txt)==normalize_text(ans):\n",
        "            align_checks['ok']+=1\n",
        "        else:\n",
        "            align_checks['mismatch']+=1\n",
        "            if len(mism_examples)<5:\n",
        "                mism_examples.append({'i':i,'slice':slice_txt,'ans':ans})\n",
        "    print('Align checks on first', sample_rows, 'rows:', align_checks)\n",
        "    if mism_examples:\n",
        "        print('Examples of mismatches (up to 5):')\n",
        "        for ex in mism_examples:\n",
        "            print(ex)\n",
        "\n",
        "# Propose grouping id by normalized context hash\n",
        "ctx_col = None\n",
        "for c in train.columns:\n",
        "    if c.lower()=='context': ctx_col=c; break\n",
        "if ctx_col is not None:\n",
        "    norm_ctx = train[ctx_col].astype(str).map(lambda x: normalize_text(x))\n",
        "    grp = norm_ctx.map(lambda x: hashlib.md5(x.encode('utf-8')).hexdigest())\n",
        "    uniq = grp.nunique()\n",
        "    print('Proposed group ids (normalized context hash) unique:', uniq, 'over', len(train))\n",
        "print('EDA done in %.2fs' % (time.time()-t0))\n",
        "\n",
        "# Notes printed; next: build CV splits and metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "d691c167-5a86-47c6-9611-4580e1ecc996",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build CV splits and metric; save folds\n",
        "import numpy as np, pandas as pd, hashlib, unicodedata, re, time\n",
        "from sklearn.model_selection import StratifiedGroupKFold, GroupKFold\n",
        "\n",
        "def remove_zw(s: str) -> str:\n",
        "    if not isinstance(s, str): return ''\n",
        "    return re.sub(r\"[\\u200B\\u200C\\u200D\\uFEFF]\", \"\", s)\n",
        "\n",
        "def norm_for_metric(s: str) -> str:\n",
        "    if not isinstance(s, str): return ''\n",
        "    s = unicodedata.normalize('NFKC', s)\n",
        "    s = remove_zw(s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def word_jaccard(a: str, b: str) -> float:\n",
        "    a = norm_for_metric(a); b = norm_for_metric(b)\n",
        "    sa = set(a.split())\n",
        "    sb = set(b.split())\n",
        "    if not sa and not sb: return 1.0\n",
        "    if not sa or not sb: return 0.0\n",
        "    inter = len(sa & sb)\n",
        "    union = len(sa | sb)\n",
        "    return inter / union if union else 0.0\n",
        "\n",
        "# Prepare grouping by normalized context\n",
        "ctx_col = 'context'; lang_col = 'language'\n",
        "norm_ctx = train[ctx_col].astype(str).map(lambda x: norm_for_metric(x))\n",
        "groups = norm_ctx.map(lambda x: hashlib.md5(x.encode('utf-8')).hexdigest())\n",
        "y_len = train['answer_text'].astype(str).map(lambda x: len(x))  # proxy to avoid constant y\n",
        "\n",
        "n_splits = 5\n",
        "if lang_col in train.columns:\n",
        "    cv = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    split_iter = cv.split(train, train[lang_col], groups)\n",
        "else:\n",
        "    cv = GroupKFold(n_splits=n_splits)\n",
        "    split_iter = cv.split(train, y_len, groups)\n",
        "\n",
        "folds = np.full(len(train), -1, dtype=int)\n",
        "for fold, (trn_idx, val_idx) in enumerate(split_iter):\n",
        "    folds[val_idx] = fold\n",
        "assert (folds>=0).all(), 'Some folds not assigned'\n",
        "train['fold'] = folds\n",
        "train.to_csv('train_folds.csv', index=False)\n",
        "print('Saved train_folds.csv with fold distribution:')\n",
        "print(train['fold'].value_counts().sort_index())\n",
        "\n",
        "# Quick metric sanity: OOF using gold answers should be 1.0 on average\n",
        "oof_j = []\n",
        "for f in range(n_splits):\n",
        "    val = train[train['fold']==f]\n",
        "    j = val.apply(lambda r: word_jaccard(r['answer_text'], r['answer_text']), axis=1).mean()\n",
        "    oof_j.append(j)\n",
        "print('Sanity OOF word-jaccard (gold vs gold) per fold:', [round(x,4) for x in oof_j], 'mean=', round(float(np.mean(oof_j)),4))\n",
        "\n",
        "print('CV setup complete. Next: implement HF QA dataset + training loop.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "ad75926b-e00f-4b19-9c46-8b4c76cf1974",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install PyTorch (cu121) and HF stack; verify GPU\n",
        "import subprocess, sys, os, shutil, time\n",
        "from pathlib import Path\n",
        "\n",
        "def pip(*args):\n",
        "    print('> pip', *args, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "# Uninstall any preexisting torch stack\n",
        "for pkg in ('torch','torchvision','torchaudio'):\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', pkg], check=False)\n",
        "\n",
        "# Clean stray site dirs (idempotent)\n",
        "for d in (\n",
        "    '/app/.pip-target/torch',\n",
        "    '/app/.pip-target/torchvision',\n",
        "    '/app/.pip-target/torchaudio',\n",
        "    '/app/.pip-target/torch-2.8.0.dist-info',\n",
        "    '/app/.pip-target/torch-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torchvision-0.23.0.dist-info',\n",
        "    '/app/.pip-target/torchvision-0.19.1.dist-info',\n",
        "    '/app/.pip-target/torchaudio-2.8.0.dist-info',\n",
        "    '/app/.pip-target/torchaudio-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torchgen',\n",
        "    '/app/.pip-target/functorch',\n",
        "):\n",
        "    if os.path.exists(d):\n",
        "        print('Removing', d); shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "print('Installing torch cu121 stack...', flush=True)\n",
        "pip('install',\n",
        "    '--index-url', 'https://download.pytorch.org/whl/cu121',\n",
        "    '--extra-index-url', 'https://pypi.org/simple',\n",
        "    'torch==2.4.1', 'torchvision==0.19.1', 'torchaudio==2.4.1')\n",
        "\n",
        "Path('constraints.txt').write_text('torch==2.4.1\\ntorchvision==0.19.1\\ntorchaudio==2.4.1\\n')\n",
        "\n",
        "print('Installing transformers/datasets/accelerate...', flush=True)\n",
        "pip('install', '-c', 'constraints.txt',\n",
        "    'transformers==4.44.2', 'accelerate==0.34.2',\n",
        "    'datasets==2.21.0', 'evaluate==0.4.2',\n",
        "    'sentencepiece', 'scikit-learn', 'numpy')\n",
        "\n",
        "import torch\n",
        "print('torch:', torch.__version__, 'built CUDA:', getattr(torch.version, 'cuda', None))\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "assert str(getattr(torch.version,'cuda','')).startswith('12.1'), f'Wrong CUDA build: {torch.version.cuda}'\n",
        "assert torch.cuda.is_available(), 'CUDA not available after install'\n",
        "print('GPU:', torch.cuda.get_device_name(0))\n",
        "print('Setup complete.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "4a716274-2433-416c-a7a1-23287d158a45",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# HF QA pipeline: tokenizer and feature preparation (no training yet)\n",
        "import pandas as pd, numpy as np, math, time, unicodedata, re, hashlib\n",
        "import torch\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "def remove_zw(s: str) -> str:\n",
        "    if not isinstance(s, str): return ''\n",
        "    return re.sub(r\"[\\u200B\\u200C\\u200D\\uFEFF]\", \"\", s)\n",
        "\n",
        "def norm_text(s: str) -> str:\n",
        "    if not isinstance(s, str): return ''\n",
        "    s = unicodedata.normalize('NFKC', s)\n",
        "    s = remove_zw(s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "model_name = 'microsoft/mdeberta-v3-base'\n",
        "max_length = 384\n",
        "doc_stride = 128\n",
        "print('Loading tokenizer:', model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "# Build features with overflow and offsets; map gold start/end to token indices\n",
        "def prepare_train_features(df: pd.DataFrame):\n",
        "    questions = df['question'].astype(str).tolist()\n",
        "    contexts = df['context'].astype(str).tolist()\n",
        "    answers = df['answer_text'].astype(str).tolist()\n",
        "    starts = df['answer_start'].astype(int).tolist()\n",
        "    tokenized = tokenizer(questions, contexts,\n",
        "                          truncation='only_second',\n",
        "                          max_length=max_length,\n",
        "                          stride=doc_stride,\n",
        "                          return_overflowing_tokens=True,\n",
        "                          return_offsets_mapping=True,\n",
        "                          padding=False)\n",
        "    sample_mapping = tokenized.pop('overflow_to_sample_mapping')\n",
        "    offset_mapping = tokenized['offset_mapping']\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        sample_idx = sample_mapping[i]\n",
        "        # sequence_ids marks question(0)/context(1)/special(None)\n",
        "        sequence_ids = tokenized.sequence_ids(i)\n",
        "        # Gold answer char positions\n",
        "        start_char = starts[sample_idx]\n",
        "        answer_text = answers[sample_idx]\n",
        "        end_char = start_char + len(answer_text)\n",
        "        # Find context token span indices\n",
        "        idx = 0\n",
        "        while idx < len(sequence_ids) and sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        idx = len(sequence_ids) - 1\n",
        "        while idx >= 0 and sequence_ids[idx] != 1:\n",
        "            idx -= 1\n",
        "        context_end = idx\n",
        "        if context_start > context_end:\n",
        "            start_positions.append(0); end_positions.append(0); continue\n",
        "        # If the answer is not fully inside this span, mark CLS\n",
        "        if not (offsets[context_start][0] <= start_char and offsets[context_end][1] >= end_char):\n",
        "            start_positions.append(0); end_positions.append(0);\n",
        "            continue\n",
        "        # Otherwise, find start token index\n",
        "        start_token = context_start\n",
        "        while start_token <= context_end and offsets[start_token][0] <= start_char:\n",
        "            start_token += 1\n",
        "        start_token -= 1\n",
        "        end_token = context_end\n",
        "        while end_token >= context_start and offsets[end_token][1] >= end_char:\n",
        "            end_token -= 1\n",
        "        end_token += 1\n",
        "        start_positions.append(start_token)\n",
        "        end_positions.append(end_token)\n",
        "    tokenized['start_positions'] = start_positions\n",
        "    tokenized['end_positions'] = end_positions\n",
        "    return tokenized\n",
        "\n",
        "def prepare_validation_features(df: pd.DataFrame):\n",
        "    questions = df['question'].astype(str).tolist()\n",
        "    contexts = df['context'].astype(str).tolist()\n",
        "    tokenized = tokenizer(questions, contexts,\n",
        "                          truncation='only_second',\n",
        "                          max_length=max_length,\n",
        "                          stride=doc_stride,\n",
        "                          return_overflowing_tokens=True,\n",
        "                          return_offsets_mapping=True,\n",
        "                          padding=False)\n",
        "    return tokenized\n",
        "\n",
        "# Smoke-build features for one fold to validate pipeline speed and shapes\n",
        "fold = 0\n",
        "df_tr = pd.read_csv('train_folds.csv')\n",
        "trn_df = df_tr[df_tr['fold']!=fold].reset_index(drop=True)\n",
        "val_df = df_tr[df_tr['fold']==fold].reset_index(drop=True)\n",
        "t0=time.time()\n",
        "trn_feats = prepare_train_features(trn_df.head(512))  # subsample for quick check\n",
        "val_feats = prepare_validation_features(val_df.head(128))\n",
        "print('Train features keys:', list(trn_feats.keys()))\n",
        "print('Num train features (overflowed examples):', len(trn_feats['input_ids']))\n",
        "print('Num val features (overflowed examples):', len(val_feats['input_ids']))\n",
        "print('First train feature lens:', len(trn_feats['input_ids'][0]))\n",
        "print('Prep time: %.2fs' % (time.time()-t0))\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if torch.cuda.is_available() else None)\n",
        "print('Tokenizer and feature pre-processing ready. Next: implement training loop per fold with logging.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "e252598e-0fff-4a0c-afc1-681bdd937883",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train baseline QA (fold 0) with mdeberta-v3-base and evaluate OOF Jaccard\n",
        "import time, math, numpy as np, pandas as pd, torch, os\n",
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed); np.random.seed(seed)\n",
        "\n",
        "max_answer_len = 30\n",
        "n_best_size = 20\n",
        "\n",
        "class QADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, features: dict, with_labels: bool=True):\n",
        "        self.features = features\n",
        "        self.with_labels = with_labels\n",
        "    def __len__(self):\n",
        "        return len(self.features['input_ids'])\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            'input_ids': torch.tensor(self.features['input_ids'][idx], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.features['attention_mask'][idx], dtype=torch.long),\n",
        "        }\n",
        "        if 'token_type_ids' in self.features:\n",
        "            item['token_type_ids'] = torch.tensor(self.features['token_type_ids'][idx], dtype=torch.long)\n",
        "        if self.with_labels:\n",
        "            item['start_positions'] = torch.tensor(self.features['start_positions'][idx], dtype=torch.long)\n",
        "            item['end_positions'] = torch.tensor(self.features['end_positions'][idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "def postprocess_predictions(features, examples_df, start_logits, end_logits, max_answer_len=30, n_best_size=20):\n",
        "    # Aggregate best span per example across overflowed features\n",
        "    sample_mapping = features['overflow_to_sample_mapping']\n",
        "    preds_text = [''] * len(examples_df)\n",
        "    preds_start = [0] * len(examples_df)\n",
        "    best_scores = [-1e30] * len(examples_df)\n",
        "    for i in range(len(sample_mapping)):\n",
        "        sample_idx = int(sample_mapping[i])\n",
        "        offsets = features['offset_mapping'][i]\n",
        "        seq_ids = features.sequence_ids(i)\n",
        "        # context token range\n",
        "        context_tokens = [j for j,sid in enumerate(seq_ids) if sid==1]\n",
        "        if not context_tokens:\n",
        "            continue\n",
        "        c_start, c_end = context_tokens[0], context_tokens[-1]\n",
        "        s_logits = start_logits[i]\n",
        "        e_logits = end_logits[i]\n",
        "        # consider top start/end within context\n",
        "        start_indexes = np.argsort(s_logits)[-n_best_size:][::-1]\n",
        "        end_indexes = np.argsort(e_logits)[-n_best_size:][::-1]\n",
        "        for si in start_indexes:\n",
        "            if si < c_start or si > c_end: continue\n",
        "            for ei in end_indexes:\n",
        "                if ei < c_start or ei > c_end: continue\n",
        "                if ei < si: continue\n",
        "                length = offsets[ei][1] - offsets[si][0]\n",
        "                if length <= 0 or (ei - si + 1) > 512: continue\n",
        "                if (offsets[ei][1] - offsets[si][0]) > max_answer_len*10:\n",
        "                    # approx char length constraint\n",
        "                    continue\n",
        "                score = s_logits[si] + e_logits[ei]\n",
        "                if score > best_scores[sample_idx]:\n",
        "                    best_scores[sample_idx] = score\n",
        "                    start_char = offsets[si][0]\n",
        "                    end_char = offsets[ei][1]\n",
        "                    ctx = examples_df.loc[sample_idx, 'context']\n",
        "                    text = ctx[start_char:end_char].strip()\n",
        "                    preds_text[sample_idx] = text\n",
        "                    preds_start[sample_idx] = start_char\n",
        "    # fallback empty to first 0\n",
        "    for i in range(len(preds_text)):\n",
        "        if preds_text[i] == '':\n",
        "            preds_text[i] = examples_df.loc[i, 'context'][:0]\n",
        "            preds_start[i] = 0\n",
        "    return preds_text, preds_start\n",
        "\n",
        "def word_jaccard(a: str, b: str) -> float:\n",
        "    import unicodedata, re\n",
        "    def norm(s):\n",
        "        if not isinstance(s,str): return ''\n",
        "        s = unicodedata.normalize('NFKC', s)\n",
        "        s = re.sub(r\"[\\u200B\\u200C\\u200D\\uFEFF]\", \"\", s)\n",
        "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "        return s\n",
        "    sa = set(norm(a).split()); sb = set(norm(b).split())\n",
        "    if not sa and not sb: return 1.0\n",
        "    if not sa or not sb: return 0.0\n",
        "    inter = len(sa & sb); union = len(sa | sb)\n",
        "    return inter/union if union else 0.0\n",
        "\n",
        "fold = 0\n",
        "df_tr = pd.read_csv('train_folds.csv')\n",
        "trn_df = df_tr[df_tr['fold']!=fold].reset_index(drop=True)\n",
        "val_df = df_tr[df_tr['fold']==fold].reset_index(drop=True)\n",
        "\n",
        "trn_feats = prepare_train_features(trn_df)\n",
        "val_feats = prepare_validation_features(val_df)\n",
        "\n",
        "train_ds = QADataset(trn_feats, with_labels=True)\n",
        "val_ds_inputs = QADataset(val_feats, with_labels=False)\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "bsz = 16\n",
        "args = TrainingArguments(\n",
        "    output_dir=f'outputs_fold{fold}',\n",
        "    per_device_train_batch_size=bsz,\n",
        "    per_device_eval_batch_size=32,\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    save_steps=5000,\n",
        "    evaluation_strategy='no',\n",
        "    seed=seed,\n",
        "    report_to=[]\n",
        ")\n",
        "\n",
        "# Use padding collator to handle variable-length sequences\n",
        "pad_to_mult = 8 if torch.cuda.is_available() else None\n",
        "data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=pad_to_mult)\n",
        "trainer = Trainer(model=model, args=args, train_dataset=train_ds, data_collator=data_collator)\n",
        "\n",
        "print('Starting training fold', fold)\n",
        "t0=time.time()\n",
        "trainer.train()\n",
        "print('Training done in %.2fs' % (time.time()-t0))\n",
        "\n",
        "# Predict on val features\n",
        "model.eval()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds_inputs, batch_size=32, shuffle=False, collate_fn=data_collator)\n",
        "all_start, all_end = [], []\n",
        "with torch.no_grad():\n",
        "    t1=time.time();\n",
        "    for step, batch in enumerate(val_loader):\n",
        "        for k in list(batch.keys()):\n",
        "            batch[k] = batch[k].to(device)\n",
        "        out = model(**batch)\n",
        "        all_start.append(out.start_logits.detach().cpu().numpy())\n",
        "        all_end.append(out.end_logits.detach().cpu().numpy())\n",
        "        if step % 20 == 0:\n",
        "            print(f'Infer step {step}, elapsed {time.time()-t1:.1f}s', flush=True)\n",
        "start_logits = np.concatenate(all_start, axis=0)\n",
        "end_logits = np.concatenate(all_end, axis=0)\n",
        "print('Val features:', start_logits.shape[0])\n",
        "\n",
        "# Post-process to text and start index\n",
        "pred_texts, pred_starts = postprocess_predictions(val_feats, val_df, start_logits, end_logits, max_answer_len=max_answer_len, n_best_size=n_best_size)\n",
        "val_df['pred_text'] = pred_texts\n",
        "val_df['pred_start'] = pred_starts\n",
        "\n",
        "val_df['jaccard'] = [word_jaccard(a, b) for a,b in zip(val_df['answer_text'].astype(str), val_df['pred_text'].astype(str))]\n",
        "print('Fold', fold, 'OOF Jaccard:', round(float(val_df['jaccard'].mean()), 5))\n",
        "val_df.to_csv(f'oof_fold{fold}.csv', index=False)\n",
        "print('Saved oof to', f'oof_fold{fold}.csv')\n",
        "print('Baseline fold0 complete. Next: expand to all folds + full inference & submission.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "bdca734e-f625-436c-a1f1-05f03b30c2b7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Upgrade to deepset xlm-roberta-large-squad2: 5-fold training with robust post-processing and OOF Jaccard\n",
        "import os\n",
        "os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')\n",
        "import sys\n",
        "# Prefer bitsandbytes if already installed; otherwise fall back to Adafactor (no inline installs to avoid torch drift)\n",
        "try:\n",
        "    import bitsandbytes as bnb  # noqa: F401\n",
        "    HAS_BNB = True\n",
        "    print('bitsandbytes available: using adamw_bnb_8bit optimizer')\n",
        "except Exception:\n",
        "    HAS_BNB = False\n",
        "    print('bitsandbytes not available: falling back to Adafactor optimizer')\n",
        "\n",
        "import pandas as pd, numpy as np, time, math, re, unicodedata, torch, glob, gc\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "\n",
        "# Precision and CUDA housekeeping\n",
        "torch.cuda.empty_cache(); gc.collect()\n",
        "torch.set_float32_matmul_precision('medium')\n",
        "try:\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ===== Fast normalization helpers (precompute tables & regexes) =====\n",
        "DEV_MAP = {ord(x): ord('0')+i for i,x in enumerate('\\u0966\\u0967\\u0968\\u0969\\u096a\\u096b\\u096c\\u096d\\u096e\\u096f')}\n",
        "TAM_MAP = {ord(x): ord('0')+i for i,x in enumerate('\\u0be6\\u0be7\\u0be8\\u0be9\\u0bea\\u0beb\\u0bec\\u0bed\\u0bee\\u0bef')}\n",
        "TRANS_TABLE = DEV_MAP | TAM_MAP\n",
        "ZW_RE = re.compile(r\"[\\u200B\\u200C\\u200D\\uFEFF]\")\n",
        "WS_RE = re.compile(r\"\\s+\")\n",
        "\n",
        "def norm_for_metric(s: str) -> str:\n",
        "    if not isinstance(s, str): return ''\n",
        "    s = s.translate(TRANS_TABLE)\n",
        "    s = unicodedata.normalize('NFKC', s)\n",
        "    s = ZW_RE.sub('', s)\n",
        "    s = WS_RE.sub(' ', s).strip()\n",
        "    return s\n",
        "\n",
        "PUNCT_STRIP = ''.join([\n",
        "    '.,:;!?\\'\\\"()[]{}',\n",
        "    '\\u2013\\u2014\\u2026\\u00AB\\u00BB\\u201C\\u201D\\u2018\\u2019',\n",
        "    '\\u0964\\u0965',  # danda, double danda\n",
        "    '\\u060C\\u061B',  # Arabic comma, semicolon\n",
        "    '\\uFF0C\\u3001\\uFF0E\\uFF1A\\uFF1B\\uFF01\\uFF1F\\uFF08\\uFF09\\uFF3B\\uFF3D\\uFF5B\\uFF5D',  # fullwidth/CJK\n",
        "])\n",
        "PUNCT_RE = re.compile(f\"^[{re.escape(PUNCT_STRIP)}\\s]+|[{re.escape(PUNCT_STRIP)}\\s]+$\")\n",
        "def edge_trim(text: str) -> str:\n",
        "    if not isinstance(text, str): return ''\n",
        "    return PUNCT_RE.sub('', text)\n",
        "\n",
        "def word_jaccard(a: str, b: str) -> float:\n",
        "    sa = set(norm_for_metric(a).split()); sb = set(norm_for_metric(b).split())\n",
        "    if not sa and not sb: return 1.0\n",
        "    if not sa or not sb: return 0.0\n",
        "    inter = len(sa & sb); union = len(sa | sb)\n",
        "    return inter/union if union else 0.0\n",
        "\n",
        "# Model and lengths\n",
        "xlmr_model = 'deepset/xlm-roberta-base-squad2'\n",
        "max_length = 384  # per expert: 384/128 sweet spot\n",
        "doc_stride = 128\n",
        "epochs = 4  # for full training later; smoke will override\n",
        "bsz = 4\n",
        "grad_accum = 4\n",
        "lr = 2e-5  # per expert\n",
        "warmup_ratio = 0.10\n",
        "max_answer_len = 50\n",
        "n_best_size = 50\n",
        "\n",
        "print('Loading tokenizer:', xlmr_model, flush=True)\n",
        "tokenizer_x = AutoTokenizer.from_pretrained(xlmr_model, use_fast=True)\n",
        "tokenizer_x.padding_side = 'right'\n",
        "\n",
        "# ===== New alignment helpers (trim boundaries, strict token snapping) =====\n",
        "ZW_SET = {'\\u200B', '\\u200C', '\\u200D', '\\uFEFF'}  # ZWSP, ZWNJ, ZWJ, BOM/ZWNBSP\n",
        "\n",
        "def _is_ws_or_punct(ch: str) -> bool:\n",
        "    if not ch: return False\n",
        "    if ch in ZW_SET: return True\n",
        "    if ch.isspace(): return True\n",
        "    cat = unicodedata.category(ch)\n",
        "    return cat and cat[0] == 'P'\n",
        "\n",
        "def _is_combining(ch: str) -> bool:\n",
        "    # virama, nukta, vowel signs etc. Category Mn\n",
        "    return unicodedata.category(ch) == 'Mn'\n",
        "\n",
        "def _trim_bounds(ctx: str, s: int, e: int) -> tuple[int,int]:\n",
        "    # advance s over ws/punct/zero-width (but never over a combining mark)\n",
        "    while s < e and s < len(ctx) and _is_ws_or_punct(ctx[s]) and not _is_combining(ctx[s]):\n",
        "        s += 1\n",
        "    # retreat e over ws/punct/zero-width (but never over a combining mark)\n",
        "    while e > s and e-1 < len(ctx) and _is_ws_or_punct(ctx[e-1]) and not _is_combining(ctx[e-1]):\n",
        "        e -= 1\n",
        "    return s, e\n",
        "\n",
        "def prepare_train_features_x(df: pd.DataFrame):\n",
        "    # Normalized target strings (do not mutate context)\n",
        "    questions = df['question'].astype(str).tolist()\n",
        "    contexts = df['context'].astype(str).tolist()\n",
        "    answers = df['answer_text'].astype(str).tolist()\n",
        "    gold_norms = [norm_for_metric(a) for a in answers]\n",
        "    gold_norms_trim = [edge_trim(x) for x in gold_norms]\n",
        "\n",
        "    tok = tokenizer_x(\n",
        "        questions, contexts,\n",
        "        truncation='only_second', max_length=max_length, stride=doc_stride,\n",
        "        return_overflowing_tokens=True, return_offsets_mapping=True, padding=False\n",
        "    )\n",
        "    sample_map = tok.pop('overflow_to_sample_mapping')\n",
        "    offsets_list = tok['offset_mapping']\n",
        "\n",
        "    start_positions, end_positions = [], []\n",
        "\n",
        "    for i, offsets in enumerate(offsets_list):\n",
        "        ex = int(sample_map[i])\n",
        "        seq_ids = tok.sequence_ids(i)\n",
        "        # context token range\n",
        "        ctx_tokens = [j for j, sid in enumerate(seq_ids) if sid == 1]\n",
        "        if not ctx_tokens:\n",
        "            start_positions.append(0); end_positions.append(0); continue\n",
        "        c0, c1 = ctx_tokens[0], ctx_tokens[-1]\n",
        "\n",
        "        ctx = contexts[ex]\n",
        "        gold = answers[ex]\n",
        "        gold_n = gold_norms[ex]\n",
        "        gold_nt = gold_norms_trim[ex]\n",
        "        if gold_n == '':\n",
        "            start_positions.append(0); end_positions.append(0); continue\n",
        "\n",
        "        # Scan token spans (limit to 35 tokens) to find a normalized exact match to gold\n",
        "        best = None  # (len_tokens, si, ei)\n",
        "        for si in range(c0, c1+1):\n",
        "            sj, ej = offsets[si]\n",
        "            if sj is None or ej is None: continue\n",
        "            for ei in range(si, min(c1, si+34)+1):\n",
        "                s2, e2 = offsets[ei]\n",
        "                if s2 is None or e2 is None: continue\n",
        "                if e2 <= sj: continue\n",
        "                span_text = ctx[sj:e2]\n",
        "                # quick exact/edge-trim equality checks before normalization\n",
        "                span_edge = edge_trim(span_text.strip())\n",
        "                if span_text == gold or span_edge == gold:\n",
        "                    cand_len = ei - si + 1\n",
        "                    if best is None or cand_len < best[0] or (cand_len == best[0] and (e2 - sj) < (offsets[best[2]][1] - offsets[best[1]][0])):\n",
        "                        best = (cand_len, si, ei)\n",
        "                        continue\n",
        "                cand_n = norm_for_metric(span_text)\n",
        "                if cand_n == gold_n or edge_trim(cand_n) == gold_n or cand_n == gold_nt:\n",
        "                    cand_len = ei - si + 1\n",
        "                    if best is None or cand_len < best[0] or (best is not None and cand_len == best[0] and (e2 - sj) < (offsets[best[2]][1] - offsets[best[1]][0])):\n",
        "                        best = (cand_len, si, ei)\n",
        "        if best is None:\n",
        "            # no exact-normalized match within this feature\n",
        "            start_positions.append(0); end_positions.append(0); continue\n",
        "        _, si, ei = best\n",
        "\n",
        "        # Final boundary tightening: trim ws/punct at char-level, then snap to strict token boundaries inside [si,ei]\n",
        "        s_char = offsets[si][0]; e_char = offsets[ei][1]\n",
        "        if s_char is None or e_char is None or e_char <= s_char:\n",
        "            start_positions.append(0); end_positions.append(0); continue\n",
        "        s_adj, e_adj = _trim_bounds(ctx, s_char, e_char)\n",
        "        if s_adj < e_adj:\n",
        "            # strict start: first token with token_start >= s_adj\n",
        "            si2 = None\n",
        "            for j in range(si, ei+1):\n",
        "                sj, ej = offsets[j]\n",
        "                if sj is None or ej is None: continue\n",
        "                if sj >= s_adj:\n",
        "                    si2 = j; break\n",
        "            # strict end: last token with token_end <= e_adj\n",
        "            ei2 = None\n",
        "            for j in range(ei, si-1, -1):\n",
        "                sj, ej = offsets[j]\n",
        "                if sj is None or ej is None: continue\n",
        "                if ej <= e_adj:\n",
        "                    ei2 = j; break\n",
        "            if si2 is not None and ei2 is not None and si2 <= ei2:\n",
        "                si, ei = si2, ei2\n",
        "\n",
        "        start_positions.append(int(si))\n",
        "        end_positions.append(int(ei))\n",
        "\n",
        "    tok['start_positions'] = start_positions\n",
        "    tok['end_positions'] = end_positions\n",
        "    return tok\n",
        "\n",
        "def prepare_features_only_x(df: pd.DataFrame):\n",
        "    return tokenizer_x(df['question'].astype(str).tolist(), df['context'].astype(str).tolist(),\n",
        "                       truncation='only_second', max_length=max_length, stride=doc_stride,\n",
        "                       return_overflowing_tokens=True, return_offsets_mapping=True, padding=False)\n",
        "\n",
        "class QADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, feats: dict, with_labels: bool):\n",
        "        self.f = feats; self.with_labels = with_labels\n",
        "    def __len__(self): return len(self.f['input_ids'])\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            'input_ids': torch.tensor(self.f['input_ids'][idx], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.f['attention_mask'][idx], dtype=torch.long),\n",
        "        }\n",
        "        if 'token_type_ids' in self.f:\n",
        "            item['token_type_ids'] = torch.tensor(self.f['token_type_ids'][idx], dtype=torch.long)\n",
        "        if self.with_labels:\n",
        "            item['start_positions'] = torch.tensor(self.f['start_positions'][idx], dtype=torch.long)\n",
        "            item['end_positions'] = torch.tensor(self.f['end_positions'][idx], dtype=torch.long)\n",
        "        # carry feature index for robust alignment in validation\n",
        "        item['feat_idx'] = torch.tensor(idx, dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "def _log_softmax_masked(x: np.ndarray, c0: int, c1: int) -> np.ndarray:\n",
        "    # Mask outside [c0,c1] to very negative, then compute log-softmax\n",
        "    m = np.full_like(x, -1e9, dtype=np.float32)\n",
        "    m[c0:c1+1] = x[c0:c1+1]\n",
        "    mx = np.max(m)\n",
        "    y = m - mx\n",
        "    expy = np.exp(y)\n",
        "    z = np.sum(expy)\n",
        "    return y - np.log(z + 1e-12)\n",
        "\n",
        "DIGIT_PAT = re.compile(r\"[0-9\\u0966-\\u096f\\u0be6-\\u0bef]\")\n",
        "def _is_punct(ch: str) -> bool:\n",
        "    return ch in PUNCT_STRIP or ch.isspace()\n",
        "\n",
        "def pool_nbest_over_features(features, examples_df, start_logits, end_logits, rerank_with_gold=False):\n",
        "    sample_mapping = features['overflow_to_sample_mapping']\n",
        "    preds_text = [''] * len(examples_df)\n",
        "    preds_start = [0] * len(examples_df)\n",
        "    best_score = [-1e30] * len(examples_df)\n",
        "    for i in range(len(sample_mapping)):\n",
        "        ex_idx = int(sample_mapping[i])\n",
        "        offsets = features['offset_mapping'][i]\n",
        "        seq_ids = features.sequence_ids(i)\n",
        "        ctx_tokens = [j for j,sid in enumerate(seq_ids) if sid==1]\n",
        "        if not ctx_tokens: continue\n",
        "        c0, c1 = ctx_tokens[0], ctx_tokens[-1]\n",
        "        s_log = start_logits[i]; e_log = end_logits[i]\n",
        "        # probability-based scoring (log-softmax over context)\n",
        "        s_lp = _log_softmax_masked(s_log, c0, c1)\n",
        "        e_lp = _log_softmax_masked(e_log, c0, c1)\n",
        "        start_idxes = np.argsort(s_lp)[-n_best_size:][::-1]\n",
        "        end_idxes = np.argsort(e_lp)[-n_best_size:][::-1]\n",
        "        cands = []\n",
        "        qtext = str(examples_df.loc[ex_idx, 'question'])\n",
        "        q_has_digit = DIGIT_PAT.search(qtext) is not None\n",
        "        ctx = examples_df.loc[ex_idx, 'context']\n",
        "        for si in start_idxes:\n",
        "            if si < c0 or si > c1: continue\n",
        "            for ei in end_idxes:\n",
        "                if ei < c0 or ei > c1 or ei < si: continue\n",
        "                if (ei - si + 1) > 40: continue  # joint token window constraint\n",
        "                stc, enc = offsets[si][0], offsets[ei][1]\n",
        "                if stc is None or enc is None or enc <= stc: continue\n",
        "                raw_span = ctx[stc:enc]\n",
        "                # punctuation penalty BEFORE trim\n",
        "                penalty = 0.0\n",
        "                if raw_span:\n",
        "                    if _is_punct(raw_span[0]): penalty -= 0.02\n",
        "                    if _is_punct(raw_span[-1]): penalty -= 0.02\n",
        "                # word-boundary bonus if aligns to boundaries\n",
        "                left_ok = (stc == 0) or _is_punct(ctx[stc-1])\n",
        "                right_ok = (enc >= len(ctx)) or _is_punct(ctx[enc:enc+1])\n",
        "                if left_ok and right_ok:\n",
        "                    penalty += 0.02  # small bonus\n",
        "                text = edge_trim(raw_span.strip())\n",
        "                if not text: continue\n",
        "                score = float(s_lp[si] + e_lp[ei]) + penalty\n",
        "                # gentle token-length penalty\n",
        "                score -= 0.002 * (ei - si + 1)\n",
        "                # optional small numeric bonus\n",
        "                if q_has_digit:\n",
        "                    cand_has_digit = DIGIT_PAT.search(text) is not None\n",
        "                    score += 0.02 if cand_has_digit else -0.02\n",
        "                cands.append((score, text, stc))\n",
        "        if not cands: continue\n",
        "        if rerank_with_gold:\n",
        "            gold = examples_df.loc[ex_idx, 'answer_text']\n",
        "            cands.sort(key=lambda x: (word_jaccard(gold, x[1]), x[0], -len(x[1])), reverse=True)\n",
        "        else:\n",
        "            cands.sort(key=lambda x: (x[0], -len(x[1])), reverse=True)\n",
        "        top = cands[0]\n",
        "        if top[0] > best_score[ex_idx]:\n",
        "            best_score[ex_idx] = top[0]\n",
        "            preds_text[ex_idx] = top[1]\n",
        "            preds_start[ex_idx] = top[2]\n",
        "    # fallback empties\n",
        "    for i in range(len(preds_text)):\n",
        "        if preds_text[i] == '':\n",
        "            preds_text[i] = edge_trim(examples_df.loc[i, 'context'][:0])\n",
        "            preds_start[i] = 0\n",
        "    return preds_text, preds_start\n",
        "\n",
        "def _try_load_fold_model(model_dir: str):\n",
        "    # Try standard load\n",
        "    try:\n",
        "        return AutoModelForQuestionAnswering.from_pretrained(model_dir, local_files_only=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "    # Fallback: instantiate base and load state dict directly if bin exists without config\n",
        "    bin_path = os.path.join(model_dir, 'pytorch_model.bin')\n",
        "    if os.path.exists(bin_path):\n",
        "        print(f'Fallback loading state_dict from {bin_path}')\n",
        "        try:\n",
        "            m = AutoModelForQuestionAnswering.from_pretrained(xlmr_model, attn_implementation='eager')\n",
        "        except Exception as e:\n",
        "            print('Load error on fallback:', e);\n",
        "            m = AutoModelForQuestionAnswering.from_pretrained(xlmr_model)\n",
        "        state = torch.load(bin_path, map_location='cpu')\n",
        "        m.load_state_dict(state, strict=True)\n",
        "        return m\n",
        "    raise FileNotFoundError(f'No loadable checkpoint in {model_dir}')\n",
        "\n",
        "def _find_checkpoint_dir(model_dir: str):\n",
        "    # If direct files exist, prefer model_dir\n",
        "    if os.path.exists(os.path.join(model_dir, 'config.json')) and (\n",
        "        os.path.exists(os.path.join(model_dir, 'pytorch_model.bin')) or os.path.exists(os.path.join(model_dir, 'model.safetensors'))):\n",
        "        return model_dir\n",
        "    # Else search for latest checkpoint-* subdir with weights\n",
        "    ckpts = sorted(glob.glob(os.path.join(model_dir, 'checkpoint-*')), key=lambda p: int(p.split('-')[-1]) if p.split('-')[-1].isdigit() else -1)\n",
        "    ckpts = [p for p in ckpts if os.path.exists(os.path.join(p,'config.json')) and (os.path.exists(os.path.join(p,'pytorch_model.bin')) or os.path.exists(os.path.join(p,'model.safetensors')))]\n",
        "    if ckpts:\n",
        "        return ckpts[-1]\n",
        "    return None\n",
        "\n",
        "def train_5fold_x():\n",
        "    df = pd.read_csv('train_folds.csv')\n",
        "    all_oof = []\n",
        "    for fold in range(5):\n",
        "        t_fold = time.time()\n",
        "        trn_df = df[df['fold']!=fold].reset_index(drop=True)\n",
        "        val_df = df[df['fold']==fold].reset_index(drop=True)\n",
        "        print(f'Fold {fold}: train {len(trn_df)} val {len(val_df)}')\n",
        "        # Build raw train features (with positives and negatives)\n",
        "        trn_feats_raw = prepare_train_features_x(trn_df)\n",
        "        # Keep only positives to remove catastrophic CLS bias\n",
        "        keep_mask = [int(sp) > 0 for sp in trn_feats_raw['start_positions']]\n",
        "        def filt(key):\n",
        "            if key not in trn_feats_raw: return None\n",
        "            vals = trn_feats_raw[key]\n",
        "            if not isinstance(vals, list): return None\n",
        "            return [v for v, k in zip(vals, keep_mask) if k]\n",
        "        trn_feats = {}\n",
        "        for key in trn_feats_raw.keys():\n",
        "            fl = filt(key)\n",
        "            if fl is not None:\n",
        "                trn_feats[key] = fl\n",
        "        # Ensure token_type_ids carried if present\n",
        "        if 'token_type_ids' in trn_feats_raw and 'token_type_ids' not in trn_feats:\n",
        "            trn_feats['token_type_ids'] = filt('token_type_ids')\n",
        "\n",
        "        val_feats = prepare_features_only_x(val_df)\n",
        "        train_ds = QADataset(trn_feats, with_labels=True)\n",
        "        val_ds = QADataset(val_feats, with_labels=False)\n",
        "        # Pre-train logging for ETA\n",
        "        num_feats = len(trn_feats['input_ids'])\n",
        "        eff_bsz = bsz * grad_accum\n",
        "        steps_per_epoch = (num_feats + eff_bsz - 1) // eff_bsz\n",
        "        print(f\"Fold {fold}: features={num_feats}, eff_bsz={eff_bsz}, steps/epoch={steps_per_epoch}, epochs={epochs}\")\n",
        "\n",
        "        model_root = f'xlmr_f{fold}'\n",
        "        ckpt_path = _find_checkpoint_dir(model_root)\n",
        "        if ckpt_path is not None:\n",
        "            print(f'Loading existing model for fold {fold} from {ckpt_path}')\n",
        "            model = _try_load_fold_model(ckpt_path)\n",
        "        else:\n",
        "            model = AutoModelForQuestionAnswering.from_pretrained(\n",
        "                xlmr_model,\n",
        "                attn_implementation='eager'\n",
        "            )\n",
        "            # Enable gradient checkpointing (helps VRAM) and disable use_cache if present\n",
        "            if hasattr(model, 'gradient_checkpointing_enable'):\n",
        "                model.gradient_checkpointing_enable()\n",
        "            if hasattr(model, 'config') and hasattr(model.config, 'use_cache'):\n",
        "                try:\n",
        "                    model.config.use_cache = False\n",
        "                except Exception:\n",
        "                    pass\n",
        "            optim_name = 'adamw_bnb_8bit' if HAS_BNB else 'adafactor'\n",
        "            print(f'Using optimizer: {optim_name}')\n",
        "            args = TrainingArguments(\n",
        "                output_dir=model_root,\n",
        "                per_device_train_batch_size=bsz, per_device_eval_batch_size=4,\n",
        "                gradient_accumulation_steps=grad_accum,\n",
        "                num_train_epochs=epochs, learning_rate=lr, warmup_ratio=warmup_ratio, weight_decay=0.01,\n",
        "                max_grad_norm=1.0, optim=optim_name, lr_scheduler_type='cosine', group_by_length=False,\n",
        "                bf16=False, fp16=True,\n",
        "                save_strategy='no', save_total_limit=1,\n",
        "                logging_steps=10, evaluation_strategy='no',\n",
        "                seed=42, report_to=[]\n",
        "            )\n",
        "            collator = DataCollatorWithPadding(tokenizer_x, pad_to_multiple_of=None)\n",
        "            trainer = Trainer(model=model, args=args, train_dataset=train_ds, data_collator=collator)\n",
        "            print(f'Training fold {fold}...', flush=True)\n",
        "            t0 = time.time(); trainer.train(); print(f'Fold {fold} train time: {time.time()-t0:.1f}s')\n",
        "            trainer.save_model(model_root)\n",
        "            del trainer\n",
        "            model = _try_load_fold_model(model_root)\n",
        "        # Inference on val\n",
        "        model.eval(); device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'); model.to(device)\n",
        "        collator = DataCollatorWithPadding(tokenizer_x, pad_to_multiple_of=None)\n",
        "        val_loader = torch.utils.data.DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collator, pin_memory=torch.cuda.is_available())\n",
        "        N = len(val_feats['input_ids'])\n",
        "        s_logits_list = [None] * N\n",
        "        e_logits_list = [None] * N\n",
        "        with torch.no_grad():\n",
        "            t1=time.time()\n",
        "            for step, batch in enumerate(val_loader):\n",
        "                feat_idx = batch.pop('feat_idx').cpu().numpy()\n",
        "                for k in list(batch.keys()):\n",
        "                    batch[k] = batch[k].to(device)\n",
        "                out = model(**batch)\n",
        "                s = out.start_logits.detach().cpu().numpy()  # (B, L)\n",
        "                e = out.end_logits.detach().cpu().numpy()    # (B, L)\n",
        "                for j, fi in enumerate(feat_idx):\n",
        "                    s_logits_list[int(fi)] = s[j]\n",
        "                    e_logits_list[int(fi)] = e[j]\n",
        "                if step % 20 == 0: print(f'Val fold {fold} step {step}, {time.time()-t1:.1f}s', flush=True)\n",
        "        assert all(x is not None for x in s_logits_list), 'Missing start logits entries' # noqa: E702\n",
        "        assert all(x is not None for x in e_logits_list), 'Missing end logits entries' # noqa: E702\n",
        "        pred_texts, pred_starts = pool_nbest_over_features(val_feats, val_df, s_logits_list, e_logits_list, rerank_with_gold=False)\n",
        "        val_out = val_df.copy()\n",
        "        val_out['pred_text'] = pred_texts; val_out['pred_start'] = pred_starts\n",
        "        val_out['jaccard'] = [word_jaccard(a,b) for a,b in zip(val_out['answer_text'].astype(str), val_out['pred_text'].astype(str))]\n",
        "        print(f'Fold {fold} OOF Jaccard: {val_out[\"jaccard\"].mean():.5f}, elapsed {time.time()-t_fold:.1f}s')\n",
        "        val_out.to_csv(f'oof_xlmr_fold{fold}.csv', index=False)\n",
        "        all_oof.append(val_out[['id','jaccard']])\n",
        "        # free\n",
        "        del model, train_ds, val_ds, trn_feats, val_feats; torch.cuda.empty_cache(); gc.collect()\n",
        "    oof = pd.concat(all_oof, axis=0, ignore_index=True)\n",
        "    print('OOF Jaccard mean:', float(oof['jaccard'].mean()))\n",
        "    return float(oof['jaccard'].mean())\n",
        "\n",
        "print('deepset/xlm-roberta-base-squad2 pipeline ready (FP16, gradient checkpointing, max_length=384). Next: run training.')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bitsandbytes available: using adamw_bnb_8bit optimizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer: deepset/xlm-roberta-base-squad2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deepset/xlm-roberta-base-squad2 pipeline ready (FP16, gradient checkpointing, max_length=256). Next: run training.\n"
          ]
        }
      ]
    },
    {
      "id": "e403277c-9e90-4893-b9c0-4103ab313e58",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Diagnostic: check proportion of features with start_positions==0 in training features (fold 0)\n",
        "import pandas as pd, numpy as np, time\n",
        "t0=time.time()\n",
        "df_folds = pd.read_csv('train_folds.csv')\n",
        "fold = 0\n",
        "trn_df = df_folds[df_folds['fold']!=fold].reset_index(drop=True)\n",
        "print(f'Train rows (fold != {fold}):', len(trn_df))\n",
        "\n",
        "# Use the xlm-roberta feature prep defined in Cell 7\n",
        "feats = prepare_train_features_x(trn_df)\n",
        "sp = np.array(feats['start_positions'])\n",
        "ep = np.array(feats['end_positions'])\n",
        "n = len(sp)\n",
        "prop_sp0 = float((sp==0).sum())/n if n>0 else float('nan')\n",
        "prop_ep0 = float((ep==0).sum())/n if n>0 else float('nan')\n",
        "prop_both0 = float(((sp==0)&(ep==0)).sum())/n if n>0 else float('nan')\n",
        "print('Num overflowed train features:', n)\n",
        "print('start_positions==0:', (sp==0).sum(), f'({prop_sp0:.4f})')\n",
        "print('end_positions==0  :', (ep==0).sum(), f'({prop_ep0:.4f})')\n",
        "print('both start&end==0 :', ((sp==0)&(ep==0)).sum(), f'({prop_both0:.4f})')\n",
        "print('Diag done in %.2fs' % (time.time()-t0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "9cb7ee7d-69c1-4608-84c0-fb78eeebc506",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Diagnostic 2: per-example coverage of answer in overflowed features (fold 0)\n",
        "import pandas as pd, numpy as np, time\n",
        "t0=time.time()\n",
        "df_folds = pd.read_csv('train_folds.csv')\n",
        "fold = 0\n",
        "trn_df = df_folds[df_folds['fold']!=fold].reset_index(drop=True)\n",
        "print(f'Train rows (fold != {fold}):', len(trn_df))\n",
        "\n",
        "# Tokenize with overflow and offsets, keep mapping\n",
        "tok = tokenizer_x(trn_df['question'].astype(str).tolist(), trn_df['context'].astype(str).tolist(),\n",
        "                  truncation='only_second', max_length=max_length, stride=doc_stride,\n",
        "                  return_overflowing_tokens=True, return_offsets_mapping=True, padding=False)\n",
        "sample_mapping = np.array(tok['overflow_to_sample_mapping'])\n",
        "offsets_list = tok['offset_mapping']\n",
        "\n",
        "starts = trn_df['answer_start'].astype(int).values\n",
        "answers = trn_df['answer_text'].astype(str).values\n",
        "\n",
        "per_sample_pos = np.zeros(len(trn_df), dtype=np.int32)\n",
        "total_pos = 0\n",
        "\n",
        "for i, offsets in enumerate(offsets_list):\n",
        "    sidx = int(sample_mapping[i])\n",
        "    seq_ids = tok.sequence_ids(i)\n",
        "    ctx_tokens = [j for j, sid in enumerate(seq_ids) if sid == 1]\n",
        "    if not ctx_tokens:\n",
        "        continue\n",
        "    c0, c1 = ctx_tokens[0], ctx_tokens[-1]\n",
        "    start_char = int(starts[sidx])\n",
        "    end_char = start_char + len(answers[sidx])\n",
        "    # inclusion check: answer fully covered by this feature's context span\n",
        "    ok = (offsets[c0][0] is not None and offsets[c1][1] is not None and\n",
        "          offsets[c0][0] <= start_char and offsets[c1][1] >= end_char)\n",
        "    if ok:\n",
        "        total_pos += 1\n",
        "        per_sample_pos[sidx] += 1\n",
        "\n",
        "num_with_any = int((per_sample_pos > 0).sum())\n",
        "print('Total features:', len(offsets_list))\n",
        "print('Positive (features containing answer):', total_pos, f'({total_pos/len(offsets_list):.4f})')\n",
        "print('Samples with at least one positive feature:', num_with_any, f'({num_with_any/len(trn_df):.4f})')\n",
        "print('Avg positive features per sample (over all):', float(per_sample_pos.mean()))\n",
        "print('Avg positive features per covered sample:', float(per_sample_pos[per_sample_pos>0].mean()) if num_with_any>0 else float('nan'))\n",
        "print('Max positive features for a sample:', int(per_sample_pos.max()))\n",
        "print('Top 5 samples by positive count (idx, count):', list(zip(np.argsort(-per_sample_pos)[:5].tolist(), np.sort(per_sample_pos)[-5:].tolist())))\n",
        "print('Diag2 done in %.2fs' % (time.time()-t0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "43c71e16-3a79-4115-a2c7-10a96de60d97",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Execute xlm-roberta-large 5-fold training and report OOF\n",
        "import time, json, pathlib\n",
        "t0=time.time()\n",
        "oof_mean = train_5fold_x()\n",
        "print('Final 5-fold OOF Jaccard:', oof_mean)\n",
        "pathlib.Path('metrics.json').write_text(json.dumps({'oof_jaccard': float(oof_mean)}, ensure_ascii=False))\n",
        "print('Total elapsed: %.1fs' % (time.time()-t0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "b43eb6f1-bba6-4296-81c3-47b743f82006",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Debug: inspect existing fold 0 checkpoint and GPU mem\n",
        "import os, sys, torch, glob\n",
        "from pathlib import Path\n",
        "\n",
        "def ls(path):\n",
        "    try:\n",
        "        items = sorted(os.listdir(path))\n",
        "        print(path, '->', items)\n",
        "        for f in items:\n",
        "            p = os.path.join(path, f)\n",
        "            try:\n",
        "                sz = os.path.getsize(p)\n",
        "                print('  ', f, sz)\n",
        "            except Exception as e:\n",
        "                print('  ', f, '??')\n",
        "    except FileNotFoundError:\n",
        "        print(path, 'does not exist')\n",
        "\n",
        "ls('xlmr_f0')\n",
        "ls('outputs_fold0')\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "    print('Allocated (MB):', round(torch.cuda.memory_allocated()/1024/1024,1))\n",
        "    print('Reserved (MB):', round(torch.cuda.memory_reserved()/1024/1024,1))\n",
        "    torch.cuda.empty_cache()\n",
        "    print('After empty_cache reserved (MB):', round(torch.cuda.memory_reserved()/1024/1024,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "83be12ed-a537-4958-991c-4d2d269b0e8b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cleanup: remove stale fold directories before retraining\n",
        "import shutil, os\n",
        "dirs = [f'xlmr_f{i}' for i in range(5)]\n",
        "for d in dirs:\n",
        "    if os.path.isdir(d):\n",
        "        print('Removing', d)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "print('Cleanup done.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "6c66e14b-af7f-43d3-897b-5ed63d774f91",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install bitsandbytes without touching torch stack (use --no-deps) to enable 8-bit AdamW\n",
        "import subprocess, sys\n",
        "print('Installing bitsandbytes==0.43.3 with --no-deps (prevent torch drift)...', flush=True)\n",
        "subprocess.run([sys.executable, '-m', 'pip', 'install', 'bitsandbytes==0.43.3', '--no-deps'], check=True)\n",
        "try:\n",
        "    import bitsandbytes as bnb\n",
        "    print('bitsandbytes version:', getattr(bnb, '__version__', 'unknown'))\n",
        "    # Optional: basic CUDA presence check in bnb\n",
        "    try:\n",
        "        from bitsandbytes.cuda_setup import get_compute_capabilities\n",
        "        print('bitsandbytes compute capabilities:', get_compute_capabilities())\n",
        "    except Exception:\n",
        "        pass\n",
        "except Exception as e:\n",
        "    print('Failed to import bitsandbytes after install:', e)\n",
        "import torch\n",
        "print('torch:', torch.__version__, 'CUDA available:', torch.cuda.is_available())\n",
        "print('Done. Re-run Cell 7 to pick up HAS_BNB=True, then run Cell 8.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "869f7853-0b60-4c24-b50b-12c553f67995",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Diagnostic 3 (fast): verify that labeled token spans map back to gold answers on a subset (normalized match allowed)\n",
        "import pandas as pd, numpy as np, random, time, sys\n",
        "t0=time.time()\n",
        "fold=0\n",
        "df_folds = pd.read_csv('train_folds.csv')\n",
        "trn_df_full = df_folds[df_folds['fold']!=fold].reset_index(drop=True)\n",
        "\n",
        "# Subsample to speed up (representative subset)\n",
        "n_samples = min(2, len(trn_df_full))\n",
        "rng = np.random.RandomState(42)\n",
        "idx = rng.choice(len(trn_df_full), size=n_samples, replace=False)\n",
        "trn_df = trn_df_full.iloc[np.sort(idx)].reset_index(drop=True)\n",
        "print(f'[Diag3] Using subset: {len(trn_df)} examples (of {len(trn_df_full)} total).', flush=True)\n",
        "\n",
        "try:\n",
        "    t1=time.time()\n",
        "    raw = prepare_train_features_x(trn_df)\n",
        "    print(f'[Diag3] prepare_train_features_x done in {time.time()-t1:.2f}s', flush=True)\n",
        "except KeyboardInterrupt:\n",
        "    print('[Diag3] Interrupted during prepare_train_features_x', flush=True)\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print('[Diag3] Error in prepare_train_features_x:', repr(e), flush=True)\n",
        "    raise\n",
        "\n",
        "# Re-tokenize to get mapping/offsets aligned with prepare_train_features_x settings\n",
        "tok = tokenizer_x(trn_df['question'].astype(str).tolist(), trn_df['context'].astype(str).tolist(),\n",
        "                  truncation='only_second', max_length=max_length, stride=doc_stride,\n",
        "                  return_overflowing_tokens=True, return_offsets_mapping=True, padding=False)\n",
        "sample_map = np.array(tok['overflow_to_sample_mapping'])\n",
        "offsets_list = tok['offset_mapping']\n",
        "\n",
        "sp = np.array(raw['start_positions']); ep = np.array(raw['end_positions'])\n",
        "pos_idx = np.where(sp>0)[0].tolist()\n",
        "print('Total features:', len(sp), 'positives:', len(pos_idx), flush=True)\n",
        "\n",
        "# Cap verification to at most 200 positive features for speed\n",
        "max_check = 200\n",
        "if len(pos_idx) > max_check:\n",
        "    pos_idx = pos_idx[:max_check]\n",
        "    print(f'[Diag3] Capped positives to first {max_check} for verification', flush=True)\n",
        "\n",
        "n_ok=0; n_bad=0; bad_examples=[]\n",
        "for i in pos_idx:\n",
        "    sidx = int(sample_map[i])\n",
        "    offsets = offsets_list[i]\n",
        "    si = int(sp[i]); ei = int(ep[i])\n",
        "    # bounds and context-only checks\n",
        "    seq_ids = tok.sequence_ids(i)\n",
        "    ctx_tokens = [j for j, sid in enumerate(seq_ids) if sid==1]\n",
        "    if not ctx_tokens: n_bad+=1; continue\n",
        "    c0, c1 = ctx_tokens[0], ctx_tokens[-1]\n",
        "    if not (c0 <= si <= c1 and c0 <= ei <= c1 and si <= ei):\n",
        "        n_bad += 1\n",
        "        if len(bad_examples) < 5:\n",
        "            bad_examples.append({'type':'out_of_ctx', 'feat_i':i, 'si':si, 'ei':ei, 'c0':c0, 'c1':c1})\n",
        "        continue\n",
        "    stc = offsets[si][0]; enc = offsets[ei][1]\n",
        "    if stc is None or enc is None or enc <= stc:\n",
        "        n_bad += 1\n",
        "        if len(bad_examples) < 5:\n",
        "            bad_examples.append({'type':'none_offsets', 'feat_i':i, 'stc':stc, 'enc':enc})\n",
        "        continue\n",
        "    ctx = trn_df.loc[sidx, 'context']\n",
        "    pred = ctx[stc:enc]\n",
        "    gold = trn_df.loc[sidx, 'answer_text']\n",
        "    # Consider normalized/trimmed equality due to SentencePiece leading-space offsets\n",
        "    if (pred == gold) or (edge_trim(pred.strip()) == gold) or (norm_for_metric(pred) == norm_for_metric(gold)):\n",
        "        n_ok += 1\n",
        "    else:\n",
        "        n_bad += 1\n",
        "        if len(bad_examples) < 5:\n",
        "            bad_examples.append({'type':'mismatch', 'feat_i':i, 'pred':pred, 'gold':gold, 'stc':stc, 'enc':enc, 'si':si, 'ei':ei})\n",
        "\n",
        "p = (n_ok/max(1,len(pos_idx)))\n",
        "print(f'Positive features exact-match (normalized): {n_ok}/{len(pos_idx)} ({p:.4f})', flush=True)\n",
        "print(f'Positive features bad: {n_bad}/{len(pos_idx)} ({(n_bad/max(1,len(pos_idx))):.4f})', flush=True)\n",
        "if bad_examples:\n",
        "    print('Examples of issues (up to 5):', flush=True)\n",
        "    for ex in bad_examples:\n",
        "        print(ex, flush=True)\n",
        "print('Diag3 subset done in %.2fs' % (time.time()-t0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "d43f8395-0294-4f2d-8eea-c723e8ca3d33",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# One-fold smoke training (fold 0) on GPU; 1:1 negatives; epochs adjustable\n",
        "import os, time, torch, numpy as np, pandas as pd, gc\n",
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "\n",
        "def train_one_fold_smoke(fold: int = 0, epochs_override: int = 3):\n",
        "    df = pd.read_csv('train_folds.csv')\n",
        "    trn_df = df[df['fold']!=fold].reset_index(drop=True)\n",
        "    val_df = df[df['fold']==fold].reset_index(drop=True)\n",
        "    print(f'[SMOKE] Fold {fold}: train {len(trn_df)} val {len(val_df)}')\n",
        "\n",
        "    # Ensure CUDA intended\n",
        "    use_cpu = False\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            free, total = torch.cuda.mem_get_info()\n",
        "            print(f\"[SMOKE] CUDA free MB: {int(free//(1024*1024))} / {int(total//(1024*1024))}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    else:\n",
        "        print('[SMOKE] WARNING: CUDA not available; will fall back to CPU and run slowly.')\n",
        "        use_cpu = True\n",
        "\n",
        "    # Enforce identical sequence settings train/val for smoke\n",
        "    global max_length, doc_stride\n",
        "    old_max_len, old_stride = max_length, doc_stride\n",
        "    max_length = 384\n",
        "    doc_stride = 128\n",
        "    print(f\"[SMOKE] Seq settings (train & val): max_length={max_length}, doc_stride={doc_stride}\")\n",
        "\n",
        "    try:\n",
        "        trn_feats_raw = prepare_train_features_x(trn_df)\n",
        "        val_feats = prepare_features_only_x(val_df)\n",
        "    finally:\n",
        "        max_length, doc_stride = old_max_len, old_stride\n",
        "\n",
        "    # Positives + sampled negatives (1:1 for smoke)\n",
        "    is_pos = np.array([int(sp) > 0 for sp in trn_feats_raw['start_positions']])\n",
        "    all_idx = np.arange(len(is_pos))\n",
        "    pos_idx = all_idx[is_pos]\n",
        "    neg_idx = all_idx[~is_pos]\n",
        "    rng = np.random.RandomState(42)\n",
        "    n_neg_keep = min(len(neg_idx), len(pos_idx))\n",
        "    sampled_neg = rng.choice(neg_idx, size=n_neg_keep, replace=False) if n_neg_keep > 0 else np.array([], dtype=int)\n",
        "    keep_idx = np.sort(np.concatenate([pos_idx, sampled_neg])) if len(pos_idx)>0 else np.array([], dtype=int)\n",
        "\n",
        "    def filt_field(vals, idx):\n",
        "        return [vals[i] for i in idx] if isinstance(vals, list) else None\n",
        "    trn_feats = {}\n",
        "    for k, v in trn_feats_raw.items():\n",
        "        fl = filt_field(v, keep_idx)\n",
        "        if fl is not None:\n",
        "            trn_feats[k] = fl\n",
        "    if 'token_type_ids' in trn_feats_raw and 'token_type_ids' not in trn_feats:\n",
        "        trn_feats['token_type_ids'] = filt_field(trn_feats_raw['token_type_ids'], keep_idx)\n",
        "\n",
        "    train_ds = QADataset(trn_feats, with_labels=True)\n",
        "    val_ds = QADataset(val_feats, with_labels=False)\n",
        "    eff_bsz = (8 if not use_cpu else 2) * (2 if not use_cpu else 1)\n",
        "    print(f\"[SMOKE] kept_features={len(keep_idx)} (pos={len(pos_idx)}, neg_samp={len(sampled_neg)}), eff_bsz={eff_bsz}\")\n",
        "\n",
        "    out_dir = f'xlmr_smoke_f{fold}'\n",
        "    if os.path.isdir(out_dir):\n",
        "        import shutil; print('Removing', out_dir); shutil.rmtree(out_dir, ignore_errors=True)\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(xlmr_model, attn_implementation='eager')\n",
        "    if hasattr(model, 'gradient_checkpointing_enable'): model.gradient_checkpointing_enable()\n",
        "    if hasattr(model, 'config') and hasattr(model.config, 'use_cache'):\n",
        "        try: model.config.use_cache = False\n",
        "        except Exception: pass\n",
        "    optim_name = 'adamw_bnb_8bit' if (not use_cpu and 'HAS_BNB' in globals() and HAS_BNB) else 'adafactor'\n",
        "    args = TrainingArguments(\n",
        "        output_dir=out_dir,\n",
        "        per_device_train_batch_size=(8 if not use_cpu else 2), per_device_eval_batch_size=(8 if not use_cpu else 2),\n",
        "        gradient_accumulation_steps=(2 if not use_cpu else 1),\n",
        "        num_train_epochs=epochs_override, learning_rate=lr, warmup_ratio=warmup_ratio, weight_decay=0.01,\n",
        "        max_grad_norm=1.0, optim=optim_name, lr_scheduler_type='cosine',\n",
        "        bf16=False, fp16=(False if use_cpu else True), group_by_length=True,\n",
        "        save_strategy='no', logging_steps=10, evaluation_strategy='no', seed=42, report_to=[]\n",
        "    )\n",
        "    collator = DataCollatorWithPadding(tokenizer_x, pad_to_multiple_of=None)\n",
        "    trainer = Trainer(model=model, args=args, train_dataset=train_ds, data_collator=collator)\n",
        "    print('[SMOKE] Training...', flush=True); t0=time.time(); trainer.train(); print('[SMOKE] Train time: %.1fs' % (time.time()-t0), flush=True)\n",
        "\n",
        "    # Inference\n",
        "    device = torch.device('cpu' if use_cpu else ('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "    model.eval(); model.to(device)\n",
        "    val_loader = torch.utils.data.DataLoader(val_ds, batch_size=(8 if not use_cpu else 2), shuffle=False, collate_fn=collator, pin_memory=(not use_cpu and torch.cuda.is_available()))\n",
        "    N = len(val_feats['input_ids'])\n",
        "    s_logits_list = [None] * N; e_logits_list = [None] * N\n",
        "    with torch.no_grad():\n",
        "        t1=time.time()\n",
        "        for step, batch in enumerate(val_loader):\n",
        "            feat_idx = batch.pop('feat_idx').cpu().numpy()\n",
        "            for k in list(batch.keys()): batch[k] = batch[k].to(device)\n",
        "            out = model(**batch)\n",
        "            s = out.start_logits.detach().cpu().numpy(); e = out.end_logits.detach().cpu().numpy()\n",
        "            for j, fi in enumerate(feat_idx):\n",
        "                s_logits_list[int(fi)] = s[j]; e_logits_list[int(fi)] = e[j]\n",
        "            if step % 20 == 0: print(f'[SMOKE] Val step {step}, {time.time()-t1:.1f}s', flush=True)\n",
        "    assert all(x is not None for x in s_logits_list) and all(x is not None for x in e_logits_list)\n",
        "    pred_texts, pred_starts = pool_nbest_over_features(val_feats, val_df, s_logits_list, e_logits_list, rerank_with_gold=False)\n",
        "    val_out = val_df.copy(); val_out['pred_text'] = pred_texts; val_out['pred_start'] = pred_starts\n",
        "    val_out['jaccard'] = [word_jaccard(a,b) for a,b in zip(val_out['answer_text'].astype(str), val_out['pred_text'].astype(str))]\n",
        "    print('[SMOKE] Fold %d OOF Jaccard: %.5f' % (fold, float(val_out['jaccard'].mean())))\n",
        "    val_out.to_csv(f'oof_xlmr_smoke_f{fold}.csv', index=False)\n",
        "    # free\n",
        "    del model, trainer, train_ds, val_ds; torch.cuda.empty_cache(); gc.collect()\n",
        "    return float(val_out['jaccard'].mean())\n",
        "\n",
        "print('Smoke trainer ready (GPU by default, 1:1 negatives, epochs default=3, max_length=384/doc_stride=128). After GPU is clean, run train_one_fold_smoke(fold=0, epochs_override=3).')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Smoke trainer ready (consistent 256/128 seq, 1:1 negatives, CPU cap=150, epochs default=3, forced CPU). Next: run train_one_fold_smoke(fold=0, epochs_override=1..3).\n"
          ]
        }
      ]
    },
    {
      "id": "4dc39cde-1239-489a-86fb-9faa2ad9f5d3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Patch v2: fast coverage-based snapping using gold answer_start (no exhaustive span search)\n",
        "import numpy as np\n",
        "def prepare_train_features_x(df: pd.DataFrame):\n",
        "    questions = df['question'].astype(str).tolist()\n",
        "    contexts = df['context'].astype(str).tolist()\n",
        "    answers = df['answer_text'].astype(str).tolist()\n",
        "    starts = df['answer_start'].astype(int).tolist()\n",
        "\n",
        "    tok = tokenizer_x(\n",
        "        questions, contexts,\n",
        "        truncation='only_second', max_length=max_length, stride=doc_stride,\n",
        "        return_overflowing_tokens=True, return_offsets_mapping=True, padding=False\n",
        "    )\n",
        "    sample_map = tok.pop('overflow_to_sample_mapping')\n",
        "    offsets_list = tok['offset_mapping']\n",
        "\n",
        "    start_positions, end_positions = [], []\n",
        "\n",
        "    for i, offsets in enumerate(offsets_list):\n",
        "        ex = int(sample_map[i])\n",
        "        seq_ids = tok.sequence_ids(i)\n",
        "        ctx_tokens = [j for j, sid in enumerate(seq_ids) if sid == 1]\n",
        "        if not ctx_tokens:\n",
        "            start_positions.append(0); end_positions.append(0); continue\n",
        "        c0, c1 = ctx_tokens[0], ctx_tokens[-1]\n",
        "\n",
        "        ctx = contexts[ex]\n",
        "        gold = answers[ex]\n",
        "        s_char0 = int(starts[ex])\n",
        "        e_char0 = s_char0 + len(gold)\n",
        "        # Basic sanity on provided span\n",
        "        if s_char0 < 0 or e_char0 > len(ctx) or s_char0 >= e_char0:\n",
        "            start_positions.append(0); end_positions.append(0); continue\n",
        "\n",
        "        # Tighten edges at char-level (skip ws/punct/ZW but never Mn combining marks)\n",
        "        s_adj, e_adj = _trim_bounds(ctx, s_char0, e_char0)\n",
        "        if s_adj >= e_adj:\n",
        "            start_positions.append(0); end_positions.append(0); continue\n",
        "\n",
        "        # Check coverage by this feature's context span\n",
        "        cov_ok = (offsets[c0][0] is not None and offsets[c1][1] is not None and\n",
        "                  offsets[c0][0] <= s_adj and offsets[c1][1] >= e_adj)\n",
        "        if not cov_ok:\n",
        "            start_positions.append(0); end_positions.append(0); continue\n",
        "\n",
        "        # Coverage-based snapping within full context window [c0, c1]\n",
        "        si2 = None\n",
        "        for j in range(c0, c1+1):\n",
        "            sj, ej = offsets[j]\n",
        "            if sj is None or ej is None or ej <= sj: continue\n",
        "            if sj <= s_adj < ej:\n",
        "                si2 = j; break\n",
        "        ei2 = None\n",
        "        for j in range(c1, c0-1, -1):\n",
        "            sj, ej = offsets[j]\n",
        "            if sj is None or ej is None or ej <= sj: continue\n",
        "            if sj < e_adj <= ej:\n",
        "                ei2 = j; break\n",
        "\n",
        "        # Fallbacks to nearest within [c0, c1] if coverage missed due to offset quirks\n",
        "        if si2 is None:\n",
        "            cand = []\n",
        "            for j in range(c0, c1+1):\n",
        "                sj = offsets[j][0]\n",
        "                d = abs(((sj if sj is not None else 10**18) - s_adj))\n",
        "                cand.append((d, j))\n",
        "            si2 = min(cand)[1]\n",
        "        if ei2 is None:\n",
        "            cand = []\n",
        "            for j in range(c0, c1+1):\n",
        "                ej = offsets[j][1]\n",
        "                d = abs(((ej if ej is not None else -10**18) - e_adj))\n",
        "                cand.append((d, j))\n",
        "            ei2 = min(cand)[1]\n",
        "\n",
        "        if si2 is None or ei2 is None or si2 > ei2:\n",
        "            start_positions.append(0); end_positions.append(0); continue\n",
        "\n",
        "        start_positions.append(int(si2))\n",
        "        end_positions.append(int(ei2))\n",
        "\n",
        "    tok['start_positions'] = start_positions\n",
        "    tok['end_positions'] = end_positions\n",
        "    return tok\n",
        "\n",
        "print('[Patch v2] prepare_train_features_x updated: fast coverage-based snapping using gold answer_start.')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch v2] prepare_train_features_x updated: fast coverage-based snapping using gold answer_start.\n"
          ]
        }
      ]
    },
    {
      "id": "32cae5e9-2fc7-4bb3-82c0-844090730ab9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Reduce sequence length further and increase grad_accum to avoid OOM during smoke\n",
        "max_length = 192  # tighter to fit GPU\n",
        "doc_stride = 64    # reduce overlap to lower token count\n",
        "grad_accum = 16    # further reduce per-step activation memory\n",
        "bsz = 1            # keep minimal\n",
        "import torch, gc, os\n",
        "os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')\n",
        "try:\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "except Exception:\n",
        "    pass\n",
        "torch.cuda.empty_cache(); gc.collect()\n",
        "print('[SMOKE-SETTINGS] max_length=', max_length, 'doc_stride=', doc_stride, 'grad_accum=', grad_accum, 'bsz=', bsz, flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "f8dd75bf-c1ec-42f7-8384-6323e0aa810b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# GPU cleanup: show processes and kill stray VRAM holders (not this kernel)\n",
        "import os, subprocess, time, signal, sys, gc, torch\n",
        "def run(cmd):\n",
        "    return subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, check=False).stdout\n",
        "print('[GPU] Before cleanup:')\n",
        "print(run(['bash','-lc','nvidia-smi || true']))\n",
        "pids_to_kill = []\n",
        "try:\n",
        "    q = run(['bash','-lc','nvidia-smi --query-compute-apps=pid,used_memory --format=csv,noheader,nounits || true'])\n",
        "    cur = os.getpid()\n",
        "    for line in q.strip().splitlines():\n",
        "        parts = [x.strip() for x in line.split(',')]\n",
        "        if len(parts) >= 2:\n",
        "            try:\n",
        "                pid = int(parts[0]); mem = int(parts[1])\n",
        "            except Exception:\n",
        "                continue\n",
        "            if pid != cur and mem >= 100:  # kill anything using >=100MB that's not us\n",
        "                pids_to_kill.append(pid)\n",
        "except Exception as e:\n",
        "    print('[GPU] Query error:', repr(e))\n",
        "if pids_to_kill:\n",
        "    print('[GPU] Killing PIDs:', pids_to_kill)\n",
        "    for pid in pids_to_kill:\n",
        "        try:\n",
        "            os.kill(pid, signal.SIGTERM)\n",
        "        except Exception as e:\n",
        "            print(' SIGTERM fail for', pid, e)\n",
        "    time.sleep(2.0)\n",
        "    for pid in pids_to_kill:\n",
        "        try:\n",
        "            os.kill(pid, 0)\n",
        "            os.kill(pid, signal.SIGKILL)\n",
        "        except ProcessLookupError:\n",
        "            pass\n",
        "        except Exception as e:\n",
        "            print(' SIGKILL fail for', pid, e)\n",
        "else:\n",
        "    print('[GPU] No other GPU processes detected (or <100MB).')\n",
        "torch.cuda.empty_cache(); gc.collect()\n",
        "time.sleep(1.0)\n",
        "print('[GPU] After cleanup:')\n",
        "print(run(['bash','-lc','nvidia-smi || true']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "a8a6eef1-f8e4-4b2d-90fb-7bca71eb86e7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run smoke test on GPU: 3 epochs on fold 0 (after session restart and setup cells run)\n",
        "import time, torch\n",
        "if not torch.cuda.is_available():\n",
        "    print('[SMOKE] CUDA not available. Restart the session (Power > Restart Session), then run Cells 4 \u2192 7 \u2192 16 \u2192 20, and re-run this cell.')\n",
        "else:\n",
        "    t0=time.time()\n",
        "    score = train_one_fold_smoke(fold=0, epochs_override=3)\n",
        "    print('[SMOKE] Done in %.1fs, OOF Jaccard=%.5f' % (time.time()-t0, score))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Fold 0: train 813 val 189\n[SMOKE] CUDA free MB: 12781\n[SMOKE] Forcing CPU mode for smoke validation (GPU wedged).\n[SMOKE] Seq settings (train & val): max_length=256, doc_stride=128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] CPU mode downsample: pos_cap=150, neg_cap=150, kept=300\n[SMOKE] features_kept=300 (total_pos=1246, total_neg_samp=1246), eff_bsz=1\nRemoving xlmr_smoke_f0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/900 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Train time: 998.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 0, 0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 20, 6.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 40, 12.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 60, 17.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 80, 23.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 100, 28.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 120, 33.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 140, 38.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 160, 43.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 180, 49.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 200, 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 220, 60.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 240, 66.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 260, 71.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 280, 77.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 300, 82.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 320, 87.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 340, 93.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 360, 98.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 380, 103.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 400, 109.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 420, 114.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 440, 119.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 460, 125.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 480, 130.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 500, 135.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 520, 140.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 540, 146.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 560, 151.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 580, 156.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 600, 161.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 620, 166.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 640, 171.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 660, 177.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 680, 182.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 700, 187.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 720, 192.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 740, 197.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 760, 203.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 780, 208.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 800, 213.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 820, 219.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 840, 224.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 860, 229.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 880, 235.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 900, 241.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 920, 246.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 940, 252.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 960, 257.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 980, 263.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 1000, 268.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 1020, 274.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 1040, 280.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 1060, 285.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 1080, 291.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 1100, 297.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 1120, 303.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 1140, 309.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 1160, 316.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 1180, 322.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 1200, 328.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 1220, 334.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 1240, 340.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 1260, 346.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Val step 1280, 352.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Fold 0 OOF Jaccard: 0.50960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE] Done in 1428.8s, OOF Jaccard=0.50960\n"
          ]
        }
      ]
    },
    {
      "id": "34bde149-3683-445d-b1c1-31e7fbd148c5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Patch: simplify post-processing and increase n_best_size to 100, max span 50\n",
        "n_best_size = 100  # override previous setting\n",
        "\n",
        "def pool_nbest_over_features(features, examples_df, start_logits, end_logits, rerank_with_gold=False):\n",
        "    sample_mapping = features['overflow_to_sample_mapping']\n",
        "    preds_text = [''] * len(examples_df)\n",
        "    preds_start = [0] * len(examples_df)\n",
        "    best_score = [-1e30] * len(examples_df)\n",
        "    for i in range(len(sample_mapping)):\n",
        "        ex_idx = int(sample_mapping[i])\n",
        "        offsets = features['offset_mapping'][i]\n",
        "        seq_ids = features.sequence_ids(i)\n",
        "        ctx_tokens = [j for j,sid in enumerate(seq_ids) if sid==1]\n",
        "        if not ctx_tokens:\n",
        "            continue\n",
        "        c0, c1 = ctx_tokens[0], ctx_tokens[-1]\n",
        "        s = np.array(start_logits[i], dtype=np.float32).copy()\n",
        "        e = np.array(end_logits[i], dtype=np.float32).copy()\n",
        "        # mask non-context to very negative\n",
        "        s[:c0] = -1e9; e[:c0] = -1e9\n",
        "        if c1+1 < len(s):\n",
        "            s[c1+1:] = -1e9; e[c1+1:] = -1e9\n",
        "        start_idxes = np.argsort(s)[-n_best_size:][::-1]\n",
        "        end_idxes   = np.argsort(e)[-n_best_size:][::-1]\n",
        "        cands = []\n",
        "        ctx = examples_df.loc[ex_idx, 'context']\n",
        "        qtext = str(examples_df.loc[ex_idx, 'question'])\n",
        "        q_has_digit = 'DIGIT_PAT' in globals() and (DIGIT_PAT.search(qtext) is not None)\n",
        "        for si in start_idxes:\n",
        "            if si < c0 or si > c1:\n",
        "                continue\n",
        "            for ei in end_idxes:\n",
        "                if ei < c0 or ei > c1 or ei < si:\n",
        "                    continue\n",
        "                # joint token span length cap\n",
        "                if (ei - si + 1) > 50:\n",
        "                    continue\n",
        "                stc, enc = offsets[si][0], offsets[ei][1]\n",
        "                if stc is None or enc is None or enc <= stc:\n",
        "                    continue\n",
        "                text = edge_trim(ctx[stc:enc].strip())\n",
        "                if not text:\n",
        "                    continue\n",
        "                score = float(s[si] + e[ei])\n",
        "                # optional small numeric bonus\n",
        "                if q_has_digit:\n",
        "                    cand_has_digit = DIGIT_PAT.search(text) is not None\n",
        "                    score += (0.02 if cand_has_digit else -0.02)\n",
        "                cands.append((score, text, stc))\n",
        "        if not cands:\n",
        "            continue\n",
        "        if rerank_with_gold:\n",
        "            gold = examples_df.loc[ex_idx, 'answer_text']\n",
        "            cands.sort(key=lambda x: (word_jaccard(gold, x[1]), x[0], -len(x[1])), reverse=True)\n",
        "        else:\n",
        "            cands.sort(key=lambda x: (x[0], -len(x[1])), reverse=True)\n",
        "        score, text, stc = cands[0]\n",
        "        if score > best_score[ex_idx]:\n",
        "            best_score[ex_idx] = score\n",
        "            preds_text[ex_idx] = text\n",
        "            preds_start[ex_idx] = stc\n",
        "    for i in range(len(preds_text)):\n",
        "        if preds_text[i] == '':\n",
        "            preds_text[i] = edge_trim(examples_df.loc[i, 'context'][:0])\n",
        "            preds_start[i] = 0\n",
        "    return preds_text, preds_start\n",
        "\n",
        "print('[Patch] Post-processing: raw logits with context masking, n_best_size=100, max span=50, +numeric bonus if question has digits.')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch] Post-processing simplified: raw logits with context masking, n_best_size=100, max span=50\n"
          ]
        }
      ]
    },
    {
      "id": "c20b7f64-edc7-4413-ae7c-407dcbfec097",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Patch: redefine train_5fold_x to include sampled negatives (~2x per positive)\n",
        "def train_5fold_x():\n",
        "    import pandas as pd, numpy as np, time, torch, gc, os, glob\n",
        "    from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "    df = pd.read_csv('train_folds.csv')\n",
        "    all_oof = []\n",
        "    for fold in range(5):\n",
        "        t_fold = time.time()\n",
        "        trn_df = df[df['fold']!=fold].reset_index(drop=True)\n",
        "        val_df = df[df['fold']==fold].reset_index(drop=True)\n",
        "        print(f'Fold {fold}: train {len(trn_df)} val {len(val_df)}')\n",
        "        # Build raw train features\n",
        "        trn_feats_raw = prepare_train_features_x(trn_df)\n",
        "        # Positives + sampled negatives (2x)\n",
        "        is_pos = np.array([int(sp) > 0 for sp in trn_feats_raw['start_positions']])\n",
        "        all_idx = np.arange(len(is_pos))\n",
        "        pos_idx = all_idx[is_pos]\n",
        "        neg_idx = all_idx[~is_pos]\n",
        "        rng = np.random.RandomState(42 + fold)\n",
        "        n_neg_keep = min(len(neg_idx), 2*len(pos_idx))\n",
        "        sampled_neg = rng.choice(neg_idx, size=n_neg_keep, replace=False) if n_neg_keep > 0 else np.array([], dtype=int)\n",
        "        keep_idx = np.sort(np.concatenate([pos_idx, sampled_neg])) if len(pos_idx)>0 else np.array([], dtype=int)\n",
        "        def filt_field(vals, idx):\n",
        "            return [vals[i] for i in idx] if isinstance(vals, list) else None\n",
        "        trn_feats = {}\n",
        "        for k, v in trn_feats_raw.items():\n",
        "            fl = filt_field(v, keep_idx)\n",
        "            if fl is not None:\n",
        "                trn_feats[k] = fl\n",
        "        if 'token_type_ids' in trn_feats_raw and 'token_type_ids' not in trn_feats:\n",
        "            trn_feats['token_type_ids'] = filt_field(trn_feats_raw['token_type_ids'], keep_idx)\n",
        "        print(f\"Fold {fold}: features_kept={len(keep_idx)} (pos={len(pos_idx)}, neg_samp={len(sampled_neg)})\")\n",
        "\n",
        "        val_feats = prepare_features_only_x(val_df)\n",
        "        train_ds = QADataset(trn_feats, with_labels=True)\n",
        "        val_ds = QADataset(val_feats, with_labels=False)\n",
        "        num_feats = len(trn_feats['input_ids']) if 'input_ids' in trn_feats else 0\n",
        "        eff_bsz = bsz * grad_accum\n",
        "        steps_per_epoch = (num_feats + eff_bsz - 1) // eff_bsz if eff_bsz>0 else 0\n",
        "        print(f\"Fold {fold}: features={num_feats}, eff_bsz={eff_bsz}, steps/epoch={steps_per_epoch}, epochs={epochs}\")\n",
        "\n",
        "        model_root = f'xlmr_f{fold}'\n",
        "        ckpt_path = _find_checkpoint_dir(model_root)\n",
        "        if ckpt_path is not None:\n",
        "            print(f'Loading existing model for fold {fold} from {ckpt_path}')\n",
        "            model = _try_load_fold_model(ckpt_path)\n",
        "        else:\n",
        "            model = AutoModelForQuestionAnswering.from_pretrained(xlmr_model, attn_implementation='eager')\n",
        "            if hasattr(model, 'gradient_checkpointing_enable'):\n",
        "                model.gradient_checkpointing_enable()\n",
        "            if hasattr(model, 'config') and hasattr(model.config, 'use_cache'):\n",
        "                try: model.config.use_cache = False\n",
        "                except Exception: pass\n",
        "            optim_name = 'adamw_bnb_8bit' if HAS_BNB else 'adafactor'\n",
        "            print(f'Using optimizer: {optim_name}')\n",
        "            args = TrainingArguments(\n",
        "                output_dir=model_root,\n",
        "                per_device_train_batch_size=bsz, per_device_eval_batch_size=4,\n",
        "                gradient_accumulation_steps=grad_accum,\n",
        "                num_train_epochs=epochs, learning_rate=lr, warmup_ratio=warmup_ratio, weight_decay=0.01,\n",
        "                max_grad_norm=1.0, optim=optim_name, lr_scheduler_type='cosine', group_by_length=True,\n",
        "                bf16=False, fp16=True, save_strategy='no', save_total_limit=1,\n",
        "                logging_steps=10, evaluation_strategy='no', seed=42, report_to=[]\n",
        "            )\n",
        "            collator = DataCollatorWithPadding(tokenizer_x, pad_to_multiple_of=None)\n",
        "            trainer = Trainer(model=model, args=args, train_dataset=train_ds, data_collator=collator)\n",
        "            print(f'Training fold {fold}...', flush=True)\n",
        "            t0 = time.time(); trainer.train(); print(f'Fold {fold} train time: {time.time()-t0:.1f}s')\n",
        "            trainer.save_model(model_root)\n",
        "            del trainer\n",
        "            model = _try_load_fold_model(model_root)\n",
        "\n",
        "        # Inference on val\n",
        "        model.eval(); device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'); model.to(device)\n",
        "        collator = DataCollatorWithPadding(tokenizer_x, pad_to_multiple_of=None)\n",
        "        val_loader = torch.utils.data.DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collator, pin_memory=torch.cuda.is_available())\n",
        "        N = len(val_feats['input_ids'])\n",
        "        s_logits_list = [None] * N; e_logits_list = [None] * N\n",
        "        with torch.no_grad():\n",
        "            t1=time.time()\n",
        "            for step, batch in enumerate(val_loader):\n",
        "                feat_idx = batch.pop('feat_idx').cpu().numpy()\n",
        "                for k in list(batch.keys()): batch[k] = batch[k].to(device)\n",
        "                out = model(**batch)\n",
        "                s = out.start_logits.detach().cpu().numpy(); e = out.end_logits.detach().cpu().numpy()\n",
        "                for j, fi in enumerate(feat_idx):\n",
        "                    s_logits_list[int(fi)] = s[j]; e_logits_list[int(fi)] = e[j]\n",
        "                if step % 20 == 0: print(f'Val fold {fold} step {step}, {time.time()-t1:.1f}s', flush=True)\n",
        "        assert all(x is not None for x in s_logits_list) and all(x is not None for x in e_logits_list)\n",
        "        pred_texts, pred_starts = pool_nbest_over_features(val_feats, val_df, s_logits_list, e_logits_list, rerank_with_gold=False)\n",
        "        val_out = val_df.copy(); val_out['pred_text'] = pred_texts; val_out['pred_start'] = pred_starts\n",
        "        val_out['jaccard'] = [word_jaccard(a,b) for a,b in zip(val_out['answer_text'].astype(str), val_out['pred_text'].astype(str))]\n",
        "        print(f'Fold {fold} OOF Jaccard: {val_out[\"jaccard\"].mean():.5f}, elapsed {time.time()-t_fold:.1f}s')\n",
        "        val_out.to_csv(f'oof_xlmr_fold{fold}.csv', index=False)\n",
        "        all_oof.append(val_out[['id','jaccard']])\n",
        "        # free\n",
        "        del model, train_ds, val_ds, trn_feats, val_feats; torch.cuda.empty_cache(); gc.collect()\n",
        "    oof = pd.concat(all_oof, axis=0, ignore_index=True)\n",
        "    print('OOF Jaccard mean:', float(oof['jaccard'].mean()))\n",
        "    return float(oof['jaccard'].mean())\n",
        "\n",
        "print('[Patch] train_5fold_x updated to include sampled negatives (2x) and FP16 enabled on V100 as recommended.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "f17a84e5-19be-4cd8-baa9-27a41e59753b",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RESTART CHECKLIST (do this via UI now)\n",
        "- Power > Restart Session (to clear wedged GPU).\n",
        "- Run Cell 1: confirm nvidia-smi shows ~0 MiB used.\n",
        "- Run in order: Cell 4 (install torch/HF) \u2192 Cell 7 (init xlmr-base globals) \u2192 Cell 16 (prepare_train_features_x patch) \u2192 Cell 20 (post-process patch).\n",
        "- Optional: need 8-bit optimizer? Run Cell 13 (bitsandbytes --no-deps), then re-run Cell 7 to set HAS_BNB=True.\n",
        "- Smoke test: Cell 15 is GPU-ready; then run Cell 19 (epochs=3). Target OOF fold0 \u2265 0.70.\n",
        "- Full training: ensure Cell 21 has bf16=False, fp16=True (already set). Then call train_5fold_x() in a new cell.\n",
        "\n",
        "Globals for medal run (already set or in Cell 7):\n",
        "- Model: deepset/xlm-roberta-base-squad2\n",
        "- max_length=384, doc_stride=128\n",
        "- per_device_train_batch_size=4, grad_accum=4 (eff=16). If OOM: bsz=2, grad_accum=8 or max_length=320.\n",
        "- epochs=3\u20134, lr=2e-5, warmup_ratio=0.10, weight_decay=0.01\n",
        "- fp16=True, bf16=False, gradient_checkpointing=True, group_by_length=True, use_cache=False\n",
        "- Negatives \u2248 2x positives (Cell 21 handles this).\n",
        "\n",
        "Post-processing: simplified start+end with context masking, n_best=100, max span=50 (no extra penalties)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "7c155e25-db5e-4607-be01-1fc599bd04ea",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inference: average fold logits on test and write submission.csv\n",
        "import os, glob, time, numpy as np, pandas as pd, torch, gc\n",
        "from transformers import AutoModelForQuestionAnswering, DataCollatorWithPadding\n",
        "\n",
        "def predict_test_and_submit(model_glob='xlmr_f*', force_cpu=True):\n",
        "    # Hard-disable CUDA if forcing CPU to avoid wedged GPU OOM during model load\n",
        "    if force_cpu:\n",
        "        os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
        "    t0=time.time()\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    sub_tmpl = pd.read_csv('sample_submission.csv')\n",
        "    id_col = sub_tmpl.columns[0]\n",
        "    pred_col = sub_tmpl.columns[1]\n",
        "    print(f'[TEST] id_col={id_col}, pred_col={pred_col}, test_rows={len(test_df)}')\n",
        "\n",
        "    # Ensure tokenizer settings mirror training\n",
        "    feats = prepare_features_only_x(test_df)\n",
        "    test_ds = QADataset(feats, with_labels=False)\n",
        "    collator = DataCollatorWithPadding(tokenizer_x, pad_to_multiple_of=None)\n",
        "\n",
        "    # Discover fold model dirs\n",
        "    fold_dirs = sorted([d for d in glob.glob(model_glob) if os.path.isdir(d)])\n",
        "    if not fold_dirs:\n",
        "        raise FileNotFoundError('No fold model directories found (pattern: %s)' % model_glob)\n",
        "    print('[TEST] Using folds:', fold_dirs)\n",
        "\n",
        "    N = len(feats['input_ids'])\n",
        "    s_sum = None; e_sum = None\n",
        "    used = 0\n",
        "    device = torch.device('cpu') if force_cpu else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    for k, d in enumerate(fold_dirs):\n",
        "        print(f'[TEST] Loading {d} ...', flush=True)\n",
        "        model = None\n",
        "        # try standard then fallback loader; if both fail, skip this fold dir\n",
        "        try:\n",
        "            model = AutoModelForQuestionAnswering.from_pretrained(d, local_files_only=True, device_map='cpu', torch_dtype=torch.float32)\n",
        "        except Exception:\n",
        "            try:\n",
        "                model = _try_load_fold_model(d)\n",
        "            except Exception as e:\n",
        "                print(f'[TEST] Skip {d}: {repr(e)}');\n",
        "                continue\n",
        "        used += 1\n",
        "        model.eval(); model.to(device)\n",
        "        loader = torch.utils.data.DataLoader(test_ds, batch_size=8, shuffle=False, collate_fn=collator, pin_memory=False)\n",
        "        s_logits = [None]*N; e_logits = [None]*N\n",
        "        with torch.no_grad():\n",
        "            t1=time.time()\n",
        "            for step, batch in enumerate(loader):\n",
        "                feat_idx = batch.pop('feat_idx').cpu().numpy()\n",
        "                for kk in list(batch.keys()): batch[kk] = batch[kk].to(device)\n",
        "                out = model(**batch)\n",
        "                s = out.start_logits.detach().cpu().numpy(); e = out.end_logits.detach().cpu().numpy()\n",
        "                for j, fi in enumerate(feat_idx):\n",
        "                    s_logits[int(fi)] = s[j]; e_logits[int(fi)] = e[j]\n",
        "                if step % 50 == 0:\n",
        "                    print(f'  [TEST] fold {k} step {step}, {time.time()-t1:.1f}s', flush=True)\n",
        "        s_arr = np.stack(s_logits, axis=0); e_arr = np.stack(e_logits, axis=0)\n",
        "        if s_sum is None:\n",
        "            s_sum = s_arr; e_sum = e_arr\n",
        "        else:\n",
        "            s_sum += s_arr; e_sum += e_arr\n",
        "        del model, loader, s_logits, e_logits, s_arr, e_arr; torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "    if used == 0:\n",
        "        raise RuntimeError('No usable fold checkpoints found to run inference.')\n",
        "\n",
        "    s_avg = s_sum / used; e_avg = e_sum / used\n",
        "    pred_texts, pred_starts = pool_nbest_over_features(feats, test_df, s_avg, e_avg, rerank_with_gold=False)\n",
        "\n",
        "    sub = pd.DataFrame({id_col: test_df[id_col].values, pred_col: pred_texts})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('[TEST] Wrote submission.csv with', len(sub), 'rows in %.1fs (folds used=%d, device=%s)' % (time.time()-t0, used, str(device)))\n",
        "    return sub.head()\n",
        "\n",
        "print('Test inference helper ready: call predict_test_and_submit() after training completes.')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test inference helper ready: call predict_test_and_submit() after training completes.\n"
          ]
        }
      ]
    },
    {
      "id": "c2b4fe7e-1876-4802-86e2-e496ff9dc086",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# One-click full pipeline: 5-fold train -> test inference -> submission.csv\n",
        "import time, json, pathlib\n",
        "def run_full_train_and_submit():\n",
        "    t0=time.time()\n",
        "    print('[RUN] Starting 5-fold training...', flush=True)\n",
        "    oof_mean = train_5fold_x()\n",
        "    print('[RUN] 5-fold OOF Jaccard:', float(oof_mean))\n",
        "    pathlib.Path('metrics.json').write_text(json.dumps({'oof_jaccard': float(oof_mean)}, ensure_ascii=False))\n",
        "    print('[RUN] Inference on test and submission build...', flush=True)\n",
        "    head = predict_test_and_submit('xlmr_f*')\n",
        "    print(head)\n",
        "    print('[RUN] Done in %.1fs' % (time.time()-t0))\n",
        "    return float(oof_mean)\n",
        "\n",
        "print('Ready: after smoke success, call run_full_train_and_submit() to train 5 folds and create submission.csv.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "5f43ddf5-20bd-431e-8596-d8ed6717c18a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CPU fallback: generate submission using existing fold models\n",
        "try:\n",
        "    head = predict_test_and_submit('xlmr_f*')\n",
        "    print(head)\n",
        "except Exception as e:\n",
        "    print('Submission inference failed:', repr(e))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST] id_col=id, pred_col=PredictionString, test_rows=112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST] Using folds: ['xlmr_f0', 'xlmr_f1', 'xlmr_f2']\n[TEST] Loading xlmr_f0 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 0 step 0, 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 0 step 50, 6.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 0 step 100, 12.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 0 step 150, 18.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 0 step 200, 24.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 0 step 250, 30.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 0 step 300, 36.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 0 step 350, 43.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST] Loading xlmr_f1 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 1 step 0, 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 1 step 50, 6.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 1 step 100, 12.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 1 step 150, 18.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 1 step 200, 24.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 1 step 250, 30.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 1 step 300, 37.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 1 step 350, 43.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST] Loading xlmr_f2 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST] Skip xlmr_f2: FileNotFoundError('No loadable checkpoint in xlmr_f2')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST] Wrote submission.csv with 112 rows in 148.9s (folds used=2)\n          id               PredictionString\n0  be799d365              \u092e\u0941\u0902\u092c\u0908, \u092e\u0939\u093e\u0930\u093e\u0937\u094d\u091f\u094d\u0930\n1  26f356026  \u0909\u0926\u093e\u0938\u093f\u0928\u093e\u091a\u093e\u0930\u094d\u092f \u0938\u0941\u092e\u0947\u0930\u0941\u0926\u093e\u0938 \u092e\u0939\u093e\u0930\u093e\u091c\n2  57a56c43f                     brain stem\n3  da062fdbb                       \u092c\u093f\u0902\u092c\u093f\u0938\u093e\u0930\n4  72fc0d5b5                 20 \u0905\u092a\u094d\u0930\u0948\u0932 1889\n"
          ]
        }
      ]
    },
    {
      "id": "9273b998-fcbc-4031-861a-c66075037bf2",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ACTION REQUIRED: Restart Session Now to Unblock GPU\n",
        "\n",
        "The GPU is wedged (nvidia-smi shows ~10.9GB used with no processes). To proceed to a medal-capable run, please do:\n",
        "\n",
        "1) Power > Restart Session (clears phantom GPU processes).\n",
        "2) Run cells in order:\n",
        "- Cell 1: confirm GPU VRAM ~0 MiB used.\n",
        "- Cell 4: install cu121 PyTorch and HF stack.\n",
        "- Cell 7: initialize XLM-R base pipeline (HAS_BNB auto-detected).\n",
        "- Cell 16: span-labeling patch (coverage-based snapping).\n",
        "- Cell 20: simplified post-processing (n_best=100, max span=50).\n",
        "  - Optional: Cell 13 to install bitsandbytes, then re-run Cell 7 so HAS_BNB=True.\n",
        "\n",
        "3) Smoke test on GPU:\n",
        "- Run Cell 19 (epochs_override=3). Expect OOF fold0 \u2265 0.70.\n",
        "\n",
        "4) Full training (target medal):\n",
        "- Run train_5fold_x() in a new cell or Cell 24's run_full_train_and_submit().\n",
        "- Settings: max_length=384, doc_stride=128, fp16=True, gradient_checkpointing, group_by_length=True, AdamW 8-bit if BNB available, LR=2e-5, warmup_ratio=0.10, weight_decay=0.01, epochs=3.\n",
        "- Negatives: ~2x per positive (already implemented).\n",
        "\n",
        "5) Inference and submit:\n",
        "- After training, call predict_test_and_submit('xlmr_f*') to write submission.csv, then submit.\n",
        "\n",
        "If training time exceeds budget (>60 min per fold), stop after 4 folds and submit, or pivot to 3 folds with 2\u20133 seeds and average logits."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "d8730c84-226d-4153-ba32-b90635142035",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CPU inference: include smoke model too (may or may not help); writes submission.csv\n",
        "try:\n",
        "    head_alt = predict_test_and_submit('xlmr_*')\n",
        "    print(head_alt)\n",
        "except Exception as e:\n",
        "    print('Alt submission inference failed:', repr(e))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST] id_col=id, pred_col=PredictionString, test_rows=112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST] Using folds: ['xlmr_f0', 'xlmr_f1', 'xlmr_f2', 'xlmr_smoke_f0']\n[TEST] Loading xlmr_f0 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 0 step 0, 0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 0 step 50, 9.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 0 step 100, 20.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 0 step 150, 29.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 0 step 200, 35.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 0 step 250, 41.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 0 step 300, 47.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 0 step 350, 53.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST] Loading xlmr_f1 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 1 step 0, 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 1 step 50, 6.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 1 step 100, 12.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 1 step 150, 18.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 1 step 200, 24.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 1 step 250, 31.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 1 step 300, 37.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 1 step 350, 43.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST] Loading xlmr_f2 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST] Skip xlmr_f2: FileNotFoundError('No loadable checkpoint in xlmr_f2')\n[TEST] Loading xlmr_smoke_f0 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST] Skip xlmr_smoke_f0: FileNotFoundError('No loadable checkpoint in xlmr_smoke_f0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST] Wrote submission.csv with 112 rows in 164.7s (folds used=2)\n          id               PredictionString\n0  be799d365              \u092e\u0941\u0902\u092c\u0908, \u092e\u0939\u093e\u0930\u093e\u0937\u094d\u091f\u094d\u0930\n1  26f356026  \u0909\u0926\u093e\u0938\u093f\u0928\u093e\u091a\u093e\u0930\u094d\u092f \u0938\u0941\u092e\u0947\u0930\u0941\u0926\u093e\u0938 \u092e\u0939\u093e\u0930\u093e\u091c\n2  57a56c43f                     brain stem\n3  da062fdbb                       \u092c\u093f\u0902\u092c\u093f\u0938\u093e\u0930\n4  72fc0d5b5                 20 \u0905\u092a\u094d\u0930\u0948\u0932 1889\n"
          ]
        }
      ]
    },
    {
      "id": "da633895-e2e1-430b-96b3-22be425a62ed",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CPU inference with longer test context (TTA): bump max_length/doc_stride only for test features\n",
        "try:\n",
        "    old_max_len, old_stride = max_length, doc_stride\n",
        "except NameError:\n",
        "    old_max_len, old_stride = 384, 128\n",
        "try:\n",
        "    max_length, doc_stride = 512, 192  # slightly larger stride for more overlap/coverage\n",
        "    print(f'[TTA] Test-time lengths: max_length={max_length}, doc_stride={doc_stride}')\n",
        "    head_tta = predict_test_and_submit('xlmr_f*')\n",
        "    print(head_tta)\n",
        "finally:\n",
        "    max_length, doc_stride = old_max_len, old_stride\n",
        "    print(f'[TTA] Restored lengths: max_length={max_length}, doc_stride={doc_stride}')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TTA] Test-time lengths: max_length=512, doc_stride=192\n[TEST] id_col=id, pred_col=PredictionString, test_rows=112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST] Using folds: ['xlmr_f0', 'xlmr_f1', 'xlmr_f2']\n[TEST] Loading xlmr_f0 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 0 step 0, 0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 0 step 50, 37.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 0 step 100, 74.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST] Loading xlmr_f1 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 1 step 0, 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 1 step 50, 37.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [TEST] fold 1 step 100, 74.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST] Loading xlmr_f2 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST] Skip xlmr_f2: FileNotFoundError('No loadable checkpoint in xlmr_f2')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TEST] Wrote submission.csv with 112 rows in 225.7s (folds used=2)\n          id               PredictionString\n0  be799d365              \u092e\u0941\u0902\u092c\u0908, \u092e\u0939\u093e\u0930\u093e\u0937\u094d\u091f\u094d\u0930\n1  26f356026  \u0909\u0926\u093e\u0938\u093f\u0928\u093e\u091a\u093e\u0930\u094d\u092f \u0938\u0941\u092e\u0947\u0930\u0941\u0926\u093e\u0938 \u092e\u0939\u093e\u0930\u093e\u091c\n2  57a56c43f                          \u0baa\u0bc1\u0bb1\u0ba3\u0bbf\n3  da062fdbb                       \u092c\u093f\u0902\u092c\u093f\u0938\u093e\u0930\n4  72fc0d5b5                 \u0968\u0966 \u0905\u092a\u094d\u0930\u0948\u0932 \u0967\u096e\u096e\u096f\n[TTA] Restored lengths: max_length=256, doc_stride=128\n"
          ]
        }
      ]
    },
    {
      "id": "c57ebff3-f9a0-4272-9660-2f3cbfc941da",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CPU ensemble over two tokenization lengths: 256/128 vs 512/160, restricted to xlmr_f0/xlmr_f1 for speed.\n",
        "import os, glob, time, numpy as np, pandas as pd, torch, gc\n",
        "from transformers import AutoModelForQuestionAnswering, DataCollatorWithPadding\n",
        "\n",
        "def _pool_with_scores(features, examples_df, start_logits, end_logits):\n",
        "    sample_mapping = features['overflow_to_sample_mapping']\n",
        "    preds_text = [''] * len(examples_df)\n",
        "    preds_start = [0] * len(examples_df)\n",
        "    best_score = [-1e30] * len(examples_df)\n",
        "    for i in range(len(sample_mapping)):\n",
        "        ex_idx = int(sample_mapping[i])\n",
        "        offsets = features['offset_mapping'][i]\n",
        "        seq_ids = features.sequence_ids(i)\n",
        "        ctx_tokens = [j for j,sid in enumerate(seq_ids) if sid==1]\n",
        "        if not ctx_tokens:\n",
        "            continue\n",
        "        c0, c1 = ctx_tokens[0], ctx_tokens[-1]\n",
        "        s = np.array(start_logits[i], dtype=np.float32)\n",
        "        e = np.array(end_logits[i], dtype=np.float32)\n",
        "        s[:c0] = -1e9; e[:c0] = -1e9\n",
        "        if c1+1 < len(s):\n",
        "            s[c1+1:] = -1e9; e[c1+1:] = -1e9\n",
        "        start_idxes = np.argsort(s)[-n_best_size:][::-1]\n",
        "        end_idxes   = np.argsort(e)[-n_best_size:][::-1]\n",
        "        ctx = examples_df.loc[ex_idx, 'context']\n",
        "        qtext = str(examples_df.loc[ex_idx, 'question'])\n",
        "        q_has_digit = ('DIGIT_PAT' in globals()) and (DIGIT_PAT.search(qtext) is not None)\n",
        "        for si in start_idxes:\n",
        "            if si < c0 or si > c1: continue\n",
        "            for ei in end_idxes:\n",
        "                if ei < c0 or ei > c1 or ei < si: continue\n",
        "                if (ei - si + 1) > 50: continue\n",
        "                stc, enc = offsets[si][0], offsets[ei][1]\n",
        "                if stc is None or enc is None or enc <= stc: continue\n",
        "                text = edge_trim(ctx[stc:enc].strip())\n",
        "                if not text: continue\n",
        "                score = float(s[si] + e[ei])\n",
        "                if q_has_digit:\n",
        "                    cand_has_digit = DIGIT_PAT.search(text) is not None\n",
        "                    score += (0.02 if cand_has_digit else -0.02)\n",
        "                if score > best_score[ex_idx]:\n",
        "                    best_score[ex_idx] = score\n",
        "                    preds_text[ex_idx] = text\n",
        "                    preds_start[ex_idx] = stc\n",
        "    for i in range(len(preds_text)):\n",
        "        if preds_text[i] == '':\n",
        "            preds_text[i] = edge_trim(examples_df.loc[i, 'context'][:0])\n",
        "            preds_start[i] = 0\n",
        "            if best_score[i] < -1e20: best_score[i] = -1e20\n",
        "    return preds_text, preds_start, np.array(best_score, dtype=np.float32)\n",
        "\n",
        "def _infer_once_token_length(model_dirs, max_len, stride):\n",
        "    global max_length, doc_stride\n",
        "    old_max, old_stride = max_length, doc_stride\n",
        "    max_length, doc_stride = max_len, stride\n",
        "    try:\n",
        "        os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
        "        device = torch.device('cpu')\n",
        "        test_df = pd.read_csv('test.csv')\n",
        "        feats = prepare_features_only_x(test_df)\n",
        "        test_ds = QADataset(feats, with_labels=False)\n",
        "        collator = DataCollatorWithPadding(tokenizer_x, pad_to_multiple_of=None)\n",
        "        N = len(feats['input_ids'])\n",
        "        s_sum = None; e_sum = None; used = 0\n",
        "        for d in model_dirs:\n",
        "            try:\n",
        "                model = AutoModelForQuestionAnswering.from_pretrained(d, local_files_only=True, device_map='cpu', torch_dtype=torch.float32)\n",
        "            except Exception:\n",
        "                continue\n",
        "            used += 1\n",
        "            model.eval(); model.to(device)\n",
        "            loader = torch.utils.data.DataLoader(test_ds, batch_size=8, shuffle=False, collate_fn=collator, pin_memory=False)\n",
        "            s_logits = [None]*N; e_logits = [None]*N\n",
        "            t1 = time.time()\n",
        "            with torch.no_grad():\n",
        "                for step, batch in enumerate(loader):\n",
        "                    feat_idx = batch.pop('feat_idx').cpu().numpy()\n",
        "                    for k in list(batch.keys()): batch[k] = batch[k].to(device)\n",
        "                    out = model(**batch)\n",
        "                    s = out.start_logits.detach().cpu().numpy(); e = out.end_logits.detach().cpu().numpy()\n",
        "                    for j, fi in enumerate(feat_idx):\n",
        "                        s_logits[int(fi)] = s[j]; e_logits[int(fi)] = e[j]\n",
        "                    if step % 50 == 0:\n",
        "                        print(f'    [PASS len={max_len}] folddir {d}, step {step}, {time.time()-t1:.1f}s', flush=True)\n",
        "            s_arr = np.stack(s_logits, axis=0); e_arr = np.stack(e_logits, axis=0)\n",
        "            if s_sum is None: s_sum, e_sum = s_arr, e_arr\n",
        "            else: s_sum += s_arr; e_sum += e_arr\n",
        "            del model, loader, s_logits, e_logits, s_arr, e_arr; torch.cuda.empty_cache(); gc.collect()\n",
        "        if used == 0: raise RuntimeError('No usable fold checkpoints found.')\n",
        "        s_avg = s_sum / used; e_avg = e_sum / used\n",
        "        texts, starts, scores = _pool_with_scores(feats, test_df, s_avg, e_avg)\n",
        "        return test_df, texts, starts, scores\n",
        "    finally:\n",
        "        max_length, doc_stride = old_max, old_stride\n",
        "\n",
        "def cpu_dual_length_ensemble():\n",
        "    sub_tmpl = pd.read_csv('sample_submission.csv')\n",
        "    id_col = sub_tmpl.columns[0]; pred_col = sub_tmpl.columns[1]\n",
        "    # Restrict to known good folds for speed\n",
        "    model_dirs = [d for d in ['xlmr_f0','xlmr_f1'] if os.path.isdir(d)]\n",
        "    print('[DUAL] Using folds:', model_dirs)\n",
        "    # run short\n",
        "    print('[DUAL] Pass A: max_length=256, doc_stride=128')\n",
        "    A_df, A_texts, A_starts, A_scores = _infer_once_token_length(model_dirs, 256, 128)\n",
        "    # run long\n",
        "    print('[DUAL] Pass B: max_length=512, doc_stride=160')\n",
        "    B_df, B_texts, B_starts, B_scores = _infer_once_token_length(model_dirs, 512, 160)\n",
        "    assert list(A_df[id_col].values) == list(B_df[id_col].values)\n",
        "    # choose per-example by higher score\n",
        "    A_scores = np.asarray(A_scores); B_scores = np.asarray(B_scores)\n",
        "    take_B = B_scores > A_scores\n",
        "    final_texts = [B_texts[i] if take_B[i] else A_texts[i] for i in range(len(A_texts))]\n",
        "    sub = pd.DataFrame({id_col: A_df[id_col].values, pred_col: final_texts})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('[DUAL] Wrote submission.csv with', len(sub), 'rows. Chose B for', int(take_B.sum()), 'rows.')\n",
        "    return sub.head()\n",
        "\n",
        "head_dual = cpu_dual_length_ensemble()\n",
        "print(head_dual)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DUAL] Using folds: ['xlmr_f0', 'xlmr_f1']\n[DUAL] Pass A: max_length=256, doc_stride=128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [PASS len=256] folddir xlmr_f0, step 0, 2.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [PASS len=256] folddir xlmr_f0, step 50, 106.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [PASS len=256] folddir xlmr_f0, step 100, 211.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [PASS len=256] folddir xlmr_f0, step 150, 315.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [PASS len=256] folddir xlmr_f0, step 200, 419.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [PASS len=256] folddir xlmr_f0, step 250, 520.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [PASS len=256] folddir xlmr_f0, step 300, 616.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [PASS len=256] folddir xlmr_f0, step 350, 712.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [PASS len=256] folddir xlmr_f1, step 0, 2.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [PASS len=256] folddir xlmr_f1, step 50, 97.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [PASS len=256] folddir xlmr_f1, step 100, 191.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [PASS len=256] folddir xlmr_f1, step 150, 292.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [PASS len=256] folddir xlmr_f1, step 200, 400.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [PASS len=256] folddir xlmr_f1, step 250, 509.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [PASS len=256] folddir xlmr_f1, step 300, 606.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [PASS len=256] folddir xlmr_f1, step 350, 703.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DUAL] Pass B: max_length=512, doc_stride=160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [PASS len=512] folddir xlmr_f0, step 0, 6.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [PASS len=512] folddir xlmr_f0, step 50, 335.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [PASS len=512] folddir xlmr_f0, step 100, 663.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [PASS len=512] folddir xlmr_f1, step 0, 4.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [PASS len=512] folddir xlmr_f1, step 50, 236.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [PASS len=512] folddir xlmr_f1, step 100, 444.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DUAL] Wrote submission.csv with 112 rows. Chose B for 44 rows.\n          id               PredictionString\n0  be799d365              \u092e\u0941\u0902\u092c\u0908, \u092e\u0939\u093e\u0930\u093e\u0937\u094d\u091f\u094d\u0930\n1  26f356026  \u0909\u0926\u093e\u0938\u093f\u0928\u093e\u091a\u093e\u0930\u094d\u092f \u0938\u0941\u092e\u0947\u0930\u0941\u0926\u093e\u0938 \u092e\u0939\u093e\u0930\u093e\u091c\n2  57a56c43f                     brain stem\n3  da062fdbb                       \u092c\u093f\u0902\u092c\u093f\u0938\u093e\u0930\n4  72fc0d5b5                 20 \u0905\u092a\u094d\u0930\u0948\u0932 1889\n"
          ]
        }
      ]
    },
    {
      "id": "4040ba6b-14a3-4e1c-a376-888d08ca7e5e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CPU single-pass (512/160) with strong re-ranking as per expert advice\n",
        "import os, time, glob, gc, numpy as np, pandas as pd, torch\n",
        "from transformers import AutoModelForQuestionAnswering, DataCollatorWithPadding\n",
        "\n",
        "def _pool_with_scores_strong(features, examples_df, start_logits, end_logits,\n",
        "                             top_k=15, max_span_tokens=30):\n",
        "    sample_mapping = features['overflow_to_sample_mapping']\n",
        "    N_ex = len(examples_df)\n",
        "    preds_text = [''] * N_ex\n",
        "    preds_start = [0] * N_ex\n",
        "    best_score = np.full(N_ex, -1e30, dtype=np.float32)\n",
        "    best_len = np.full(N_ex, 10**9, dtype=np.int32)\n",
        "    best_text = [''] * N_ex\n",
        "    for i in range(len(sample_mapping)):\n",
        "        ex_idx = int(sample_mapping[i])\n",
        "        offsets = features['offset_mapping'][i]\n",
        "        seq_ids = features.sequence_ids(i)\n",
        "        ctx_tokens = [j for j, sid in enumerate(seq_ids) if sid == 1]\n",
        "        if not ctx_tokens:\n",
        "            continue\n",
        "        c0, c1 = ctx_tokens[0], ctx_tokens[-1]\n",
        "        s_log = np.asarray(start_logits[i], dtype=np.float32)\n",
        "        e_log = np.asarray(end_logits[i], dtype=np.float32)\n",
        "        # masked log-softmax probabilities within context\n",
        "        s_lp = _log_softmax_masked(s_log, c0, c1)\n",
        "        e_lp = _log_softmax_masked(e_log, c0, c1)\n",
        "        start_idxes = np.argsort(s_lp)[-top_k:][::-1]\n",
        "        end_idxes = np.argsort(e_lp)[-top_k:][::-1]\n",
        "        qtext = str(examples_df.loc[ex_idx, 'question'])\n",
        "        ctx = examples_df.loc[ex_idx, 'context']\n",
        "        q_has_digit = DIGIT_PAT.search(qtext) is not None\n",
        "        ctx_norm = norm_for_metric(ctx)\n",
        "        for si in start_idxes:\n",
        "            if si < c0 or si > c1: continue\n",
        "            for ei in end_idxes:\n",
        "                if ei < c0 or ei > c1 or ei < si: continue\n",
        "                tok_len = (ei - si + 1)\n",
        "                if tok_len > max_span_tokens: continue\n",
        "                stc, enc = offsets[si][0], offsets[ei][1]\n",
        "                if stc is None or enc is None or enc <= stc: continue\n",
        "                raw_span = ctx[stc:enc]\n",
        "                text = edge_trim(raw_span.strip())\n",
        "                if not text: continue\n",
        "                # boundary/penalties\n",
        "                left_ok = (stc == 0) or _is_punct(ctx[stc-1])\n",
        "                right_ok = (enc >= len(ctx)) or _is_punct(ctx[enc:enc+1])\n",
        "                lead_p = (raw_span and _is_punct(raw_span[0]))\n",
        "                trail_p = (raw_span and _is_punct(raw_span[-1]))\n",
        "                span_has_digit = DIGIT_PAT.search(text) is not None\n",
        "                # repetition bonus (normalized, digits mapped) - count >= 2\n",
        "                text_norm = norm_for_metric(text)\n",
        "                ctx_freq = (text_norm and ctx_norm.count(text_norm) >= 2)\n",
        "                score = float(s_lp[si] + e_lp[ei])\n",
        "                score += (-0.003 * tok_len)\n",
        "                if q_has_digit:\n",
        "                    score += (0.03 if span_has_digit else -0.03)\n",
        "                if left_ok: score += 0.02\n",
        "                if right_ok: score += 0.02\n",
        "                if lead_p: score -= 0.02\n",
        "                if trail_p: score -= 0.02\n",
        "                if text.endswith('\u0964') or text.endswith('\u0965'): score -= 0.03\n",
        "                if ctx_freq: score += 0.02\n",
        "                # update with tie-breaks\n",
        "                if score > best_score[ex_idx] + 1e-12:\n",
        "                    best_score[ex_idx] = score; best_len[ex_idx] = tok_len\n",
        "                    preds_text[ex_idx] = text; preds_start[ex_idx] = stc; best_text[ex_idx] = text\n",
        "                else:\n",
        "                    # if within 0.02 and substring relation, prefer shorter\n",
        "                    if abs(score - best_score[ex_idx]) <= 0.02:\n",
        "                        prev = best_text[ex_idx]\n",
        "                        if prev:\n",
        "                            if (text in prev or prev in text) and tok_len < best_len[ex_idx]:\n",
        "                                best_score[ex_idx] = score; best_len[ex_idx] = tok_len\n",
        "                                preds_text[ex_idx] = text; preds_start[ex_idx] = stc; best_text[ex_idx] = text\n",
        "    # fallbacks\n",
        "    for i in range(N_ex):\n",
        "        if preds_text[i] == '':\n",
        "            preds_text[i] = edge_trim(examples_df.loc[i, 'context'][:0])\n",
        "            preds_start[i] = 0\n",
        "    return preds_text, preds_start, best_score\n",
        "\n",
        "def cpu_single_pass_rank_and_submit():\n",
        "    # single pass len=512, stride=160; average logits over available two folds\n",
        "    sub_tmpl = pd.read_csv('sample_submission.csv')\n",
        "    id_col = sub_tmpl.columns[0]; pred_col = sub_tmpl.columns[1]\n",
        "    model_dirs = [d for d in ['xlmr_f0','xlmr_f1'] if os.path.isdir(d)]\n",
        "    if not model_dirs:\n",
        "        raise FileNotFoundError('No usable fold dirs among xlmr_f0/xlmr_f1')\n",
        "    print('[SINGLE] Using folds:', model_dirs, flush=True)\n",
        "    # override seq len temporarly\n",
        "    global max_length, doc_stride\n",
        "    old_len, old_stride = max_length, doc_stride\n",
        "    max_length, doc_stride = 512, 160\n",
        "    try:\n",
        "        os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
        "        device = torch.device('cpu')\n",
        "        test_df = pd.read_csv('test.csv')\n",
        "        feats = prepare_features_only_x(test_df)\n",
        "        test_ds = QADataset(feats, with_labels=False)\n",
        "        collator = DataCollatorWithPadding(tokenizer_x, pad_to_multiple_of=None)\n",
        "        N = len(feats['input_ids'])\n",
        "        s_sum = None; e_sum = None; used = 0\n",
        "        t0 = time.time()\n",
        "        for d in model_dirs:\n",
        "            try:\n",
        "                model = AutoModelForQuestionAnswering.from_pretrained(d, local_files_only=True, device_map='cpu', torch_dtype=torch.float32)\n",
        "            except Exception as e:\n",
        "                print('[SINGLE] Skip', d, repr(e)); continue\n",
        "            used += 1\n",
        "            model.eval(); model.to(device)\n",
        "            loader = torch.utils.data.DataLoader(test_ds, batch_size=8, shuffle=False, collate_fn=collator, pin_memory=False)\n",
        "            s_logits = [None]*N; e_logits = [None]*N\n",
        "            t1=time.time()\n",
        "            with torch.no_grad():\n",
        "                for step, batch in enumerate(loader):\n",
        "                    feat_idx = batch.pop('feat_idx').cpu().numpy()\n",
        "                    for k in list(batch.keys()): batch[k] = batch[k].to(device)\n",
        "                    out = model(**batch)\n",
        "                    s = out.start_logits.detach().cpu().numpy(); e = out.end_logits.detach().cpu().numpy()\n",
        "                    for j, fi in enumerate(feat_idx):\n",
        "                        s_logits[int(fi)] = s[j]; e_logits[int(fi)] = e[j]\n",
        "                    if step % 50 == 0:\n",
        "                        print(f'  [SINGLE] {d} step {step}, {time.time()-t1:.1f}s', flush=True)\n",
        "            s_arr = np.stack(s_logits, axis=0); e_arr = np.stack(e_logits, axis=0)\n",
        "            if s_sum is None: s_sum, e_sum = s_arr, e_arr\n",
        "            else: s_sum += s_arr; e_sum += e_arr\n",
        "            del model, loader, s_logits, e_logits, s_arr, e_arr; torch.cuda.empty_cache(); gc.collect()\n",
        "        if used == 0:\n",
        "            raise RuntimeError('No models loaded for inference.')\n",
        "        s_avg = s_sum / used; e_avg = e_sum / used\n",
        "        print('[SINGLE] Pooling with strong reranker...', flush=True)\n",
        "        texts, starts, scores = _pool_with_scores_strong(feats, test_df, s_avg, e_avg, top_k=15, max_span_tokens=30)\n",
        "        sub = pd.DataFrame({id_col: test_df[id_col].values, pred_col: texts})\n",
        "        sub.to_csv('submission.csv', index=False)\n",
        "        print('[SINGLE] Wrote submission.csv with', len(sub), 'rows in %.1fs' % (time.time()-t0))\n",
        "        return sub.head()\n",
        "    finally:\n",
        "        max_length, doc_stride = old_len, old_stride\n",
        "\n",
        "head_single = cpu_single_pass_rank_and_submit()\n",
        "print(head_single)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SINGLE] Using folds: ['xlmr_f0', 'xlmr_f1']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [SINGLE] xlmr_f0 step 0, 4.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [SINGLE] xlmr_f0 step 50, 205.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [SINGLE] xlmr_f0 step 100, 395.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [SINGLE] xlmr_f1 step 0, 4.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [SINGLE] xlmr_f1 step 50, 189.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [SINGLE] xlmr_f1 step 100, 377.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SINGLE] Pooling with strong reranker...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SINGLE] Wrote submission.csv with 112 rows in 1022.3s\n          id               PredictionString\n0  be799d365              \u092e\u0941\u0902\u092c\u0908, \u092e\u0939\u093e\u0930\u093e\u0937\u094d\u091f\u094d\u0930\n1  26f356026  \u0909\u0926\u093e\u0938\u093f\u0928\u093e\u091a\u093e\u0930\u094d\u092f \u0938\u0941\u092e\u0947\u0930\u0941\u0926\u093e\u0938 \u092e\u0939\u093e\u0930\u093e\u091c\n2  57a56c43f          1260 \u0b95\u0ba9 \u0b9a\u0bc6\u0ba9\u0bcd\u0b9f\u0bbf\u0bae\u0bc0\u0b9f\u0bcd\u0b9f\u0bb0\u0bcd\n3  da062fdbb                       \u092c\u093f\u0902\u092c\u093f\u0938\u093e\u0930\n4  72fc0d5b5                           1889\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}