{
  "cells": [
    {
      "id": "8e225d05-df8b-49bf-9c47-4250087eed4b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "def pip(*args):\n",
        "    print('>', *args, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "# Set PIP_TARGET to writable directory\n",
        "pip_target = '/app/.pip-target'\n",
        "os.environ['PIP_TARGET'] = pip_target\n",
        "if os.path.exists(pip_target):\n",
        "    print('Removing existing', pip_target)\n",
        "    shutil.rmtree(pip_target, ignore_errors=True)\n",
        "\n",
        "# 0) Hard reset any prior torch stacks\n",
        "for pkg in ('torch', 'torchvision', 'torchaudio'):\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', pkg], check=False)\n",
        "\n",
        "# Clean stray site dirs\n",
        "for d in (\n",
        "    f'{pip_target}/torch',\n",
        "    f'{pip_target}/torch-2.8.0.dist-info',\n",
        "    f'{pip_target}/torch-2.4.1.dist-info',\n",
        "    f'{pip_target}/torchvision',\n",
        "    f'{pip_target}/torchvision-0.23.0.dist-info',\n",
        "    f'{pip_target}/torchvision-0.19.1.dist-info',\n",
        "    f'{pip_target}/torchaudio',\n",
        "    f'{pip_target}/torchaudio-2.8.0.dist-info',\n",
        "    f'{pip_target}/torchaudio-2.4.1.dist-info',\n",
        "    f'{pip_target}/torchgen',\n",
        "    f'{pip_target}/functorch',\n",
        "):\n",
        "    if os.path.exists(d):\n",
        "        print('Removing', d)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "# 1) Install the EXACT cu121 torch stack FIRST with --no-deps to avoid system dir installs\n",
        "pip('install',\n",
        "    '--index-url', 'https://download.pytorch.org/whl/cu121',\n",
        "    '--extra-index-url', 'https://pypi.org/simple',\n",
        "    '--force-reinstall', '--no-deps',\n",
        "    'torch==2.4.1', 'torchvision==0.19.1', 'torchaudio==2.4.1')\n",
        "\n",
        "# 2) Create a constraints file\n",
        "Path('constraints.txt').write_text(\n",
        "    'torch==2.4.1\\n'\n",
        "    'torchvision==0.19.1\\n'\n",
        "    'torchaudio==2.4.1\\n'\n",
        ")\n",
        "\n",
        "# 3) Install NON-torch deps\n",
        "pip('install', '-c', 'constraints.txt',\n",
        "    'transformers==4.44.2', 'accelerate==0.34.2',\n",
        "    'datasets==2.21.0', 'evaluate==0.4.2',\n",
        "    'sentencepiece', 'scikit-learn',\n",
        "    '--upgrade-strategy', 'only-if-needed')\n",
        "\n",
        "# 4) Sanity gate - add pip_target to sys.path\n",
        "sys.path.insert(0, pip_target)\n",
        "import torch\n",
        "print('torch:', torch.__version__, 'built CUDA:', getattr(torch.version, 'cuda', None))\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "assert str(getattr(torch.version,'cuda','')).startswith('12.1'), f'Wrong CUDA build: {torch.version.cuda}'\n",
        "assert torch.cuda.is_available(), 'CUDA not available'\n",
        "print('GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# Install additional packages with PIP_TARGET\n",
        "pip('install', 'rank_bm25')\n",
        "pip('install', 'langdetect')\n",
        "pip('install', 'indic-nlp-library', 'pyarrow')\n",
        "\n",
        "# Downgrade fsspec\n",
        "pip('install', '-c', 'constraints.txt', 'fsspec[http]<=2024.6.1,>=2023.1.0', '--upgrade')\n",
        "\n",
        "# Verify additional imports\n",
        "try:\n",
        "    from rank_bm25 import BM25Okapi\n",
        "    print('BM25 available')\n",
        "except ImportError:\n",
        "    print('BM25 not available')\n",
        "try:\n",
        "    from langdetect import detect\n",
        "    print('langdetect available')\n",
        "except ImportError:\n",
        "    print('langdetect not available')\n",
        "print('Environment setup complete')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing existing /app/.pip-target\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torch as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torchvision as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> install --index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://pypi.org/simple --force-reinstall --no-deps torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torchaudio as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121, https://pypi.org/simple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (799.0 MB)\n"
          ]
        }
      ]
    },
    {
      "id": "a2160ba8-209f-44e6-9e15-3c88fe294b55",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "import gc\n",
        "import ast\n",
        "import sys\n",
        "import copy\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, TensorDataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoConfig,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    )\n",
        "from transformers import default_data_collator\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import hashlib\n",
        "\n",
        "import subprocess\n",
        "import shutil\n",
        "import unicodedata\n",
        "\n",
        "# Add pip_target to sys.path if not already\n",
        "pip_target = '/app/.pip-target'\n",
        "if pip_target not in sys.path:\n",
        "    sys.path.insert(0, pip_target)\n",
        "\n",
        "# BM25 and langdetect\n",
        "BM25_AVAILABLE = False\n",
        "try:\n",
        "    from rank_bm25 import BM25Okapi\n",
        "    BM25_AVAILABLE = True\n",
        "    print('BM25 available')\n",
        "except ImportError:\n",
        "    print('BM25 not available, falling back to TF-IDF only')\n",
        "\n",
        "LANGDETECT_AVAILABLE = False\n",
        "try:\n",
        "    from langdetect import detect\n",
        "    LANGDETECT_AVAILABLE = True\n",
        "    print('langdetect available')\n",
        "except ImportError:\n",
        "    print('langdetect not available, using script fallback')\n",
        "\n",
        "# Script-based language detection fallback\n",
        "def detect_lang(text):\n",
        "    if not isinstance(text, str):\n",
        "        return 'hindi'\n",
        "    for c in text:\n",
        "        if 0x0B80 <= ord(c) <= 0x0BFF:  # Tamil Unicode range\n",
        "            return 'tamil'\n",
        "    return 'hindi'\n",
        "\n",
        "# Set seeds\n",
        "def set_seed(seed=123):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed(123)\n",
        "\n",
        "# Constants with coach tweaks for seed 123\n",
        "DEBUG = False  # Set to True for rapid prototyping\n",
        "MAX_LEN = 512\n",
        "DOC_STRIDE = 128\n",
        "N_SPLITS = 5\n",
        "BATCH_SIZE = 2\n",
        "GRAD_ACCUM_STEPS = 12\n",
        "EPOCHS = 4\n",
        "LR = 2.5e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "NEG_WEIGHT = 0.3\n",
        "USE_RETRIEVAL = True\n",
        "TOP_K_CHUNKS_TRAIN = 12\n",
        "TOP_K_CHUNKS_EVAL_HINDI = 10\n",
        "TOP_K_CHUNKS_EVAL_TAMIL = 35  # Coach tweak for better Tamil recall\n",
        "CHUNK_SIZE = 1800\n",
        "OVERLAP = 250\n",
        "NEG_POS_RATIO = 2\n",
        "MODEL_NAME = 'deepset/xlm-roberta-large-squad2'\n",
        "PUNCT = '\\u0964,.\\uff0c!\\uff01?\\uff1f\"\\\\\\'\\u201c\\u201d\\u2018\\u2019()[\\]{}:;'\n",
        "MAX_ANSWER_LENGTH = 80  # Coach tweak for longer spans\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "if DEBUG:\n",
        "    train_df = train_df.sample(n=200, random_state=123).reset_index(drop=True)\n",
        "    print(f'DEBUG mode: using {len(train_df)} samples')\n",
        "else:\n",
        "    print(f'Full mode: using {len(train_df)} samples')\n",
        "\n",
        "print('Train shape:', train_df.shape)\n",
        "print('Test shape:', test_df.shape)\n",
        "\n",
        "# Label alignment fix with progress tracking\n",
        "print('Before fix_span')\n",
        "def fix_span(row):\n",
        "    ctx, ans, s = row['context'], row['answer_text'], row['answer_start']\n",
        "    if s < 0 or ctx[s:s+len(ans)] != ans:\n",
        "        idx = ctx.find(ans)\n",
        "        if idx != -1:\n",
        "            row['answer_start'] = idx\n",
        "    return row\n",
        "\n",
        "train_df = train_df.apply(fix_span, axis=1)\n",
        "print('After fix_span')\n",
        "\n",
        "# Context groups for CV (hash first 1024 chars to group same articles)\n",
        "def get_context_hash(context):\n",
        "    return hashlib.md5(context[:1024].encode()).hexdigest()\n",
        "\n",
        "train_df['context_hash'] = train_df['context'].apply(get_context_hash)\n",
        "print('Context hashes computed')\n",
        "\n",
        "# Jaccard metric with NFKC normalization\n",
        "def jaccard_word(pred, true):\n",
        "    pred = unicodedata.normalize('NFKC', pred).lower()\n",
        "    true = unicodedata.normalize('NFKC', true).lower()\n",
        "    if not pred or not true:\n",
        "        return 0.0\n",
        "    pw, tw = set(pred.split()), set(true.split())\n",
        "    return len(pw & tw) / len(pw | tw) if pw and tw else 0.0\n",
        "\n",
        "def compute_jaccard(preds, trues):\n",
        "    return np.mean([jaccard_word(p, t) for p, t in zip(preds, trues)])\n",
        "\n",
        "# Assign language to test_df using langdetect or fallback\n",
        "print('Assigning language to test_df...')\n",
        "if LANGDETECT_AVAILABLE:\n",
        "    test_df['language'] = test_df['question'].apply(lambda x: {'ta':'tamil','hi':'hindi'}.get(detect(x), 'hindi') if isinstance(x, str) else 'hindi')\n",
        "else:\n",
        "    test_df['language'] = test_df['question'].apply(detect_lang)\n",
        "print('Test language dist:', test_df['language'].value_counts())\n",
        "\n",
        "# CV splitting with StratifiedGroupKFold\n",
        "sgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=123)\n",
        "train_df['fold'] = -1\n",
        "for fold, (trn_idx, val_idx) in enumerate(sgkf.split(train_df, train_df['language'], groups=train_df['context_hash'])):\n",
        "    train_df.loc[val_idx, 'fold'] = fold\n",
        "\n",
        "print('Fold distribution:')\n",
        "print(train_df.groupby(['fold', 'language']).size())\n",
        "print(f'Folds created: {train_df[\"fold\"].nunique()}')\n",
        "\n",
        "N_FOLDS = 3 if DEBUG else N_SPLITS\n",
        "print(f'Using {N_FOLDS} folds for training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "c4449d50-ba91-41cc-9b98-33dbe229ad27",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "print('Tokenizer loaded:', tokenizer.name_or_path)\n",
        "\n",
        "# TF-IDF Retrieval setup with language-specific vectorizers\n",
        "if USE_RETRIEVAL:\n",
        "    print('Fitting language-specific TF-IDF vectorizers...')\n",
        "    hindi_df = train_df[train_df['language'] == 'hindi']\n",
        "    tamil_df = train_df[train_df['language'] == 'tamil']\n",
        "    \n",
        "    # Hindi vectorizer\n",
        "    print('Processing Hindi...')\n",
        "    hindi_questions = hindi_df['question'].tolist()\n",
        "    hindi_contexts = hindi_df['context'].tolist()\n",
        "    hindi_chunks = []\n",
        "    for ctx in tqdm(hindi_contexts, desc='Chunking Hindi contexts'):\n",
        "        chunks = []\n",
        "        for i in range(0, len(ctx), CHUNK_SIZE - OVERLAP):\n",
        "            chunk = ctx[i:i + CHUNK_SIZE]\n",
        "            if len(chunk) > 100:\n",
        "                chunks.append(chunk)\n",
        "        hindi_chunks.extend(chunks)\n",
        "    print(f'Hindi chunks total: {len(hindi_chunks)}')\n",
        "    hindi_corpus = hindi_questions + random.sample(hindi_chunks, min(3000, len(hindi_chunks)))\n",
        "    print(f'Hindi corpus size: {len(hindi_corpus)}')\n",
        "    hindi_vectorizer = TfidfVectorizer(\n",
        "        analyzer='char_wb',\n",
        "        ngram_range=(2, 4),\n",
        "        max_features=5000,\n",
        "        min_df=2,\n",
        "        max_df=0.95,\n",
        "        lowercase=False,\n",
        "        sublinear_tf=True,\n",
        "        dtype=np.float32\n",
        "    )\n",
        "    print('Fitting Hindi vectorizer...')\n",
        "    start_time = time.time()\n",
        "    hindi_vectorizer.fit(hindi_corpus)\n",
        "    fit_time = time.time() - start_time\n",
        "    print(f'Hindi TF-IDF fitted in {fit_time:.2f}s: {len(hindi_corpus)} docs')\n",
        "    \n",
        "    # Tamil vectorizer - fixed to char n-grams for better recall\n",
        "    print('Processing Tamil...')\n",
        "    tamil_questions = tamil_df['question'].tolist()\n",
        "    tamil_contexts = tamil_df['context'].tolist()\n",
        "    tamil_chunks = []\n",
        "    for ctx in tqdm(tamil_contexts, desc='Chunking Tamil contexts'):\n",
        "        chunks = []\n",
        "        for i in range(0, len(ctx), CHUNK_SIZE - OVERLAP):\n",
        "            chunk = ctx[i:i + CHUNK_SIZE]\n",
        "            if len(chunk) > 100:\n",
        "                chunks.append(chunk)\n",
        "        tamil_chunks.extend(chunks)\n",
        "    print(f'Tamil chunks total: {len(tamil_chunks)}')\n",
        "    tamil_corpus = tamil_questions + random.sample(tamil_chunks, min(1500, len(tamil_chunks)))\n",
        "    print(f'Tamil corpus size: {len(tamil_corpus)}')\n",
        "    tamil_vectorizer = TfidfVectorizer(\n",
        "        analyzer='char_wb',\n",
        "        ngram_range=(3, 5),\n",
        "        max_features=15000,\n",
        "        min_df=3,\n",
        "        max_df=0.9,\n",
        "        lowercase=False,\n",
        "        sublinear_tf=True,\n",
        "        dtype=np.float32\n",
        "    )\n",
        "    print('Fitting Tamil vectorizer...')\n",
        "    start_time = time.time()\n",
        "    tamil_vectorizer.fit(tamil_corpus)\n",
        "    fit_time = time.time() - start_time\n",
        "    print(f'Tamil TF-IDF fitted in {fit_time:.2f}s: {len(tamil_corpus)} docs')\n",
        "else:\n",
        "    hindi_vectorizer = tamil_vectorizer = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "ef50bb1f-e22d-4d40-81db-ab23385437ab",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare training features with hybrid retrieval and sliding windows\n",
        "def prepare_train_features(examples, neg_pos_ratio=NEG_POS_RATIO):\n",
        "    features = []\n",
        "    for ex in examples:\n",
        "        q, ctx, ans, ex_id, lang = ex['question'].strip(), ex['context'].strip(), {'text': ex['answer_text'], 'answer_start': ex['answer_start']}, ex['id'], ex['language']\n",
        "        \n",
        "        if USE_RETRIEVAL:\n",
        "            # Chunk context\n",
        "            chunks = []\n",
        "            chunk_starts = []\n",
        "            for i in range(0, len(ctx), CHUNK_SIZE - OVERLAP):\n",
        "                chunk = ctx[i:i + CHUNK_SIZE]\n",
        "                if len(chunk) > 100:\n",
        "                    chunks.append(chunk)\n",
        "                    chunk_starts.append(i)\n",
        "            \n",
        "            if not chunks:\n",
        "                continue\n",
        "            \n",
        "            # Select vectorizer by language\n",
        "            if lang == 'hindi':\n",
        "                vectorizer = hindi_vectorizer\n",
        "            else:\n",
        "                vectorizer = tamil_vectorizer\n",
        "            \n",
        "            # TF-IDF retrieval\n",
        "            q_vec = vectorizer.transform([q])\n",
        "            chunk_vecs = vectorizer.transform(chunks)\n",
        "            similarities = cosine_similarity(q_vec, chunk_vecs).flatten()\n",
        "            \n",
        "            # BM25 hybrid if available\n",
        "            if BM25_AVAILABLE:\n",
        "                tokenized_chunks = [chunk.lower().split() for chunk in chunks]\n",
        "                bm25 = BM25Okapi(tokenized_chunks)\n",
        "                q_tokens = q.lower().split()\n",
        "                bm25_scores = bm25.get_scores(q_tokens)\n",
        "                if np.max(bm25_scores) > 0:\n",
        "                    norm_bm25 = bm25_scores / np.max(bm25_scores)\n",
        "                else:\n",
        "                    norm_bm25 = np.zeros_like(bm25_scores)\n",
        "                hybrid_scores = 0.5 * norm_bm25 + 0.5 * similarities\n",
        "            else:\n",
        "                hybrid_scores = similarities\n",
        "            top_indices = np.argsort(hybrid_scores)[-TOP_K_CHUNKS_TRAIN:]\n",
        "            \n",
        "            # Guarantee gold chunk inclusion for training by replacing lowest sim if needed\n",
        "            start_char = ans['answer_start']\n",
        "            end_char = start_char + len(ans['text'])\n",
        "            pos_idx = None\n",
        "            for ci, st in enumerate(chunk_starts):\n",
        "                if start_char >= st and end_char <= st + len(chunks[ci]):\n",
        "                    pos_idx = ci\n",
        "                    break\n",
        "            if pos_idx is not None and pos_idx not in top_indices:\n",
        "                # Replace the lowest hybrid score in top_indices with pos_idx\n",
        "                min_hybrid_arg = np.argmin(hybrid_scores[top_indices])\n",
        "                top_indices[min_hybrid_arg] = pos_idx\n",
        "            # Sort by hybrid descending\n",
        "            sort_args = np.argsort(hybrid_scores[top_indices])[::-1]\n",
        "            top_indices = top_indices[sort_args]\n",
        "            \n",
        "            # Get top chunks with their global start positions\n",
        "            top_chunks = [(hybrid_scores[idx], chunk_starts[idx], chunks[idx]) for idx in top_indices]\n",
        "        else:\n",
        "            top_chunks = [(1.0, 0, ctx)]  # full context if no retrieval\n",
        "        \n",
        "        # Now process each top chunk with sliding windows\n",
        "        pos_feats, neg_feats = [], []\n",
        "        for sim, chunk_start, chunk in top_chunks:\n",
        "            tokenized = tokenizer(\n",
        "                q,\n",
        "                chunk,\n",
        "                truncation='only_second',\n",
        "                max_length=MAX_LEN,\n",
        "                stride=DOC_STRIDE,\n",
        "                return_overflowing_tokens=True,\n",
        "                return_offsets_mapping=True,\n",
        "                padding=False,\n",
        "            )\n",
        "            \n",
        "            for j in range(len(tokenized['input_ids'])):\n",
        "                input_ids = tokenized['input_ids'][j]\n",
        "                attention_mask = tokenized['attention_mask'][j]\n",
        "                offsets = tokenized['offset_mapping'][j]\n",
        "                sequence_ids = tokenized.sequence_ids(j)\n",
        "                \n",
        "                # Skip windows without context tokens\n",
        "                if 1 not in sequence_ids:\n",
        "                    continue\n",
        "                \n",
        "                # Global offsets: add chunk_start to context offsets\n",
        "                global_offsets = []\n",
        "                ctx_start = 0\n",
        "                while ctx_start < len(sequence_ids) and sequence_ids[ctx_start] != 1:\n",
        "                    global_offsets.append(None)\n",
        "                    ctx_start += 1\n",
        "                while ctx_start < len(sequence_ids) and sequence_ids[ctx_start] == 1:\n",
        "                    local_offset = offsets[ctx_start]\n",
        "                    global_offset = (local_offset[0] + chunk_start, local_offset[1] + chunk_start) if local_offset else None\n",
        "                    global_offsets.append(global_offset)\n",
        "                    ctx_start += 1\n",
        "                while ctx_start < len(sequence_ids):\n",
        "                    global_offsets.append(None)\n",
        "                    ctx_start += 1\n",
        "                \n",
        "                # Find start/end positions using global offsets\n",
        "                start_pos = -1\n",
        "                end_pos = -1\n",
        "                is_positive = False\n",
        "                start_char = ans['answer_start']\n",
        "                end_char = start_char + len(ans['text'])\n",
        "                \n",
        "                for tok_idx, off in enumerate(global_offsets):\n",
        "                    if off is not None and off[0] <= start_char < off[1]:\n",
        "                        start_pos = tok_idx\n",
        "                    if off is not None and off[0] < end_char <= off[1]:\n",
        "                        end_pos = tok_idx\n",
        "                if start_pos != -1 and end_pos != -1 and end_pos >= start_pos:\n",
        "                    is_positive = True\n",
        "                else:\n",
        "                    start_pos = 0\n",
        "                    end_pos = 0\n",
        "                \n",
        "                # Pad/truncate\n",
        "                pad_len = MAX_LEN - len(input_ids)\n",
        "                if pad_len > 0:\n",
        "                    input_ids += [tokenizer.pad_token_id] * pad_len\n",
        "                    attention_mask += [0] * pad_len\n",
        "                else:\n",
        "                    input_ids = input_ids[:MAX_LEN]\n",
        "                    attention_mask = attention_mask[:MAX_LEN]\n",
        "                \n",
        "                feat = {\n",
        "                    'input_ids': input_ids,\n",
        "                    'attention_mask': attention_mask,\n",
        "                    'start_positions': start_pos,\n",
        "                    'end_positions': end_pos,\n",
        "                    'example_id': ex_id,\n",
        "                    'is_positive': is_positive\n",
        "                }\n",
        "                (pos_feats if is_positive else neg_feats).append(feat)\n",
        "        \n",
        "        # Oversample positives as per expert advice\n",
        "        if pos_feats:\n",
        "            pos_feats = pos_feats * 2\n",
        "        \n",
        "        # Cap negatives\n",
        "        if pos_feats:\n",
        "            features.extend(pos_feats)\n",
        "            random.shuffle(neg_feats)\n",
        "            n_neg = min(len(neg_feats), neg_pos_ratio * len(pos_feats))\n",
        "            features.extend(neg_feats[:n_neg])\n",
        "        elif neg_feats:\n",
        "            features.append(random.choice(neg_feats))\n",
        "    return features\n",
        "\n",
        "# Prepare validation features (lang-specific TOP_K_EVAL)\n",
        "def prepare_validation_features(examples):\n",
        "    features = []\n",
        "    for ex in examples:\n",
        "        q, ctx, ex_id, lang = ex['question'].strip(), ex['context'].strip(), ex['id'], ex['language']\n",
        "        \n",
        "        if USE_RETRIEVAL:\n",
        "            # Same chunking and retrieval as train, but use lang-specific TOP_K_EVAL\n",
        "            chunks = []\n",
        "            chunk_starts = []\n",
        "            for i in range(0, len(ctx), CHUNK_SIZE - OVERLAP):\n",
        "                chunk = ctx[i:i + CHUNK_SIZE]\n",
        "                if len(chunk) > 100:\n",
        "                    chunks.append(chunk)\n",
        "                    chunk_starts.append(i)\n",
        "            \n",
        "            if not chunks:\n",
        "                continue\n",
        "            \n",
        "            # Select vectorizer by language\n",
        "            if lang == 'hindi':\n",
        "                vectorizer = hindi_vectorizer\n",
        "                top_k_eval = TOP_K_CHUNKS_EVAL_HINDI\n",
        "            else:\n",
        "                vectorizer = tamil_vectorizer\n",
        "                top_k_eval = TOP_K_CHUNKS_EVAL_TAMIL\n",
        "            \n",
        "            # TF-IDF\n",
        "            q_vec = vectorizer.transform([q])\n",
        "            chunk_vecs = vectorizer.transform(chunks)\n",
        "            similarities = cosine_similarity(q_vec, chunk_vecs).flatten()\n",
        "            \n",
        "            # BM25 hybrid if available\n",
        "            if BM25_AVAILABLE:\n",
        "                tokenized_chunks = [chunk.lower().split() for chunk in chunks]\n",
        "                bm25 = BM25Okapi(tokenized_chunks)\n",
        "                q_tokens = q.lower().split()\n",
        "                bm25_scores = bm25.get_scores(q_tokens)\n",
        "                if np.max(bm25_scores) > 0:\n",
        "                    norm_bm25 = bm25_scores / np.max(bm25_scores)\n",
        "                else:\n",
        "                    norm_bm25 = np.zeros_like(bm25_scores)\n",
        "                hybrid_scores = 0.5 * norm_bm25 + 0.5 * similarities\n",
        "            else:\n",
        "                hybrid_scores = similarities\n",
        "            top_indices = np.argsort(hybrid_scores)[-top_k_eval:]\n",
        "            top_chunks = [(hybrid_scores[idx], chunk_starts[idx], chunks[idx]) for idx in top_indices]\n",
        "        else:\n",
        "            top_chunks = [(1.0, 0, ctx)]\n",
        "        \n",
        "        # Process each top chunk\n",
        "        for sim, chunk_start, chunk in top_chunks:\n",
        "            tokenized = tokenizer(\n",
        "                q,\n",
        "                chunk,\n",
        "                truncation='only_second',\n",
        "                max_length=MAX_LEN,\n",
        "                stride=DOC_STRIDE,\n",
        "                return_overflowing_tokens=True,\n",
        "                return_offsets_mapping=True,\n",
        "                padding=False,\n",
        "            )\n",
        "            \n",
        "            for j in range(len(tokenized['input_ids'])):\n",
        "                input_ids = tokenized['input_ids'][j]\n",
        "                attention_mask = tokenized['attention_mask'][j]\n",
        "                offsets = tokenized['offset_mapping'][j]\n",
        "                sequence_ids = tokenized.sequence_ids(j)\n",
        "                \n",
        "                # Skip windows without context tokens\n",
        "                if 1 not in sequence_ids:\n",
        "                    continue\n",
        "                \n",
        "                # Global offsets for post-processing\n",
        "                global_offsets = []\n",
        "                ctx_start = 0\n",
        "                while ctx_start < len(sequence_ids) and sequence_ids[ctx_start] != 1:\n",
        "                    global_offsets.append(None)\n",
        "                    ctx_start += 1\n",
        "                while ctx_start < len(sequence_ids) and sequence_ids[ctx_start] == 1:\n",
        "                    local_offset = offsets[ctx_start]\n",
        "                    global_offset = (local_offset[0] + chunk_start, local_offset[1] + chunk_start) if local_offset else None\n",
        "                    global_offsets.append(global_offset)\n",
        "                    ctx_start += 1\n",
        "                while ctx_start < len(sequence_ids):\n",
        "                    global_offsets.append(None)\n",
        "                    ctx_start += 1\n",
        "                \n",
        "                # Pad/truncate\n",
        "                pad_len = MAX_LEN - len(input_ids)\n",
        "                if pad_len > 0:\n",
        "                    input_ids += [tokenizer.pad_token_id] * pad_len\n",
        "                    attention_mask += [0] * pad_len\n",
        "                    global_offsets += [None] * pad_len\n",
        "                else:\n",
        "                    input_ids = input_ids[:MAX_LEN]\n",
        "                    attention_mask = attention_mask[:MAX_LEN]\n",
        "                    global_offsets = global_offsets[:MAX_LEN]\n",
        "                \n",
        "                features.append({\n",
        "                    'input_ids': input_ids,\n",
        "                    'attention_mask': attention_mask,\n",
        "                    'offset_mapping': global_offsets,\n",
        "                    'example_id': ex_id,\n",
        "                })\n",
        "    return features\n",
        "\n",
        "# Test on small batch\n",
        "test_examples = train_df.head(1).to_dict('records')\n",
        "print('Testing on example:', test_examples[0]['id'], 'Language:', test_examples[0]['language'])\n",
        "print('Gold answer:', test_examples[0]['answer_text'], 'at', test_examples[0]['answer_start'])\n",
        "train_features = prepare_train_features(test_examples)\n",
        "val_features = prepare_validation_features(test_examples)\n",
        "print(f'Train features: {len(train_features)}')\n",
        "print(f'Val features: {len(val_features)}')\n",
        "if train_features:\n",
        "    print('Sample train feature keys:', list(train_features[0].keys()))\n",
        "    print('Sample input_ids len:', len(train_features[0]['input_ids']))\n",
        "    print('Sample is_positive:', train_features[0]['is_positive'])\n",
        "if val_features:\n",
        "    print('Sample val offset_mapping len:', len(val_features[0]['offset_mapping']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "240579d3-1724-484e-b5d2-4085781cb7f2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Post-processing to aggregate predictions across sliding windows with improved scoring\n",
        "def get_predictions(features, start_logits, end_logits, n_best_size=50, max_answer_length=80):\n",
        "    example_to_features = {}\n",
        "    for i, f in enumerate(features):\n",
        "        example_to_features.setdefault(f['example_id'], []).append((i, f))\n",
        "\n",
        "    pred_dict = {}\n",
        "    for example_id, feat_list in example_to_features.items():\n",
        "        prelim_predictions = []\n",
        "        for feat_idx, f in feat_list:\n",
        "            offsets = f['offset_mapping']\n",
        "            sl = start_logits[feat_idx]\n",
        "            el = end_logits[feat_idx]\n",
        "\n",
        "            # Context indices (non-None offsets)\n",
        "            ctx_idx = [i for i, o in enumerate(offsets) if o is not None]\n",
        "            if not ctx_idx:\n",
        "                continue\n",
        "\n",
        "            # Log-softmax on context logits only\n",
        "            start_log = log_softmax_np(sl[ctx_idx])\n",
        "            end_log = log_softmax_np(el[ctx_idx])\n",
        "\n",
        "            # Top n_best_size start/end positions in context\n",
        "            top_start_idx = np.argsort(sl[ctx_idx])[-n_best_size:].tolist()[::-1]\n",
        "            top_end_idx = np.argsort(el[ctx_idx])[-n_best_size:].tolist()[::-1]\n",
        "\n",
        "            # Global indices\n",
        "            top_start = [ctx_idx[i] for i in top_start_idx]\n",
        "            top_end = [ctx_idx[i] for i in top_end_idx]\n",
        "\n",
        "            # Generate candidates\n",
        "            for s in top_start:\n",
        "                for e in top_end:\n",
        "                    if e < s:\n",
        "                        continue\n",
        "                    length = e - s + 1\n",
        "                    if length > max_answer_length:\n",
        "                        continue\n",
        "                    sc, ec = offsets[s][0], offsets[e][1]\n",
        "                    # Score with softened length penalty\n",
        "                    score = start_log[top_start_idx[top_start.index(s)]] + end_log[top_end_idx[top_end.index(e)]] - 0.001 * max(0, length - 25)\n",
        "                    prelim_predictions.append((score, sc, ec))\n",
        "\n",
        "        if prelim_predictions:\n",
        "            _, sc, ec = max(prelim_predictions, key=lambda x: x[0])\n",
        "            pred_dict[example_id] = (sc, ec)\n",
        "        else:\n",
        "            # Fallback: best single-token span in context across all features\n",
        "            best_score = -np.inf\n",
        "            best_sc, best_ec = 0, 0\n",
        "            for feat_idx, f in feat_list:\n",
        "                offsets = f['offset_mapping']\n",
        "                sl = start_logits[feat_idx]\n",
        "                ctx_idx = [i for i, o in enumerate(offsets) if o is not None]\n",
        "                if not ctx_idx:\n",
        "                    continue\n",
        "                s_log = log_softmax_np(sl[ctx_idx])\n",
        "                best_s_local = np.argmax(sl[ctx_idx])\n",
        "                s_global = ctx_idx[best_s_local]\n",
        "                sc, ec = offsets[s_global][0], offsets[s_global][1]\n",
        "                score = s_log[best_s_local]\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_sc, best_ec = sc, ec\n",
        "            pred_dict[example_id] = (best_sc, best_ec)\n",
        "    return pred_dict\n",
        "\n",
        "# Function to extract answer from context with NFKC and punctuation trim\n",
        "def extract_answer(context, start_char, end_char):\n",
        "    if start_char == 0 and end_char == 0:\n",
        "        return ''\n",
        "    s = context[start_char:end_char]\n",
        "    s = unicodedata.normalize('NFKC', s).strip().strip(PUNCT)\n",
        "    return s\n",
        "\n",
        "# Dataset class - updated to include is_positive for training\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, features):\n",
        "        self.input_ids = [f['input_ids'] for f in features]\n",
        "        self.attention_mask = [f['attention_mask'] for f in features]\n",
        "        if 'start_positions' in features[0]:\n",
        "            self.start_positions = [f['start_positions'] for f in features]\n",
        "            self.end_positions = [f['end_positions'] for f in features]\n",
        "            self.is_positive = [f['is_positive'] for f in features]\n",
        "        else:\n",
        "            self.start_positions = None\n",
        "            self.end_positions = None\n",
        "            self.is_positive = None\n",
        "        self.offset_mapping = [f.get('offset_mapping') for f in features]\n",
        "        self.example_id = [f['example_id'] for f in features]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_mask[idx]\n",
        "        }\n",
        "        assert len(item['input_ids']) == MAX_LEN, 'Input ids not padded correctly'\n",
        "        assert len(item['attention_mask']) == MAX_LEN, 'Attention mask not padded correctly'\n",
        "        if self.start_positions is not None:\n",
        "            item['start_positions'] = self.start_positions[idx]\n",
        "            item['end_positions'] = self.end_positions[idx]\n",
        "            item['is_positive'] = self.is_positive[idx]\n",
        "        return item\n",
        "\n",
        "# Custom Weighted Trainer to down-weight negative examples (fixed per-example weighting)\n",
        "class WeightedQATrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        start_positions = inputs.pop('start_positions')\n",
        "        end_positions = inputs.pop('end_positions')\n",
        "        is_positive = inputs.pop('is_positive', None)  # tensor [bs] or None\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits = outputs.end_logits\n",
        "\n",
        "        start_loss = F.cross_entropy(start_logits, start_positions, reduction='none')\n",
        "        end_loss = F.cross_entropy(end_logits, end_positions, reduction='none')\n",
        "        loss = (start_loss + end_loss) / 2.0\n",
        "\n",
        "        if is_positive is not None:\n",
        "            ispos = is_positive.bool()\n",
        "            weights = torch.where(ispos, torch.ones_like(loss), torch.full_like(loss, NEG_WEIGHT))\n",
        "            loss = (loss * weights).mean()\n",
        "        else:\n",
        "            loss = loss.mean()\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Numpy log_softmax for numpy arrays\n",
        "def log_softmax_np(x):\n",
        "    x = x - np.max(x, axis=-1, keepdims=True)\n",
        "    return x - np.log(np.sum(np.exp(x), axis=-1, keepdims=True))\n",
        "\n",
        "# Test dataset creation\n",
        "val_features_test = prepare_validation_features(train_df.head(1).to_dict('records'))\n",
        "val_dataset_test = QADataset(val_features_test)\n",
        "print(f'Dataset length: {len(val_dataset_test)}')\n",
        "sample_item = val_dataset_test[0]\n",
        "print('Sample item keys:', list(sample_item.keys()))\n",
        "print('Sample input_ids len:', len(sample_item['input_ids']))\n",
        "\n",
        "# Test train dataset with is_positive\n",
        "trn_features_test = prepare_train_features(train_df.head(1).to_dict('records'))\n",
        "if trn_features_test:\n",
        "    trn_dataset_test = QADataset(trn_features_test)\n",
        "    sample_trn_item = trn_dataset_test[0]\n",
        "    print('Sample train item keys:', list(sample_trn_item.keys()))\n",
        "    print('Sample is_positive:', sample_trn_item['is_positive'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "fac42e05-795b-478a-b045-390d6f8d1850",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Precompute test features once (language already set in Cell 1)\n",
        "print('Test language distribution:', test_df['language'].value_counts())\n",
        "test_features = prepare_validation_features(test_df.to_dict('records'))\n",
        "test_dataset = QADataset(test_features)\n",
        "test_start_sum = None\n",
        "test_end_sum = None\n",
        "\n",
        "# Training loop for seed 123\n",
        "oof_preds = []\n",
        "oof_trues = []\n",
        "oof_ids = []\n",
        "fold_jaccards = []\n",
        "\n",
        "for fold in range(N_FOLDS):\n",
        "    print(f'\\n=== Fold {fold} ===')\n",
        "    trn_df = train_df[train_df['fold'] != fold].reset_index(drop=True)\n",
        "    val_df = train_df[train_df['fold'] == fold].reset_index(drop=True)\n",
        "    print(f'Train: {len(trn_df)}, Val: {len(val_df)}')\n",
        "\n",
        "    # 2x Tamil oversampling for better balance\n",
        "    trn_df = pd.concat([trn_df, trn_df[trn_df['language'] == 'tamil']]).reset_index(drop=True)\n",
        "\n",
        "    print('Preparing train features...')\n",
        "    start_time = time.time()\n",
        "    trn_features = prepare_train_features(trn_df.to_dict('records'))\n",
        "    prep_time = time.time() - start_time\n",
        "    print(f'Trn features prepared in {prep_time:.2f}s: {len(trn_features)}')\n",
        "\n",
        "    print('Preparing val features...')\n",
        "    start_time = time.time()\n",
        "    val_features = prepare_validation_features(val_df.to_dict('records'))\n",
        "    prep_time = time.time() - start_time\n",
        "    print(f'Val features prepared in {prep_time:.2f}s: {len(val_features)}')\n",
        "\n",
        "    trn_dataset = QADataset(trn_features)\n",
        "    val_dataset = QADataset(val_features)\n",
        "\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "    model.gradient_checkpointing_enable()\n",
        "    param_count = sum(p.numel() for p in model.parameters())\n",
        "    print(f'Model param count: {param_count:,}')\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'/tmp/model_seed123_{fold}',\n",
        "        bf16=True,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=16,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        learning_rate=LR,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        save_strategy='no',\n",
        "        report_to='none',\n",
        "        dataloader_pin_memory=False,\n",
        "        dataloader_num_workers=2,\n",
        "        remove_unused_columns=False,\n",
        "        warmup_ratio=0.1,\n",
        "        lr_scheduler_type='linear',\n",
        "        max_grad_norm=1.0,\n",
        "        logging_steps=10,  # More frequent logging\n",
        "    )\n",
        "\n",
        "    trainer = WeightedQATrainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=trn_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=default_data_collator,\n",
        "    )\n",
        "\n",
        "    print('Starting training...')\n",
        "    train_start = time.time()\n",
        "    trainer.train()\n",
        "    train_time = time.time() - train_start\n",
        "    print(f'Training completed in {train_time:.2f}s')\n",
        "\n",
        "    predictions = trainer.predict(val_dataset)\n",
        "    pred_dict = get_predictions(val_features, predictions.predictions[0], predictions.predictions[1], n_best_size=50, max_answer_length=MAX_ANSWER_LENGTH)\n",
        "\n",
        "    fold_preds = []\n",
        "    for idx, row in val_df.iterrows():\n",
        "        start_char, end_char = pred_dict.get(row['id'], (0, 0))\n",
        "        pred = extract_answer(row['context'], start_char, end_char)\n",
        "        fold_preds.append(pred)\n",
        "\n",
        "    print('Empty OOF preds:', (np.array(fold_preds) == '').mean())\n",
        "\n",
        "    fold_trues = val_df['answer_text'].tolist()\n",
        "    fold_jacc = compute_jaccard(fold_preds, fold_trues)\n",
        "    fold_jaccards.append(fold_jacc)\n",
        "    print(f'Fold {fold} Jaccard: {fold_jacc:.4f}')\n",
        "\n",
        "    oof_preds.extend(fold_preds)\n",
        "    oof_trues.extend(fold_trues)\n",
        "    oof_ids.extend(val_df['id'].tolist())\n",
        "\n",
        "    # Per language\n",
        "    hindi_mask = val_df['language'] == 'hindi'\n",
        "    if hindi_mask.sum() > 0:\n",
        "        pred_hindi = np.array(fold_preds)[hindi_mask]\n",
        "        true_hindi = val_df.loc[hindi_mask, 'answer_text'].tolist()\n",
        "        jacc_hindi = compute_jaccard(pred_hindi, true_hindi)\n",
        "        print(f'  Hindi Jaccard: {jacc_hindi:.4f}')\n",
        "    tamil_mask = val_df['language'] == 'tamil'\n",
        "    if tamil_mask.sum() > 0:\n",
        "        pred_tamil = np.array(fold_preds)[tamil_mask]\n",
        "        true_tamil = val_df.loc[tamil_mask, 'answer_text'].tolist()\n",
        "        jacc_tamil = compute_jaccard(pred_tamil, true_tamil)\n",
        "        print(f'  Tamil Jaccard: {jacc_tamil:.4f}')\n",
        "\n",
        "    # Accumulate test logits\n",
        "    test_out = trainer.predict(test_dataset)\n",
        "    if test_start_sum is None:\n",
        "        test_start_sum = test_out.predictions[0]\n",
        "        test_end_sum = test_out.predictions[1]\n",
        "    else:\n",
        "        test_start_sum += test_out.predictions[0]\n",
        "        test_end_sum += test_out.predictions[1]\n",
        "\n",
        "    del model, trainer, trn_dataset, val_dataset, trn_features, val_features\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(f'\\nMean fold Jaccard: {np.mean(fold_jaccards):.4f} (+/- {np.std(fold_jaccards):.4f})')\n",
        "overall_jacc = compute_jaccard(oof_preds, oof_trues)\n",
        "print(f'Overall OOF Jaccard: {overall_jacc:.4f}')\n",
        "\n",
        "# Save OOF for analysis\n",
        "oof_df = pd.DataFrame({'id': oof_ids, 'pred': oof_preds, 'true': oof_trues})\n",
        "oof_df.to_csv('oof_predictions_seed123.csv', index=False)\n",
        "print('OOF saved to oof_predictions_seed123.csv')\n",
        "\n",
        "# Generate submission from averaged test logits with per-language max_answer_length\n",
        "test_start_avg = test_start_sum / N_FOLDS\n",
        "test_end_avg = test_end_sum / N_FOLDS\n",
        "\n",
        "# Compute predictions with different max lengths\n",
        "pred60 = get_predictions(test_features, test_start_avg, test_end_avg, n_best_size=50, max_answer_length=60)\n",
        "pred90 = get_predictions(test_features, test_start_avg, test_end_avg, n_best_size=50, max_answer_length=90)\n",
        "\n",
        "# Select per language\n",
        "test_pred_dict = {}\n",
        "for idx, row in test_df.iterrows():\n",
        "    ex_id = row['id']\n",
        "    if row['language'] == 'tamil':\n",
        "        test_pred_dict[ex_id] = pred90.get(ex_id, (0, 0))\n",
        "    else:\n",
        "        test_pred_dict[ex_id] = pred60.get(ex_id, (0, 0))\n",
        "\n",
        "submission_preds = []\n",
        "for idx, row in test_df.iterrows():\n",
        "    start_char, end_char = test_pred_dict.get(row['id'], (0, 0))\n",
        "    pred = extract_answer(row['context'], start_char, end_char)\n",
        "    submission_preds.append(pred)\n",
        "\n",
        "submission = pd.DataFrame({'id': test_df['id'], 'PredictionString': submission_preds})\n",
        "submission.to_csv('submission_seed123.csv', index=False)\n",
        "print('Submission saved to submission_seed123.csv')\n",
        "\n",
        "# Save test logits and feature order for ensembling (seed 123)\n",
        "import json\n",
        "np.savez('test_logits_seed123_sum.npz', start=test_start_sum, end=test_end_sum, n_folds=N_FOLDS)\n",
        "json.dump([f['example_id'] for f in test_features], open('test_features_order_seed123.json', 'w'))\n",
        "print('Test logits and feature order saved for ensembling')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test language distribution: language\nhindi    84\ntamil    28\nName: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== Fold 0 ===\nTrain: 800, Val: 202\nPreparing train features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trn features prepared in 22.27s: 6343\nPreparing val features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val features prepared in 3.73s: 1914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model param count: 558,842,882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='1056' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1056 : < :, Epoch 0.00/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 2067.23s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/120 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty OOF preds: 0.0\nFold 0 Jaccard: 0.6256\n  Hindi Jaccard: 0.6636\n  Tamil Jaccard: 0.5627\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/68 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== Fold 1 ===\nTrain: 797, Val: 205\nPreparing train features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trn features prepared in 22.44s: 6406\nPreparing val features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val features prepared in 4.05s: 1957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model param count: 558,842,882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='1064' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1064 : < :, Epoch 0.00/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 2085.02s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/123 : < :]\n    </div>\n    "
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}