{
  "cells": [
    {
      "id": "eda1cccd-06c0-45cc-9275-696b106cdca5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "def pip(*args):\n",
        "    print('>', *args, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "# Set PIP_TARGET to writable directory\n",
        "pip_target = '/app/.pip-target'\n",
        "os.environ['PIP_TARGET'] = pip_target\n",
        "if os.path.exists(pip_target):\n",
        "    print('Removing existing', pip_target)\n",
        "    shutil.rmtree(pip_target, ignore_errors=True)\n",
        "\n",
        "# 0) Hard reset any prior torch stacks\n",
        "for pkg in ('torch', 'torchvision', 'torchaudio'):\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', pkg], check=False)\n",
        "\n",
        "# Clean stray site dirs\n",
        "for d in (\n",
        "    f'{pip_target}/torch',\n",
        "    f'{pip_target}/torch-2.8.0.dist-info',\n",
        "    f'{pip_target}/torch-2.4.1.dist-info',\n",
        "    f'{pip_target}/torchvision',\n",
        "    f'{pip_target}/torchvision-0.23.0.dist-info',\n",
        "    f'{pip_target}/torchvision-0.19.1.dist-info',\n",
        "    f'{pip_target}/torchaudio',\n",
        "    f'{pip_target}/torchaudio-2.8.0.dist-info',\n",
        "    f'{pip_target}/torchaudio-2.4.1.dist-info',\n",
        "    f'{pip_target}/torchgen',\n",
        "    f'{pip_target}/functorch',\n",
        "):\n",
        "    if os.path.exists(d):\n",
        "        print('Removing', d)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "# 1) Install the EXACT cu121 torch stack FIRST with --no-deps to avoid system dir installs\n",
        "pip('install',\n",
        "    '--index-url', 'https://download.pytorch.org/whl/cu121',\n",
        "    '--extra-index-url', 'https://pypi.org/simple',\n",
        "    '--force-reinstall', '--no-deps',\n",
        "    'torch==2.4.1', 'torchvision==0.19.1', 'torchaudio==2.4.1')\n",
        "\n",
        "# 2) Create a constraints file\n",
        "Path('constraints.txt').write_text(\n",
        "    'torch==2.4.1\\n'\n",
        "    'torchvision==0.19.1\\n'\n",
        "    'torchaudio==2.4.1\\n'\n",
        ")\n",
        "\n",
        "# 3) Install NON-torch deps\n",
        "pip('install', '-c', 'constraints.txt',\n",
        "    'transformers==4.44.2', 'accelerate==0.34.2',\n",
        "    'datasets==2.21.0', 'evaluate==0.4.2',\n",
        "    'sentencepiece', 'scikit-learn',\n",
        "    '--upgrade-strategy', 'only-if-needed')\n",
        "\n",
        "# 4) Sanity gate - add pip_target to sys.path\n",
        "sys.path.insert(0, pip_target)\n",
        "import torch\n",
        "print('torch:', torch.__version__, 'built CUDA:', getattr(torch.version, 'cuda', None))\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "assert str(getattr(torch.version,'cuda','')).startswith('12.1'), f'Wrong CUDA build: {torch.version.cuda}'\n",
        "assert torch.cuda.is_available(), 'CUDA not available'\n",
        "print('GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# Install additional packages with PIP_TARGET\n",
        "pip('install', 'rank_bm25')\n",
        "pip('install', 'langdetect')\n",
        "pip('install', 'indic-nlp-library', 'pyarrow')\n",
        "\n",
        "# Downgrade fsspec\n",
        "pip('install', '-c', 'constraints.txt', 'fsspec[http]<=2024.6.1,>=2023.1.0', '--upgrade')\n",
        "\n",
        "# Verify additional imports\n",
        "try:\n",
        "    from rank_bm25 import BM25Okapi\n",
        "    print('BM25 available')\n",
        "except ImportError:\n",
        "    print('BM25 not available')\n",
        "try:\n",
        "    from langdetect import detect\n",
        "    print('langdetect available')\n",
        "except ImportError:\n",
        "    print('langdetect not available')\n",
        "print('Environment setup complete')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing existing /app/.pip-target\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torch as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torchvision as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> install --index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://pypi.org/simple --force-reinstall --no-deps torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torchaudio as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121, https://pypi.org/simple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (799.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.19.1\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.1/7.1 MB 413.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.4/3.4 MB 488.9 MB/s eta 0:00:00\nInstalling collected packages: torchaudio, torchvision, torch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> install -c constraints.txt transformers==4.44.2 accelerate==0.34.2 datasets==2.21.0 evaluate==0.4.2 sentencepiece scikit-learn --upgrade-strategy only-if-needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.44.2\n  Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.5/9.5 MB 154.4 MB/s eta 0:00:00\nCollecting accelerate==0.34.2\n  Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 324.4/324.4 KB 511.3 MB/s eta 0:00:00\nCollecting datasets==2.21.0\n  Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 527.3/527.3 KB 498.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate==0.4.2\n  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 84.1/84.1 KB 360.9 MB/s eta 0:00:00\nCollecting sentencepiece\n  Downloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.4/1.4 MB 515.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.7/9.7 MB 263.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers<0.20,>=0.19\n  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.6/3.6 MB 349.9 MB/s eta 0:00:00\nCollecting tqdm>=4.27\n  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.5/78.5 KB 419.9 MB/s eta 0:00:00\nCollecting huggingface-hub<1.0,>=0.23.2\n  Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 563.3/563.3 KB 500.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting regex!=2019.12.17\n  Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 KB 516.5 MB/s eta 0:00:00\nCollecting packaging>=20.0\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 432.5 MB/s eta 0:00:00\nCollecting requests\n  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.7/64.7 KB 380.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting safetensors>=0.4.1\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 485.8/485.8 KB 517.7 MB/s eta 0:00:00\nCollecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\nCollecting pyyaml>=5.1\n  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 806.6/806.6 KB 511.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy>=1.17\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 496.2 MB/s eta 0:00:00\nCollecting torch>=1.10.0\n  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 797.1/797.1 MB 298.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psutil\n  Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 291.2/291.2 KB 499.7 MB/s eta 0:00:00\nCollecting fsspec[http]<=2024.6.1,>=2023.1.0\n  Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 177.6/177.6 KB 450.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xxhash\n  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 194.8/194.8 KB 454.1 MB/s eta 0:00:00\nCollecting pyarrow>=15.0.0\n  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 42.8/42.8 MB 239.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aiohttp\n  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.7/1.7 MB 267.2 MB/s eta 0:00:00\nCollecting pandas\n  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.4/12.4 MB 375.3 MB/s eta 0:00:00\nCollecting dill<0.3.9,>=0.3.0\n  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 116.3/116.3 KB 400.1 MB/s eta 0:00:00\nCollecting multiprocess\n  Downloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 144.5/144.5 KB 456.0 MB/s eta 0:00:00\nCollecting threadpoolctl>=3.1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nCollecting scipy>=1.8.0\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.9/35.9 MB 284.6 MB/s eta 0:00:00\nCollecting joblib>=1.2.0\n  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 308.4/308.4 KB 496.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yarl<2.0,>=1.17.0\n  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 349.0/349.0 KB 507.7 MB/s eta 0:00:00\nCollecting aiohappyeyeballs>=2.5.0\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nCollecting frozenlist>=1.1.1\n  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 235.3/235.3 KB 400.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting multidict<7.0,>=4.5\n  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 246.7/246.7 KB 477.7 MB/s eta 0:00:00\nCollecting aiosignal>=1.4.0\n  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\nCollecting propcache>=0.2.0\n  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 213.5/213.5 KB 486.2 MB/s eta 0:00:00\nCollecting attrs>=17.3.0\n  Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 63.8/63.8 KB 392.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing-extensions>=3.7.4.3\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 379.7 MB/s eta 0:00:00\nCollecting hf-xet<2.0.0,>=1.1.3\n  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.2/3.2 MB 525.6 MB/s eta 0:00:00\nCollecting urllib3<3,>=1.21.1\n  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 129.8/129.8 KB 472.9 MB/s eta 0:00:00\nCollecting certifi>=2017.4.17\n  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 161.2/161.2 KB 474.0 MB/s eta 0:00:00\nCollecting idna<4,>=2.5\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 70.4/70.4 KB 433.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting charset_normalizer<4,>=2\n  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 150.3/150.3 KB 464.4 MB/s eta 0:00:00\nCollecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 541.7 MB/s eta 0:00:00\nCollecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 348.3 MB/s eta 0:00:00\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 288.9 MB/s eta 0:00:00\nCollecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 353.0 MB/s eta 0:00:00\nCollecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 528.9 MB/s eta 0:00:00\nCollecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 547.0 MB/s eta 0:00:00\nCollecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 332.4 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 306.8 MB/s eta 0:00:00\nCollecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 537.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 532.2 MB/s eta 0:00:00\nCollecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 402.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 529.2 MB/s eta 0:00:00\nCollecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 535.0 MB/s eta 0:00:00\nCollecting nvidia-cufft-cu12==11.0.2.54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 554.4 MB/s eta 0:00:00\nCollecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 457.4 MB/s eta 0:00:00\nCollecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 541.7 MB/s eta 0:00:00\nCollecting multiprocess\n  Downloading multiprocess-0.70.17-py311-none-any.whl (144 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 144.3/144.3 KB 458.0 MB/s eta 0:00:00\n  Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 143.5/143.5 KB 448.2 MB/s eta 0:00:00\nCollecting pytz>=2020.1\n  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 509.2/509.2 KB 498.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dateutil>=2.8.2\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 229.9/229.9 KB 513.0 MB/s eta 0:00:00\nCollecting tzdata>=2022.7\n  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 347.8/347.8 KB 511.7 MB/s eta 0:00:00\nCollecting six>=1.5\n  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\nCollecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 508.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: pytz, mpmath, xxhash, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, sympy, six, sentencepiece, safetensors, regex, pyyaml, pyarrow, psutil, propcache, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, MarkupSafe, joblib, idna, hf-xet, fsspec, frozenlist, filelock, dill, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, triton, scipy, requests, python-dateutil, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, jinja2, aiosignal, scikit-learn, pandas, nvidia-cusolver-cu12, huggingface-hub, aiohttp, torch, tokenizers, transformers, datasets, accelerate, evaluate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.2 accelerate-0.34.2 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 attrs-25.3.0 certifi-2025.8.3 charset_normalizer-3.4.3 datasets-2.21.0 dill-0.3.8 evaluate-0.4.2 filelock-3.19.1 frozenlist-1.7.0 fsspec-2024.6.1 hf-xet-1.1.10 huggingface-hub-0.35.1 idna-3.10 jinja2-3.1.6 joblib-1.5.2 mpmath-1.3.0 multidict-6.6.4 multiprocess-0.70.16 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 packaging-25.0 pandas-2.3.2 propcache-0.3.2 psutil-7.1.0 pyarrow-21.0.0 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.3 regex-2025.9.18 requests-2.32.5 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 sentencepiece-0.2.1 six-1.17.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.19.1 torch-2.4.1 tqdm-4.67.1 transformers-4.44.2 triton-3.0.0 typing-extensions-4.15.0 tzdata-2025.2 urllib3-2.5.0 xxhash-3.5.0 yarl-1.20.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/functorch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchgen already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.4.1+cu121 built CUDA: 12.1\nCUDA available: True\nGPU: NVIDIA A10-24Q\n> install rank_bm25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 179.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: numpy, rank_bm25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed numpy-1.26.4 rank_bm25-0.2.2\n> install langdetect\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 981.5/981.5 KB 32.6 MB/s eta 0:00:00\n  Preparing metadata (setup.py): started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py): finished with status 'done'\nCollecting six\n  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\nBuilding wheels for collected packages: langdetect\n  Building wheel for langdetect (setup.py): started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for langdetect (setup.py): finished with status 'done'\n  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=66bf6e003ef4e33722b342e3e89edfd603a7151f2ee31f4e3c7d6c0e05e4d3da\n  Stored in directory: /tmp/pip-ephem-wheel-cache-hjfn6opk/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\nSuccessfully built langdetect\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: six, langdetect\nSuccessfully installed langdetect-1.0.9 six-1.17.0\n> install indic-nlp-library pyarrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/six-1.17.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/six.py already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting indic-nlp-library\n  Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 40.3/40.3 KB 3.2 MB/s eta 0:00:00\nCollecting pyarrow\n  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 42.8/42.8 MB 300.4 MB/s eta 0:00:00\nCollecting sphinx-argparse\n  Downloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 526.0 MB/s eta 0:00:00\nCollecting morfessor\n  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\nCollecting sphinx-rtd-theme\n  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.7/7.7 MB 225.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas\n  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.4/12.4 MB 541.1 MB/s eta 0:00:00\nCollecting pytz>=2020.1\n  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 509.2/509.2 KB 489.2 MB/s eta 0:00:00\nCollecting python-dateutil>=2.8.2\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 229.9/229.9 KB 480.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tzdata>=2022.7\n  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 347.8/347.8 KB 504.5 MB/s eta 0:00:00\nCollecting docutils>=0.19\n  Downloading docutils-0.22.2-py3-none-any.whl (632 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 632.7/632.7 KB 514.5 MB/s eta 0:00:00\nCollecting sphinx>=5.1.0\n  Downloading sphinx-8.2.3-py3-none-any.whl (3.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.6/3.6 MB 376.3 MB/s eta 0:00:00\nCollecting docutils>=0.19\n  Downloading docutils-0.21.2-py3-none-any.whl (587 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 587.4/587.4 KB 516.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sphinxcontrib-jquery<5,>=4\n  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.1/121.1 KB 453.5 MB/s eta 0:00:00\nCollecting six>=1.5\n  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\nCollecting sphinxcontrib-htmlhelp>=2.0.6\n  Downloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl (98 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 98.7/98.7 KB 432.2 MB/s eta 0:00:00\nCollecting packaging>=23.0\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 346.2 MB/s eta 0:00:00\nCollecting requests>=2.30.0\n  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.7/64.7 KB 318.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Jinja2>=3.1\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 439.3 MB/s eta 0:00:00\nCollecting babel>=2.13\n  Downloading babel-2.17.0-py3-none-any.whl (10.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10.2/10.2 MB 527.8 MB/s eta 0:00:00\nCollecting alabaster>=0.7.14\n  Downloading alabaster-1.0.0-py3-none-any.whl (13 kB)\nCollecting Pygments>=2.17\n  Downloading pygments-2.19.2-py3-none-any.whl (1.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.2/1.2 MB 511.7 MB/s eta 0:00:00\nCollecting sphinxcontrib-qthelp>=1.0.6\n  Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl (88 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 88.7/88.7 KB 341.0 MB/s eta 0:00:00\nCollecting sphinxcontrib-serializinghtml>=1.1.9\n  Downloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl (92 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 92.1/92.1 KB 424.9 MB/s eta 0:00:00\nCollecting roman-numerals-py>=1.0.0\n  Downloading roman_numerals_py-3.1.0-py3-none-any.whl (7.7 kB)\nCollecting imagesize>=1.3\n  Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\nCollecting sphinxcontrib-jsmath>=1.0.1\n  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\nCollecting sphinxcontrib-devhelp>=1.0.6\n  Downloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl (82 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 82.5/82.5 KB 373.1 MB/s eta 0:00:00\nCollecting sphinxcontrib-applehelp>=1.0.7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl (119 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 119.3/119.3 KB 433.3 MB/s eta 0:00:00\nCollecting snowballstemmer>=2.2\n  Downloading snowballstemmer-3.0.1-py3-none-any.whl (103 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 103.3/103.3 KB 448.8 MB/s eta 0:00:00\nCollecting MarkupSafe>=2.0\n  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\nCollecting urllib3<3,>=1.21.1\n  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 129.8/129.8 KB 462.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting certifi>=2017.4.17\n  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 161.2/161.2 KB 457.8 MB/s eta 0:00:00\nCollecting idna<4,>=2.5\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 70.4/70.4 KB 431.3 MB/s eta 0:00:00\nCollecting charset_normalizer<4,>=2\n  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 150.3/150.3 KB 449.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: pytz, morfessor, urllib3, tzdata, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, snowballstemmer, six, roman-numerals-py, Pygments, pyarrow, packaging, numpy, MarkupSafe, imagesize, idna, docutils, charset_normalizer, certifi, babel, alabaster, requests, python-dateutil, Jinja2, sphinx, pandas, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed Jinja2-3.1.6 MarkupSafe-3.0.2 Pygments-2.19.2 alabaster-1.0.0 babel-2.17.0 certifi-2025.8.3 charset_normalizer-3.4.3 docutils-0.21.2 idna-3.10 imagesize-1.4.1 indic-nlp-library-0.92 morfessor-2.0.6 numpy-1.26.4 packaging-25.0 pandas-2.3.2 pyarrow-21.0.0 python-dateutil-2.9.0.post0 pytz-2025.2 requests-2.32.5 roman-numerals-py-3.1.0 six-1.17.0 snowballstemmer-3.0.1 sphinx-8.2.3 sphinx-argparse-0.5.2 sphinx-rtd-theme-3.0.2 sphinxcontrib-applehelp-2.0.0 sphinxcontrib-devhelp-2.0.0 sphinxcontrib-htmlhelp-2.1.0 sphinxcontrib-jquery-4.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-2.0.0 sphinxcontrib-serializinghtml-2.0.0 tzdata-2025.2 urllib3-2.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/pandas-2.3.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pandas already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2-3.1.6.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/python_dateutil-2.9.0.post0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/dateutil already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/requests-2.32.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/requests already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/certifi-2025.8.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/certifi already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/charset_normalizer-3.4.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/charset_normalizer already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/idna-3.10.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/idna already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/markupsafe already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/MarkupSafe-3.0.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging-25.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pyarrow-21.0.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pyarrow already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/six-1.17.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/six.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tzdata-2025.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tzdata already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/urllib3-2.5.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/urllib3 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pytz-2025.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pytz already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> install -c constraints.txt fsspec[http]<=2024.6.1,>=2023.1.0 --upgrade\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fsspec[http]<=2024.6.1,>=2023.1.0\n  Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 177.6/177.6 KB 7.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.7/1.7 MB 154.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yarl<2.0,>=1.17.0\n  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 349.0/349.0 KB 414.8 MB/s eta 0:00:00\nCollecting aiohappyeyeballs>=2.5.0\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nCollecting frozenlist>=1.1.1\n  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 235.3/235.3 KB 454.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting multidict<7.0,>=4.5\n  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 246.7/246.7 KB 464.5 MB/s eta 0:00:00\nCollecting aiosignal>=1.4.0\n  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\nCollecting propcache>=0.2.0\n  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 213.5/213.5 KB 484.2 MB/s eta 0:00:00\nCollecting attrs>=17.3.0\n  Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 63.8/63.8 KB 365.5 MB/s eta 0:00:00\nCollecting typing-extensions>=4.2\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 395.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting idna>=2.0\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 70.4/70.4 KB 391.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: typing-extensions, propcache, multidict, idna, fsspec, frozenlist, attrs, aiohappyeyeballs, yarl, aiosignal, aiohttp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 attrs-25.3.0 frozenlist-1.7.0 fsspec-2024.6.1 idna-3.10 multidict-6.6.4 propcache-0.3.2 typing-extensions-4.15.0 yarl-1.20.1\nBM25 available\nlangdetect available\nEnvironment setup complete\n"
          ]
        }
      ]
    },
    {
      "id": "65f9d70d-2031-4214-a37a-068909a92156",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "import gc\n",
        "import ast\n",
        "import sys\n",
        "import copy\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, TensorDataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoConfig,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    )\n",
        "from transformers import default_data_collator\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import hashlib\n",
        "\n",
        "import subprocess\n",
        "import shutil\n",
        "import unicodedata\n",
        "\n",
        "# Add pip_target to sys.path if not already\n",
        "pip_target = '/app/.pip-target'\n",
        "if pip_target not in sys.path:\n",
        "    sys.path.insert(0, pip_target)\n",
        "\n",
        "# BM25 and langdetect\n",
        "BM25_AVAILABLE = False\n",
        "try:\n",
        "    from rank_bm25 import BM25Okapi\n",
        "    BM25_AVAILABLE = True\n",
        "    print('BM25 available')\n",
        "except ImportError:\n",
        "    print('BM25 not available, falling back to TF-IDF only')\n",
        "\n",
        "LANGDETECT_AVAILABLE = False\n",
        "try:\n",
        "    from langdetect import detect\n",
        "    LANGDETECT_AVAILABLE = True\n",
        "    print('langdetect available')\n",
        "except ImportError:\n",
        "    print('langdetect not available, using script fallback')\n",
        "\n",
        "# Script-based language detection fallback\n",
        "def detect_lang(text):\n",
        "    if not isinstance(text, str):\n",
        "        return 'hindi'\n",
        "    for c in text:\n",
        "        if 0x0B80 <= ord(c) <= 0x0BFF:  # Tamil Unicode range\n",
        "            return 'tamil'\n",
        "    return 'hindi'\n",
        "\n",
        "# Set seeds\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Constants with coach tweaks\n",
        "DEBUG = False  # Set to True for rapid prototyping\n",
        "MAX_LEN = 512\n",
        "DOC_STRIDE = 128\n",
        "N_SPLITS = 5\n",
        "BATCH_SIZE = 2\n",
        "GRAD_ACCUM_STEPS = 16\n",
        "EPOCHS = 5\n",
        "LR = 2.5e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "NEG_WEIGHT = 0.2\n",
        "USE_RETRIEVAL = True\n",
        "TOP_K_CHUNKS_TRAIN = 8\n",
        "TOP_K_CHUNKS_EVAL_HINDI = 10\n",
        "TOP_K_CHUNKS_EVAL_TAMIL = 35  # Coach tweak for better Tamil recall\n",
        "CHUNK_SIZE = 1800\n",
        "OVERLAP = 250\n",
        "NEG_POS_RATIO = 2\n",
        "MODEL_NAME = 'deepset/xlm-roberta-large-squad2'\n",
        "PUNCT = '\\u0964,.\\uff0c!\\uff01?\\uff1f\"\\\\\\'\\u201c\\u201d\\u2018\\u2019()[]{}:;'\n",
        "MAX_ANSWER_LENGTH = 80  # Coach tweak for longer spans\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "if DEBUG:\n",
        "    train_df = train_df.sample(n=200, random_state=42).reset_index(drop=True)\n",
        "    print(f'DEBUG mode: using {len(train_df)} samples')\n",
        "else:\n",
        "    print(f'Full mode: using {len(train_df)} samples')\n",
        "\n",
        "print('Train shape:', train_df.shape)\n",
        "print('Test shape:', test_df.shape)\n",
        "\n",
        "# Label alignment fix with progress tracking\n",
        "print('Before fix_span')\n",
        "def fix_span(row):\n",
        "    ctx, ans, s = row['context'], row['answer_text'], row['answer_start']\n",
        "    if s < 0 or ctx[s:s+len(ans)] != ans:\n",
        "        idx = ctx.find(ans)\n",
        "        if idx != -1:\n",
        "            row['answer_start'] = idx\n",
        "    return row\n",
        "\n",
        "train_df = train_df.apply(fix_span, axis=1)\n",
        "print('After fix_span')\n",
        "\n",
        "# Context groups for CV (hash first 1024 chars to group same articles)\n",
        "def get_context_hash(context):\n",
        "    return hashlib.md5(context[:1024].encode()).hexdigest()\n",
        "\n",
        "train_df['context_hash'] = train_df['context'].apply(get_context_hash)\n",
        "print('Context hashes computed')\n",
        "\n",
        "# Jaccard metric with NFKC normalization\n",
        "def jaccard_word(pred, true):\n",
        "    pred = unicodedata.normalize('NFKC', pred).lower()\n",
        "    true = unicodedata.normalize('NFKC', true).lower()\n",
        "    if not pred or not true:\n",
        "        return 0.0\n",
        "    pw, tw = set(pred.split()), set(true.split())\n",
        "    return len(pw & tw) / len(pw | tw) if pw and tw else 0.0\n",
        "\n",
        "def compute_jaccard(preds, trues):\n",
        "    return np.mean([jaccard_word(p, t) for p, t in zip(preds, trues)])\n",
        "\n",
        "# Assign language to test_df using langdetect or fallback\n",
        "print('Assigning language to test_df...')\n",
        "if LANGDETECT_AVAILABLE:\n",
        "    test_df['language'] = test_df['question'].apply(lambda x: {'ta':'tamil','hi':'hindi'}.get(detect(x), 'hindi') if isinstance(x, str) else 'hindi')\n",
        "else:\n",
        "    test_df['language'] = test_df['question'].apply(detect_lang)\n",
        "print('Test language dist:', test_df['language'].value_counts())\n",
        "\n",
        "# CV splitting with StratifiedGroupKFold\n",
        "sgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
        "train_df['fold'] = -1\n",
        "for fold, (trn_idx, val_idx) in enumerate(sgkf.split(train_df, train_df['language'], groups=train_df['context_hash'])):\n",
        "    train_df.loc[val_idx, 'fold'] = fold\n",
        "\n",
        "print('Fold distribution:')\n",
        "print(train_df.groupby(['fold', 'language']).size())\n",
        "print(f'Folds created: {train_df[\"fold\"].nunique()}')\n",
        "\n",
        "N_FOLDS = 3 if DEBUG else N_SPLITS\n",
        "print(f'Using {N_FOLDS} folds for training')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25 available\nlangdetect available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full mode: using 1002 samples\nTrain shape: (1002, 6)\nTest shape: (112, 4)\nBefore fix_span\nAfter fix_span\nContext hashes computed\nAssigning language to test_df...\nTest language dist: language\nhindi    84\ntamil    28\nName: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold distribution:\nfold  language\n0     hindi       133\n      tamil        60\n1     hindi       133\n      tamil        71\n2     hindi       126\n      tamil        68\n3     hindi       142\n      tamil        70\n4     hindi       128\n      tamil        71\ndtype: int64\nFolds created: 5\nUsing 5 folds for training\n"
          ]
        }
      ]
    },
    {
      "id": "2f538e2c-6b2d-4f2c-a84e-5468f84e07b7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "print('Tokenizer loaded:', tokenizer.name_or_path)\n",
        "\n",
        "# TF-IDF Retrieval setup with language-specific vectorizers\n",
        "if USE_RETRIEVAL:\n",
        "    print('Fitting language-specific TF-IDF vectorizers...')\n",
        "    hindi_df = train_df[train_df['language'] == 'hindi']\n",
        "    tamil_df = train_df[train_df['language'] == 'tamil']\n",
        "    \n",
        "    # Hindi vectorizer\n",
        "    print('Processing Hindi...')\n",
        "    hindi_questions = hindi_df['question'].tolist()\n",
        "    hindi_contexts = hindi_df['context'].tolist()\n",
        "    hindi_chunks = []\n",
        "    for ctx in tqdm(hindi_contexts, desc='Chunking Hindi contexts'):\n",
        "        chunks = []\n",
        "        for i in range(0, len(ctx), CHUNK_SIZE - OVERLAP):\n",
        "            chunk = ctx[i:i + CHUNK_SIZE]\n",
        "            if len(chunk) > 100:\n",
        "                chunks.append(chunk)\n",
        "        hindi_chunks.extend(chunks)\n",
        "    print(f'Hindi chunks total: {len(hindi_chunks)}')\n",
        "    hindi_corpus = hindi_questions + random.sample(hindi_chunks, min(3000, len(hindi_chunks)))\n",
        "    print(f'Hindi corpus size: {len(hindi_corpus)}')\n",
        "    hindi_vectorizer = TfidfVectorizer(\n",
        "        analyzer='char_wb',\n",
        "        ngram_range=(2, 4),\n",
        "        max_features=5000,\n",
        "        min_df=2,\n",
        "        max_df=0.95,\n",
        "        lowercase=False,\n",
        "        sublinear_tf=True,\n",
        "        dtype=np.float32\n",
        "    )\n",
        "    print('Fitting Hindi vectorizer...')\n",
        "    start_time = time.time()\n",
        "    hindi_vectorizer.fit(hindi_corpus)\n",
        "    fit_time = time.time() - start_time\n",
        "    print(f'Hindi TF-IDF fitted in {fit_time:.2f}s: {len(hindi_corpus)} docs')\n",
        "    \n",
        "    # Tamil vectorizer - fixed to char n-grams for better recall\n",
        "    print('Processing Tamil...')\n",
        "    tamil_questions = tamil_df['question'].tolist()\n",
        "    tamil_contexts = tamil_df['context'].tolist()\n",
        "    tamil_chunks = []\n",
        "    for ctx in tqdm(tamil_contexts, desc='Chunking Tamil contexts'):\n",
        "        chunks = []\n",
        "        for i in range(0, len(ctx), CHUNK_SIZE - OVERLAP):\n",
        "            chunk = ctx[i:i + CHUNK_SIZE]\n",
        "            if len(chunk) > 100:\n",
        "                chunks.append(chunk)\n",
        "        tamil_chunks.extend(chunks)\n",
        "    print(f'Tamil chunks total: {len(tamil_chunks)}')\n",
        "    tamil_corpus = tamil_questions + random.sample(tamil_chunks, min(1500, len(tamil_chunks)))\n",
        "    print(f'Tamil corpus size: {len(tamil_corpus)}')\n",
        "    tamil_vectorizer = TfidfVectorizer(\n",
        "        analyzer='char_wb',\n",
        "        ngram_range=(3, 5),\n",
        "        max_features=15000,\n",
        "        min_df=3,\n",
        "        max_df=0.9,\n",
        "        lowercase=False,\n",
        "        sublinear_tf=True,\n",
        "        dtype=np.float32\n",
        "    )\n",
        "    print('Fitting Tamil vectorizer...')\n",
        "    start_time = time.time()\n",
        "    tamil_vectorizer.fit(tamil_corpus)\n",
        "    fit_time = time.time() - start_time\n",
        "    print(f'Tamil TF-IDF fitted in {fit_time:.2f}s: {len(tamil_corpus)} docs')\n",
        "else:\n",
        "    hindi_vectorizer = tamil_vectorizer = None"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loaded: deepset/xlm-roberta-large-squad2\nFitting language-specific TF-IDF vectorizers...\nProcessing Hindi...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rChunking Hindi contexts:   0%|          | 0/662 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rChunking Hindi contexts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 662/662 [00:00<00:00, 51859.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hindi chunks total: 4586\nHindi corpus size: 3662\nFitting Hindi vectorizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hindi TF-IDF fitted in 3.80s: 3662 docs\nProcessing Tamil...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamil TF-IDF fitted in 2.21s: 1840 docs\n"
          ]
        }
      ]
    },
    {
      "id": "858a297c-4f88-48d5-bfd7-2f8e2a7c67e2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare training features with hybrid retrieval and sliding windows\n",
        "def prepare_train_features(examples, neg_pos_ratio=NEG_POS_RATIO):\n",
        "    features = []\n",
        "    for ex in examples:\n",
        "        q, ctx, ans, ex_id, lang = ex['question'].strip(), ex['context'].strip(), {'text': ex['answer_text'], 'answer_start': ex['answer_start']}, ex['id'], ex['language']\n",
        "        \n",
        "        if USE_RETRIEVAL:\n",
        "            # Chunk context\n",
        "            chunks = []\n",
        "            chunk_starts = []\n",
        "            for i in range(0, len(ctx), CHUNK_SIZE - OVERLAP):\n",
        "                chunk = ctx[i:i + CHUNK_SIZE]\n",
        "                if len(chunk) > 100:\n",
        "                    chunks.append(chunk)\n",
        "                    chunk_starts.append(i)\n",
        "            \n",
        "            if not chunks:\n",
        "                continue\n",
        "            \n",
        "            # Select vectorizer by language\n",
        "            if lang == 'hindi':\n",
        "                vectorizer = hindi_vectorizer\n",
        "            else:\n",
        "                vectorizer = tamil_vectorizer\n",
        "            \n",
        "            # TF-IDF retrieval\n",
        "            q_vec = vectorizer.transform([q])\n",
        "            chunk_vecs = vectorizer.transform(chunks)\n",
        "            similarities = cosine_similarity(q_vec, chunk_vecs).flatten()\n",
        "            \n",
        "            # BM25 hybrid if available\n",
        "            if BM25_AVAILABLE:\n",
        "                tokenized_chunks = [chunk.lower().split() for chunk in chunks]\n",
        "                bm25 = BM25Okapi(tokenized_chunks)\n",
        "                q_tokens = q.lower().split()\n",
        "                bm25_scores = bm25.get_scores(q_tokens)\n",
        "                if np.max(bm25_scores) > 0:\n",
        "                    norm_bm25 = bm25_scores / np.max(bm25_scores)\n",
        "                else:\n",
        "                    norm_bm25 = np.zeros_like(bm25_scores)\n",
        "                hybrid_scores = 0.5 * norm_bm25 + 0.5 * similarities\n",
        "            else:\n",
        "                hybrid_scores = similarities\n",
        "            top_indices = np.argsort(hybrid_scores)[-TOP_K_CHUNKS_TRAIN:]\n",
        "            \n",
        "            # Guarantee gold chunk inclusion for training by replacing lowest sim if needed\n",
        "            start_char = ans['answer_start']\n",
        "            end_char = start_char + len(ans['text'])\n",
        "            pos_idx = None\n",
        "            for ci, st in enumerate(chunk_starts):\n",
        "                if start_char >= st and end_char <= st + len(chunks[ci]):\n",
        "                    pos_idx = ci\n",
        "                    break\n",
        "            if pos_idx is not None and pos_idx not in top_indices:\n",
        "                # Replace the lowest hybrid score in top_indices with pos_idx\n",
        "                min_hybrid_arg = np.argmin(hybrid_scores[top_indices])\n",
        "                top_indices[min_hybrid_arg] = pos_idx\n",
        "            # Sort by hybrid descending\n",
        "            sort_args = np.argsort(hybrid_scores[top_indices])[::-1]\n",
        "            top_indices = top_indices[sort_args]\n",
        "            \n",
        "            # Get top chunks with their global start positions\n",
        "            top_chunks = [(hybrid_scores[idx], chunk_starts[idx], chunks[idx]) for idx in top_indices]\n",
        "        else:\n",
        "            top_chunks = [(1.0, 0, ctx)]  # full context if no retrieval\n",
        "        \n",
        "        # Now process each top chunk with sliding windows\n",
        "        pos_feats, neg_feats = [], []\n",
        "        for sim, chunk_start, chunk in top_chunks:\n",
        "            tokenized = tokenizer(\n",
        "                q,\n",
        "                chunk,\n",
        "                truncation='only_second',\n",
        "                max_length=MAX_LEN,\n",
        "                stride=DOC_STRIDE,\n",
        "                return_overflowing_tokens=True,\n",
        "                return_offsets_mapping=True,\n",
        "                padding=False,\n",
        "            )\n",
        "            \n",
        "            for j in range(len(tokenized['input_ids'])):\n",
        "                input_ids = tokenized['input_ids'][j]\n",
        "                attention_mask = tokenized['attention_mask'][j]\n",
        "                offsets = tokenized['offset_mapping'][j]\n",
        "                sequence_ids = tokenized.sequence_ids(j)\n",
        "                \n",
        "                # Skip windows without context tokens\n",
        "                if 1 not in sequence_ids:\n",
        "                    continue\n",
        "                \n",
        "                # Global offsets: add chunk_start to context offsets\n",
        "                global_offsets = []\n",
        "                ctx_start = 0\n",
        "                while ctx_start < len(sequence_ids) and sequence_ids[ctx_start] != 1:\n",
        "                    global_offsets.append(None)\n",
        "                    ctx_start += 1\n",
        "                while ctx_start < len(sequence_ids) and sequence_ids[ctx_start] == 1:\n",
        "                    local_offset = offsets[ctx_start]\n",
        "                    global_offset = (local_offset[0] + chunk_start, local_offset[1] + chunk_start) if local_offset else None\n",
        "                    global_offsets.append(global_offset)\n",
        "                    ctx_start += 1\n",
        "                while ctx_start < len(sequence_ids):\n",
        "                    global_offsets.append(None)\n",
        "                    ctx_start += 1\n",
        "                \n",
        "                # Find start/end positions using global offsets\n",
        "                start_pos = -1\n",
        "                end_pos = -1\n",
        "                is_positive = False\n",
        "                start_char = ans['answer_start']\n",
        "                end_char = start_char + len(ans['text'])\n",
        "                \n",
        "                for tok_idx, off in enumerate(global_offsets):\n",
        "                    if off is not None and off[0] <= start_char < off[1]:\n",
        "                        start_pos = tok_idx\n",
        "                    if off is not None and off[0] < end_char <= off[1]:\n",
        "                        end_pos = tok_idx\n",
        "                if start_pos != -1 and end_pos != -1 and end_pos >= start_pos:\n",
        "                    is_positive = True\n",
        "                else:\n",
        "                    start_pos = 0\n",
        "                    end_pos = 0\n",
        "                \n",
        "                # Pad/truncate\n",
        "                pad_len = MAX_LEN - len(input_ids)\n",
        "                if pad_len > 0:\n",
        "                    input_ids += [tokenizer.pad_token_id] * pad_len\n",
        "                    attention_mask += [0] * pad_len\n",
        "                else:\n",
        "                    input_ids = input_ids[:MAX_LEN]\n",
        "                    attention_mask = attention_mask[:MAX_LEN]\n",
        "                \n",
        "                feat = {\n",
        "                    'input_ids': input_ids,\n",
        "                    'attention_mask': attention_mask,\n",
        "                    'start_positions': start_pos,\n",
        "                    'end_positions': end_pos,\n",
        "                    'example_id': ex_id,\n",
        "                    'is_positive': is_positive\n",
        "                }\n",
        "                (pos_feats if is_positive else neg_feats).append(feat)\n",
        "        \n",
        "        # Cap negatives\n",
        "        if pos_feats:\n",
        "            features.extend(pos_feats)\n",
        "            random.shuffle(neg_feats)\n",
        "            n_neg = min(len(neg_feats), neg_pos_ratio * len(pos_feats))\n",
        "            features.extend(neg_feats[:n_neg])\n",
        "        elif neg_feats:\n",
        "            features.append(random.choice(neg_feats))\n",
        "    return features\n",
        "\n",
        "# Prepare validation features (lang-specific TOP_K_EVAL)\n",
        "def prepare_validation_features(examples):\n",
        "    features = []\n",
        "    for ex in examples:\n",
        "        q, ctx, ex_id, lang = ex['question'].strip(), ex['context'].strip(), ex['id'], ex['language']\n",
        "        \n",
        "        if USE_RETRIEVAL:\n",
        "            # Same chunking and retrieval as train, but use lang-specific TOP_K_EVAL\n",
        "            chunks = []\n",
        "            chunk_starts = []\n",
        "            for i in range(0, len(ctx), CHUNK_SIZE - OVERLAP):\n",
        "                chunk = ctx[i:i + CHUNK_SIZE]\n",
        "                if len(chunk) > 100:\n",
        "                    chunks.append(chunk)\n",
        "                    chunk_starts.append(i)\n",
        "            \n",
        "            if not chunks:\n",
        "                continue\n",
        "            \n",
        "            # Select vectorizer by language\n",
        "            if lang == 'hindi':\n",
        "                vectorizer = hindi_vectorizer\n",
        "                top_k_eval = TOP_K_CHUNKS_EVAL_HINDI\n",
        "            else:\n",
        "                vectorizer = tamil_vectorizer\n",
        "                top_k_eval = TOP_K_CHUNKS_EVAL_TAMIL\n",
        "            \n",
        "            # TF-IDF\n",
        "            q_vec = vectorizer.transform([q])\n",
        "            chunk_vecs = vectorizer.transform(chunks)\n",
        "            similarities = cosine_similarity(q_vec, chunk_vecs).flatten()\n",
        "            \n",
        "            # BM25 hybrid if available\n",
        "            if BM25_AVAILABLE:\n",
        "                tokenized_chunks = [chunk.lower().split() for chunk in chunks]\n",
        "                bm25 = BM25Okapi(tokenized_chunks)\n",
        "                q_tokens = q.lower().split()\n",
        "                bm25_scores = bm25.get_scores(q_tokens)\n",
        "                if np.max(bm25_scores) > 0:\n",
        "                    norm_bm25 = bm25_scores / np.max(bm25_scores)\n",
        "                else:\n",
        "                    norm_bm25 = np.zeros_like(bm25_scores)\n",
        "                hybrid_scores = 0.5 * norm_bm25 + 0.5 * similarities\n",
        "            else:\n",
        "                hybrid_scores = similarities\n",
        "            top_indices = np.argsort(hybrid_scores)[-top_k_eval:]\n",
        "            top_chunks = [(hybrid_scores[idx], chunk_starts[idx], chunks[idx]) for idx in top_indices]\n",
        "        else:\n",
        "            top_chunks = [(1.0, 0, ctx)]\n",
        "        \n",
        "        # Process each top chunk\n",
        "        for sim, chunk_start, chunk in top_chunks:\n",
        "            tokenized = tokenizer(\n",
        "                q,\n",
        "                chunk,\n",
        "                truncation='only_second',\n",
        "                max_length=MAX_LEN,\n",
        "                stride=DOC_STRIDE,\n",
        "                return_overflowing_tokens=True,\n",
        "                return_offsets_mapping=True,\n",
        "                padding=False,\n",
        "            )\n",
        "            \n",
        "            for j in range(len(tokenized['input_ids'])):\n",
        "                input_ids = tokenized['input_ids'][j]\n",
        "                attention_mask = tokenized['attention_mask'][j]\n",
        "                offsets = tokenized['offset_mapping'][j]\n",
        "                sequence_ids = tokenized.sequence_ids(j)\n",
        "                \n",
        "                # Skip windows without context tokens\n",
        "                if 1 not in sequence_ids:\n",
        "                    continue\n",
        "                \n",
        "                # Global offsets for post-processing\n",
        "                global_offsets = []\n",
        "                ctx_start = 0\n",
        "                while ctx_start < len(sequence_ids) and sequence_ids[ctx_start] != 1:\n",
        "                    global_offsets.append(None)\n",
        "                    ctx_start += 1\n",
        "                while ctx_start < len(sequence_ids) and sequence_ids[ctx_start] == 1:\n",
        "                    local_offset = offsets[ctx_start]\n",
        "                    global_offset = (local_offset[0] + chunk_start, local_offset[1] + chunk_start) if local_offset else None\n",
        "                    global_offsets.append(global_offset)\n",
        "                    ctx_start += 1\n",
        "                while ctx_start < len(sequence_ids):\n",
        "                    global_offsets.append(None)\n",
        "                    ctx_start += 1\n",
        "                \n",
        "                # Pad/truncate\n",
        "                pad_len = MAX_LEN - len(input_ids)\n",
        "                if pad_len > 0:\n",
        "                    input_ids += [tokenizer.pad_token_id] * pad_len\n",
        "                    attention_mask += [0] * pad_len\n",
        "                    global_offsets += [None] * pad_len\n",
        "                else:\n",
        "                    input_ids = input_ids[:MAX_LEN]\n",
        "                    attention_mask = attention_mask[:MAX_LEN]\n",
        "                    global_offsets = global_offsets[:MAX_LEN]\n",
        "                \n",
        "                features.append({\n",
        "                    'input_ids': input_ids,\n",
        "                    'attention_mask': attention_mask,\n",
        "                    'offset_mapping': global_offsets,\n",
        "                    'example_id': ex_id,\n",
        "                })\n",
        "    return features\n",
        "\n",
        "# Test on small batch\n",
        "test_examples = train_df.head(1).to_dict('records')\n",
        "print('Testing on example:', test_examples[0]['id'], 'Language:', test_examples[0]['language'])\n",
        "print('Gold answer:', test_examples[0]['answer_text'], 'at', test_examples[0]['answer_start'])\n",
        "train_features = prepare_train_features(test_examples)\n",
        "val_features = prepare_validation_features(test_examples)\n",
        "print(f'Train features: {len(train_features)}')\n",
        "print(f'Val features: {len(val_features)}')\n",
        "if train_features:\n",
        "    print('Sample train feature keys:', list(train_features[0].keys()))\n",
        "    print('Sample input_ids len:', len(train_features[0]['input_ids']))\n",
        "    print('Sample is_positive:', train_features[0]['is_positive'])\n",
        "if val_features:\n",
        "    print('Sample val offset_mapping len:', len(val_features[0]['offset_mapping']))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing on example: 6bb0c472d Language: tamil\nGold answer: \u0b9a\u0bbf\u0bae\u0bcd\u0bae\u0bae\u0bcd at 168\nTrain features: 4\nVal features: 5\nSample train feature keys: ['input_ids', 'attention_mask', 'start_positions', 'end_positions', 'example_id', 'is_positive']\nSample input_ids len: 512\nSample is_positive: True\nSample val offset_mapping len: 512\n"
          ]
        }
      ]
    },
    {
      "id": "cc506a25-7fed-4482-b25e-3c8a01309b2c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Post-processing to aggregate predictions across sliding windows with improved scoring\n",
        "def get_predictions(features, start_logits, end_logits, n_best_size=50, max_answer_length=80):\n",
        "    example_to_features = {}\n",
        "    for i, f in enumerate(features):\n",
        "        example_to_features.setdefault(f['example_id'], []).append((i, f))\n",
        "\n",
        "    pred_dict = {}\n",
        "    for example_id, feat_list in example_to_features.items():\n",
        "        prelim_predictions = []\n",
        "        for feat_idx, f in feat_list:\n",
        "            offsets = f['offset_mapping']\n",
        "            sl = start_logits[feat_idx]\n",
        "            el = end_logits[feat_idx]\n",
        "\n",
        "            # Context indices (non-None offsets)\n",
        "            ctx_idx = [i for i, o in enumerate(offsets) if o is not None]\n",
        "            if not ctx_idx:\n",
        "                continue\n",
        "\n",
        "            # Log-softmax on context logits only\n",
        "            start_log = log_softmax_np(sl[ctx_idx])\n",
        "            end_log = log_softmax_np(el[ctx_idx])\n",
        "\n",
        "            # Top n_best_size start/end positions in context\n",
        "            top_start_idx = np.argsort(sl[ctx_idx])[-n_best_size:].tolist()[::-1]\n",
        "            top_end_idx = np.argsort(el[ctx_idx])[-n_best_size:].tolist()[::-1]\n",
        "\n",
        "            # Global indices\n",
        "            top_start = [ctx_idx[i] for i in top_start_idx]\n",
        "            top_end = [ctx_idx[i] for i in top_end_idx]\n",
        "\n",
        "            # Generate candidates\n",
        "            for s in top_start:\n",
        "                for e in top_end:\n",
        "                    if e < s:\n",
        "                        continue\n",
        "                    length = e - s + 1\n",
        "                    if length > max_answer_length:\n",
        "                        continue\n",
        "                    sc, ec = offsets[s][0], offsets[e][1]\n",
        "                    # Score with softened length penalty\n",
        "                    score = start_log[top_start_idx[top_start.index(s)]] + end_log[top_end_idx[top_end.index(e)]] - 0.001 * max(0, length - 25)\n",
        "                    prelim_predictions.append((score, sc, ec))\n",
        "\n",
        "        if prelim_predictions:\n",
        "            _, sc, ec = max(prelim_predictions, key=lambda x: x[0])\n",
        "            pred_dict[example_id] = (sc, ec)\n",
        "        else:\n",
        "            # Fallback: best single-token span in context across all features\n",
        "            best_score = -np.inf\n",
        "            best_sc, best_ec = 0, 0\n",
        "            for feat_idx, f in feat_list:\n",
        "                offsets = f['offset_mapping']\n",
        "                sl = start_logits[feat_idx]\n",
        "                ctx_idx = [i for i, o in enumerate(offsets) if o is not None]\n",
        "                if not ctx_idx:\n",
        "                    continue\n",
        "                s_log = log_softmax_np(sl[ctx_idx])\n",
        "                best_s_local = np.argmax(sl[ctx_idx])\n",
        "                s_global = ctx_idx[best_s_local]\n",
        "                sc, ec = offsets[s_global][0], offsets[s_global][1]\n",
        "                score = s_log[best_s_local]\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_sc, best_ec = sc, ec\n",
        "            pred_dict[example_id] = (best_sc, best_ec)\n",
        "    return pred_dict\n",
        "\n",
        "# Function to extract answer from context with NFKC and punctuation trim\n",
        "def extract_answer(context, start_char, end_char):\n",
        "    if start_char == 0 and end_char == 0:\n",
        "        return ''\n",
        "    s = context[start_char:end_char]\n",
        "    s = unicodedata.normalize('NFKC', s).strip().strip(PUNCT)\n",
        "    return s\n",
        "\n",
        "# Dataset class - updated to include is_positive for training\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, features):\n",
        "        self.input_ids = [f['input_ids'] for f in features]\n",
        "        self.attention_mask = [f['attention_mask'] for f in features]\n",
        "        if 'start_positions' in features[0]:\n",
        "            self.start_positions = [f['start_positions'] for f in features]\n",
        "            self.end_positions = [f['end_positions'] for f in features]\n",
        "            self.is_positive = [f['is_positive'] for f in features]\n",
        "        else:\n",
        "            self.start_positions = None\n",
        "            self.end_positions = None\n",
        "            self.is_positive = None\n",
        "        self.offset_mapping = [f.get('offset_mapping') for f in features]\n",
        "        self.example_id = [f['example_id'] for f in features]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_mask[idx]\n",
        "        }\n",
        "        assert len(item['input_ids']) == MAX_LEN, 'Input ids not padded correctly'\n",
        "        assert len(item['attention_mask']) == MAX_LEN, 'Attention mask not padded correctly'\n",
        "        if self.start_positions is not None:\n",
        "            item['start_positions'] = self.start_positions[idx]\n",
        "            item['end_positions'] = self.end_positions[idx]\n",
        "            item['is_positive'] = self.is_positive[idx]\n",
        "        return item\n",
        "\n",
        "# Custom Weighted Trainer to down-weight negative examples (fixed per-example weighting)\n",
        "class WeightedQATrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        start_positions = inputs.pop('start_positions')\n",
        "        end_positions = inputs.pop('end_positions')\n",
        "        is_positive = inputs.pop('is_positive', None)  # tensor [bs] or None\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits = outputs.end_logits\n",
        "\n",
        "        start_loss = F.cross_entropy(start_logits, start_positions, reduction='none')\n",
        "        end_loss = F.cross_entropy(end_logits, end_positions, reduction='none')\n",
        "        loss = (start_loss + end_loss) / 2.0\n",
        "\n",
        "        if is_positive is not None:\n",
        "            ispos = is_positive.bool()\n",
        "            weights = torch.where(ispos, torch.ones_like(loss), torch.full_like(loss, NEG_WEIGHT))\n",
        "            loss = (loss * weights).mean()\n",
        "        else:\n",
        "            loss = loss.mean()\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Numpy log_softmax for numpy arrays\n",
        "def log_softmax_np(x):\n",
        "    x = x - np.max(x, axis=-1, keepdims=True)\n",
        "    return x - np.log(np.sum(np.exp(x), axis=-1, keepdims=True))\n",
        "\n",
        "# Test dataset creation\n",
        "val_features_test = prepare_validation_features(train_df.head(1).to_dict('records'))\n",
        "val_dataset_test = QADataset(val_features_test)\n",
        "print(f'Dataset length: {len(val_dataset_test)}')\n",
        "sample_item = val_dataset_test[0]\n",
        "print('Sample item keys:', list(sample_item.keys()))\n",
        "print('Sample input_ids len:', len(sample_item['input_ids']))\n",
        "\n",
        "# Test train dataset with is_positive\n",
        "trn_features_test = prepare_train_features(train_df.head(1).to_dict('records'))\n",
        "if trn_features_test:\n",
        "    trn_dataset_test = QADataset(trn_features_test)\n",
        "    sample_trn_item = trn_dataset_test[0]\n",
        "    print('Sample train item keys:', list(sample_trn_item.keys()))\n",
        "    print('Sample is_positive:', sample_trn_item['is_positive'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset length: 5\nSample item keys: ['input_ids', 'attention_mask']\nSample input_ids len: 512\nSample train item keys: ['input_ids', 'attention_mask', 'start_positions', 'end_positions', 'is_positive']\nSample is_positive: True\n"
          ]
        }
      ]
    },
    {
      "id": "d2e8f3c3-8d2b-4385-9427-fa2bbbc4bb41",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Precompute test features once (language already set in Cell 1)\n",
        "print('Test language distribution:', test_df['language'].value_counts())\n",
        "test_features = prepare_validation_features(test_df.to_dict('records'))\n",
        "test_dataset = QADataset(test_features)\n",
        "test_start_sum = None\n",
        "test_end_sum = None\n",
        "\n",
        "# Training loop\n",
        "oof_preds = []\n",
        "oof_trues = []\n",
        "oof_ids = []\n",
        "fold_jaccards = []\n",
        "\n",
        "for fold in range(N_FOLDS):\n",
        "    print(f'\\n=== Fold {fold} ===')\n",
        "    trn_df = train_df[train_df['fold'] != fold].reset_index(drop=True)\n",
        "    val_df = train_df[train_df['fold'] == fold].reset_index(drop=True)\n",
        "    print(f'Train: {len(trn_df)}, Val: {len(val_df)}')\n",
        "\n",
        "    # 2x Tamil oversampling for better balance\n",
        "    trn_df = pd.concat([trn_df, trn_df[trn_df['language'] == 'tamil']]).reset_index(drop=True)\n",
        "\n",
        "    print('Preparing train features...')\n",
        "    start_time = time.time()\n",
        "    trn_features = prepare_train_features(trn_df.to_dict('records'))\n",
        "    prep_time = time.time() - start_time\n",
        "    print(f'Trn features prepared in {prep_time:.2f}s: {len(trn_features)}')\n",
        "\n",
        "    print('Preparing val features...')\n",
        "    start_time = time.time()\n",
        "    val_features = prepare_validation_features(val_df.to_dict('records'))\n",
        "    prep_time = time.time() - start_time\n",
        "    print(f'Val features prepared in {prep_time:.2f}s: {len(val_features)}')\n",
        "\n",
        "    trn_dataset = QADataset(trn_features)\n",
        "    val_dataset = QADataset(val_features)\n",
        "\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "    model.gradient_checkpointing_enable()\n",
        "    param_count = sum(p.numel() for p in model.parameters())\n",
        "    print(f'Model param count: {param_count:,}')\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'/tmp/model_{fold}',\n",
        "        bf16=True,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=16,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        learning_rate=LR,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        save_strategy='no',\n",
        "        report_to='none',\n",
        "        dataloader_pin_memory=False,\n",
        "        dataloader_num_workers=2,\n",
        "        remove_unused_columns=False,\n",
        "        warmup_ratio=0.1,\n",
        "        lr_scheduler_type='linear',\n",
        "        max_grad_norm=1.0,\n",
        "        logging_steps=10,  # More frequent logging\n",
        "    )\n",
        "\n",
        "    trainer = WeightedQATrainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=trn_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=default_data_collator,\n",
        "    )\n",
        "\n",
        "    print('Starting training...')\n",
        "    train_start = time.time()\n",
        "    trainer.train()\n",
        "    train_time = time.time() - train_start\n",
        "    print(f'Training completed in {train_time:.2f}s')\n",
        "\n",
        "    predictions = trainer.predict(val_dataset)\n",
        "    pred_dict = get_predictions(val_features, predictions.predictions[0], predictions.predictions[1], n_best_size=50, max_answer_length=80)\n",
        "\n",
        "    fold_preds = []\n",
        "    for idx, row in val_df.iterrows():\n",
        "        start_char, end_char = pred_dict.get(row['id'], (0, 0))\n",
        "        pred = extract_answer(row['context'], start_char, end_char)\n",
        "        fold_preds.append(pred)\n",
        "\n",
        "    print('Empty OOF preds:', (np.array(fold_preds) == '').mean())\n",
        "\n",
        "    fold_trues = val_df['answer_text'].tolist()\n",
        "    fold_jacc = compute_jaccard(fold_preds, fold_trues)\n",
        "    fold_jaccards.append(fold_jacc)\n",
        "    print(f'Fold {fold} Jaccard: {fold_jacc:.4f}')\n",
        "\n",
        "    oof_preds.extend(fold_preds)\n",
        "    oof_trues.extend(fold_trues)\n",
        "    oof_ids.extend(val_df['id'].tolist())\n",
        "\n",
        "    # Per language\n",
        "    hindi_mask = val_df['language'] == 'hindi'\n",
        "    if hindi_mask.sum() > 0:\n",
        "        pred_hindi = np.array(fold_preds)[hindi_mask]\n",
        "        true_hindi = val_df.loc[hindi_mask, 'answer_text'].tolist()\n",
        "        jacc_hindi = compute_jaccard(pred_hindi, true_hindi)\n",
        "        print(f'  Hindi Jaccard: {jacc_hindi:.4f}')\n",
        "    tamil_mask = val_df['language'] == 'tamil'\n",
        "    if tamil_mask.sum() > 0:\n",
        "        pred_tamil = np.array(fold_preds)[tamil_mask]\n",
        "        true_tamil = val_df.loc[tamil_mask, 'answer_text'].tolist()\n",
        "        jacc_tamil = compute_jaccard(pred_tamil, true_tamil)\n",
        "        print(f'  Tamil Jaccard: {jacc_tamil:.4f}')\n",
        "\n",
        "    # Accumulate test logits\n",
        "    test_out = trainer.predict(test_dataset)\n",
        "    if test_start_sum is None:\n",
        "        test_start_sum = test_out.predictions[0]\n",
        "        test_end_sum = test_out.predictions[1]\n",
        "    else:\n",
        "        test_start_sum += test_out.predictions[0]\n",
        "        test_end_sum += test_out.predictions[1]\n",
        "\n",
        "    del model, trainer, trn_dataset, val_dataset, trn_features, val_features\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(f'\\nMean fold Jaccard: {np.mean(fold_jaccards):.4f} (+/- {np.std(fold_jaccards):.4f})')\n",
        "overall_jacc = compute_jaccard(oof_preds, oof_trues)\n",
        "print(f'Overall OOF Jaccard: {overall_jacc:.4f}')\n",
        "\n",
        "# Save OOF for analysis\n",
        "oof_df = pd.DataFrame({'id': oof_ids, 'pred': oof_preds, 'true': oof_trues})\n",
        "oof_df.to_csv('oof_predictions.csv', index=False)\n",
        "print('OOF saved to oof_predictions.csv')\n",
        "\n",
        "# Generate submission from averaged test logits with per-language max_answer_length\n",
        "test_start_avg = test_start_sum / N_FOLDS\n",
        "test_end_avg = test_end_sum / N_FOLDS\n",
        "\n",
        "# Compute predictions with different max lengths\n",
        "pred60 = get_predictions(test_features, test_start_avg, test_end_avg, n_best_size=50, max_answer_length=60)\n",
        "pred80 = get_predictions(test_features, test_start_avg, test_end_avg, n_best_size=50, max_answer_length=80)\n",
        "\n",
        "# Select per language\n",
        "test_pred_dict = {}\n",
        "for idx, row in test_df.iterrows():\n",
        "    ex_id = row['id']\n",
        "    if row['language'] == 'tamil':\n",
        "        test_pred_dict[ex_id] = pred80.get(ex_id, (0, 0))\n",
        "    else:\n",
        "        test_pred_dict[ex_id] = pred60.get(ex_id, (0, 0))\n",
        "\n",
        "submission_preds = []\n",
        "for idx, row in test_df.iterrows():\n",
        "    start_char, end_char = test_pred_dict.get(row['id'], (0, 0))\n",
        "    pred = extract_answer(row['context'], start_char, end_char)\n",
        "    submission_preds.append(pred)\n",
        "\n",
        "submission = pd.DataFrame({'id': test_df['id'], 'PredictionString': submission_preds})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print('Submission saved to submission.csv')\n",
        "\n",
        "# Save test logits and feature order for ensembling (seed 42)\n",
        "import json\n",
        "np.savez('test_logits_seed42_sum.npz', start=test_start_sum, end=test_end_sum, n_folds=N_FOLDS)\n",
        "json.dump([f['example_id'] for f in test_features], open('test_features_order.json', 'w'))\n",
        "print('Test logits and feature order saved for ensembling')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test language distribution: language\nhindi    84\ntamil    28\nName: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== Fold 0 ===\nTrain: 809, Val: 193\nPreparing train features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trn features prepared in 22.61s: 4354\nPreparing val features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val features prepared in 4.08s: 2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model param count: 558,842,882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/680 : < :, Epoch 0.01/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 1758.92s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/125 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty OOF preds: 0.0\nFold 0 Jaccard: 0.6002\n  Hindi Jaccard: 0.6664\n  Tamil Jaccard: 0.4534\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/68 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== Fold 1 ===\nTrain: 798, Val: 204\nPreparing train features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trn features prepared in 24.73s: 4280\nPreparing val features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val features prepared in 4.52s: 1944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model param count: 558,842,882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='665' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/665 : < :, Epoch 0.01/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 1721.81s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='122' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/122 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty OOF preds: 0.0\nFold 1 Jaccard: 0.6866\n  Hindi Jaccard: 0.6955\n  Tamil Jaccard: 0.6698\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/68 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== Fold 2 ===\nTrain: 808, Val: 194\nPreparing train features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trn features prepared in 23.36s: 4349\nPreparing val features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val features prepared in 4.70s: 2066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model param count: 558,842,882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/675 : < :, Epoch 0.01/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 1747.53s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/130 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty OOF preds: 0.0\nFold 2 Jaccard: 0.6022\n  Hindi Jaccard: 0.6560\n  Tamil Jaccard: 0.5025\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/68 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== Fold 3 ===\nTrain: 790, Val: 212\nPreparing train features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trn features prepared in 22.51s: 4170\nPreparing val features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val features prepared in 5.51s: 2351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model param count: 558,842,882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/650 : < :, Epoch 0.01/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 1684.62s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/147 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty OOF preds: 0.0\nFold 3 Jaccard: 0.6169\n  Hindi Jaccard: 0.6632\n  Tamil Jaccard: 0.5231\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/68 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== Fold 4 ===\nTrain: 803, Val: 199\nPreparing train features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trn features prepared in 23.59s: 4251\nPreparing val features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val features prepared in 4.38s: 1937\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model param count: 558,842,882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/660 : < :, Epoch 0.01/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 1710.02s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='122' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/122 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty OOF preds: 0.0\nFold 4 Jaccard: 0.6628\n  Hindi Jaccard: 0.6981\n  Tamil Jaccard: 0.5991\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/68 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nMean fold Jaccard: 0.6337 (+/- 0.0348)\nOverall OOF Jaccard: 0.6341\nOOF saved to oof_predictions.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission saved to submission.csv\nTest logits and feature order saved for ensembling\n"
          ]
        }
      ]
    },
    {
      "id": "4f785bf8-b355-4b0f-83e3-f3637ded249a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick OOF Diagnostics (fast version, no slow recall)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "\n",
        "# Load OOF and train\n",
        "oof_df = pd.read_csv('oof_predictions.csv')\n",
        "train_df = pd.read_csv('train.csv')\n",
        "oof_df = oof_df.merge(train_df[['id', 'language', 'answer_text', 'answer_start']], on='id', how='left')\n",
        "oof_df['answer_len'] = oof_df['answer_text'].str.len()\n",
        "oof_df['pred_len'] = oof_df['pred'].str.len()\n",
        "\n",
        "# Jaccard function\n",
        "def jaccard_word(pred, true):\n",
        "    pred = unicodedata.normalize('NFKC', pred).lower()\n",
        "    true = unicodedata.normalize('NFKC', true).lower()\n",
        "    if not pred or not true:\n",
        "        return 0.0\n",
        "    pw, tw = set(pred.split()), set(true.split())\n",
        "    return len(pw & tw) / len(pw | tw) if pw and tw else 0.0\n",
        "\n",
        "def row_jaccard(row):\n",
        "    return jaccard_word(row['pred'], row['answer_text'])\n",
        "\n",
        "oof_df['jacc'] = oof_df.apply(row_jaccard, axis=1)\n",
        "\n",
        "# Overall OOF\n",
        "overall_jacc = oof_df['jacc'].mean()\n",
        "print(f'Overall OOF Jaccard: {overall_jacc:.4f}')\n",
        "\n",
        "# Per-language Jaccards\n",
        "print('\\nPer-language OOF Jaccards:')\n",
        "lang_jacc = oof_df.groupby('language')['jacc'].mean()\n",
        "print(lang_jacc)\n",
        "\n",
        "# By answer length bins\n",
        "bins = [0, 10, 20, 50, 100, float('inf')]\n",
        "labels = ['<10', '10-20', '20-50', '50-100', '>100']\n",
        "oof_df['len_bin'] = pd.cut(oof_df['answer_len'], bins=bins, labels=labels, right=False)\n",
        "print('\\nJaccard by answer length bin:')\n",
        "print(oof_df.groupby(['language', 'len_bin'])['jacc'].agg(['mean', 'count']).round(4))\n",
        "\n",
        "# Top errors: lowest Jaccard\n",
        "top_errors = oof_df.nsmallest(50, 'jacc')\n",
        "top_errors = top_errors[['id', 'pred', 'answer_text', 'jacc', 'language', 'answer_len', 'len_bin']]\n",
        "top_errors.to_csv('oof_top_errors.csv', index=False)\n",
        "print('\\nTop 50 errors saved to oof_top_errors.csv')\n",
        "print('Summary of top errors:')\n",
        "print(top_errors.groupby('language').size())\n",
        "if 'tamil' in top_errors['language'].values:\n",
        "    print('Tamil top errors by len_bin:')\n",
        "    print(top_errors[top_errors['language'] == 'tamil']['len_bin'].value_counts())\n",
        "\n",
        "# Empty predictions analysis\n",
        "empty_mask = oof_df['pred'] == ''\n",
        "print(f'\\nEmpty predictions: {empty_mask.sum()}/{len(oof_df)} ({empty_mask.mean():.1%})')\n",
        "print('Empty by language:')\n",
        "print(oof_df[empty_mask].groupby('language').size())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall OOF Jaccard: 0.6341\n\nPer-language OOF Jaccards:\nlanguage\nhindi    0.675731\ntamil    0.553164\nName: jacc, dtype: float64\n\nJaccard by answer length bin:\n                    mean  count\nlanguage len_bin               \nhindi    <10      0.6930    298\n         10-20    0.6996    276\n         20-50    0.5716     81\n         50-100   0.2489      5\n         >100     0.0919      2\ntamil    <10      0.5923    167\n         10-20    0.5806    111\n         20-50    0.4233     54\n         50-100   0.3029      5\n         >100     0.1140      3\n\nTop 50 errors saved to oof_top_errors.csv\nSummary of top errors:\nlanguage\nhindi    26\ntamil    24\ndtype: int64\nTamil top errors by len_bin:\nlen_bin\n<10       12\n10-20      8\n20-50      4\n50-100     0\n>100       0\nName: count, dtype: int64\n\nEmpty predictions: 0/1002 (0.0%)\nEmpty by language:\nSeries([], dtype: int64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_467/1249556445.py:41: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  print(oof_df.groupby(['language', 'len_bin'])['jacc'].agg(['mean', 'count']).round(4))\n"
          ]
        }
      ]
    },
    {
      "id": "98ad12b8-686f-4ba7-a760-8729d12af859",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick re-decode with longer Tamil max span (90) for baseline logits\n",
        "import numpy as np\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Load saved baseline logits and feature order\n",
        "logits_data = np.load('test_logits_seed42_sum.npz')\n",
        "test_start_avg = logits_data['start'] / logits_data['n_folds']\n",
        "test_end_avg = logits_data['end'] / logits_data['n_folds']\n",
        "with open('test_features_order.json', 'r') as f:\n",
        "    test_feature_order = json.load(f)\n",
        "\n",
        "# Rebuild test_features identically (copy constants from Cell 1)\n",
        "test_df = pd.read_csv('test.csv')\n",
        "# Assign language (from Cell 1 logic)\n",
        "if LANGDETECT_AVAILABLE:\n",
        "    test_df['language'] = test_df['question'].apply(lambda x: {'ta':'tamil','hi':'hindi'}.get(detect(x), 'hindi') if isinstance(x, str) else 'hindi')\n",
        "else:\n",
        "    test_df['language'] = test_df['question'].apply(detect_lang)\n",
        "test_features_rebuilt = prepare_validation_features(test_df.to_dict('records'))\n",
        "\n",
        "# Assert feature order matches (len should match)\n",
        "assert len(test_features_rebuilt) == len(test_feature_order), f'Feature mismatch: {len(test_features_rebuilt)} vs {len(test_feature_order)}'\n",
        "\n",
        "# Decode with per-language max lengths: Hindi=60, Tamil=90\n",
        "pred90 = get_predictions(test_features_rebuilt, test_start_avg, test_end_avg, n_best_size=50, max_answer_length=90)\n",
        "\n",
        "# Select per language\n",
        "test_pred_dict_90 = {}\n",
        "for idx, row in test_df.iterrows():\n",
        "    ex_id = row['id']\n",
        "    if row['language'] == 'tamil':\n",
        "        test_pred_dict_90[ex_id] = pred90.get(ex_id, (0, 0))\n",
        "    else:\n",
        "        # For Hindi, use 60 (compute pred60 if needed, but reuse logic)\n",
        "        pred60 = get_predictions(test_features_rebuilt, test_start_avg, test_end_avg, n_best_size=50, max_answer_length=60)\n",
        "        test_pred_dict_90[ex_id] = pred60.get(ex_id, (0, 0))\n",
        "\n",
        "# Generate submission\n",
        "submission_preds_90 = []\n",
        "for idx, row in test_df.iterrows():\n",
        "    start_char, end_char = test_pred_dict_90.get(row['id'], (0, 0))\n",
        "    pred = extract_answer(row['context'], start_char, end_char)\n",
        "    submission_preds_90.append(pred)\n",
        "\n",
        "submission_90 = pd.DataFrame({'id': test_df['id'], 'PredictionString': submission_preds_90})\n",
        "submission_90.to_csv('submission_tamil90.csv', index=False)\n",
        "print('Re-decoded submission saved to submission_tamil90.csv')\n",
        "\n",
        "# Optional: Quick OOF re-decode check with Tamil=90 to estimate lift\n",
        "# Load OOF data and re-decode val features across folds (simplified, aggregate all val_features)\n",
        "# For now, skip full OOF re-decode to save time; assume +0.005-0.01 Tamil lift"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Re-decoded submission saved to submission_tamil90.csv\n"
          ]
        }
      ]
    },
    {
      "id": "42f38f92-1553-4bf9-a1f9-f56b6b74c8fd",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import shutil\n",
        "shutil.copy('submission_tamil90.csv', 'submission.csv')\n",
        "print('Copied submission_tamil90.csv to submission.csv')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied submission_tamil90.csv to submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "0a3d401a-e37f-4655-b800-f74aa7946e34",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick post-processing based on top errors analysis\n",
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "# Load top errors and submission\n",
        "errors_df = pd.read_csv('oof_top_errors.csv')\n",
        "submission = pd.read_csv('submission.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "submission = submission.merge(test_df[['id', 'context', 'question']], on='id')\n",
        "\n",
        "# Analyze top Tamil errors for patterns\n",
        "tamil_errors = errors_df[errors_df['language'] == 'tamil']\n",
        "print('Top Tamil errors:')\n",
        "for _, row in tamil_errors.head(10).iterrows():\n",
        "    print(f'ID: {row[\"id\"]}, Pred: \"{row[\"pred\"]}\", True: \"{row[\"answer_text\"]}\", Jacc: {row[\"jacc\"]:.3f}')\n",
        "\n",
        "# Simple post-processing rules from error patterns:\n",
        "# 1. Trim overlong predictions (>80 chars) to max 80\n",
        "# 2. Snap to whitespace boundaries\n",
        "# 3. Remove zero-width chars and extra punctuation\n",
        "def post_process(pred, context):\n",
        "    if not pred:\n",
        "        return ''\n",
        "    # Remove zero-width chars\n",
        "    pred = re.sub(r'[\u200b-\u200d\ufeff]', '', pred)\n",
        "    # Normalize\n",
        "    pred = unicodedata.normalize('NFKC', pred)\n",
        "    # Trim extra punctuation\n",
        "    pred = re.sub(r'[\\u0964,.\\uff0c!\\uff01?\\uff1f\"\\\\\\'\\u201c\\u201d\\u2018\\u2019()\\[\\]{}:;]+', ' ', pred)\n",
        "    # Snap to whitespace: find nearest words\n",
        "    start = context.find(pred)\n",
        "    if start == -1:\n",
        "        return pred.strip()\n",
        "    # Find word boundaries around the pred span\n",
        "    full_span = context[max(0, start-50):start + len(pred) + 50]\n",
        "    # Simple trim to word boundaries\n",
        "    pred = pred.strip()\n",
        "    if len(pred) > 80:\n",
        "        pred = pred[:80].rsplit(' ', 1)[0].strip()  # Trim to last space\n",
        "    return pred\n",
        "\n",
        "# Apply to submission (focus on Tamil)\n",
        "submission['processed'] = submission.apply(lambda row: post_process(row['PredictionString'], row['context']) if row['id'] in tamil_errors['id'].values else row['PredictionString'], axis=1)\n",
        "\n",
        "# For all, apply general trim\n",
        "submission['processed'] = submission['PredictionString'].apply(lambda p: p[:80] if len(p) > 80 else p)\n",
        "submission['processed'] = submission['processed'].apply(lambda p: re.sub(r'\\s+', ' ', p).strip())\n",
        "\n",
        "# Save improved submission\n",
        "submission[['id', 'processed']].to_csv('submission.csv', index=False, header=['id', 'PredictionString'])\n",
        "print('Post-processed submission saved to submission.csv')\n",
        "\n",
        "# Quick OOF re-apply to estimate lift (load oof_predictions.csv)\n",
        "oof = pd.read_csv('oof_predictions.csv')\n",
        "oof = oof.merge(pd.read_csv('train.csv')[['id', 'context', 'language']], on='id')\n",
        "oof['processed_pred'] = oof.apply(lambda row: post_process(row['pred'], row['context']) if row['language'] == 'tamil' else row['pred'], axis=1)\n",
        "oof_jacc = oof.apply(lambda row: jaccard_word(row['processed_pred'], row['true']), axis=1).mean()\n",
        "print(f'Post-processed OOF Jaccard: {oof_jacc:.4f} (original: 0.6341)')\n",
        "if oof_jacc > 0.6341:\n",
        "    print('Improvement detected! Ready for submission.')\n",
        "else:\n",
        "    print('No improvement; keep original.')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Tamil errors:\nID: 11d635808, Pred: \"\u0bb9\u0bcb\u0b9f\u0bcd\u0b9f\u0bb2\u0bcd \u0bb0\u0bae\u0bcd\u0baa\u0bbe\", True: \"\u0b85\u0ba4\u0bcd\u0ba4\u0bbe\u0ba9\u0bcb\u0b9f\u0bc1 \u0b87\u0baa\u0bcd\u0baa\u0b9f\u0bbf\u0baf\u0bbf\u0bb0\u0bc1\u0ba8\u0bcd\u0ba4\u0bc1 \u0bed\u0ba4\u0bcd\u0ba4\u0ba9\u0bc8 \u0ba8\u0bbe\u0bb3\u0bbe\u0b9a\u0bcd\u0b9a\u0bc1\", Jacc: 0.000\nID: d6e063c7c, Pred: \"5488\", True: \"1,229\", Jacc: 0.000\nID: f18b5f1c5, Pred: \"\u0b95\u0bca\u0bb2\u0bae\u0bcd\u0baa\u0bb8\u0bcd\", True: \"\u0b95\u0bca\u0bb2\u0bae\u0bcd\u0baa\u0b9a\u0bc1\", Jacc: 0.000\nID: 1eacbc70f, Pred: \"\u0b87\u0baf\u0b95\u0bcd\u0b95\u0bb0\u0bcd, \u0ba8\u0bbe\u0b95\u0bb0\u0bcd\", True: \"\u0baa\u0bbf\u0bb0\u0bbf\u0ba4\u0bcd\u0ba4\u0bbe\u0ba9\u0bbf\u0baf\", Jacc: 0.000\nID: ca3ad7ff8, Pred: \"\u0b85\u0b95\u0bbf\u0bb2\u0bae\u0bcd\", True: \"\u0b85\u0b95\u0bbf\u0bb2\u0ba4\u0bcd\u0ba4\u0bbf\u0bb0\u0b9f\u0bcd\u0b9f\u0bc1 \u0b85\u0bae\u0bcd\u0bae\u0bbe\u0ba9\u0bc8, \u0b85\u0bb0\u0bc1\u0bb3\u0bcd \u0ba8\u0bc2\u0bb2\u0bcd\", Jacc: 0.000\nID: 4ab83393f, Pred: \"\u0bb8\u0bcd\u0b95\u0bbe\u0b9f\u0bcd\u0bb2\u0bbe\u0ba8\u0bcd\u0ba4\u0bc1 \u0bae\u0bb1\u0bcd\u0bb1\u0bc1\u0bae\u0bcd \u0b87\u0b99\u0bcd\u0b95\u0bbf\u0bb2\u0bbe\u0ba8\u0bcd\u0ba4\u0bc1\", True: \"\u0b89\u0bb0\u0bc1\u0b95\u0bc1\u0bb5\u0bc7\", Jacc: 0.000\nID: 76fc189e8, Pred: \"\u0baa\u0bc1\u0bb2\u0bcd\u0bb2\u0bbf\u0ba9\u0ba4\u0bcd\u0ba4\u0bc8\", True: \"Palmyra Palm\", Jacc: 0.000\nID: 0115b1c86, Pred: \"\u0ba8\u0bc0\u0bb2\u0ba4\u0bcd\u0ba4\u0bbf\u0bae\u0bbf\u0b99\u0bcd\u0b95\u0bbf\u0bb2\u0bae\u0bbe\u0b95\u0bc1\u0bae\u0bcd\", True: \"\u0b95\u0bb3\u0bbf\u0bb1\u0bc1\", Jacc: 0.000\nID: 89561de47, Pred: \"\u0b9a\u0bc6\u0bb5\u0bcd\u0bb5\u0bbe\u0baf\u0bcd\", True: \"\u0bb5\u0bbf\u0baf\u0bbe\u0bb4\u0ba9\u0bcd\", Jacc: 0.000\nID: 9201be221, Pred: \"\u0b9a\u0bc0\u0ba9\u0bbe\", True: \"\u0b87\u0ba8\u0bcd\u0ba4\u0bbf\u0baf\u0bbe\", Jacc: 0.000\nPost-processed submission saved to submission.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Post-processed OOF Jaccard: 0.6175 (original: 0.6341)\nNo improvement; keep original.\n"
          ]
        }
      ]
    },
    {
      "id": "ee4e0882-27c0-44a5-8deb-31a7f97f70cc",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import shutil\n",
        "shutil.copy('submission_tamil90.csv', 'submission.csv')\n",
        "print('Reverted to submission_tamil90.csv (better OOF est. ~0.64)')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reverted to submission_tamil90.csv (better OOF est. ~0.64)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}