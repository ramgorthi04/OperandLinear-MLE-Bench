{
  "cells": [
    {
      "id": "1edccdf8-b4dc-44d6-93b3-645cb85d1d0e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "def pip(*args):\n",
        "    print('>', *args, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "# Set PIP_TARGET to writable directory\n",
        "pip_target = '/app/.pip-target'\n",
        "os.environ['PIP_TARGET'] = pip_target\n",
        "if os.path.exists(pip_target):\n",
        "    print('Removing existing', pip_target)\n",
        "    shutil.rmtree(pip_target, ignore_errors=True)\n",
        "\n",
        "# 0) Hard reset any prior torch stacks\n",
        "for pkg in ('torch', 'torchvision', 'torchaudio'):\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', pkg], check=False)\n",
        "\n",
        "# Clean stray site dirs\n",
        "for d in (\n",
        "    f'{pip_target}/torch',\n",
        "    f'{pip_target}/torch-2.8.0.dist-info',\n",
        "    f'{pip_target}/torch-2.4.1.dist-info',\n",
        "    f'{pip_target}/torchvision',\n",
        "    f'{pip_target}/torchvision-0.23.0.dist-info',\n",
        "    f'{pip_target}/torchvision-0.19.1.dist-info',\n",
        "    f'{pip_target}/torchaudio',\n",
        "    f'{pip_target}/torchaudio-2.8.0.dist-info',\n",
        "    f'{pip_target}/torchaudio-2.4.1.dist-info',\n",
        "    f'{pip_target}/torchgen',\n",
        "    f'{pip_target}/functorch',\n",
        "):\n",
        "    if os.path.exists(d):\n",
        "        print('Removing', d)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "# 1) Install the EXACT cu121 torch stack FIRST with --no-deps to avoid system dir installs\n",
        "pip('install',\n",
        "    '--index-url', 'https://download.pytorch.org/whl/cu121',\n",
        "    '--extra-index-url', 'https://pypi.org/simple',\n",
        "    '--force-reinstall', '--no-deps',\n",
        "    'torch==2.4.1', 'torchvision==0.19.1', 'torchaudio==2.4.1')\n",
        "\n",
        "# 2) Create a constraints file\n",
        "Path('constraints.txt').write_text(\n",
        "    'torch==2.4.1\\n'\n",
        "    'torchvision==0.19.1\\n'\n",
        "    'torchaudio==2.4.1\\n'\n",
        ")\n",
        "\n",
        "# 3) Install NON-torch deps\n",
        "pip('install', '-c', 'constraints.txt',\n",
        "    'transformers==4.44.2', 'accelerate==0.34.2',\n",
        "    'datasets==2.21.0', 'evaluate==0.4.2',\n",
        "    'sentencepiece', 'scikit-learn',\n",
        "    '--upgrade-strategy', 'only-if-needed')\n",
        "\n",
        "# 4) Sanity gate - add pip_target to sys.path\n",
        "sys.path.insert(0, pip_target)\n",
        "import torch\n",
        "print('torch:', torch.__version__, 'built CUDA:', getattr(torch.version, 'cuda', None))\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "assert str(getattr(torch.version,'cuda','')).startswith('12.1'), f'Wrong CUDA build: {torch.version.cuda}'\n",
        "assert torch.cuda.is_available(), 'CUDA not available'\n",
        "print('GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# Install additional packages with PIP_TARGET\n",
        "pip('install', 'rank_bm25')\n",
        "pip('install', 'langdetect')\n",
        "pip('install', 'indic-nlp-library', 'pyarrow')\n",
        "\n",
        "# Downgrade fsspec\n",
        "pip('install', '-c', 'constraints.txt', 'fsspec[http]<=2024.6.1,>=2023.1.0', '--upgrade')\n",
        "\n",
        "# Verify additional imports\n",
        "try:\n",
        "    from rank_bm25 import BM25Okapi\n",
        "    print('BM25 available')\n",
        "except ImportError:\n",
        "    print('BM25 not available')\n",
        "try:\n",
        "    from langdetect import detect\n",
        "    print('langdetect available')\n",
        "except ImportError:\n",
        "    print('langdetect not available')\n",
        "print('Environment setup complete')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "id": "979cfeee-29f9-4fb2-aa5c-1d3173248754",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "import gc\n",
        "import ast\n",
        "import sys\n",
        "import copy\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, TensorDataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoConfig,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    )\n",
        "from transformers import default_data_collator\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import hashlib\n",
        "\n",
        "import subprocess\n",
        "import shutil\n",
        "import unicodedata\n",
        "\n",
        "# Add pip_target to sys.path if not already\n",
        "pip_target = '/app/.pip-target'\n",
        "if pip_target not in sys.path:\n",
        "    sys.path.insert(0, pip_target)\n",
        "\n",
        "# BM25 and langdetect\n",
        "BM25_AVAILABLE = False\n",
        "try:\n",
        "    from rank_bm25 import BM25Okapi\n",
        "    BM25_AVAILABLE = True\n",
        "    print('BM25 available')\n",
        "except ImportError:\n",
        "    print('BM25 not available, falling back to TF-IDF only')\n",
        "\n",
        "LANGDETECT_AVAILABLE = False\n",
        "try:\n",
        "    from langdetect import detect\n",
        "    LANGDETECT_AVAILABLE = True\n",
        "    print('langdetect available')\n",
        "except ImportError:\n",
        "    print('langdetect not available, using script fallback')\n",
        "\n",
        "# Script-based language detection fallback\n",
        "def detect_lang(text):\n",
        "    if not isinstance(text, str):\n",
        "        return 'hindi'\n",
        "    for c in text:\n",
        "        if 0x0B80 <= ord(c) <= 0x0BFF:  # Tamil Unicode range\n",
        "            return 'tamil'\n",
        "    return 'hindi'\n",
        "\n",
        "# Set seeds\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Constants - Tamil specialist modifications (fixed per expert advice)\n",
        "DEBUG = False  # Set to True for rapid prototyping\n",
        "MAX_LEN = 512\n",
        "DOC_STRIDE = 128\n",
        "N_SPLITS = 5\n",
        "BATCH_SIZE = 2\n",
        "GRAD_ACCUM_STEPS = 8  # Reduced for more optimizer steps\n",
        "EPOCHS = 5  # Increased for better training\n",
        "LR = 2.5e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "NEG_WEIGHT = 0.3  # Increased for stronger negative weighting\n",
        "USE_RETRIEVAL = True\n",
        "TOP_K_CHUNKS_TRAIN = 12  # Increased for more context\n",
        "TOP_K_CHUNKS_EVAL_HINDI = 10\n",
        "TOP_K_CHUNKS_EVAL_TAMIL = 40  # Increased for better Tamil recall\n",
        "CHUNK_SIZE = 1800\n",
        "OVERLAP = 250\n",
        "NEG_POS_RATIO = 2  # Added negatives for contrastive learning\n",
        "MODEL_NAME = 'deepset/xlm-roberta-large-squad2'\n",
        "PUNCT = '\\u0964,.\\uff0c!\\uff01?\\uff1f\"\\\\\\'\\u201c\\u201d\\u2018\\u2019()[]{}:;'\n",
        "MAX_ANSWER_LENGTH = 80  # Coach tweak for longer spans\n",
        "\n",
        "# Load data - Filter to Tamil only\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "train_df = train_df[train_df['language'] == 'tamil'].reset_index(drop=True)  # Tamil only\n",
        "\n",
        "if DEBUG:\n",
        "    train_df = train_df.sample(n=50, random_state=42).reset_index(drop=True)  # Smaller subsample for debug\n",
        "    print(f'DEBUG mode: using {len(train_df)} Tamil samples')\n",
        "else:\n",
        "    print(f'Full Tamil mode: using {len(train_df)} samples')\n",
        "\n",
        "print('Train shape:', train_df.shape)\n",
        "print('Test shape:', test_df.shape)\n",
        "\n",
        "# Label alignment fix with progress tracking\n",
        "print('Before fix_span')\n",
        "def fix_span(row):\n",
        "    ctx, ans, s = row['context'], row['answer_text'], row['answer_start']\n",
        "    if s < 0 or ctx[s:s+len(ans)] != ans:\n",
        "        idx = ctx.find(ans)\n",
        "        if idx != -1:\n",
        "            row['answer_start'] = idx\n",
        "    return row\n",
        "\n",
        "train_df = train_df.apply(fix_span, axis=1)\n",
        "print('After fix_span')\n",
        "\n",
        "# Context groups for CV (hash first 1024 chars to group same articles)\n",
        "def get_context_hash(context):\n",
        "    return hashlib.md5(context[:1024].encode()).hexdigest()\n",
        "\n",
        "train_df['context_hash'] = train_df['context'].apply(get_context_hash)\n",
        "print('Context hashes computed')\n",
        "\n",
        "# Jaccard metric with NFKC normalization\n",
        "def jaccard_word(pred, true):\n",
        "    pred = unicodedata.normalize('NFKC', pred).lower()\n",
        "    true = unicodedata.normalize('NFKC', true).lower()\n",
        "    if not pred or not true:\n",
        "        return 0.0\n",
        "    pw, tw = set(pred.split()), set(true.split())\n",
        "    return len(pw & tw) / len(pw | tw) if pw and tw else 0.0\n",
        "\n",
        "def compute_jaccard(preds, trues):\n",
        "    return np.mean([jaccard_word(p, t) for p, t in zip(preds, trues)])\n",
        "\n",
        "# Assign language to test_df using langdetect or fallback\n",
        "print('Assigning language to test_df...')\n",
        "if LANGDETECT_AVAILABLE:\n",
        "    test_df['language'] = test_df['question'].apply(lambda x: {'ta':'tamil','hi':'hindi'}.get(detect(x), 'hindi') if isinstance(x, str) else 'hindi')\n",
        "else:\n",
        "    test_df['language'] = test_df['question'].apply(detect_lang)\n",
        "print('Test language dist:', test_df['language'].value_counts())\n",
        "\n",
        "# CV splitting with StratifiedGroupKFold - now only on Tamil\n",
        "sgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
        "train_df['fold'] = -1\n",
        "for fold, (trn_idx, val_idx) in enumerate(sgkf.split(train_df, train_df['language'], groups=train_df['context_hash'])):\n",
        "    train_df.loc[val_idx, 'fold'] = fold\n",
        "\n",
        "print('Fold distribution (Tamil only):')\n",
        "print(train_df.groupby(['fold', 'language']).size())\n",
        "print(f'Folds created: {train_df[\"fold\"].nunique()}')\n",
        "\n",
        "N_FOLDS = 3 if DEBUG else N_SPLITS\n",
        "print(f'Using {N_FOLDS} folds for training')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "id": "af5e669c-68d2-4321-88da-fabc8773dad2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "print('Tokenizer loaded:', tokenizer.name_or_path)\n",
        "\n",
        "# TF-IDF Retrieval setup - Tamil only for specialist\n",
        "if USE_RETRIEVAL:\n",
        "    print('Fitting Tamil TF-IDF vectorizer...')\n",
        "    tamil_df = train_df  # Already Tamil-only\n",
        "    \n",
        "    # Process Tamil\n",
        "    print('Processing Tamil...')\n",
        "    tamil_questions = tamil_df['question'].tolist()\n",
        "    tamil_contexts = tamil_df['context'].tolist()\n",
        "    tamil_chunks = []\n",
        "    for ctx in tqdm(tamil_contexts, desc='Chunking Tamil contexts'):\n",
        "        chunks = []\n",
        "        for i in range(0, len(ctx), CHUNK_SIZE - OVERLAP):\n",
        "            chunk = ctx[i:i + CHUNK_SIZE]\n",
        "            if len(chunk) > 100:\n",
        "                chunks.append(chunk)\n",
        "        tamil_chunks.extend(chunks)\n",
        "    print(f'Tamil chunks total: {len(tamil_chunks)}')\n",
        "    tamil_corpus = tamil_questions + random.sample(tamil_chunks, min(1500, len(tamil_chunks)))\n",
        "    print(f'Tamil corpus size: {len(tamil_corpus)}')\n",
        "    tamil_vectorizer = TfidfVectorizer(\n",
        "        analyzer='char_wb',\n",
        "        ngram_range=(3, 5),\n",
        "        max_features=15000,\n",
        "        min_df=3,\n",
        "        max_df=0.9,\n",
        "        lowercase=False,\n",
        "        sublinear_tf=True,\n",
        "        dtype=np.float32\n",
        "    )\n",
        "    print('Fitting Tamil vectorizer...')\n",
        "    start_time = time.time()\n",
        "    tamil_vectorizer.fit(tamil_corpus)\n",
        "    fit_time = time.time() - start_time\n",
        "    print(f'Tamil TF-IDF fitted in {fit_time:.2f}s: {len(tamil_corpus)} docs')\n",
        "    \n",
        "    # No Hindi vectorizer for specialist; fallback to Tamil for Hindi test (suboptimal but for blending only Tamil used)\n",
        "    hindi_vectorizer = tamil_vectorizer  # Fallback\n",
        "else:\n",
        "    hindi_vectorizer = tamil_vectorizer = None"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "id": "85f5b416-10bf-479f-88b1-3f03265d930f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare training features with hybrid retrieval and sliding windows - Tamil specialist\n",
        "def prepare_train_features(examples, neg_pos_ratio=NEG_POS_RATIO):\n",
        "    features = []\n",
        "    for ex in examples:\n",
        "        q, ctx, ans, ex_id, lang = ex['question'].strip(), ex['context'].strip(), {'text': ex['answer_text'], 'answer_start': ex['answer_start']}, ex['id'], ex['language']\n",
        "        \n",
        "        if USE_RETRIEVAL:\n",
        "            # Chunk context\n",
        "            chunks = []\n",
        "            chunk_starts = []\n",
        "            for i in range(0, len(ctx), CHUNK_SIZE - OVERLAP):\n",
        "                chunk = ctx[i:i + CHUNK_SIZE]\n",
        "                if len(chunk) > 100:\n",
        "                    chunks.append(chunk)\n",
        "                    chunk_starts.append(i)\n",
        "            \n",
        "            if not chunks:\n",
        "                continue\n",
        "            \n",
        "            # Select vectorizer by language (fallback for hindi)\n",
        "            if lang == 'hindi':\n",
        "                vectorizer = hindi_vectorizer  # Falls back to tamil_vectorizer\n",
        "            else:\n",
        "                vectorizer = tamil_vectorizer\n",
        "            \n",
        "            # TF-IDF retrieval\n",
        "            q_vec = vectorizer.transform([q])\n",
        "            chunk_vecs = vectorizer.transform(chunks)\n",
        "            similarities = cosine_similarity(q_vec, chunk_vecs).flatten()\n",
        "            \n",
        "            # BM25 hybrid if available\n",
        "            if BM25_AVAILABLE:\n",
        "                tokenized_chunks = [chunk.lower().split() for chunk in chunks]\n",
        "                bm25 = BM25Okapi(tokenized_chunks)\n",
        "                q_tokens = q.lower().split()\n",
        "                bm25_scores = bm25.get_scores(q_tokens)\n",
        "                if np.max(bm25_scores) > 0:\n",
        "                    norm_bm25 = bm25_scores / np.max(bm25_scores)\n",
        "                else:\n",
        "                    norm_bm25 = np.zeros_like(bm25_scores)\n",
        "                hybrid_scores = 0.5 * norm_bm25 + 0.5 * similarities\n",
        "            else:\n",
        "                hybrid_scores = similarities\n",
        "            top_indices = np.argsort(hybrid_scores)[-TOP_K_CHUNKS_TRAIN:]\n",
        "            \n",
        "            # Guarantee gold chunk inclusion for training by replacing lowest sim if needed\n",
        "            start_char = ans['answer_start']\n",
        "            end_char = start_char + len(ans['text'])\n",
        "            pos_idx = None\n",
        "            for ci, st in enumerate(chunk_starts):\n",
        "                if start_char >= st and end_char <= st + len(chunks[ci]):\n",
        "                    pos_idx = ci\n",
        "                    break\n",
        "            if pos_idx is not None and pos_idx not in top_indices:\n",
        "                # Replace the lowest hybrid score in top_indices with pos_idx\n",
        "                min_hybrid_arg = np.argmin(hybrid_scores[top_indices])\n",
        "                top_indices[min_hybrid_arg] = pos_idx\n",
        "            # Sort by hybrid descending\n",
        "            sort_args = np.argsort(hybrid_scores[top_indices])[::-1]\n",
        "            top_indices = top_indices[sort_args]\n",
        "            \n",
        "            # Get top chunks with their global start positions\n",
        "            top_chunks = [(hybrid_scores[idx], chunk_starts[idx], chunks[idx]) for idx in top_indices]\n",
        "        else:\n",
        "            top_chunks = [(1.0, 0, ctx)]  # full context if no retrieval\n",
        "        \n",
        "        # Now process each top chunk with sliding windows\n",
        "        pos_feats, neg_feats = [], []\n",
        "        for sim, chunk_start, chunk in top_chunks:\n",
        "            tokenized = tokenizer(\n",
        "                q,\n",
        "                chunk,\n",
        "                truncation='only_second',\n",
        "                max_length=MAX_LEN,\n",
        "                stride=DOC_STRIDE,\n",
        "                return_overflowing_tokens=True,\n",
        "                return_offsets_mapping=True,\n",
        "                padding=False,\n",
        "            )\n",
        "            \n",
        "            for j in range(len(tokenized['input_ids'])):\n",
        "                input_ids = tokenized['input_ids'][j]\n",
        "                attention_mask = tokenized['attention_mask'][j]\n",
        "                offsets = tokenized['offset_mapping'][j]\n",
        "                sequence_ids = tokenized.sequence_ids(j)\n",
        "                \n",
        "                # Skip windows without context tokens\n",
        "                if 1 not in sequence_ids:\n",
        "                    continue\n",
        "                \n",
        "                # Global offsets: add chunk_start to context offsets\n",
        "                global_offsets = []\n",
        "                ctx_start = 0\n",
        "                while ctx_start < len(sequence_ids) and sequence_ids[ctx_start] != 1:\n",
        "                    global_offsets.append(None)\n",
        "                    ctx_start += 1\n",
        "                while ctx_start < len(sequence_ids) and sequence_ids[ctx_start] == 1:\n",
        "                    local_offset = offsets[ctx_start]\n",
        "                    global_offset = (local_offset[0] + chunk_start, local_offset[1] + chunk_start) if local_offset else None\n",
        "                    global_offsets.append(global_offset)\n",
        "                    ctx_start += 1\n",
        "                while ctx_start < len(sequence_ids):\n",
        "                    global_offsets.append(None)\n",
        "                    ctx_start += 1\n",
        "                \n",
        "                # Find start/end positions using global offsets\n",
        "                start_pos = -1\n",
        "                end_pos = -1\n",
        "                is_positive = False\n",
        "                start_char = ans['answer_start']\n",
        "                end_char = start_char + len(ans['text'])\n",
        "                \n",
        "                for tok_idx, off in enumerate(global_offsets):\n",
        "                    if off is not None and off[0] <= start_char < off[1]:\n",
        "                        start_pos = tok_idx\n",
        "                    if off is not None and off[0] < end_char <= off[1]:\n",
        "                        end_pos = tok_idx\n",
        "                if start_pos != -1 and end_pos != -1 and end_pos >= start_pos:\n",
        "                    is_positive = True\n",
        "                else:\n",
        "                    start_pos = 0\n",
        "                    end_pos = 0\n",
        "                \n",
        "                # Pad/truncate\n",
        "                pad_len = MAX_LEN - len(input_ids)\n",
        "                if pad_len > 0:\n",
        "                    input_ids += [tokenizer.pad_token_id] * pad_len\n",
        "                    attention_mask += [0] * pad_len\n",
        "                else:\n",
        "                    input_ids = input_ids[:MAX_LEN]\n",
        "                    attention_mask = attention_mask[:MAX_LEN]\n",
        "                \n",
        "                feat = {\n",
        "                    'input_ids': input_ids,\n",
        "                    'attention_mask': attention_mask,\n",
        "                    'start_positions': start_pos,\n",
        "                    'end_positions': end_pos,\n",
        "                    'example_id': ex_id,\n",
        "                    'is_positive': is_positive\n",
        "                }\n",
        "                (pos_feats if is_positive else neg_feats).append(feat)\n",
        "        \n",
        "        # Optional oversampling of positives\n",
        "        if pos_feats:\n",
        "            pos_feats = pos_feats * 2\n",
        "        \n",
        "        # Cap negatives\n",
        "        if pos_feats:\n",
        "            features.extend(pos_feats)\n",
        "            random.shuffle(neg_feats)\n",
        "            n_neg = min(len(neg_feats), neg_pos_ratio * len(pos_feats))\n",
        "            features.extend(neg_feats[:n_neg])\n",
        "        elif neg_feats:\n",
        "            features.append(random.choice(neg_feats))\n",
        "    return features\n",
        "\n",
        "# Prepare validation features (lang-specific TOP_K_EVAL) - for test, use fallback\n",
        "def prepare_validation_features(examples):\n",
        "    features = []\n",
        "    for ex in examples:\n",
        "        q, ctx, ex_id, lang = ex['question'].strip(), ex['context'].strip(), ex['id'], ex['language']\n",
        "        \n",
        "        if USE_RETRIEVAL:\n",
        "            # Same chunking and retrieval as train, but use lang-specific TOP_K_EVAL\n",
        "            chunks = []\n",
        "            chunk_starts = []\n",
        "            for i in range(0, len(ctx), CHUNK_SIZE - OVERLAP):\n",
        "                chunk = ctx[i:i + CHUNK_SIZE]\n",
        "                if len(chunk) > 100:\n",
        "                    chunks.append(chunk)\n",
        "                    chunk_starts.append(i)\n",
        "            \n",
        "            if not chunks:\n",
        "                continue\n",
        "            \n",
        "            # Select vectorizer by language\n",
        "            if lang == 'hindi':\n",
        "                vectorizer = hindi_vectorizer  # Fallback to tamil\n",
        "                top_k_eval = TOP_K_CHUNKS_EVAL_HINDI\n",
        "            else:\n",
        "                vectorizer = tamil_vectorizer\n",
        "                top_k_eval = TOP_K_CHUNKS_EVAL_TAMIL\n",
        "            \n",
        "            # TF-IDF\n",
        "            q_vec = vectorizer.transform([q])\n",
        "            chunk_vecs = vectorizer.transform(chunks)\n",
        "            similarities = cosine_similarity(q_vec, chunk_vecs).flatten()\n",
        "            \n",
        "            # BM25 hybrid if available\n",
        "            if BM25_AVAILABLE:\n",
        "                tokenized_chunks = [chunk.lower().split() for chunk in chunks]\n",
        "                bm25 = BM25Okapi(tokenized_chunks)\n",
        "                q_tokens = q.lower().split()\n",
        "                bm25_scores = bm25.get_scores(q_tokens)\n",
        "                if np.max(bm25_scores) > 0:\n",
        "                    norm_bm25 = bm25_scores / np.max(bm25_scores)\n",
        "                else:\n",
        "                    norm_bm25 = np.zeros_like(bm25_scores)\n",
        "                hybrid_scores = 0.5 * norm_bm25 + 0.5 * similarities\n",
        "            else:\n",
        "                hybrid_scores = similarities\n",
        "            top_indices = np.argsort(hybrid_scores)[-top_k_eval:]\n",
        "            top_chunks = [(hybrid_scores[idx], chunk_starts[idx], chunks[idx]) for idx in top_indices]\n",
        "        else:\n",
        "            top_chunks = [(1.0, 0, ctx)]\n",
        "        \n",
        "        # Process each top chunk\n",
        "        for sim, chunk_start, chunk in top_chunks:\n",
        "            tokenized = tokenizer(\n",
        "                q,\n",
        "                chunk,\n",
        "                truncation='only_second',\n",
        "                max_length=MAX_LEN,\n",
        "                stride=DOC_STRIDE,\n",
        "                return_overflowing_tokens=True,\n",
        "                return_offsets_mapping=True,\n",
        "                padding=False,\n",
        "            )\n",
        "            \n",
        "            for j in range(len(tokenized['input_ids'])):\n",
        "                input_ids = tokenized['input_ids'][j]\n",
        "                attention_mask = tokenized['attention_mask'][j]\n",
        "                offsets = tokenized['offset_mapping'][j]\n",
        "                sequence_ids = tokenized.sequence_ids(j)\n",
        "                \n",
        "                # Skip windows without context tokens\n",
        "                if 1 not in sequence_ids:\n",
        "                    continue\n",
        "                \n",
        "                # Global offsets for post-processing\n",
        "                global_offsets = []\n",
        "                ctx_start = 0\n",
        "                while ctx_start < len(sequence_ids) and sequence_ids[ctx_start] != 1:\n",
        "                    global_offsets.append(None)\n",
        "                    ctx_start += 1\n",
        "                while ctx_start < len(sequence_ids) and sequence_ids[ctx_start] == 1:\n",
        "                    local_offset = offsets[ctx_start]\n",
        "                    global_offset = (local_offset[0] + chunk_start, local_offset[1] + chunk_start) if local_offset else None\n",
        "                    global_offsets.append(global_offset)\n",
        "                    ctx_start += 1\n",
        "                while ctx_start < len(sequence_ids):\n",
        "                    global_offsets.append(None)\n",
        "                    ctx_start += 1\n",
        "                \n",
        "                # Pad/truncate\n",
        "                pad_len = MAX_LEN - len(input_ids)\n",
        "                if pad_len > 0:\n",
        "                    input_ids += [tokenizer.pad_token_id] * pad_len\n",
        "                    attention_mask += [0] * pad_len\n",
        "                    global_offsets += [None] * pad_len\n",
        "                else:\n",
        "                    input_ids = input_ids[:MAX_LEN]\n",
        "                    attention_mask = attention_mask[:MAX_LEN]\n",
        "                    global_offsets = global_offsets[:MAX_LEN]\n",
        "                \n",
        "                features.append({\n",
        "                    'input_ids': input_ids,\n",
        "                    'attention_mask': attention_mask,\n",
        "                    'offset_mapping': global_offsets,\n",
        "                    'example_id': ex_id,\n",
        "                })\n",
        "    return features\n",
        "\n",
        "# Test on small batch\n",
        "test_examples = train_df.head(1).to_dict('records')\n",
        "print('Testing on example:', test_examples[0]['id'], 'Language:', test_examples[0]['language'])\n",
        "print('Gold answer:', test_examples[0]['answer_text'], 'at', test_examples[0]['answer_start'])\n",
        "train_features = prepare_train_features(test_examples)\n",
        "val_features = prepare_validation_features(test_examples)\n",
        "print(f'Train features: {len(train_features)}')\n",
        "print(f'Val features: {len(val_features)}')\n",
        "if train_features:\n",
        "    print('Sample train feature keys:', list(train_features[0].keys()))\n",
        "    print('Sample input_ids len:', len(train_features[0]['input_ids']))\n",
        "    print('Sample is_positive:', train_features[0]['is_positive'])\n",
        "if val_features:\n",
        "    print('Sample val offset_mapping len:', len(val_features[0]['offset_mapping']))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "id": "00dd2c8d-d558-480c-a8b5-156e51e50798",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Post-processing to aggregate predictions across sliding windows with improved scoring\n",
        "def get_predictions(features, start_logits, end_logits, n_best_size=50, max_answer_length=80):\n",
        "    example_to_features = {}\n",
        "    for i, f in enumerate(features):\n",
        "        example_to_features.setdefault(f['example_id'], []).append((i, f))\n",
        "\n",
        "    pred_dict = {}\n",
        "    for example_id, feat_list in example_to_features.items():\n",
        "        prelim_predictions = []\n",
        "        for feat_idx, f in feat_list:\n",
        "            offsets = f['offset_mapping']\n",
        "            sl = start_logits[feat_idx]\n",
        "            el = end_logits[feat_idx]\n",
        "\n",
        "            # Context indices (non-None offsets)\n",
        "            ctx_idx = [i for i, o in enumerate(offsets) if o is not None]\n",
        "            if not ctx_idx:\n",
        "                continue\n",
        "\n",
        "            # Log-softmax on context logits only\n",
        "            start_log = log_softmax_np(sl[ctx_idx])\n",
        "            end_log = log_softmax_np(el[ctx_idx])\n",
        "\n",
        "            # Top n_best_size start/end positions in context\n",
        "            top_start_idx = np.argsort(sl[ctx_idx])[-n_best_size:].tolist()[::-1]\n",
        "            top_end_idx = np.argsort(el[ctx_idx])[-n_best_size:].tolist()[::-1]\n",
        "\n",
        "            # Global indices\n",
        "            top_start = [ctx_idx[i] for i in top_start_idx]\n",
        "            top_end = [ctx_idx[i] for i in top_end_idx]\n",
        "\n",
        "            # Generate candidates\n",
        "            for s in top_start:\n",
        "                for e in top_end:\n",
        "                    if e < s:\n",
        "                        continue\n",
        "                    length = e - s + 1\n",
        "                    if length > max_answer_length:\n",
        "                        continue\n",
        "                    sc, ec = offsets[s][0], offsets[e][1]\n",
        "                    # Score with softened length penalty\n",
        "                    score = start_log[top_start_idx[top_start.index(s)]] + end_log[top_end_idx[top_end.index(e)]] - 0.001 * max(0, length - 25)\n",
        "                    prelim_predictions.append((score, sc, ec))\n",
        "\n",
        "        if prelim_predictions:\n",
        "            _, sc, ec = max(prelim_predictions, key=lambda x: x[0])\n",
        "            pred_dict[example_id] = (sc, ec)\n",
        "        else:\n",
        "            # Fallback: best single-token span in context across all features\n",
        "            best_score = -np.inf\n",
        "            best_sc, best_ec = 0, 0\n",
        "            for feat_idx, f in feat_list:\n",
        "                offsets = f['offset_mapping']\n",
        "                sl = start_logits[feat_idx]\n",
        "                ctx_idx = [i for i, o in enumerate(offsets) if o is not None]\n",
        "                if not ctx_idx:\n",
        "                    continue\n",
        "                s_log = log_softmax_np(sl[ctx_idx])\n",
        "                best_s_local = np.argmax(sl[ctx_idx])\n",
        "                s_global = ctx_idx[best_s_local]\n",
        "                sc, ec = offsets[s_global][0], offsets[s_global][1]\n",
        "                score = s_log[best_s_local]\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_sc, best_ec = sc, ec\n",
        "            pred_dict[example_id] = (best_sc, best_ec)\n",
        "    return pred_dict\n",
        "\n",
        "# Function to extract answer from context with NFKC and punctuation trim\n",
        "def extract_answer(context, start_char, end_char):\n",
        "    if start_char == 0 and end_char == 0:\n",
        "        return ''\n",
        "    s = context[start_char:end_char]\n",
        "    s = unicodedata.normalize('NFKC', s).strip().strip(PUNCT)\n",
        "    return s\n",
        "\n",
        "# Dataset class - updated to include is_positive for training\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, features):\n",
        "        self.input_ids = [f['input_ids'] for f in features]\n",
        "        self.attention_mask = [f['attention_mask'] for f in features]\n",
        "        if 'start_positions' in features[0]:\n",
        "            self.start_positions = [f['start_positions'] for f in features]\n",
        "            self.end_positions = [f['end_positions'] for f in features]\n",
        "            self.is_positive = [f['is_positive'] for f in features]\n",
        "        else:\n",
        "            self.start_positions = None\n",
        "            self.end_positions = None\n",
        "            self.is_positive = None\n",
        "        self.offset_mapping = [f.get('offset_mapping') for f in features]\n",
        "        self.example_id = [f['example_id'] for f in features]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_mask[idx]\n",
        "        }\n",
        "        assert len(item['input_ids']) == MAX_LEN, 'Input ids not padded correctly'\n",
        "        assert len(item['attention_mask']) == MAX_LEN, 'Attention mask not padded correctly'\n",
        "        if self.start_positions is not None:\n",
        "            item['start_positions'] = self.start_positions[idx]\n",
        "            item['end_positions'] = self.end_positions[idx]\n",
        "            item['is_positive'] = self.is_positive[idx]\n",
        "        return item\n",
        "\n",
        "# Custom Weighted Trainer to down-weight negative examples (fixed per-example weighting)\n",
        "class WeightedQATrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        start_positions = inputs.pop('start_positions')\n",
        "        end_positions = inputs.pop('end_positions')\n",
        "        is_positive = inputs.pop('is_positive', None)  # tensor [bs] or None\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits = outputs.end_logits\n",
        "\n",
        "        start_loss = F.cross_entropy(start_logits, start_positions, reduction='none')\n",
        "        end_loss = F.cross_entropy(end_logits, end_positions, reduction='none')\n",
        "        loss = (start_loss + end_loss) / 2.0\n",
        "\n",
        "        if is_positive is not None:\n",
        "            ispos = is_positive.bool()\n",
        "            weights = torch.where(ispos, torch.ones_like(loss), torch.full_like(loss, NEG_WEIGHT))\n",
        "            loss = (loss * weights).mean()\n",
        "        else:\n",
        "            loss = loss.mean()\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Numpy log_softmax for numpy arrays\n",
        "def log_softmax_np(x):\n",
        "    x = x - np.max(x, axis=-1, keepdims=True)\n",
        "    return x - np.log(np.sum(np.exp(x), axis=-1, keepdims=True))\n",
        "\n",
        "# Test dataset creation\n",
        "val_features_test = prepare_validation_features(train_df.head(1).to_dict('records'))\n",
        "val_dataset_test = QADataset(val_features_test)\n",
        "print(f'Dataset length: {len(val_dataset_test)}')\n",
        "sample_item = val_dataset_test[0]\n",
        "print('Sample item keys:', list(sample_item.keys()))\n",
        "print('Sample input_ids len:', len(sample_item['input_ids']))\n",
        "\n",
        "# Test train dataset with is_positive\n",
        "trn_features_test = prepare_train_features(train_df.head(1).to_dict('records'))\n",
        "if trn_features_test:\n",
        "    trn_dataset_test = QADataset(trn_features_test)\n",
        "    sample_trn_item = trn_dataset_test[0]\n",
        "    print('Sample train item keys:', list(sample_trn_item.keys()))\n",
        "    print('Sample is_positive:', sample_trn_item['is_positive'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "id": "e86a8301-d017-4eb9-b0ec-c728925e46a2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Precompute test features once (language already set in Cell 1)\n",
        "print('Test language distribution:', test_df['language'].value_counts())\n",
        "test_features = prepare_validation_features(test_df.to_dict('records'))\n",
        "test_dataset = QADataset(test_features)\n",
        "test_start_sum = None\n",
        "test_end_sum = None\n",
        "\n",
        "# Training loop - Tamil specialist (no oversampling, positive-only)\n",
        "oof_preds = []\n",
        "oof_trues = []\n",
        "oof_ids = []\n",
        "fold_jaccards = []\n",
        "\n",
        "for fold in range(N_FOLDS):\n",
        "    print(f'\\n=== Fold {fold} ===')\n",
        "    trn_df = train_df[train_df['fold'] != fold].reset_index(drop=True)\n",
        "    val_df = train_df[train_df['fold'] == fold].reset_index(drop=True)\n",
        "    print(f'Train: {len(trn_df)}, Val: {len(val_df)}')\n",
        "\n",
        "    # No oversampling - all Tamil\n",
        "\n",
        "    print('Preparing train features...')\n",
        "    start_time = time.time()\n",
        "    trn_features = prepare_train_features(trn_df.to_dict('records'))\n",
        "    prep_time = time.time() - start_time\n",
        "    print(f'Trn features prepared in {prep_time:.2f}s: {len(trn_features)} (positive-only)')\n",
        "\n",
        "    print('Preparing val features...')\n",
        "    start_time = time.time()\n",
        "    val_features = prepare_validation_features(val_df.to_dict('records'))\n",
        "    prep_time = time.time() - start_time\n",
        "    print(f'Val features prepared in {prep_time:.2f}s: {len(val_features)}')\n",
        "\n",
        "    trn_dataset = QADataset(trn_features)\n",
        "    val_dataset = QADataset(val_features)\n",
        "\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "    model.gradient_checkpointing_enable()\n",
        "    param_count = sum(p.numel() for p in model.parameters())\n",
        "    print(f'Model param count: {param_count:,}')\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'/tmp/model_tamil_{fold}',\n",
        "        bf16=True,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=16,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        learning_rate=LR,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        save_strategy='no',\n",
        "        report_to='none',\n",
        "        dataloader_pin_memory=False,\n",
        "        dataloader_num_workers=2,\n",
        "        remove_unused_columns=False,\n",
        "        warmup_ratio=0.1,\n",
        "        lr_scheduler_type='linear',\n",
        "        max_grad_norm=1.0,\n",
        "        logging_steps=10,  # More frequent logging\n",
        "    )\n",
        "\n",
        "    trainer = WeightedQATrainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=trn_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=default_data_collator,\n",
        "    )\n",
        "\n",
        "    print('Starting training...')\n",
        "    train_start = time.time()\n",
        "    trainer.train()\n",
        "    train_time = time.time() - train_start\n",
        "    print(f'Training completed in {train_time:.2f}s')\n",
        "\n",
        "    predictions = trainer.predict(val_dataset)\n",
        "    pred_dict = get_predictions(val_features, predictions.predictions[0], predictions.predictions[1], n_best_size=50, max_answer_length=MAX_ANSWER_LENGTH)\n",
        "\n",
        "    fold_preds = []\n",
        "    for idx, row in val_df.iterrows():\n",
        "        start_char, end_char = pred_dict.get(row['id'], (0, 0))\n",
        "        pred = extract_answer(row['context'], start_char, end_char)\n",
        "        fold_preds.append(pred)\n",
        "\n",
        "    print('Empty OOF preds:', (np.array(fold_preds) == '').mean())\n",
        "\n",
        "    fold_trues = val_df['answer_text'].tolist()\n",
        "    fold_jacc = compute_jaccard(fold_preds, fold_trues)\n",
        "    fold_jaccards.append(fold_jacc)\n",
        "    print(f'Fold {fold} Tamil Jaccard: {fold_jacc:.4f}')\n",
        "\n",
        "    oof_preds.extend(fold_preds)\n",
        "    oof_trues.extend(fold_trues)\n",
        "    oof_ids.extend(val_df['id'].tolist())\n",
        "\n",
        "    # Accumulate test logits\n",
        "    test_out = trainer.predict(test_dataset)\n",
        "    if test_start_sum is None:\n",
        "        test_start_sum = test_out.predictions[0]\n",
        "        test_end_sum = test_out.predictions[1]\n",
        "    else:\n",
        "        test_start_sum += test_out.predictions[0]\n",
        "        test_end_sum += test_out.predictions[1]\n",
        "\n",
        "    del model, trainer, trn_dataset, val_dataset, trn_features, val_features\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(f'\\nMean fold Tamil Jaccard: {np.mean(fold_jaccards):.4f} (+/- {np.std(fold_jaccards):.4f})')\n",
        "overall_tamil_jacc = compute_jaccard(oof_preds, oof_trues)\n",
        "print(f'Overall Tamil OOF Jaccard: {overall_tamil_jacc:.4f}')\n",
        "\n",
        "# Save Tamil OOF for analysis\n",
        "oof_df = pd.DataFrame({'id': oof_ids, 'pred': oof_preds, 'true': oof_trues})\n",
        "oof_df.to_csv('oof_predictions_tamil_specialist.csv', index=False)\n",
        "print('Tamil OOF saved to oof_predictions_tamil_specialist.csv')\n",
        "\n",
        "# Save test logits and feature order for blending (Tamil specialist)\n",
        "np.savez('test_logits_tamil_specialist_sum.npz', start=test_start_sum, end=test_end_sum, n_folds=N_FOLDS)\n",
        "with open('test_features_order_tamil_specialist.json', 'w') as f:\n",
        "    json.dump([f['example_id'] for f in test_features], f)\n",
        "print('Tamil specialist test logits and feature order saved for blending')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test language distribution: language\nhindi    84\ntamil    28\nName: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== Fold 0 ===\nTrain: 273, Val: 67\nPreparing train features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trn features prepared in 6.13s: 1786 (positive-only)\nPreparing val features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val features prepared in 1.96s: 1037\nModel param count: 558,842,882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='555' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/555 : < :, Epoch 0.01/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 742.32s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/65 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty OOF preds: 0.0\nFold 0 Tamil Jaccard: 0.3995\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/68 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== Fold 1 ===\nTrain: 275, Val: 65\nPreparing train features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trn features prepared in 6.48s: 1795 (positive-only)\nPreparing val features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val features prepared in 1.71s: 898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model param count: 558,842,882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/560 : < :, Epoch 0.01/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 752.46s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/57 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty OOF preds: 0.0\nFold 1 Tamil Jaccard: 0.5556\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/68 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== Fold 2 ===\nTrain: 271, Val: 69\nPreparing train features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trn features prepared in 6.48s: 1756 (positive-only)\nPreparing val features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val features prepared in 1.71s: 944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model param count: 558,842,882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/545 : < :, Epoch 0.01/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 731.40s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/59 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty OOF preds: 0.014492753623188406\nFold 2 Tamil Jaccard: 0.5146\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/68 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== Fold 3 ===\nTrain: 265, Val: 75\nPreparing train features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trn features prepared in 6.45s: 1737 (positive-only)\nPreparing val features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val features prepared in 1.64s: 861\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model param count: 558,842,882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/540 : < :, Epoch 0.01/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 724.39s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/54 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty OOF preds: 0.0\nFold 3 Tamil Jaccard: 0.5560\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/68 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== Fold 4 ===\nTrain: 276, Val: 64\nPreparing train features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trn features prepared in 6.45s: 1806 (positive-only)\nPreparing val features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val features prepared in 1.80s: 895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model param count: 558,842,882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/560 : < :, Epoch 0.01/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 752.23s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/56 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty OOF preds: 0.046875\nFold 4 Tamil Jaccard: 0.5820\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/68 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nMean fold Tamil Jaccard: 0.5215 (+/- 0.0647)\nOverall Tamil OOF Jaccard: 0.5216\nTamil OOF saved to oof_predictions_tamil_specialist.csv\nTamil specialist test logits and feature order saved for blending\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}