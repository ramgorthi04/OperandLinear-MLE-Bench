{
  "cells": [
    {
      "id": "bac88b25-6671-4f63-9195-fe2c06bbbc2b",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan: MLSP 2013 Bird Classification (Medal-Oriented)\n",
        "\n",
        "Goals:\n",
        "- Build a strong multi-label classifier using provided assets (spectrogram bitmaps, WAVs, folds, species list).\n",
        "- Optimize for ROC-AUC with reliable CV and efficient training.\n",
        "\n",
        "Data understanding (to verify):\n",
        "- essential_data/src_wavs/: raw 10s audio clips.\n",
        "- supplemental_data/spectrograms/ and filtered_spectrograms/: 1 image per clip.\n",
        "- CVfolds_2.txt: cross-validation assignments (likely per recording).\n",
        "- rec_id2filename.txt: map between recording IDs and filenames.\n",
        "- species_list.txt: ordered list of target classes.\n",
        "- rec_labels_test_hidden.txt: indicates test set identifiers; true labels hidden.\n",
        "\n",
        "Initial approach (fast, competitive):\n",
        "1) Use spectrogram images as inputs; multi-label targets per recording.\n",
        "2) Parse folds and train/val splits from CVfolds_2.txt; determine train/test split via rec_labels_test_hidden and rec_id2filename.\n",
        "3) Model A (baseline, quick):\n",
        "   - Extract image features (e.g., statistics, HOG, color/texture) and train LightGBM/Logistic multi-label one-vs-rest.\n",
        "   - Purpose: quick sanity check; establish CV and submission format.\n",
        "4) Model B (primary):\n",
        "   - Fine-tune a lightweight CNN (e.g., timm EfficientNet-B0/ConvNeXt-Tiny) on spectrogram images.\n",
        "   - Multi-label BCEWithLogits, stratified folds from CVfolds_2.\n",
        "   - Augmentations: light (random time/freq masking via image ops, flips limited to time axis).\n",
        "   - Inference: TTA horizontal shifts/crops if necessary.\n",
        "5) Ensembling:\n",
        "   - Average probabilities from Model A + Model B (and possibly filtered vs unfiltered spectrogram variants) for lift.\n",
        "\n",
        "Validation protocol:\n",
        "- Use provided CVfolds_2 to avoid leakage, monitor macro ROC-AUC per fold.\n",
        "- Log times per fold; early stop if overfitting.\n",
        "\n",
        "Deliverables:\n",
        "- submission.csv with columns: [file, species_1, ..., species_K] (confirm exact header from sample_submission.csv).\n",
        "\n",
        "Next steps:\n",
        "1) Inspect/parse files (species list, folds, id <-> filename, sample submission) to confirm target ordering and split.\n",
        "2) Implement data loader mapping image paths to labels.\n",
        "3) Baseline feature model for quick CV.\n",
        "4) CNN training with careful logging.\n",
        "5) Blend and generate submission."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "4555ea29-dfa4-4f7d-a1e6-8411c1089906",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inspect core files and prepare mappings for baseline (robust parsing)\n",
        "import pandas as pd, numpy as np, os, json, sys, time, re\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path('.')\n",
        "ess = DATA_DIR / 'essential_data'\n",
        "supp = DATA_DIR / 'supplemental_data'\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[INFO] {msg}\")\n",
        "\n",
        "# Sample submission defines exact column order and species list\n",
        "sub_path = DATA_DIR / 'sample_submission.csv'\n",
        "df_sub = pd.read_csv(sub_path)\n",
        "assert 'Id' in df_sub.columns[0], \"First column in sample_submission must be Id\"\n",
        "species = list(df_sub.columns[1:])\n",
        "n_classes = len(species)\n",
        "log(f\"Sample submission loaded. n_test_rows={len(df_sub)}, n_species={n_classes}. First 5 species: {species[:5]}\")\n",
        "\n",
        "# Load folds + labels (CVfolds_2.txt): rec_id, fold, label_1...label_K\n",
        "folds_path = ess / 'CVfolds_2.txt'\n",
        "# Use whitespace separator (space or tab)\n",
        "df_folds = pd.read_csv(folds_path, header=None, sep=r\"\\s+\", engine='python')\n",
        "expected_cols = 2 + n_classes\n",
        "log(f\"CVfolds_2.txt shape raw: {df_folds.shape}\")\n",
        "assert df_folds.shape[1] == expected_cols, f\"Unexpected columns in CVfolds_2.txt: got {df_folds.shape[1]}, expected {expected_cols}\"\n",
        "df_folds.columns = ['rec_id','fold'] + species\n",
        "log(f\"Loaded CVfolds_2.txt: {df_folds.shape[0]} records, folds: {sorted(df_folds.fold.unique().tolist())}\")\n",
        "log(df_folds.head(3).to_string(index=False))\n",
        "\n",
        "# Load test IDs (whitespace separated list of rec_ids)\n",
        "test_ids_path = ess / 'rec_labels_test_hidden.txt'\n",
        "df_test_ids = pd.read_csv(test_ids_path, header=None, names=['rec_id'], sep=r\"\\s+\", engine='python')\n",
        "test_ids = set(df_test_ids['rec_id'].tolist())\n",
        "log(f\"Loaded test IDs: {len(test_ids)}\")\n",
        "\n",
        "# Train set = all rec_ids in folds not in test_ids\n",
        "is_test = df_folds['rec_id'].isin(test_ids)\n",
        "df_train = df_folds.loc[~is_test].reset_index(drop=True)\n",
        "df_test_folds = df_folds.loc[is_test].reset_index(drop=True)\n",
        "log(f\"Train records: {df_train.shape[0]}, Test records in folds listing: {df_test_folds.shape[0]}\")\n",
        "\n",
        "# Load rec_id -> filename (whitespace separated)\n",
        "id2fn_path = ess / 'rec_id2filename.txt'\n",
        "df_id2fn = pd.read_csv(id2fn_path, header=None, names=['rec_id','filename'], sep=r\"\\s+\", engine='python')\n",
        "log(f\"Loaded id->filename map: {df_id2fn.shape[0]} rows. Sample:\\n{df_id2fn.head(3)}\")\n",
        "\n",
        "# Segment features (for baseline) - try tab, then whitespace\n",
        "seg_feat_path = supp / 'segment_features.txt'\n",
        "try:\n",
        "    df_feats = pd.read_csv(seg_feat_path, sep='\\t', header=None)\n",
        "except Exception:\n",
        "    df_feats = pd.read_csv(seg_feat_path, sep=r\"\\s+\", header=None, engine='python')\n",
        "log(f\"Loaded segment_features: shape {df_feats.shape}\")\n",
        "\n",
        "# Heuristic: first column is rec_id, ensure it matches df_folds\n",
        "df_feats = df_feats.rename(columns={0:'rec_id'})\n",
        "if not np.issubdtype(df_feats['rec_id'].dtype, np.number):\n",
        "    try:\n",
        "        df_feats['rec_id'] = df_feats['rec_id'].astype(int)\n",
        "    except Exception as e:\n",
        "        log(f\"Warning: could not cast rec_id to int: {e}\")\n",
        "\n",
        "log(f\"Unique rec_ids in features: {df_feats['rec_id'].nunique()}\")\n",
        "log(f\"Feature preview:\\n{df_feats.head(3)}\")\n",
        "\n",
        "# Align features with train/test IDs\n",
        "train_ids = set(df_train['rec_id'])\n",
        "missing_train = len(train_ids - set(df_feats['rec_id']))\n",
        "missing_test = len(test_ids - set(df_feats['rec_id']))\n",
        "log(f\"Features coverage -> missing train: {missing_train}, missing test: {missing_test}\")\n",
        "\n",
        "# Save quick summaries for reference\n",
        "summary = {\n",
        "    'n_species': n_classes,\n",
        "    'folds': sorted(df_folds.fold.unique().tolist()),\n",
        "    'n_train': int(df_train.shape[0]),\n",
        "    'n_test_ids': int(len(test_ids)),\n",
        "    'features_shape': tuple(df_feats.shape),\n",
        "    'features_missing_train': int(missing_train),\n",
        "    'features_missing_test': int(missing_test),\n",
        "}\n",
        "print(json.dumps(summary, indent=2))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Sample submission loaded. n_test_rows=1216, n_species=1. First 5 species: ['Probability']\n[INFO] CVfolds_2.txt shape raw: (323, 1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Unexpected columns in CVfolds_2.txt: got 1, expected 3",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m expected_cols = \u001b[32m2\u001b[39m + n_classes\n\u001b[32m     25\u001b[39m log(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCVfolds_2.txt shape raw: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_folds.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m df_folds.shape[\u001b[32m1\u001b[39m] == expected_cols, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected columns in CVfolds_2.txt: got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_folds.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m df_folds.columns = [\u001b[33m'\u001b[39m\u001b[33mrec_id\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mfold\u001b[39m\u001b[33m'\u001b[39m] + species\n\u001b[32m     28\u001b[39m log(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded CVfolds_2.txt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_folds.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m records, folds: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msorted\u001b[39m(df_folds.fold.unique().tolist())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mAssertionError\u001b[39m: Unexpected columns in CVfolds_2.txt: got 1, expected 3"
          ]
        }
      ]
    },
    {
      "id": "f5b528e5-129f-4b95-a72a-e5e264e1e8db",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Peek at raw files to determine correct delimiters and formats\n",
        "from pathlib import Path\n",
        "\n",
        "def head_lines(path, n=5):\n",
        "    print(f\"\\n===== HEAD of {path} =====\")\n",
        "    with open(path, 'r', errors='ignore') as f:\n",
        "        for i in range(n):\n",
        "            line = f.readline()\n",
        "            if not line: break\n",
        "            print(line.rstrip('\\n'))\n",
        "\n",
        "ess = Path('essential_data')\n",
        "supp = Path('supplemental_data')\n",
        "\n",
        "head_lines('sample_submission.csv', 5)\n",
        "head_lines(ess / 'species_list.txt', 10)\n",
        "head_lines(ess / 'CVfolds_2.txt', 10)\n",
        "head_lines(ess / 'rec_id2filename.txt', 10)\n",
        "head_lines(ess / 'rec_labels_test_hidden.txt', 10)\n",
        "head_lines(supp / 'segment_features.txt', 5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n===== HEAD of sample_submission.csv =====\nId,Probability\n100,0\n101,0\n102,0\n103,0\n\n===== HEAD of essential_data/species_list.txt =====\nclass_id,code,species\n0,BRCR,Brown Creeper\n1,PAWR,Pacific Wren\n2,PSFL,Pacific-slope Flycatcher\n3,RBNU,Red-breasted Nuthatch\n4,DEJU,Dark-eyed Junco\n5,OSFL,Olive-sided Flycatcher\n6,HETH,Hermit Thrush\n7,CBCH,Chestnut-backed Chickadee\n8,VATH,Varied Thrush\n\n===== HEAD of essential_data/CVfolds_2.txt =====\nrec_id,fold\n0,0\n1,1\n2,0\n3,0\n4,0\n5,0\n6,1\n7,1\n8,1\n\n===== HEAD of essential_data/rec_id2filename.txt =====\nrec_id,filename\n0,PC1_20090606_050012_0010\n1,PC1_20090606_070012_0010\n2,PC1_20090705_070000_0010\n3,PC1_20100513_043000_0010\n4,PC1_20100513_043000_0720\n5,PC1_20100606_070000_0010\n6,PC1_20100705_070002_0010\n7,PC1_20100804_050000_0010\n8,PC1_20100804_070000_0010\n\n===== HEAD of essential_data/rec_labels_test_hidden.txt =====\nrec_id,[labels]\n0,11,12\n1,?\n2,10\n3\n4\n5\n6,?\n7,?\n8,?\n\n===== HEAD of supplemental_data/segment_features.txt =====\nrec_id,[histogram of segment features]\n0,0,0.954148,0.938716,0.403581,0.508965,0.000536,0.067958,0.000000,0.001418,0.000001,0.009373,0.407843,0.352941,24.048851,7.664318,91.000000,115.000000,24.000000,18.000000,348.000000,73.000000,15.313218,0.805556,0.067227,0.000000,0.184874,0.042017,0.067227,0.067227,0.025210,0.008403,0.084034,0.008403,0.067227,0.134454,0.008403,0.033613,0.058824,0.142857\n0,1,0.941444,0.963083,0.397784,0.503964,0.000328,0.064116,0.000000,0.000520,0.000000,0.008495,0.403922,0.517241,23.310722,7.340112,92.000000,111.000000,19.000000,30.000000,457.000000,92.000000,18.520788,0.801754,0.016854,0.005618,0.044944,0.067416,0.112360,0.252809,0.000000,0.005618,0.022472,0.011236,0.028090,0.134831,0.028090,0.219101,0.022472,0.028090\n0,2,0.965339,0.913682,0.446249,0.530913,0.000977,0.069738,0.000002,0.000856,0.000002,0.009757,0.466667,0.333333,24.411079,8.615371,98.000000,130.000000,32.000000,13.000000,343.000000,70.000000,14.285714,0.824519,0.180451,0.045113,0.082707,0.045113,0.037594,0.030075,0.052632,0.007519,0.135338,0.022556,0.082707,0.022556,0.015038,0.022556,0.045113,0.172932\n0,3,0.969508,0.909917,0.501135,0.537447,0.001256,0.063361,0.000006,-0.000336,0.000003,0.008907,0.474510,0.416667,24.620689,9.255116,111.000000,146.000000,35.000000,13.000000,377.000000,74.000000,14.525199,0.828571,0.209877,0.012346,0.104938,0.030864,0.012346,0.024691,0.037037,0.055556,0.160494,0.037037,0.037037,0.012346,0.018519,0.018519,0.018519,0.209877\n"
          ]
        }
      ]
    },
    {
      "id": "d8631819-1d41-4fb9-be88-8ea851f27614",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Parse labels from rec_labels_test_hidden.txt and inspect mapping to sample submission\n",
        "import csv\n",
        "from collections import defaultdict\n",
        "\n",
        "labels_path = Path('essential_data/rec_labels_test_hidden.txt')\n",
        "label_rows = []\n",
        "with open(labels_path, 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    header = next(reader)  # ['rec_id','[labels]']\n",
        "    for row in reader:\n",
        "        if not row:\n",
        "            continue\n",
        "        rec_id = int(row[0])\n",
        "        if len(row) == 1:\n",
        "            labels = []  # no labels listed\n",
        "            status = 'known'\n",
        "        else:\n",
        "            if row[1].strip() == '?' or row[1].strip() == '[labels]':\n",
        "                labels = None\n",
        "                status = 'test'\n",
        "            else:\n",
        "                # remaining entries after rec_id are label indices (possibly multiple)\n",
        "                labels = [int(x) for x in row[1:]]\n",
        "                status = 'known'\n",
        "        label_rows.append({'rec_id': rec_id, 'labels': labels, 'status': status})\n",
        "\n",
        "df_lbl = pd.DataFrame(label_rows)\n",
        "df_lbl['binary_target'] = df_lbl['labels'].apply(lambda x: np.nan if x is None else (1 if len(x) > 0 else 0))\n",
        "log(f\"Label file parsed: total {len(df_lbl)}, known={df_lbl['status'].eq('known').sum()}, test={df_lbl['status'].eq('test').sum()}\")\n",
        "log(df_lbl.head(10).to_string(index=False))\n",
        "\n",
        "# Sanity: merge folds with labels\n",
        "df_folds_csv = pd.read_csv(ess / 'CVfolds_2.txt')\n",
        "assert list(df_folds_csv.columns) == ['rec_id','fold'], 'Unexpected columns in CVfolds_2.txt'\n",
        "df_folds_csv['rec_id'] = df_folds_csv['rec_id'].astype(int)\n",
        "df_all = df_folds_csv.merge(df_lbl[['rec_id','binary_target','status','labels']], on='rec_id', how='left')\n",
        "log(f\"After merge: shape={df_all.shape}; fold counts: {df_all['fold'].value_counts().to_dict()}\")\n",
        "log(df_all.head(10).to_string(index=False))\n",
        "\n",
        "# Determine train/test ids from labels file (test = status=='test')\n",
        "train_mask = df_all['status'].eq('known')\n",
        "test_mask = df_all['status'].eq('test')\n",
        "df_train_ids = df_all.loc[train_mask, ['rec_id','fold','binary_target','labels']].reset_index(drop=True)\n",
        "df_test_ids = df_all.loc[test_mask, ['rec_id','fold']].reset_index(drop=True)\n",
        "log(f\"Train IDs: {len(df_train_ids)}, Test IDs: {len(df_test_ids)}\")\n",
        "log(f\"Train positives: {int(df_train_ids['binary_target'].sum())}, negatives: {int((df_train_ids['binary_target']==0).sum())}\")\n",
        "\n",
        "# Check correspondence to sample_submission Ids\n",
        "df_sub = pd.read_csv('sample_submission.csv')\n",
        "sub_ids = df_sub['Id'].tolist()\n",
        "log(f\"Sample submission Id range: min={min(sub_ids)}, max={max(sub_ids)}, n_unique={len(set(sub_ids))}\")\n",
        "log(f\"rec_id range: min={df_all.rec_id.min()}, max={df_all.rec_id.max()}, n_unique={df_all.rec_id.nunique()}\")\n",
        "\n",
        "# Parse segment_features: skip header line, comma-separated. Columns: rec_id, seg_id, feat1..N\n",
        "seg_feat_path = Path('supplemental_data/segment_features.txt')\n",
        "df_seg = pd.read_csv(seg_feat_path, header=None, skiprows=1)\n",
        "assert df_seg.shape[1] >= 3, 'segment_features must have at least 3 columns (rec_id, seg_id, features...)'\n",
        "df_seg = df_seg.rename(columns={0:'rec_id', 1:'seg_id'})\n",
        "feature_cols = [c for c in df_seg.columns if c not in ['rec_id','seg_id']]\n",
        "log(f\"Segment features: rows={len(df_seg)}, n_features={len(feature_cols)}\")\n",
        "df_agg = df_seg.groupby('rec_id')[feature_cols].mean().reset_index()\n",
        "log(f\"Aggregated features per rec_id: {df_agg.shape}\")\n",
        "\n",
        "# Coverage check for train/test\n",
        "miss_train = set(df_train_ids.rec_id) - set(df_agg.rec_id)\n",
        "miss_test = set(df_test_ids.rec_id) - set(df_agg.rec_id)\n",
        "log(f\"Aggregated features coverage -> missing train: {len(miss_train)}, missing test: {len(miss_test)}\")\n",
        "\n",
        "# Quick probe: does sample_submission Id encode class_id sequence of 19 per rec_id?\n",
        "first_ids = df_sub['Id'].head(25).tolist()\n",
        "print(\"First 25 sample Ids:\", first_ids)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Label file parsed: total 322, known=258, test=64\n[INFO]  rec_id   labels status  binary_target\n      0 [11, 12]  known            1.0\n      1     None   test            NaN\n      2     [10]  known            1.0\n      3       []  known            0.0\n      4       []  known            0.0\n      5       []  known            0.0\n      6     None   test            NaN\n      7     None   test            NaN\n      8     None   test            NaN\n      9       []  known            0.0\n[INFO] After merge: shape=(322, 5); fold counts: {0: 258, 1: 64}\n[INFO]  rec_id  fold  binary_target status   labels\n      0     0            1.0  known [11, 12]\n      1     1            NaN   test     None\n      2     0            1.0  known     [10]\n      3     0            0.0  known       []\n      4     0            0.0  known       []\n      5     0            0.0  known       []\n      6     1            NaN   test     None\n      7     1            NaN   test     None\n      8     1            NaN   test     None\n      9     0            0.0  known       []\n[INFO] Train IDs: 258, Test IDs: 64\n[INFO] Train positives: 145, negatives: 113\n[INFO] Sample submission Id range: min=100, max=30318, n_unique=1216\n[INFO] rec_id range: min=0, max=321, n_unique=322\n[INFO] Segment features: rows=1119, n_features=38\n[INFO] Aggregated features per rec_id: (154, 39)\n[INFO] Aggregated features coverage -> missing train: 136, missing test: 32\nFirst 25 sample Ids: [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 600, 601, 602, 603, 604, 605]\n"
          ]
        }
      ]
    },
    {
      "id": "4ff82b78-7133-4234-99a8-a745b03acdb4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Baseline: image features + one-vs-rest Logistic Regression; generate submission\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "ess = Path('essential_data')\n",
        "supp = Path('supplemental_data')\n",
        "\n",
        "# Load id -> filename map\n",
        "df_id2fn = pd.read_csv(ess / 'rec_id2filename.txt')\n",
        "id2fn = dict(zip(df_id2fn.rec_id.astype(int), df_id2fn.filename.astype(str)))\n",
        "\n",
        "def load_img_feature(rec_id, folder='filtered_spectrograms', size=(64,64)):\n",
        "    fn = id2fn.get(int(rec_id), None)\n",
        "    if fn is None: return None\n",
        "    img_path = supp / folder / f\"{fn}.bmp\"\n",
        "    if not img_path.exists():\n",
        "        return None\n",
        "    try:\n",
        "        img = Image.open(img_path).convert('L')\n",
        "    except Exception:\n",
        "        return None\n",
        "    # resize preserving aspect ratio in height; then center crop/pad width to target\n",
        "    Ht = size[0]\n",
        "    w, h = img.size\n",
        "    new_w = int(round(w * (Ht / h)))\n",
        "    img_resized = img.resize((new_w, Ht), Image.BILINEAR)\n",
        "    target_w = size[1]\n",
        "    if new_w >= target_w:\n",
        "        start = (new_w - target_w)//2\n",
        "        img_crop = img_resized.crop((start, 0, start + target_w, Ht))\n",
        "    else:\n",
        "        pad_left = (target_w - new_w)//2\n",
        "        pad_right = target_w - new_w - pad_left\n",
        "        canvas = Image.new('L', (target_w, Ht), color=0)\n",
        "        canvas.paste(img_resized, (pad_left, 0))\n",
        "        img_crop = canvas\n",
        "    imgf = np.asarray(img_crop, dtype=np.float32) / 255.0\n",
        "    mean = imgf.mean(); std = imgf.std(); p1 = np.percentile(imgf, 1); p5 = np.percentile(imgf,5); p95 = np.percentile(imgf,95); p99 = np.percentile(imgf,99)\n",
        "    small = np.asarray(img_crop.resize((32,32), Image.BILINEAR), dtype=np.float32) / 255.0\n",
        "    feat = np.concatenate([[mean, std, p1, p5, p95, p99], small.ravel()])\n",
        "    return feat\n",
        "\n",
        "# Targets: 19 classes from species_list class_id 0..18\n",
        "species_df = pd.read_csv(ess / 'species_list.txt')\n",
        "num_classes = species_df.shape[0]\n",
        "class_ids = list(range(num_classes))\n",
        "\n",
        "# training known records from cell 3 (df_train_ids available)\n",
        "train_rows = df_train_ids.copy()\n",
        "train_rec_ids = train_rows['rec_id'].tolist()\n",
        "y_multi = np.zeros((len(train_rows), num_classes), dtype=np.float32)\n",
        "for i, labs in enumerate(train_rows['labels']):\n",
        "    for c in labs:\n",
        "        y_multi[i, c] = 1.0\n",
        "\n",
        "X_list = []; valid_idx = []\n",
        "for i, rid in enumerate(train_rec_ids):\n",
        "    f = load_img_feature(rid, folder='filtered_spectrograms', size=(64,64))\n",
        "    if f is None:\n",
        "        f = load_img_feature(rid, folder='spectrograms', size=(64,64))\n",
        "    if f is None:\n",
        "        continue\n",
        "    X_list.append(f); valid_idx.append(i)\n",
        "X = np.vstack(X_list) if X_list else np.zeros((0, 6+1024), dtype=np.float32)\n",
        "y = y_multi[valid_idx]\n",
        "print(f\"[INFO] Image features built: X shape {X.shape}, y shape {y.shape}\")\n",
        "\n",
        "if X.shape[0] == 0:\n",
        "    raise RuntimeError('No image features extracted; cannot train baseline.')\n",
        "\n",
        "# Train One-vs-Rest Logistic Regression with standardization\n",
        "pipe = Pipeline([\n",
        "    ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
        "    ('clf', OneVsRestClassifier(LogisticRegression(max_iter=2000, class_weight='balanced', solver='liblinear')))\n",
        "])\n",
        "\n",
        "t0 = time.time()\n",
        "pipe.fit(X, y)\n",
        "print(f\"[INFO] Training completed in {time.time()-t0:.2f}s\")\n",
        "\n",
        "# Prepare test features\n",
        "test_rec_sorted = sorted(df_test_ids['rec_id'].tolist())\n",
        "test_feats = {}\n",
        "for rid in test_rec_sorted:\n",
        "    f = load_img_feature(rid, folder='filtered_spectrograms', size=(64,64))\n",
        "    if f is None:\n",
        "        f = load_img_feature(rid, folder='spectrograms', size=(64,64))\n",
        "    if f is None:\n",
        "        f = np.zeros(X.shape[1], dtype=np.float32)\n",
        "    test_feats[rid] = f\n",
        "X_test = np.vstack([test_feats[rid] for rid in test_rec_sorted])\n",
        "probs = pipe.predict_proba(X_test)  # shape: (n_rec_test, num_classes)\n",
        "\n",
        "# Build Id -> prob mapping using formula: Id = rec_id * 100 + class_id\n",
        "id2prob = {}\n",
        "for i, rid in enumerate(test_rec_sorted):\n",
        "    for c in range(num_classes):\n",
        "        Id = int(rid) * 100 + c\n",
        "        id2prob[Id] = float(probs[i, c])\n",
        "\n",
        "# Fill submission by Id mapping\n",
        "df_submit = pd.read_csv('sample_submission.csv')\n",
        "df_submit['Probability'] = df_submit['Id'].map(id2prob).fillna(0.1)\n",
        "df_submit.to_csv('submission.csv', index=False)\n",
        "print('[INFO] Saved submission.csv')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Image features built: X shape (258, 1030), y shape (258, 19)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Training completed in 1.65s\n[INFO] Saved submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "b7188b40-8593-46b2-9041-b850eba67586",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# LightGBM on aggregated segment features; predict and ensemble with image-logit baseline\n",
        "import sys, subprocess, importlib, numpy as np, pandas as pd, time\n",
        "from pathlib import Path\n",
        "\n",
        "def ensure_pkg(pkg):\n",
        "    try:\n",
        "        importlib.import_module(pkg)\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(f\"[INFO] Installing {pkg}...\")\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg, '-q'])\n",
        "        importlib.invalidate_caches()\n",
        "        return True\n",
        "\n",
        "ensure_pkg('lightgbm')\n",
        "import lightgbm as lgb\n",
        "\n",
        "ess = Path('essential_data'); supp = Path('supplemental_data')\n",
        "\n",
        "# Reload label data prepared earlier (cell 3 variables should exist). If not, parse again quickly.\n",
        "try:\n",
        "    df_train_ids, df_test_ids\n",
        "except NameError:\n",
        "    df_folds_csv = pd.read_csv(ess / 'CVfolds_2.txt')\n",
        "    lab = pd.read_csv(ess / 'rec_labels_test_hidden.txt')\n",
        "    # quick parse\n",
        "    rows = []\n",
        "    for _, r in lab.iterrows():\n",
        "        items = str(r.iloc[0]).split(',')\n",
        "    # For brevity, assume prior cell ran; otherwise skip LGBM.\n",
        "    raise RuntimeError('Labels not prepared; run cell 3 first.')\n",
        "\n",
        "# Load aggregated segment features (from cell 3) or rebuild if missing\n",
        "try:\n",
        "    df_agg\n",
        "except NameError:\n",
        "    seg_feat_path = Path('supplemental_data/segment_features.txt')\n",
        "    df_seg = pd.read_csv(seg_feat_path, header=None, skiprows=1)\n",
        "    df_seg = df_seg.rename(columns={0:'rec_id', 1:'seg_id'})\n",
        "    feature_cols = [c for c in df_seg.columns if c not in ['rec_id','seg_id']]\n",
        "    df_agg = df_seg.groupby('rec_id')[feature_cols].mean().reset_index()\n",
        "\n",
        "# Build multi-hot matrix for known training rec_ids present in df_agg\n",
        "species_df = pd.read_csv(ess / 'species_list.txt')\n",
        "num_classes = species_df.shape[0]\n",
        "\n",
        "df_train_merge = df_train_ids[['rec_id','labels']].merge(df_agg, on='rec_id', how='inner')\n",
        "X_train = df_train_merge.drop(columns=['rec_id','labels']).values.astype(np.float32)\n",
        "Y = np.zeros((len(df_train_merge), num_classes), dtype=np.float32)\n",
        "for i, labs in enumerate(df_train_merge['labels']):\n",
        "    for c in labs:\n",
        "        Y[i, c] = 1.0\n",
        "print(f\"[INFO] LGBM training data: X {X_train.shape}, Y {Y.shape}\")\n",
        "\n",
        "# Prepare test features for available rec_ids\n",
        "df_test_merge = df_test_ids[['rec_id']].merge(df_agg, on='rec_id', how='inner')\n",
        "test_rec_available = df_test_merge['rec_id'].tolist()\n",
        "X_test = df_test_merge.drop(columns=['rec_id']).values.astype(np.float32)\n",
        "print(f\"[INFO] LGBM test available rec_ids: {len(test_rec_available)} / {len(df_test_ids)}\")\n",
        "\n",
        "if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
        "    print('[WARN] Insufficient data for LGBM; skipping.')\n",
        "else:\n",
        "    # Train one model per class quickly with small params\n",
        "    id2prob_lgb = {}\n",
        "    for c in range(num_classes):\n",
        "        y_c = Y[:, c]\n",
        "        # handle class imbalance via scale_pos_weight\n",
        "        pos = y_c.sum(); neg = len(y_c) - pos\n",
        "        spw = float(neg / max(pos, 1.0)) if pos > 0 else 1.0\n",
        "        train_set = lgb.Dataset(X_train, label=y_c)\n",
        "        params = {\n",
        "            'objective': 'binary',\n",
        "            'metric': 'auc',\n",
        "            'learning_rate': 0.05,\n",
        "            'num_leaves': 31,\n",
        "            'min_data_in_leaf': 10,\n",
        "            'feature_fraction': 0.9,\n",
        "            'bagging_fraction': 0.9,\n",
        "            'bagging_freq': 1,\n",
        "            'verbose': -1,\n",
        "            'scale_pos_weight': spw,\n",
        "        }\n",
        "        num_boost_round = 200\n",
        "        bst = lgb.train(params, train_set, num_boost_round=num_boost_round)\n",
        "        p = bst.predict(X_test)\n",
        "        for i, rid in enumerate(test_rec_available):\n",
        "            Id = int(rid) * 100 + c\n",
        "            id2prob_lgb[Id] = float(p[i])\n",
        "    # Blend with existing submission\n",
        "    df_submit = pd.read_csv('submission.csv')\n",
        "    df_sub_base = pd.read_csv('sample_submission.csv')\n",
        "    # Map lgb preds by Id\n",
        "    lgb_series = df_sub_base['Id'].map(id2prob_lgb)\n",
        "    # Simple average where both present, otherwise keep existing\n",
        "    blended = df_submit['Probability'].copy()\n",
        "    mask = lgb_series.notna()\n",
        "    blended.loc[mask] = 0.5 * blended.loc[mask].values + 0.5 * lgb_series.loc[mask].values\n",
        "    df_submit['Probability'] = blended\n",
        "    df_submit.to_csv('submission.csv', index=False)\n",
        "    print('[INFO] Saved blended submission.csv (image-logit + LGBM)')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] LGBM training data: X (122, 38), Y (122, 19)\n[INFO] LGBM test available rec_ids: 32 / 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Saved blended submission.csv (image-logit + LGBM)\n"
          ]
        }
      ]
    },
    {
      "id": "16e8aa16-c2b6-4b00-ad20-fee9883e5d4b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cross-Validation framework with MultilabelStratifiedKFold on image features; OOF AUC + CV inference\n",
        "import numpy as np, pandas as pd, time, sys, subprocess, importlib\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def ensure_pkg(pkg):\n",
        "    try:\n",
        "        importlib.import_module(pkg)\n",
        "        return True\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg, '-q'])\n",
        "        importlib.invalidate_caches()\n",
        "        return True\n",
        "\n",
        "ensure_pkg('iterative-stratification')\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "ess = Path('essential_data'); supp = Path('supplemental_data')\n",
        "\n",
        "# Load id->filename\n",
        "df_id2fn = pd.read_csv(ess / 'rec_id2filename.txt')\n",
        "id2fn = dict(zip(df_id2fn.rec_id.astype(int), df_id2fn.filename.astype(str)))\n",
        "\n",
        "def load_img_feature(rec_id, folder='filtered_spectrograms', size=(64,64)):\n",
        "    fn = id2fn.get(int(rec_id), None)\n",
        "    if fn is None: return None\n",
        "    img_path = supp / folder / f\"{fn}.bmp\"\n",
        "    if not img_path.exists():\n",
        "        return None\n",
        "    try:\n",
        "        img = Image.open(img_path).convert('L')\n",
        "    except Exception:\n",
        "        return None\n",
        "    Ht = size[0]\n",
        "    w, h = img.size\n",
        "    new_w = int(round(w * (Ht / h)))\n",
        "    img_resized = img.resize((new_w, Ht), Image.BILINEAR)\n",
        "    target_w = size[1]\n",
        "    if new_w >= target_w:\n",
        "        start = (new_w - target_w)//2\n",
        "        img_crop = img_resized.crop((start, 0, start + target_w, Ht))\n",
        "    else:\n",
        "        pad_left = (target_w - new_w)//2\n",
        "        canvas = Image.new('L', (target_w, Ht), color=0)\n",
        "        canvas.paste(img_resized, (pad_left, 0))\n",
        "        img_crop = canvas\n",
        "    imgf = np.asarray(img_crop, dtype=np.float32) / 255.0\n",
        "    mean = imgf.mean(); std = imgf.std(); p1 = np.percentile(imgf, 1); p5 = np.percentile(imgf,5); p95 = np.percentile(imgf,95); p99 = np.percentile(imgf,99)\n",
        "    small = np.asarray(img_crop.resize((32,32), Image.BILINEAR), dtype=np.float32) / 255.0\n",
        "    feat = np.concatenate([[mean, std, p1, p5, p95, p99], small.ravel()])\n",
        "    return feat\n",
        "\n",
        "# Prepare labeled dataset\n",
        "species_df = pd.read_csv(ess / 'species_list.txt')\n",
        "num_classes = species_df.shape[0]\n",
        "\n",
        "# df_train_ids (rec_id, labels) and df_test_ids are prepared in cell 3\n",
        "train_rows = df_train_ids.copy()\n",
        "train_rec_ids = train_rows['rec_id'].tolist()\n",
        "Y_full = np.zeros((len(train_rows), num_classes), dtype=np.float32)\n",
        "for i, labs in enumerate(train_rows['labels']):\n",
        "    for c in labs:\n",
        "        Y_full[i, c] = 1.0\n",
        "\n",
        "X_list = []; keep_idx = []\n",
        "for i, rid in enumerate(train_rec_ids):\n",
        "    f = load_img_feature(rid, folder='filtered_spectrograms', size=(64,64))\n",
        "    if f is None:\n",
        "        f = load_img_feature(rid, folder='spectrograms', size=(64,64))\n",
        "    if f is None:\n",
        "        continue\n",
        "    X_list.append(f); keep_idx.append(i)\n",
        "X_full = np.vstack(X_list)\n",
        "Y = Y_full[keep_idx]\n",
        "rec_ids_kept = [train_rec_ids[i] for i in keep_idx]\n",
        "print(f\"[CV] Train matrix: X {X_full.shape}, Y {Y.shape}, kept {len(rec_ids_kept)} of {len(train_rows)}\")\n",
        "\n",
        "# CV setup\n",
        "mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof = np.zeros_like(Y, dtype=np.float32)\n",
        "fold_aucs = []\n",
        "\n",
        "test_rec_sorted = sorted(df_test_ids['rec_id'].tolist())\n",
        "X_test_list = []\n",
        "for rid in test_rec_sorted:\n",
        "    f = load_img_feature(rid, folder='filtered_spectrograms', size=(64,64))\n",
        "    if f is None:\n",
        "        f = load_img_feature(rid, folder='spectrograms', size=(64,64))\n",
        "    if f is None:\n",
        "        # backstop: vector of zeros (rare)\n",
        "        f = np.zeros(X_full.shape[1], dtype=np.float32)\n",
        "    X_test_list.append(f)\n",
        "X_test_all = np.vstack(X_test_list)\n",
        "test_preds_accum = np.zeros((len(test_rec_sorted), num_classes), dtype=np.float32)\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(mskf.split(X_full, Y), 1):\n",
        "    t0 = time.time()\n",
        "    X_tr, X_val = X_full[trn_idx], X_full[val_idx]\n",
        "    y_tr, y_val = Y[trn_idx], Y[val_idx]\n",
        "    pipe = Pipeline([\n",
        "        ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression(max_iter=2000, class_weight='balanced', solver='liblinear')))\n",
        "    ])\n",
        "    pipe.fit(X_tr, y_tr)\n",
        "    val_proba = pipe.predict_proba(X_val)\n",
        "    oof[val_idx] = val_proba\n",
        "    # per-class AUC, macro avg over classes with at least one positive and one negative in val\n",
        "    aucs = []\n",
        "    for c in range(num_classes):\n",
        "        yv = y_val[:, c]\n",
        "        if yv.sum() > 0 and (len(yv) - yv.sum()) > 0:\n",
        "            try:\n",
        "                aucs.append(roc_auc_score(yv, val_proba[:, c]))\n",
        "            except Exception:\n",
        "                pass\n",
        "    fold_auc = float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "    fold_aucs.append(fold_auc)\n",
        "    print(f\"[CV] Fold {fold}: macro AUC={fold_auc:.4f} using {len(aucs)} classes; time {time.time()-t0:.2f}s\")\n",
        "    # test preds\n",
        "    test_preds_accum += pipe.predict_proba(X_test_all)\n",
        "\n",
        "oof_classes = []\n",
        "class_aucs = []\n",
        "for c in range(num_classes):\n",
        "    yc = Y[:, c]\n",
        "    if yc.sum() > 0 and (len(yc) - yc.sum()) > 0:\n",
        "        try:\n",
        "            class_aucs.append(roc_auc_score(yc, oof[:, c]))\n",
        "            oof_classes.append(c)\n",
        "        except Exception:\n",
        "            pass\n",
        "oof_macro_auc = float(np.mean(class_aucs)) if len(class_aucs) else float('nan')\n",
        "print(f\"[CV] OOF macro AUC over {len(class_aucs)} classes: {oof_macro_auc:.4f}\")\n",
        "\n",
        "# Average test predictions over folds\n",
        "test_preds = test_preds_accum / 5.0\n",
        "\n",
        "# Build Id -> prob mapping and write submission\n",
        "id2prob = {}\n",
        "for i, rid in enumerate(test_rec_sorted):\n",
        "    for c in range(num_classes):\n",
        "        Id = int(rid) * 100 + c\n",
        "        id2prob[Id] = float(test_preds[i, c])\n",
        "df_submit_base = pd.read_csv('sample_submission.csv')\n",
        "missing = df_submit_base['Id'][~df_submit_base['Id'].isin(id2prob.keys())]\n",
        "if len(missing) > 0:\n",
        "    print(f\"[WARN] Missing {len(missing)} Ids in predictions; filling with small constant 0.05\")\n",
        "df_submit_base['Probability'] = df_submit_base['Id'].map(id2prob).fillna(0.05)\n",
        "df_submit_base.to_csv('submission.csv', index=False)\n",
        "print('[CV] Saved CV-based submission.csv')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] Train matrix: X (258, 1030), Y (258, 19), kept 258 of 258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] Fold 1: macro AUC=0.5353 using 18 classes; time 1.38s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] Fold 2: macro AUC=0.6305 using 17 classes; time 1.42s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] Fold 3: macro AUC=0.6466 using 17 classes; time 1.41s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] Fold 4: macro AUC=0.5259 using 19 classes; time 1.35s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] Fold 5: macro AUC=0.6728 using 18 classes; time 1.33s\n[CV] OOF macro AUC over 19 classes: 0.6174\n[CV] Saved CV-based submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "89d6d386-5388-42c2-95a3-8192225cccad",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Tabular baseline v4: Added energy/geometry features, histogram shape stats, 5-fold + seed bagging, per-class blending\n",
        "import pandas as pd, numpy as np, time, sys, subprocess, importlib\n",
        "from pathlib import Path\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "def ensure_pkg(pkg):\n",
        "    try:\n",
        "        importlib.import_module(pkg)\n",
        "        return True\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg, '-q'])\n",
        "        importlib.invalidate_caches()\n",
        "        return True\n",
        "\n",
        "ensure_pkg('lightgbm')\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "ess = Path('essential_data'); supp = Path('supplemental_data')\n",
        "\n",
        "# Robust readers\n",
        "def read_segment_features(path):\n",
        "    df = pd.read_csv(path, header=None, skiprows=1)\n",
        "    df = df.rename(columns={0:'rec_id', 1:'seg_id'})\n",
        "    return df\n",
        "\n",
        "def read_segment_rectangles(path):\n",
        "    # Robust parser: attempt multiple separators and skip bad lines; return empty DF on failure\n",
        "    variants = [\n",
        "        dict(header=None, sep=',', engine='python', on_bad_lines='skip'),\n",
        "        dict(header=None, sep='\\t', engine='python', on_bad_lines='skip'),\n",
        "        dict(header=None, sep=r'\\s+', engine='python', on_bad_lines='skip'),\n",
        "    ]\n",
        "    df = None\n",
        "    for kw in variants:\n",
        "        try:\n",
        "            tmp = pd.read_csv(path, **kw)\n",
        "            if tmp.shape[1] >= 6:\n",
        "                df = tmp\n",
        "                break\n",
        "        except Exception:\n",
        "            continue\n",
        "    if df is None:\n",
        "        # final fallback: read lines and split manually by comma\n",
        "        try:\n",
        "            rows = []\n",
        "            with open(path, 'r', errors='ignore') as f:\n",
        "                for line in f:\n",
        "                    parts = [p.strip() for p in line.strip().split(',')]\n",
        "                    if len(parts) >= 6:\n",
        "                        rows.append(parts[:6])\n",
        "            if len(rows) > 0:\n",
        "                df = pd.DataFrame(rows)\n",
        "            else:\n",
        "                raise RuntimeError('No parsable rows')\n",
        "        except Exception:\n",
        "            # return empty to skip geometry features\n",
        "            return pd.DataFrame({'rec_id': pd.Series(dtype=int), 'seg_id': pd.Series(dtype=int)})\n",
        "    # assume columns: rec_id, seg_id, x, y, w, h, ...\n",
        "    ren = {0:'rec_id', 1:'seg_id', 2:'x', 3:'y', 4:'w', 5:'h'}\n",
        "    df = df.rename(columns=ren)\n",
        "    for c in ['rec_id','seg_id','x','y','w','h']:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "    df = df.dropna(subset=['rec_id','seg_id']).copy()\n",
        "    if 'rec_id' in df.columns:\n",
        "        df['rec_id'] = df['rec_id'].astype(int)\n",
        "    return df\n",
        "\n",
        "def read_histogram_segments(path):\n",
        "    variants = [\n",
        "        dict(header=0, sep=','),\n",
        "        dict(header=None, skiprows=1, sep=','),\n",
        "        dict(header=0, sep=r'\\s+', engine='python'),\n",
        "        dict(header=None, skiprows=1, sep=r'\\s+', engine='python'),\n",
        "    ]\n",
        "    df = None\n",
        "    for kw in variants:\n",
        "        try:\n",
        "            tmp = pd.read_csv(path, **kw)\n",
        "            if tmp.shape[1] >= 3:\n",
        "                df = tmp\n",
        "                break\n",
        "        except Exception:\n",
        "            continue\n",
        "    if df is None:\n",
        "        df = pd.read_csv(path, header=None)\n",
        "    cols = list(df.columns)\n",
        "    if len(cols) >= 1: df = df.rename(columns={cols[0]: 'rec_id'})\n",
        "    if len(cols) >= 2: df = df.rename(columns={cols[1]: 'seg_id'})\n",
        "    rem = [c for c in df.columns if c not in ['rec_id','seg_id']]\n",
        "    if any(isinstance(c, int) for c in rem) or any(str(c).isdigit() for c in rem):\n",
        "        rename_map = {}; idx = 0\n",
        "        for c in rem:\n",
        "            rename_map[c] = f'hist_{idx}'; idx += 1\n",
        "        df = df.rename(columns=rename_map)\n",
        "    df['rec_id'] = pd.to_numeric(df['rec_id'], errors='coerce')\n",
        "    df = df.dropna(subset=['rec_id']).copy()\n",
        "    df['rec_id'] = df['rec_id'].astype(int)\n",
        "    # ensure numeric bins\n",
        "    for c in [c for c in df.columns if c not in ['rec_id','seg_id']]:\n",
        "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "    return df\n",
        "\n",
        "# Labels\n",
        "species_df = pd.read_csv(ess / 'species_list.txt')\n",
        "num_classes = species_df.shape[0]\n",
        "train_rows = df_train_ids[['rec_id','labels']].copy()\n",
        "test_rows = df_test_ids[['rec_id']].copy()\n",
        "print(f\"[TAB] Using labels from rec_labels_test_hidden: train={len(train_rows)}, test={len(test_rows)}, classes={num_classes}\")\n",
        "\n",
        "# Segment features: expanded aggs\n",
        "seg_feat_path = supp / 'segment_features.txt'\n",
        "df_seg = read_segment_features(seg_feat_path)\n",
        "feat_cols = [c for c in df_seg.columns if c not in ['rec_id','seg_id']]\n",
        "# basic statistics\n",
        "g_base = df_seg.groupby('rec_id')[feat_cols].agg(['mean','std','min','max','median'])\n",
        "g_base.columns = [f\"f{col}_{stat}\" for col, stat in g_base.columns.to_flat_index()]\n",
        "g_base = g_base.reset_index()\n",
        "# quantiles\n",
        "g_q25 = df_seg.groupby('rec_id')[feat_cols].quantile(0.25).reset_index()\n",
        "g_q75 = df_seg.groupby('rec_id')[feat_cols].quantile(0.75).reset_index()\n",
        "g_q25.columns = ['rec_id'] + [f\"f{c}_q25\" for c in feat_cols]\n",
        "g_q75.columns = ['rec_id'] + [f\"f{c}_q75\" for c in feat_cols]\n",
        "# skew and kurtosis computed separately\n",
        "g_skew = df_seg.groupby('rec_id')[feat_cols].agg('skew').reset_index()\n",
        "g_skew.columns = ['rec_id'] + [f\"f{c}_skew\" for c in feat_cols]\n",
        "g_kurt = df_seg.groupby('rec_id')[feat_cols].agg(pd.Series.kurt).reset_index()\n",
        "g_kurt.columns = ['rec_id'] + [f\"f{c}_kurt\" for c in feat_cols]\n",
        "# count of segments\n",
        "cnt = df_seg.groupby('rec_id')['seg_id'].count().rename('n_segments').reset_index()\n",
        "\n",
        "# Segment rectangles: geometry features\n",
        "rect_path = supp / 'segment_rectangles.txt'\n",
        "df_rect = read_segment_rectangles(rect_path)\n",
        "if set(['rec_id','seg_id','w','h']).issubset(df_rect.columns):\n",
        "    # duration ~ width, bandwidth ~ height; area, aspect\n",
        "    df_rect = df_rect.copy()\n",
        "    df_rect['duration'] = df_rect['w'].clip(lower=0)\n",
        "    df_rect['bandwidth'] = df_rect['h'].clip(lower=0)\n",
        "    df_rect['area'] = (df_rect['w'].clip(lower=0) * df_rect['h'].clip(lower=0))\n",
        "    df_rect['aspect_ratio'] = df_rect['h'].clip(lower=0) / np.clip(df_rect['w'].clip(lower=0), 1e-6, None)\n",
        "    geom_cols = ['duration','bandwidth','area','aspect_ratio']\n",
        "    g_geom_base = df_rect.groupby('rec_id')[geom_cols].agg(['mean','std','sum','min','max','median']).reset_index()\n",
        "    g_geom_q25 = df_rect.groupby('rec_id')[geom_cols].quantile(0.25).reset_index()\n",
        "    g_geom_q75 = df_rect.groupby('rec_id')[geom_cols].quantile(0.75).reset_index()\n",
        "    # flatten\n",
        "    g_geom_base.columns = ['rec_id'] + [f\"geom_{c}_{stat}\" for c, stat in g_geom_base.columns.to_flat_index()[1:]]\n",
        "    g_geom_q25.columns = ['rec_id'] + [f\"geom_{c}_q25\" for c in geom_cols]\n",
        "    g_geom_q75.columns = ['rec_id'] + [f\"geom_{c}_q75\" for c in geom_cols]\n",
        "else:\n",
        "    # empty placeholders\n",
        "    g_geom_base = pd.DataFrame({'rec_id': []})\n",
        "    g_geom_q25 = pd.DataFrame({'rec_id': []})\n",
        "    g_geom_q75 = pd.DataFrame({'rec_id': []})\n",
        "\n",
        "# merge all seg aggs\n",
        "df_seg_agg = g_base.merge(g_q25, on='rec_id', how='left').merge(g_q75, on='rec_id', how='left').merge(g_skew, on='rec_id', how='left').merge(g_kurt, on='rec_id', how='left').merge(cnt, on='rec_id', how='left')\n",
        "if 'rec_id' in g_geom_base.columns:\n",
        "    df_seg_agg = df_seg_agg.merge(g_geom_base, on='rec_id', how='left').merge(g_geom_q25, on='rec_id', how='left').merge(g_geom_q75, on='rec_id', how='left')\n",
        "df_seg_agg = df_seg_agg.fillna(0.0)\n",
        "print(f\"[TAB] seg_agg shape: {df_seg_agg.shape}\")\n",
        "\n",
        "# Histogram features\n",
        "hist_path = supp / 'histogram_of_segments.txt'\n",
        "df_hist = read_histogram_segments(hist_path)\n",
        "hist_bins = [c for c in df_hist.columns if c not in ['rec_id','seg_id']]\n",
        "print(f\"[TAB] histogram raw shape: {df_hist.shape}; first cols: {list(df_hist.columns)[:6]}\")\n",
        "if len(hist_bins) == 0:\n",
        "    raise RuntimeError('No histogram bins parsed')\n",
        "\n",
        "# A) Raw-sum features per rec_id\n",
        "raw_sum = df_hist.groupby('rec_id')[hist_bins].sum().reset_index()\n",
        "raw_sum_total = raw_sum[hist_bins].sum(axis=1).values.reshape(-1,1)\n",
        "raw_sum_frac = raw_sum.copy()\n",
        "raw_sum_frac[hist_bins] = (raw_sum[hist_bins].values / np.clip(raw_sum_total, 1e-12, None))\n",
        "# record-level stats from fractions\n",
        "frac_vals = raw_sum_frac[hist_bins].values\n",
        "entropy = -(frac_vals * np.log(np.clip(frac_vals, 1e-12, None))).sum(axis=1)\n",
        "herfindahl = (frac_vals**2).sum(axis=1)\n",
        "gini = 1.0 - herfindahl\n",
        "top1 = np.max(frac_vals, axis=1)\n",
        "top2 = np.partition(frac_vals, -2, axis=1)[:, -2:].sum(axis=1)\n",
        "top3 = np.partition(frac_vals, -3, axis=1)[:, -3:].sum(axis=1)\n",
        "argmax = frac_vals.argmax(axis=1).astype(int)\n",
        "nonzero_bins = (frac_vals > 0).sum(axis=1)\n",
        "\n",
        "# per-record skew/kurt over normalized bins\n",
        "def row_skew(x):\n",
        "    m = x.mean(); s = x.std()\n",
        "    if s <= 1e-12: return 0.0\n",
        "    z = (x - m) / s\n",
        "    return float((z**3).mean())\n",
        "def row_kurt(x):\n",
        "    m = x.mean(); s = x.std()\n",
        "    if s <= 1e-12: return 0.0\n",
        "    z = (x - m) / s\n",
        "    return float((z**4).mean() - 3.0)\n",
        "row_skews = np.apply_along_axis(row_skew, 1, frac_vals)\n",
        "row_kurts = np.apply_along_axis(row_kurt, 1, frac_vals)\n",
        "\n",
        "df_hist_sum = raw_sum.add_prefix('sum_')\n",
        "df_hist_sum = df_hist_sum.rename(columns={'sum_rec_id':'rec_id'})\n",
        "df_hist_frac = raw_sum_frac.add_prefix('frac_')\n",
        "df_hist_frac = df_hist_frac.rename(columns={'frac_rec_id':'rec_id'})\n",
        "df_hist_rec = df_hist_sum.merge(df_hist_frac, on='rec_id', how='left')\n",
        "df_hist_rec['hist_entropy'] = entropy\n",
        "df_hist_rec['hist_gini'] = gini\n",
        "df_hist_rec['hist_herfindahl'] = herfindahl\n",
        "df_hist_rec['hist_top1'] = top1\n",
        "df_hist_rec['hist_top2_sum'] = top2\n",
        "df_hist_rec['hist_top3_sum'] = top3\n",
        "df_hist_rec['hist_argmax'] = argmax\n",
        "df_hist_rec['hist_nonzero_bins'] = nonzero_bins\n",
        "df_hist_rec['hist_frac_skew'] = row_skews\n",
        "df_hist_rec['hist_frac_kurt'] = row_kurts\n",
        "df_hist_rec['raw_sum_total'] = raw_sum_total.ravel()\n",
        "\n",
        "# per-segment totals and their aggs\n",
        "seg_totals = df_hist.copy()\n",
        "seg_totals['seg_total'] = seg_totals[hist_bins].sum(axis=1)\n",
        "seg_agg = seg_totals.groupby('rec_id')['seg_total']\n",
        "seg_total_feats = pd.DataFrame({\n",
        "    'rec_id': seg_agg.size().index,\n",
        "    'seg_total_mean': seg_agg.mean().values,\n",
        "    'seg_total_std': seg_agg.std().fillna(0).values,\n",
        "    'seg_total_min': seg_agg.min().values,\n",
        "    'seg_total_max': seg_agg.max().values,\n",
        "    'seg_total_q25': seg_agg.quantile(0.25).values,\n",
        "    'seg_total_q75': seg_agg.quantile(0.75).values,\n",
        "    'seg_total_sum': seg_agg.sum().values,\n",
        "})\n",
        "\n",
        "# simple peak stats on record-level normalized histogram (count local maxima)\n",
        "def count_peaks(row):\n",
        "    x = row.values.astype(float)\n",
        "    cnt = 0\n",
        "    for i in range(1, len(x)-1):\n",
        "        if x[i] > x[i-1] and x[i] > x[i+1]:\n",
        "            cnt += 1\n",
        "    return cnt\n",
        "try:\n",
        "    df_tmp_frac = df_hist_frac[[c for c in df_hist_frac.columns if c.startswith('frac_hist_')]].copy()\n",
        "    num_peaks = df_tmp_frac.apply(count_peaks, axis=1).values\n",
        "    df_hist_rec['hist_num_peaks'] = num_peaks\n",
        "except Exception:\n",
        "    df_hist_rec['hist_num_peaks'] = 0\n",
        "\n",
        "# B) Shape features from per-segment normalized histograms (mean/std/skew/kurt across segments)\n",
        "H = df_hist[hist_bins].to_numpy(dtype=float)\n",
        "H = np.nan_to_num(H, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "H = np.maximum(H, 0.0)\n",
        "row_sum = np.clip(H.sum(axis=1, keepdims=True), 1e-12, None)\n",
        "Hn = H / row_sum\n",
        "df_hist_norm = df_hist[['rec_id']].copy()\n",
        "df_hist_norm[hist_bins] = Hn\n",
        "shape_mean = df_hist_norm.groupby('rec_id')[hist_bins].mean().reset_index()\n",
        "shape_std = df_hist_norm.groupby('rec_id')[hist_bins].std().fillna(0.0).reset_index()\n",
        "shape_skew = df_hist_norm.groupby('rec_id')[hist_bins].agg('skew').fillna(0.0).reset_index()\n",
        "shape_kurt = df_hist_norm.groupby('rec_id')[hist_bins].agg(pd.Series.kurt).fillna(0.0).reset_index()\n",
        "shape_mean.columns = ['rec_id'] + [f'shape_mean_{c}' for c in hist_bins]\n",
        "shape_std.columns = ['rec_id'] + [f'shape_std_{c}' for c in hist_bins]\n",
        "shape_skew.columns = ['rec_id'] + [f'shape_skew_{c}' for c in hist_bins]\n",
        "shape_kurt.columns = ['rec_id'] + [f'shape_kurt_{c}' for c in hist_bins]\n",
        "\n",
        "# Combine histogram record-level features\n",
        "df_hist_features = df_hist_rec.merge(seg_total_feats, on='rec_id', how='left').merge(shape_mean, on='rec_id', how='left').merge(shape_std, on='rec_id', how='left').merge(shape_skew, on='rec_id', how='left').merge(shape_kurt, on='rec_id', how='left')\n",
        "df_hist_features = df_hist_features.fillna(0.0)\n",
        "print(f\"[TAB] hist_features shape: {df_hist_features.shape}\")\n",
        "\n",
        "# Two datasets:\n",
        "# Model A features: Seg + Hist (inner on rec_id)\n",
        "df_feat_A = df_seg_agg.merge(df_hist_features, on='rec_id', how='inner')\n",
        "print(f\"[TAB] Model A feature shape: {df_feat_A.shape}\")\n",
        "# Model B features: Hist-only (all with histogram)\n",
        "df_feat_B = df_hist_features.copy()\n",
        "print(f\"[TAB] Model B feature shape: {df_feat_B.shape}\")\n",
        "\n",
        "# Prepare train/test merges for A and B\n",
        "train_A = train_rows.merge(df_feat_A, on='rec_id', how='inner')\n",
        "test_A = test_rows.merge(df_feat_A, on='rec_id', how='inner')\n",
        "train_B = train_rows.merge(df_feat_B, on='rec_id', how='inner')\n",
        "test_B = test_rows.merge(df_feat_B, on='rec_id', how='inner')\n",
        "print(f\"[TAB] Train A: {len(train_A)} recs; Test A: {len(test_A)} recs\")\n",
        "print(f\"[TAB] Train B: {len(train_B)} recs; Test B: {len(test_B)} recs\")\n",
        "\n",
        "def build_XY(df):\n",
        "    X = df.drop(columns=['rec_id','labels']).values.astype(np.float32)\n",
        "    Y = np.zeros((len(df), num_classes), dtype=np.float32)\n",
        "    for i, labs in enumerate(df['labels']):\n",
        "        for c in labs: Y[i, c] = 1.0\n",
        "    return X, Y\n",
        "\n",
        "X_A, Y_A = build_XY(train_A) if len(train_A) else (np.zeros((0,0),np.float32), np.zeros((0, num_classes), np.float32))\n",
        "X_B, Y_B = build_XY(train_B)\n",
        "XA_test = test_A.drop(columns=['rec_id']).values.astype(np.float32) if len(test_A) else np.zeros((0,0),np.float32)\n",
        "XB_test = test_B.drop(columns=['rec_id']).values.astype(np.float32)\n",
        "test_rec_A = test_A['rec_id'].tolist() if len(test_A) else []\n",
        "test_rec_B = test_B['rec_id'].tolist()\n",
        "print(f\"[TAB] X_A {X_A.shape}, X_B {X_B.shape}; test A {len(test_rec_A)}, test B {len(test_rec_B)}\")\n",
        "\n",
        "# Training function with MLSK, rare-class augmentation, robust params; seed bagging\n",
        "def train_single_seed(X, Y, X_test, n_splits=5, seed=42, label='A'):\n",
        "    if X.shape[0] == 0:\n",
        "        return np.zeros((0, num_classes), np.float32), np.zeros((X_test.shape[0], num_classes), np.float32), float('nan'), []\n",
        "    mskf = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "    oof = np.zeros_like(Y, dtype=np.float32)\n",
        "    test_accum = np.zeros((X_test.shape[0], num_classes), dtype=np.float32)\n",
        "    fold_class_aucs = []\n",
        "    for fold, (trn_idx, val_idx) in enumerate(mskf.split(X, Y), 1):\n",
        "        t0 = time.time()\n",
        "        X_tr, X_val = X[trn_idx], X[val_idx]\n",
        "        y_tr, y_val = Y[trn_idx], Y[val_idx]\n",
        "        val_pred = np.zeros_like(y_val, dtype=np.float32)\n",
        "        test_fold = np.zeros((X_test.shape[0], num_classes), dtype=np.float32)\n",
        "        for c in range(num_classes):\n",
        "            ytr_c = y_tr[:, c]; yval_c = y_val[:, c]\n",
        "            # rare class augmentation: ensure at least 10 positives via bootstrapping\n",
        "            pos_idx = np.where(ytr_c == 1)[0]\n",
        "            neg_idx = np.where(ytr_c == 0)[0]\n",
        "            X_tr_c = X_tr; ytr_c_aug = ytr_c\n",
        "            if len(pos_idx) > 0 and len(pos_idx) < 10:\n",
        "                need = 10 - len(pos_idx)\n",
        "                dup_idx = np.random.RandomState(seed + fold + c).choice(pos_idx, size=need, replace=True)\n",
        "                X_tr_c = np.concatenate([X_tr, X_tr[dup_idx]], axis=0)\n",
        "                ytr_c_aug = np.concatenate([ytr_c, np.ones(need, dtype=ytr_c.dtype)], axis=0)\n",
        "            pos = float(ytr_c_aug.sum()); neg = float(len(ytr_c_aug) - pos)\n",
        "            spw = float(neg / max(pos, 1.0)) if pos > 0 else 1.0\n",
        "            spw = min(100.0, spw)\n",
        "            dtr = lgb.Dataset(X_tr_c, label=ytr_c_aug)\n",
        "            dval = lgb.Dataset(X_val, label=yval_c, reference=dtr)\n",
        "            params = {\n",
        "                'objective': 'binary', 'metric': 'auc', 'verbose': -1,\n",
        "                'learning_rate': 0.03, 'num_leaves': 31, 'min_data_in_leaf': 15,\n",
        "                'feature_fraction': 0.6, 'bagging_fraction': 0.8, 'bagging_freq': 1,\n",
        "                'lambda_l1': 0.1, 'lambda_l2': 1.0, 'scale_pos_weight': spw,\n",
        "                'extra_trees': True, 'min_sum_hessian_in_leaf': 0.1, 'min_gain_to_split': 0.01,\n",
        "            }\n",
        "            callbacks = [lgb.early_stopping(stopping_rounds=250, verbose=False)]\n",
        "            bst = lgb.train(params, dtr, num_boost_round=5000, valid_sets=[dval], callbacks=callbacks)\n",
        "            val_pred[:, c] = bst.predict(X_val, num_iteration=bst.best_iteration)\n",
        "            if X_test.shape[0] > 0:\n",
        "                test_fold[:, c] = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
        "        oof[val_idx] = val_pred\n",
        "        aucs = []\n",
        "        for c in range(num_classes):\n",
        "            yv = y_val[:, c]\n",
        "            if yv.sum() > 0 and (len(yv) - yv.sum()) > 0:\n",
        "                try: aucs.append(roc_auc_score(yv, val_pred[:, c]))\n",
        "                except Exception: pass\n",
        "        fold_auc = float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "        fold_class_aucs.append(aucs)\n",
        "        test_accum += test_fold\n",
        "        print(f\"[TAB-{label}] Seed {seed} Fold {fold}: macro AUC={fold_auc:.4f}; time {time.time()-t0:.2f}s\")\n",
        "    # OOF macro over classes with pos/neg\n",
        "    class_aucs = []\n",
        "    for c in range(num_classes):\n",
        "        yc = Y[:, c]\n",
        "        if yc.sum() > 0 and (len(yc) - yc.sum()) > 0:\n",
        "            try: class_aucs.append(roc_auc_score(yc, oof[:, c]))\n",
        "            except Exception: pass\n",
        "    oof_macro = float(np.mean(class_aucs)) if len(class_aucs) else float('nan')\n",
        "    print(f\"[TAB-{label}] Seed {seed} OOF macro AUC over {len(class_aucs)} classes: {oof_macro:.4f}\")\n",
        "    return oof, (test_accum / n_splits), oof_macro, class_aucs\n",
        "\n",
        "def run_bag(X, Y, X_test, seeds, label):\n",
        "    oofs = []; tests = []; class_auc_list = []\n",
        "    for sd in seeds:\n",
        "        oof, tpred, oof_macro, class_aucs = train_single_seed(X, Y, X_test, n_splits=5, seed=sd, label=label)\n",
        "        oofs.append(oof); tests.append(tpred); class_auc_list.append(class_aucs)\n",
        "    oof_avg = np.mean(oofs, axis=0)\n",
        "    test_avg = np.mean(tests, axis=0)\n",
        "    # average per-class AUCs across seeds\n",
        "    class_auc_avg = np.nanmean(np.vstack([np.array(c + [np.nan]*(num_classes - len(c))) for c in class_auc_list]), axis=0)\n",
        "    return oof_avg, test_avg, class_auc_avg\n",
        "\n",
        "seeds = [42, 1337, 2025]\n",
        "oof_A, test_A_pred, class_auc_A = run_bag(X_A, Y_A, XA_test, seeds, label='A') if X_A.shape[0] else (np.zeros((0, num_classes), np.float32), np.zeros((XA_test.shape[0], num_classes), np.float32), np.zeros((num_classes,), dtype=float))\n",
        "oof_B, test_B_pred, class_auc_B = run_bag(X_B, Y_B, XB_test, seeds, label='B')\n",
        "\n",
        "# Per-class blending weights from OOF AUCs\n",
        "wA = np.array(class_auc_A, dtype=float)\n",
        "wB = np.array(class_auc_B, dtype=float)\n",
        "wA = np.nan_to_num(wA, nan=0.0); wB = np.nan_to_num(wB, nan=0.0)\n",
        "den = wA + wB\n",
        "wA_norm = np.where(den > 0, wA / den, 0.5)\n",
        "wB_norm = 1.0 - wA_norm\n",
        "print('[TAB] Per-class blend weight A (first 10):', np.round(wA_norm[:10], 3))\n",
        "\n",
        "# Build combined OOF for monitoring (use per-class weights where both available)\n",
        "rid_A = train_A['rec_id'].tolist() if len(train_A) else []\n",
        "rid_B = train_B['rec_id'].tolist()\n",
        "rid2idxA = {r:i for i,r in enumerate(rid_A)}\n",
        "rid2idxB = {r:i for i,r in enumerate(rid_B)}\n",
        "all_train_rids = train_rows['rec_id'].tolist()\n",
        "oof_combined = []\n",
        "for r in all_train_rids:\n",
        "    if r in rid2idxA and r in rid2idxB:\n",
        "        ia, ib = rid2idxA[r], rid2idxB[r]\n",
        "        # apply per-class blend\n",
        "        oof_combined.append(wA_norm * oof_A[ia] + wB_norm * oof_B[ib])\n",
        "    elif r in rid2idxA:\n",
        "        oof_combined.append(oof_A[rid2idxA[r]])\n",
        "    elif r in rid2idxB:\n",
        "        oof_combined.append(oof_B[rid2idxB[r]])\n",
        "    else:\n",
        "        oof_combined.append(np.full((num_classes,), np.nan, dtype=np.float32))\n",
        "oof_combined = np.stack(oof_combined, axis=0)\n",
        "Y_all = np.zeros((len(all_train_rids), num_classes), dtype=np.float32)\n",
        "for i, labs in enumerate(train_rows['labels']):\n",
        "    for c in labs: Y_all[i, c] = 1.0\n",
        "class_aucs = []\n",
        "for c in range(num_classes):\n",
        "    y = Y_all[:, c]\n",
        "    preds = oof_combined[:, c]\n",
        "    mask = ~np.isnan(preds)\n",
        "    yv = y[mask]; pv = preds[mask]\n",
        "    if len(yv) > 0 and yv.sum() > 0 and (len(yv)-yv.sum()) > 0:\n",
        "        try: class_aucs.append(roc_auc_score(yv, pv))\n",
        "        except Exception: pass\n",
        "oof_macro_combined = float(np.mean(class_aucs)) if len(class_aucs) else float('nan')\n",
        "print(f\"[TAB] Combined OOF macro AUC over {len(class_aucs)} classes: {oof_macro_combined:.4f}\")\n",
        "\n",
        "# Build test predictions combining A and B with per-class weights\n",
        "id2prob = {}\n",
        "test_recids_all = sorted(test_rows['rec_id'].tolist())\n",
        "pred_map_A = {r: test_A_pred[i] for i, r in enumerate(test_rec_A)} if len(test_rec_A) else {}\n",
        "pred_map_B = {r: test_B_pred[i] for i, r in enumerate(test_rec_B)}\n",
        "\n",
        "# post-process: clip ultra-rare class predictions\n",
        "pos_counts = Y_all.sum(axis=0)\n",
        "ultra_rare = set(np.where(pos_counts <= 3)[0].tolist())\n",
        "\n",
        "for r in test_recids_all:\n",
        "    if (r in pred_map_A) and (r in pred_map_B):\n",
        "        p = wA_norm * pred_map_A[r] + wB_norm * pred_map_B[r]\n",
        "    elif r in pred_map_A:\n",
        "        p = pred_map_A[r]\n",
        "    elif r in pred_map_B:\n",
        "        p = pred_map_B[r]\n",
        "    else:\n",
        "        p = np.full((num_classes,), 0.05, dtype=np.float32)\n",
        "    # clip ultra-rare\n",
        "    if len(ultra_rare) > 0:\n",
        "        for c in ultra_rare:\n",
        "            p[c] = float(np.clip(p[c], 0.15, 0.85))\n",
        "    for c in range(num_classes):\n",
        "        Id = int(r) * 100 + c\n",
        "        id2prob[Id] = float(p[c])\n",
        "\n",
        "# Write/Blend submission\n",
        "df_base = pd.read_csv('sample_submission.csv')\n",
        "df_out = df_base.copy()\n",
        "df_out['Probability'] = df_out['Id'].map(id2prob).fillna(0.05)\n",
        "if Path('submission.csv').exists():\n",
        "    df_prev = pd.read_csv('submission.csv')\n",
        "    blended = df_prev['Probability'].copy()\n",
        "    mask = df_out['Probability'].notna()\n",
        "    blended.loc[mask] = 0.5 * blended.loc[mask].values + 0.5 * df_out.loc[mask, 'Probability'].values\n",
        "    df_prev['Probability'] = blended\n",
        "    df_prev.to_csv('submission.csv', index=False)\n",
        "    print('[TAB] Blended with previous submission and saved submission.csv')\n",
        "else:\n",
        "    df_out.to_csv('submission.csv', index=False)\n",
        "    print('[TAB] Saved submission.csv from tabular pipeline')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB] Using labels from rec_labels_test_hidden: train=258, test=64, classes=19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB] seg_agg shape: (154, 376)\n[TAB] histogram raw shape: (322, 101); first cols: ['rec_id', 'seg_id', 'hist_0', 'hist_1', 'hist_2', 'hist_3']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB] hist_features shape: (322, 614)\n[TAB] Model A feature shape: (154, 989)\n[TAB] Model B feature shape: (322, 614)\n[TAB] Train A: 122 recs; Test A: 32 recs\n[TAB] Train B: 258 recs; Test B: 64 recs\n[TAB] X_A (122, 988), X_B (258, 613); test A 32, test B 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-A] Seed 42 Fold 1: macro AUC=0.8200; time 1.93s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-A] Seed 42 Fold 2: macro AUC=0.8582; time 2.08s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-A] Seed 42 Fold 3: macro AUC=0.8559; time 2.05s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-A] Seed 42 Fold 4: macro AUC=0.9232; time 2.10s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-A] Seed 42 Fold 5: macro AUC=0.8905; time 1.82s\n[TAB-A] Seed 42 OOF macro AUC over 19 classes: 0.7866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-A] Seed 1337 Fold 1: macro AUC=0.8488; time 2.07s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-A] Seed 1337 Fold 2: macro AUC=0.8572; time 1.90s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-A] Seed 1337 Fold 3: macro AUC=0.8497; time 1.92s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-A] Seed 1337 Fold 4: macro AUC=0.7808; time 1.88s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-A] Seed 1337 Fold 5: macro AUC=0.8282; time 1.82s\n[TAB-A] Seed 1337 OOF macro AUC over 19 classes: 0.7314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-A] Seed 2025 Fold 1: macro AUC=0.8221; time 1.91s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-A] Seed 2025 Fold 2: macro AUC=0.8321; time 2.00s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-A] Seed 2025 Fold 3: macro AUC=0.8334; time 1.74s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-A] Seed 2025 Fold 4: macro AUC=0.8323; time 1.99s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-A] Seed 2025 Fold 5: macro AUC=0.8415; time 2.42s\n[TAB-A] Seed 2025 OOF macro AUC over 19 classes: 0.7595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-B] Seed 42 Fold 1: macro AUC=0.7395; time 2.06s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-B] Seed 42 Fold 2: macro AUC=0.8033; time 1.83s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-B] Seed 42 Fold 3: macro AUC=0.8266; time 1.46s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-B] Seed 42 Fold 4: macro AUC=0.7558; time 1.78s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-B] Seed 42 Fold 5: macro AUC=0.7813; time 1.80s\n[TAB-B] Seed 42 OOF macro AUC over 19 classes: 0.7013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-B] Seed 1337 Fold 1: macro AUC=0.7313; time 1.67s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-B] Seed 1337 Fold 2: macro AUC=0.7895; time 1.76s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-B] Seed 1337 Fold 3: macro AUC=0.7971; time 1.52s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-B] Seed 1337 Fold 4: macro AUC=0.7838; time 1.64s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-B] Seed 1337 Fold 5: macro AUC=0.7821; time 1.40s\n[TAB-B] Seed 1337 OOF macro AUC over 19 classes: 0.6984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-B] Seed 2025 Fold 1: macro AUC=0.7601; time 1.96s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-B] Seed 2025 Fold 2: macro AUC=0.8055; time 1.82s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-B] Seed 2025 Fold 3: macro AUC=0.7496; time 2.41s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-B] Seed 2025 Fold 4: macro AUC=0.7986; time 1.79s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:743: UserWarning: Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB-B] Seed 2025 Fold 5: macro AUC=0.8082; time 1.81s\n[TAB-B] Seed 2025 OOF macro AUC over 19 classes: 0.6870\n[TAB] Per-class blend weight A (first 10): [0.514 0.526 0.519 0.518 0.526 0.47  0.541 0.579 0.514 0.49 ]\n[TAB] Combined OOF macro AUC over 19 classes: 0.7589\n[TAB] Blended with previous submission and saved submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "fe0116ab-d077-437d-a6f9-6e8bac0f72d8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Diagnostics: per-class support and OOF AUCs for LGBM pipeline\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "try:\n",
        "    Y_all, oof, num_classes\n",
        "except NameError:\n",
        "    raise RuntimeError(\"Run Cell 7 first to populate Y_all and oof.\")\n",
        "\n",
        "pos_counts = Y_all.sum(axis=0).astype(int)\n",
        "neg_counts = (Y_all.shape[0] - pos_counts).astype(int)\n",
        "aucs = []\n",
        "for c in range(num_classes):\n",
        "    y = Y_all[:, c]\n",
        "    if y.sum() > 0 and (len(y) - y.sum()) > 0:\n",
        "        try:\n",
        "            aucs.append(roc_auc_score(y, oof[:, c]))\n",
        "        except Exception:\n",
        "            aucs.append(np.nan)\n",
        "    else:\n",
        "        aucs.append(np.nan)\n",
        "df_diag = pd.DataFrame({\n",
        "    'class_id': np.arange(num_classes),\n",
        "    'pos': pos_counts,\n",
        "    'neg': neg_counts,\n",
        "    'oof_auc': aucs\n",
        "})\n",
        "df_diag_sorted = df_diag.sort_values('oof_auc')\n",
        "print('[DIAG] Per-class OOF AUC (worst 10):')\n",
        "print(df_diag_sorted.head(10).to_string(index=False))\n",
        "print('\\n[DIAG] Per-class OOF AUC (best 10):')\n",
        "print(df_diag_sorted.tail(10).to_string(index=False))\n",
        "valid_aucs = df_diag['oof_auc'][df_diag['oof_auc'].notna()]\n",
        "print(f\"\\n[DIAG] Macro OOF over {valid_aucs.shape[0]} classes: {valid_aucs.mean():.4f}\")\n",
        "\n",
        "# Check class id bounds from labels to ensure mapping is correct\n",
        "try:\n",
        "    train_labels_series = train_rows['labels']\n",
        "    all_labs = [lab for labs in train_labels_series for lab in labs]\n",
        "    if len(all_labs):\n",
        "        print(f\"[DIAG] Labels min={min(all_labs)}, max={max(all_labs)}, unique={len(set(all_labs))}\")\n",
        "except Exception as e:\n",
        "    print(f\"[DIAG] Label range check skipped: {e}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DIAG] Per-class OOF AUC (worst 10):\n class_id  pos  neg  oof_auc\n       16    2  120 0.160417\n       17    2  120 0.195833\n        9   17  105 0.545658\n       13    4  118 0.572034\n        5    5  117 0.596581\n        3    2  120 0.620833\n       15    6  116 0.681034\n        6   13  109 0.715596\n       14   14  108 0.729167\n        7   14  108 0.764550\n\n[DIAG] Per-class OOF AUC (best 10):\n class_id  pos  neg  oof_auc\n        7   14  108 0.764550\n       18   12  110 0.803788\n        4    8  114 0.805921\n       12   10  112 0.807143\n       10   38   84 0.809994\n        2   18  104 0.812233\n        1   25   97 0.828247\n        8   23   99 0.859025\n        0    7  115 0.867081\n       11    8  114 0.938048\n\n[DIAG] Macro OOF over 19 classes: 0.6902\n[DIAG] Labels min=0, max=18, unique=19\n"
          ]
        }
      ]
    },
    {
      "id": "dfc2c6ad-64a7-41c6-bb67-73a025a04a24",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CNN baseline v3: EfficientNet-B0 offline pretrained, moderate res, simple cosine, stronger SpecAugment (no EMA)\n",
        "import sys, subprocess, importlib, os, time, math, random, gc, urllib.request\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "def ensure_pkg(pkg):\n",
        "    try:\n",
        "        importlib.import_module(pkg)\n",
        "        return True\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg, '-q'])\n",
        "        importlib.invalidate_caches()\n",
        "        return True\n",
        "\n",
        "ensure_pkg('torch'); ensure_pkg('torchvision'); ensure_pkg('timm'); ensure_pkg('iterative-stratification')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models as tvm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "ess = Path('essential_data'); supp = Path('supplemental_data')\n",
        "\n",
        "species_df = pd.read_csv(ess / 'species_list.txt')\n",
        "num_classes = species_df.shape[0]\n",
        "df_id2fn = pd.read_csv(ess / 'rec_id2filename.txt')\n",
        "id2fn = dict(zip(df_id2fn.rec_id.astype(int), df_id2fn.filename.astype(str)))\n",
        "\n",
        "# Moderate input preserving more time resolution\n",
        "IMG_H, IMG_W = 224, 640\n",
        "\n",
        "def load_spectrogram(rec_id: int, folder='filtered_spectrograms'):\n",
        "    fn = id2fn.get(int(rec_id), None)\n",
        "    if fn is None: return None\n",
        "    p = supp / folder / f\"{fn}.bmp\"\n",
        "    if not p.exists():\n",
        "        return None\n",
        "    try:\n",
        "        img = Image.open(p).convert('L')\n",
        "    except Exception:\n",
        "        return None\n",
        "    # aspect-preserving resize on height\n",
        "    w, h = img.size\n",
        "    new_w = int(round(w * (IMG_H / h)))\n",
        "    img_resized = img.resize((new_w, IMG_H), Image.BILINEAR)\n",
        "    if new_w >= IMG_W:\n",
        "        start = (new_w - IMG_W)//2\n",
        "        img_crop = img_resized.crop((start, 0, start + IMG_W, IMG_H))\n",
        "    else:\n",
        "        pad_left = (IMG_W - new_w)//2\n",
        "        canvas = Image.new('L', (IMG_W, IMG_H), color=0)\n",
        "        canvas.paste(img_resized, (pad_left, 0))\n",
        "        img_crop = canvas\n",
        "    img3 = Image.merge('RGB', (img_crop, img_crop, img_crop))\n",
        "    return img3\n",
        "\n",
        "class SpecAugment:\n",
        "    def __init__(self, time_masks=2, time_max=80, freq_masks=2, freq_max=40):\n",
        "        self.time_masks = time_masks; self.time_max = time_max\n",
        "        self.freq_masks = freq_masks; self.freq_max = freq_max\n",
        "    def __call__(self, x):\n",
        "        C, H, W = x.shape\n",
        "        for _ in range(self.time_masks):\n",
        "            w = random.randint(0, self.time_max)\n",
        "            if w > 0:\n",
        "                t0 = random.randint(0, max(0, W - w))\n",
        "                x[:, :, t0:t0+w] = 0.0\n",
        "        for _ in range(self.freq_masks):\n",
        "            h = random.randint(0, self.freq_max)\n",
        "            if h > 0:\n",
        "                f0 = random.randint(0, max(0, H - h))\n",
        "                x[:, f0:f0+h, :] = 0.0\n",
        "        return x\n",
        "\n",
        "class SpectrogramDataset(Dataset):\n",
        "    def __init__(self, rec_ids, labels=None, folder='filtered_spectrograms', train=True):\n",
        "        self.rec_ids = list(rec_ids); self.labels = labels\n",
        "        self.folder = folder; self.train = train\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "        self.specaug = SpecAugment(time_masks=2, time_max=80, freq_masks=2, freq_max=40)\n",
        "        self.norm = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "        self.max_shift = int(0.05 * IMG_W)  # ~5% horizontal roll\n",
        "    def __len__(self):\n",
        "        return len(self.rec_ids)\n",
        "    def __getitem__(self, idx):\n",
        "        rid = int(self.rec_ids[idx])\n",
        "        img = load_spectrogram(rid, self.folder)\n",
        "        if img is None:\n",
        "            img = load_spectrogram(rid, 'spectrograms')\n",
        "            if img is None:\n",
        "                img = Image.new('RGB', (IMG_W, IMG_H), color=0)\n",
        "        x = self.to_tensor(img)\n",
        "        if self.train:\n",
        "            if self.max_shift > 0 and random.random() < 0.5:\n",
        "                shift = random.randint(-self.max_shift, self.max_shift)\n",
        "                if shift != 0:\n",
        "                    x = torch.roll(x, shifts=shift, dims=2)\n",
        "            x = self.specaug(x)\n",
        "        x = self.norm(x)\n",
        "        if self.labels is None:\n",
        "            return x, rid\n",
        "        y = torch.zeros(num_classes, dtype=torch.float32)\n",
        "        for c in self.labels[idx]:\n",
        "            y[c] = 1.0\n",
        "        return x, y\n",
        "\n",
        "def build_targets(df_rows):\n",
        "    rec_ids = df_rows['rec_id'].tolist()\n",
        "    labels = [labs for labs in df_rows['labels']]\n",
        "    return rec_ids, labels\n",
        "\n",
        "def macro_auc(y_true, y_prob):\n",
        "    aucs = []\n",
        "    for c in range(y_true.shape[1]):\n",
        "        yc = y_true[:, c]\n",
        "        if yc.sum() > 0 and (len(yc) - yc.sum()) > 0:\n",
        "            try: aucs.append(roc_auc_score(yc, y_prob[:, c]))\n",
        "            except Exception: pass\n",
        "    return float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "\n",
        "# Offline pretrained weights for EfficientNet-B0\n",
        "CACHE_DIR = Path('./torch_cache')\n",
        "os.environ['TORCH_HOME'] = str(CACHE_DIR.resolve())\n",
        "CHECKPOINTS_DIR = CACHE_DIR / 'hub' / 'checkpoints'\n",
        "CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "assert os.access(CHECKPOINTS_DIR, os.W_OK), 'Cache directory is not writable.'\n",
        "EFF_URL = 'https://download.pytorch.org/models/efficientnet_b0_rwightman-3dd342df.pth'\n",
        "EFF_FILE = CHECKPOINTS_DIR / 'efficientnet_b0_rwightman-3dd342df.pth'\n",
        "\n",
        "def ensure_file(url, path: Path):\n",
        "    if not path.exists():\n",
        "        print(f\"[SETUP] Downloading weights to {path}\")\n",
        "        urllib.request.urlretrieve(url, path)\n",
        "    else:\n",
        "        print(f\"[SETUP] Weights already exist at {path}\")\n",
        "\n",
        "def create_efficientnet_b0_offline(num_classes):\n",
        "    ensure_file(EFF_URL, EFF_FILE)\n",
        "    model = tvm.efficientnet_b0(weights=None)\n",
        "    state_dict = torch.load(EFF_FILE, map_location='cpu')\n",
        "    state_dict = {k: v for k, v in state_dict.items() if not k.startswith('classifier.')}\n",
        "    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, num_classes)\n",
        "    print('[CNN] EfficientNet-B0 loaded from local cache. Missing:', missing, 'Unexpected:', unexpected)\n",
        "    return model\n",
        "\n",
        "def train_cnn_filtered(seed=42, batch_size=32, max_epochs=25, patience=6, lr=3e-4, wd=1e-2):\n",
        "    torch.manual_seed(seed); np.random.seed(seed); random.seed(seed)\n",
        "    train_df = df_train_ids[['rec_id','labels']].copy().reset_index(drop=True)\n",
        "    rec_ids, labels = build_targets(train_df)\n",
        "    Y = np.zeros((len(labels), num_classes), dtype=np.float32)\n",
        "    for i, labs in enumerate(labels):\n",
        "        for c in labs: Y[i, c] = 1.0\n",
        "    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "    oof = np.zeros_like(Y, dtype=np.float32)\n",
        "    test_rec_sorted = sorted(df_test_ids['rec_id'].tolist())\n",
        "    test_ds = SpectrogramDataset(test_rec_sorted, labels=None, folder='filtered_spectrograms', train=False)\n",
        "    test_logits_accum = np.zeros((len(test_rec_sorted), num_classes), dtype=np.float32)\n",
        "    tta_shifts = [-0.15, -0.075, 0.0, 0.075, 0.15]\n",
        "\n",
        "    for fold, (trn_idx, val_idx) in enumerate(mskf.split(np.arange(len(rec_ids)), Y), 1):\n",
        "        t_fold = time.time()\n",
        "        trn_ids = [rec_ids[i] for i in trn_idx]\n",
        "        trn_labels = [labels[i] for i in trn_idx]\n",
        "        val_ids = [rec_ids[i] for i in val_idx]\n",
        "        val_labels = [labels[i] for i in val_idx]\n",
        "\n",
        "        train_ds = SpectrogramDataset(trn_ids, trn_labels, folder='filtered_spectrograms', train=True)\n",
        "        val_ds = SpectrogramDataset(val_ids, val_labels, folder='filtered_spectrograms', train=False)\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, drop_last=False)\n",
        "        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "        model = create_efficientnet_b0_offline(num_classes).to(device)\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
        "        scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
        "\n",
        "        # pos_weight per class\n",
        "        y_tr = np.zeros((len(trn_labels), num_classes), dtype=np.float32)\n",
        "        for i, labs in enumerate(trn_labels):\n",
        "            for c in labs: y_tr[i, c] = 1.0\n",
        "        pos = y_tr.sum(axis=0); neg = (y_tr.shape[0] - pos)\n",
        "        pos_weight = np.divide(neg, np.clip(pos, 1.0, None))\n",
        "        pos_weight = np.clip(pos_weight, 1.0, 15.0)\n",
        "        pos_weight_t = torch.tensor(pos_weight, dtype=torch.float32, device=device)\n",
        "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n",
        "\n",
        "        best_auc = -1.0; best_state = None; no_improve = 0\n",
        "        for epoch in range(1, max_epochs+1):\n",
        "            t0 = time.time(); model.train()\n",
        "            running = 0.0; n_batches = 0\n",
        "            for xb, yb in train_loader:\n",
        "                xb = xb.to(device, non_blocking=True); yb = yb.to(device, non_blocking=True)\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "                    logits = model(xb)\n",
        "                    loss = criterion(logits, yb)\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer); scaler.update()\n",
        "                running += loss.item(); n_batches += 1\n",
        "            scheduler.step()\n",
        "            # validate\n",
        "            model.eval()\n",
        "            val_logits = []; val_targets = []\n",
        "            with torch.no_grad():\n",
        "                for xb, yb in val_loader:\n",
        "                    xb = xb.to(device, non_blocking=True)\n",
        "                    logits = model(xb)\n",
        "                    val_logits.append(logits.detach().cpu().float().numpy())\n",
        "                    val_targets.append(yb.numpy())\n",
        "            val_logits = np.concatenate(val_logits, axis=0)\n",
        "            val_targets = np.concatenate(val_targets, axis=0)\n",
        "            val_probs = 1.0 / (1.0 + np.exp(-val_logits))\n",
        "            fold_auc = macro_auc(val_targets, val_probs)\n",
        "            if fold_auc > best_auc + 1e-4:\n",
        "                best_auc = fold_auc; no_improve = 0\n",
        "                best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "            else:\n",
        "                no_improve += 1\n",
        "            print(f\"[CNN] Fold {fold} Epoch {epoch}/{max_epochs} loss={running/max(1,n_batches):.4f} valAUC={fold_auc:.4f} best={best_auc:.4f} time={time.time()-t0:.1f}s\")\n",
        "            if no_improve >= patience:\n",
        "                print(f\"[CNN] Early stop on fold {fold} at epoch {epoch}\")\n",
        "                break\n",
        "        if best_state is not None:\n",
        "            model.load_state_dict(best_state)\n",
        "        # OOF\n",
        "        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "        all_logits = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb = xb.to(device)\n",
        "                logits = model(xb)\n",
        "                all_logits.append(logits.detach().cpu().float().numpy())\n",
        "        all_logits = np.concatenate(all_logits, axis=0)\n",
        "        oof[val_idx] = 1.0 / (1.0 + np.exp(-all_logits))\n",
        "\n",
        "        # TTA on test\n",
        "        def tta_preds():\n",
        "            preds_acc = np.zeros((len(test_ds), num_classes), dtype=np.float32)\n",
        "            with torch.no_grad():\n",
        "                for shift in [-0.1, 0.0, 0.1]:\n",
        "                    for start in range(0, len(test_ds), batch_size):\n",
        "                        end = min(len(test_ds), start+batch_size)\n",
        "                        batch = []\n",
        "                        for i in range(start, end):\n",
        "                            img = load_spectrogram(test_rec_sorted[i], 'filtered_spectrograms')\n",
        "                            if img is None:\n",
        "                                img = load_spectrogram(test_rec_sorted[i], 'spectrograms')\n",
        "                            if img is None:\n",
        "                                img = Image.new('RGB', (IMG_W, IMG_H), color=0)\n",
        "                            x = transforms.ToTensor()(img)\n",
        "                            pixels = int(shift * IMG_W)\n",
        "                            if pixels != 0:\n",
        "                                x = torch.roll(x, shifts=pixels, dims=2)\n",
        "                            x = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])(x)\n",
        "                            batch.append(x)\n",
        "                        xb = torch.stack(batch, dim=0).to(device)\n",
        "                        logits = model(xb).detach().cpu().float().numpy()\n",
        "                        preds_acc[start:end] += logits\n",
        "            preds_acc /= 3.0\n",
        "            return 1.0 / (1.0 + np.exp(-preds_acc))\n",
        "        tpreds = tta_preds()\n",
        "        test_logits_accum += tpreds.astype(np.float32)\n",
        "        print(f\"[CNN] Fold {fold} done in {time.time()-t_fold:.1f}s; bestAUC={best_auc:.4f}\")\n",
        "        del model; gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    test_preds = test_logits_accum / 5.0\n",
        "    oof_auc = macro_auc(Y, oof)\n",
        "    print(f\"[CNN] Filtered spectrograms 5-fold OOF macro AUC: {oof_auc:.4f}\")\n",
        "    id2prob = {}\n",
        "    for i, rid in enumerate(test_rec_sorted):\n",
        "        for c in range(num_classes):\n",
        "            Id = int(rid) * 100 + c\n",
        "            id2prob[Id] = float(test_preds[i, c])\n",
        "    return oof, test_preds, id2prob, oof_auc\n",
        "\n",
        "t0_all = time.time()\n",
        "oof_cnn_filt, test_cnn_filt, id2prob_cnn_filt, oof_auc_cnn = train_cnn_filtered(seed=42, batch_size=32, max_epochs=25, patience=6, lr=3e-4, wd=1e-2)\n",
        "print(f\"[CNN] Finished filtered CNN in {time.time()-t0_all:.1f}s; OOF={oof_auc_cnn:.4f}\")\n",
        "\n",
        "df_base = pd.read_csv('sample_submission.csv')\n",
        "cnn_series = df_base['Id'].map(id2prob_cnn_filt).astype(float)\n",
        "if Path('submission.csv').exists():\n",
        "    df_prev = pd.read_csv('submission.csv')\n",
        "    prev = df_prev['Probability'].astype(float)\n",
        "    blended = 0.7 * cnn_series.fillna(prev) + 0.3 * prev\n",
        "    df_prev['Probability'] = blended.fillna(0.05)\n",
        "    df_prev.to_csv('submission.csv', index=False)\n",
        "    print('[CNN] Blended CNN(filtered) 0.7 with existing submission and saved submission.csv')\n",
        "else:\n",
        "    out = df_base.copy(); out['Probability'] = cnn_series.fillna(0.05)\n",
        "    out.to_csv('submission.csv', index=False)\n",
        "    print('[CNN] Saved CNN(filtered)-only submission.csv')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/iterative_stratification-0.1.9.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tests already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/iterstrat already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scikit_learn-1.7.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sklearn already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scikit_learn.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy-1.16.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/joblib-1.5.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/joblib already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/threadpoolctl-3.6.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/threadpoolctl.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SETUP] Weights already exist at torch_cache/hub/checkpoints/efficientnet_b0_rwightman-3dd342df.pth\n[CNN] EfficientNet-B0 loaded from local cache. Missing: ['classifier.1.weight', 'classifier.1.bias'] Unexpected: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:180: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 Epoch 1/25 loss=1.0700 valAUC=0.4344 best=0.4344 time=24.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 Epoch 2/25 loss=1.0228 valAUC=0.4172 best=0.4344 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 Epoch 3/25 loss=0.9594 valAUC=0.5379 best=0.5379 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 Epoch 4/25 loss=0.8996 valAUC=0.6087 best=0.6087 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 Epoch 5/25 loss=0.8532 valAUC=0.6140 best=0.6140 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 Epoch 6/25 loss=0.8071 valAUC=0.5770 best=0.6140 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 Epoch 7/25 loss=0.7952 valAUC=0.6255 best=0.6255 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 Epoch 8/25 loss=0.7418 valAUC=0.7094 best=0.7094 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 Epoch 9/25 loss=0.7452 valAUC=0.7271 best=0.7271 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 Epoch 10/25 loss=0.6994 valAUC=0.7369 best=0.7369 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 Epoch 11/25 loss=0.7146 valAUC=0.7347 best=0.7369 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 Epoch 12/25 loss=0.6568 valAUC=0.7422 best=0.7422 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 Epoch 13/25 loss=0.6477 valAUC=0.7475 best=0.7475 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 Epoch 14/25 loss=0.6057 valAUC=0.7545 best=0.7545 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 Epoch 15/25 loss=0.6271 valAUC=0.7462 best=0.7545 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 Epoch 16/25 loss=0.6037 valAUC=0.7563 best=0.7563 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 Epoch 17/25 loss=0.5944 valAUC=0.7549 best=0.7563 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 Epoch 18/25 loss=0.5677 valAUC=0.7502 best=0.7563 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 Epoch 19/25 loss=0.5496 valAUC=0.7422 best=0.7563 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 Epoch 20/25 loss=0.5498 valAUC=0.7391 best=0.7563 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 Epoch 21/25 loss=0.5598 valAUC=0.7315 best=0.7563 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 Epoch 22/25 loss=0.5727 valAUC=0.7412 best=0.7563 time=2.5s\n[CNN] Early stop on fold 1 at epoch 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 1 done in 80.6s; bestAUC=0.7563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SETUP] Weights already exist at torch_cache/hub/checkpoints/efficientnet_b0_rwightman-3dd342df.pth\n[CNN] EfficientNet-B0 loaded from local cache. Missing: ['classifier.1.weight', 'classifier.1.bias'] Unexpected: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:180: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 2 Epoch 1/25 loss=1.0741 valAUC=0.4661 best=0.4661 time=13.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 2 Epoch 2/25 loss=1.0245 valAUC=0.5028 best=0.5028 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 2 Epoch 3/25 loss=0.9585 valAUC=0.5699 best=0.5699 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 2 Epoch 4/25 loss=0.8873 valAUC=0.5651 best=0.5699 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 2 Epoch 5/25 loss=0.8961 valAUC=0.6094 best=0.6094 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 2 Epoch 6/25 loss=0.8593 valAUC=0.7133 best=0.7133 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 2 Epoch 7/25 loss=0.8165 valAUC=0.7201 best=0.7201 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 2 Epoch 8/25 loss=0.7840 valAUC=0.7638 best=0.7638 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 2 Epoch 9/25 loss=0.7566 valAUC=0.7297 best=0.7638 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 2 Epoch 10/25 loss=0.7455 valAUC=0.7348 best=0.7638 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 2 Epoch 11/25 loss=0.7379 valAUC=0.7851 best=0.7851 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 2 Epoch 12/25 loss=0.6825 valAUC=0.7652 best=0.7851 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 2 Epoch 13/25 loss=0.6681 valAUC=0.8017 best=0.8017 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 2 Epoch 14/25 loss=0.6666 valAUC=0.7337 best=0.8017 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 2 Epoch 15/25 loss=0.6322 valAUC=0.7278 best=0.8017 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 2 Epoch 16/25 loss=0.6203 valAUC=0.7749 best=0.8017 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 2 Epoch 17/25 loss=0.6180 valAUC=0.7881 best=0.8017 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 2 Epoch 18/25 loss=0.6121 valAUC=0.7771 best=0.8017 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 2 Epoch 19/25 loss=0.6104 valAUC=0.7773 best=0.8017 time=2.5s\n[CNN] Early stop on fold 2 at epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 2 done in 61.8s; bestAUC=0.8017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SETUP] Weights already exist at torch_cache/hub/checkpoints/efficientnet_b0_rwightman-3dd342df.pth\n[CNN] EfficientNet-B0 loaded from local cache. Missing: ['classifier.1.weight', 'classifier.1.bias'] Unexpected: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:180: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 3 Epoch 1/25 loss=1.0667 valAUC=0.4787 best=0.4787 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 3 Epoch 2/25 loss=1.0258 valAUC=0.4583 best=0.4787 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 3 Epoch 3/25 loss=0.9789 valAUC=0.5585 best=0.5585 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 3 Epoch 4/25 loss=0.8953 valAUC=0.7350 best=0.7350 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 3 Epoch 5/25 loss=0.8407 valAUC=0.7814 best=0.7814 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 3 Epoch 6/25 loss=0.8269 valAUC=0.7813 best=0.7814 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 3 Epoch 7/25 loss=0.7771 valAUC=0.7875 best=0.7875 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 3 Epoch 8/25 loss=0.7720 valAUC=0.7944 best=0.7944 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 3 Epoch 9/25 loss=0.7328 valAUC=0.7960 best=0.7960 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 3 Epoch 10/25 loss=0.7310 valAUC=0.7995 best=0.7995 time=2.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 3 Epoch 11/25 loss=0.6667 valAUC=0.7882 best=0.7995 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 3 Epoch 12/25 loss=0.6939 valAUC=0.7287 best=0.7995 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 3 Epoch 13/25 loss=0.6449 valAUC=0.7066 best=0.7995 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 3 Epoch 14/25 loss=0.6035 valAUC=0.6808 best=0.7995 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 3 Epoch 15/25 loss=0.5900 valAUC=0.7170 best=0.7995 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 3 Epoch 16/25 loss=0.5558 valAUC=0.7881 best=0.7995 time=2.6s\n[CNN] Early stop on fold 3 at epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 3 done in 44.5s; bestAUC=0.7995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SETUP] Weights already exist at torch_cache/hub/checkpoints/efficientnet_b0_rwightman-3dd342df.pth\n[CNN] EfficientNet-B0 loaded from local cache. Missing: ['classifier.1.weight', 'classifier.1.bias'] Unexpected: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:180: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 4 Epoch 1/25 loss=1.0673 valAUC=0.4228 best=0.4228 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 4 Epoch 2/25 loss=1.0055 valAUC=0.3788 best=0.4228 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 4 Epoch 3/25 loss=0.9251 valAUC=0.4995 best=0.4995 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 4 Epoch 4/25 loss=0.9058 valAUC=0.5518 best=0.5518 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 4 Epoch 5/25 loss=0.8286 valAUC=0.6103 best=0.6103 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 4 Epoch 6/25 loss=0.7779 valAUC=0.6900 best=0.6900 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 4 Epoch 7/25 loss=0.7628 valAUC=0.7372 best=0.7372 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 4 Epoch 8/25 loss=0.7485 valAUC=0.7597 best=0.7597 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 4 Epoch 9/25 loss=0.6940 valAUC=0.7821 best=0.7821 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 4 Epoch 10/25 loss=0.6791 valAUC=0.7818 best=0.7821 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 4 Epoch 11/25 loss=0.6390 valAUC=0.7875 best=0.7875 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 4 Epoch 12/25 loss=0.6383 valAUC=0.7777 best=0.7875 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 4 Epoch 13/25 loss=0.6008 valAUC=0.8080 best=0.8080 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 4 Epoch 14/25 loss=0.5626 valAUC=0.8122 best=0.8122 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 4 Epoch 15/25 loss=0.5723 valAUC=0.8188 best=0.8188 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 4 Epoch 16/25 loss=0.5508 valAUC=0.7984 best=0.8188 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 4 Epoch 17/25 loss=0.5593 valAUC=0.8031 best=0.8188 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 4 Epoch 18/25 loss=0.5250 valAUC=0.8043 best=0.8188 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 4 Epoch 19/25 loss=0.5524 valAUC=0.8084 best=0.8188 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 4 Epoch 20/25 loss=0.5207 valAUC=0.8017 best=0.8188 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 4 Epoch 21/25 loss=0.5296 valAUC=0.8030 best=0.8188 time=2.6s\n[CNN] Early stop on fold 4 at epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 4 done in 57.2s; bestAUC=0.8188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SETUP] Weights already exist at torch_cache/hub/checkpoints/efficientnet_b0_rwightman-3dd342df.pth\n[CNN] EfficientNet-B0 loaded from local cache. Missing: ['classifier.1.weight', 'classifier.1.bias'] Unexpected: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:180: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 5 Epoch 1/25 loss=1.0579 valAUC=0.4799 best=0.4799 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 5 Epoch 2/25 loss=1.0178 valAUC=0.4199 best=0.4799 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 5 Epoch 3/25 loss=0.9145 valAUC=0.5146 best=0.5146 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 5 Epoch 4/25 loss=0.8652 valAUC=0.6460 best=0.6460 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 5 Epoch 5/25 loss=0.8136 valAUC=0.7388 best=0.7388 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 5 Epoch 6/25 loss=0.8127 valAUC=0.7781 best=0.7781 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 5 Epoch 7/25 loss=0.7679 valAUC=0.7974 best=0.7974 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 5 Epoch 8/25 loss=0.7703 valAUC=0.7903 best=0.7974 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 5 Epoch 9/25 loss=0.7079 valAUC=0.7956 best=0.7974 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 5 Epoch 10/25 loss=0.6746 valAUC=0.8032 best=0.8032 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 5 Epoch 11/25 loss=0.6466 valAUC=0.7996 best=0.8032 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 5 Epoch 12/25 loss=0.6446 valAUC=0.8074 best=0.8074 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 5 Epoch 13/25 loss=0.6084 valAUC=0.8139 best=0.8139 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 5 Epoch 14/25 loss=0.5887 valAUC=0.8469 best=0.8469 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 5 Epoch 15/25 loss=0.5867 valAUC=0.8535 best=0.8535 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 5 Epoch 16/25 loss=0.5472 valAUC=0.8466 best=0.8535 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 5 Epoch 17/25 loss=0.5528 valAUC=0.8482 best=0.8535 time=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 5 Epoch 18/25 loss=0.5166 valAUC=0.8451 best=0.8535 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 5 Epoch 19/25 loss=0.5077 valAUC=0.8449 best=0.8535 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 5 Epoch 20/25 loss=0.5239 valAUC=0.8423 best=0.8535 time=2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2612406714.py:199: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 5 Epoch 21/25 loss=0.5152 valAUC=0.8487 best=0.8535 time=2.6s\n[CNN] Early stop on fold 5 at epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Fold 5 done in 56.3s; bestAUC=0.8535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN] Filtered spectrograms 5-fold OOF macro AUC: 0.7791\n[CNN] Finished filtered CNN in 302.4s; OOF=0.7791\n[CNN] Blended CNN(filtered) 0.7 with existing submission and saved submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "464bd67d-963a-404e-b285-152a3c5cea32",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CNN diagnostics: environment and quick data probe\n",
        "import importlib, torch, timm, os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "print('[DIAG-CNN] torch', torch.__version__, 'cuda?', torch.cuda.is_available(), 'device_count', torch.cuda.device_count())\n",
        "print('[DIAG-CNN] timm version:', importlib.metadata.version('timm') if hasattr(importlib, 'metadata') else 'n/a')\n",
        "try:\n",
        "    print('[DIAG-CNN] CUDA device:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'cpu')\n",
        "except Exception as e:\n",
        "    print('[DIAG-CNN] CUDA device query error:', e)\n",
        "from pathlib import Path\n",
        "ess = Path('essential_data'); supp = Path('supplemental_data')\n",
        "print('[DIAG-CNN] filtered_spectrograms exists?', (supp / 'filtered_spectrograms').exists())\n",
        "import pandas as pd\n",
        "df_id2fn = pd.read_csv(ess / 'rec_id2filename.txt')\n",
        "id2fn = dict(zip(df_id2fn.rec_id.astype(int), df_id2fn.filename.astype(str)))\n",
        "sample_rec = list(id2fn.keys())[0]\n",
        "p = supp / 'filtered_spectrograms' / f\"{id2fn[sample_rec]}.bmp\"\n",
        "print('[DIAG-CNN] sample image path:', p, 'exists?', p.exists())\n",
        "try:\n",
        "    img = Image.open(p).convert('L') if p.exists() else None\n",
        "    print('[DIAG-CNN] image size:', img.size if img else None)\n",
        "except Exception as e:\n",
        "    print('[DIAG-CNN] PIL open error:', e)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DIAG-CNN] torch 2.8.0+cu128 cuda? True device_count 1\n[DIAG-CNN] timm version: 1.0.19\n[DIAG-CNN] CUDA device: Tesla T4\n[DIAG-CNN] filtered_spectrograms exists? True\n[DIAG-CNN] sample image path: supplemental_data/filtered_spectrograms/PC1_20090606_050012_0010.bmp exists? True\n[DIAG-CNN] image size: (1246, 256)\n"
          ]
        }
      ]
    },
    {
      "id": "d2736cbc-d78a-407e-9dee-84fac46a1540",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick CNN OOF diagnostics and submission sanity\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "try:\n",
        "    print('[CHECK] CNN OOF macro AUC:', round(float(oof_auc_cnn), 6))\n",
        "    print('[CHECK] OOF shape:', getattr(oof_cnn_filt, 'shape', None), 'Test preds shape:', getattr(test_cnn_filt, 'shape', None))\n",
        "    # Optional per-class AUCs\n",
        "    Y = np.zeros_like(oof_cnn_filt, dtype=np.float32)\n",
        "    for i, labs in enumerate(df_train_ids['labels']):\n",
        "        for c in labs: Y[i, c] = 1.0\n",
        "    aucs = []\n",
        "    for c in range(Y.shape[1]):\n",
        "        yc = Y[:, c]\n",
        "        if yc.sum() > 0 and (len(yc) - yc.sum()) > 0:\n",
        "            try: aucs.append(roc_auc_score(yc, oof_cnn_filt[:, c]))\n",
        "            except Exception: pass\n",
        "    print('[CHECK] CNN OOF per-class macro across', len(aucs), 'classes:', round(float(np.mean(aucs)), 6))\n",
        "    # Submission sanity\n",
        "    df_sub = pd.read_csv('submission.csv')\n",
        "    print('[CHECK] submission.csv rows:', df_sub.shape, 'NaNs:', int(df_sub['Probability'].isna().sum()))\n",
        "    print(df_sub.head())\n",
        "except NameError as e:\n",
        "    print('[CHECK] CNN variables not found:', e)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CHECK] CNN OOF macro AUC: 0.753191\n[CHECK] OOF shape: (258, 19) Test preds shape: (64, 19)\n[CHECK] CNN OOF per-class macro across 19 classes: 0.753191\n[CHECK] submission.csv rows: (1216, 2) NaNs: 0\n    Id  Probability\n0  100     0.171752\n1  101     0.176060\n2  102     0.166035\n3  103     0.125629\n4  104     0.106490\n"
          ]
        }
      ]
    },
    {
      "id": "fcee2af7-4e19-4e46-aeb3-06e9f86723b2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CNN Model A: ConvNeXt-Tiny @ 256x768 on filtered spectrograms with warmup+cosine, strong SpecAug, per-fold OOF and TTA\n",
        "import sys, subprocess, importlib, os, time, math, random, gc, urllib.request\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "ess = Path('essential_data'); supp = Path('supplemental_data')\n",
        "\n",
        "# Globals from earlier cells\n",
        "species_df = pd.read_csv(ess / 'species_list.txt')\n",
        "num_classes = species_df.shape[0]\n",
        "df_id2fn = pd.read_csv(ess / 'rec_id2filename.txt')\n",
        "id2fn = dict(zip(df_id2fn.rec_id.astype(int), df_id2fn.filename.astype(str)))\n",
        "\n",
        "# Input resolution per expert guidance\n",
        "IMG_H, IMG_W = 256, 768\n",
        "\n",
        "def macro_auc(y_true, y_prob):\n",
        "    aucs = []\n",
        "    for c in range(y_true.shape[1]):\n",
        "        yc = y_true[:, c]\n",
        "        if yc.sum() > 0 and (len(yc) - yc.sum()) > 0:\n",
        "            try: aucs.append(roc_auc_score(yc, y_prob[:, c]))\n",
        "            except Exception: pass\n",
        "    return float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "\n",
        "def load_resized(rec_id: int, folder='filtered_spectrograms'):\n",
        "    fn = id2fn.get(int(rec_id), None)\n",
        "    if fn is None: return None\n",
        "    p = supp / folder / f\"{fn}.bmp\"\n",
        "    if not p.exists():\n",
        "        return None\n",
        "    try:\n",
        "        img = Image.open(p).convert('L')\n",
        "    except Exception:\n",
        "        return None\n",
        "    w, h = img.size\n",
        "    new_w = int(round(w * (IMG_H / h)))\n",
        "    img_resized = img.resize((new_w, IMG_H), Image.BILINEAR)\n",
        "    return img_resized  # grayscale, height fixed, width variable\n",
        "\n",
        "class SpecAugment:\n",
        "    def __init__(self, time_masks=3, time_max=120, freq_masks=2, freq_max=60):\n",
        "        self.time_masks=time_masks; self.time_max=time_max; self.freq_masks=freq_masks; self.freq_max=freq_max\n",
        "    def __call__(self, x):\n",
        "        C,H,W = x.shape\n",
        "        for _ in range(self.time_masks):\n",
        "            w = random.randint(0, self.time_max)\n",
        "            if w>0:\n",
        "                t0 = random.randint(0, max(0, W - w))\n",
        "                x[:, :, t0:t0+w] = 0.0\n",
        "        for _ in range(self.freq_masks):\n",
        "            h = random.randint(0, self.freq_max)\n",
        "            if h>0:\n",
        "                f0 = random.randint(0, max(0, H - h))\n",
        "                x[:, f0:f0+h, :] = 0.0\n",
        "        return x\n",
        "\n",
        "class SpectrogramDatasetCnxt(Dataset):\n",
        "    def __init__(self, rec_ids, labels=None, folder='filtered_spectrograms', train=True):\n",
        "        self.rec_ids = list(rec_ids); self.labels = labels\n",
        "        self.folder=folder; self.train=train\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "        self.specaug = SpecAugment()\n",
        "        self.norm = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "        self.roll_px = int(0.15 * IMG_W) if train else 0\n",
        "    def __len__(self):\n",
        "        return len(self.rec_ids)\n",
        "    def _time_crop(self, img_resized):\n",
        "        # img_resized: grayscale PIL with H=IMG_H, width >= 1\n",
        "        w = img_resized.size[0]\n",
        "        if w >= IMG_W:\n",
        "            if self.train:\n",
        "                start = random.randint(0, w - IMG_W)\n",
        "            else:\n",
        "                start = (w - IMG_W)//2\n",
        "            crop = img_resized.crop((start, 0, start+IMG_W, IMG_H))\n",
        "        else:\n",
        "            pad_left = (IMG_W - w)//2\n",
        "            pad_right = IMG_W - w - pad_left\n",
        "            canvas = Image.new('L', (IMG_W, IMG_H), color=0)\n",
        "            canvas.paste(img_resized, (pad_left, 0))\n",
        "            crop = canvas\n",
        "        img3 = Image.merge('RGB', (crop, crop, crop))\n",
        "        return img3\n",
        "    def __getitem__(self, idx):\n",
        "        rid = int(self.rec_ids[idx])\n",
        "        img_resized = load_resized(rid, self.folder)\n",
        "        if img_resized is None:\n",
        "            alt = load_resized(rid, 'spectrograms')\n",
        "            img_resized = alt if alt is not None else Image.new('L', (IMG_W, IMG_H), color=0)\n",
        "        img = self._time_crop(img_resized)\n",
        "        x = self.to_tensor(img)\n",
        "        if self.train:\n",
        "            if self.roll_px>0 and random.random()<0.8:\n",
        "                shift = random.randint(-self.roll_px, self.roll_px)\n",
        "                if shift!=0: x = torch.roll(x, shifts=shift, dims=2)\n",
        "            x = self.specaug(x)\n",
        "        x = self.norm(x)\n",
        "        if self.labels is None:\n",
        "            return x, rid\n",
        "        y = torch.zeros(num_classes, dtype=torch.float32)\n",
        "        for c in self.labels[idx]: y[c] = 1.0\n",
        "        return x, y\n",
        "\n",
        "# Offline weights for ConvNeXt-Tiny\n",
        "CACHE_DIR = Path('./torch_cache'); os.environ['TORCH_HOME'] = str(CACHE_DIR.resolve())\n",
        "CKPT_DIR = CACHE_DIR / 'hub' / 'checkpoints'; CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "CNXT_URL = 'https://download.pytorch.org/models/convnext_tiny-983f1562.pth'\n",
        "CNXT_FILE = CKPT_DIR / 'convnext_tiny-983f1562.pth'\n",
        "\n",
        "def ensure_file(url, path: Path):\n",
        "    if not path.exists():\n",
        "        print(f\"[SETUP] Downloading weights to {path}\")\n",
        "        urllib.request.urlretrieve(url, path)\n",
        "    else:\n",
        "        print(f\"[SETUP] Weights already exist at {path}\")\n",
        "\n",
        "def create_convnext_tiny_offline(num_classes):\n",
        "    ensure_file(CNXT_URL, CNXT_FILE)\n",
        "    # Build convnext_tiny manually to avoid internet\n",
        "    from torchvision.models.convnext import convnext_tiny, ConvNeXt_Tiny_Weights\n",
        "    model = convnext_tiny(weights=None)\n",
        "    sd = torch.load(CNXT_FILE, map_location='cpu')\n",
        "    missing, unexpected = model.load_state_dict(sd, strict=False)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, num_classes)\n",
        "    print('[CNN-A] ConvNeXt-Tiny loaded. Missing:', missing, 'Unexpected:', unexpected)\n",
        "    return model\n",
        "\n",
        "def linear_warmup_cosine(optimizer, warmup_steps, total_steps, min_lr=1e-6):\n",
        "    def lr_lambda(step):\n",
        "        if step < warmup_steps:\n",
        "            return (step + 1) / max(1, warmup_steps)\n",
        "        progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
        "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "def train_convnext_filtered(seed=42, batch_size=24, max_epochs=35, patience=9, base_lr=1e-4, wd=1e-2):\n",
        "    torch.manual_seed(seed); np.random.seed(seed); random.seed(seed)\n",
        "    train_df = df_train_ids[['rec_id','labels']].copy().reset_index(drop=True)\n",
        "    rec_ids = train_df['rec_id'].tolist()\n",
        "    labels = train_df['labels'].tolist()\n",
        "    Y = np.zeros((len(labels), num_classes), dtype=np.float32)\n",
        "    for i,labs in enumerate(labels):\n",
        "        for c in labs: Y[i,c]=1.0\n",
        "    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "    oof = np.zeros_like(Y, dtype=np.float32)\n",
        "    test_rec_sorted = sorted(df_test_ids['rec_id'].tolist())\n",
        "    test_ds = SpectrogramDatasetCnxt(test_rec_sorted, labels=None, folder='filtered_spectrograms', train=False)\n",
        "    test_logits_accum = np.zeros((len(test_ds), num_classes), dtype=np.float32)\n",
        "\n",
        "    for fold, (trn_idx, val_idx) in enumerate(mskf.split(np.arange(len(rec_ids)), Y), 1):\n",
        "        t_fold = time.time()\n",
        "        trn_ids = [rec_ids[i] for i in trn_idx]; trn_labels = [labels[i] for i in trn_idx]\n",
        "        val_ids = [rec_ids[i] for i in val_idx]; val_labels = [labels[i] for i in val_idx]\n",
        "        train_ds = SpectrogramDatasetCnxt(trn_ids, trn_labels, train=True)\n",
        "        val_ds = SpectrogramDatasetCnxt(val_ids, val_labels, train=False)\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "        model = create_convnext_tiny_offline(num_classes).to(device)\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, weight_decay=wd)\n",
        "        total_steps = max_epochs * max(1, math.ceil(len(train_loader)))\n",
        "        warmup_steps = max(1, int(0.05 * total_steps))  # ~1-2 epochs equivalent\n",
        "        scheduler = linear_warmup_cosine(optimizer, warmup_steps, total_steps)\n",
        "        scaler = torch.amp.GradScaler('cuda', enabled=(device.type=='cuda'))\n",
        "\n",
        "        # pos_weight cap at 12\n",
        "        y_tr = np.zeros((len(trn_labels), num_classes), dtype=np.float32)\n",
        "        for i,labs in enumerate(trn_labels):\n",
        "            for c in labs: y_tr[i,c]=1.0\n",
        "        pos = y_tr.sum(axis=0); neg = (y_tr.shape[0]-pos)\n",
        "        pos_weight = np.clip(np.divide(neg, np.clip(pos, 1.0, None)), 1.0, 12.0)\n",
        "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight, dtype=torch.float32, device=device))\n",
        "\n",
        "        best_auc = -1.0; best_state = None; no_imp = 0; step=0\n",
        "        for epoch in range(1, max_epochs+1):\n",
        "            t0 = time.time(); model.train(); running=0.0; nb=0\n",
        "            for xb, yb in train_loader:\n",
        "                xb = xb.to(device, non_blocking=True); yb = yb.to(device, non_blocking=True)\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):\n",
        "                    logits = model(xb)\n",
        "                    loss = criterion(logits, yb)\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer); scaler.update()\n",
        "                scheduler.step(); step+=1; running += float(loss.item()); nb+=1\n",
        "            # validate\n",
        "            model.eval(); v_logits=[]; v_targets=[]\n",
        "            with torch.no_grad():\n",
        "                for xb, yb in val_loader:\n",
        "                    xb = xb.to(device, non_blocking=True)\n",
        "                    v = model(xb).detach().cpu().float().numpy()\n",
        "                    v_logits.append(v); v_targets.append(yb.numpy())\n",
        "            v_logits = np.concatenate(v_logits, 0); v_targets = np.concatenate(v_targets, 0)\n",
        "            v_probs = 1.0 / (1.0 + np.exp(-v_logits))\n",
        "            v_auc = macro_auc(v_targets, v_probs)\n",
        "            if v_auc > best_auc + 1e-4:\n",
        "                best_auc = v_auc; no_imp = 0\n",
        "                best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "            else:\n",
        "                no_imp += 1\n",
        "            print(f\"[CNN-A] Fold {fold} Epoch {epoch}/{max_epochs} loss={running/max(1,nb):.4f} valAUC={v_auc:.4f} best={best_auc:.4f} time={time.time()-t0:.1f}s\")\n",
        "            if no_imp >= patience:\n",
        "                print(f\"[CNN-A] Early stop on fold {fold} at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "        if best_state is not None:\n",
        "            model.load_state_dict(best_state)\n",
        "\n",
        "        # OOF for this fold\n",
        "        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "        outs=[]\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb = xb.to(device, non_blocking=True)\n",
        "                outs.append(model(xb).detach().cpu().float().numpy())\n",
        "        outs = np.concatenate(outs, 0)\n",
        "        oof[val_idx] = 1.0/(1.0+np.exp(-outs))\n",
        "\n",
        "        # TTA logits average with 5 time shifts\n",
        "        def tta_test_logits():\n",
        "            preds = np.zeros((len(test_ds), num_classes), dtype=np.float32)\n",
        "            shifts = [-0.2, -0.1, 0.0, 0.1, 0.2]\n",
        "            with torch.no_grad():\n",
        "                for sh in shifts:\n",
        "                    cur_batch=[]; cur_idx=[]\n",
        "                    for i in range(len(test_ds)):\n",
        "                        # manual fetch to apply roll\n",
        "                        rid = test_ds.rec_ids[i]\n",
        "                        img_resized = load_resized(rid, 'filtered_spectrograms')\n",
        "                        if img_resized is None:\n",
        "                            alt = load_resized(rid, 'spectrograms')\n",
        "                            img_resized = alt if alt is not None else Image.new('L', (IMG_W, IMG_H), color=0)\n",
        "                        img = test_ds._time_crop(img_resized)\n",
        "                        x = transforms.ToTensor()(img)\n",
        "                        px = int(sh * IMG_W)\n",
        "                        if px != 0:\n",
        "                            x = torch.roll(x, shifts=px, dims=2)\n",
        "                        x = test_ds.norm(x)\n",
        "                        cur_batch.append(x); cur_idx.append(i)\n",
        "                        if len(cur_batch) == batch_size or i == len(test_ds)-1:\n",
        "                            xb = torch.stack(cur_batch,0).to(device)\n",
        "                            logits = model(xb).detach().cpu().float().numpy()\n",
        "                            preds[cur_idx[0]:cur_idx[0]+len(cur_batch)] += logits\n",
        "                            cur_batch=[]; cur_idx=[]\n",
        "            preds /= len(shifts)\n",
        "            return 1.0/(1.0+np.exp(-preds))\n",
        "\n",
        "        tp = tta_test_logits()\n",
        "        test_logits_accum += tp.astype(np.float32)\n",
        "        print(f\"[CNN-A] Fold {fold} done in {time.time()-t_fold:.1f}s; bestAUC={best_auc:.4f}\")\n",
        "        del model; gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    test_preds = test_logits_accum / 5.0\n",
        "    oof_auc = macro_auc(Y, oof)\n",
        "    print(f\"[CNN-A] ConvNeXt-Tiny filtered 5-fold OOF macro AUC: {oof_auc:.4f}\")\n",
        "    id2prob = {}\n",
        "    for i, rid in enumerate(test_ds.rec_ids):\n",
        "        for c in range(num_classes):\n",
        "            Id = int(rid) * 100 + c\n",
        "            id2prob[Id] = float(test_preds[i, c])\n",
        "    return oof, test_preds, id2prob, oof_auc\n",
        "\n",
        "t0 = time.time()\n",
        "oof_cnxt_filt, test_cnxt_filt, id2prob_cnxt_filt, oof_auc_cnxt = train_convnext_filtered(seed=42, batch_size=24, max_epochs=35, patience=9, base_lr=1e-4, wd=1e-2)\n",
        "print(f\"[CNN-A] Finished ConvNeXt-Tiny filtered in {time.time()-t0:.1f}s; OOF={oof_auc_cnxt:.4f}\")\n",
        "\n",
        "# Blend this model into submission conservatively (logit/prob avg) to keep LB stable; will switch to per-class later\n",
        "df_base = pd.read_csv('sample_submission.csv')\n",
        "cnxt_series = df_base['Id'].map(id2prob_cnxt_filt).astype(float)\n",
        "if Path('submission.csv').exists():\n",
        "    df_prev = pd.read_csv('submission.csv')\n",
        "    prev = df_prev['Probability'].astype(float)\n",
        "    df_prev['Probability'] = (0.7 * cnxt_series.fillna(prev) + 0.3 * prev).fillna(0.05)\n",
        "    df_prev.to_csv('submission.csv', index=False)\n",
        "    print('[CNN-A] Blended ConvNeXt-Tiny(filtered) 0.7 with existing submission and saved submission.csv')\n",
        "else:\n",
        "    out = df_base.copy(); out['Probability'] = cnxt_series.fillna(0.05); out.to_csv('submission.csv', index=False)\n",
        "    print('[CNN-A] Saved ConvNeXt-Tiny(filtered)-only submission.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SETUP] Weights already exist at torch_cache/hub/checkpoints/convnext_tiny-983f1562.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN-A] ConvNeXt-Tiny loaded. Missing: [] Unexpected: []\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}