{
  "cells": [
    {
      "id": "3289910c-62db-4307-aa3e-1e517e442fc1",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan to Medal: MLSP 2013 Birds (AUC-ROC, multilabel)\n",
        "\n",
        "Objectives:\n",
        "- Build a strong baseline fast using provided spectrogram artifacts (BMPs) to avoid audio preprocessing overhead.\n",
        "- Use multilabel image classifier (e.g., EfficientNet/BiT/ResNet) with sigmoid BCE, stratified CV per CVfolds_2.txt.\n",
        "- Generate test predictions and create submission.csv.\n",
        "\n",
        "Data understanding tasks:\n",
        "1) Parse species_list.txt (class order).\n",
        "2) Parse rec_id2filename.txt (recording IDs, mapping to wav/bmp names).\n",
        "3) Parse CVfolds_2.txt (train/val split IDs).\n",
        "4) Determine train/test split using rec_labels_test_hidden.txt and sample_submission.csv.\n",
        "5) Confirm image availability in supplemental_data/spectrograms or filtered_spectrograms.\n",
        "\n",
        "Baseline modeling plan:\n",
        "- Input: 2D grayscale BMP spectrograms (use both spectrograms/ and filtered_spectrograms/ experiments later).\n",
        "- Model: timm EfficientNet-B0 or ResNet50 with pretrained ImageNet, single channel replicated to 3 channels.\n",
        "- Loss: BCEWithLogitsLoss; Metrics: micro/macro AUC on CV.\n",
        "- Augmentations: light (RandomResizedCrop, HorizontalFlip=FALSE, VerticalFlip=maybe; keep time-axis horizontal; do time masking later if needed).\n",
        "- Optimizer: AdamW; LR: 1e-3 with cosine or OneCycle; epochs: 10-20 (early stop).\n",
        "- Batch size: fit GPU (T4 16GB) -> start 32.\n",
        "- CV: use CVfolds_2.txt (2-fold) to iterate quickly; log per-fold AUC and time.\n",
        "\n",
        "Efficiency:\n",
        "- Cache dataset indices; use WebDataset-like loader not needed; simple ImageFolder-like custom dataset.\n",
        "- Use mixed precision (AMP) and cudnn.benchmark=True.\n",
        "- Log progress every N steps; estimate ETA.\n",
        "\n",
        "Milestones:\n",
        "A) Data loading + label matrix built.\n",
        "B) Sanity-check a small model overfit on tiny subset.\n",
        "C) Full 2-fold training run; evaluate CV AUC.\n",
        "D) Inference on test; write submission.csv.\n",
        "\n",
        "Expert checkpoints:\n",
        "- After this plan.\n",
        "- After data parsing/EDA.\n",
        "- After first baseline CV results (decide on filtered vs raw spectrograms, augmentations, thresholds).\n",
        "- Before long training runs or ensembling.\n",
        "\n",
        "Stretch improvements (time permitting):\n",
        "- Try filtered_spectrograms vs spectrograms; ensemble logits.\n",
        "- Add mixup/cutmix (weak for multilabel images but can help).\n",
        "- Fine-tune a stronger backbone (tf_efficientnet_b3_ns) and TTA (center + horizontal crop).\n",
        "- Post-processing: none required for AUC; consider calibration if needed.\n",
        "\n",
        "Next step:\n",
        "- Implement data parsing notebook cells and verify counts and shapes."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "75da090e-2a3e-4cfe-9999-58b2636b99d4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Data parsing and integrity checks (revised for actual file formats)\n",
        "import os, sys, json, time, math, random, re, gc, hashlib\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "BASE = Path('.')\n",
        "ESS = BASE/'essential_data'\n",
        "SUP = BASE/'supplemental_data'\n",
        "\n",
        "def log(s):\n",
        "    print(f\"[{time.strftime('%H:%M:%S')}] {s}\")\n",
        "\n",
        "# 1) species_list.txt (CSV with columns: class_id, code, species)\n",
        "sp_df = pd.read_csv(ESS/'species_list.txt')\n",
        "assert {'class_id','code','species'}.issubset(sp_df.columns), 'species_list.txt must have class_id,code,species'\n",
        "sp_df = sp_df.sort_values('class_id').reset_index(drop=True)\n",
        "num_classes = sp_df.shape[0]\n",
        "species_codes = sp_df['code'].tolist()\n",
        "species_names = sp_df['species'].tolist()\n",
        "log(f\"Loaded species_list: {num_classes} classes -> {species_codes[:5]} ...\")\n",
        "\n",
        "# 2) rec_id2filename.txt (CSV with header: rec_id, filename)\n",
        "rid2fn_df = pd.read_csv(ESS/'rec_id2filename.txt')\n",
        "assert {'rec_id','filename'}.issubset(rid2fn_df.columns), 'rec_id2filename.txt must have rec_id,filename'\n",
        "rid2fn_df['rec_id'] = rid2fn_df['rec_id'].astype(int)\n",
        "id2fn = dict(zip(rid2fn_df['rec_id'], rid2fn_df['filename']))\n",
        "log(f\"Loaded rec_id2filename: {len(id2fn)} mappings\")\n",
        "\n",
        "# 3) rec_labels_test_hidden.txt -> parse train labels and identify hidden test\n",
        "# Format: header 'rec_id,[labels]'; rows like '0,11,12' or '1,?' or '3' (no labels) etc.\n",
        "train_labels = {}  # rec_id -> set(class_ids)\n",
        "all_ids_in_labels_file = []\n",
        "with open(ESS/'rec_labels_test_hidden.txt', 'r') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        if i == 0 and line.lower().startswith('rec_id'):\n",
        "            continue  # header\n",
        "        parts = [p.strip() for p in line.split(',') if p.strip()!='']\n",
        "        if len(parts) == 0:\n",
        "            continue\n",
        "        rec_id = int(parts[0])\n",
        "        all_ids_in_labels_file.append(rec_id)\n",
        "        if len(parts) == 1:\n",
        "            continue  # hidden test row with no labels\n",
        "        if parts[1] == '?' or parts[1] == '[labels]':\n",
        "            continue  # hidden test indicator\n",
        "        # remaining parts are class_ids\n",
        "        class_ids = []\n",
        "        for tok in parts[1:]:\n",
        "            tok = tok.strip().strip('[]')\n",
        "            if tok == '?' or tok == '':\n",
        "                continue\n",
        "            try:\n",
        "                cid = int(tok)\n",
        "            except:\n",
        "                continue\n",
        "            class_ids.append(cid)\n",
        "        if len(class_ids) > 0:\n",
        "            train_labels[rec_id] = set(class_ids)\n",
        "\n",
        "all_ids_in_labels_file = pd.Index(sorted(set(all_ids_in_labels_file)))\n",
        "train_ids = pd.Index(sorted(train_labels.keys()))\n",
        "hidden_test_ids = all_ids_in_labels_file.difference(train_ids)\n",
        "log(f\"Parsed labels: total IDs listed={len(all_ids_in_labels_file)}, train IDs with labels={len(train_ids)}, hidden test IDs={len(hidden_test_ids)}\")\n",
        "\n",
        "# 4) CVfolds_2.txt (CSV with columns: rec_id, fold)\n",
        "cv_df = pd.read_csv(ESS/'CVfolds_2.txt')\n",
        "assert {'rec_id','fold'}.issubset(cv_df.columns), 'CVfolds_2.txt must have rec_id,fold'\n",
        "cv_df['rec_id'] = cv_df['rec_id'].astype(int)\n",
        "cv_df['fold'] = cv_df['fold'].astype(int)\n",
        "fold_map = dict(zip(cv_df['rec_id'], cv_df['fold']))\n",
        "log(f\"Loaded CVfolds_2: {len(fold_map)} entries, folds={sorted(cv_df['fold'].unique().tolist())}\")\n",
        "\n",
        "# 5) sample_submission.csv -> Id,Probability with Id encoding rec_id*100 + class_id\n",
        "sub_df = pd.read_csv('sample_submission.csv')\n",
        "assert {'Id','Probability'}.issubset(sub_df.columns), 'sample_submission must have Id,Probability columns'\n",
        "sub_df['rec_id'] = (sub_df['Id'] // 100).astype(int)\n",
        "sub_df['class_id'] = (sub_df['Id'] % 100).astype(int)\n",
        "assert sub_df['class_id'].between(0, num_classes-1).all(), 'class_id out of range in sample_submission'\n",
        "submission_test_ids = pd.Index(sorted(sub_df['rec_id'].unique()))\n",
        "log(f\"sample_submission: rows={len(sub_df)}, unique test rec_ids={len(submission_test_ids)}\")\n",
        "\n",
        "# Determine train vs test sets\n",
        "train_rec_ids = train_ids\n",
        "test_rec_ids = submission_test_ids\n",
        "leak_ids = train_rec_ids.intersection(test_rec_ids)\n",
        "assert len(leak_ids) == 0, f\"Leakage: {len(leak_ids)} rec_ids appear in both train and test\"\n",
        "\n",
        "# Build label matrix for train (multilabel one-hot over class_ids 0..num_classes-1)\n",
        "y_list = []\n",
        "train_rows = []\n",
        "for rid in train_rec_ids:\n",
        "    labs = train_labels.get(rid, set())\n",
        "    y = np.zeros(num_classes, dtype=np.float32)\n",
        "    for cid in labs:\n",
        "        if 0 <= cid < num_classes:\n",
        "            y[cid] = 1.0\n",
        "    train_rows.append({'rec_id': rid})\n",
        "    y_list.append(y)\n",
        "Y = np.vstack(y_list) if len(y_list) else np.zeros((0, num_classes), dtype=np.float32)\n",
        "train_df = pd.DataFrame(train_rows)\n",
        "log(f\"Train matrix: n={len(train_df)}, num_classes={Y.shape[1]}, positive labels={int(Y.sum())}\")\n",
        "\n",
        "# Attach fold assignments\n",
        "train_df['fold'] = train_df['rec_id'].map(fold_map).astype('Int64')\n",
        "if train_df['fold'].isna().any():\n",
        "    miss = train_df[train_df['fold'].isna()]['rec_id'].tolist()[:10]\n",
        "    log(f\"WARNING: {train_df['fold'].isna().sum()} train rec_ids missing CV fold mapping. Examples: {miss}\")\n",
        "    train_df['fold'] = train_df['fold'].fillna(-1).astype(int)\n",
        "else:\n",
        "    train_df['fold'] = train_df['fold'].astype(int)\n",
        "\n",
        "# Map rec_id -> filename stems\n",
        "train_df['filename'] = train_df['rec_id'].map(id2fn)\n",
        "test_df = sub_df[['Id','rec_id','class_id']].copy()\n",
        "test_df['filename'] = test_df['rec_id'].map(id2fn)\n",
        "\n",
        "# Choose input view: filtered_spectrograms\n",
        "VIEW_DIR = SUP/'filtered_spectrograms'\n",
        "def bmp_path(stem):\n",
        "    return VIEW_DIR/f\"{stem}.bmp\" if isinstance(stem, str) else None\n",
        "train_df['bmp'] = train_df['filename'].map(bmp_path)\n",
        "test_df['bmp'] = test_df['filename'].map(bmp_path)\n",
        "\n",
        "# Assert files exist\n",
        "missing_train = train_df[~train_df['bmp'].map(lambda p: p is not None and p.exists())]\n",
        "missing_test = test_df[~test_df['bmp'].map(lambda p: p is not None and p.exists())]\n",
        "log(f\"Missing BMPs -> train: {len(missing_train)}, test rows: {len(missing_test)} (note: test_df has multiple rows per rec_id)\")\n",
        "if len(missing_train) > 0:\n",
        "    log(f\"Example missing train: {missing_train.head(3).to_dict(orient='records')}\")\n",
        "if len(missing_test) > 0:\n",
        "    log(f\"Example missing test: {missing_test.head(3).to_dict(orient='records')}\")\n",
        "\n",
        "# Store artifacts for later cells\n",
        "data_contract = {\n",
        "    'num_classes': int(num_classes),\n",
        "    'species_codes': species_codes,\n",
        "    'species_names': species_names,\n",
        "    'train_df_shape': tuple(train_df.shape),\n",
        "    'test_df_shape': tuple(test_df.shape),\n",
        "    'view_dir': str(VIEW_DIR),\n",
        "}\n",
        "log(json.dumps(data_contract)[:300] + ('...' if len(json.dumps(data_contract))>300 else ''))\n",
        "\n",
        "# Preview\n",
        "display(train_df.head())\n",
        "display(test_df.head())\n",
        "pos_per_class = Y.sum(axis=0)\n",
        "log(f\"Classes with zero positives in train: {(pos_per_class==0).sum()} / {num_classes}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04:45:46] Loaded species_list: 19 classes -> ['BRCR', 'PAWR', 'PSFL', 'RBNU', 'DEJU'] ...\n[04:45:46] Loaded rec_id2filename: 322 mappings\n[04:45:46] Parsed labels: total IDs listed=322, train IDs with labels=145, hidden test IDs=177\n[04:45:46] Loaded CVfolds_2: 322 entries, folds=[0, 1]\n[04:45:46] sample_submission: rows=1216, unique test rec_ids=64\n[04:45:46] Train matrix: n=145, num_classes=19, positive labels=270\n[04:45:46] Missing BMPs -> train: 0, test rows: 0 (note: test_df has multiple rows per rec_id)\n[04:45:46] {\"num_classes\": 19, \"species_codes\": [\"BRCR\", \"PAWR\", \"PSFL\", \"RBNU\", \"DEJU\", \"OSFL\", \"HETH\", \"CBCH\", \"VATH\", \"HEWA\", \"SWTH\", \"HAFL\", \"WETA\", \"BHGB\", \"GCKI\", \"WAVI\", \"MGWA\", \"STJA\", \"CONI\"], \"species_names\": [\"Brown Creeper\", \"Pacific Wren\", \"Pacific-slope Flycatcher\", \"Red-breasted Nuthatch\", \"Dark...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "   rec_id  fold                  filename  \\\n0       0     0  PC1_20090606_050012_0010   \n1       2     0  PC1_20090705_070000_0010   \n2      10     0  PC2_20090606_050000_0010   \n3      11     0  PC2_20090606_070000_0010   \n4      19     0  PC4_20100606_050000_0010   \n\n                                                 bmp  \n0  supplemental_data/filtered_spectrograms/PC1_20...  \n1  supplemental_data/filtered_spectrograms/PC1_20...  \n2  supplemental_data/filtered_spectrograms/PC2_20...  \n3  supplemental_data/filtered_spectrograms/PC2_20...  \n4  supplemental_data/filtered_spectrograms/PC4_20...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rec_id</th>\n      <th>fold</th>\n      <th>filename</th>\n      <th>bmp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>PC1_20090606_050012_0010</td>\n      <td>supplemental_data/filtered_spectrograms/PC1_20...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n      <td>PC1_20090705_070000_0010</td>\n      <td>supplemental_data/filtered_spectrograms/PC1_20...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10</td>\n      <td>0</td>\n      <td>PC2_20090606_050000_0010</td>\n      <td>supplemental_data/filtered_spectrograms/PC2_20...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11</td>\n      <td>0</td>\n      <td>PC2_20090606_070000_0010</td>\n      <td>supplemental_data/filtered_spectrograms/PC2_20...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>19</td>\n      <td>0</td>\n      <td>PC4_20100606_050000_0010</td>\n      <td>supplemental_data/filtered_spectrograms/PC4_20...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  rec_id  class_id                  filename  \\\n0  100       1         0  PC1_20090606_070012_0010   \n1  101       1         1  PC1_20090606_070012_0010   \n2  102       1         2  PC1_20090606_070012_0010   \n3  103       1         3  PC1_20090606_070012_0010   \n4  104       1         4  PC1_20090606_070012_0010   \n\n                                                 bmp  \n0  supplemental_data/filtered_spectrograms/PC1_20...  \n1  supplemental_data/filtered_spectrograms/PC1_20...  \n2  supplemental_data/filtered_spectrograms/PC1_20...  \n3  supplemental_data/filtered_spectrograms/PC1_20...  \n4  supplemental_data/filtered_spectrograms/PC1_20...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>rec_id</th>\n      <th>class_id</th>\n      <th>filename</th>\n      <th>bmp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC1_20090606_070012_0010</td>\n      <td>supplemental_data/filtered_spectrograms/PC1_20...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>1</td>\n      <td>1</td>\n      <td>PC1_20090606_070012_0010</td>\n      <td>supplemental_data/filtered_spectrograms/PC1_20...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>1</td>\n      <td>2</td>\n      <td>PC1_20090606_070012_0010</td>\n      <td>supplemental_data/filtered_spectrograms/PC1_20...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>1</td>\n      <td>3</td>\n      <td>PC1_20090606_070012_0010</td>\n      <td>supplemental_data/filtered_spectrograms/PC1_20...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>1</td>\n      <td>4</td>\n      <td>PC1_20090606_070012_0010</td>\n      <td>supplemental_data/filtered_spectrograms/PC1_20...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04:45:46] Classes with zero positives in train: 0 / 19\n"
          ]
        }
      ]
    },
    {
      "id": "22eceb39-834a-403c-98fd-01a0ef3dea6d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute mean/std for filtered_spectrograms (train only)\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import time, sys\n",
        "\n",
        "def compute_mean_std(image_paths):\n",
        "    s = 0.0\n",
        "    ss = 0.0\n",
        "    n = 0\n",
        "    h_acc = 0\n",
        "    w_acc = 0\n",
        "    t0 = time.time()\n",
        "    for i, p in enumerate(image_paths):\n",
        "        if i % 50 == 0:\n",
        "            elapsed = time.time() - t0\n",
        "            print(f\"[{i}/{len(image_paths)}] elapsed {elapsed:.1f}s\", flush=True)\n",
        "        img = Image.open(p).convert('L')\n",
        "        arr = np.asarray(img, dtype=np.float32) / 255.0\n",
        "        s += arr.sum()\n",
        "        ss += (arr * arr).sum()\n",
        "        n += arr.size\n",
        "        h_acc += arr.shape[0]\n",
        "        w_acc += arr.shape[1]\n",
        "    mean = s / n if n > 0 else 0.0\n",
        "    var = ss / n - mean * mean if n > 0 else 0.0\n",
        "    std = float(np.sqrt(max(var, 0.0)))\n",
        "    h_mean = h_acc / len(image_paths) if len(image_paths) else 0\n",
        "    w_mean = w_acc / len(image_paths) if len(image_paths) else 0\n",
        "    return mean, std, int(h_mean), int(w_mean)\n",
        "\n",
        "stats_cache = Path('filtered_stats.npy')\n",
        "if stats_cache.exists():\n",
        "    cached = np.load(stats_cache, allow_pickle=True).item()\n",
        "    f_mean, f_std, Hmean, Wmean = cached['mean'], cached['std'], cached['Hmean'], cached['Wmean']\n",
        "    print(f\"Loaded cached stats: mean={f_mean:.6f}, std={f_std:.6f}, Hmean={Hmean}, Wmean={Wmean}\")\n",
        "else:\n",
        "    paths = train_df['bmp'].tolist()\n",
        "    f_mean, f_std, Hmean, Wmean = compute_mean_std(paths)\n",
        "    print(f\"Computed stats: mean={f_mean:.6f}, std={f_std:.6f}, Hmean={Hmean}, Wmean={Wmean}\")\n",
        "    np.save(stats_cache, {'mean': f_mean, 'std': f_std, 'Hmean': Hmean, 'Wmean': Wmean})\n",
        "\n",
        "# For single-channel replicated to 3: use scalar mean/std for all 3 channels\n",
        "norm_mean = [f_mean, f_mean, f_mean]\n",
        "norm_std = [f_std, f_std, f_std]\n",
        "print('Normalization params:', norm_mean, norm_std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "a01e2b70-39f6-4ade-9aaf-9e07b6f5cfde",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required packages (PyTorch CUDA 12.1 build, timm, sklearn, albumentations, cv2)\n",
        "import sys, subprocess, time\n",
        "def pip_install(pkg_line):\n",
        "    print(f\"Installing: {pkg_line}\", flush=True)\n",
        "    code = subprocess.call([sys.executable, '-m', 'pip', 'install'] + pkg_line.split())\n",
        "    print(f\"Return code: {code}\", flush=True)\n",
        "    return code\n",
        "\n",
        "pkgs = [\n",
        "    \"--index-url https://download.pytorch.org/whl/cu121 torch torchvision torchaudio\",\n",
        "    \"timm==0.9.16\",\n",
        "    \"albumentations==1.4.8\",\n",
        "    \"opencv-python-headless==4.10.0.84\",\n",
        "    \"scikit-learn==1.5.1\"\n",
        "]\n",
        "t0=time.time()\n",
        "for p in pkgs:\n",
        "    pip_install(p)\n",
        "print(f\"All installs done in {time.time()-t0:.1f}s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "2d3e98d6-73f0-4c7c-ae4b-5a92bd3e5321",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Diagnostics: import libs and print versions to debug kernel crash\n",
        "import sys\n",
        "import torch, torchvision, timm\n",
        "print('Python:', sys.version)\n",
        "print('Torch:', torch.__version__, 'CUDA available:', torch.cuda.is_available())\n",
        "print('Torchvision:', torchvision.__version__)\n",
        "print('timm:', timm.__version__)\n",
        "print('CUDA device count:', torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    print('CUDA device:', torch.cuda.get_device_name(0))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KernelDied",
          "evalue": "Kernel died unexpectedly.",
          "traceback": []
        }
      ]
    },
    {
      "id": "b92c42c8-4129-458c-bbce-4befdb967647",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Dataset, transforms, ASL loss, metrics, and training skeleton (torchvision-only, no albumentations)\n",
        "import math, time, os, random, gc\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import timm\n",
        "from torchvision import transforms as T\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Global normalization from cell 2\n",
        "NORM_MEAN = norm_mean\n",
        "NORM_STD = norm_std\n",
        "\n",
        "class TimeFreqMask:\n",
        "    def __init__(self, time_mask_frac=0.2, freq_mask_frac=0.2, num_time_masks=1, num_freq_masks=1, p=0.5):\n",
        "        self.time_mask_frac = time_mask_frac\n",
        "        self.freq_mask_frac = freq_mask_frac\n",
        "        self.num_time_masks = num_time_masks\n",
        "        self.num_freq_masks = num_freq_masks\n",
        "        self.p = p\n",
        "    def __call__(self, img_t):\n",
        "        # img_t: Tensor CxHxW in [0,1]\n",
        "        if self.p <= 0 or random.random() > self.p:\n",
        "            return img_t\n",
        "        C, H, W = img_t.shape\n",
        "        out = img_t.clone()\n",
        "        # time masks: along width\n",
        "        max_w = max(1, int(W * self.time_mask_frac))\n",
        "        for _ in range(self.num_time_masks):\n",
        "            w = random.randint(1, max_w)\n",
        "            x0 = random.randint(0, max(0, W - w))\n",
        "            out[:, :, x0:x0+w] = 0.0\n",
        "        # freq masks: along height\n",
        "        max_h = max(1, int(H * self.freq_mask_frac))\n",
        "        for _ in range(self.num_freq_masks):\n",
        "            h = random.randint(1, max_h)\n",
        "            y0 = random.randint(0, max(0, H - h))\n",
        "            out[:, y0:y0+h, :] = 0.0\n",
        "        return out\n",
        "\n",
        "def get_train_transforms(out_size=224):\n",
        "    return T.Compose([\n",
        "        T.Resize((out_size, out_size), interpolation=InterpolationMode.BILINEAR),\n",
        "        T.RandomAffine(degrees=0, translate=(0.1, 0.1), fill=0),\n",
        "        T.ToTensor(),\n",
        "        TimeFreqMask(time_mask_frac=0.2, freq_mask_frac=0.2, num_time_masks=2, num_freq_masks=1, p=0.7),\n",
        "        T.Normalize(mean=NORM_MEAN, std=NORM_STD),\n",
        "    ])\n",
        "\n",
        "def get_valid_transforms(out_size=224):\n",
        "    return T.Compose([\n",
        "        T.Resize((out_size, out_size), interpolation=InterpolationMode.BILINEAR),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=NORM_MEAN, std=NORM_STD),\n",
        "    ])\n",
        "\n",
        "class BirdDataset(Dataset):\n",
        "    def __init__(self, df, labels_matrix=None, transforms=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.labels = labels_matrix\n",
        "        self.transforms = transforms\n",
        "        self.has_labels = labels_matrix is not None\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row['bmp']).convert('RGB')  # replicate grayscale to 3 channels\n",
        "        if self.transforms is not None:\n",
        "            img_t = self.transforms(img)\n",
        "        else:\n",
        "            img_t = T.ToTensor()(img)\n",
        "            img_t = T.Normalize(mean=NORM_MEAN, std=NORM_STD)(img_t)\n",
        "        if self.has_labels:\n",
        "            y = torch.from_numpy(self.labels[idx])\n",
        "            return img_t, y\n",
        "        else:\n",
        "            return img_t, row['Id'], row['rec_id'], row['class_id']\n",
        "\n",
        "# Asymmetric Loss for multilabel\n",
        "class AsymmetricLoss(nn.Module):\n",
        "    def __init__(self, gamma_pos=1.0, gamma_neg=4.0, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=True):\n",
        "        super().__init__()\n",
        "        self.gamma_pos = gamma_pos\n",
        "        self.gamma_neg = gamma_neg\n",
        "        self.clip = clip\n",
        "        self.eps = eps\n",
        "        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n",
        "    def forward(self, logits, targets):\n",
        "        x_sigmoid = torch.sigmoid(logits)\n",
        "        xs_pos = x_sigmoid\n",
        "        xs_neg = 1.0 - x_sigmoid\n",
        "        if self.clip is not None and self.clip > 0:\n",
        "            xs_neg = (xs_neg + self.clip).clamp(max=1)\n",
        "        los_pos = targets * torch.log(xs_pos.clamp(min=self.eps))\n",
        "        los_neg = (1 - targets) * torch.log(xs_neg.clamp(min=self.eps))\n",
        "        if self.gamma_pos > 0 or self.gamma_neg > 0:\n",
        "            if self.disable_torch_grad_focal_loss:\n",
        "                torch.set_grad_enabled(False)\n",
        "            pt0 = xs_pos * targets\n",
        "            pt1 = xs_neg * (1 - targets)\n",
        "            one_sided_gamma = self.gamma_pos * targets + self.gamma_neg * (1 - targets)\n",
        "            one_sided_w = torch.pow(1.0 - (pt0 + pt1), one_sided_gamma)\n",
        "            if self.disable_torch_grad_focal_loss:\n",
        "                torch.set_grad_enabled(True)\n",
        "            los_pos *= one_sided_w\n",
        "            los_neg *= one_sided_w\n",
        "        loss = - (los_pos + los_neg).mean()\n",
        "        return loss\n",
        "\n",
        "def compute_auc(y_true, y_pred, average='macro'):\n",
        "    # y_true, y_pred: numpy arrays [N, C]\n",
        "    aucs = []\n",
        "    C = y_true.shape[1]\n",
        "    for c in range(C):\n",
        "        yt = y_true[:, c]\n",
        "        yp = y_pred[:, c]\n",
        "        if yt.max() == yt.min():\n",
        "            continue  # skip ill-defined\n",
        "        try:\n",
        "            aucs.append(roc_auc_score(yt, yp))\n",
        "        except Exception:\n",
        "            continue\n",
        "    macro = float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "    micro = float(roc_auc_score(y_true.ravel(), y_pred.ravel())) if (y_true.sum()>0 and y_true.sum()<y_true.size) else float('nan')\n",
        "    return macro, micro, aucs\n",
        "\n",
        "def build_model(model_name='tf_efficientnet_b0_ns', num_classes=19, pretrained=True, drop_rate=0.2):\n",
        "    model = timm.create_model(model_name, pretrained=pretrained, in_chans=3, num_classes=num_classes, drop_rate=drop_rate)\n",
        "    return model\n",
        "\n",
        "def get_fold_indices(train_df, fold_id):\n",
        "    trn_idx = train_df.index[train_df['fold'] != fold_id].to_numpy()\n",
        "    val_idx = train_df.index[train_df['fold'] == fold_id].to_numpy()\n",
        "    return trn_idx, val_idx\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, scaler, criterion, device='cuda', log_interval=50):\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "    n = 0\n",
        "    t0 = time.time()\n",
        "    for i, (imgs, targets) in enumerate(loader):\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, targets)\n",
        "        scaler.scale(loss).step(optimizer)\n",
        "        scaler.update()\n",
        "        running += loss.item() * imgs.size(0)\n",
        "        n += imgs.size(0)\n",
        "        if (i+1) % log_interval == 0:\n",
        "            print(f\"  [train] step {i+1}/{len(loader)} loss={running/max(n,1):.4f} elapsed={time.time()-t0:.1f}s\", flush=True)\n",
        "    return running / max(n,1)\n",
        "\n",
        "def validate(model, loader, device='cuda'):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    targets_all = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in loader:\n",
        "            imgs = imgs.to(device, non_blocking=True)\n",
        "            logits = model(imgs)\n",
        "            probs = torch.sigmoid(logits).float().cpu().numpy()\n",
        "            preds.append(probs)\n",
        "            targets_all.append(targets.numpy())\n",
        "    y_pred = np.concatenate(preds, axis=0)\n",
        "    y_true = np.concatenate(targets_all, axis=0)\n",
        "    macro, micro, aucs = compute_auc(y_true, y_pred)\n",
        "    return macro, micro, y_true, y_pred\n",
        "\n",
        "def make_loaders(trn_df, val_df, Y, out_size=224, bs=32, num_workers=4):\n",
        "    trn_ds = BirdDataset(trn_df, labels_matrix=Y[trn_df.index], transforms=get_train_transforms(out_size=out_size))\n",
        "    val_ds = BirdDataset(val_df, labels_matrix=Y[val_df.index], transforms=get_valid_transforms(out_size=out_size))\n",
        "    trn_ld = DataLoader(trn_ds, batch_size=bs, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=False)\n",
        "    val_ld = DataLoader(val_ds, batch_size=bs*2, shuffle=False, num_workers=num_workers, pin_memory=True, drop_last=False)\n",
        "    return trn_ld, val_ld\n",
        "\n",
        "def run_cv(model_name='tf_efficientnet_b0_ns', out_size=224, epochs=12, lr=1e-3, wd=1e-4, bs=32, device='cuda'):\n",
        "    folds = sorted(train_df['fold'].unique())\n",
        "    folds = [f for f in folds if f >= 0]\n",
        "    all_fold_metrics = []\n",
        "    for f in folds:\n",
        "        print(f\"=== Fold {f} / {max(folds)} ===\", flush=True)\n",
        "        trn_idx, val_idx = get_fold_indices(train_df, f)\n",
        "        trn_df = train_df.loc[trn_idx].reset_index(drop=True)\n",
        "        val_df = train_df.loc[val_idx].reset_index(drop=True)\n",
        "\n",
        "        model = build_model(model_name=model_name, num_classes=Y.shape[1], pretrained=True, drop_rate=0.2)\n",
        "        model.to(device)\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "        scaler = torch.cuda.amp.GradScaler()\n",
        "        criterion = AsymmetricLoss(gamma_pos=1.0, gamma_neg=4.0, clip=0.05)\n",
        "\n",
        "        trn_ld, val_ld = make_loaders(trn_df, val_df, Y, out_size=out_size, bs=bs, num_workers=4)\n",
        "        best_macro = -1.0\n",
        "        best_state = None\n",
        "        t_fold0 = time.time()\n",
        "        for ep in range(1, epochs+1):\n",
        "            t_ep0 = time.time()\n",
        "            loss = train_one_epoch(model, trn_ld, optimizer, scaler, criterion, device=device, log_interval=25)\n",
        "            macro, micro, y_true, y_pred = validate(model, val_ld, device=device)\n",
        "            print(f\"Fold {f} Epoch {ep}/{epochs} | train_loss={loss:.4f} | val_macroAUC={macro:.4f} | val_microAUC={micro:.4f} | ep_time={time.time()-t_ep0:.1f}s\", flush=True)\n",
        "            if macro > best_macro:\n",
        "                best_macro = macro\n",
        "                best_state = { 'model': model.state_dict(), 'macro': macro, 'micro': micro, 'epoch': ep }\n",
        "        print(f\"Fold {f} best macroAUC={best_macro:.4f} | fold_time={time.time()-t_fold0:.1f}s\", flush=True)\n",
        "        # Save best\n",
        "        os.makedirs('checkpoints', exist_ok=True)\n",
        "        torch.save(best_state, f'checkpoints/{model_name}_fold{f}_best.pth')\n",
        "        all_fold_metrics.append(best_macro)\n",
        "        # Free\n",
        "        del model; gc.collect(); torch.cuda.empty_cache()\n",
        "    print(f\"CV macro AUCs: {all_fold_metrics} | mean={np.mean(all_fold_metrics):.4f}\", flush=True)\n",
        "    return all_fold_metrics\n",
        "\n",
        "print(\"Dataset, transforms (torchvision), ASL, and training skeleton ready.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "91f0c213-1239-4473-b8c2-382e1e780674",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick sanity CV run (short) with EfficientNet-B0 @224\n",
        "import time, torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device:', device)\n",
        "t0 = time.time()\n",
        "cv_metrics = run_cv(model_name='tf_efficientnet_b0_ns', out_size=224, epochs=4, lr=1e-3, wd=1e-4, bs=32, device=device)\n",
        "print('CV metrics:', cv_metrics, 'elapsed:', f'{time.time()-t0:.1f}s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "a08ad003-dd44-46bd-b552-de728bf84ba9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Classical ML baseline using histogram_of_segments (no torch) with robust CV fallback\n",
        "import pandas as pd, numpy as np, time, os, json, random\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "t0 = time.time()\n",
        "hist_path = SUP/'histogram_of_segments.txt'\n",
        "# Load: first row is a header; skip it\n",
        "hist_df = pd.read_csv(hist_path, header=None, skiprows=1)\n",
        "assert hist_df.shape[1] >= 2, 'histogram_of_segments must have rec_id and features'\n",
        "hist_df = hist_df.rename(columns={0:'rec_id'})\n",
        "hist_df['rec_id'] = hist_df['rec_id'].astype(int)\n",
        "\n",
        "# Features per rec_id\n",
        "feat_cols = [c for c in hist_df.columns if c != 'rec_id']\n",
        "agg_df = hist_df[['rec_id'] + feat_cols].copy().sort_values('rec_id').reset_index(drop=True)\n",
        "log(f'Histogram features: rows={agg_df.shape[0]}, num_features={len(feat_cols)}')\n",
        "\n",
        "# Build train/test feature matrices aligned to rec_ids\n",
        "train_ids_sorted = pd.Index(sorted(train_rec_ids))\n",
        "test_ids_sorted = pd.Index(sorted(test_rec_ids))\n",
        "\n",
        "X_train = agg_df.set_index('rec_id').reindex(train_ids_sorted)[feat_cols].values\n",
        "X_test = agg_df.set_index('rec_id').reindex(test_ids_sorted)[feat_cols].values\n",
        "assert not np.isnan(X_train).any(), 'NaNs in train features after reindex (histogram)'\n",
        "assert not np.isnan(X_test).any(), 'NaNs in test features after reindex (histogram)'\n",
        "\n",
        "# Labels matrix Y aligned\n",
        "Y_df = pd.DataFrame(Y, index=train_rec_ids)\n",
        "Y_aligned = Y_df.reindex(train_ids_sorted).values.astype(np.float32)\n",
        "assert Y_aligned.shape[0] == X_train.shape[0], 'Train features/labels mismatch'\n",
        "\n",
        "# Map folds for these train_ids\n",
        "fold_series = pd.Series(train_ids_sorted).map(fold_map)\n",
        "fold_series = fold_series.fillna(-1).astype(int)\n",
        "folds = sorted([f for f in fold_series.unique() if f >= 0])\n",
        "log(f'Folds present (raw): {folds}')\n",
        "\n",
        "def compute_auc_macro_micro(y_true, y_pred):\n",
        "    aucs = []\n",
        "    C = y_true.shape[1]\n",
        "    for c in range(C):\n",
        "        yt = y_true[:, c]\n",
        "        yp = y_pred[:, c]\n",
        "        if yt.max() == yt.min():\n",
        "            continue\n",
        "        try:\n",
        "            aucs.append(roc_auc_score(yt, yp))\n",
        "        except Exception:\n",
        "            pass\n",
        "    macro = float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "    micro = float(roc_auc_score(y_true.ravel(), y_pred.ravel())) if (y_true.sum()>0 and y_true.sum()<y_true.size) else float('nan')\n",
        "    return macro, micro, aucs\n",
        "\n",
        "# CV loop with validation checks; fallback to random split if provided folds invalid\n",
        "cv_macros = []\n",
        "valid_cv_done = False\n",
        "for f in folds:\n",
        "    trn_mask = (fold_series.values != f)\n",
        "    val_mask = (fold_series.values == f)\n",
        "    if trn_mask.sum() == 0 or val_mask.sum() == 0:\n",
        "        continue\n",
        "    X_tr, X_val = X_train[trn_mask], X_train[val_mask]\n",
        "    y_tr, y_val = Y_aligned[trn_mask], Y_aligned[val_mask]\n",
        "    log(f'Fold {f}: train {X_tr.shape[0]}, val {X_val.shape[0]}')\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_trs = scaler.fit_transform(X_tr)\n",
        "    X_vals = scaler.transform(X_val)\n",
        "    clf = OneVsRestClassifier(LogisticRegression(max_iter=1000, solver='lbfgs', class_weight='balanced'))\n",
        "    t_fit0 = time.time()\n",
        "    clf.fit(X_trs, y_tr)\n",
        "    log(f'  Fit time: {time.time()-t_fit0:.2f}s')\n",
        "    val_proba = np.vstack([est.predict_proba(X_vals)[:,1] for est in clf.estimators_]).T\n",
        "    macro, micro, _ = compute_auc_macro_micro(y_val, val_proba)\n",
        "    log(f'  Fold {f} AUC macro={macro:.4f} micro={micro:.4f}')\n",
        "    cv_macros.append(macro)\n",
        "    valid_cv_done = True\n",
        "\n",
        "if valid_cv_done:\n",
        "    log(f'CV macro AUCs: {cv_macros} | mean={np.mean(cv_macros):.4f}')\n",
        "else:\n",
        "    # Fallback: single random 80/20 split for a quick sanity metric\n",
        "    n = X_train.shape[0]\n",
        "    idx = np.arange(n)\n",
        "    rng = np.random.default_rng(42)\n",
        "    rng.shuffle(idx)\n",
        "    cut = int(0.8 * n)\n",
        "    tr_idx, va_idx = idx[:cut], idx[cut:]\n",
        "    X_tr, X_val = X_train[tr_idx], X_train[va_idx]\n",
        "    y_tr, y_val = Y_aligned[tr_idx], Y_aligned[va_idx]\n",
        "    log(f'Random split: train {X_tr.shape[0]}, val {X_val.shape[0]}')\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_trs = scaler.fit_transform(X_tr)\n",
        "    X_vals = scaler.transform(X_val)\n",
        "    clf = OneVsRestClassifier(LogisticRegression(max_iter=1000, solver='lbfgs', class_weight='balanced'))\n",
        "    t_fit0 = time.time()\n",
        "    clf.fit(X_trs, y_tr)\n",
        "    log(f'  Fit time: {time.time()-t_fit0:.2f}s')\n",
        "    val_proba = np.vstack([est.predict_proba(X_vals)[:,1] for est in clf.estimators_]).T\n",
        "    macro, micro, _ = compute_auc_macro_micro(y_val, val_proba)\n",
        "    log(f'  Random split AUC macro={macro:.4f} micro={micro:.4f}')\n",
        "\n",
        "# Fit on full train and predict test\n",
        "scaler_full = StandardScaler(with_mean=True, with_std=True)\n",
        "X_tr_full = scaler_full.fit_transform(X_train)\n",
        "X_te_full = scaler_full.transform(X_test)\n",
        "clf_full = OneVsRestClassifier(LogisticRegression(max_iter=1000, solver='lbfgs', class_weight='balanced'))\n",
        "t_fit_full0 = time.time()\n",
        "clf_full.fit(X_tr_full, Y_aligned)\n",
        "log(f'Full fit time: {time.time()-t_fit_full0:.2f}s')\n",
        "test_proba_mat = np.vstack([est.predict_proba(X_te_full)[:,1] for est in clf_full.estimators_]).T  # shape [n_test_ids, num_classes]\n",
        "\n",
        "# Build submission in sample_submission order\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(test_ids_sorted)}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx.get(rid, None)\n",
        "    assert ti is not None, f'rec_id {rid} not found in test features'\n",
        "    p = float(test_proba_mat[ti, cid])\n",
        "    if not np.isfinite(p): p = 0.0\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "log(f'Wrote submission.csv with {len(sub_out)} rows in {time.time()-t0:.1f}s')\n",
        "display(sub_out.head())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04:50:26] Histogram features: rows=322, num_features=100\n[04:50:26] Folds present (raw): [0]\n[04:50:26] Random split: train 116, val 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04:50:27]   Fit time: 1.31s\n[04:50:28]   Random split AUC macro=0.7522 micro=0.7636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04:50:31] Full fit time: 3.03s\n[04:50:31] Wrote submission.csv with 1216 rows in 4.5s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.003139\n1  101     0.271228\n2  102     0.154523\n3  103     0.668181\n4  104     0.287230",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.003139</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.271228</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.154523</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.668181</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.287230</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "f4742ceb-6b2c-481d-9e69-2e635fb3614b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install classical ML deps and iterative multilabel stratification\n",
        "import sys, subprocess, time\n",
        "def pip_install(pkg_line):\n",
        "    print(f\"Installing: {pkg_line}\", flush=True)\n",
        "    code = subprocess.call([sys.executable, '-m', 'pip', 'install'] + pkg_line.split())\n",
        "    print(f\"Return code: {code}\", flush=True)\n",
        "    return code\n",
        "\n",
        "pkgs = [\n",
        "    \"iterative-stratification==0.1.7\",\n",
        "    \"lightgbm==4.5.0\",\n",
        "    \"xgboost==2.1.1\"\n",
        "]\n",
        "t0=time.time()\n",
        "for p in pkgs:\n",
        "    pip_install(p)\n",
        "print(f\"All installs done in {time.time()-t0:.1f}s\")\n",
        "\n",
        "# Set global seeds for reproducibility\n",
        "import os, random, numpy as np\n",
        "SEED = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "print('Seeds set to', SEED)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing: iterative-stratification==0.1.7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting iterative-stratification==0.1.7\n  Downloading iterative_stratification-0.1.7-py3-none-any.whl (8.5 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.7/9.7 MB 18.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 102.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy\n  Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.4/35.4 MB 251.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting threadpoolctl>=3.1.0\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nCollecting joblib>=1.2.0\n  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 308.4/308.4 KB 396.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn, iterative-stratification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed iterative-stratification-0.1.7 joblib-1.5.2 numpy-1.26.4 scikit-learn-1.7.2 scipy-1.16.1 threadpoolctl-3.6.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/scikit_learn-1.7.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sklearn already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scikit_learn.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy-1.16.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/joblib-1.5.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/joblib already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/threadpoolctl-3.6.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/threadpoolctl.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Return code: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing: lightgbm==4.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightgbm==4.5.0\n  Downloading lightgbm-4.5.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.6/3.6 MB 42.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy\n  Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.4/35.4 MB 251.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy>=1.17.0\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 301.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: numpy, scipy, lightgbm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed lightgbm-4.5.0 numpy-1.26.4 scipy-1.16.1\nReturn code: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing: xgboost==2.1.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/scipy-1.16.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xgboost==2.1.1\n  Downloading xgboost-2.1.1-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 153.9/153.9 MB 325.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy\n  Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.4/35.4 MB 268.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 276.1 MB/s eta 0:00:00\nCollecting nvidia-nccl-cu12\n  Downloading nvidia_nccl_cu12-2.28.3-py3-none-manylinux_2_18_x86_64.whl (295.9 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 295.9/295.9 MB 300.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: nvidia-nccl-cu12, numpy, scipy, xgboost\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorch 2.8.0 requires nvidia-nccl-cu12==2.27.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nccl-cu12 2.28.3 which is incompatible.\ntorch 2.8.0 requires nvidia-nvjitlink-cu12==12.8.93; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.86 which is incompatible.\nWARNING: Target directory /app/.pip-target/scipy-1.16.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed numpy-1.26.4 nvidia-nccl-cu12-2.28.3 scipy-1.16.1 xgboost-2.1.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Return code: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All installs done in 40.2s\nSeeds set to 42\n"
          ]
        }
      ]
    },
    {
      "id": "903c9e18-2e96-4412-b347-94d13b293218",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Feature engineering + 5-fold MultilabelStratified CV + LGBM/LR OVR + Ensembling\n",
        "import pandas as pd, numpy as np, time, os, gc, random\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "import lightgbm as lgb\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "def auc_macro_micro(y_true, y_pred):\n",
        "    aucs = []\n",
        "    C = y_true.shape[1]\n",
        "    for c in range(C):\n",
        "        yt = y_true[:, c]\n",
        "        yp = y_pred[:, c]\n",
        "        if yt.max() == yt.min():\n",
        "            continue\n",
        "        try:\n",
        "            aucs.append(roc_auc_score(yt, yp))\n",
        "        except Exception:\n",
        "            pass\n",
        "    macro = float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "    micro = float(roc_auc_score(y_true.ravel(), y_pred.ravel())) if (y_true.sum()>0 and y_true.sum()<y_true.size) else float('nan')\n",
        "    return macro, micro\n",
        "\n",
        "# 1) Load histogram_of_segments and build features\n",
        "hist_path = SUP/'histogram_of_segments.txt'\n",
        "hist_df = pd.read_csv(hist_path, header=None, skiprows=1)\n",
        "hist_df = hist_df.rename(columns={0:'rec_id'})\n",
        "hist_df['rec_id'] = hist_df['rec_id'].astype(int)\n",
        "hist_feat_cols = [c for c in hist_df.columns if c != 'rec_id']\n",
        "\n",
        "# Base histogram stats per row (vectorized; avoid per-column inserts)\n",
        "bins_mat = hist_df[hist_feat_cols].astype(np.float32).values\n",
        "row_sum = bins_mat.sum(axis=1, keepdims=True) + 1e-6\n",
        "row_mean = bins_mat.mean(axis=1, keepdims=True)\n",
        "row_std = bins_mat.std(axis=1, keepdims=True)\n",
        "proportions = bins_mat / row_sum\n",
        "entropy = -(proportions * np.log(proportions + 1e-12)).sum(axis=1).astype(np.float32)\n",
        "p10 = np.percentile(bins_mat, 10, axis=1).astype(np.float32)\n",
        "p25 = np.percentile(bins_mat, 25, axis=1).astype(np.float32)\n",
        "p75 = np.percentile(bins_mat, 75, axis=1).astype(np.float32)\n",
        "p90 = np.percentile(bins_mat, 90, axis=1).astype(np.float32)\n",
        "\n",
        "# Proportions features\n",
        "prop_colnames = [f'{col}_prop' for col in hist_feat_cols]\n",
        "prop_df = pd.DataFrame(proportions, columns=prop_colnames, index=hist_df.index).astype(np.float32)\n",
        "\n",
        "# log1p of raw histogram counts\n",
        "log_df = pd.DataFrame(np.log1p(bins_mat), columns=[f'{col}_log1p' for col in hist_feat_cols], index=hist_df.index).astype(np.float32)\n",
        "\n",
        "# Ratios between consecutive bins (vectorized)\n",
        "ratios = bins_mat[:, 1:] / (np.abs(bins_mat[:, :-1]) + 1e-6)\n",
        "ratio_cols = [f'h_ratio_{i}' for i in range(1, len(hist_feat_cols))]\n",
        "ratio_df = pd.DataFrame(ratios.astype(np.float32), columns=ratio_cols, index=hist_df.index)\n",
        "\n",
        "# Row-level summary stats\n",
        "summary_df = pd.DataFrame({\n",
        "    'hist_row_sum': row_sum[:,0].astype(np.float32),\n",
        "    'hist_row_mean': row_mean[:,0].astype(np.float32),\n",
        "    'hist_row_std': row_std[:,0].astype(np.float32),\n",
        "    'hist_entropy': entropy,\n",
        "    'hist_p10': p10,\n",
        "    'hist_p25': p25,\n",
        "    'hist_p75': p75,\n",
        "    'hist_p90': p90\n",
        "}, index=hist_df.index)\n",
        "\n",
        "# Assemble histogram feature frame once (avoid fragmentation)\n",
        "hist_feats = pd.concat([hist_df[['rec_id']], hist_df[hist_feat_cols].astype(np.float32), prop_df, log_df, ratio_df, summary_df], axis=1)\n",
        "hist_df = hist_feats  # reuse downstream\n",
        "\n",
        "# 2) Load segment_features and aggregate per rec_id\n",
        "seg_path = SUP/'segment_features.txt'\n",
        "seg_raw = pd.read_csv(seg_path, header=None, skiprows=1)\n",
        "seg_raw = seg_raw.rename(columns={0:'rec_id', 1:'seg_idx'})\n",
        "seg_raw['rec_id'] = seg_raw['rec_id'].astype(int)\n",
        "num_cols = [c for c in seg_raw.columns if c not in ['rec_id','seg_idx']]\n",
        "for c in num_cols:\n",
        "    seg_raw[c] = pd.to_numeric(seg_raw[c], errors='coerce')\n",
        "\n",
        "def q25(x): return np.nanpercentile(x, 25)\n",
        "def q75(x): return np.nanpercentile(x, 75)\n",
        "\n",
        "agg_funcs = {c:['mean','std','min','max','median',q25,q75] for c in num_cols}\n",
        "agg_df = seg_raw.groupby('rec_id').agg(agg_funcs)\n",
        "# Flatten MultiIndex columns\n",
        "agg_df.columns = ['%s_%s'% (col[0], ('q25' if col[1]==q25 else 'q75' if col[1]==q75 else col[1])) for col in agg_df.columns.values]\n",
        "agg_df = agg_df.reset_index()\n",
        "agg_df['n_seg'] = seg_raw.groupby('rec_id').size().reindex(agg_df['rec_id']).values.astype(np.float32)\n",
        "# std/mean ratios\n",
        "for c in num_cols:\n",
        "    m = agg_df[f'{c}_mean'].replace(0, np.nan)\n",
        "    s = agg_df[f'{c}_std']\n",
        "    agg_df[f'{c}_std_over_mean'] = (s / m).replace([np.inf, -np.inf], np.nan).astype(np.float32)\n",
        "\n",
        "# has_seg indicator and log1p of n_seg\n",
        "agg_df['has_seg'] = 1.0\n",
        "agg_df['n_seg_log1p'] = np.log1p(agg_df['n_seg'].fillna(0.0)).astype(np.float32)\n",
        "\n",
        "# 3) Merge hist + ratios + seg aggregations\n",
        "feat_df = hist_df.merge(agg_df, on='rec_id', how='left')\n",
        "feat_df['has_seg'] = feat_df['has_seg'].fillna(0.0).astype(np.float32)\n",
        "feat_df['n_seg'] = feat_df['n_seg'].fillna(0.0).astype(np.float32)\n",
        "feat_df['n_seg_log1p'] = feat_df['n_seg_log1p'].fillna(0.0).astype(np.float32)\n",
        "# Replace inf/nan\n",
        "feat_df = feat_df.replace([np.inf, -np.inf], 0.0)\n",
        "feat_df = feat_df.fillna(0.0)\n",
        "\n",
        "# Drop near-constant columns\n",
        "all_feat_cols = [c for c in feat_df.columns if c != 'rec_id']\n",
        "vars_ = feat_df[all_feat_cols].astype(np.float32).var(axis=0).values\n",
        "keep_mask = vars_ > 1e-8\n",
        "kept_cols = [col for col, keep in zip(all_feat_cols, keep_mask) if keep]\n",
        "X_all = feat_df[['rec_id'] + kept_cols].copy()\n",
        "print(f'Engineered features: total={len(all_feat_cols)}, kept={len(kept_cols)}')\n",
        "\n",
        "# Build train/test matrices aligned to rec_ids\n",
        "train_ids_sorted = pd.Index(sorted(train_rec_ids))\n",
        "test_ids_sorted = pd.Index(sorted(test_rec_ids))\n",
        "X_train_df = X_all.set_index('rec_id').reindex(train_ids_sorted)[kept_cols].astype(np.float32)\n",
        "X_test_df = X_all.set_index('rec_id').reindex(test_ids_sorted)[kept_cols].astype(np.float32)\n",
        "assert not np.isnan(X_train_df.values).any() and not np.isnan(X_test_df.values).any(), 'NaNs after engineering'\n",
        "\n",
        "Y_df = pd.DataFrame(Y, index=train_rec_ids)\n",
        "Y_aligned = Y_df.reindex(train_ids_sorted).values.astype(np.float32)\n",
        "\n",
        "# 4) MultilabelStratifiedKFold(5) + simple models (kept here for reference, but downstream improved cell will use X_train_df/X_test_df)\n",
        "SEED = 42\n",
        "mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "\n",
        "N = X_train_df.shape[0]\n",
        "C = Y_aligned.shape[1]\n",
        "oof_lgb = np.zeros((N, C), dtype=np.float32)\n",
        "oof_lr = np.zeros((N, C), dtype=np.float32)\n",
        "test_lgb_folds = []\n",
        "test_lr_folds = []\n",
        "\n",
        "# LightGBM params\n",
        "lgb_params = dict(\n",
        "    objective='binary', boosting_type='gbdt',\n",
        "    n_estimators=2000, learning_rate=0.03, max_depth=4, num_leaves=16,\n",
        "    min_child_samples=10, subsample=0.8, subsample_freq=1, colsample_bytree=0.7,\n",
        "    reg_alpha=1.0, reg_lambda=5.0, random_state=SEED, n_jobs=-1, verbosity=-1\n",
        ")\n",
        "\n",
        "fold_macros = {'lgb':[], 'lr':[]}\n",
        "fold_micros = {'lgb':[], 'lr':[]}\n",
        "\n",
        "X_all_np = X_train_df.values\n",
        "T_all_np = X_test_df.values\n",
        "\n",
        "fold_idx = 0\n",
        "for tr_idx, va_idx in mskf.split(X_all_np, Y_aligned):\n",
        "    t_fold0 = time.time()\n",
        "    print(f'=== Fold {fold_idx} ===', flush=True)\n",
        "    X_tr, X_va = X_all_np[tr_idx], X_all_np[va_idx]\n",
        "    y_tr, y_va = Y_aligned[tr_idx], Y_aligned[va_idx]\n",
        "\n",
        "    # -- LightGBM OVR (class-wise) with early stopping\n",
        "    te_pred_lgb_fold = np.zeros((T_all_np.shape[0], C), dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        y_tr_c = y_tr[:, c]\n",
        "        y_va_c = y_va[:, c]\n",
        "        if y_tr_c.max() == y_tr_c.min():\n",
        "            oof_lgb[va_idx, c] = 0.0\n",
        "            te_pred_lgb_fold[:, c] = 0.0\n",
        "            continue\n",
        "        clf = lgb.LGBMClassifier(**lgb_params)\n",
        "        clf.fit(\n",
        "            X_tr, y_tr_c,\n",
        "            eval_set=[(X_va, y_va_c)],\n",
        "            eval_metric='auc',\n",
        "            callbacks=[lgb.early_stopping(100, verbose=False), lgb.log_evaluation(period=0)]\n",
        "        )\n",
        "        oof_lgb[va_idx, c] = clf.predict_proba(X_va)[:,1]\n",
        "        te_pred_lgb_fold[:, c] = clf.predict_proba(T_all_np)[:,1]\n",
        "    m_lgb, mi_lgb = auc_macro_micro(Y_aligned[va_idx], oof_lgb[va_idx])\n",
        "    fold_macros['lgb'].append(m_lgb); fold_micros['lgb'].append(mi_lgb)\n",
        "    print(f'  LGBM fold macro={m_lgb:.4f} micro={mi_lgb:.4f}', flush=True)\n",
        "\n",
        "    # -- LogisticRegression (elastic-net) OVR with StandardScaler (fit in-fold)\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_trs = scaler.fit_transform(X_tr)\n",
        "    X_vas = scaler.transform(X_va)\n",
        "    X_tes = scaler.transform(T_all_np)\n",
        "    lr_clf = OneVsRestClassifier(LogisticRegression(solver='saga', penalty='elasticnet',\n",
        "                                                    l1_ratio=0.5, C=0.5, max_iter=5000,\n",
        "                                                    class_weight='balanced', n_jobs=-1))\n",
        "    lr_clf.fit(X_trs, y_tr)\n",
        "    lr_val_pred = np.vstack([est.predict_proba(X_vas)[:,1] for est in lr_clf.estimators_]).T\n",
        "    lr_te_pred = np.vstack([est.predict_proba(X_tes)[:,1] for est in lr_clf.estimators_]).T\n",
        "    oof_lr[va_idx] = lr_val_pred.astype(np.float32)\n",
        "    m_lr, mi_lr = auc_macro_micro(Y_aligned[va_idx], lr_val_pred)\n",
        "    fold_macros['lr'].append(m_lr); fold_micros['lr'].append(mi_lr)\n",
        "    print(f'  LR(enet) fold macro={m_lr:.4f} micro={mi_lr:.4f}', flush=True)\n",
        "\n",
        "    test_lgb_folds.append(te_pred_lgb_fold)\n",
        "    test_lr_folds.append(lr_te_pred.astype(np.float32))\n",
        "    print(f'Fold {fold_idx} time: {time.time()-t_fold0:.1f}s', flush=True)\n",
        "    fold_idx += 1\n",
        "\n",
        "print('CV LGBM macro:', fold_macros['lgb'], 'mean=', np.nanmean(fold_macros['lgb']))\n",
        "print('CV LGBM micro:', fold_micros['lgb'], 'mean=', np.nanmean(fold_micros['lgb']))\n",
        "print('CV LR   macro:', fold_macros['lr'], 'mean=', np.nanmean(fold_macros['lr']))\n",
        "print('CV LR   micro:', fold_micros['lr'], 'mean=', np.nanmean(fold_micros['lr']))\n",
        "\n",
        "# 5) Tune blend weight on OOF (alpha for LGBM; (1-alpha) for LR) to maximize macro AUC\n",
        "best_alpha = 1.0\n",
        "best_macro = -1.0\n",
        "for a in np.linspace(0.0, 1.0, 11):\n",
        "    oof_blend_try = a * oof_lgb + (1.0 - a) * oof_lr\n",
        "    m_try, _ = auc_macro_micro(Y_aligned, oof_blend_try)\n",
        "    if m_try > best_macro:\n",
        "        best_macro = m_try\n",
        "        best_alpha = float(a)\n",
        "print(f'Best OOF blend alpha (LGBM weight)={best_alpha:.2f} | macro={best_macro:.4f}')\n",
        "\n",
        "# 6) Average test preds across folds within each model, then blend with tuned alpha\n",
        "test_lgb = np.mean(test_lgb_folds, axis=0)\n",
        "test_lr = np.mean(test_lr_folds, axis=0)\n",
        "test_blend = best_alpha * test_lgb + (1.0 - best_alpha) * test_lr\n",
        "\n",
        "# 7) Build submission according to sample_submission order\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(test_ids_sorted)}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_blend[ti, cid])\n",
        "    if not np.isfinite(p): p = 0.0\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f'Wrote submission.csv with {len(sub_out)} rows')\n",
        "display(sub_out.head())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Engineered features: total=714, kept=697\n=== Fold 0 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  LGBM fold macro=0.8130 micro=0.7576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 191\u001b[39m\n\u001b[32m    187\u001b[39m X_tes = scaler.transform(T_all_np)\n\u001b[32m    188\u001b[39m lr_clf = OneVsRestClassifier(LogisticRegression(solver=\u001b[33m'\u001b[39m\u001b[33msaga\u001b[39m\u001b[33m'\u001b[39m, penalty=\u001b[33m'\u001b[39m\u001b[33melasticnet\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    189\u001b[39m                                                 l1_ratio=\u001b[32m0.5\u001b[39m, C=\u001b[32m0.5\u001b[39m, max_iter=\u001b[32m5000\u001b[39m,\n\u001b[32m    190\u001b[39m                                                 class_weight=\u001b[33m'\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m'\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[43mlr_clf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_trs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m lr_val_pred = np.vstack([est.predict_proba(X_vas)[:,\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m est \u001b[38;5;129;01min\u001b[39;00m lr_clf.estimators_]).T\n\u001b[32m    193\u001b[39m lr_te_pred = np.vstack([est.predict_proba(X_tes)[:,\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m est \u001b[38;5;129;01min\u001b[39;00m lr_clf.estimators_]).T\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/multiclass.py:376\u001b[39m, in \u001b[36mOneVsRestClassifier.fit\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    372\u001b[39m columns = (col.toarray().ravel() \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m Y.T)\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# In cases where individual estimators are very fast to train setting\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;66;03m# n_jobs > 1 in can results in slower performance due to the overhead\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# of spawning threads.  See joblib issue #112.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_binary\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnot \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_binarizer_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_binarizer_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimators_[\u001b[32m0\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mn_features_in_\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    391\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_features_in_ = \u001b[38;5;28mself\u001b[39m.estimators_[\u001b[32m0\u001b[39m].n_features_in_\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/utils/parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/joblib/parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/joblib/parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/utils/parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/multiclass.py:96\u001b[39m, in \u001b[36m_fit_binary\u001b[39m\u001b[34m(estimator, X, y, fit_params, classes)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     95\u001b[39m     estimator = clone(estimator)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/linear_model/_logistic.py:1384\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1382\u001b[39m     n_threads = \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1384\u001b[39m fold_coefs_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1409\u001b[39m fold_coefs_, _, n_iter_ = \u001b[38;5;28mzip\u001b[39m(*fold_coefs_)\n\u001b[32m   1410\u001b[39m \u001b[38;5;28mself\u001b[39m.n_iter_ = np.asarray(n_iter_, dtype=np.int32)[:, \u001b[32m0\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/utils/parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "18571978-2d93-471d-8a9f-64ec9f4dc93f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Improved CV with fixed tie-aware ranking, RepeatedMultilabelStratifiedKFold, stronger regularization, per-class scale_pos_weight (clip[1,20]), LGBM+XGB, rank-averaged ensembling\n",
        "import numpy as np, pandas as pd, time, os, gc, warnings\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from iterstrat.ml_stratifiers import RepeatedMultilabelStratifiedKFold\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "def masked_auc_macro(y_true, y_pred, valid_mask, return_counts=False):\n",
        "    C = y_true.shape[1]\n",
        "    aucs = []\n",
        "    counts = []\n",
        "    for c in range(C):\n",
        "        mask = valid_mask[:, c].astype(bool)\n",
        "        if mask.sum() <= 1:\n",
        "            continue\n",
        "        yt = y_true[mask, c]\n",
        "        yp = y_pred[mask, c]\n",
        "        if yt.max() == yt.min():\n",
        "            continue\n",
        "        try:\n",
        "            aucs.append(roc_auc_score(yt, yp))\n",
        "            counts.append(mask.sum())\n",
        "        except Exception:\n",
        "            pass\n",
        "    macro = float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "    if return_counts:\n",
        "        return macro, aucs, counts\n",
        "    return macro, aucs\n",
        "\n",
        "# Tie-aware rank functions (PRIORITY 1 fix)\n",
        "def rank_cols_with_mask(mat, valid_mask):\n",
        "    from scipy.stats import rankdata\n",
        "    N, C = mat.shape\n",
        "    out = np.full((N, C), np.nan, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        mask = valid_mask[:, c].astype(bool)\n",
        "        if not mask.any():\n",
        "            continue\n",
        "        col = mat[mask, c]\n",
        "        r = rankdata(col, method='average')\n",
        "        r = (r - 1) / (len(r) - 1) if len(r) > 1 else r*0.0\n",
        "        out[mask, c] = r.astype(np.float32)\n",
        "    return out\n",
        "\n",
        "def rank_cols(mat):\n",
        "    from scipy.stats import rankdata\n",
        "    N, C = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (N - 1) if N > 1 else r*0.0\n",
        "    return out\n",
        "\n",
        "SEED = 42\n",
        "N = X_train_df.shape[0]\n",
        "C = Y_aligned.shape[1]\n",
        "X_all_np = X_train_df.values.astype(np.float32)\n",
        "T_all_np = X_test_df.values.astype(np.float32)\n",
        "\n",
        "# Feature reduction via MI (global top-k by mean MI across classes, not union) [PRIORITY 2]\n",
        "top_k = 220\n",
        "mi_scores = []\n",
        "for c in range(C):\n",
        "    y = Y_aligned[:, c]\n",
        "    if y.sum() < 2 or y.sum() == len(y):\n",
        "        continue\n",
        "    try:\n",
        "        s = mutual_info_classif(X_all_np, y, discrete_features=False, random_state=SEED)\n",
        "        mi_scores.append(s)\n",
        "    except Exception:\n",
        "        continue\n",
        "if len(mi_scores) > 0:\n",
        "    mi_scores = np.vstack(mi_scores)\n",
        "    mi_mean = mi_scores.mean(axis=0)\n",
        "    idx_sel = np.argsort(-mi_mean)[:min(top_k, X_all_np.shape[1])].astype(int)\n",
        "else:\n",
        "    idx_sel = np.arange(X_all_np.shape[1])\n",
        "X_all_np = X_all_np[:, idx_sel]\n",
        "T_all_np = T_all_np[:, idx_sel]\n",
        "print(f'MI selection (mean across classes) -> kept features: {X_all_np.shape[1]} / {X_train_df.shape[1]}', flush=True)\n",
        "\n",
        "# Repeated Multilabel Stratified K-Fold (5x2) for stability\n",
        "rmskf = RepeatedMultilabelStratifiedKFold(n_splits=5, n_repeats=2, random_state=SEED)\n",
        "\n",
        "oof_lgb = np.zeros((N, C), dtype=np.float32)\n",
        "oof_xgb = np.zeros((N, C), dtype=np.float32)\n",
        "oof_valid_mask_lgb = np.zeros((N, C), dtype=np.uint8)\n",
        "oof_valid_mask_xgb = np.zeros((N, C), dtype=np.uint8)\n",
        "test_lgb_folds = []\n",
        "test_xgb_folds = []\n",
        "\n",
        "# LightGBM params (stronger regularization, shallow trees)\n",
        "lgb_base = dict(\n",
        "    objective='binary', boosting_type='gbdt',\n",
        "    n_estimators=5000, learning_rate=0.025, max_depth=3, num_leaves=7,\n",
        "    min_child_samples=22, subsample=0.75, subsample_freq=1, colsample_bytree=0.5,\n",
        "    reg_alpha=1.0, reg_lambda=15.0, random_state=SEED, n_jobs=-1, verbosity=-1\n",
        ")\n",
        "\n",
        "# XGBoost native params\n",
        "xgb_base = dict(\n",
        "    objective='binary:logistic', eval_metric='auc', tree_method='hist',\n",
        "    eta=0.025, max_depth=3, min_child_weight=8, subsample=0.7, colsample_bytree=0.55,\n",
        "    alpha=1.0, **{'lambda': 15.0}, seed=SEED, nthread=-1\n",
        ")\n",
        "\n",
        "fold_id = 0\n",
        "for tr_idx, va_idx in rmskf.split(X_all_np, Y_aligned):\n",
        "    t0 = time.time()\n",
        "    print(f'=== Fold {fold_id} ===', flush=True)\n",
        "    X_tr, X_va = X_all_np[tr_idx], X_all_np[va_idx]\n",
        "    y_tr, y_va = Y_aligned[tr_idx], Y_aligned[va_idx]\n",
        "\n",
        "    te_pred_lgb = np.zeros((T_all_np.shape[0], C), dtype=np.float32)\n",
        "    te_pred_xgb = np.zeros((T_all_np.shape[0], C), dtype=np.float32)\n",
        "\n",
        "    for c in range(C):\n",
        "        y_tr_c = y_tr[:, c]\n",
        "        y_va_c = y_va[:, c]\n",
        "        tr_pos = int((y_tr_c == 1).sum()); tr_neg = int((y_tr_c == 0).sum())\n",
        "        va_pos = int((y_va_c == 1).sum()); va_neg = int((y_va_c == 0).sum())\n",
        "        valid = (tr_pos > 0 and tr_neg > 0 and va_pos > 0 and va_neg > 0)\n",
        "        if not valid:\n",
        "            continue\n",
        "        spw = float(tr_neg) / float(max(tr_pos, 1))\n",
        "        spw = float(np.clip(spw, 1.0, 20.0))\n",
        "\n",
        "        # LightGBM\n",
        "        lgb_params = dict(lgb_base)\n",
        "        lgb_params['scale_pos_weight'] = spw\n",
        "        lgb_clf = lgb.LGBMClassifier(**lgb_params)\n",
        "        lgb_clf.fit(\n",
        "            X_tr, y_tr_c,\n",
        "            eval_set=[(X_va, y_va_c)],\n",
        "            eval_metric='auc',\n",
        "            callbacks=[lgb.early_stopping(250, verbose=False), lgb.log_evaluation(period=0)]\n",
        "        )\n",
        "        oof_lgb[va_idx, c] = lgb_clf.predict_proba(X_va)[:, 1]\n",
        "        te_pred_lgb[:, c] = lgb_clf.predict_proba(T_all_np)[:, 1]\n",
        "        oof_valid_mask_lgb[va_idx, c] = 1\n",
        "\n",
        "        # XGBoost (native)\n",
        "        x_params = dict(xgb_base)\n",
        "        x_params['scale_pos_weight'] = spw\n",
        "        dtr = xgb.DMatrix(X_tr, label=y_tr_c)\n",
        "        dva = xgb.DMatrix(X_va, label=y_va_c)\n",
        "        dte = xgb.DMatrix(T_all_np)\n",
        "        bst = xgb.train(\n",
        "            params=x_params,\n",
        "            dtrain=dtr,\n",
        "            num_boost_round=4000,\n",
        "            evals=[(dva, 'valid')],\n",
        "            early_stopping_rounds=250,\n",
        "            verbose_eval=False\n",
        "        )\n",
        "        oof_xgb[va_idx, c] = bst.predict(dva)\n",
        "        te_pred_xgb[:, c] = bst.predict(dte)\n",
        "        oof_valid_mask_xgb[va_idx, c] = 1\n",
        "\n",
        "    test_lgb_folds.append(te_pred_lgb)\n",
        "    test_xgb_folds.append(te_pred_xgb)\n",
        "\n",
        "    fold_mask_l = (oof_valid_mask_lgb[va_idx] > 0).astype(np.uint8)\n",
        "    m_lgb, _ = masked_auc_macro(y_va, oof_lgb[va_idx], fold_mask_l)\n",
        "    fold_mask_x = (oof_valid_mask_xgb[va_idx] > 0).astype(np.uint8)\n",
        "    m_xgb, _ = masked_auc_macro(y_va, oof_xgb[va_idx], fold_mask_x)\n",
        "    print(f'  Fold {fold_id} masked macro: LGBM={m_lgb:.4f} | XGB={m_xgb:.4f} | time={time.time()-t0:.1f}s', flush=True)\n",
        "    fold_id += 1\n",
        "\n",
        "# Global masked OOF\n",
        "macro_lgb, _ = masked_auc_macro(Y_aligned, oof_lgb, oof_valid_mask_lgb)\n",
        "macro_xgb, _ = masked_auc_macro(Y_aligned, oof_xgb, oof_valid_mask_xgb)\n",
        "print(f'Global masked OOF macro: LGBM={macro_lgb:.4f} | XGB={macro_xgb:.4f}')\n",
        "\n",
        "# Rank-avg tuning using intersection mask\n",
        "valid_inter = (oof_valid_mask_lgb.astype(bool) & oof_valid_mask_xgb.astype(bool)).astype(np.uint8)\n",
        "oof_lgb_rank = rank_cols_with_mask(oof_lgb, valid_inter)\n",
        "oof_xgb_rank = rank_cols_with_mask(oof_xgb, valid_inter)\n",
        "best_alpha = 0.5\n",
        "best_macro = -1.0\n",
        "for a in np.linspace(0.0, 1.0, 21):\n",
        "    blend = a * oof_lgb_rank + (1.0 - a) * oof_xgb_rank\n",
        "    m, _ = masked_auc_macro(Y_aligned, blend, valid_inter)\n",
        "    if m > best_macro:\n",
        "        best_macro = m\n",
        "        best_alpha = float(a)\n",
        "print(f'Best masked OOF rank-blend alpha (LGBM weight)={best_alpha:.2f} | macro={best_macro:.4f}')\n",
        "\n",
        "# Average test preds across folds and rank-average with tuned alpha\n",
        "test_lgb = np.mean(test_lgb_folds, axis=0)\n",
        "test_xgb = np.mean(test_xgb_folds, axis=0)\n",
        "test_lgb_rank = rank_cols(test_lgb)\n",
        "test_xgb_rank = rank_cols(test_xgb)\n",
        "test_blend_rank = best_alpha * test_lgb_rank + (1.0 - best_alpha) * test_xgb_rank\n",
        "\n",
        "# Build submission\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_blend_rank[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f'Wrote submission.csv with {len(sub_out)} rows (rank-averaged LGBM+XGB, repeated CV, MI-selected features)')\n",
        "display(sub_out.head())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MI selection (mean across classes) -> kept features: 220 / 697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Fold 0 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0 masked macro: LGBM=0.7457 | XGB=0.7659 | time=6.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Fold 1 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1 masked macro: LGBM=0.7583 | XGB=0.7088 | time=6.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Fold 2 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2 masked macro: LGBM=0.7885 | XGB=0.6649 | time=5.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Fold 3 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3 masked macro: LGBM=0.7695 | XGB=0.7625 | time=6.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Fold 4 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4 masked macro: LGBM=0.7949 | XGB=0.8093 | time=5.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Fold 5 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 5 masked macro: LGBM=0.7217 | XGB=0.7305 | time=5.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Fold 6 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 6 masked macro: LGBM=0.7494 | XGB=0.7470 | time=5.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Fold 7 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 7 masked macro: LGBM=0.7788 | XGB=0.7600 | time=5.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Fold 8 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 8 masked macro: LGBM=0.7753 | XGB=0.7256 | time=6.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Fold 9 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n/app/.pip-target/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 9 masked macro: LGBM=0.7148 | XGB=0.6932 | time=6.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global masked OOF macro: LGBM=0.6944 | XGB=0.6989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best masked OOF rank-blend alpha (LGBM weight)=0.55 | macro=0.7108\nWrote submission.csv with 1216 rows (rank-averaged LGBM+XGB, repeated CV, MI-selected features)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.302381\n1  101     0.420635\n2  102     0.420635\n3  103     0.753968\n4  104     0.584921",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.302381</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.420635</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.420635</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.753968</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.584921</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "c3c8d10b-f6c0-48ce-a06d-d9250e3cc345",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Add CatBoost model with strong regularization and rank-ensemble with LGBM+XGB\n",
        "import time, numpy as np, pandas as pd, warnings, sys, subprocess\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def pip_install(pkg_line):\n",
        "    print(f\"Installing: {pkg_line}\", flush=True)\n",
        "    code = subprocess.call([sys.executable, '-m', 'pip', 'install'] + pkg_line.split())\n",
        "    print(f\"Return code: {code}\")\n",
        "    return code\n",
        "\n",
        "try:\n",
        "    import catboost\n",
        "except Exception:\n",
        "    pip_install('catboost==1.2.5')\n",
        "    import catboost\n",
        "\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "SEED = 42\n",
        "N = X_all_np.shape[0]\n",
        "C = Y_aligned.shape[1]\n",
        "\n",
        "# OOF/test containers for CatBoost\n",
        "oof_cb = np.zeros((N, C), dtype=np.float32)\n",
        "oof_valid_mask_cb = np.zeros((N, C), dtype=np.uint8)\n",
        "test_cb_folds = []\n",
        "\n",
        "# Reuse the same RepeatedMultilabelStratifiedKFold splits for fairness\n",
        "from iterstrat.ml_stratifiers import RepeatedMultilabelStratifiedKFold\n",
        "rmskf = RepeatedMultilabelStratifiedKFold(n_splits=5, n_repeats=2, random_state=SEED)\n",
        "\n",
        "fold_id = 0\n",
        "for tr_idx, va_idx in rmskf.split(X_all_np, Y_aligned):\n",
        "    t0 = time.time()\n",
        "    print(f'=== CB Fold {fold_id} ===', flush=True)\n",
        "    X_tr, X_va = X_all_np[tr_idx], X_all_np[va_idx]\n",
        "    y_tr, y_va = Y_aligned[tr_idx], Y_aligned[va_idx]\n",
        "    te_pred_cb = np.zeros((T_all_np.shape[0], C), dtype=np.float32)\n",
        "\n",
        "    for c in range(C):\n",
        "        y_tr_c = y_tr[:, c]\n",
        "        y_va_c = y_va[:, c]\n",
        "        tr_pos = int((y_tr_c == 1).sum()); tr_neg = int((y_tr_c == 0).sum())\n",
        "        va_pos = int((y_va_c == 1).sum()); va_neg = int((y_va_c == 0).sum())\n",
        "        valid = (tr_pos > 0 and tr_neg > 0 and va_pos > 0 and va_neg > 0)\n",
        "        if not valid:\n",
        "            continue\n",
        "        train_pool = Pool(X_tr, label=y_tr_c)\n",
        "        valid_pool = Pool(X_va, label=y_va_c)\n",
        "        cb = CatBoostClassifier(\n",
        "            loss_function='Logloss',\n",
        "            depth=4,\n",
        "            learning_rate=0.025,\n",
        "            iterations=5000,\n",
        "            l2_leaf_reg=15.0,\n",
        "            auto_class_weights='Balanced',\n",
        "            random_seed=SEED,\n",
        "            early_stopping_rounds=250,\n",
        "            verbose=False,\n",
        "            task_type='CPU',\n",
        "            thread_count=-1\n",
        "        )\n",
        "        cb.fit(train_pool, eval_set=valid_pool)\n",
        "        oof_cb[va_idx, c] = cb.predict_proba(X_va)[:, 1]\n",
        "        te_pred_cb[:, c] = cb.predict_proba(T_all_np)[:, 1]\n",
        "        oof_valid_mask_cb[va_idx, c] = 1\n",
        "\n",
        "    test_cb_folds.append(te_pred_cb)\n",
        "    # Per-fold monitor\n",
        "    fold_mask_cb = (oof_valid_mask_cb[va_idx] > 0).astype(np.uint8)\n",
        "    m_cb, _ = masked_auc_macro(y_va, oof_cb[va_idx], fold_mask_cb)\n",
        "    print(f'  CB Fold {fold_id} masked macro: {m_cb:.4f} | time={time.time()-t0:.1f}s', flush=True)\n",
        "    fold_id += 1\n",
        "\n",
        "# Global CatBoost masked OOF\n",
        "macro_cb, _ = masked_auc_macro(Y_aligned, oof_cb, oof_valid_mask_cb)\n",
        "print(f'Global masked OOF macro: CatBoost={macro_cb:.4f}')\n",
        "\n",
        "# Rank-ensemble across LGBM, XGB, CatBoost using intersection mask\n",
        "valid_inter_all = (oof_valid_mask_lgb.astype(bool) & oof_valid_mask_xgb.astype(bool) & oof_valid_mask_cb.astype(bool)).astype(np.uint8)\n",
        "oof_lgb_rank = rank_cols_with_mask(oof_lgb, valid_inter_all)\n",
        "oof_xgb_rank = rank_cols_with_mask(oof_xgb, valid_inter_all)\n",
        "oof_cb_rank  = rank_cols_with_mask(oof_cb,  valid_inter_all)\n",
        "\n",
        "# Coarse grid search for weights that sum to 1: w_lgb, w_xgb, w_cb\n",
        "best_w = (1/3, 1/3, 1/3)\n",
        "best_macro = -1.0\n",
        "grid = np.linspace(0.0, 1.0, 6)\n",
        "for wl in grid:\n",
        "    for wx in grid:\n",
        "        wc = 1.0 - wl - wx\n",
        "        if wc < 0 or wc > 1:\n",
        "            continue\n",
        "        blend = wl * oof_lgb_rank + wx * oof_xgb_rank + wc * oof_cb_rank\n",
        "        m, _ = masked_auc_macro(Y_aligned, blend, valid_inter_all)\n",
        "        if m > best_macro:\n",
        "            best_macro = m\n",
        "            best_w = (float(wl), float(wx), float(wc))\n",
        "print(f'Best 3-model rank-blend weights (LGBM, XGB, CB)={best_w} | masked OOF macro={best_macro:.4f}')\n",
        "\n",
        "# Average test preds across folds for CatBoost\n",
        "test_cb = np.mean(test_cb_folds, axis=0)\n",
        "test_lgb = np.mean(test_lgb_folds, axis=0)\n",
        "test_xgb = np.mean(test_xgb_folds, axis=0)\n",
        "test_lgb_rank = rank_cols(test_lgb)\n",
        "test_xgb_rank = rank_cols(test_xgb)\n",
        "test_cb_rank  = rank_cols(test_cb)\n",
        "\n",
        "wl, wx, wc = best_w\n",
        "test_blend_rank3 = wl * test_lgb_rank + wx * test_xgb_rank + wc * test_cb_rank\n",
        "\n",
        "# Build submission\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_blend_rank3[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f'Wrote submission.csv with {len(sub_out)} rows (rank-averaged LGBM+XGB+CatBoost)')\n",
        "display(sub_out.head())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB Fold 0 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB Fold 0 masked macro: 0.7851 | time=37.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB Fold 1 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB Fold 1 masked macro: 0.7479 | time=16.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB Fold 2 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB Fold 2 masked macro: 0.7568 | time=23.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB Fold 3 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB Fold 3 masked macro: 0.8154 | time=41.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB Fold 4 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB Fold 4 masked macro: 0.8180 | time=20.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB Fold 5 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB Fold 5 masked macro: 0.7424 | time=25.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB Fold 6 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB Fold 6 masked macro: 0.7677 | time=25.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB Fold 7 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB Fold 7 masked macro: 0.7923 | time=29.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB Fold 8 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB Fold 8 masked macro: 0.7989 | time=36.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB Fold 9 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB Fold 9 masked macro: 0.7455 | time=21.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global masked OOF macro: CatBoost=0.7996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best 3-model rank-blend weights (LGBM, XGB, CB)=(0.2, 0.0, 0.8) | masked OOF macro=0.8018\nWrote submission.csv with 1216 rows (rank-averaged LGBM+XGB+CatBoost)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.258730\n1  101     0.522222\n2  102     0.433333\n3  103     0.753968\n4  104     0.538095",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.258730</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.522222</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.433333</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.753968</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.538095</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "cd21b517-1fc3-40c5-8497-d66921e11a6e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost variant (depth=3) + 4-model rank-ensemble (LGBM, XGB, CB_d4, CB_d3)\n",
        "import numpy as np, time, pandas as pd\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from iterstrat.ml_stratifiers import RepeatedMultilabelStratifiedKFold\n",
        "\n",
        "SEED = 42\n",
        "N = X_all_np.shape[0]\n",
        "C = Y_aligned.shape[1]\n",
        "\n",
        "oof_cb2 = np.zeros((N, C), dtype=np.float32)\n",
        "oof_valid_mask_cb2 = np.zeros((N, C), dtype=np.uint8)\n",
        "test_cb2_folds = []\n",
        "\n",
        "rmskf = RepeatedMultilabelStratifiedKFold(n_splits=5, n_repeats=2, random_state=SEED)\n",
        "fold_id = 0\n",
        "for tr_idx, va_idx in rmskf.split(X_all_np, Y_aligned):\n",
        "    t0 = time.time()\n",
        "    print(f'=== CB2 Fold {fold_id} ===', flush=True)\n",
        "    X_tr, X_va = X_all_np[tr_idx], X_all_np[va_idx]\n",
        "    y_tr, y_va = Y_aligned[tr_idx], Y_aligned[va_idx]\n",
        "    te_pred_cb2 = np.zeros((T_all_np.shape[0], C), dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        y_tr_c = y_tr[:, c]; y_va_c = y_va[:, c]\n",
        "        tr_pos = int((y_tr_c == 1).sum()); tr_neg = int((y_tr_c == 0).sum())\n",
        "        va_pos = int((y_va_c == 1).sum()); va_neg = int((y_va_c == 0).sum())\n",
        "        if not (tr_pos > 0 and tr_neg > 0 and va_pos > 0 and va_neg > 0):\n",
        "            continue\n",
        "        train_pool = Pool(X_tr, label=y_tr_c)\n",
        "        valid_pool = Pool(X_va, label=y_va_c)\n",
        "        cb2 = CatBoostClassifier(\n",
        "            loss_function='Logloss',\n",
        "            depth=3,\n",
        "            learning_rate=0.03,\n",
        "            iterations=5000,\n",
        "            l2_leaf_reg=20.0,\n",
        "            auto_class_weights='Balanced',\n",
        "            random_seed=SEED,\n",
        "            early_stopping_rounds=250,\n",
        "            verbose=False,\n",
        "            task_type='CPU',\n",
        "            thread_count=-1\n",
        "        )\n",
        "        cb2.fit(train_pool, eval_set=valid_pool)\n",
        "        oof_cb2[va_idx, c] = cb2.predict_proba(X_va)[:, 1]\n",
        "        te_pred_cb2[:, c] = cb2.predict_proba(T_all_np)[:, 1]\n",
        "        oof_valid_mask_cb2[va_idx, c] = 1\n",
        "    test_cb2_folds.append(te_pred_cb2)\n",
        "    fold_mask_cb2 = (oof_valid_mask_cb2[va_idx] > 0).astype(np.uint8)\n",
        "    m_cb2, _ = masked_auc_macro(y_va, oof_cb2[va_idx], fold_mask_cb2)\n",
        "    print(f'  CB2 Fold {fold_id} masked macro: {m_cb2:.4f} | time={time.time()-t0:.1f}s', flush=True)\n",
        "    fold_id += 1\n",
        "\n",
        "macro_cb2, _ = masked_auc_macro(Y_aligned, oof_cb2, oof_valid_mask_cb2)\n",
        "print(f'Global masked OOF macro: CatBoost(depth=3)={macro_cb2:.4f}')\n",
        "\n",
        "# Rank-ensemble 4 models: LGBM, XGB, CB(depth=4)=oof_cb, CB(depth=3)=oof_cb2\n",
        "valid_inter_4 = (oof_valid_mask_lgb.astype(bool) & oof_valid_mask_xgb.astype(bool) & oof_valid_mask_cb.astype(bool) & oof_valid_mask_cb2.astype(bool)).astype(np.uint8)\n",
        "o_lgb_r = rank_cols_with_mask(oof_lgb, valid_inter_4)\n",
        "o_xgb_r = rank_cols_with_mask(oof_xgb, valid_inter_4)\n",
        "o_cb1_r = rank_cols_with_mask(oof_cb,  valid_inter_4)\n",
        "o_cb2_r = rank_cols_with_mask(oof_cb2, valid_inter_4)\n",
        "\n",
        "best_w = (0.2, 0.0, 0.4, 0.4)  # init guess (LGB, XGB, CB1, CB2)\n",
        "best_macro = -1.0\n",
        "grid = np.linspace(0.0, 1.0, 11)\n",
        "for wl in grid:\n",
        "    for wx in grid:\n",
        "        for wc1 in grid:\n",
        "            wc2 = 1.0 - wl - wx - wc1\n",
        "            if wc2 < 0 or wc2 > 1:\n",
        "                continue\n",
        "            blend = wl * o_lgb_r + wx * o_xgb_r + wc1 * o_cb1_r + wc2 * o_cb2_r\n",
        "            m, _ = masked_auc_macro(Y_aligned, blend, valid_inter_4)\n",
        "            if m > best_macro:\n",
        "                best_macro = m\n",
        "                best_w = (float(wl), float(wx), float(wc1), float(wc2))\n",
        "print(f'Best 4-model weights (LGBM, XGB, CBd4, CBd3)={best_w} | masked OOF macro={best_macro:.4f}')\n",
        "\n",
        "# Build final submission with 4-model weights\n",
        "test_cb = np.mean(test_cb_folds, axis=0)\n",
        "test_cb2 = np.mean(test_cb2_folds, axis=0)\n",
        "test_lgb = np.mean(test_lgb_folds, axis=0)\n",
        "test_xgb = np.mean(test_xgb_folds, axis=0)\n",
        "t_lgb_r = rank_cols(test_lgb); t_xgb_r = rank_cols(test_xgb); t_cb1_r = rank_cols(test_cb); t_cb2_r = rank_cols(test_cb2)\n",
        "wl, wx, wc1, wc2 = best_w\n",
        "test_blend_rank4 = wl * t_lgb_r + wx * t_xgb_r + wc1 * t_cb1_r + wc2 * t_cb2_r\n",
        "\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_blend_rank4[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f'Wrote submission.csv with {len(sub_out)} rows (4-model rank ensemble)')\n",
        "display(sub_out.head())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB2 Fold 0 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB2 Fold 0 masked macro: 0.8004 | time=33.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB2 Fold 1 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB2 Fold 1 masked macro: 0.7753 | time=21.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB2 Fold 2 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB2 Fold 2 masked macro: 0.7896 | time=18.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB2 Fold 3 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB2 Fold 3 masked macro: 0.8137 | time=28.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB2 Fold 4 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB2 Fold 4 masked macro: 0.8453 | time=18.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB2 Fold 5 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB2 Fold 5 masked macro: 0.7454 | time=25.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB2 Fold 6 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB2 Fold 6 masked macro: 0.7709 | time=25.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB2 Fold 7 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB2 Fold 7 masked macro: 0.8051 | time=23.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB2 Fold 8 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB2 Fold 8 masked macro: 0.8114 | time=25.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB2 Fold 9 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB2 Fold 9 masked macro: 0.7387 | time=30.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global masked OOF macro: CatBoost(depth=3)=0.8079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best 4-model weights (LGBM, XGB, CBd4, CBd3)=(0.1, 0.0, 0.0, 0.9) | masked OOF macro=0.8107\nWrote submission.csv with 1216 rows (4-model rank ensemble)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.252381\n1  101     0.534921\n2  102     0.434921\n3  103     0.753968\n4  104     0.549206",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.252381</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.534921</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.434921</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.753968</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.549206</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "3265ae8e-849f-45b6-b7fe-d2edeabe304c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost variant 3 (depth=4, lr=0.02, l2=20, seed=123) + CB-only 3-model rank ensemble\n",
        "import numpy as np, time, pandas as pd\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from iterstrat.ml_stratifiers import RepeatedMultilabelStratifiedKFold\n",
        "\n",
        "SEED3 = 123\n",
        "N = X_all_np.shape[0]\n",
        "C = Y_aligned.shape[1]\n",
        "\n",
        "oof_cb3 = np.zeros((N, C), dtype=np.float32)\n",
        "oof_valid_mask_cb3 = np.zeros((N, C), dtype=np.uint8)\n",
        "test_cb3_folds = []\n",
        "\n",
        "rmskf = RepeatedMultilabelStratifiedKFold(n_splits=5, n_repeats=2, random_state=SEED3)\n",
        "fold_id = 0\n",
        "for tr_idx, va_idx in rmskf.split(X_all_np, Y_aligned):\n",
        "    t0 = time.time()\n",
        "    print(f'=== CB3 Fold {fold_id} ===', flush=True)\n",
        "    X_tr, X_va = X_all_np[tr_idx], X_all_np[va_idx]\n",
        "    y_tr, y_va = Y_aligned[tr_idx], Y_aligned[va_idx]\n",
        "    te_pred_cb3 = np.zeros((T_all_np.shape[0], C), dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        y_tr_c = y_tr[:, c]; y_va_c = y_va[:, c]\n",
        "        tr_pos = int((y_tr_c == 1).sum()); tr_neg = int((y_tr_c == 0).sum())\n",
        "        va_pos = int((y_va_c == 1).sum()); va_neg = int((y_va_c == 0).sum())\n",
        "        if not (tr_pos > 0 and tr_neg > 0 and va_pos > 0 and va_neg > 0):\n",
        "            continue\n",
        "        train_pool = Pool(X_tr, label=y_tr_c)\n",
        "        valid_pool = Pool(X_va, label=y_va_c)\n",
        "        cb3 = CatBoostClassifier(\n",
        "            loss_function='Logloss',\n",
        "            depth=4,\n",
        "            learning_rate=0.02,\n",
        "            iterations=5000,\n",
        "            l2_leaf_reg=20.0,\n",
        "            auto_class_weights='Balanced',\n",
        "            random_seed=SEED3,\n",
        "            early_stopping_rounds=250,\n",
        "            verbose=False,\n",
        "            task_type='CPU',\n",
        "            thread_count=-1\n",
        "        )\n",
        "        cb3.fit(train_pool, eval_set=valid_pool)\n",
        "        oof_cb3[va_idx, c] = cb3.predict_proba(X_va)[:, 1]\n",
        "        te_pred_cb3[:, c] = cb3.predict_proba(T_all_np)[:, 1]\n",
        "        oof_valid_mask_cb3[va_idx, c] = 1\n",
        "    test_cb3_folds.append(te_pred_cb3)\n",
        "    fold_mask_cb3 = (oof_valid_mask_cb3[va_idx] > 0).astype(np.uint8)\n",
        "    m_cb3, _ = masked_auc_macro(y_va, oof_cb3[va_idx], fold_mask_cb3)\n",
        "    print(f'  CB3 Fold {fold_id} masked macro: {m_cb3:.4f} | time={time.time()-t0:.1f}s', flush=True)\n",
        "    fold_id += 1\n",
        "\n",
        "macro_cb3, _ = masked_auc_macro(Y_aligned, oof_cb3, oof_valid_mask_cb3)\n",
        "print(f'Global masked OOF macro: CatBoost(depth=4, lr=0.02, l2=20, seed=123)={macro_cb3:.4f}')\n",
        "\n",
        "# Rank-ensemble CB-only (CB1=oof_cb, CB2=oof_cb2, CB3=oof_cb3) using intersection mask\n",
        "valid_inter_cb = (oof_valid_mask_cb.astype(bool) & oof_valid_mask_cb2.astype(bool) & oof_valid_mask_cb3.astype(bool)).astype(np.uint8)\n",
        "o_cb1_r = rank_cols_with_mask(oof_cb,  valid_inter_cb)\n",
        "o_cb2_r = rank_cols_with_mask(oof_cb2, valid_inter_cb)\n",
        "o_cb3_r = rank_cols_with_mask(oof_cb3, valid_inter_cb)\n",
        "\n",
        "best_w = (1/3, 1/3, 1/3)\n",
        "best_macro = -1.0\n",
        "grid = np.linspace(0.0, 1.0, 11)\n",
        "for w1 in grid:\n",
        "    for w2 in grid:\n",
        "        w3 = 1.0 - w1 - w2\n",
        "        if w3 < 0 or w3 > 1:\n",
        "            continue\n",
        "        blend = w1 * o_cb1_r + w2 * o_cb2_r + w3 * o_cb3_r\n",
        "        m, _ = masked_auc_macro(Y_aligned, blend, valid_inter_cb)\n",
        "        if m > best_macro:\n",
        "            best_macro = m\n",
        "            best_w = (float(w1), float(w2), float(w3))\n",
        "print(f'Best CB-only weights (CBd4_s42, CBd3_s42, CBd4_s123)={best_w} | masked OOF macro={best_macro:.4f}')\n",
        "\n",
        "# Build final submission with CB-only weights\n",
        "test_cb1 = np.mean(test_cb_folds, axis=0)\n",
        "test_cb2 = np.mean(test_cb2_folds, axis=0)\n",
        "test_cb3 = np.mean(test_cb3_folds, axis=0)\n",
        "t_cb1_r = rank_cols(test_cb1); t_cb2_r = rank_cols(test_cb2); t_cb3_r = rank_cols(test_cb3)\n",
        "w1, w2, w3 = best_w\n",
        "test_blend_rank_cb = w1 * t_cb1_r + w2 * t_cb2_r + w3 * t_cb3_r\n",
        "\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_blend_rank_cb[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f'Wrote submission.csv with {len(sub_out)} rows (CatBoost-only 3-model rank ensemble)')\n",
        "display(sub_out.head())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB3 Fold 0 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB3 Fold 0 masked macro: 0.7633 | time=21.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB3 Fold 1 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB3 Fold 1 masked macro: 0.8363 | time=27.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB3 Fold 2 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB3 Fold 2 masked macro: 0.8168 | time=58.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB3 Fold 3 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB3 Fold 3 masked macro: 0.7522 | time=32.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB3 Fold 4 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB3 Fold 4 masked macro: 0.8264 | time=15.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB3 Fold 5 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB3 Fold 5 masked macro: 0.8126 | time=27.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB3 Fold 6 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB3 Fold 6 masked macro: 0.8322 | time=43.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB3 Fold 7 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB3 Fold 7 masked macro: 0.8047 | time=34.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB3 Fold 8 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB3 Fold 8 masked macro: 0.8143 | time=35.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CB3 Fold 9 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CB3 Fold 9 masked macro: 0.8080 | time=33.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global masked OOF macro: CatBoost(depth=4, lr=0.02, l2=20, seed=123)=0.8109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best CB-only weights (CBd4_s42, CBd3_s42, CBd4_s123)=(0.30000000000000004, 0.1, 0.6) | masked OOF macro=0.8365\nWrote submission.csv with 1216 rows (CatBoost-only 3-model rank ensemble)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.246032\n1  101     0.538095\n2  102     0.417460\n3  103     0.753968\n4  104     0.523810",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.246032</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.538095</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.417460</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.753968</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.523810</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "f2c9baae-08f5-4084-a60b-6bccde625a88",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Official CV (CVfolds_2) + CatBoost seed bag (equal-weight rank avg), no global MI, no weight tuning\n",
        "import numpy as np, pandas as pd, time\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "# Prepare matrices from full engineered features (no global MI)\n",
        "X_tr_full = X_train_df.values.astype(np.float32)\n",
        "X_te_full = X_test_df.values.astype(np.float32)\n",
        "Y_full = Y_aligned.astype(np.float32)\n",
        "N, C = X_tr_full.shape[0], Y_full.shape[1]\n",
        "\n",
        "# Official folds mapping for the labeled train ids\n",
        "fold_series = pd.Series(pd.Index(sorted(train_rec_ids))).map(fold_map).astype('Int64')\n",
        "fold_series = fold_series.fillna(-1).astype(int)\n",
        "folds_present = sorted([f for f in np.unique(fold_series.values) if f >= 0])\n",
        "print('Folds present among labeled train ids:', folds_present, flush=True)\n",
        "\n",
        "# Define CatBoost variants (diverse, strong regularization)\n",
        "variants = [\n",
        "    dict(name='cb_d3_lr003_l2_20_rsm06_s42', depth=3, learning_rate=0.03, l2_leaf_reg=20.0, rsm=0.6, subsample=0.8, random_strength=0.8, seed=42),\n",
        "    dict(name='cb_d4_lr0025_l2_15_rsm06_s123', depth=4, learning_rate=0.025, l2_leaf_reg=15.0, rsm=0.6, subsample=0.8, random_strength=0.8, seed=123),\n",
        "    dict(name='cb_d4_lr002_l2_25_rsm05_s456', depth=4, learning_rate=0.02, l2_leaf_reg=25.0, rsm=0.5, subsample=0.8, random_strength=1.0, seed=456),\n",
        "    dict(name='cb_d3_lr0025_l2_25_rsm07_s789', depth=3, learning_rate=0.025, l2_leaf_reg=25.0, rsm=0.7, subsample=0.8, random_strength=0.6, seed=789),\n",
        "]\n",
        "\n",
        "oof_list = []\n",
        "mask_list = []\n",
        "test_pred_list = []\n",
        "\n",
        "def masked_auc_macro(y_true, y_pred, valid_mask):\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    C = y_true.shape[1]\n",
        "    aucs = []\n",
        "    for c in range(C):\n",
        "        m = valid_mask[:, c].astype(bool)\n",
        "        if m.sum() <= 1:\n",
        "            continue\n",
        "        yt = y_true[m, c]\n",
        "        yp = y_pred[m, c]\n",
        "        if yt.max() == yt.min():\n",
        "            continue\n",
        "        try:\n",
        "            aucs.append(roc_auc_score(yt, yp))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "\n",
        "def rank_cols_with_mask(mat, valid_mask):\n",
        "    from scipy.stats import rankdata\n",
        "    N, C = mat.shape\n",
        "    out = np.full((N, C), np.nan, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        mask = valid_mask[:, c].astype(bool)\n",
        "        if not mask.any():\n",
        "            continue\n",
        "        col = mat[mask, c]\n",
        "        r = rankdata(col, method='average')\n",
        "        r = (r - 1) / (len(r) - 1) if len(r) > 1 else r*0.0\n",
        "        out[mask, c] = r.astype(np.float32)\n",
        "    return out\n",
        "\n",
        "def rank_cols(mat):\n",
        "    from scipy.stats import rankdata\n",
        "    N, C = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (N - 1) if N > 1 else r*0.0\n",
        "    return out\n",
        "\n",
        "for v in variants:\n",
        "    print(f\"=== Variant {v['name']} ===\", flush=True)\n",
        "    oof = np.zeros((N, C), dtype=np.float32)\n",
        "    vmask = np.zeros((N, C), dtype=np.uint8)\n",
        "    test_folds = []\n",
        "    for fold in [0, 1]:\n",
        "        trn_mask = (fold_series.values != fold)\n",
        "        val_mask = (fold_series.values == fold)\n",
        "        if trn_mask.sum() == 0 or val_mask.sum() == 0:\n",
        "            print(f'  Skip fold {fold}: trn={trn_mask.sum()}, val={val_mask.sum()}', flush=True)\n",
        "            continue\n",
        "        X_tr, X_va = X_tr_full[trn_mask], X_tr_full[val_mask]\n",
        "        y_tr, y_va = Y_full[trn_mask], Y_full[val_mask]\n",
        "        te_pred = np.zeros((X_te_full.shape[0], C), dtype=np.float32)\n",
        "        t0 = time.time()\n",
        "        for c in range(C):\n",
        "            y_tr_c = y_tr[:, c]; y_va_c = y_va[:, c]\n",
        "            tr_pos = int((y_tr_c == 1).sum()); tr_neg = int((y_tr_c == 0).sum())\n",
        "            va_pos = int((y_va_c == 1).sum()); va_neg = int((y_va_c == 0).sum())\n",
        "            if not (tr_pos > 0 and tr_neg > 0 and va_pos > 0 and va_neg > 0):\n",
        "                continue\n",
        "            train_pool = Pool(X_tr, label=y_tr_c)\n",
        "            valid_pool = Pool(X_va, label=y_va_c)\n",
        "            cb = CatBoostClassifier(\n",
        "                loss_function='Logloss',\n",
        "                depth=v['depth'],\n",
        "                learning_rate=v['learning_rate'],\n",
        "                iterations=5000,\n",
        "                l2_leaf_reg=v['l2_leaf_reg'],\n",
        "                rsm=v['rsm'],\n",
        "                subsample=v['subsample'],\n",
        "                random_strength=v['random_strength'],\n",
        "                auto_class_weights='Balanced',\n",
        "                random_seed=v['seed'],\n",
        "                early_stopping_rounds=250,\n",
        "                verbose=False,\n",
        "                task_type='CPU',\n",
        "                thread_count=-1\n",
        "            )\n",
        "            cb.fit(train_pool, eval_set=valid_pool)\n",
        "            oof[val_mask, c] = cb.predict_proba(X_va)[:, 1]\n",
        "            te_pred[:, c] = cb.predict_proba(X_te_full)[:, 1]\n",
        "            vmask[val_mask, c] = 1\n",
        "        fold_macro = masked_auc_macro(y_va, oof[val_mask], vmask[val_mask])\n",
        "        print(f\"  Fold {fold} masked macro: {fold_macro:.4f} | time={time.time()-t0:.1f}s\", flush=True)\n",
        "        test_folds.append(te_pred)\n",
        "    test_mean = np.mean(test_folds, axis=0) if len(test_folds) else np.zeros((X_te_full.shape[0], C), dtype=np.float32)\n",
        "    oof_list.append(oof); mask_list.append(vmask); test_pred_list.append(test_mean)\n",
        "    macro = masked_auc_macro(Y_full, oof, vmask)\n",
        "    print(f\"Variant OOF masked macro: {macro:.4f}\", flush=True)\n",
        "\n",
        "# Equal-weight rank ensemble across variants\n",
        "inter_mask = mask_list[0].astype(bool)\n",
        "for m in mask_list[1:]:\n",
        "    inter_mask &= m.astype(bool)\n",
        "inter_mask = inter_mask.astype(np.uint8)\n",
        "ranked_oofs = [rank_cols_with_mask(o, inter_mask) for o in oof_list]\n",
        "oof_ens = np.mean(ranked_oofs, axis=0)\n",
        "macro_ens = masked_auc_macro(Y_full, oof_ens, inter_mask)\n",
        "print(f'Ensemble (equal-weight) masked OOF macro: {macro_ens:.4f}', flush=True)\n",
        "\n",
        "# Build test ensemble submission (equal-weight rank avg)\n",
        "ranked_tests = [rank_cols(tp) for tp in test_pred_list]\n",
        "test_ens = np.mean(ranked_tests, axis=0)\n",
        "\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_ens[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (Official 2-fold CV, CB seed-bag equal-weight ranks)\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folds present among labeled train ids: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_d3_lr003_l2_20_rsm06_s42 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Skip fold 0: trn=0, val=145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Skip fold 1: trn=145, val=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_d4_lr0025_l2_15_rsm06_s123 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Skip fold 0: trn=0, val=145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Skip fold 1: trn=145, val=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_d4_lr002_l2_25_rsm05_s456 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Skip fold 0: trn=0, val=145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Skip fold 1: trn=145, val=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_d3_lr0025_l2_25_rsm07_s789 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Skip fold 0: trn=0, val=145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Skip fold 1: trn=145, val=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble (equal-weight) masked OOF macro: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with 1216 rows (Official 2-fold CV, CB seed-bag equal-weight ranks)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100          0.5\n1  101          0.5\n2  102          0.5\n3  103          0.5\n4  104          0.5",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "0b52d88e-106e-4295-98f9-808e71390279",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Full-train CatBoost seed bag (no CV), equal-weight rank average, no global MI\n",
        "import numpy as np, pandas as pd, time\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "# Use full engineered features (no MI) from X_train_df/X_test_df\n",
        "X_full = X_train_df.values.astype(np.float32)\n",
        "T_full = X_test_df.values.astype(np.float32)\n",
        "Y_full = Y_aligned.astype(np.float32)\n",
        "N, C = X_full.shape[0], Y_full.shape[1]\n",
        "\n",
        "# Diverse CatBoost variants (bagging by seeds/params)\n",
        "variants = [\n",
        "    dict(name='cb_full_d3_lr003_l2_20_rsm06_s42',  depth=3, learning_rate=0.03,  l2_leaf_reg=20.0, rsm=0.6, subsample=0.8, random_strength=0.8, seed=42),\n",
        "    dict(name='cb_full_d4_lr0025_l2_15_rsm06_s123', depth=4, learning_rate=0.025, l2_leaf_reg=15.0, rsm=0.6, subsample=0.8, random_strength=0.8, seed=123),\n",
        "    dict(name='cb_full_d4_lr002_l2_25_rsm05_s456', depth=4, learning_rate=0.02,  l2_leaf_reg=25.0, rsm=0.5, subsample=0.8, random_strength=1.0, seed=456),\n",
        "    dict(name='cb_full_d3_lr0025_l2_25_rsm07_s789',depth=3, learning_rate=0.025, l2_leaf_reg=25.0, rsm=0.7, subsample=0.8, random_strength=0.6, seed=789),\n",
        "    dict(name='cb_full_d3_lr002_l2_30_rsm06_s321', depth=3, learning_rate=0.02,  l2_leaf_reg=30.0, rsm=0.6, subsample=0.85, random_strength=0.9, seed=321),\n",
        "]\n",
        "\n",
        "def rank_cols(mat):\n",
        "    from scipy.stats import rankdata\n",
        "    N, C = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (N - 1) if N > 1 else r*0.0\n",
        "    return out\n",
        "\n",
        "test_preds = []\n",
        "for v in variants:\n",
        "    print(f\"=== Train variant {v['name']} on FULL train ===\", flush=True)\n",
        "    t0 = time.time()\n",
        "    te_pred = np.zeros((T_full.shape[0], C), dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        y_c = Y_full[:, c]\n",
        "        pos = int((y_c == 1).sum()); neg = int((y_c == 0).sum())\n",
        "        if not (pos > 0 and neg > 0):\n",
        "            continue\n",
        "        tr_pool = Pool(X_full, label=y_c)\n",
        "        cb = CatBoostClassifier(\n",
        "            loss_function='Logloss',\n",
        "            depth=v['depth'],\n",
        "            learning_rate=v['learning_rate'],\n",
        "            iterations=5000,\n",
        "            l2_leaf_reg=v['l2_leaf_reg'],\n",
        "            rsm=v['rsm'],\n",
        "            subsample=v['subsample'],\n",
        "            random_strength=v['random_strength'],\n",
        "            auto_class_weights='Balanced',\n",
        "            random_seed=v['seed'],\n",
        "            verbose=False,\n",
        "            task_type='CPU',\n",
        "            thread_count=-1\n",
        "        )\n",
        "        # Fit on full data; no eval_set to avoid using any CV fold\n",
        "        cb.fit(tr_pool)\n",
        "        te_pred[:, c] = cb.predict_proba(T_full)[:, 1]\n",
        "    test_preds.append(te_pred)\n",
        "    print(f\"  Done {v['name']} in {time.time()-t0:.1f}s\", flush=True)\n",
        "\n",
        "# Equal-weight rank-average across variants\n",
        "ranked_tests = [rank_cols(tp) for tp in test_preds]\n",
        "test_ens = np.mean(ranked_tests, axis=0)\n",
        "\n",
        "# Build submission\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_ens[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (Full-train CatBoost seed bag, equal-weight rank avg)\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Train variant cb_full_d3_lr003_l2_20_rsm06_s42 on FULL train ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Done cb_full_d3_lr003_l2_20_rsm06_s42 in 275.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Train variant cb_full_d4_lr0025_l2_15_rsm06_s123 on FULL train ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Done cb_full_d4_lr0025_l2_15_rsm06_s123 in 370.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Train variant cb_full_d4_lr002_l2_25_rsm05_s456 on FULL train ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Done cb_full_d4_lr002_l2_25_rsm05_s456 in 349.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Train variant cb_full_d3_lr0025_l2_25_rsm07_s789 on FULL train ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Done cb_full_d3_lr0025_l2_25_rsm07_s789 in 283.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Train variant cb_full_d3_lr002_l2_30_rsm06_s321 on FULL train ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Done cb_full_d3_lr002_l2_30_rsm06_s321 in 274.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with 1216 rows (Full-train CatBoost seed bag, equal-weight rank avg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.385714\n1  101     0.601587\n2  102     0.658730\n3  103     0.753968\n4  104     0.753968",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.385714</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.601587</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.658730</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.753968</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.753968</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "a5dcc7c1-347e-46e2-9d00-25ab684211f6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Adversarial validation to drop shifted features + CB RMSKF bag (equal weights) and submit\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from iterstrat.ml_stratifiers import RepeatedMultilabelStratifiedKFold\n",
        "\n",
        "def rank_cols(mat):\n",
        "    from scipy.stats import rankdata\n",
        "    N, C = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (N - 1) if N > 1 else r*0.0\n",
        "    return out\n",
        "\n",
        "# 1) Build adversarial dataset (train vs test) on current engineered features (697 cols)\n",
        "X_tr_full = X_train_df.copy()\n",
        "X_te_full = X_test_df.copy()\n",
        "X_adv = pd.concat([X_tr_full.assign(is_test=0), X_te_full.assign(is_test=1)], axis=0).reset_index(drop=True)\n",
        "y_adv = X_adv['is_test'].values.astype(int)\n",
        "X_adv = X_adv.drop(columns=['is_test']).values.astype(np.float32)\n",
        "\n",
        "# 2) Train a quick LGBM classifier with CV to get feature importances\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "feat_importance = np.zeros(X_adv.shape[1], dtype=np.float64)\n",
        "for fi, (tr, va) in enumerate(skf.split(X_adv, y_adv)):\n",
        "    Xtr, Xva = X_adv[tr], X_adv[va]\n",
        "    ytr, yva = y_adv[tr], y_adv[va]\n",
        "    clf = lgb.LGBMClassifier(\n",
        "        objective='binary',\n",
        "        n_estimators=1000, learning_rate=0.05,\n",
        "        max_depth=3, num_leaves=7,\n",
        "        subsample=0.8, colsample_bytree=0.6,\n",
        "        reg_alpha=0.5, reg_lambda=5.0,\n",
        "        n_jobs=-1, random_state=42, verbosity=-1\n",
        "    )\n",
        "    clf.fit(Xtr, ytr, eval_set=[(Xva, yva)], eval_metric='auc', callbacks=[lgb.early_stopping(100, verbose=False), lgb.log_evaluation(period=0)])\n",
        "    feat_importance += clf.booster_.feature_importance(importance_type='gain').astype(np.float64)\n",
        "\n",
        "feat_importance /= max(1, skf.get_n_splits())\n",
        "cols = X_train_df.columns.tolist()\n",
        "order = np.argsort(-feat_importance)  # descending\n",
        "drop_top = 40\n",
        "drop_cols = [cols[i] for i in order[:min(drop_top, len(cols))]]\n",
        "keep_cols = [c for c in cols if c not in drop_cols]\n",
        "print(f'Adversarial validation: dropping top {len(drop_cols)} drifted features; keeping {len(keep_cols)}', flush=True)\n",
        "\n",
        "# Reduced features\n",
        "X_tr_red = X_train_df[keep_cols].values.astype(np.float32)\n",
        "X_te_red = X_test_df[keep_cols].values.astype(np.float32)\n",
        "Y_mat = Y_aligned.astype(np.float32)\n",
        "N, C = X_tr_red.shape[0], Y_mat.shape[1]\n",
        "\n",
        "# 3) Train 2 CatBoost variants with RepeatedMultilabelStratifiedKFold (5x2), equal-weight rank-avg\n",
        "rmskf = RepeatedMultilabelStratifiedKFold(n_splits=5, n_repeats=2, random_state=42)\n",
        "variants = [\n",
        "    dict(name='cb_rmskf_d3_lr0025_l2_25_rsm06_s42', depth=3, learning_rate=0.025, l2_leaf_reg=25.0, rsm=0.6, subsample=0.8, random_strength=0.8, seed=42),\n",
        "    dict(name='cb_rmskf_d4_lr002_l2_20_rsm05_s123', depth=4, learning_rate=0.02,  l2_leaf_reg=20.0, rsm=0.5, subsample=0.8, random_strength=1.0, seed=123),\n",
        "]\n",
        "\n",
        "test_pred_list = []\n",
        "for v in variants:\n",
        "    print(f\"=== Variant {v['name']} (RMSKF 5x2) ===\", flush=True)\n",
        "    fold_id = 0\n",
        "    test_folds = []\n",
        "    for tr_idx, va_idx in rmskf.split(X_tr_red, Y_mat):\n",
        "        t0 = time.time()\n",
        "        Xtr, Xva = X_tr_red[tr_idx], X_tr_red[va_idx]\n",
        "        ytr, yva = Y_mat[tr_idx], Y_mat[va_idx]\n",
        "        te_pred = np.zeros((X_te_red.shape[0], C), dtype=np.float32)\n",
        "        for c in range(C):\n",
        "            ytr_c = ytr[:, c]; yva_c = yva[:, c]\n",
        "            tr_pos = int((ytr_c == 1).sum()); tr_neg = int((ytr_c == 0).sum())\n",
        "            va_pos = int((yva_c == 1).sum()); va_neg = int((yva_c == 0).sum())\n",
        "            if not (tr_pos > 0 and tr_neg > 0 and va_pos > 0 and va_neg > 0):\n",
        "                continue\n",
        "            tr_pool = Pool(Xtr, label=ytr_c)\n",
        "            va_pool = Pool(Xva, label=yva_c)\n",
        "            cb = CatBoostClassifier(\n",
        "                loss_function='Logloss',\n",
        "                depth=v['depth'],\n",
        "                learning_rate=v['learning_rate'],\n",
        "                iterations=5000,\n",
        "                l2_leaf_reg=v['l2_leaf_reg'],\n",
        "                rsm=v['rsm'],\n",
        "                subsample=v['subsample'],\n",
        "                random_strength=v['random_strength'],\n",
        "                auto_class_weights='Balanced',\n",
        "                random_seed=v['seed'],\n",
        "                early_stopping_rounds=250,\n",
        "                verbose=False,\n",
        "                task_type='CPU',\n",
        "                thread_count=-1\n",
        "            )\n",
        "            cb.fit(tr_pool, eval_set=va_pool)\n",
        "            te_pred[:, c] = cb.predict_proba(X_te_red)[:, 1]\n",
        "        test_folds.append(te_pred)\n",
        "        print(f\"  Fold {fold_id} done in {time.time()-t0:.1f}s\", flush=True)\n",
        "        fold_id += 1\n",
        "    test_mean = np.mean(test_folds, axis=0)\n",
        "    test_pred_list.append(test_mean)\n",
        "\n",
        "# 4) Equal-weight rank averaging across variants and write submission\n",
        "ranked_tests = [rank_cols(tp) for tp in test_pred_list]\n",
        "test_ens = np.mean(ranked_tests, axis=0)\n",
        "\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_ens[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (Adv-validated feature drop {len(drop_cols)}, 2x CB RMSKF equal-weight rank avg)\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial validation: dropping top 40 drifted features; keeping 657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_rmskf_d3_lr0025_l2_25_rsm06_s42 (RMSKF 5x2) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0 done in 88.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1 done in 20.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2 done in 29.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3 done in 53.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4 done in 31.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 5 done in 33.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 6 done in 31.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 7 done in 37.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 8 done in 29.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 9 done in 49.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_rmskf_d4_lr002_l2_20_rsm05_s123 (RMSKF 5x2) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0 done in 92.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1 done in 26.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2 done in 36.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3 done in 90.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4 done in 42.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 5 done in 47.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 6 done in 42.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 7 done in 73.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 8 done in 58.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 9 done in 62.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with 1216 rows (Adv-validated feature drop 40, 2x CB RMSKF equal-weight rank avg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.246032\n1  101     0.531746\n2  102     0.412698\n3  103     0.753968\n4  104     0.539683",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.246032</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.531746</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.412698</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.753968</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.539683</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "6344b736-851f-4c63-a536-b3883e67f21d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Add HOG spectrogram features (filtered_spectrograms) and train CatBoost RMSKF bag (equal-weight ranks)\n",
        "import numpy as np, pandas as pd, time, os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from skimage.feature import hog\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from iterstrat.ml_stratifiers import RepeatedMultilabelStratifiedKFold\n",
        "\n",
        "VIEW_DIR = SUP/'filtered_spectrograms'\n",
        "HOG_CACHE_T = Path('hog_train.npy')\n",
        "HOG_CACHE_S = Path('hog_test.npy')\n",
        "\n",
        "def load_bmp(stem):\n",
        "    p = VIEW_DIR/f\"{stem}.bmp\"\n",
        "    img = Image.open(p).convert('L')\n",
        "    return img\n",
        "\n",
        "def compute_hog_for_ids(id_index):\n",
        "    feats = []\n",
        "    for rid in id_index:\n",
        "        stem = id2fn[int(rid)]\n",
        "        img = load_bmp(stem)\n",
        "        img = img.resize((256,256))\n",
        "        arr = np.asarray(img, dtype=np.float32) / 255.0\n",
        "        f = hog(arr, orientations=9, pixels_per_cell=(16,16), cells_per_block=(2,2), block_norm='L2-Hys', feature_vector=True)\n",
        "        feats.append(f.astype(np.float32))\n",
        "    return np.vstack(feats)\n",
        "\n",
        "# Train/test rec_id order aligned to existing matrices\n",
        "train_ids_sorted = pd.Index(sorted(train_rec_ids))\n",
        "test_ids_sorted = pd.Index(sorted(test_rec_ids))\n",
        "\n",
        "if HOG_CACHE_T.exists() and HOG_CACHE_S.exists():\n",
        "    hog_train = np.load(HOG_CACHE_T)\n",
        "    hog_test = np.load(HOG_CACHE_S)\n",
        "else:\n",
        "    t0=time.time()\n",
        "    hog_train = compute_hog_for_ids(train_ids_sorted)\n",
        "    hog_test = compute_hog_for_ids(test_ids_sorted)\n",
        "    np.save(HOG_CACHE_T, hog_train); np.save(HOG_CACHE_S, hog_test)\n",
        "    print(f'HOG computed in {time.time()-t0:.1f}s -> train {hog_train.shape}, test {hog_test.shape}', flush=True)\n",
        "\n",
        "# Concatenate HOG to engineered tabular features\n",
        "X_tr_aug = np.concatenate([X_train_df.values.astype(np.float32), hog_train], axis=1)\n",
        "X_te_aug = np.concatenate([X_test_df.values.astype(np.float32), hog_test], axis=1)\n",
        "Y_mat = Y_aligned.astype(np.float32)\n",
        "N, C = X_tr_aug.shape[0], Y_mat.shape[1]\n",
        "print('Augmented dims:', X_tr_aug.shape, X_te_aug.shape, flush=True)\n",
        "\n",
        "# RepeatedMultilabelStratifiedKFold CatBoost bag (2 variants), equal-weight rank avg\n",
        "rmskf = RepeatedMultilabelStratifiedKFold(n_splits=5, n_repeats=2, random_state=42)\n",
        "variants = [\n",
        "    dict(name='cb_hog_d3_lr0025_l2_25_rsm06_s42', depth=3, learning_rate=0.025, l2_leaf_reg=25.0, rsm=0.6, subsample=0.85, random_strength=0.8, seed=42),\n",
        "    dict(name='cb_hog_d4_lr002_l2_20_rsm05_s123', depth=4, learning_rate=0.02,  l2_leaf_reg=20.0, rsm=0.5, subsample=0.85, random_strength=1.0, seed=123),\n",
        "]\n",
        "\n",
        "def rank_cols(mat):\n",
        "    from scipy.stats import rankdata\n",
        "    Nn, Cc = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(Cc):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (Nn - 1) if Nn > 1 else r*0.0\n",
        "    return out\n",
        "\n",
        "test_pred_list = []\n",
        "for v in variants:\n",
        "    print(f\"=== Variant {v['name']} (RMSKF 5x2) ===\", flush=True)\n",
        "    test_folds = []\n",
        "    fold_id = 0\n",
        "    for tr_idx, va_idx in rmskf.split(X_tr_aug, Y_mat):\n",
        "        t0 = time.time()\n",
        "        Xtr, Xva = X_tr_aug[tr_idx], X_tr_aug[va_idx]\n",
        "        ytr, yva = Y_mat[tr_idx], Y_mat[va_idx]\n",
        "        te_pred = np.zeros((X_te_aug.shape[0], C), dtype=np.float32)\n",
        "        for c in range(C):\n",
        "            ytr_c = ytr[:, c]; yva_c = yva[:, c]\n",
        "            tr_pos = int((ytr_c == 1).sum()); tr_neg = int((ytr_c == 0).sum())\n",
        "            va_pos = int((yva_c == 1).sum()); va_neg = int((yva_c == 0).sum())\n",
        "            if not (tr_pos > 0 and tr_neg > 0 and va_pos > 0 and va_neg > 0):\n",
        "                continue\n",
        "            tr_pool = Pool(Xtr, label=ytr_c)\n",
        "            va_pool = Pool(Xva, label=yva_c)\n",
        "            cb = CatBoostClassifier(\n",
        "                loss_function='Logloss',\n",
        "                depth=v['depth'],\n",
        "                learning_rate=v['learning_rate'],\n",
        "                iterations=5000,\n",
        "                l2_leaf_reg=v['l2_leaf_reg'],\n",
        "                rsm=v['rsm'],\n",
        "                subsample=v['subsample'],\n",
        "                random_strength=v['random_strength'],\n",
        "                auto_class_weights='Balanced',\n",
        "                random_seed=v['seed'],\n",
        "                early_stopping_rounds=250,\n",
        "                verbose=False,\n",
        "                task_type='CPU',\n",
        "                thread_count=-1\n",
        "            )\n",
        "            cb.fit(tr_pool, eval_set=va_pool)\n",
        "            te_pred[:, c] = cb.predict_proba(X_te_aug)[:, 1]\n",
        "        test_folds.append(te_pred)\n",
        "        print(f\"  Fold {fold_id} done in {time.time()-t0:.1f}s\", flush=True)\n",
        "        fold_id += 1\n",
        "    test_mean = np.mean(test_folds, axis=0)\n",
        "    test_pred_list.append(test_mean)\n",
        "\n",
        "# Equal-weight rank-average and write submission\n",
        "ranked_tests = [rank_cols(tp) for tp in test_pred_list]\n",
        "test_ens = np.mean(ranked_tests, axis=0)\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_ens[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (HOG+tabular, 2x CB RMSKF equal-weight ranks)\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HOG computed in 2.9s -> train (145, 8100), test (64, 8100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmented dims: (145, 8797) (64, 8797)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_hog_d3_lr0025_l2_25_rsm06_s42 (RMSKF 5x2) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0 done in 401.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1 done in 226.4s\n"
          ]
        }
      ]
    },
    {
      "id": "19e90d73-9a96-4f32-b23d-9c6790e392b7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Full-train CatBoost seed bag on augmented (tabular + HOG) features, equal-weight rank average\n",
        "import numpy as np, pandas as pd, time\n",
        "from pathlib import Path\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "# Load cached HOG (from cell 17) or compute if missing\n",
        "HOG_CACHE_T = Path('hog_train.npy')\n",
        "HOG_CACHE_S = Path('hog_test.npy')\n",
        "if HOG_CACHE_T.exists() and HOG_CACHE_S.exists():\n",
        "    hog_train = np.load(HOG_CACHE_T)\n",
        "    hog_test = np.load(HOG_CACHE_S)\n",
        "else:\n",
        "    # Fallback: compute quickly (same routine as cell 17)\n",
        "    from PIL import Image\n",
        "    from skimage.feature import hog\n",
        "    VIEW_DIR = SUP/'filtered_spectrograms'\n",
        "    def load_bmp(stem):\n",
        "        p = VIEW_DIR/f\"{stem}.bmp\"\n",
        "        img = Image.open(p).convert('L')\n",
        "        return img\n",
        "    def compute_hog_for_ids(id_index):\n",
        "        feats = []\n",
        "        for rid in id_index:\n",
        "            stem = id2fn[int(rid)]\n",
        "            img = load_bmp(stem)\n",
        "            img = img.resize((256,256))\n",
        "            arr = np.asarray(img, dtype=np.float32) / 255.0\n",
        "            f = hog(arr, orientations=9, pixels_per_cell=(16,16), cells_per_block=(2,2), block_norm='L2-Hys', feature_vector=True)\n",
        "            feats.append(f.astype(np.float32))\n",
        "        return np.vstack(feats)\n",
        "    train_ids_sorted = pd.Index(sorted(train_rec_ids))\n",
        "    test_ids_sorted = pd.Index(sorted(test_rec_ids))\n",
        "    t0=time.time()\n",
        "    hog_train = compute_hog_for_ids(train_ids_sorted)\n",
        "    hog_test = compute_hog_for_ids(test_ids_sorted)\n",
        "    np.save(HOG_CACHE_T, hog_train); np.save(HOG_CACHE_S, hog_test)\n",
        "    print(f'HOG computed in {time.time()-t0:.1f}s -> train {hog_train.shape}, test {hog_test.shape}', flush=True)\n",
        "\n",
        "# Build augmented matrices\n",
        "X_tab = X_train_df.values.astype(np.float32)\n",
        "T_tab = X_test_df.values.astype(np.float32)\n",
        "X_full = np.concatenate([X_tab, hog_train], axis=1)\n",
        "T_full = np.concatenate([T_tab, hog_test], axis=1)\n",
        "Y_full = Y_aligned.astype(np.float32)\n",
        "N, D = X_full.shape[0], X_full.shape[1]\n",
        "C = Y_full.shape[1]\n",
        "print('Augmented dims (full-train):', X_full.shape, T_full.shape, flush=True)\n",
        "\n",
        "# CatBoost seed/param bag (CPU), strong regularization\n",
        "variants = [\n",
        "    dict(name='cb_aug_d3_lr003_l2_25_rsm06_s42',  depth=3, learning_rate=0.03,  l2_leaf_reg=25.0, rsm=0.6, subsample=0.85, random_strength=0.8, seed=42),\n",
        "    dict(name='cb_aug_d4_lr0025_l2_20_rsm06_s123', depth=4, learning_rate=0.025, l2_leaf_reg=20.0, rsm=0.6, subsample=0.85, random_strength=0.8, seed=123),\n",
        "    dict(name='cb_aug_d4_lr002_l2_30_rsm05_s456',  depth=4, learning_rate=0.02,  l2_leaf_reg=30.0, rsm=0.5, subsample=0.85, random_strength=1.0, seed=456),\n",
        "    dict(name='cb_aug_d3_lr0025_l2_30_rsm07_s789', depth=3, learning_rate=0.025, l2_leaf_reg=30.0, rsm=0.7, subsample=0.9,  random_strength=0.6, seed=789),\n",
        "]\n",
        "\n",
        "def rank_cols(mat):\n",
        "    from scipy.stats import rankdata\n",
        "    Nn, Cc = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(Cc):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (Nn - 1) if Nn > 1 else r*0.0\n",
        "    return out\n",
        "\n",
        "test_preds = []\n",
        "for v in variants:\n",
        "    print(f\"=== FULL-TRAIN {v['name']} ===\", flush=True)\n",
        "    t0 = time.time()\n",
        "    te_pred = np.zeros((T_full.shape[0], C), dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        y_c = Y_full[:, c]\n",
        "        pos = int((y_c == 1).sum()); neg = int((y_c == 0).sum())\n",
        "        if not (pos > 0 and neg > 0):\n",
        "            continue\n",
        "        tr_pool = Pool(X_full, label=y_c)\n",
        "        cb = CatBoostClassifier(\n",
        "            loss_function='Logloss',\n",
        "            depth=v['depth'],\n",
        "            learning_rate=v['learning_rate'],\n",
        "            iterations=5000,\n",
        "            l2_leaf_reg=v['l2_leaf_reg'],\n",
        "            rsm=v['rsm'],\n",
        "            subsample=v['subsample'],\n",
        "            random_strength=v['random_strength'],\n",
        "            auto_class_weights='Balanced',\n",
        "            random_seed=v['seed'],\n",
        "            verbose=False,\n",
        "            task_type='CPU',\n",
        "            thread_count=-1\n",
        "        )\n",
        "        cb.fit(tr_pool)\n",
        "        te_pred[:, c] = cb.predict_proba(T_full)[:, 1]\n",
        "    test_preds.append(te_pred)\n",
        "    print(f\"  Done in {time.time()-t0:.1f}s\", flush=True)\n",
        "\n",
        "# Equal-weight rank-average\n",
        "ranked_tests = [rank_cols(tp) for tp in test_preds]\n",
        "test_ens = np.mean(ranked_tests, axis=0)\n",
        "\n",
        "# Build submission\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_ens[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (Full-train CatBoost seed bag on augmented features, equal-weight rank avg)\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmented dims (full-train): (145, 8797) (64, 8797)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== FULL-TRAIN cb_aug_d3_lr003_l2_25_rsm06_s42 ===\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 92\u001b[39m\n\u001b[32m     76\u001b[39m     tr_pool = Pool(X_full, label=y_c)\n\u001b[32m     77\u001b[39m     cb = CatBoostClassifier(\n\u001b[32m     78\u001b[39m         loss_function=\u001b[33m'\u001b[39m\u001b[33mLogloss\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     79\u001b[39m         depth=v[\u001b[33m'\u001b[39m\u001b[33mdepth\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m         thread_count=-\u001b[32m1\u001b[39m\n\u001b[32m     91\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[43mcb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_pool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     te_pred[:, c] = cb.predict_proba(T_full)[:, \u001b[32m1\u001b[39m]\n\u001b[32m     94\u001b[39m test_preds.append(te_pred)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/catboost/core.py:5245\u001b[39m, in \u001b[36mCatBoostClassifier.fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   5242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[32m   5243\u001b[39m     CatBoostClassifier._check_is_compatible_loss(params[\u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m5245\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5246\u001b[39m \u001b[43m          \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5247\u001b[39m \u001b[43m          \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5248\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/catboost/core.py:2410\u001b[39m, in \u001b[36mCatBoost._fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   2407\u001b[39m allow_clear_pool = train_params[\u001b[33m\"\u001b[39m\u001b[33mallow_clear_pool\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   2409\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[33m'\u001b[39m\u001b[33mTraining plots\u001b[39m\u001b[33m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m.get_params())]):\n\u001b[32m-> \u001b[39m\u001b[32m2410\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_sets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minit_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   2416\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[32m   2419\u001b[39m loss = \u001b[38;5;28mself\u001b[39m._object._get_loss_function_name()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/catboost/core.py:1790\u001b[39m, in \u001b[36m_CatBoostBase._train\u001b[39m\u001b[34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_object\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_trained_model_attributes()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m_catboost.pyx:5023\u001b[39m, in \u001b[36m_catboost._CatBoost._train\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m_catboost.pyx:5072\u001b[39m, in \u001b[36m_catboost._CatBoost._train\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "db49564c-2e7b-4db3-afc9-6e1b7ed6e76e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fast pipeline: HOG+tabular -> TruncatedSVD -> OneVsRest LogisticRegression (balanced) -> submission\n",
        "import numpy as np, pandas as pd, time\n",
        "from pathlib import Path\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Load cached HOG features\n",
        "HOG_CACHE_T = Path('hog_train.npy')\n",
        "HOG_CACHE_S = Path('hog_test.npy')\n",
        "assert HOG_CACHE_T.exists() and HOG_CACHE_S.exists(), 'Run cell 17 to create hog_train.npy/hog_test.npy first.'\n",
        "hog_train = np.load(HOG_CACHE_T)\n",
        "hog_test = np.load(HOG_CACHE_S)\n",
        "\n",
        "# Build augmented matrices (tabular + HOG)\n",
        "X_tab = X_train_df.values.astype(np.float32)\n",
        "T_tab = X_test_df.values.astype(np.float32)\n",
        "X_aug = np.concatenate([X_tab, hog_train], axis=1).astype(np.float32)\n",
        "T_aug = np.concatenate([T_tab, hog_test], axis=1).astype(np.float32)\n",
        "Y_mat = Y_aligned.astype(np.float32)\n",
        "N, D = X_aug.shape\n",
        "C = Y_mat.shape[1]\n",
        "print('Aug dims:', X_aug.shape, T_aug.shape, flush=True)\n",
        "\n",
        "# Scale then SVD to reduce dimensionality (fast, stable for high-D). Fit on train only.\n",
        "t0 = time.time()\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "X_s = scaler.fit_transform(X_aug)\n",
        "T_s = scaler.transform(T_aug)\n",
        "n_comp = min(256, X_s.shape[1]-1, X_s.shape[0]-1)\n",
        "svd = TruncatedSVD(n_components=n_comp, random_state=42)\n",
        "X_z = svd.fit_transform(X_s)\n",
        "T_z = svd.transform(T_s)\n",
        "print(f'SVD -> comps={n_comp}, explained_var_sum={svd.explained_variance_ratio_.sum():.4f}, time={time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "# OneVsRest LogisticRegression (L2, balanced) on SVD components\n",
        "clf = OneVsRestClassifier(LogisticRegression(solver='lbfgs', C=1.0, max_iter=3000, class_weight='balanced', n_jobs=-1))\n",
        "t1 = time.time()\n",
        "clf.fit(X_z, Y_mat)\n",
        "print(f'LR OVR fit in {time.time()-t1:.1f}s', flush=True)\n",
        "test_proba = np.vstack([est.predict_proba(T_z)[:,1] for est in clf.estimators_]).T.astype(np.float32)\n",
        "\n",
        "# Build submission\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_proba[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f'Wrote submission.csv with {len(sub_out)} rows (LR-OVR on SVD(HOG+tabular))')\n",
        "display(sub_out.head())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aug dims: (145, 8797) (64, 8797)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVD -> comps=144, explained_var_sum=1.0000, time=1.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR OVR fit in 24.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with 1216 rows (LR-OVR on SVD(HOG+tabular))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.000213\n1  101     0.009447\n2  102     0.000718\n3  103     0.000686\n4  104     0.000461",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.000213</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.009447</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.000718</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.000686</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.000461</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "924a1dec-a5e5-42be-9e98-b8e0b64c7817",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Blend CB-only rank ensemble (cell 13) with LR-SVD(HOG+tabular) (cell 19) and write submission\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "def rank_cols(mat):\n",
        "    from scipy.stats import rankdata\n",
        "    N, C = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (N - 1) if N > 1 else r*0.0\n",
        "    return out\n",
        "\n",
        "# Expect test_blend_rank_cb from cell 13 and test_proba from cell 19\n",
        "assert 'test_blend_rank_cb' in globals(), 'Run cell 13 (CB-only rank ensemble) first.'\n",
        "assert 'test_proba' in globals(), 'Run cell 19 (LR-OVR SVD) first.'\n",
        "\n",
        "test_lr_rank = rank_cols(test_proba)\n",
        "# Fixed equal-weight or slightly CB-heavy. Start with CB 0.8 / LR 0.2\n",
        "w_cb, w_lr = 0.8, 0.2\n",
        "test_blend_final = w_cb * test_blend_rank_cb + w_lr * test_lr_rank\n",
        "\n",
        "# Build submission\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_blend_final[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f'Wrote submission.csv with {len(sub_out)} rows (CB-only rank blend 0.8 + LR-SVD 0.2)')\n",
        "display(sub_out.head())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with 1216 rows (CB-only rank blend 0.8 + LR-SVD 0.2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.288889\n1  101     0.576508\n2  102     0.429206\n3  103     0.771429\n4  104     0.542857",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.288889</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.576508</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.429206</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.771429</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.542857</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "39ffcb2d-ee1e-483f-a372-17c39c93b94a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Grouped CV by recording station (PCxx) + CatBoost bag (no MI) + rank ensemble submission\n",
        "import re, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "def extract_station(stem):\n",
        "    if not isinstance(stem, str):\n",
        "        return None\n",
        "    m = re.search(r'(PC\\d+)', stem)\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "# Build groups (station) for labeled train\n",
        "train_meta = train_df[['rec_id','filename']].copy()\n",
        "train_meta['station'] = train_meta['filename'].map(extract_station)\n",
        "groups = train_meta['station'].fillna('UNK').values\n",
        "\n",
        "# Matrices (full engineered features without MI)\n",
        "X_tr_full = X_train_df.values.astype(np.float32)\n",
        "X_te_full = X_test_df.values.astype(np.float32)\n",
        "Y_full = Y_aligned.astype(np.float32)\n",
        "N, C = X_tr_full.shape[0], Y_full.shape[1]\n",
        "\n",
        "def masked_auc_macro(y_true, y_pred, valid_mask):\n",
        "    aucs = []\n",
        "    for c in range(y_true.shape[1]):\n",
        "        m = valid_mask[:, c].astype(bool)\n",
        "        if m.sum() <= 1:\n",
        "            continue\n",
        "        yt = y_true[m, c]; yp = y_pred[m, c]\n",
        "        if yt.max() == yt.min():\n",
        "            continue\n",
        "        try:\n",
        "            aucs.append(roc_auc_score(yt, yp))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "\n",
        "def rank_cols(mat):\n",
        "    from scipy.stats import rankdata\n",
        "    Nn, Cc = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(Cc):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (Nn - 1) if Nn > 1 else r*0.0\n",
        "    return out\n",
        "\n",
        "# Define 4 diverse CatBoost variants (shallow, strong reg, feature subsampling and subsample)\n",
        "variants = [\n",
        "    dict(name='cb_gcv_d3_lr002_l2_30_rsm06_sub085_s42', depth=3, learning_rate=0.02,  l2_leaf_reg=30.0, rsm=0.6, subsample=0.85, random_strength=0.8, seed=42),\n",
        "    dict(name='cb_gcv_d4_lr002_l2_40_rsm05_sub08_s123', depth=4, learning_rate=0.02,  l2_leaf_reg=40.0, rsm=0.5, subsample=0.80, random_strength=1.0, seed=123),\n",
        "    dict(name='cb_gcv_d3_lr0025_l2_25_rsm06_sub085_s456', depth=3, learning_rate=0.025, l2_leaf_reg=25.0, rsm=0.6, subsample=0.85, random_strength=0.8, seed=456),\n",
        "    dict(name='cb_gcv_d4_lr0015_l2_50_rsm05_sub075_s789', depth=4, learning_rate=0.015, l2_leaf_reg=50.0, rsm=0.5, subsample=0.75, random_strength=1.0, seed=789),\n",
        "]\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "folds = list(gkf.split(X_tr_full, Y_full, groups))\n",
        "print('GroupKFold by station -> n_folds:', len(folds), 'unique stations:', len(np.unique(groups)))\n",
        "\n",
        "test_pred_list = []\n",
        "oof_list = []\n",
        "mask_list = []\n",
        "\n",
        "for v in variants:\n",
        "    print(f\"=== Variant {v['name']} ===\", flush=True)\n",
        "    oof = np.zeros((N, C), dtype=np.float32)\n",
        "    vmask = np.zeros((N, C), dtype=np.uint8)\n",
        "    test_folds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds):\n",
        "        t0 = time.time()\n",
        "        Xtr, Xva = X_tr_full[tr_idx], X_tr_full[va_idx]\n",
        "        ytr, yva = Y_full[tr_idx], Y_full[va_idx]\n",
        "        te_pred = np.zeros((X_te_full.shape[0], C), dtype=np.float32)\n",
        "        for c in range(C):\n",
        "            ytr_c = ytr[:, c]; yva_c = yva[:, c]\n",
        "            tr_pos = int((ytr_c == 1).sum()); tr_neg = int((ytr_c == 0).sum())\n",
        "            va_pos = int((yva_c == 1).sum()); va_neg = int((yva_c == 0).sum())\n",
        "            if not (tr_pos > 0 and tr_neg > 0 and va_pos > 0 and va_neg > 0):\n",
        "                continue\n",
        "            tr_pool = Pool(Xtr, label=ytr_c)\n",
        "            va_pool = Pool(Xva, label=yva_c)\n",
        "            cb = CatBoostClassifier(\n",
        "                loss_function='Logloss',\n",
        "                depth=v['depth'],\n",
        "                learning_rate=v['learning_rate'],\n",
        "                iterations=5000,\n",
        "                l2_leaf_reg=v['l2_leaf_reg'],\n",
        "                rsm=v['rsm'],\n",
        "                subsample=v['subsample'],\n",
        "                random_strength=v['random_strength'],\n",
        "                auto_class_weights='Balanced',\n",
        "                random_seed=v['seed'],\n",
        "                early_stopping_rounds=200,\n",
        "                verbose=False,\n",
        "                task_type='CPU',\n",
        "                thread_count=-1\n",
        "            )\n",
        "            cb.fit(tr_pool, eval_set=va_pool)\n",
        "            oof[va_idx, c] = cb.predict_proba(Xva)[:, 1]\n",
        "            te_pred[:, c] = cb.predict_proba(X_te_full)[:, 1]\n",
        "            vmask[va_idx, c] = 1\n",
        "        fold_macro = masked_auc_macro(yva, oof[va_idx], vmask[va_idx])\n",
        "        print(f\"  Fold {fi}: masked macro={fold_macro:.4f} | n_val={len(va_idx)} | time={time.time()-t0:.1f}s\", flush=True)\n",
        "        test_folds.append(te_pred)\n",
        "    test_mean = np.mean(test_folds, axis=0)\n",
        "    oof_list.append(oof); mask_list.append(vmask); test_pred_list.append(test_mean)\n",
        "    macro_v = masked_auc_macro(Y_full, oof, vmask)\n",
        "    print(f\"Variant OOF masked macro: {macro_v:.4f}\", flush=True)\n",
        "\n",
        "# Rank-ensemble across variants (equal weights)\n",
        "inter_mask = mask_list[0].astype(bool)\n",
        "for m in mask_list[1:]:\n",
        "    inter_mask &= m.astype(bool)\n",
        "inter_mask = inter_mask.astype(np.uint8)\n",
        "\n",
        "# Rank OOFs and tests\n",
        "ranked_oofs = []\n",
        "for o in oof_list:\n",
        "    # rank with mask: only rank within observed rows per class\n",
        "    from scipy.stats import rankdata\n",
        "    Nn, Cc = o.shape\n",
        "    R = np.full_like(o, np.nan, dtype=np.float32)\n",
        "    for c in range(Cc):\n",
        "        m = inter_mask[:, c].astype(bool)\n",
        "        if not m.any():\n",
        "            continue\n",
        "        col = o[m, c]\n",
        "        r = rankdata(col, method='average')\n",
        "        R[m, c] = ((r - 1) / (len(r) - 1) if len(r) > 1 else r*0.0).astype(np.float32)\n",
        "    ranked_oofs.append(R)\n",
        "oof_ens = np.nanmean(ranked_oofs, axis=0)\n",
        "macro_ens = masked_auc_macro(Y_full, oof_ens, inter_mask)\n",
        "print(f'Ensemble (equal-weight, rank OOF) masked macro: {macro_ens:.4f}', flush=True)\n",
        "\n",
        "ranked_tests = [rank_cols(tp) for tp in test_pred_list]\n",
        "test_ens = np.mean(ranked_tests, axis=0)\n",
        "\n",
        "# Build submission\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_ens[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (GroupKFold-by-station CB bag, equal-weight rank ensemble)\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GroupKFold by station -> n_folds: 5 unique stations: 13\n=== Variant cb_gcv_d3_lr002_l2_30_rsm06_sub085_s42 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7448 | n_val=31 | time=25.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7934 | n_val=27 | time=34.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7036 | n_val=26 | time=38.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.6803 | n_val=31 | time=30.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7185 | n_val=30 | time=14.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_gcv_d4_lr002_l2_40_rsm05_sub08_s123 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.6899 | n_val=31 | time=31.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7857 | n_val=27 | time=52.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7296 | n_val=26 | time=68.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7026 | n_val=31 | time=42.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.6444 | n_val=30 | time=19.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_gcv_d3_lr0025_l2_25_rsm06_sub085_s456 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7525 | n_val=31 | time=24.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7615 | n_val=27 | time=28.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.6980 | n_val=26 | time=33.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7031 | n_val=31 | time=40.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7544 | n_val=30 | time=14.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_gcv_d4_lr0015_l2_50_rsm05_sub075_s789 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7208 | n_val=31 | time=34.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7406 | n_val=27 | time=60.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7077 | n_val=26 | time=69.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7301 | n_val=31 | time=50.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7148 | n_val=30 | time=25.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble (equal-weight, rank OOF) masked macro: 0.7418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with 1216 rows (GroupKFold-by-station CB bag, equal-weight rank ensemble)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.246032\n1  101     0.476190\n2  102     0.408730\n3  103     0.732143\n4  104     0.384921",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.246032</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.476190</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.408730</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.732143</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.384921</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "601f352e-48ff-4dbf-aab3-6e855828c5ad",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# GroupKFold by station with 4 folds (better class coverage) + CB bag and rank ensemble\n",
        "import re, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "def extract_station(stem):\n",
        "    if not isinstance(stem, str):\n",
        "        return None\n",
        "    m = re.search(r'(PC\\d+)', stem)\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "# Build groups (station) for labeled train\n",
        "train_meta = train_df[['rec_id','filename']].copy()\n",
        "train_meta['station'] = train_meta['filename'].map(extract_station)\n",
        "groups = train_meta['station'].fillna('UNK').values\n",
        "\n",
        "# Matrices\n",
        "X_tr_full = X_train_df.values.astype(np.float32)\n",
        "X_te_full = X_test_df.values.astype(np.float32)\n",
        "Y_full = Y_aligned.astype(np.float32)\n",
        "N, C = X_tr_full.shape[0], Y_full.shape[1]\n",
        "\n",
        "def masked_auc_macro(y_true, y_pred, valid_mask):\n",
        "    aucs = []\n",
        "    for c in range(y_true.shape[1]):\n",
        "        m = valid_mask[:, c].astype(bool)\n",
        "        if m.sum() <= 1:\n",
        "            continue\n",
        "        yt = y_true[m, c]; yp = y_pred[m, c]\n",
        "        if yt.max() == yt.min():\n",
        "            continue\n",
        "        try:\n",
        "            aucs.append(roc_auc_score(yt, yp))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "\n",
        "def rank_cols(mat):\n",
        "    from scipy.stats import rankdata\n",
        "    Nn, Cc = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(Cc):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (Nn - 1) if Nn > 1 else r*0.0\n",
        "    return out\n",
        "\n",
        "# Slightly adjusted variants for diversity\n",
        "variants = [\n",
        "    dict(name='cb_gcv4_d3_lr002_l2_35_rsm06_sub085_s42',  depth=3, learning_rate=0.02,  l2_leaf_reg=35.0, rsm=0.6, subsample=0.85, random_strength=0.8, seed=42),\n",
        "    dict(name='cb_gcv4_d4_lr002_l2_45_rsm05_sub08_s123',  depth=4, learning_rate=0.02,  l2_leaf_reg=45.0, rsm=0.5, subsample=0.80, random_strength=1.0, seed=123),\n",
        "    dict(name='cb_gcv4_d3_lr0025_l2_30_rsm06_sub09_s456', depth=3, learning_rate=0.025, l2_leaf_reg=30.0, rsm=0.6, subsample=0.90, random_strength=0.9, seed=456),\n",
        "    dict(name='cb_gcv4_d4_lr0015_l2_55_rsm05_sub075_s789',depth=4, learning_rate=0.015, l2_leaf_reg=55.0, rsm=0.5, subsample=0.75, random_strength=1.0, seed=789),\n",
        "]\n",
        "\n",
        "gkf = GroupKFold(n_splits=4)\n",
        "folds = list(gkf.split(X_tr_full, Y_full, groups))\n",
        "print('GroupKFold by station -> n_folds:', len(folds), 'unique stations:', len(np.unique(groups)))\n",
        "\n",
        "test_pred_list = []\n",
        "oof_list = []\n",
        "mask_list = []\n",
        "\n",
        "for v in variants:\n",
        "    print(f\"=== Variant {v['name']} ===\", flush=True)\n",
        "    oof = np.zeros((N, C), dtype=np.float32)\n",
        "    vmask = np.zeros((N, C), dtype=np.uint8)\n",
        "    test_folds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds):\n",
        "        t0 = time.time()\n",
        "        Xtr, Xva = X_tr_full[tr_idx], X_tr_full[va_idx]\n",
        "        ytr, yva = Y_full[tr_idx], Y_full[va_idx]\n",
        "        te_pred = np.zeros((X_te_full.shape[0], C), dtype=np.float32)\n",
        "        for c in range(C):\n",
        "            ytr_c = ytr[:, c]; yva_c = yva[:, c]\n",
        "            tr_pos = int((ytr_c == 1).sum()); tr_neg = int((ytr_c == 0).sum())\n",
        "            va_pos = int((yva_c == 1).sum()); va_neg = int((yva_c == 0).sum())\n",
        "            if not (tr_pos > 0 and tr_neg > 0 and va_pos > 0 and va_neg > 0):\n",
        "                continue\n",
        "            tr_pool = Pool(Xtr, label=ytr_c)\n",
        "            va_pool = Pool(Xva, label=yva_c)\n",
        "            cb = CatBoostClassifier(\n",
        "                loss_function='Logloss',\n",
        "                depth=v['depth'],\n",
        "                learning_rate=v['learning_rate'],\n",
        "                iterations=5000,\n",
        "                l2_leaf_reg=v['l2_leaf_reg'],\n",
        "                rsm=v['rsm'],\n",
        "                subsample=v['subsample'],\n",
        "                random_strength=v['random_strength'],\n",
        "                auto_class_weights='Balanced',\n",
        "                random_seed=v['seed'],\n",
        "                early_stopping_rounds=200,\n",
        "                verbose=False,\n",
        "                task_type='CPU',\n",
        "                thread_count=-1\n",
        "            )\n",
        "            cb.fit(tr_pool, eval_set=va_pool)\n",
        "            oof[va_idx, c] = cb.predict_proba(Xva)[:, 1]\n",
        "            te_pred[:, c] = cb.predict_proba(X_te_full)[:, 1]\n",
        "            vmask[va_idx, c] = 1\n",
        "        fold_macro = masked_auc_macro(yva, oof[va_idx], vmask[va_idx])\n",
        "        print(f\"  Fold {fi}: masked macro={fold_macro:.4f} | n_val={len(va_idx)} | time={time.time()-t0:.1f}s\", flush=True)\n",
        "        test_folds.append(te_pred)\n",
        "    test_mean = np.mean(test_folds, axis=0)\n",
        "    oof_list.append(oof); mask_list.append(vmask); test_pred_list.append(test_mean)\n",
        "    macro_v = masked_auc_macro(Y_full, oof, vmask)\n",
        "    print(f\"Variant OOF masked macro: {macro_v:.4f}\", flush=True)\n",
        "\n",
        "# Rank-ensemble across variants (equal weights)\n",
        "inter_mask = mask_list[0].astype(bool)\n",
        "for m in mask_list[1:]:\n",
        "    inter_mask &= m.astype(bool)\n",
        "inter_mask = inter_mask.astype(np.uint8)\n",
        "\n",
        "# Rank OOFs and tests\n",
        "ranked_oofs = []\n",
        "from scipy.stats import rankdata\n",
        "for o in oof_list:\n",
        "    Nn, Cc = o.shape\n",
        "    R = np.full_like(o, np.nan, dtype=np.float32)\n",
        "    for c in range(Cc):\n",
        "        m = inter_mask[:, c].astype(bool)\n",
        "        if not m.any():\n",
        "            continue\n",
        "        col = o[m, c]\n",
        "        r = rankdata(col, method='average')\n",
        "        R[m, c] = ((r - 1) / (len(r) - 1) if len(r) > 1 else r*0.0).astype(np.float32)\n",
        "    ranked_oofs.append(R)\n",
        "oof_ens = np.nanmean(ranked_oofs, axis=0)\n",
        "macro_ens = masked_auc_macro(Y_full, oof_ens, inter_mask)\n",
        "print(f'Ensemble (equal-weight, rank OOF) masked macro: {macro_ens:.4f}', flush=True)\n",
        "\n",
        "ranked_tests = [rank_cols(tp) for tp in test_pred_list]\n",
        "test_ens = np.mean(ranked_tests, axis=0)\n",
        "\n",
        "# Build submission\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_ens[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (GroupKFold-4-by-station CB bag, equal-weight rank ensemble)\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GroupKFold by station -> n_folds: 4 unique stations: 13\n=== Variant cb_gcv4_d3_lr002_l2_35_rsm06_sub085_s42 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7211 | n_val=39 | time=12.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7248 | n_val=34 | time=36.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7279 | n_val=37 | time=28.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.6715 | n_val=35 | time=28.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_gcv4_d4_lr002_l2_45_rsm05_sub08_s123 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.6916 | n_val=39 | time=15.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7342 | n_val=34 | time=55.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.6963 | n_val=37 | time=39.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7109 | n_val=35 | time=35.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_gcv4_d3_lr0025_l2_30_rsm06_sub09_s456 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.6223 | n_val=39 | time=10.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7476 | n_val=34 | time=29.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.6815 | n_val=37 | time=26.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7147 | n_val=35 | time=22.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.6931\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_gcv4_d4_lr0015_l2_55_rsm05_sub075_s789 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.6723 | n_val=39 | time=18.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7616 | n_val=34 | time=78.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7173 | n_val=37 | time=42.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.6857 | n_val=35 | time=39.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble (equal-weight, rank OOF) masked macro: 0.7312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with 1216 rows (GroupKFold-4-by-station CB bag, equal-weight rank ensemble)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.301587\n1  101     0.484127\n2  102     0.488095\n3  103     0.652778\n4  104     0.392857",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.301587</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.484127</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.488095</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.652778</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.392857</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "464a9cda-bb74-48ff-bfb9-56b782196016",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Blend GroupKFold CB-bag ranks (test_ens) with LR-SVD(HOG+tab) at small weight (0.1) and write submission\n",
        "import numpy as np, pandas as pd\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "def rank_cols(mat):\n",
        "    N, C = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (N - 1) if N > 1 else r*0.0\n",
        "    return out\n",
        "\n",
        "assert 'test_ens' in globals(), 'Run GroupKFold CB bag cell (21 or 22) first to get test_ens.'\n",
        "assert 'test_proba' in globals(), 'Run cell 19 (LR-OVR SVD) first to get test_proba.'\n",
        "\n",
        "lr_rank = rank_cols(test_proba.astype(np.float32))\n",
        "w_cb, w_lr = 0.9, 0.1\n",
        "blend_rank = w_cb * test_ens + w_lr * lr_rank\n",
        "\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(blend_rank[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (GroupKFold CB bag 0.9 + LR-SVD 0.1 rank blend)\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with 1216 rows (GroupKFold CB bag 0.9 + LR-SVD 0.1 rank blend)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.317460\n1  101     0.508730\n2  102     0.486905\n3  103     0.671627\n4  104     0.415476",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.317460</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.508730</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.486905</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.671627</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.415476</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "b761832d-6a1d-471a-b640-cda479d36f22",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Three-way rank blend: GroupCV CB bag (test_ens) + CB-only (cell 13) + LR-SVD (cell 19)\n",
        "import numpy as np, pandas as pd\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "def rank_cols(mat):\n",
        "    N, C = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (N - 1) if N > 1 else r*0.0\n",
        "    return out\n",
        "\n",
        "assert 'test_ens' in globals(), 'Run GroupKFold CB bag cell (21 or 22) first.'\n",
        "assert 'test_blend_rank_cb' in globals(), 'Run CB-only ensemble (cell 13) first.'\n",
        "assert 'test_proba' in globals(), 'Run LR-OVR SVD (cell 19) first.'\n",
        "\n",
        "lr_rank = rank_cols(test_proba.astype(np.float32))\n",
        "w_gcb, w_cb, w_lr = 0.7, 0.2, 0.1\n",
        "blend_rank3 = w_gcb * test_ens + w_cb * test_blend_rank_cb + w_lr * lr_rank\n",
        "\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(blend_rank3[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (GroupCV CB 0.7 + CB-only 0.2 + LR-SVD 0.1 rank blend)\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with 1216 rows (GroupCV CB 0.7 + CB-only 0.2 + LR-SVD 0.1 rank blend)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.306349\n1  101     0.519524\n2  102     0.472778\n3  103     0.691865\n4  104     0.441667",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.306349</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.519524</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.472778</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.691865</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.441667</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "b351107c-7a50-4c2d-b30d-54485219a590",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Grouped CV by station + adversarial feature drop (top-30) + CatBoost bag + rank ensemble submission\n",
        "import numpy as np, pandas as pd, time, re\n",
        "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "def extract_station(stem):\n",
        "    if not isinstance(stem, str): return None\n",
        "    m = re.search(r'(PC\\d+)', stem); return m.group(1) if m else None\n",
        "\n",
        "# Build groups (station) for labeled train\n",
        "train_meta = train_df[['rec_id','filename']].copy()\n",
        "train_meta['station'] = train_meta['filename'].map(extract_station)\n",
        "groups = train_meta['station'].fillna('UNK').values\n",
        "\n",
        "# Full engineered features\n",
        "X_tr_full_df = X_train_df.copy()\n",
        "X_te_full_df = X_test_df.copy()\n",
        "Y_full = Y_aligned.astype(np.float32)\n",
        "N, C = X_tr_full_df.shape[0], Y_full.shape[1]\n",
        "\n",
        "# 1) Adversarial validation to identify drifted features (train vs test) and drop top-K\n",
        "X_adv = pd.concat([X_tr_full_df.assign(is_test=0), X_te_full_df.assign(is_test=1)], axis=0).reset_index(drop=True)\n",
        "y_adv = X_adv['is_test'].values.astype(int)\n",
        "X_adv_mat = X_adv.drop(columns=['is_test']).values.astype(np.float32)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "feat_importance = np.zeros(X_adv_mat.shape[1], dtype=np.float64)\n",
        "for fi, (tr, va) in enumerate(skf.split(X_adv_mat, y_adv)):\n",
        "    Xtr, Xva = X_adv_mat[tr], X_adv_mat[va]\n",
        "    ytr, yva = y_adv[tr], y_adv[va]\n",
        "    clf = lgb.LGBMClassifier(\n",
        "        objective='binary', n_estimators=800, learning_rate=0.05,\n",
        "        max_depth=3, num_leaves=7, subsample=0.8, colsample_bytree=0.6,\n",
        "        reg_alpha=0.5, reg_lambda=5.0, n_jobs=-1, random_state=42, verbosity=-1\n",
        "    )\n",
        "    clf.fit(Xtr, ytr, eval_set=[(Xva, yva)], eval_metric='auc',\n",
        "            callbacks=[lgb.early_stopping(100, verbose=False), lgb.log_evaluation(period=0)])\n",
        "    feat_importance += clf.booster_.feature_importance(importance_type='gain').astype(np.float64)\n",
        "feat_importance /= max(1, skf.get_n_splits())\n",
        "cols = X_tr_full_df.columns.tolist()\n",
        "order = np.argsort(-feat_importance)\n",
        "drop_top = 30\n",
        "drop_cols = [cols[i] for i in order[:min(drop_top, len(cols))]]\n",
        "keep_cols = [c for c in cols if c not in drop_cols]\n",
        "print(f'Adversarial drop: top {len(drop_cols)} features removed; keeping {len(keep_cols)}', flush=True)\n",
        "\n",
        "X_tr = X_tr_full_df[keep_cols].values.astype(np.float32)\n",
        "X_te = X_te_full_df[keep_cols].values.astype(np.float32)\n",
        "\n",
        "def masked_auc_macro(y_true, y_pred, valid_mask):\n",
        "    aucs = []\n",
        "    for c in range(y_true.shape[1]):\n",
        "        m = valid_mask[:, c].astype(bool)\n",
        "        if m.sum() <= 1: continue\n",
        "        yt = y_true[m, c]; yp = y_pred[m, c]\n",
        "        if yt.max() == yt.min(): continue\n",
        "        try: aucs.append(roc_auc_score(yt, yp))\n",
        "        except Exception: pass\n",
        "    return float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "\n",
        "def rank_cols(mat):\n",
        "    from scipy.stats import rankdata\n",
        "    Nn, Cc = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(Cc):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (Nn - 1) if Nn > 1 else r*0.0\n",
        "    return out\n",
        "\n",
        "# 2) GroupKFold by station (5 folds); if poor coverage, user can switch to 4\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "folds = list(gkf.split(X_tr, Y_full, groups))\n",
        "print('GroupKFold by station -> n_folds:', len(folds), 'unique stations:', len(np.unique(groups)))\n",
        "\n",
        "# 3) CatBoost variants (shallow, strong regularization, feature subsampling + subsample)\n",
        "variants = [\n",
        "    dict(name='cb_gcv_adv_d3_lr002_l2_30_rsm06_sub085_s42', depth=3, learning_rate=0.02,  l2_leaf_reg=30.0, rsm=0.6, subsample=0.85, random_strength=0.8, seed=42),\n",
        "    dict(name='cb_gcv_adv_d4_lr002_l2_40_rsm05_sub08_s123', depth=4, learning_rate=0.02,  l2_leaf_reg=40.0, rsm=0.5, subsample=0.80, random_strength=1.0, seed=123),\n",
        "    dict(name='cb_gcv_adv_d3_lr0025_l2_25_rsm06_sub09_s456', depth=3, learning_rate=0.025, l2_leaf_reg=25.0, rsm=0.6, subsample=0.90, random_strength=0.9, seed=456),\n",
        "]\n",
        "\n",
        "test_pred_list = []; oof_list = []; mask_list = []\n",
        "for v in variants:\n",
        "    print(f\"=== Variant {v['name']} ===\", flush=True)\n",
        "    oof = np.zeros((N, C), dtype=np.float32)\n",
        "    vmask = np.zeros((N, C), dtype=np.uint8)\n",
        "    test_folds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds):\n",
        "        t0 = time.time()\n",
        "        Xtr_f, Xva_f = X_tr[tr_idx], X_tr[va_idx]\n",
        "        ytr, yva = Y_full[tr_idx], Y_full[va_idx]\n",
        "        te_pred = np.zeros((X_te.shape[0], C), dtype=np.float32)\n",
        "        for c in range(C):\n",
        "            ytr_c = ytr[:, c]; yva_c = yva[:, c]\n",
        "            tr_pos = int((ytr_c == 1).sum()); tr_neg = int((ytr_c == 0).sum())\n",
        "            va_pos = int((yva_c == 1).sum()); va_neg = int((yva_c == 0).sum())\n",
        "            if not (tr_pos > 0 and tr_neg > 0 and va_pos > 0 and va_neg > 0):\n",
        "                continue\n",
        "            tr_pool = Pool(Xtr_f, label=ytr_c)\n",
        "            va_pool = Pool(Xva_f, label=yva_c)\n",
        "            cb = CatBoostClassifier(\n",
        "                loss_function='Logloss',\n",
        "                depth=v['depth'],\n",
        "                learning_rate=v['learning_rate'],\n",
        "                iterations=5000,\n",
        "                l2_leaf_reg=v['l2_leaf_reg'],\n",
        "                rsm=v['rsm'],\n",
        "                subsample=v['subsample'],\n",
        "                random_strength=v['random_strength'],\n",
        "                auto_class_weights='Balanced',\n",
        "                random_seed=v['seed'],\n",
        "                early_stopping_rounds=200,\n",
        "                verbose=False,\n",
        "                task_type='CPU',\n",
        "                thread_count=-1\n",
        "            )\n",
        "            cb.fit(tr_pool, eval_set=va_pool)\n",
        "            oof[va_idx, c] = cb.predict_proba(Xva_f)[:, 1]\n",
        "            te_pred[:, c] = cb.predict_proba(X_te)[:, 1]\n",
        "            vmask[va_idx, c] = 1\n",
        "        fold_macro = masked_auc_macro(yva, oof[va_idx], vmask[va_idx])\n",
        "        print(f\"  Fold {fi}: masked macro={fold_macro:.4f} | n_val={len(va_idx)} | time={time.time()-t0:.1f}s\", flush=True)\n",
        "        test_folds.append(te_pred)\n",
        "    test_mean = np.mean(test_folds, axis=0)\n",
        "    oof_list.append(oof); mask_list.append(vmask); test_pred_list.append(test_mean)\n",
        "    macro_v = masked_auc_macro(Y_full, oof, vmask)\n",
        "    print(f\"Variant OOF masked macro: {macro_v:.4f}\", flush=True)\n",
        "\n",
        "# 4) Equal-weight rank ensemble across variants\n",
        "inter_mask = mask_list[0].astype(bool)\n",
        "for m in mask_list[1:]:\n",
        "    inter_mask &= m.astype(bool)\n",
        "inter_mask = inter_mask.astype(np.uint8)\n",
        "\n",
        "ranked_oofs = []\n",
        "from scipy.stats import rankdata\n",
        "for o in oof_list:\n",
        "    Nn, Cc = o.shape\n",
        "    R = np.full_like(o, np.nan, dtype=np.float32)\n",
        "    for c in range(Cc):\n",
        "        m = inter_mask[:, c].astype(bool)\n",
        "        if not m.any():\n",
        "            continue\n",
        "        col = o[m, c]\n",
        "        r = rankdata(col, method='average')\n",
        "        R[m, c] = ((r - 1) / (len(r) - 1) if len(r) > 1 else r*0.0).astype(np.float32)\n",
        "    ranked_oofs.append(R)\n",
        "oof_ens = np.nanmean(ranked_oofs, axis=0)\n",
        "macro_ens = masked_auc_macro(Y_full, oof_ens, inter_mask)\n",
        "print(f'Ensemble (equal-weight, rank OOF) masked macro: {macro_ens:.4f}', flush=True)\n",
        "\n",
        "ranked_tests = [rank_cols(tp) for tp in test_pred_list]\n",
        "test_ens_adv = np.mean(ranked_tests, axis=0)\n",
        "\n",
        "# 5) Build submission\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_ens_adv[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (GroupKFold-by-station + adv-drop{drop_top}, CB bag, rank ensemble)\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial drop: top 30 features removed; keeping 667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GroupKFold by station -> n_folds: 5 unique stations: 13\n=== Variant cb_gcv_adv_d3_lr002_l2_30_rsm06_sub085_s42 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7473 | n_val=31 | time=23.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7609 | n_val=27 | time=30.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.6889 | n_val=26 | time=35.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7023 | n_val=31 | time=27.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7574 | n_val=30 | time=12.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7610\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_gcv_adv_d4_lr002_l2_40_rsm05_sub08_s123 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.6911 | n_val=31 | time=30.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7614 | n_val=27 | time=46.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.6957 | n_val=26 | time=48.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7210 | n_val=31 | time=34.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7474 | n_val=30 | time=16.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_gcv_adv_d3_lr0025_l2_25_rsm06_sub09_s456 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7250 | n_val=31 | time=23.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7450 | n_val=27 | time=24.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7060 | n_val=26 | time=30.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7345 | n_val=31 | time=27.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7102 | n_val=30 | time=11.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble (equal-weight, rank OOF) masked macro: 0.7527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with 1216 rows (GroupKFold-by-station + adv-drop30, CB bag, rank ensemble)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.246032\n1  101     0.457672\n2  102     0.436508\n3  103     0.716931\n4  104     0.420635",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.246032</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.457672</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.436508</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.716931</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.420635</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "1135e272-7d1a-42df-a0e2-6c421db30319",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# GroupKFold-by-station with expanded 8-variant CatBoost bag (strong reg, diverse seeds) + rank ensemble submission\n",
        "import re, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "def extract_station(stem):\n",
        "    if not isinstance(stem, str):\n",
        "        return None\n",
        "    m = re.search(r'(PC\\d+)', stem)\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "# Groups\n",
        "train_meta = train_df[['rec_id','filename']].copy()\n",
        "train_meta['station'] = train_meta['filename'].map(extract_station)\n",
        "groups = train_meta['station'].fillna('UNK').values\n",
        "\n",
        "# Features and labels (full engineered, no MI)\n",
        "X_tr_full = X_train_df.values.astype(np.float32)\n",
        "X_te_full = X_test_df.values.astype(np.float32)\n",
        "Y_full = Y_aligned.astype(np.float32)\n",
        "N, C = X_tr_full.shape[0], Y_full.shape[1]\n",
        "\n",
        "def masked_auc_macro(y_true, y_pred, valid_mask):\n",
        "    aucs = []\n",
        "    for c in range(y_true.shape[1]):\n",
        "        m = valid_mask[:, c].astype(bool)\n",
        "        if m.sum() <= 1:\n",
        "            continue\n",
        "        yt = y_true[m, c]; yp = y_pred[m, c]\n",
        "        if yt.max() == yt.min():\n",
        "            continue\n",
        "        try:\n",
        "            aucs.append(roc_auc_score(yt, yp))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "\n",
        "def rank_cols(mat):\n",
        "    from scipy.stats import rankdata\n",
        "    Nn, Cc = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(Cc):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (Nn - 1) if Nn > 1 else r*0.0\n",
        "    return out\n",
        "\n",
        "# 8 diverse CB variants (shallow, strong L2, feature subsampling, subsample, varied seeds)\n",
        "variants = [\n",
        "    dict(name='cb_gcv_d3_lr002_l2_35_rsm06_sub085_s42',  depth=3, learning_rate=0.02,  l2_leaf_reg=35.0, rsm=0.6, subsample=0.85, random_strength=0.8, seed=42),\n",
        "    dict(name='cb_gcv_d4_lr002_l2_45_rsm05_sub08_s123',  depth=4, learning_rate=0.02,  l2_leaf_reg=45.0, rsm=0.5, subsample=0.80, random_strength=1.0, seed=123),\n",
        "    dict(name='cb_gcv_d3_lr0025_l2_30_rsm06_sub09_s456', depth=3, learning_rate=0.025, l2_leaf_reg=30.0, rsm=0.6, subsample=0.90, random_strength=0.9, seed=456),\n",
        "    dict(name='cb_gcv_d4_lr0015_l2_55_rsm05_sub075_s789',depth=4, learning_rate=0.015, l2_leaf_reg=55.0, rsm=0.5, subsample=0.75, random_strength=1.0, seed=789),\n",
        "    dict(name='cb_gcv_d3_lr002_l2_50_rsm07_sub08_s101',  depth=3, learning_rate=0.02,  l2_leaf_reg=50.0, rsm=0.7, subsample=0.80, random_strength=0.7, seed=101),\n",
        "    dict(name='cb_gcv_d4_lr002_l2_60_rsm04_sub08_s202',  depth=4, learning_rate=0.02,  l2_leaf_reg=60.0, rsm=0.4, subsample=0.80, random_strength=0.9, seed=202),\n",
        "    dict(name='cb_gcv_d3_lr0015_l2_45_rsm05_sub09_s303', depth=3, learning_rate=0.015, l2_leaf_reg=45.0, rsm=0.5, subsample=0.90, random_strength=1.0, seed=303),\n",
        "    dict(name='cb_gcv_d4_lr0025_l2_35_rsm06_sub085_s404',depth=4, learning_rate=0.025, l2_leaf_reg=35.0, rsm=0.6, subsample=0.85, random_strength=0.8, seed=404),\n",
        "]\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "folds = list(gkf.split(X_tr_full, Y_full, groups))\n",
        "print('GroupKFold by station -> n_folds:', len(folds), 'unique stations:', len(np.unique(groups)))\n",
        "\n",
        "test_pred_list = []\n",
        "oof_list = []\n",
        "mask_list = []\n",
        "\n",
        "for v in variants:\n",
        "    print(f\"=== Variant {v['name']} ===\", flush=True)\n",
        "    oof = np.zeros((N, C), dtype=np.float32)\n",
        "    vmask = np.zeros((N, C), dtype=np.uint8)\n",
        "    test_folds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds):\n",
        "        t0 = time.time()\n",
        "        Xtr, Xva = X_tr_full[tr_idx], X_tr_full[va_idx]\n",
        "        ytr, yva = Y_full[tr_idx], Y_full[va_idx]\n",
        "        te_pred = np.zeros((X_te_full.shape[0], C), dtype=np.float32)\n",
        "        for c in range(C):\n",
        "            ytr_c = ytr[:, c]; yva_c = yva[:, c]\n",
        "            tr_pos = int((ytr_c == 1).sum()); tr_neg = int((ytr_c == 0).sum())\n",
        "            va_pos = int((yva_c == 1).sum()); va_neg = int((yva_c == 0).sum())\n",
        "            if not (tr_pos > 0 and tr_neg > 0 and va_pos > 0 and va_neg > 0):\n",
        "                continue\n",
        "            tr_pool = Pool(Xtr, label=ytr_c)\n",
        "            va_pool = Pool(Xva, label=yva_c)\n",
        "            cb = CatBoostClassifier(\n",
        "                loss_function='Logloss',\n",
        "                depth=v['depth'],\n",
        "                learning_rate=v['learning_rate'],\n",
        "                iterations=5000,\n",
        "                l2_leaf_reg=v['l2_leaf_reg'],\n",
        "                rsm=v['rsm'],\n",
        "                subsample=v['subsample'],\n",
        "                random_strength=v['random_strength'],\n",
        "                auto_class_weights='Balanced',\n",
        "                random_seed=v['seed'],\n",
        "                early_stopping_rounds=200,\n",
        "                verbose=False,\n",
        "                task_type='CPU',\n",
        "                thread_count=-1\n",
        "            )\n",
        "            cb.fit(tr_pool, eval_set=va_pool)\n",
        "            oof[va_idx, c] = cb.predict_proba(Xva)[:, 1]\n",
        "            te_pred[:, c] = cb.predict_proba(X_te_full)[:, 1]\n",
        "            vmask[va_idx, c] = 1\n",
        "        fold_macro = masked_auc_macro(yva, oof[va_idx], vmask[va_idx])\n",
        "        print(f\"  Fold {fi}: masked macro={fold_macro:.4f} | n_val={len(va_idx)} | time={time.time()-t0:.1f}s\", flush=True)\n",
        "        test_folds.append(te_pred)\n",
        "    test_mean = np.mean(test_folds, axis=0)\n",
        "    oof_list.append(oof); mask_list.append(vmask); test_pred_list.append(test_mean)\n",
        "    macro_v = masked_auc_macro(Y_full, oof, vmask)\n",
        "    print(f\"Variant OOF masked macro: {macro_v:.4f}\", flush=True)\n",
        "\n",
        "# Equal-weight rank ensemble across 8 variants\n",
        "inter_mask = mask_list[0].astype(bool)\n",
        "for m in mask_list[1:]:\n",
        "    inter_mask &= m.astype(bool)\n",
        "inter_mask = inter_mask.astype(np.uint8)\n",
        "\n",
        "ranked_oofs = []\n",
        "from scipy.stats import rankdata\n",
        "for o in oof_list:\n",
        "    Nn, Cc = o.shape\n",
        "    R = np.full_like(o, np.nan, dtype=np.float32)\n",
        "    for c in range(Cc):\n",
        "        m = inter_mask[:, c].astype(bool)\n",
        "        if not m.any():\n",
        "            continue\n",
        "        col = o[m, c]\n",
        "        r = rankdata(col, method='average')\n",
        "        R[m, c] = ((r - 1) / (len(r) - 1) if len(r) > 1 else r*0.0).astype(np.float32)\n",
        "    ranked_oofs.append(R)\n",
        "oof_ens = np.nanmean(ranked_oofs, axis=0)\n",
        "macro_ens = masked_auc_macro(Y_full, oof_ens, inter_mask)\n",
        "print(f'Ensemble (8x CB, equal-weight rank OOF) masked macro: {macro_ens:.4f}', flush=True)\n",
        "\n",
        "ranked_tests = [rank_cols(tp) for tp in test_pred_list]\n",
        "test_ens8 = np.mean(ranked_tests, axis=0)\n",
        "\n",
        "# Build submission\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_ens8[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (GroupKFold-by-station, 8x CB bag, equal-weight rank ensemble)\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GroupKFold by station -> n_folds: 5 unique stations: 13\n=== Variant cb_gcv_d3_lr002_l2_35_rsm06_sub085_s42 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7490 | n_val=31 | time=25.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7750 | n_val=27 | time=31.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7013 | n_val=26 | time=40.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.6840 | n_val=31 | time=30.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7147 | n_val=30 | time=14.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_gcv_d4_lr002_l2_45_rsm05_sub08_s123 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.6901 | n_val=31 | time=32.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7651 | n_val=27 | time=55.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7354 | n_val=26 | time=68.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7044 | n_val=31 | time=47.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.6526 | n_val=30 | time=20.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_gcv_d3_lr0025_l2_30_rsm06_sub09_s456 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7330 | n_val=31 | time=25.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7682 | n_val=27 | time=27.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.6894 | n_val=26 | time=34.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7429 | n_val=31 | time=31.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7512 | n_val=30 | time=14.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_gcv_d4_lr0015_l2_55_rsm05_sub075_s789 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7230 | n_val=31 | time=35.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7388 | n_val=27 | time=66.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7202 | n_val=26 | time=69.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7254 | n_val=31 | time=56.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7152 | n_val=30 | time=24.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_gcv_d3_lr002_l2_50_rsm07_sub08_s101 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7479 | n_val=31 | time=28.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7183 | n_val=27 | time=36.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7224 | n_val=26 | time=46.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7515 | n_val=31 | time=30.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.6993 | n_val=30 | time=18.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_gcv_d4_lr002_l2_60_rsm04_sub08_s202 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7273 | n_val=31 | time=31.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7793 | n_val=27 | time=48.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7294 | n_val=26 | time=61.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7230 | n_val=31 | time=46.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7056 | n_val=30 | time=20.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_gcv_d3_lr0015_l2_45_rsm05_sub09_s303 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7119 | n_val=31 | time=28.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7327 | n_val=27 | time=42.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7049 | n_val=26 | time=37.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7281 | n_val=31 | time=35.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.6851 | n_val=30 | time=16.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_gcv_d4_lr0025_l2_35_rsm06_sub085_s404 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7160 | n_val=31 | time=32.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7435 | n_val=27 | time=41.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.6820 | n_val=26 | time=60.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7658 | n_val=31 | time=42.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7285 | n_val=30 | time=19.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble (8x CB, equal-weight rank OOF) masked macro: 0.7474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with 1216 rows (GroupKFold-by-station, 8x CB bag, equal-weight rank ensemble)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.246032\n1  101     0.464286\n2  102     0.422619\n3  103     0.719246\n4  104     0.396825",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.246032</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.464286</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.422619</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.719246</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.396825</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "94457640-0c46-4ca1-9a20-5f4230f44b2f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Group-aware CV: Random station-to-fold search to maximize class coverage + CB bag + rank ensemble\n",
        "import numpy as np, pandas as pd, time, re, random\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "def extract_station(stem):\n",
        "    if not isinstance(stem, str): return None\n",
        "    m = re.search(r'(PC\\d+)', stem); return m.group(1) if m else None\n",
        "\n",
        "# Build groups and per-station label matrix\n",
        "train_meta = train_df[['rec_id','filename']].copy()\n",
        "train_meta['station'] = train_meta['filename'].map(extract_station).fillna('UNK')\n",
        "stations = sorted(train_meta['station'].unique().tolist())\n",
        "st2idx = {s:i for i,s in enumerate(stations)}\n",
        "N, C = X_train_df.shape[0], Y_aligned.shape[1]\n",
        "st_labels = np.zeros((len(stations), C), dtype=np.int32)\n",
        "for i, row in train_df.iterrows():\n",
        "    st = train_meta.loc[i, 'station']\n",
        "    st_i = st2idx[st]\n",
        "    st_labels[st_i] += (Y_aligned[i] > 0).astype(np.int32)\n",
        "\n",
        "# Candidate fold assignment search\n",
        "def score_assignment(assign, n_folds):\n",
        "    # assign: list length = n_stations with fold id in [0..n_folds-1]\n",
        "    # Score: minimize total missing classes across folds (both in fold and in complement), and balance fold sizes\n",
        "    total_miss = 0\n",
        "    sizes = [0]*n_folds\n",
        "    # compute per-fold label sums\n",
        "    fold_lab = [np.zeros(C, dtype=np.int32) for _ in range(n_folds)]\n",
        "    for si, f in enumerate(assign):\n",
        "        fold_lab[f] += st_labels[si]\n",
        "        sizes[f] += (train_meta['station'].values == stations[si]).sum()\n",
        "    # per-fold missing positives in val\n",
        "    for f in range(n_folds):\n",
        "        val_pos = fold_lab[f]\n",
        "        trn_pos = (np.sum(fold_lab, axis=0) - val_pos)\n",
        "        miss_val = int((val_pos == 0).sum())\n",
        "        miss_trn = int((trn_pos == 0).sum())\n",
        "        total_miss += (miss_val + miss_trn)\n",
        "    # size imbalance penalty\n",
        "    sizes = np.array(sizes, dtype=np.int32)\n",
        "    size_pen = int(np.var(sizes))\n",
        "    return total_miss * 1000 + size_pen  # miss dominates\n",
        "\n",
        "def random_assignments(n_st, n_folds, trials=256, seed=42):\n",
        "    rng = random.Random(seed)\n",
        "    best = None; best_score = None\n",
        "    for t in range(trials):\n",
        "        assign = [rng.randrange(n_folds) for _ in range(n_st)]\n",
        "        # ensure all folds non-empty\n",
        "        if len(set(assign)) < n_folds:\n",
        "            continue\n",
        "        sc = score_assignment(assign, n_folds)\n",
        "        if (best_score is None) or (sc < best_score):\n",
        "            best = assign; best_score = sc\n",
        "    return best, best_score\n",
        "\n",
        "n_folds = 5\n",
        "best_assign, best_sc = random_assignments(len(stations), n_folds, trials=512, seed=42)\n",
        "print('Stations:', len(stations), '| Best assign score:', best_sc, '| Assignment:', best_assign)\n",
        "st2fold = {s: f for s, f in zip(stations, best_assign)}\n",
        "fold_indices = []\n",
        "for f in range(n_folds):\n",
        "    va_idx = train_meta.index[train_meta['station'].map(st2fold) == f].to_numpy()\n",
        "    tr_idx = train_meta.index[train_meta['station'].map(st2fold) != f].to_numpy()\n",
        "    fold_indices.append((tr_idx, va_idx))\n",
        "    print(f'Fold {f}: n_val={len(va_idx)} n_trn={len(tr_idx)}')\n",
        "\n",
        "# Features and labels\n",
        "X_tr_full = X_train_df.values.astype(np.float32)\n",
        "X_te_full = X_test_df.values.astype(np.float32)\n",
        "Y_full = Y_aligned.astype(np.float32)\n",
        "N, C = X_tr_full.shape[0], Y_full.shape[1]\n",
        "\n",
        "def masked_auc_macro(y_true, y_pred, valid_mask):\n",
        "    aucs = []\n",
        "    for c in range(y_true.shape[1]):\n",
        "        m = valid_mask[:, c].astype(bool)\n",
        "        if m.sum() <= 1: continue\n",
        "        yt = y_true[m, c]; yp = y_pred[m, c]\n",
        "        if yt.max() == yt.min(): continue\n",
        "        try: aucs.append(roc_auc_score(yt, yp))\n",
        "        except Exception: pass\n",
        "    return float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "\n",
        "def rank_cols(mat):\n",
        "    from scipy.stats import rankdata\n",
        "    Nn, Cc = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(Cc):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (Nn - 1) if Nn > 1 else r*0.0\n",
        "    return out\n",
        "\n",
        "# CatBoost variants (4 diverse, shallow, strong reg)\n",
        "variants = [\n",
        "    dict(name='cb_stfold_d3_lr002_l2_35_rsm06_sub085_s42',  depth=3, learning_rate=0.02,  l2_leaf_reg=35.0, rsm=0.6, subsample=0.85, random_strength=0.8, seed=42),\n",
        "    dict(name='cb_stfold_d4_lr002_l2_45_rsm05_sub08_s123',  depth=4, learning_rate=0.02,  l2_leaf_reg=45.0, rsm=0.5, subsample=0.80, random_strength=1.0, seed=123),\n",
        "    dict(name='cb_stfold_d3_lr0025_l2_30_rsm06_sub09_s456', depth=3, learning_rate=0.025, l2_leaf_reg=30.0, rsm=0.6, subsample=0.90, random_strength=0.9, seed=456),\n",
        "    dict(name='cb_stfold_d4_lr0015_l2_55_rsm05_sub075_s789',depth=4, learning_rate=0.015, l2_leaf_reg=55.0, rsm=0.5, subsample=0.75, random_strength=1.0, seed=789),\n",
        "]\n",
        "\n",
        "test_pred_list = []; oof_list = []; mask_list = []\n",
        "for v in variants:\n",
        "    print(f\"=== Variant {v['name']} ===\", flush=True)\n",
        "    oof = np.zeros((N, C), dtype=np.float32)\n",
        "    vmask = np.zeros((N, C), dtype=np.uint8)\n",
        "    test_folds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(fold_indices):\n",
        "        t0 = time.time()\n",
        "        Xtr, Xva = X_tr_full[tr_idx], X_tr_full[va_idx]\n",
        "        ytr, yva = Y_full[tr_idx], Y_full[va_idx]\n",
        "        te_pred = np.zeros((X_te_full.shape[0], C), dtype=np.float32)\n",
        "        for c in range(C):\n",
        "            ytr_c = ytr[:, c]; yva_c = yva[:, c]\n",
        "            tr_pos = int((ytr_c == 1).sum()); tr_neg = int((ytr_c == 0).sum())\n",
        "            va_pos = int((yva_c == 1).sum()); va_neg = int((yva_c == 0).sum())\n",
        "            if not (tr_pos > 0 and tr_neg > 0 and va_pos > 0 and va_neg > 0):\n",
        "                continue\n",
        "            tr_pool = Pool(Xtr, label=ytr_c)\n",
        "            va_pool = Pool(Xva, label=yva_c)\n",
        "            cb = CatBoostClassifier(\n",
        "                loss_function='Logloss',\n",
        "                depth=v['depth'], learning_rate=v['learning_rate'],\n",
        "                iterations=5000, l2_leaf_reg=v['l2_leaf_reg'],\n",
        "                rsm=v['rsm'], subsample=v['subsample'], random_strength=v['random_strength'],\n",
        "                auto_class_weights='Balanced', random_seed=v['seed'],\n",
        "                early_stopping_rounds=200, verbose=False, task_type='CPU', thread_count=-1\n",
        "            )\n",
        "            cb.fit(tr_pool, eval_set=va_pool)\n",
        "            oof[va_idx, c] = cb.predict_proba(Xva)[:, 1]\n",
        "            te_pred[:, c] = cb.predict_proba(X_te_full)[:, 1]\n",
        "            vmask[va_idx, c] = 1\n",
        "        fold_macro = masked_auc_macro(yva, oof[va_idx], vmask[va_idx])\n",
        "        print(f\"  Fold {fi}: masked macro={fold_macro:.4f} | n_val={len(va_idx)} | time={time.time()-t0:.1f}s\", flush=True)\n",
        "        test_folds.append(te_pred)\n",
        "    test_mean = np.mean(test_folds, axis=0)\n",
        "    oof_list.append(oof); mask_list.append(vmask); test_pred_list.append(test_mean)\n",
        "    macro_v = masked_auc_macro(Y_full, oof, vmask)\n",
        "    print(f\"Variant OOF masked macro: {macro_v:.4f}\", flush=True)\n",
        "\n",
        "# Equal-weight rank ensemble across variants\n",
        "inter_mask = mask_list[0].astype(bool)\n",
        "for m in mask_list[1:]: inter_mask &= m.astype(bool)\n",
        "inter_mask = inter_mask.astype(np.uint8)\n",
        "\n",
        "ranked_oofs = []\n",
        "from scipy.stats import rankdata\n",
        "for o in oof_list:\n",
        "    Nn, Cc = o.shape\n",
        "    R = np.full_like(o, np.nan, dtype=np.float32)\n",
        "    for c in range(Cc):\n",
        "        m = inter_mask[:, c].astype(bool)\n",
        "        if not m.any(): continue\n",
        "        col = o[m, c]; r = rankdata(col, method='average')\n",
        "        R[m, c] = ((r - 1) / (len(r) - 1) if len(r) > 1 else r*0.0).astype(np.float32)\n",
        "    ranked_oofs.append(R)\n",
        "oof_ens = np.nanmean(ranked_oofs, axis=0)\n",
        "macro_ens = masked_auc_macro(Y_full, oof_ens, inter_mask)\n",
        "print(f'Ensemble (station-fold search, equal-weight rank OOF) masked macro: {macro_ens:.4f}', flush=True)\n",
        "\n",
        "ranked_tests = [rank_cols(tp) for tp in test_pred_list]\n",
        "test_ens_st = np.mean(ranked_tests, axis=0)\n",
        "\n",
        "# Build submission\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_ens_st[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (Group station-fold search CB bag, equal-weight rank ensemble)\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stations: 13 | Best assign score: 26107 | Assignment: [4, 4, 1, 2, 0, 3, 0, 2, 3, 0, 0, 1, 2]\nFold 0: n_val=48 n_trn=97\nFold 1: n_val=30 n_trn=115\nFold 2: n_val=18 n_trn=127\nFold 3: n_val=22 n_trn=123\nFold 4: n_val=27 n_trn=118\n=== Variant cb_stfold_d3_lr002_l2_35_rsm06_sub085_s42 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.6392 | n_val=48 | time=34.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7072 | n_val=30 | time=13.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7767 | n_val=18 | time=18.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.6884 | n_val=22 | time=40.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7750 | n_val=27 | time=31.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_stfold_d4_lr002_l2_45_rsm05_sub08_s123 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.6342 | n_val=48 | time=59.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.6465 | n_val=30 | time=12.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.8068 | n_val=18 | time=35.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.6753 | n_val=22 | time=58.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7651 | n_val=27 | time=55.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_stfold_d3_lr0025_l2_30_rsm06_sub09_s456 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.6107 | n_val=48 | time=30.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7338 | n_val=30 | time=8.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.8315 | n_val=18 | time=14.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7535 | n_val=22 | time=33.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7682 | n_val=27 | time=27.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_stfold_d4_lr0015_l2_55_rsm05_sub075_s789 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.6316 | n_val=48 | time=62.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.6725 | n_val=30 | time=13.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.8232 | n_val=18 | time=43.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7336 | n_val=22 | time=72.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7388 | n_val=27 | time=67.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble (station-fold search, equal-weight rank OOF) masked macro: 0.7528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with 1216 rows (Group station-fold search CB bag, equal-weight rank ensemble)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.246032\n1  101     0.452381\n2  102     0.440476\n3  103     0.666667\n4  104     0.392857",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.246032</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.452381</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.440476</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.666667</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.392857</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "c736b43d-30d2-4af6-b0a9-85d3ab5b6fd4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Station-to-fold assignment search with constraints (5 folds, trials=2048) -> cached fold indices for views\n",
        "import numpy as np, pandas as pd, re, random, time\n",
        "from collections import Counter\n",
        "\n",
        "def extract_station(stem):\n",
        "    if not isinstance(stem, str): return None\n",
        "    m = re.search(r'(PC\\d+)', stem); return m.group(1) if m else None\n",
        "\n",
        "# Prepare station groups and label matrix\n",
        "train_meta = train_df[['rec_id','filename']].copy()\n",
        "train_meta['station'] = train_meta['filename'].map(extract_station).fillna('UNK')\n",
        "stations = sorted(train_meta['station'].unique().tolist())\n",
        "st2idx = {s:i for i,s in enumerate(stations)}\n",
        "N, C = X_train_df.shape[0], Y_aligned.shape[1]\n",
        "st_labels = np.zeros((len(stations), C), dtype=np.int32)\n",
        "st_sizes = np.zeros(len(stations), dtype=np.int32)\n",
        "for i in range(len(train_df)):\n",
        "    st = train_meta.loc[i, 'station']\n",
        "    si = st2idx[st]\n",
        "    st_labels[si] += (Y_aligned[i] > 0).astype(np.int32)\n",
        "    st_sizes[si] += 1\n",
        "\n",
        "def build_fold_indices(assign, n_folds):\n",
        "    st2fold = {s:f for s,f in zip(stations, assign)}\n",
        "    folds = []\n",
        "    for f in range(n_folds):\n",
        "        va_idx = train_meta.index[train_meta['station'].map(st2fold) == f].to_numpy()\n",
        "        tr_idx = train_meta.index[train_meta['station'].map(st2fold) != f].to_numpy()\n",
        "        folds.append((tr_idx, va_idx))\n",
        "    return folds\n",
        "\n",
        "def violates_constraints(assign, n_folds, min_fold=25, max_fold=40, min_pos=3):\n",
        "    # Fold sizes\n",
        "    fold_sizes = [0]*n_folds\n",
        "    fold_lab = [np.zeros(C, dtype=np.int32) for _ in range(n_folds)]\n",
        "    for si, f in enumerate(assign):\n",
        "        fold_sizes[f] += st_sizes[si]\n",
        "        fold_lab[f] += st_labels[si]\n",
        "    # Size bounds check\n",
        "    for fs in fold_sizes:\n",
        "        if fs < min_fold or fs > max_fold:\n",
        "            return True\n",
        "    # Class coverage: for each fold, for each class, val_pos >= min_pos and train_pos >= min_pos\n",
        "    total = np.sum(fold_lab, axis=0)\n",
        "    for f in range(n_folds):\n",
        "        val_pos = fold_lab[f]\n",
        "        trn_pos = total - val_pos\n",
        "        if (val_pos < min_pos).any():\n",
        "            return True\n",
        "        if (trn_pos < min_pos).any():\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def score_assignment(assign, n_folds):\n",
        "    # Minimize size variance; constraints handled separately\n",
        "    fold_sizes = [0]*n_folds\n",
        "    for si, f in enumerate(assign):\n",
        "        fold_sizes[f] += st_sizes[si]\n",
        "    sizes = np.array(fold_sizes, dtype=np.float32)\n",
        "    return float(np.var(sizes))\n",
        "\n",
        "def random_search_station_folds(n_folds=5, trials=2048, seed=42, min_fold=25, max_fold=40, min_pos=3):\n",
        "    rng = random.Random(seed)\n",
        "    best = None; best_score = None; tried = 0; kept = 0\n",
        "    for t in range(trials):\n",
        "        # Random assign ensuring all folds non-empty\n",
        "        assign = [rng.randrange(n_folds) for _ in stations]\n",
        "        if len(set(assign)) < n_folds:\n",
        "            continue\n",
        "        tried += 1\n",
        "        if violates_constraints(assign, n_folds, min_fold, max_fold, min_pos):\n",
        "            continue\n",
        "        kept += 1\n",
        "        sc = score_assignment(assign, n_folds)\n",
        "        if (best_score is None) or (sc < best_score):\n",
        "            best = list(assign); best_score = sc\n",
        "    return best, best_score, tried, kept\n",
        "\n",
        "n_folds = 5\n",
        "t0 = time.time()\n",
        "assign1, sc1, tried1, kept1 = random_search_station_folds(n_folds=n_folds, trials=2048, seed=42, min_fold=25, max_fold=40, min_pos=3)\n",
        "print(f'[Search1] tried={tried1} kept={kept1} best_score={sc1} assign={assign1} time={time.time()-t0:.1f}s', flush=True)\n",
        "if assign1 is None:\n",
        "    print('WARNING: No feasible assignment found with constraints. Consider relaxing min_pos or size bounds.', flush=True)\n",
        "folds_station_opt1 = build_fold_indices(assign1, n_folds) if assign1 is not None else []\n",
        "for f,(tr_idx,va_idx) in enumerate(folds_station_opt1):\n",
        "    print(f'  Opt1 Fold {f}: n_val={len(va_idx)} n_trn={len(tr_idx)}')\n",
        "\n",
        "# Optional second view with different seed\n",
        "t1 = time.time()\n",
        "assign2, sc2, tried2, kept2 = random_search_station_folds(n_folds=n_folds, trials=2048, seed=1337, min_fold=25, max_fold=40, min_pos=3)\n",
        "print(f'[Search2] tried={tried2} kept={kept2} best_score={sc2} assign={assign2} time={time.time()-t1:.1f}s', flush=True)\n",
        "folds_station_opt2 = build_fold_indices(assign2, n_folds) if assign2 is not None else []\n",
        "for f,(tr_idx,va_idx) in enumerate(folds_station_opt2):\n",
        "    print(f'  Opt2 Fold {f}: n_val={len(va_idx)} n_trn={len(tr_idx)}')\n",
        "\n",
        "# Quick per-fold class coverage summary for Opt1\n",
        "if assign1 is not None:\n",
        "    st2fold1 = {s:f for s,f in zip(stations, assign1)}\n",
        "    for f in range(n_folds):\n",
        "        va_mask = (train_meta['station'].map(st2fold1).values == f)\n",
        "        tr_mask = ~va_mask\n",
        "        yv = Y_aligned[va_mask]\n",
        "        yt = Y_aligned[tr_mask]\n",
        "        val_pos = yv.sum(axis=0); trn_pos = yt.sum(axis=0)\n",
        "        print(f'  Opt1 Fold {f}: min val_pos={int(val_pos.min())} min trn_pos={int(trn_pos.min())}', flush=True)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Search1] tried=1504 kept=0 best_score=None assign=None time=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: No feasible assignment found with constraints. Consider relaxing min_pos or size bounds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Search2] tried=1517 kept=0 best_score=None assign=None time=0.0s\n"
          ]
        }
      ]
    },
    {
      "id": "15acc8d6-8373-4633-86ec-74f150807379",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Relaxed constrained station-to-fold assignment search (multi-try) to obtain feasible folds\n",
        "import numpy as np, random, time\n",
        "\n",
        "def relaxed_search(n_folds=5, trial_grid=(2048, 4096), size_bounds=[(25,40), (22,42), (20,45)], min_pos_list=[3,2,1], seeds=[42,1337]):\n",
        "    results = {}\n",
        "    def violates(assign, min_fold, max_fold, min_pos):\n",
        "        fold_sizes = [0]*n_folds\n",
        "        fold_lab = [np.zeros(C, dtype=np.int32) for _ in range(n_folds)]\n",
        "        for si, f in enumerate(assign):\n",
        "            fold_sizes[f] += st_sizes[si]\n",
        "            fold_lab[f] += st_labels[si]\n",
        "        for fs in fold_sizes:\n",
        "            if fs < min_fold or fs > max_fold:\n",
        "                return True\n",
        "        total = np.sum(fold_lab, axis=0)\n",
        "        for f in range(n_folds):\n",
        "            val_pos = fold_lab[f]\n",
        "            trn_pos = total - val_pos\n",
        "            if (val_pos < min_pos).any():\n",
        "                return True\n",
        "            if (trn_pos < min_pos).any():\n",
        "                return True\n",
        "        return False\n",
        "    def score(assign):\n",
        "        fold_sizes = [0]*n_folds\n",
        "        for si, f in enumerate(assign):\n",
        "            fold_sizes[f] += st_sizes[si]\n",
        "        sizes = np.array(fold_sizes, dtype=np.float32)\n",
        "        return float(np.var(sizes))\n",
        "    def build(assign):\n",
        "        st2fold = {s:f for s,f in zip(stations, assign)}\n",
        "        folds = []\n",
        "        for f in range(n_folds):\n",
        "            va_idx = train_meta.index[train_meta['station'].map(st2fold) == f].to_numpy()\n",
        "            tr_idx = train_meta.index[train_meta['station'].map(st2fold) != f].to_numpy()\n",
        "            folds.append((tr_idx, va_idx))\n",
        "        return folds\n",
        "    for sd in seeds:\n",
        "        got = False\n",
        "        best = None; best_sc = None; best_cfg = None; tried_total=0; kept_total=0\n",
        "        for trials in trial_grid:\n",
        "            if got: break\n",
        "            for (mn_sz, mx_sz) in size_bounds:\n",
        "                if got: break\n",
        "                for mp in min_pos_list:\n",
        "                    rng = random.Random(sd)\n",
        "                    best_local = None; best_sc_local=None; tried=0; kept=0\n",
        "                    t0=time.time()\n",
        "                    for t in range(trials):\n",
        "                        assign = [rng.randrange(n_folds) for _ in stations]\n",
        "                        if len(set(assign)) < n_folds:\n",
        "                            continue\n",
        "                        tried += 1\n",
        "                        if violates(assign, mn_sz, mx_sz, mp):\n",
        "                            continue\n",
        "                        kept += 1\n",
        "                        sc = score(assign)\n",
        "                        if (best_sc_local is None) or (sc < best_sc_local):\n",
        "                            best_local = list(assign); best_sc_local = sc\n",
        "                    tried_total += tried; kept_total += kept\n",
        "                    print(f'[Seed {sd}] trials={trials} size=({mn_sz},{mx_sz}) min_pos={mp} -> tried={tried} kept={kept} best_var={best_sc_local}', flush=True)\n",
        "                    if best_sc_local is not None:\n",
        "                        best = best_local; best_sc = best_sc_local; best_cfg=(mn_sz, mx_sz, mp, trials); got=True; break\n",
        "        if best is not None:\n",
        "            folds_opt = build(best)\n",
        "            key = f'opt_seed_{sd}'\n",
        "            results[key] = dict(assign=best, score=best_sc, cfg=best_cfg, folds=folds_opt, tried=tried_total, kept=kept_total)\n",
        "            print(f'[Seed {sd}] FOUND assignment: score={best_sc} cfg={best_cfg} assign={best}', flush=True)\n",
        "            for f,(tr_idx,va_idx) in enumerate(folds_opt):\n",
        "                yv = Y_aligned[va_idx]; yt = Y_aligned[tr_idx]\n",
        "                print(f'  Fold {f}: n_val={len(va_idx)} n_trn={len(tr_idx)} | min val_pos={int(yv.sum(axis=0).min())} min trn_pos={int(yt.sum(axis=0).min())}', flush=True)\n",
        "        else:\n",
        "            print(f'[Seed {sd}] No feasible assignment found under all relaxations.', flush=True)\n",
        "    return results\n",
        "\n",
        "t0=time.time()\n",
        "station_fold_search = relaxed_search()\n",
        "print('Search total time:', f'{time.time()-t0:.1f}s')\n",
        "\n",
        "# Export first two found assignments (if any) to folds_station_opt1/opt2\n",
        "found_keys = list(station_fold_search.keys())\n",
        "if len(found_keys) >= 1:\n",
        "    folds_station_opt1 = station_fold_search[found_keys[0]]['folds']\n",
        "    print('folds_station_opt1 ready with key:', found_keys[0])\n",
        "if len(found_keys) >= 2:\n",
        "    folds_station_opt2 = station_fold_search[found_keys[1]]['folds']\n",
        "    print('folds_station_opt2 ready with key:', found_keys[1])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42] trials=2048 size=(25,40) min_pos=3 -> tried=1504 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42] trials=2048 size=(25,40) min_pos=2 -> tried=1504 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42] trials=2048 size=(25,40) min_pos=1 -> tried=1504 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42] trials=2048 size=(22,42) min_pos=3 -> tried=1504 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42] trials=2048 size=(22,42) min_pos=2 -> tried=1504 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42] trials=2048 size=(22,42) min_pos=1 -> tried=1504 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42] trials=2048 size=(20,45) min_pos=3 -> tried=1504 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42] trials=2048 size=(20,45) min_pos=2 -> tried=1504 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42] trials=2048 size=(20,45) min_pos=1 -> tried=1504 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42] trials=4096 size=(25,40) min_pos=3 -> tried=3039 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42] trials=4096 size=(25,40) min_pos=2 -> tried=3039 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42] trials=4096 size=(25,40) min_pos=1 -> tried=3039 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42] trials=4096 size=(22,42) min_pos=3 -> tried=3039 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42] trials=4096 size=(22,42) min_pos=2 -> tried=3039 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42] trials=4096 size=(22,42) min_pos=1 -> tried=3039 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42] trials=4096 size=(20,45) min_pos=3 -> tried=3039 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42] trials=4096 size=(20,45) min_pos=2 -> tried=3039 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42] trials=4096 size=(20,45) min_pos=1 -> tried=3039 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 42] No feasible assignment found under all relaxations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337] trials=2048 size=(25,40) min_pos=3 -> tried=1517 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337] trials=2048 size=(25,40) min_pos=2 -> tried=1517 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337] trials=2048 size=(25,40) min_pos=1 -> tried=1517 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337] trials=2048 size=(22,42) min_pos=3 -> tried=1517 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337] trials=2048 size=(22,42) min_pos=2 -> tried=1517 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337] trials=2048 size=(22,42) min_pos=1 -> tried=1517 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337] trials=2048 size=(20,45) min_pos=3 -> tried=1517 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337] trials=2048 size=(20,45) min_pos=2 -> tried=1517 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337] trials=2048 size=(20,45) min_pos=1 -> tried=1517 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337] trials=4096 size=(25,40) min_pos=3 -> tried=3029 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337] trials=4096 size=(25,40) min_pos=2 -> tried=3029 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337] trials=4096 size=(25,40) min_pos=1 -> tried=3029 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337] trials=4096 size=(22,42) min_pos=3 -> tried=3029 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337] trials=4096 size=(22,42) min_pos=2 -> tried=3029 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337] trials=4096 size=(22,42) min_pos=1 -> tried=3029 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337] trials=4096 size=(20,45) min_pos=3 -> tried=3029 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337] trials=4096 size=(20,45) min_pos=2 -> tried=3029 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337] trials=4096 size=(20,45) min_pos=1 -> tried=3029 kept=0 best_var=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed 1337] No feasible assignment found under all relaxations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search total time: 2.5s\n"
          ]
        }
      ]
    },
    {
      "id": "1bf8ab65-cace-4345-a1f0-57af5337aa52",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# View-level CB bags (GroupKFold-by-station) + Rank Stacking (Ridge) across views A (full) and B (adv-drop30)\n",
        "import numpy as np, pandas as pd, time, re\n",
        "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.linear_model import Ridge\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "import lightgbm as lgb\n",
        "\n",
        "def extract_station(stem):\n",
        "    if not isinstance(stem, str): return None\n",
        "    m = re.search(r'(PC\\d+)', stem); return m.group(1) if m else None\n",
        "\n",
        "def masked_auc_macro(y_true, y_pred, valid_mask):\n",
        "    aucs = []\n",
        "    for c in range(y_true.shape[1]):\n",
        "        m = valid_mask[:, c].astype(bool)\n",
        "        if m.sum() <= 1: continue\n",
        "        yt = y_true[m, c]; yp = y_pred[m, c]\n",
        "        if yt.max() == yt.min(): continue\n",
        "        try: aucs.append(roc_auc_score(yt, yp))\n",
        "        except Exception: pass\n",
        "    return float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "\n",
        "def rank_cols(mat):\n",
        "    from scipy.stats import rankdata\n",
        "    Nn, Cc = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(Cc):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (Nn - 1) if Nn > 1 else r*0.0\n",
        "    return out\n",
        "\n",
        "# Fixed 5-fold GroupKFold by station\n",
        "train_meta = train_df[['rec_id','filename']].copy()\n",
        "train_meta['station'] = train_meta['filename'].map(extract_station).fillna('UNK')\n",
        "groups = train_meta['station'].values\n",
        "X_full = X_train_df.values.astype(np.float32)\n",
        "T_full = X_test_df.values.astype(np.float32)\n",
        "Y_full = Y_aligned.astype(np.float32)\n",
        "N, C = X_full.shape[0], Y_full.shape[1]\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "folds = list(gkf.split(X_full, Y_full, groups))\n",
        "print('GroupKFold-by-station: folds=', len(folds), 'unique stations=', len(np.unique(groups))),\n",
        "\n",
        "# ---- View A: Full features, 4 CB variants, equal-weight rank within view ----\n",
        "variants_A = [\n",
        "    dict(depth=3, learning_rate=0.02,  l2_leaf_reg=30.0, rsm=0.6, subsample=0.85, random_strength=0.8, seed=42),\n",
        "    dict(depth=4, learning_rate=0.02,  l2_leaf_reg=40.0, rsm=0.5, subsample=0.80, random_strength=1.0, seed=123),\n",
        "    dict(depth=3, learning_rate=0.025, l2_leaf_reg=25.0, rsm=0.6, subsample=0.85, random_strength=0.8, seed=456),\n",
        "    dict(depth=4, learning_rate=0.015, l2_leaf_reg=50.0, rsm=0.5, subsample=0.75, random_strength=1.0, seed=789),\n",
        "]\n",
        "\n",
        "def train_view(X_tr, T_te, Y, folds, variants, label='A'):\n",
        "    N, C = X_tr.shape[0], Y.shape[1]\n",
        "    view_oofs = []\n",
        "    view_masks = []\n",
        "    view_tests = []\n",
        "    for vi, v in enumerate(variants):\n",
        "        print(f'[View {label}] Variant {vi+1}/{len(variants)}', flush=True)\n",
        "        oof = np.zeros((N, C), dtype=np.float32)\n",
        "        vmask = np.zeros((N, C), dtype=np.uint8)\n",
        "        test_folds = []\n",
        "        for fi, (tr_idx, va_idx) in enumerate(folds):\n",
        "            t0 = time.time()\n",
        "            Xtr, Xva = X_tr[tr_idx], X_tr[va_idx]\n",
        "            ytr, yva = Y[tr_idx], Y[va_idx]\n",
        "            te_pred = np.zeros((T_te.shape[0], C), dtype=np.float32)\n",
        "            for c in range(C):\n",
        "                ytr_c = ytr[:, c]; yva_c = yva[:, c]\n",
        "                tr_pos = int((ytr_c == 1).sum()); tr_neg = int((ytr_c == 0).sum())\n",
        "                va_pos = int((yva_c == 1).sum()); va_neg = int((yva_c == 0).sum())\n",
        "                if not (tr_pos > 0 and tr_neg > 0 and va_pos > 0 and va_neg > 0):\n",
        "                    continue\n",
        "                tr_pool = Pool(Xtr, label=ytr_c)\n",
        "                va_pool = Pool(Xva, label=yva_c)\n",
        "                cb = CatBoostClassifier(\n",
        "                    loss_function='Logloss',\n",
        "                    depth=v['depth'], learning_rate=v['learning_rate'],\n",
        "                    iterations=5000, l2_leaf_reg=v['l2_leaf_reg'],\n",
        "                    rsm=v['rsm'], subsample=v['subsample'], random_strength=v['random_strength'],\n",
        "                    auto_class_weights='Balanced', random_seed=v['seed'],\n",
        "                    early_stopping_rounds=200, verbose=False, task_type='CPU', thread_count=-1\n",
        "                )\n",
        "                cb.fit(tr_pool, eval_set=va_pool)\n",
        "                oof[va_idx, c] = cb.predict_proba(Xva)[:, 1]\n",
        "                te_pred[:, c] = cb.predict_proba(T_te)[:, 1]\n",
        "                vmask[va_idx, c] = 1\n",
        "            fold_macro = masked_auc_macro(yva, oof[va_idx], vmask[va_idx])\n",
        "            print(f'  Fold {fi}: masked macro={fold_macro:.4f} | n_val={len(va_idx)} | time={time.time()-t0:.1f}s', flush=True)\n",
        "            test_folds.append(te_pred)\n",
        "        view_oofs.append(oof)\n",
        "        view_masks.append(vmask)\n",
        "        view_tests.append(np.mean(test_folds, axis=0))\n",
        "        macro_v = masked_auc_macro(Y, oof, vmask)\n",
        "        print(f'[View {label}] Variant OOF masked macro: {macro_v:.4f}', flush=True)\n",
        "    # intersection mask across variants\n",
        "    inter_mask = view_masks[0].astype(bool)\n",
        "    for m in view_masks[1:]: inter_mask &= m.astype(bool)\n",
        "    inter_mask = inter_mask.astype(np.uint8)\n",
        "    # rank-avg OOF within view\n",
        "    ranked_oofs = []\n",
        "    from scipy.stats import rankdata\n",
        "    for o in view_oofs:\n",
        "        Nn, Cc = o.shape\n",
        "        R = np.full_like(o, np.nan, dtype=np.float32)\n",
        "        for c in range(Cc):\n",
        "            m = inter_mask[:, c].astype(bool)\n",
        "            if not m.any(): continue\n",
        "            col = o[m, c]; r = rankdata(col, method='average')\n",
        "            R[m, c] = ((r - 1) / (len(r) - 1) if len(r) > 1 else r*0.0).astype(np.float32)\n",
        "        ranked_oofs.append(R)\n",
        "    view_oof_rank = np.nanmean(ranked_oofs, axis=0)\n",
        "    # rank-avg test within view\n",
        "    ranked_tests = [rank_cols(tp) for tp in view_tests]\n",
        "    view_test_rank = np.mean(ranked_tests, axis=0)\n",
        "    view_macro = masked_auc_macro(Y, view_oof_rank, inter_mask)\n",
        "    print(f'[View {label}] View-level OOF masked macro: {view_macro:.4f}', flush=True)\n",
        "    return view_oof_rank.astype(np.float32), inter_mask.astype(np.uint8), view_test_rank.astype(np.float32)\n",
        "\n",
        "OA, MA, TA = train_view(X_full, T_full, Y_full, folds, variants_A, label='A')\n",
        "\n",
        "# ---- View B: Adv-drop-30, 6 CB variants per guidance ----\n",
        "print('[View B] Adversarial feature pruning (top-30) ...', flush=True)\n",
        "X_tr_full_df = X_train_df.copy(); X_te_full_df = X_test_df.copy()\n",
        "X_adv = pd.concat([X_tr_full_df.assign(is_test=0), X_te_full_df.assign(is_test=1)], axis=0).reset_index(drop=True)\n",
        "y_adv = X_adv['is_test'].values.astype(int)\n",
        "X_adv_mat = X_adv.drop(columns=['is_test']).values.astype(np.float32)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "feat_importance = np.zeros(X_adv_mat.shape[1], dtype=np.float64)\n",
        "for fi, (tr, va) in enumerate(skf.split(X_adv_mat, y_adv)):\n",
        "    Xtr, Xva = X_adv_mat[tr], X_adv_mat[va]\n",
        "    ytr, yva = y_adv[tr], y_adv[va]\n",
        "    clf = lgb.LGBMClassifier(objective='binary', n_estimators=800, learning_rate=0.05,\n",
        "                             max_depth=3, num_leaves=7, subsample=0.8, colsample_bytree=0.6,\n",
        "                             reg_alpha=0.5, reg_lambda=5.0, n_jobs=-1, random_state=42, verbosity=-1)\n",
        "    clf.fit(Xtr, ytr, eval_set=[(Xva, yva)], eval_metric='auc',\n",
        "            callbacks=[lgb.early_stopping(100, verbose=False), lgb.log_evaluation(period=0)])\n",
        "    feat_importance += clf.booster_.feature_importance(importance_type='gain').astype(np.float64)\n",
        "feat_importance /= max(1, skf.get_n_splits())\n",
        "cols = X_tr_full_df.columns.tolist()\n",
        "order = np.argsort(-feat_importance)\n",
        "drop_top = 30\n",
        "drop_cols = [cols[i] for i in order[:min(drop_top, len(cols))]]\n",
        "keep_cols = [c for c in cols if c not in drop_cols]\n",
        "print(f'[View B] Dropping top {len(drop_cols)} features; keeping {len(keep_cols)}', flush=True)\n",
        "X_B = X_tr_full_df[keep_cols].values.astype(np.float32)\n",
        "T_B = X_te_full_df[keep_cols].values.astype(np.float32)\n",
        "variants_B = []\n",
        "# depth=3, seeds=[42,123,456]\n",
        "for s in [42,123,456]:\n",
        "    variants_B.append(dict(depth=3, learning_rate=0.015, l2_leaf_reg=40.0, rsm=0.55, subsample=0.85, random_strength=0.8, seed=s))\n",
        "# depth=4, seeds=[789,101,202]\n",
        "for s in [789,101,202]:\n",
        "    variants_B.append(dict(depth=4, learning_rate=0.01,  l2_leaf_reg=50.0, rsm=0.45, subsample=0.80, random_strength=1.0, seed=s))\n",
        "OB, MB, TB = train_view(X_B, T_B, Y_full, folds, variants_B, label='B')\n",
        "\n",
        "# ---- Rank Stacking (Ridge) across views OA, OB using folds from View A ----\n",
        "print('[Stack] Rank stacking over views A,B with Ridge(alpha=15), fold-wise to avoid leakage', flush=True)\n",
        "views_oof = [OA, OB]\n",
        "views_test = [TA, TB]\n",
        "inter_mask_views = (MA.astype(bool) & MB.astype(bool)).astype(np.uint8)\n",
        "\n",
        "meta_oof = np.zeros_like(OA, dtype=np.float32)\n",
        "test_meta_folds = []\n",
        "for fi, (tr_idx, va_idx) in enumerate(folds):\n",
        "    t0 = time.time()\n",
        "    # build meta-train rows over intersection mask\n",
        "    for c in range(C):\n",
        "        m_full = inter_mask_views[:, c].astype(bool)\n",
        "        m_tr = np.zeros(N, dtype=bool); m_tr[tr_idx] = True; m_tr &= m_full\n",
        "        m_va = np.zeros(N, dtype=bool); m_va[va_idx] = True; m_va &= m_full\n",
        "        if m_tr.sum() < 5 or m_va.sum() < 2:\n",
        "            continue\n",
        "        yt_tr = Y_full[m_tr, c]\n",
        "        if yt_tr.max() == yt_tr.min():\n",
        "            continue\n",
        "        Xtr_meta = np.column_stack([v[m_tr, c] for v in views_oof]).astype(np.float32)\n",
        "        Xva_meta = np.column_stack([v[m_va, c] for v in views_oof]).astype(np.float32)\n",
        "        # Fit Ridge on ranks\n",
        "        reg = Ridge(alpha=15.0, positive=False, random_state=42)\n",
        "        try:\n",
        "            reg.fit(Xtr_meta, yt_tr)\n",
        "        except Exception:\n",
        "            continue\n",
        "        meta_oof[m_va, c] = reg.predict(Xva_meta).astype(np.float32)\n",
        "    print(f'  Stack fold {fi} done in {time.time()-t0:.1f}s', flush=True)\n",
        "    # test fold prediction: fit per-class on training rows and predict test ranks\n",
        "    te_pred = np.zeros((T_full.shape[0], C), dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        m_full = inter_mask_views[:, c].astype(bool)\n",
        "        m_tr = np.zeros(N, dtype=bool); m_tr[tr_idx] = True; m_tr &= m_full\n",
        "        if m_tr.sum() < 5: continue\n",
        "        yt_tr = Y_full[m_tr, c]\n",
        "        if yt_tr.max() == yt_tr.min():\n",
        "            continue\n",
        "        Xtr_meta = np.column_stack([v[m_tr, c] for v in views_oof]).astype(np.float32)\n",
        "        Xte_meta = np.column_stack([vt[:, c] for vt in views_test]).astype(np.float32)\n",
        "        reg = Ridge(alpha=15.0, positive=False, random_state=42)\n",
        "        try:\n",
        "            reg.fit(Xtr_meta, yt_tr)\n",
        "            te_pred[:, c] = reg.predict(Xte_meta).astype(np.float32)\n",
        "        except Exception:\n",
        "            continue\n",
        "    test_meta_folds.append(te_pred)\n",
        "\n",
        "# Evaluate meta OOF on intersection mask\n",
        "meta_macro = masked_auc_macro(Y_full, meta_oof, inter_mask_views)\n",
        "print(f'[Stack] Meta OOF masked macro: {meta_macro:.4f}', flush=True)\n",
        "test_meta = np.mean(test_meta_folds, axis=0)\n",
        "test_meta_rank = rank_cols(test_meta.astype(np.float32))\n",
        "\n",
        "# Build submission\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_meta_rank[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (Rank-stacked views A+B)\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GroupKFold-by-station: folds= 5 unique stations= 13\n[View A] Variant 1/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7448 | n_val=31 | time=25.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7934 | n_val=27 | time=34.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7036 | n_val=26 | time=38.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.6803 | n_val=31 | time=31.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7185 | n_val=30 | time=14.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View A] Variant OOF masked macro: 0.7528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View A] Variant 2/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.6899 | n_val=31 | time=31.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7857 | n_val=27 | time=52.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7296 | n_val=26 | time=68.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7026 | n_val=31 | time=42.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.6444 | n_val=30 | time=19.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View A] Variant OOF masked macro: 0.7158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View A] Variant 3/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7525 | n_val=31 | time=24.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7615 | n_val=27 | time=28.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.6980 | n_val=26 | time=33.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7031 | n_val=31 | time=40.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7544 | n_val=30 | time=14.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View A] Variant OOF masked macro: 0.7517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View A] Variant 4/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7208 | n_val=31 | time=35.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7406 | n_val=27 | time=60.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7077 | n_val=26 | time=69.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7301 | n_val=31 | time=51.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7148 | n_val=30 | time=25.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View A] Variant OOF masked macro: 0.7346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View A] View-level OOF masked macro: 0.7418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View B] Adversarial feature pruning (top-30) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View B] Dropping top 30 features; keeping 667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View B] Variant 1/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7063 | n_val=31 | time=24.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7604 | n_val=27 | time=37.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7070 | n_val=26 | time=36.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7190 | n_val=31 | time=24.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7141 | n_val=30 | time=14.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View B] Variant OOF masked macro: 0.7334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View B] Variant 2/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7385 | n_val=31 | time=25.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7322 | n_val=27 | time=37.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.6652 | n_val=26 | time=37.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7017 | n_val=31 | time=19.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7430 | n_val=30 | time=17.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View B] Variant OOF masked macro: 0.7430\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View B] Variant 3/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7208 | n_val=31 | time=25.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7306 | n_val=27 | time=39.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7077 | n_val=26 | time=36.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7200 | n_val=31 | time=28.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.6820 | n_val=30 | time=17.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View B] Variant OOF masked macro: 0.7162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View B] Variant 4/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7475 | n_val=31 | time=35.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7357 | n_val=27 | time=70.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.6511 | n_val=26 | time=52.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7260 | n_val=31 | time=42.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.6984 | n_val=30 | time=24.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View B] Variant OOF masked macro: 0.7374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View B] Variant 5/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.6936 | n_val=31 | time=35.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7588 | n_val=27 | time=65.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7067 | n_val=26 | time=48.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7651 | n_val=31 | time=43.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.6914 | n_val=30 | time=24.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View B] Variant OOF masked macro: 0.7422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View B] Variant 6/6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7296 | n_val=31 | time=34.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7376 | n_val=27 | time=66.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.6961 | n_val=26 | time=49.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7798 | n_val=31 | time=41.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7100 | n_val=30 | time=23.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View B] Variant OOF masked macro: 0.7561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View B] View-level OOF masked macro: 0.7453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stack] Rank stacking over views A,B with Ridge(alpha=15), fold-wise to avoid leakage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Stack fold 0 done in 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Stack fold 1 done in 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Stack fold 2 done in 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Stack fold 3 done in 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Stack fold 4 done in 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stack] Meta OOF masked macro: 0.6573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with 1216 rows (Rank-stacked views A+B)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.246032\n1  101     0.468254\n2  102     0.404762\n3  103     0.246032\n4  104     0.373016",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.246032</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.468254</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.404762</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.246032</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.373016</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "73bc53e0-d295-408b-ab1e-7bdeb8ae725d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fallback: rank-blend views A (full) and B (adv-drop30) with OOF-tuned weight; write submission\n",
        "import numpy as np, pandas as pd\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "def rank_cols(mat):\n",
        "    N, C = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (N - 1) if N > 1 else r*0.0\n",
        "    return out\n",
        "\n",
        "assert 'OA' in globals() and 'OB' in globals() and 'MA' in globals() and 'MB' in globals(), 'Run cell 30 to compute view ranks (OA/OB) and masks (MA/MB) first.'\n",
        "assert 'TA' in globals() and 'TB' in globals(), 'Run cell 30 to compute view test ranks (TA/TB) first.'\n",
        "\n",
        "# Intersection mask\n",
        "inter_mask = (MA.astype(bool) & MB.astype(bool)).astype(np.uint8)\n",
        "\n",
        "# Grid search weight for View B (adv-drop) on masked OOF\n",
        "best_wb, best_macro = 0.5, -1.0\n",
        "for wb in np.linspace(0.0, 1.0, 21):\n",
        "    blend_oof = (1.0 - wb) * OA + wb * OB\n",
        "    # compute masked macro AUC\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    aucs = []\n",
        "    C = blend_oof.shape[1]\n",
        "    for c in range(C):\n",
        "        m = inter_mask[:, c].astype(bool)\n",
        "        if not m.any():\n",
        "            continue\n",
        "        yt = Y_aligned[m, c]\n",
        "        yp = blend_oof[m, c]\n",
        "        if yt.max() == yt.min():\n",
        "            continue\n",
        "        try:\n",
        "            aucs.append(roc_auc_score(yt, yp))\n",
        "        except Exception:\n",
        "            pass\n",
        "    macro = float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "    if macro > best_macro:\n",
        "        best_macro = macro\n",
        "        best_wb = float(wb)\n",
        "print(f'[Blend A+B] Best OOF masked macro={best_macro:.4f} at wB={best_wb:.2f}')\n",
        "\n",
        "# Blend test ranks with best weight\n",
        "test_blend_rank = (1.0 - best_wb) * TA + best_wb * TB\n",
        "\n",
        "# Build submission\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_blend_rank[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (Rank-blend Views A+B, wB={best_wb:.2f})\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Blend A+B] Best OOF masked macro=0.7476 at wB=0.80\nWrote submission.csv with 1216 rows (Rank-blend Views A+B, wB=0.80)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.246032\n1  101     0.452910\n2  102     0.405556\n3  103     0.692460\n4  104     0.385979",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.246032</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.452910</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.405556</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.692460</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.385979</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "95a017a0-da76-4766-9178-c4a73234308f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3-view rank blend per expert fallback: View A (TA), View B (TB), View C (station-search test_ens_st)\n",
        "import numpy as np, pandas as pd\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "def ensure_rank(mat):\n",
        "    # If input not in [0,1] ranks, convert to ranks per column\n",
        "    if mat.min() < 0 or mat.max() > 1.0 + 1e-6:\n",
        "        N, C = mat.shape\n",
        "        out = np.zeros_like(mat, dtype=np.float32)\n",
        "        for c in range(C):\n",
        "            r = rankdata(mat[:, c], method='average')\n",
        "            out[:, c] = (r - 1) / (N - 1) if N > 1 else r*0.0\n",
        "        return out.astype(np.float32)\n",
        "    return mat.astype(np.float32)\n",
        "\n",
        "assert 'TA' in globals() and 'TB' in globals(), 'Run cell 30 to compute TA/TB first.'\n",
        "assert 'test_ens_st' in globals(), 'Run cell 27 to compute station-search view (test_ens_st) first.'\n",
        "\n",
        "RA = ensure_rank(TA)\n",
        "RB = ensure_rank(TB)\n",
        "RC = ensure_rank(test_ens_st)\n",
        "\n",
        "# Default weights: wA=0.2 (full), wB=0.4 (adv-drop30), wC=0.4 (station-search)\n",
        "wA, wB, wC = 0.2, 0.4, 0.4\n",
        "test_blend_rank_3v = wA * RA + wB * RB + wC * RC\n",
        "\n",
        "# Build submission\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_blend_rank_3v[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (3-view rank blend: A=0.2, B=0.4, C=0.4)\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with 1216 rows (3-view rank blend: A=0.2, B=0.4, C=0.4)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.246032\n1  101     0.455026\n2  102     0.419841\n3  103     0.686111\n4  104     0.388624",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.246032</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.455026</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.419841</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.686111</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.388624</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "c90e3b10-7fe9-41eb-baeb-5e210faf257f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3-view OOF-tuned rank blend (Views A:OA/TA, B:OB/TB, C:oof_ens/test_ens_st) and submission\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "def masked_auc_macro(y_true, y_pred, valid_mask):\n",
        "    aucs = []\n",
        "    C = y_true.shape[1]\n",
        "    for c in range(C):\n",
        "        m = valid_mask[:, c].astype(bool)\n",
        "        if m.sum() <= 1:\n",
        "            continue\n",
        "        yt = y_true[m, c]; yp = y_pred[m, c]\n",
        "        if yt.max() == yt.min():\n",
        "            continue\n",
        "        try:\n",
        "            aucs.append(roc_auc_score(yt, yp))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "\n",
        "def ensure_rank(mat):\n",
        "    if mat.min() < 0 or mat.max() > 1.0 + 1e-6:\n",
        "        N, C = mat.shape\n",
        "        out = np.zeros_like(mat, dtype=np.float32)\n",
        "        for c in range(C):\n",
        "            r = rankdata(mat[:, c], method='average')\n",
        "            out[:, c] = (r - 1) / (N - 1) if N > 1 else r*0.0\n",
        "        return out.astype(np.float32)\n",
        "    return mat.astype(np.float32)\n",
        "\n",
        "assert 'OA' in globals() and 'OB' in globals() and 'MA' in globals() and 'MB' in globals(), 'Run cell 30 first to get OA/OB/MA/MB/TA/TB.'\n",
        "assert 'TA' in globals() and 'TB' in globals(), 'Run cell 30 first to get TA/TB.'\n",
        "assert 'oof_ens' in globals() and 'test_ens_st' in globals(), 'Run cell 27 first to get oof_ens and test_ens_st.'\n",
        "\n",
        "# View oofs and masks\n",
        "OA_r = ensure_rank(OA)\n",
        "OB_r = ensure_rank(OB)\n",
        "OC_r = ensure_rank(oof_ens.astype(np.float32))\n",
        "MC = np.isfinite(OC_r).astype(np.uint8)  # mask where OC exists\n",
        "\n",
        "# Intersection mask across all three views\n",
        "inter_mask_abc = (MA.astype(bool) & MB.astype(bool) & MC.astype(bool)).astype(np.uint8)\n",
        "\n",
        "# Grid search weights wB, wC (wA = 1 - wB - wC)\n",
        "best_wB, best_wC, best_macro = 0.4, 0.4, -1.0\n",
        "grid = np.linspace(0.0, 1.0, 21)\n",
        "for wB in grid:\n",
        "    for wC in grid:\n",
        "        wA = 1.0 - wB - wC\n",
        "        if wA < 0 or wA > 1:\n",
        "            continue\n",
        "        blend_oof = wA * OA_r + wB * OB_r + wC * OC_r\n",
        "        macro = masked_auc_macro(Y_aligned, blend_oof, inter_mask_abc)\n",
        "        if macro > best_macro:\n",
        "            best_macro = macro; best_wB = float(wB); best_wC = float(wC); best_wA = float(wA)\n",
        "print(f'[3-view blend] Best masked OOF macro={best_macro:.4f} at weights A={best_wA:.2f}, B={best_wB:.2f}, C={best_wC:.2f}')\n",
        "\n",
        "# Blend test ranks with best weights\n",
        "RA = ensure_rank(TA); RB = ensure_rank(TB); RC = ensure_rank(test_ens_st.astype(np.float32))\n",
        "test_blend_rank_3v_opt = best_wA * RA + best_wB * RB + best_wC * RC\n",
        "\n",
        "# Build submission\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_blend_rank_3v_opt[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (3-view OOF-tuned rank blend: A={best_wA:.2f}, B={best_wB:.2f}, C={best_wC:.2f})\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3-view blend] Best masked OOF macro=0.7652 at weights A=0.50, B=0.05, C=0.45\nWrote submission.csv with 1216 rows (3-view OOF-tuned rank blend: A=0.50, B=0.05, C=0.45)\n"
          ]
        }
      ]
    },
    {
      "id": "d045365d-4868-42ac-8cfc-6dba028dc4fd",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Pseudo-labeling with high-confidence test ranks + GroupKFold-by-station CB bag (augment train folds only), rank-ensemble submission\n",
        "import re, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "def extract_station(stem):\n",
        "    if not isinstance(stem, str):\n",
        "        return None\n",
        "    m = re.search(r'(PC\\d+)', stem)\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "# Use best available test ranks for pseudo-labeling\n",
        "assert 'test_blend_rank_3v_opt' in globals() or 'test_blend_rank' in globals() or 'test_ens' in globals(), 'Need test rank predictions (run cells 30/31/27).'\n",
        "if 'test_blend_rank_3v_opt' in globals():\n",
        "    T_rank = test_blend_rank_3v_opt.astype(np.float32)\n",
        "elif 'test_blend_rank' in globals():\n",
        "    T_rank = test_blend_rank.astype(np.float32)\n",
        "else:\n",
        "    T_rank = test_ens.astype(np.float32)\n",
        "\n",
        "# Build test rec_id index map\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "test_rec_sorted = pd.Index(sorted(test_rec_ids))\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(test_rec_sorted)}\n",
        "\n",
        "# Pseudo-labels: select top/bottom rank thresholds per class\n",
        "thr_pos, thr_neg = 0.98, 0.02\n",
        "n_te, C = T_rank.shape\n",
        "Y_pseudo = np.full((n_te, C), np.nan, dtype=np.float32)\n",
        "pos_ct = 0; neg_ct = 0\n",
        "for c in range(C):\n",
        "    col = T_rank[:, c]\n",
        "    pos_mask = col >= thr_pos\n",
        "    neg_mask = col <= thr_neg\n",
        "    Y_pseudo[pos_mask, c] = 1.0; pos_ct += int(pos_mask.sum())\n",
        "    Y_pseudo[neg_mask, c] = 0.0; neg_ct += int(neg_mask.sum())\n",
        "print(f'Pseudo labels selected: pos={pos_ct}, neg={neg_ct} across {C} classes (thr_pos={thr_pos}, thr_neg={thr_neg})', flush=True)\n",
        "\n",
        "# GroupKFold-by-station on original train; augment only the training split with pseudo-labeled test rows (class-wise)\n",
        "train_meta = train_df[['rec_id','filename']].copy()\n",
        "train_meta['station'] = train_meta['filename'].map(extract_station).fillna('UNK')\n",
        "groups = train_meta['station'].values\n",
        "\n",
        "X_tr_full = X_train_df.values.astype(np.float32)\n",
        "X_te_full = X_test_df.values.astype(np.float32)\n",
        "Y_full = Y_aligned.astype(np.float32)\n",
        "N = X_tr_full.shape[0]\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "folds = list(gkf.split(X_tr_full, Y_full, groups))\n",
        "print('GroupKFold by station -> n_folds:', len(folds), 'unique stations:', len(np.unique(groups)))\n",
        "\n",
        "def masked_auc_macro(y_true, y_pred, valid_mask):\n",
        "    aucs = []\n",
        "    for c in range(y_true.shape[1]):\n",
        "        m = valid_mask[:, c].astype(bool)\n",
        "        if m.sum() <= 1:\n",
        "            continue\n",
        "        yt = y_true[m, c]; yp = y_pred[m, c]\n",
        "        if yt.max() == yt.min():\n",
        "            continue\n",
        "        try:\n",
        "            aucs.append(roc_auc_score(yt, yp))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "\n",
        "def rank_cols(mat):\n",
        "    Nn, Cc = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(Cc):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (Nn - 1) if Nn > 1 else r*0.0\n",
        "    return out\n",
        "\n",
        "# Two diverse CB variants to start (can add more later)\n",
        "variants = [\n",
        "    dict(name='cb_pl_d3_lr0015_l2_45_rsm06_sub085_s42', depth=3, learning_rate=0.015, l2_leaf_reg=45.0, rsm=0.6, subsample=0.85, random_strength=0.8, seed=42),\n",
        "    dict(name='cb_pl_d4_lr0015_l2_55_rsm05_sub08_s123', depth=4, learning_rate=0.015, l2_leaf_reg=55.0, rsm=0.5, subsample=0.80, random_strength=1.0, seed=123),\n",
        "]\n",
        "\n",
        "test_pred_list = []\n",
        "oof_list = []\n",
        "mask_list = []\n",
        "\n",
        "for v in variants:\n",
        "    print(f\"=== Variant {v['name']} (pseudo-augmented folds) ===\", flush=True)\n",
        "    oof = np.zeros((N, C), dtype=np.float32)\n",
        "    vmask = np.zeros((N, C), dtype=np.uint8)\n",
        "    test_folds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds):\n",
        "        t0 = time.time()\n",
        "        Xtr, Xva = X_tr_full[tr_idx], X_tr_full[va_idx]\n",
        "        ytr, yva = Y_full[tr_idx], Y_full[va_idx]\n",
        "        te_pred = np.zeros((X_te_full.shape[0], C), dtype=np.float32)\n",
        "        for c in range(C):\n",
        "            # Build class-wise augmented training data: original train split + pseudo rows with labels for class c\n",
        "            ytr_c = ytr[:, c]; yva_c = yva[:, c]\n",
        "            valid_val = (yva_c.max() != yva_c.min())\n",
        "            tr_pos = int((ytr_c == 1).sum()); tr_neg = int((ytr_c == 0).sum())\n",
        "            if not (tr_pos > 0 and tr_neg > 0) or not valid_val:\n",
        "                continue\n",
        "            # Select pseudo rows for class c\n",
        "            pseudo_mask_c = np.isfinite(Y_pseudo[:, c])\n",
        "            X_pseudo_c = X_te_full[pseudo_mask_c]\n",
        "            y_pseudo_c = Y_pseudo[pseudo_mask_c, c].astype(np.float32)\n",
        "            # Concatenate\n",
        "            if X_pseudo_c.shape[0] > 0:\n",
        "                Xtr_c = np.concatenate([Xtr, X_pseudo_c], axis=0)\n",
        "                ytr_c_aug = np.concatenate([ytr_c, y_pseudo_c], axis=0)\n",
        "            else:\n",
        "                Xtr_c = Xtr\n",
        "                ytr_c_aug = ytr_c\n",
        "            tr_pool = Pool(Xtr_c, label=ytr_c_aug)\n",
        "            va_pool = Pool(Xva, label=yva_c)\n",
        "            cb = CatBoostClassifier(\n",
        "                loss_function='Logloss',\n",
        "                depth=v['depth'],\n",
        "                learning_rate=v['learning_rate'],\n",
        "                iterations=5000,\n",
        "                l2_leaf_reg=v['l2_leaf_reg'],\n",
        "                rsm=v['rsm'],\n",
        "                subsample=v['subsample'],\n",
        "                random_strength=v['random_strength'],\n",
        "                auto_class_weights='Balanced',\n",
        "                random_seed=v['seed'],\n",
        "                early_stopping_rounds=200,\n",
        "                verbose=False,\n",
        "                task_type='CPU',\n",
        "                thread_count=-1\n",
        "            )\n",
        "            cb.fit(tr_pool, eval_set=va_pool)\n",
        "            oof[va_idx, c] = cb.predict_proba(Xva)[:, 1]\n",
        "            te_pred[:, c] = cb.predict_proba(X_te_full)[:, 1]\n",
        "            vmask[va_idx, c] = 1\n",
        "        fold_macro = masked_auc_macro(yva, oof[va_idx], vmask[va_idx])\n",
        "        print(f\"  Fold {fi}: masked macro={fold_macro:.4f} | n_val={len(va_idx)} | time={time.time()-t0:.1f}s\", flush=True)\n",
        "        test_folds.append(te_pred)\n",
        "    test_mean = np.mean(test_folds, axis=0)\n",
        "    oof_list.append(oof); mask_list.append(vmask); test_pred_list.append(test_mean)\n",
        "    macro_v = masked_auc_macro(Y_full, oof, vmask)\n",
        "    print(f\"Variant OOF masked macro: {macro_v:.4f}\", flush=True)\n",
        "\n",
        "# Rank-ensemble across variants\n",
        "inter_mask = mask_list[0].astype(bool)\n",
        "for m in mask_list[1:]:\n",
        "    inter_mask &= m.astype(bool)\n",
        "inter_mask = inter_mask.astype(np.uint8)\n",
        "\n",
        "ranked_oofs = []\n",
        "for o in oof_list:\n",
        "    Nn, Cc = o.shape\n",
        "    R = np.full_like(o, np.nan, dtype=np.float32)\n",
        "    for c in range(Cc):\n",
        "        m = inter_mask[:, c].astype(bool)\n",
        "        if not m.any():\n",
        "            continue\n",
        "        col = o[m, c]\n",
        "        r = rankdata(col, method='average')\n",
        "        R[m, c] = ((r - 1) / (len(r) - 1) if len(r) > 1 else r*0.0).astype(np.float32)\n",
        "    ranked_oofs.append(R)\n",
        "oof_ens_pl = np.nanmean(ranked_oofs, axis=0)\n",
        "macro_ens_pl = masked_auc_macro(Y_full, oof_ens_pl, inter_mask)\n",
        "print(f'Pseudo-label ensemble (rank OOF) masked macro: {macro_ens_pl:.4f}', flush=True)\n",
        "\n",
        "ranked_tests = [rank_cols(tp) for tp in test_pred_list]\n",
        "test_ens_pl = np.mean(ranked_tests, axis=0)\n",
        "\n",
        "# Build submission from pseudo-label ensemble ranks\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_ens_pl[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (Pseudo-label GroupKFold-by-station CB bag, rank ensemble)\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pseudo labels selected: pos=18, neg=16 across 19 classes (thr_pos=0.98, thr_neg=0.02)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GroupKFold by station -> n_folds: 5 unique stations: 13\n=== Variant cb_pl_d3_lr0015_l2_45_rsm06_sub085_s42 (pseudo-augmented folds) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7805 | n_val=31 | time=40.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7724 | n_val=27 | time=42.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7069 | n_val=26 | time=40.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7231 | n_val=31 | time=37.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7477 | n_val=30 | time=21.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_pl_d4_lr0015_l2_55_rsm05_sub08_s123 (pseudo-augmented folds) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.6815 | n_val=31 | time=51.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7744 | n_val=27 | time=71.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.6850 | n_val=26 | time=70.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7146 | n_val=31 | time=53.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7062 | n_val=30 | time=38.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pseudo-label ensemble (rank OOF) masked macro: 0.7470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with 1216 rows (Pseudo-label GroupKFold-by-station CB bag, rank ensemble)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.246032\n1  101     0.460317\n2  102     0.436508\n3  103     0.726190\n4  104     0.380952",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.246032</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.460317</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.436508</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.726190</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.380952</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "d1816322-c028-4de1-84ff-f761769e578c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Final 4-view blend: OOF-tuned combination of 3-view blend and pseudo-label ensemble\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "def masked_auc_macro(y_true, y_pred, valid_mask):\n",
        "    aucs = []\n",
        "    C = y_true.shape[1]\n",
        "    for c in range(C):\n",
        "        m = valid_mask[:, c].astype(bool)\n",
        "        if m.sum() <= 1:\n",
        "            continue\n",
        "        yt = y_true[m, c]; yp = y_pred[m, c]\n",
        "        if yt.max() == yt.min():\n",
        "            continue\n",
        "        try:\n",
        "            aucs.append(roc_auc_score(yt, yp))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "\n",
        "assert 'OA_r' in globals() and 'OB_r' in globals() and 'OC_r' in globals() and 'inter_mask_abc' in globals(), 'Run cell 33 first to get 3-view OOF ranks and mask.'\n",
        "assert 'best_wA' in globals() and 'best_wB' in globals() and 'best_wC' in globals(), 'Weights from cell 33 not found.'\n",
        "assert 'oof_ens_pl' in globals(), 'Run cell 34 first to compute pseudo-label OOF ensemble.'\n",
        "assert 'test_blend_rank_3v_opt' in globals() and 'test_ens_pl' in globals(), 'Need test ranks from 3-view and pseudo-label models.'\n",
        "\n",
        "# Build 3-view OOF blend\n",
        "oof_3v = best_wA * OA_r + best_wB * OB_r + best_wC * OC_r\n",
        "\n",
        "# Combined intersection mask\n",
        "mask_pl = np.isfinite(oof_ens_pl).astype(np.uint8)\n",
        "inter_mask_comb = (inter_mask_abc.astype(bool) & mask_pl.astype(bool)).astype(np.uint8)\n",
        "\n",
        "# Tune weight for pseudo-label ensemble\n",
        "best_wp, best_macro = 0.0, -1.0\n",
        "for wp in np.linspace(0.0, 1.0, 21):\n",
        "    blend_oof = (1.0 - wp) * oof_3v + wp * oof_ens_pl\n",
        "    macro = masked_auc_macro(Y_aligned, blend_oof, inter_mask_comb)\n",
        "    if macro > best_macro:\n",
        "        best_macro = macro; best_wp = float(wp)\n",
        "print(f'[4-view blend] Best masked OOF macro={best_macro:.4f} at pseudo weight={best_wp:.2f}', flush=True)\n",
        "\n",
        "# Apply best weight to test predictions\n",
        "test_final = (1.0 - best_wp) * test_blend_rank_3v_opt + best_wp * test_ens_pl\n",
        "\n",
        "# Write submission\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_final[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f'Wrote submission.csv with {len(sub_out)} rows (4-view: 3-view blend + pseudo-label ensemble, wp={best_wp:.2f})')\n",
        "display(sub_out.head())"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4-view blend] Best masked OOF macro=0.7652 at pseudo weight=0.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with 1216 rows (4-view: 3-view blend + pseudo-label ensemble, wp=0.00)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.246032\n1  101     0.464021\n2  102     0.422817\n3  103     0.700198\n4  104     0.388558",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.246032</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.464021</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.422817</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.700198</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.388558</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "74214297-4cc5-4de1-8d90-b8dde1de3bd2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Pseudo-labeling V2: consensus 2/3 views, class-conditional thresholds, hard labels with confidence weights, 5-fold GroupKFold-by-station CB bag, 4-view reblend\n",
        "import numpy as np, pandas as pd, re, time\n",
        "from sklearn.metrics import roc_auc_score, precision_score\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "def extract_station(stem):\n",
        "    if not isinstance(stem, str): return None\n",
        "    m = re.search(r'(PC\\d+)', stem); return m.group(1) if m else None\n",
        "\n",
        "def masked_auc_macro(y_true, y_pred, valid_mask):\n",
        "    aucs = []\n",
        "    C = y_true.shape[1]\n",
        "    for c in range(C):\n",
        "        m = valid_mask[:, c].astype(bool)\n",
        "        if m.sum() <= 1: continue\n",
        "        yt = y_true[m, c]; yp = y_pred[m, c]\n",
        "        if yt.max() == yt.min(): continue\n",
        "        try: aucs.append(roc_auc_score(yt, yp))\n",
        "        except Exception: pass\n",
        "    return float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "\n",
        "def rank_cols(mat):\n",
        "    N, C = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (N - 1) if N > 1 else r*0.0\n",
        "    return out.astype(np.float32)\n",
        "\n",
        "# Preconditions\n",
        "assert 'OA_r' in globals() and 'OB_r' in globals() and 'OC_r' in globals() and 'inter_mask_abc' in globals(), 'Run cell 33 first to get OOF ranks/mask.'\n",
        "assert 'TA' in globals() and 'TB' in globals() and 'test_ens_st' in globals(), 'Run cells 30 and 27 first to get test view ranks.'\n",
        "assert 'test_blend_rank_3v_opt' in globals(), 'Run cell 33 to get best 3-view test ranks.'\n",
        "\n",
        "# Per-class class-conditional positive threshold t_pos_c: smallest threshold with precision >= 0.90 on OOF intersection mask,\n",
        "# using the averaged OOF rank across the three views.\n",
        "OOF_mean = (OA_r + OB_r + OC_r) / 3.0\n",
        "C = OOF_mean.shape[1]\n",
        "t_pos = np.full(C, 0.90, dtype=np.float32)  # fallback\n",
        "for c in range(C):\n",
        "    m = inter_mask_abc[:, c].astype(bool)\n",
        "    if not m.any():\n",
        "        continue\n",
        "    yt = Y_aligned[m, c].astype(np.int32)\n",
        "    if yt.max() == yt.min():\n",
        "        continue\n",
        "    yp = OOF_mean[m, c]\n",
        "    # grid thresholds from 0.5 to 0.99, finer near top\n",
        "    grid = np.unique(np.concatenate([np.linspace(0.5, 0.95, 10), np.linspace(0.95, 0.99, 9)]))\n",
        "    found = False\n",
        "    for thr in grid:\n",
        "        pred = (yp >= thr).astype(int)\n",
        "        if pred.sum() == 0:\n",
        "            continue\n",
        "        prec = precision_score(yt, pred, zero_division=0)\n",
        "        if prec >= 0.90:\n",
        "            t_pos[c] = float(thr)\n",
        "            found = True\n",
        "            break\n",
        "    if not found:\n",
        "        t_pos[c] = 0.90\n",
        "\n",
        "# Build consensus pseudo-positive mask on TEST:\n",
        "# require at least 2/3 views >= t_pos_c and mean >= t_pos_c.\n",
        "RA = TA.astype(np.float32); RB = TB.astype(np.float32); RC = test_ens_st.astype(np.float32)\n",
        "RA = RA if RA.min() >= 0 and RA.max() <= 1.0 else rank_cols(RA)\n",
        "RB = RB if RB.min() >= 0 and RB.max() <= 1.0 else rank_cols(RB)\n",
        "RC = RC if RC.min() >= 0 and RC.max() <= 1.0 else rank_cols(RC)\n",
        "T_mean = (RA + RB + RC) / 3.0\n",
        "n_te = RA.shape[0]\n",
        "Y_pseudo = np.full((n_te, C), np.nan, dtype=np.float32)\n",
        "added_ct = 0\n",
        "for c in range(C):\n",
        "    thr = t_pos[c]\n",
        "    mA = RA[:, c] >= thr; mB = RB[:, c] >= thr; mC = RC[:, c] >= thr\n",
        "    votes = (mA.astype(int) + mB.astype(int) + mC.astype(int))\n",
        "    pos_mask = (votes >= 2) & (T_mean[:, c] >= thr)\n",
        "    # store candidate positives (we'll cap per fold/class later)\n",
        "    Y_pseudo[pos_mask, c] = 1.0\n",
        "    added_ct += int(pos_mask.sum())\n",
        "print(f'[PL V2] Initial consensus pseudo-positive candidates: {added_ct} across {C} classes')\n",
        "\n",
        "# GroupKFold-by-station (5 folds)\n",
        "train_meta = train_df[['rec_id','filename']].copy()\n",
        "train_meta['station'] = train_meta['filename'].map(extract_station).fillna('UNK')\n",
        "groups = train_meta['station'].values\n",
        "X_tr_full = X_train_df.values.astype(np.float32)\n",
        "X_te_full = X_test_df.values.astype(np.float32)\n",
        "Y_full = Y_aligned.astype(np.float32)\n",
        "N = X_tr_full.shape[0]\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "folds = list(gkf.split(X_tr_full, Y_full, groups))\n",
        "print('GroupKFold by station -> n_folds:', len(folds), 'unique stations:', len(np.unique(groups)))\n",
        "\n",
        "# PL V2 CatBoost variants (stronger regularization)\n",
        "variants = [\n",
        "    dict(name='cb_plv2_d3_lr0015_l2_60_rsm05_sub08_s42', depth=3, learning_rate=0.015, l2_leaf_reg=60.0, rsm=0.5, subsample=0.80, random_strength=0.8, seed=42),\n",
        "    dict(name='cb_plv2_d4_lr0015_l2_70_rsm04_sub075_s123', depth=4, learning_rate=0.015, l2_leaf_reg=70.0, rsm=0.4, subsample=0.75, random_strength=1.0, seed=123),\n",
        "]\n",
        "\n",
        "oof_list = []; mask_list = []; test_pred_list = []\n",
        "for v in variants:\n",
        "    print(f\"=== Variant {v['name']} (PL V2 pseudo-aug) ===\", flush=True)\n",
        "    oof = np.zeros((N, C), dtype=np.float32)\n",
        "    vmask = np.zeros((N, C), dtype=np.uint8)\n",
        "    test_folds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds):\n",
        "        t0 = time.time()\n",
        "        Xtr = X_tr_full[tr_idx]; Xva = X_tr_full[va_idx]\n",
        "        ytr = Y_full[tr_idx]; yva = Y_full[va_idx]\n",
        "        te_pred = np.zeros((X_te_full.shape[0], C), dtype=np.float32)\n",
        "        # per-class training\n",
        "        for c in range(C):\n",
        "            ytr_c = ytr[:, c]; yva_c = yva[:, c]\n",
        "            if ytr_c.max() == ytr_c.min() or yva_c.max() == yva_c.min():\n",
        "                continue\n",
        "            tr_pos = int((ytr_c == 1).sum())\n",
        "            tr_neg = int((ytr_c == 0).sum())\n",
        "            if tr_pos == 0 or tr_neg == 0:\n",
        "                continue\n",
        "            # compute per-class cap\n",
        "            cap_base = min(6, int(round(0.08 * tr_pos + 1)))\n",
        "            cap_pct = int(max(1, np.floor(0.15 * tr_pos)))\n",
        "            cap = max(0, min(cap_base, cap_pct))\n",
        "            # candidate pseudo indices for this class\n",
        "            cand_mask = np.isfinite(Y_pseudo[:, c]) & (Y_pseudo[:, c] == 1.0)\n",
        "            if cap > 0 and cand_mask.any():\n",
        "                # score by blended rank (best 3-view tuned), keep top-k\n",
        "                scores = test_blend_rank_3v_opt[cand_mask, c]\n",
        "                order = np.argsort(-scores)\n",
        "                take = min(cap, order.size)\n",
        "                idx_te = np.where(cand_mask)[0][order[:take]]\n",
        "                X_p = X_te_full[idx_te]\n",
        "                y_p = np.ones(take, dtype=np.float32)\n",
        "                # confidence weights: w = clip(((rank-0.5)*2)^2, 0.3, 1.0)\n",
        "                r = test_blend_rank_3v_opt[idx_te, c].astype(np.float32)\n",
        "                w_p = np.clip(((r - 0.5) * 2.0) ** 2, 0.3, 1.0).astype(np.float32)\n",
        "                # assemble augmented training\n",
        "                Xtr_c = np.concatenate([Xtr, X_p], axis=0)\n",
        "                ytr_c_aug = np.concatenate([ytr_c, y_p], axis=0)\n",
        "                w_tr = np.concatenate([np.ones(Xtr.shape[0], dtype=np.float32), w_p], axis=0)\n",
        "            else:\n",
        "                Xtr_c = Xtr; ytr_c_aug = ytr_c; w_tr = np.ones(Xtr.shape[0], dtype=np.float32)\n",
        "            tr_pool = Pool(Xtr_c, label=ytr_c_aug, weight=w_tr)\n",
        "            va_pool = Pool(Xva, label=yva_c)\n",
        "            cb = CatBoostClassifier(\n",
        "                loss_function='Logloss',\n",
        "                depth=v['depth'], learning_rate=v['learning_rate'],\n",
        "                iterations=5000, l2_leaf_reg=v['l2_leaf_reg'],\n",
        "                rsm=v['rsm'], subsample=v['subsample'], random_strength=v['random_strength'],\n",
        "                auto_class_weights='Balanced', random_seed=v['seed'],\n",
        "                early_stopping_rounds=200, verbose=False, task_type='CPU', thread_count=-1\n",
        "            )\n",
        "            cb.fit(tr_pool, eval_set=va_pool)\n",
        "            oof[va_idx, c] = cb.predict_proba(Xva)[:, 1]\n",
        "            te_pred[:, c] = cb.predict_proba(X_te_full)[:, 1]\n",
        "            vmask[va_idx, c] = 1\n",
        "        fold_macro = masked_auc_macro(yva, oof[va_idx], vmask[va_idx])\n",
        "        print(f\"  Fold {fi}: masked macro={fold_macro:.4f} | n_val={len(va_idx)} | time={time.time()-t0:.1f}s\", flush=True)\n",
        "        test_folds.append(te_pred)\n",
        "    test_pred_list.append(np.mean(test_folds, axis=0))\n",
        "    oof_list.append(oof); mask_list.append(vmask)\n",
        "    macro_v = masked_auc_macro(Y_full, oof, vmask)\n",
        "    print(f\"Variant OOF masked macro: {macro_v:.4f}\", flush=True)\n",
        "\n",
        "# Rank-ensemble PL V2 variants\n",
        "inter_mask_plv2 = mask_list[0].astype(bool)\n",
        "for m in mask_list[1:]: inter_mask_plv2 &= m.astype(bool)\n",
        "inter_mask_plv2 = inter_mask_plv2.astype(np.uint8)\n",
        "ranked_oofs = []\n",
        "for o in oof_list:\n",
        "    Nn, Cc = o.shape\n",
        "    R = np.full_like(o, np.nan, dtype=np.float32)\n",
        "    for c in range(Cc):\n",
        "        m = inter_mask_plv2[:, c].astype(bool)\n",
        "        if not m.any(): continue\n",
        "        col = o[m, c]\n",
        "        r = rankdata(col, method='average')\n",
        "        R[m, c] = ((r - 1) / (len(r) - 1) if len(r) > 1 else r*0.0).astype(np.float32)\n",
        "    ranked_oofs.append(R)\n",
        "oof_ens_pl_v2 = np.nanmean(ranked_oofs, axis=0)\n",
        "macro_pl_v2 = masked_auc_macro(Y_full, oof_ens_pl_v2, inter_mask_plv2)\n",
        "print(f'[PL V2] Rank-ensemble OOF masked macro: {macro_pl_v2:.4f}', flush=True)\n",
        "\n",
        "ranked_tests = [rank_cols(tp) for tp in test_pred_list]\n",
        "test_ens_pl_v2 = np.mean(ranked_tests, axis=0).astype(np.float32)\n",
        "\n",
        "# 4-view reblend: (A,B,C) 3-view tuned + PL V2 with small weight grid [0..0.3]\n",
        "assert 'test_blend_rank_3v_opt' in globals(), 'Need 3-view test ranks'\n",
        "oof_3v = best_wA * OA_r + best_wB * OB_r + best_wC * OC_r\n",
        "mask_3v = inter_mask_abc\n",
        "mask_comb = (mask_3v.astype(bool) & inter_mask_plv2.astype(bool)).astype(np.uint8)\n",
        "best_wp, best_macro = 0.0, -1.0\n",
        "for wp in np.linspace(0.0, 0.3, 16):\n",
        "    blend_oof = (1.0 - wp) * oof_3v + wp * oof_ens_pl_v2\n",
        "    macro = masked_auc_macro(Y_full, blend_oof, mask_comb)\n",
        "    if macro > best_macro:\n",
        "        best_macro = macro; best_wp = float(wp)\n",
        "print(f\"[4-view (PL V2)] Best masked OOF macro={best_macro:.4f} at wp={best_wp:.2f}\")\n",
        "\n",
        "test_final = (1.0 - best_wp) * test_blend_rank_3v_opt + best_wp * test_ens_pl_v2\n",
        "\n",
        "# Write submission\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_final[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (4-view with PL V2, wp={best_wp:.2f})\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PL V2] Initial consensus pseudo-positive candidates: 54 across 19 classes\nGroupKFold by station -> n_folds: 5 unique stations: 13\n=== Variant cb_plv2_d3_lr0015_l2_60_rsm05_sub08_s42 (PL V2 pseudo-aug) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7373 | n_val=31 | time=27.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7641 | n_val=27 | time=45.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7107 | n_val=26 | time=43.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7399 | n_val=31 | time=42.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7630 | n_val=30 | time=32.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_plv2_d4_lr0015_l2_70_rsm04_sub075_s123 (PL V2 pseudo-aug) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7461 | n_val=31 | time=48.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7593 | n_val=27 | time=72.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7388 | n_val=26 | time=63.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7450 | n_val=31 | time=67.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7478 | n_val=30 | time=35.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PL V2] Rank-ensemble OOF masked macro: 0.7681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4-view (PL V2)] Best masked OOF macro=0.7682 at wp=0.30\nWrote submission.csv with 1216 rows (4-view with PL V2, wp=0.30)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.246032\n1  101     0.474815\n2  102     0.426925\n3  103     0.709186\n4  104     0.383896",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.246032</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.474815</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.426925</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.709186</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.383896</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "591346e2-fba8-4ddd-b92c-7a578e76accb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Power-mean rank transform + joint weight tuning for 3-view + PL V2 (4-view) blend\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def masked_auc_macro(y_true, y_pred, valid_mask):\n",
        "    C = y_true.shape[1]\n",
        "    aucs = []\n",
        "    for c in range(C):\n",
        "        m = valid_mask[:, c].astype(bool)\n",
        "        if m.sum() <= 1: continue\n",
        "        yt = y_true[m, c]; yp = y_pred[m, c]\n",
        "        if yt.max() == yt.min(): continue\n",
        "        try: aucs.append(roc_auc_score(yt, yp))\n",
        "        except Exception: pass\n",
        "    return float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "\n",
        "def pow_rank(mat, gamma):\n",
        "    # ranks in [0,1]; power transform keeps [0,1]\n",
        "    return np.clip(np.power(mat, gamma, dtype=np.float64), 0.0, 1.0).astype(np.float32)\n",
        "\n",
        "assert 'OA_r' in globals() and 'OB_r' in globals() and 'OC_r' in globals() and 'inter_mask_abc' in globals(), 'Run cell 33 first.'\n",
        "assert 'oof_ens_pl_v2' in globals() and 'inter_mask_plv2' in globals(), 'Run cell 36 (PL V2) first.'\n",
        "assert 'TA' in globals() and 'TB' in globals() and 'test_ens_st' in globals() and 'test_ens_pl_v2' in globals(), 'Need test ranks from views A/B/C and PL V2.'\n",
        "\n",
        "# Grids\n",
        "gammas = [1.0, 1.1, 1.2, 1.3, 1.4, 1.5]\n",
        "w_grid = np.linspace(0.0, 1.0, 21)\n",
        "wp_grid = np.linspace(0.0, 0.3, 16)\n",
        "\n",
        "best_cfg = None\n",
        "best_macro = -1.0\n",
        "\n",
        "for g in gammas:\n",
        "    A_o = pow_rank(OA_r, g); B_o = pow_rank(OB_r, g); C_o = pow_rank(OC_r, g); PL_o = pow_rank(oof_ens_pl_v2, g)\n",
        "    inter_mask_comb = (inter_mask_abc.astype(bool) & inter_mask_plv2.astype(bool)).astype(np.uint8)\n",
        "    for wB in w_grid:\n",
        "        for wC in w_grid:\n",
        "            wA = 1.0 - wB - wC\n",
        "            if wA < 0 or wA > 1: continue\n",
        "            oof_3 = wA * A_o + wB * B_o + wC * C_o\n",
        "            for wp in wp_grid:\n",
        "                blend_oof = (1.0 - wp) * oof_3 + wp * PL_o\n",
        "                macro = masked_auc_macro(Y_aligned, blend_oof, inter_mask_comb)\n",
        "                if macro > best_macro:\n",
        "                    best_macro = macro\n",
        "                    best_cfg = (float(g), float(wA), float(wB), float(wC), float(wp))\n",
        "\n",
        "g_best, wA_best, wB_best, wC_best, wp_best = best_cfg\n",
        "print(f\"[Power+Weights] Best masked OOF macro={best_macro:.4f} | gamma={g_best} | wA={wA_best:.2f} wB={wB_best:.2f} wC={wC_best:.2f} | wp={wp_best:.2f}\", flush=True)\n",
        "\n",
        "# Build test with best config\n",
        "RA = pow_rank(TA.astype(np.float32), g_best)\n",
        "RB = pow_rank(TB.astype(np.float32), g_best)\n",
        "RC = pow_rank(test_ens_st.astype(np.float32), g_best)\n",
        "RPL = pow_rank(test_ens_pl_v2.astype(np.float32), g_best)\n",
        "test_3 = wA_best * RA + wB_best * RB + wC_best * RC\n",
        "test_final = (1.0 - wp_best) * test_3 + wp_best * RPL\n",
        "\n",
        "# Write submission\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_final[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (Power+Weights tuned: gamma={g_best}, wA={wA_best:.2f}, wB={wB_best:.2f}, wC={wC_best:.2f}, wp={wp_best:.2f})\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Power+Weights] Best masked OOF macro=0.7691 | gamma=1.1 | wA=0.45 wB=0.05 wC=0.50 | wp=0.30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with 1216 rows (Power+Weights tuned: gamma=1.1, wA=0.45, wB=0.05, wC=0.50, wp=0.30)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.213841\n1  101     0.439926\n2  102     0.393238\n3  103     0.682867\n4  104     0.349133",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.213841</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.439926</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.393238</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.682867</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.349133</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "978c6013-4899-4273-b04a-83457ab87ca2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Advanced power-rank per-view tuning + joint weight search (3-view + PL V2)\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def masked_auc_macro(y_true, y_pred, valid_mask):\n",
        "    C = y_true.shape[1]\n",
        "    aucs = []\n",
        "    for c in range(C):\n",
        "        m = valid_mask[:, c].astype(bool)\n",
        "        if m.sum() <= 1: continue\n",
        "        yt = y_true[m, c]; yp = y_pred[m, c]\n",
        "        if yt.max() == yt.min(): continue\n",
        "        try: aucs.append(roc_auc_score(yt, yp))\n",
        "        except Exception: pass\n",
        "    return float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "\n",
        "def pow_rank(mat, gamma):\n",
        "    return np.clip(np.power(mat, gamma, dtype=np.float64), 0.0, 1.0).astype(np.float32)\n",
        "\n",
        "assert 'OA_r' in globals() and 'OB_r' in globals() and 'OC_r' in globals() and 'inter_mask_abc' in globals(), 'Run cell 33 first.'\n",
        "assert 'oof_ens_pl_v2' in globals() and 'inter_mask_plv2' in globals(), 'Run cell 36 first (PL V2).'\n",
        "assert 'TA' in globals() and 'TB' in globals() and 'test_ens_st' in globals() and 'test_ens_pl_v2' in globals(), 'Need test ranks for A/B/C/PL V2.'\n",
        "\n",
        "# Small per-view gamma sets to keep search tractable\n",
        "gammas = [1.0, 1.1, 1.2]\n",
        "w_grid = np.linspace(0.0, 1.0, 21)  # for B and C; A = 1 - B - C\n",
        "wp_grid = np.linspace(0.0, 0.3, 16)\n",
        "\n",
        "best = dict(macro=-1.0, gA=None, gB=None, gC=None, gP=None, wA=None, wB=None, wC=None, wp=None)\n",
        "\n",
        "inter_mask_comb = (inter_mask_abc.astype(bool) & inter_mask_plv2.astype(bool)).astype(np.uint8)\n",
        "\n",
        "for gA in gammas:\n",
        "    A_o = pow_rank(OA_r, gA)\n",
        "    for gB in gammas:\n",
        "        B_o = pow_rank(OB_r, gB)\n",
        "        for gC in gammas:\n",
        "            C_o = pow_rank(OC_r, gC)\n",
        "            for wB in w_grid:\n",
        "                for wC in w_grid:\n",
        "                    wA = 1.0 - wB - wC\n",
        "                    if wA < 0 or wA > 1: continue\n",
        "                    oof_3 = wA * A_o + wB * B_o + wC * C_o\n",
        "                    for gP in gammas:\n",
        "                        PL_o = pow_rank(oof_ens_pl_v2, gP)\n",
        "                        for wp in wp_grid:\n",
        "                            blend_oof = (1.0 - wp) * oof_3 + wp * PL_o\n",
        "                            macro = masked_auc_macro(Y_aligned, blend_oof, inter_mask_comb)\n",
        "                            if macro > best['macro']:\n",
        "                                best.update(macro=float(macro), gA=float(gA), gB=float(gB), gC=float(gC), gP=float(gP),\n",
        "                                            wA=float(wA), wB=float(wB), wC=float(wC), wp=float(wp))\n",
        "\n",
        "print(f\"[Per-view Power+Weights] Best masked OOF macro={best['macro']:.4f} | gA={best['gA']} gB={best['gB']} gC={best['gC']} gP={best['gP']} | wA={best['wA']:.2f} wB={best['wB']:.2f} wC={best['wC']:.2f} | wp={best['wp']:.2f}\", flush=True)\n",
        "\n",
        "# Build test with best config\n",
        "RA = pow_rank(TA.astype(np.float32), best['gA'])\n",
        "RB = pow_rank(TB.astype(np.float32), best['gB'])\n",
        "RC = pow_rank(test_ens_st.astype(np.float32), best['gC'])\n",
        "RPL = pow_rank(test_ens_pl_v2.astype(np.float32), best['gP'])\n",
        "test_3 = best['wA'] * RA + best['wB'] * RB + best['wC'] * RC\n",
        "test_final = (1.0 - best['wp']) * test_3 + best['wp'] * RPL\n",
        "\n",
        "# Write submission\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_final[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (Per-view Power+Weights tuned)\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "id": "ef2421de-a8f3-4fb1-9fd9-1e0cf96e841f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PL V2: add 2 more CatBoost variants to the bag, re-ensemble, re-run power+weights search, and write submission\n",
        "import numpy as np, time\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from scipy.stats import rankdata\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def rank_cols(mat):\n",
        "    N, C = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (N - 1) if N > 1 else r*0.0\n",
        "    return out.astype(np.float32)\n",
        "\n",
        "def masked_auc_macro(y_true, y_pred, valid_mask):\n",
        "    C = y_true.shape[1]\n",
        "    aucs = []\n",
        "    for c in range(C):\n",
        "        m = valid_mask[:, c].astype(bool)\n",
        "        if m.sum() <= 1: continue\n",
        "        yt = y_true[m, c]; yp = y_pred[m, c]\n",
        "        if yt.max() == yt.min(): continue\n",
        "        try: aucs.append(roc_auc_score(yt, yp))\n",
        "        except Exception: pass\n",
        "    return float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "\n",
        "# Preconditions from PL V2 cell\n",
        "assert 'folds' in globals() and 'X_tr_full' in globals() and 'X_te_full' in globals() and 'Y_full' in globals(), 'Run cell 36 first.'\n",
        "assert 'Y_pseudo' in globals() and 'test_blend_rank_3v_opt' in globals(), 'PL candidates and 3-view ranks required.'\n",
        "assert 'oof_list' in globals() and 'mask_list' in globals() and 'test_pred_list' in globals(), 'PL V2 lists missing; run cell 36.'\n",
        "\n",
        "# Two additional PL V2 variants per expert guidance\n",
        "extra_variants = [\n",
        "    dict(name='cb_plv2_d3_lr0015_l2_80_rsm05_sub08_s456', depth=3, learning_rate=0.015, l2_leaf_reg=80.0, rsm=0.5, subsample=0.80, random_strength=0.8, seed=456),\n",
        "    dict(name='cb_plv2_d4_lr0015_l2_100_rsm04_sub075_s789', depth=4, learning_rate=0.015, l2_leaf_reg=100.0, rsm=0.4, subsample=0.75, random_strength=1.0, seed=789),\n",
        "]\n",
        "\n",
        "C = Y_full.shape[1]\n",
        "for v in extra_variants:\n",
        "    print(f\"=== Variant {v['name']} (PL V2 pseudo-aug EXTRA) ===\", flush=True)\n",
        "    oof = np.zeros((X_tr_full.shape[0], C), dtype=np.float32)\n",
        "    vmask = np.zeros((X_tr_full.shape[0], C), dtype=np.uint8)\n",
        "    test_folds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds):\n",
        "        t0 = time.time()\n",
        "        Xtr = X_tr_full[tr_idx]; Xva = X_tr_full[va_idx]\n",
        "        ytr = Y_full[tr_idx]; yva = Y_full[va_idx]\n",
        "        te_pred = np.zeros((X_te_full.shape[0], C), dtype=np.float32)\n",
        "        for c in range(C):\n",
        "            ytr_c = ytr[:, c]; yva_c = yva[:, c]\n",
        "            if ytr_c.max() == ytr_c.min() or yva_c.max() == yva_c.min():\n",
        "                continue\n",
        "            tr_pos = int((ytr_c == 1).sum()); tr_neg = int((ytr_c == 0).sum())\n",
        "            if tr_pos == 0 or tr_neg == 0:\n",
        "                continue\n",
        "            # per-class cap (same as PL V2)\n",
        "            cap_base = min(6, int(round(0.08 * tr_pos + 1)))\n",
        "            cap_pct = int(max(1, np.floor(0.15 * tr_pos)))\n",
        "            cap = max(0, min(cap_base, cap_pct))\n",
        "            cand_mask = np.isfinite(Y_pseudo[:, c]) & (Y_pseudo[:, c] == 1.0)\n",
        "            if cap > 0 and cand_mask.any():\n",
        "                scores = test_blend_rank_3v_opt[cand_mask, c]\n",
        "                order = np.argsort(-scores)\n",
        "                take = min(cap, order.size)\n",
        "                idx_te = np.where(cand_mask)[0][order[:take]]\n",
        "                X_p = X_te_full[idx_te]\n",
        "                y_p = np.ones(take, dtype=np.float32)\n",
        "                r = test_blend_rank_3v_opt[idx_te, c].astype(np.float32)\n",
        "                w_p = np.clip(((r - 0.5) * 2.0) ** 2, 0.3, 1.0).astype(np.float32)\n",
        "                Xtr_c = np.concatenate([Xtr, X_p], axis=0)\n",
        "                ytr_c_aug = np.concatenate([ytr_c, y_p], axis=0)\n",
        "                w_tr = np.concatenate([np.ones(Xtr.shape[0], dtype=np.float32), w_p], axis=0)\n",
        "            else:\n",
        "                Xtr_c = Xtr; ytr_c_aug = ytr_c; w_tr = np.ones(Xtr.shape[0], dtype=np.float32)\n",
        "            tr_pool = Pool(Xtr_c, label=ytr_c_aug, weight=w_tr)\n",
        "            va_pool = Pool(Xva, label=yva_c)\n",
        "            cb = CatBoostClassifier(\n",
        "                loss_function='Logloss',\n",
        "                depth=v['depth'], learning_rate=v['learning_rate'],\n",
        "                iterations=5000, l2_leaf_reg=v['l2_leaf_reg'],\n",
        "                rsm=v['rsm'], subsample=v['subsample'], random_strength=v['random_strength'],\n",
        "                auto_class_weights='Balanced', random_seed=v['seed'],\n",
        "                early_stopping_rounds=200, verbose=False, task_type='CPU', thread_count=-1\n",
        "            )\n",
        "            cb.fit(tr_pool, eval_set=va_pool)\n",
        "            oof[va_idx, c] = cb.predict_proba(Xva)[:, 1]\n",
        "            te_pred[:, c] = cb.predict_proba(X_te_full)[:, 1]\n",
        "            vmask[va_idx, c] = 1\n",
        "        fold_macro = masked_auc_macro(yva, oof[va_idx], vmask[va_idx])\n",
        "        print(f\"  Fold {fi}: masked macro={fold_macro:.4f} | n_val={len(va_idx)} | time={time.time()-t0:.1f}s\", flush=True)\n",
        "        test_folds.append(te_pred)\n",
        "    test_pred_list.append(np.mean(test_folds, axis=0))\n",
        "    oof_list.append(oof); mask_list.append(vmask)\n",
        "    macro_v = masked_auc_macro(Y_full, oof, vmask)\n",
        "    print(f\"Variant OOF masked macro: {macro_v:.4f}\", flush=True)\n",
        "\n",
        "# Recompute PL V2 rank-ensemble with 4 variants\n",
        "inter_mask_plv2 = mask_list[0].astype(bool)\n",
        "for m in mask_list[1:]: inter_mask_plv2 &= m.astype(bool)\n",
        "inter_mask_plv2 = inter_mask_plv2.astype(np.uint8)\n",
        "ranked_oofs = []\n",
        "for o in oof_list:\n",
        "    Nn, Cc = o.shape\n",
        "    R = np.full_like(o, np.nan, dtype=np.float32)\n",
        "    for c in range(Cc):\n",
        "        m = inter_mask_plv2[:, c].astype(bool)\n",
        "        if not m.any(): continue\n",
        "        col = o[m, c]\n",
        "        r = rankdata(col, method='average')\n",
        "        R[m, c] = ((r - 1) / (len(r) - 1) if len(r) > 1 else r*0.0).astype(np.float32)\n",
        "    ranked_oofs.append(R)\n",
        "oof_ens_pl_v2 = np.nanmean(ranked_oofs, axis=0)\n",
        "macro_pl_v2 = masked_auc_macro(Y_full, oof_ens_pl_v2, inter_mask_plv2)\n",
        "print(f\"[PL V2 x4] Rank-ensemble OOF masked macro: {macro_pl_v2:.4f}\", flush=True)\n",
        "\n",
        "ranked_tests = [rank_cols(tp) for tp in test_pred_list]\n",
        "test_ens_pl_v2 = np.mean(ranked_tests, axis=0).astype(np.float32)\n",
        "\n",
        "# Re-run power+weights grid (same as cell 37) with updated PL V2 ensemble\n",
        "def pow_rank(mat, gamma):\n",
        "    return np.clip(np.power(mat, gamma, dtype=np.float64), 0.0, 1.0).astype(np.float32)\n",
        "assert 'OA_r' in globals() and 'OB_r' in globals() and 'OC_r' in globals() and 'inter_mask_abc' in globals(), 'Need 3-view OOF ranks and mask (cell 33)'\n",
        "assert 'TA' in globals() and 'TB' in globals() and 'test_ens_st' in globals(), 'Need A/B/C test ranks'\n",
        "gammas = [1.0, 1.1, 1.2, 1.3]\n",
        "w_grid = np.linspace(0.0, 1.0, 21)\n",
        "wp_grid = np.linspace(0.0, 0.3, 16)\n",
        "best_cfg = None; best_macro = -1.0\n",
        "inter_mask_comb = (inter_mask_abc.astype(bool) & inter_mask_plv2.astype(bool)).astype(np.uint8)\n",
        "for g in gammas:\n",
        "    A_o = pow_rank(OA_r, g); B_o = pow_rank(OB_r, g); C_o = pow_rank(OC_r, g); PL_o = pow_rank(oof_ens_pl_v2, g)\n",
        "    for wB in w_grid:\n",
        "        for wC in w_grid:\n",
        "            wA = 1.0 - wB - wC\n",
        "            if wA < 0 or wA > 1: continue\n",
        "            oof_3 = wA * A_o + wB * B_o + wC * C_o\n",
        "            for wp in wp_grid:\n",
        "                blend_oof = (1.0 - wp) * oof_3 + wp * PL_o\n",
        "                macro = masked_auc_macro(Y_aligned, blend_oof, inter_mask_comb)\n",
        "                if macro > best_macro:\n",
        "                    best_macro = macro; best_cfg = (float(g), float(wA), float(wB), float(wC), float(wp))\n",
        "g_best, wA_best, wB_best, wC_best, wp_best = best_cfg\n",
        "print(f\"[Power+Weights x4 PL] Best masked OOF macro={best_macro:.4f} | gamma={g_best} | wA={wA_best:.2f} wB={wB_best:.2f} wC={wC_best:.2f} | wp={wp_best:.2f}\", flush=True)\n",
        "\n",
        "# Build and write submission\n",
        "RA = pow_rank(TA.astype(np.float32), g_best)\n",
        "RB = pow_rank(TB.astype(np.float32), g_best)\n",
        "RC = pow_rank(test_ens_st.astype(np.float32), g_best)\n",
        "RPL = pow_rank(test_ens_pl_v2.astype(np.float32), g_best)\n",
        "test_3 = wA_best * RA + wB_best * RB + wC_best * RC\n",
        "test_final = (1.0 - wp_best) * test_3 + wp_best * RPL\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_final[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (PL V2 x4 + Power+Weights tuned)\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_plv2_d3_lr0015_l2_80_rsm05_sub08_s456 (PL V2 pseudo-aug EXTRA) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7481 | n_val=31 | time=29.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7696 | n_val=27 | time=52.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.6882 | n_val=26 | time=43.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7215 | n_val=31 | time=48.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7317 | n_val=30 | time=34.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_plv2_d4_lr0015_l2_100_rsm04_sub075_s789 (PL V2 pseudo-aug EXTRA) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7556 | n_val=31 | time=51.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7519 | n_val=27 | time=76.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7085 | n_val=26 | time=64.5s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[32m     76\u001b[39m va_pool = Pool(Xva, label=yva_c)\n\u001b[32m     77\u001b[39m cb = CatBoostClassifier(\n\u001b[32m     78\u001b[39m     loss_function=\u001b[33m'\u001b[39m\u001b[33mLogloss\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     79\u001b[39m     depth=v[\u001b[33m'\u001b[39m\u001b[33mdepth\u001b[39m\u001b[33m'\u001b[39m], learning_rate=v[\u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     83\u001b[39m     early_stopping_rounds=\u001b[32m200\u001b[39m, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m, task_type=\u001b[33m'\u001b[39m\u001b[33mCPU\u001b[39m\u001b[33m'\u001b[39m, thread_count=-\u001b[32m1\u001b[39m\n\u001b[32m     84\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[43mcb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mva_pool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m oof[va_idx, c] = cb.predict_proba(Xva)[:, \u001b[32m1\u001b[39m]\n\u001b[32m     87\u001b[39m te_pred[:, c] = cb.predict_proba(X_te_full)[:, \u001b[32m1\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/catboost/core.py:5245\u001b[39m, in \u001b[36mCatBoostClassifier.fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   5242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[32m   5243\u001b[39m     CatBoostClassifier._check_is_compatible_loss(params[\u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m5245\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5246\u001b[39m \u001b[43m          \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5247\u001b[39m \u001b[43m          \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5248\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/catboost/core.py:2410\u001b[39m, in \u001b[36mCatBoost._fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   2407\u001b[39m allow_clear_pool = train_params[\u001b[33m\"\u001b[39m\u001b[33mallow_clear_pool\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   2409\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[33m'\u001b[39m\u001b[33mTraining plots\u001b[39m\u001b[33m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m.get_params())]):\n\u001b[32m-> \u001b[39m\u001b[32m2410\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_sets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minit_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   2416\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[32m   2419\u001b[39m loss = \u001b[38;5;28mself\u001b[39m._object._get_loss_function_name()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/catboost/core.py:1790\u001b[39m, in \u001b[36m_CatBoostBase._train\u001b[39m\u001b[34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_object\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_trained_model_attributes()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m_catboost.pyx:5023\u001b[39m, in \u001b[36m_catboost._CatBoost._train\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m_catboost.pyx:5072\u001b[39m, in \u001b[36m_catboost._CatBoost._train\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "48e40a52-587a-4176-a525-ce1e46a50f1f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PL V2: add 2 more CatBoost variants (total 6), re-ensemble, re-run power+weights search, and write submission\n",
        "import numpy as np, time\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from scipy.stats import rankdata\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def rank_cols(mat):\n",
        "    N, C = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (N - 1) if N > 1 else r*0.0\n",
        "    return out.astype(np.float32)\n",
        "\n",
        "def masked_auc_macro(y_true, y_pred, valid_mask):\n",
        "    C = y_true.shape[1]\n",
        "    aucs = []\n",
        "    for c in range(C):\n",
        "        m = valid_mask[:, c].astype(bool)\n",
        "        if m.sum() <= 1: continue\n",
        "        yt = y_true[m, c]; yp = y_pred[m, c]\n",
        "        if yt.max() == yt.min(): continue\n",
        "        try: aucs.append(roc_auc_score(yt, yp))\n",
        "        except Exception: pass\n",
        "    return float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "\n",
        "# Preconditions from PL V2 cell\n",
        "assert 'folds' in globals() and 'X_tr_full' in globals() and 'X_te_full' in globals() and 'Y_full' in globals(), 'Run cell 36 first.'\n",
        "assert 'Y_pseudo' in globals() and 'test_blend_rank_3v_opt' in globals(), 'PL candidates and 3-view ranks required.'\n",
        "assert 'oof_list' in globals() and 'mask_list' in globals() and 'test_pred_list' in globals(), 'PL V2 lists missing; run cell 36/39.'\n",
        "\n",
        "# Two more PL V2 variants (diverse seeds/regs)\n",
        "extra_variants2 = [\n",
        "    dict(name='cb_plv2_d3_lr0015_l2_90_rsm05_sub08_s1010', depth=3, learning_rate=0.015, l2_leaf_reg=90.0, rsm=0.5, subsample=0.80, random_strength=0.9, seed=1010),\n",
        "    dict(name='cb_plv2_d4_lr0015_l2_110_rsm04_sub075_s2020', depth=4, learning_rate=0.015, l2_leaf_reg=110.0, rsm=0.4, subsample=0.75, random_strength=1.1, seed=2020),\n",
        "]\n",
        "\n",
        "C = Y_full.shape[1]\n",
        "for v in extra_variants2:\n",
        "    print(f\"=== Variant {v['name']} (PL V2 pseudo-aug EXTRA2) ===\", flush=True)\n",
        "    oof = np.zeros((X_tr_full.shape[0], C), dtype=np.float32)\n",
        "    vmask = np.zeros((X_tr_full.shape[0], C), dtype=np.uint8)\n",
        "    test_folds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds):\n",
        "        t0 = time.time()\n",
        "        Xtr = X_tr_full[tr_idx]; Xva = X_tr_full[va_idx]\n",
        "        ytr = Y_full[tr_idx]; yva = Y_full[va_idx]\n",
        "        te_pred = np.zeros((X_te_full.shape[0], C), dtype=np.float32)\n",
        "        for c in range(C):\n",
        "            ytr_c = ytr[:, c]; yva_c = yva[:, c]\n",
        "            if ytr_c.max() == ytr_c.min() or yva_c.max() == yva_c.min():\n",
        "                continue\n",
        "            tr_pos = int((ytr_c == 1).sum()); tr_neg = int((ytr_c == 0).sum())\n",
        "            if tr_pos == 0 or tr_neg == 0:\n",
        "                continue\n",
        "            cap_base = min(6, int(round(0.08 * tr_pos + 1)))\n",
        "            cap_pct = int(max(1, np.floor(0.15 * tr_pos)))\n",
        "            cap = max(0, min(cap_base, cap_pct))\n",
        "            cand_mask = np.isfinite(Y_pseudo[:, c]) & (Y_pseudo[:, c] == 1.0)\n",
        "            if cap > 0 and cand_mask.any():\n",
        "                scores = test_blend_rank_3v_opt[cand_mask, c]\n",
        "                order = np.argsort(-scores)\n",
        "                take = min(cap, order.size)\n",
        "                idx_te = np.where(cand_mask)[0][order[:take]]\n",
        "                X_p = X_te_full[idx_te]\n",
        "                y_p = np.ones(take, dtype=np.float32)\n",
        "                r = test_blend_rank_3v_opt[idx_te, c].astype(np.float32)\n",
        "                w_p = np.clip(((r - 0.5) * 2.0) ** 2, 0.3, 1.0).astype(np.float32)\n",
        "                Xtr_c = np.concatenate([Xtr, X_p], axis=0)\n",
        "                ytr_c_aug = np.concatenate([ytr_c, y_p], axis=0)\n",
        "                w_tr = np.concatenate([np.ones(Xtr.shape[0], dtype=np.float32), w_p], axis=0)\n",
        "            else:\n",
        "                Xtr_c = Xtr; ytr_c_aug = ytr_c; w_tr = np.ones(Xtr.shape[0], dtype=np.float32)\n",
        "            tr_pool = Pool(Xtr_c, label=ytr_c_aug, weight=w_tr)\n",
        "            va_pool = Pool(Xva, label=yva_c)\n",
        "            cb = CatBoostClassifier(\n",
        "                loss_function='Logloss',\n",
        "                depth=v['depth'], learning_rate=v['learning_rate'],\n",
        "                iterations=5000, l2_leaf_reg=v['l2_leaf_reg'],\n",
        "                rsm=v['rsm'], subsample=v['subsample'], random_strength=v['random_strength'],\n",
        "                auto_class_weights='Balanced', random_seed=v['seed'],\n",
        "                early_stopping_rounds=200, verbose=False, task_type='CPU', thread_count=-1\n",
        "            )\n",
        "            cb.fit(tr_pool, eval_set=va_pool)\n",
        "            oof[va_idx, c] = cb.predict_proba(Xva)[:, 1]\n",
        "            te_pred[:, c] = cb.predict_proba(X_te_full)[:, 1]\n",
        "            vmask[va_idx, c] = 1\n",
        "        fold_macro = masked_auc_macro(yva, oof[va_idx], vmask[va_idx])\n",
        "        print(f\"  Fold {fi}: masked macro={fold_macro:.4f} | n_val={len(va_idx)} | time={time.time()-t0:.1f}s\", flush=True)\n",
        "        test_folds.append(te_pred)\n",
        "    test_pred_list.append(np.mean(test_folds, axis=0))\n",
        "    oof_list.append(oof); mask_list.append(vmask)\n",
        "    macro_v = masked_auc_macro(Y_full, oof, vmask)\n",
        "    print(f\"Variant OOF masked macro: {macro_v:.4f}\", flush=True)\n",
        "\n",
        "# Recompute PL V2 rank-ensemble with 6 variants\n",
        "inter_mask_plv2 = mask_list[0].astype(bool)\n",
        "for m in mask_list[1:]: inter_mask_plv2 &= m.astype(bool)\n",
        "inter_mask_plv2 = inter_mask_plv2.astype(np.uint8)\n",
        "ranked_oofs = []\n",
        "for o in oof_list:\n",
        "    Nn, Cc = o.shape\n",
        "    R = np.full_like(o, np.nan, dtype=np.float32)\n",
        "    for c in range(Cc):\n",
        "        m = inter_mask_plv2[:, c].astype(bool)\n",
        "        if not m.any(): continue\n",
        "        col = o[m, c]\n",
        "        r = rankdata(col, method='average')\n",
        "        R[m, c] = ((r - 1) / (len(r) - 1) if len(r) > 1 else r*0.0).astype(np.float32)\n",
        "    ranked_oofs.append(R)\n",
        "oof_ens_pl_v2 = np.nanmean(ranked_oofs, axis=0)\n",
        "macro_pl_v2 = masked_auc_macro(Y_full, oof_ens_pl_v2, inter_mask_plv2)\n",
        "print(f\"[PL V2 x6] Rank-ensemble OOF masked macro: {macro_pl_v2:.4f}\", flush=True)\n",
        "\n",
        "ranked_tests = [rank_cols(tp) for tp in test_pred_list]\n",
        "test_ens_pl_v2 = np.mean(ranked_tests, axis=0).astype(np.float32)\n",
        "\n",
        "# Re-run power+weights grid (same as cell 37) with updated PL V2 ensemble\n",
        "def pow_rank(mat, gamma):\n",
        "    return np.clip(np.power(mat, gamma, dtype=np.float64), 0.0, 1.0).astype(np.float32)\n",
        "assert 'OA_r' in globals() and 'OB_r' in globals() and 'OC_r' in globals() and 'inter_mask_abc' in globals(), 'Need 3-view OOF ranks and mask (cell 33)'\n",
        "assert 'TA' in globals() and 'TB' in globals() and 'test_ens_st' in globals(), 'Need A/B/C test ranks'\n",
        "gammas = [1.0, 1.1, 1.2, 1.3]\n",
        "w_grid = np.linspace(0.0, 1.0, 21)\n",
        "wp_grid = np.linspace(0.0, 0.3, 16)\n",
        "best_cfg = None; best_macro = -1.0\n",
        "inter_mask_comb = (inter_mask_abc.astype(bool) & inter_mask_plv2.astype(bool)).astype(np.uint8)\n",
        "for g in gammas:\n",
        "    A_o = pow_rank(OA_r, g); B_o = pow_rank(OB_r, g); C_o = pow_rank(OC_r, g); PL_o = pow_rank(oof_ens_pl_v2, g)\n",
        "    for wB in w_grid:\n",
        "        for wC in w_grid:\n",
        "            wA = 1.0 - wB - wC\n",
        "            if wA < 0 or wA > 1: continue\n",
        "            oof_3 = wA * A_o + wB * B_o + wC * C_o\n",
        "            for wp in wp_grid:\n",
        "                blend_oof = (1.0 - wp) * oof_3 + wp * PL_o\n",
        "                macro = masked_auc_macro(Y_aligned, blend_oof, inter_mask_comb)\n",
        "                if macro > best_macro:\n",
        "                    best_macro = macro; best_cfg = (float(g), float(wA), float(wB), float(wC), float(wp))\n",
        "g_best, wA_best, wB_best, wC_best, wp_best = best_cfg\n",
        "print(f\"[Power+Weights x6 PL] Best masked OOF macro={best_macro:.4f} | gamma={g_best} | wA={wA_best:.2f} wB={wB_best:.2f} wC={wC_best:.2f} | wp={wp_best:.2f}\", flush=True)\n",
        "\n",
        "# Build and write submission\n",
        "RA = pow_rank(TA.astype(np.float32), g_best)\n",
        "RB = pow_rank(TB.astype(np.float32), g_best)\n",
        "RC = pow_rank(test_ens_st.astype(np.float32), g_best)\n",
        "RPL = pow_rank(test_ens_pl_v2.astype(np.float32), g_best)\n",
        "test_3 = wA_best * RA + wB_best * RB + wC_best * RC\n",
        "test_final = (1.0 - wp_best) * test_3 + wp_best * RPL\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_final[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (PL V2 x6 + Power+Weights tuned)\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_plv2_d3_lr0015_l2_90_rsm05_sub08_s1010 (PL V2 pseudo-aug EXTRA2) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7403 | n_val=31 | time=30.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7714 | n_val=27 | time=57.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.6868 | n_val=26 | time=46.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7457 | n_val=31 | time=51.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7184 | n_val=30 | time=30.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Variant cb_plv2_d4_lr0015_l2_110_rsm04_sub075_s2020 (PL V2 pseudo-aug EXTRA2) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7493 | n_val=31 | time=51.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7645 | n_val=27 | time=80.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7043 | n_val=26 | time=64.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7585 | n_val=31 | time=69.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7370 | n_val=30 | time=37.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PL V2 x6] Rank-ensemble OOF masked macro: 0.7706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Power+Weights x6 PL] Best masked OOF macro=0.7702 | gamma=1.0 | wA=0.40 wB=0.05 wC=0.55 | wp=0.30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with 1216 rows (PL V2 x6 + Power+Weights tuned)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.246032\n1  101     0.472355\n2  102     0.429147\n3  103     0.694682\n4  104     0.385245",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.246032</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.472355</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.429147</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.694682</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.385245</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "7f75275d-42fc-4693-ac7a-60517cc195cf",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Lightweight linear View (LR on standardized tabular features with GroupKFold-by-station) + add as 5th view with small weight to the power+weights 4-view blend\n",
        "import numpy as np, pandas as pd, time, re\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "def extract_station(stem):\n",
        "    if not isinstance(stem, str): return None\n",
        "    m = re.search(r'(PC\\d+)', stem); return m.group(1) if m else None\n",
        "\n",
        "def masked_auc_macro(y_true, y_pred, valid_mask):\n",
        "    C = y_true.shape[1]\n",
        "    aucs = []\n",
        "    for c in range(C):\n",
        "        m = valid_mask[:, c].astype(bool)\n",
        "        if m.sum() <= 1: continue\n",
        "        yt = y_true[m, c]; yp = y_pred[m, c]\n",
        "        if yt.max() == yt.min(): continue\n",
        "        try: aucs.append(roc_auc_score(yt, yp))\n",
        "        except Exception: pass\n",
        "    return float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "\n",
        "def rank_cols(mat):\n",
        "    N, C = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (N - 1) if N > 1 else r*0.0\n",
        "    return out.astype(np.float32)\n",
        "\n",
        "def pow_rank(mat, gamma):\n",
        "    return np.clip(np.power(mat, gamma, dtype=np.float64), 0.0, 1.0).astype(np.float32)\n",
        "\n",
        "# Preconditions: reuse best 4-view config from power+weights (cell 37) and PL V2 (cell 36)\n",
        "assert 'g_best' in globals() and 'wA_best' in globals() and 'wB_best' in globals() and 'wC_best' in globals() and 'wp_best' in globals(), 'Run cell 37 first (power+weights best cfg)'\n",
        "assert 'OA_r' in globals() and 'OB_r' in globals() and 'OC_r' in globals() and 'oof_ens_pl_v2' in globals(), 'Need OOF ranks from views and PL V2'\n",
        "assert 'TA' in globals() and 'TB' in globals() and 'test_ens_st' in globals() and 'test_ens_pl_v2' in globals(), 'Need test ranks from views and PL V2'\n",
        "assert 'inter_mask_abc' in globals() and 'inter_mask_plv2' in globals(), 'Need masks from previous cells'\n",
        "\n",
        "# Build GroupKFold by station splits\n",
        "train_meta = train_df[['rec_id','filename']].copy()\n",
        "train_meta['station'] = train_meta['filename'].map(extract_station).fillna('UNK')\n",
        "groups = train_meta['station'].values\n",
        "X_full_tab = X_train_df.values.astype(np.float32)\n",
        "T_full_tab = X_test_df.values.astype(np.float32)\n",
        "Y_full = Y_aligned.astype(np.float32)\n",
        "N, C = X_full_tab.shape[0], Y_full.shape[1]\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "folds_lin = list(gkf.split(X_full_tab, Y_full, groups))\n",
        "\n",
        "# Train LR-OVR per fold with in-fold StandardScaler; collect OOF and test preds (rank later)\n",
        "oof_lin = np.zeros((N, C), dtype=np.float32)\n",
        "vmask_lin = np.zeros((N, C), dtype=np.uint8)\n",
        "test_lin_folds = []\n",
        "for fi, (tr_idx, va_idx) in enumerate(folds_lin):\n",
        "    t0 = time.time()\n",
        "    Xtr, Xva = X_full_tab[tr_idx], X_full_tab[va_idx]\n",
        "    ytr, yva = Y_full[tr_idx], Y_full[va_idx]\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    Xtr_s = scaler.fit_transform(Xtr)\n",
        "    Xva_s = scaler.transform(Xva)\n",
        "    Tes_s = scaler.transform(T_full_tab)\n",
        "    clf = OneVsRestClassifier(LogisticRegression(solver='lbfgs', penalty='l2', C=1.0, max_iter=2000, class_weight='balanced', n_jobs=-1))\n",
        "    clf.fit(Xtr_s, ytr)\n",
        "    va_pred = np.vstack([est.predict_proba(Xva_s)[:,1] for est in clf.estimators_]).T.astype(np.float32)\n",
        "    te_pred = np.vstack([est.predict_proba(Tes_s)[:,1] for est in clf.estimators_]).T.astype(np.float32)\n",
        "    oof_lin[va_idx] = va_pred\n",
        "    vmask_lin[va_idx] = 1\n",
        "    test_lin_folds.append(te_pred)\n",
        "    fold_macro = masked_auc_macro(yva, va_pred, np.ones_like(va_pred, dtype=np.uint8))\n",
        "    print(f\"[LR View] Fold {fi}: macro={fold_macro:.4f} | n_val={len(va_idx)} | time={time.time()-t0:.1f}s\", flush=True)\n",
        "\n",
        "# Rank OOF/test for LR view\n",
        "oof_lin_r = np.full_like(oof_lin, np.nan, dtype=np.float32)\n",
        "for c in range(C):\n",
        "    m = vmask_lin[:, c].astype(bool)\n",
        "    if not m.any(): continue\n",
        "    col = oof_lin[m, c]\n",
        "    r = rankdata(col, method='average')\n",
        "    oof_lin_r[m, c] = ((r - 1) / (len(r) - 1) if len(r) > 1 else r*0.0).astype(np.float32)\n",
        "test_lin_mean = np.mean(test_lin_folds, axis=0)\n",
        "test_lin_r = rank_cols(test_lin_mean)\n",
        "\n",
        "# Build current best 4-view OOF under power transform g_best\n",
        "A_o = pow_rank(OA_r, g_best); B_o = pow_rank(OB_r, g_best); C_o = pow_rank(OC_r, g_best); P_o = pow_rank(oof_ens_pl_v2, g_best)\n",
        "oof_3 = wA_best * A_o + wB_best * B_o + wC_best * C_o\n",
        "oof_4 = (1.0 - wp_best) * oof_3 + wp_best * P_o\n",
        "\n",
        "# Apply same power to LR view ranks\n",
        "L_o = pow_rank(oof_lin_r, g_best)\n",
        "\n",
        "# Combined intersection mask (previous 4-view mask) \u2229 LR mask\n",
        "mask_4 = (inter_mask_abc.astype(bool) & inter_mask_plv2.astype(bool)).astype(np.uint8)\n",
        "mask_lin = vmask_lin.astype(np.uint8)\n",
        "mask_comb5 = (mask_4.astype(bool) & mask_lin.astype(bool)).astype(np.uint8)\n",
        "\n",
        "# Tune small weight for LR view in [0..0.15]\n",
        "best_wl, best_macro = 0.0, masked_auc_macro(Y_full, oof_4, mask_4)\n",
        "for wl in np.linspace(0.0, 0.15, 16):\n",
        "    blend_oof5 = (1.0 - wl) * oof_4 + wl * L_o\n",
        "    macro = masked_auc_macro(Y_full, blend_oof5, mask_comb5)\n",
        "    if macro > best_macro:\n",
        "        best_macro = macro; best_wl = float(wl)\n",
        "print(f\"[5-view add LR] Best masked OOF macro={best_macro:.4f} at w_lr={best_wl:.3f}\", flush=True)\n",
        "\n",
        "# Build test with best_wl\n",
        "RA = pow_rank(TA.astype(np.float32), g_best)\n",
        "RB = pow_rank(TB.astype(np.float32), g_best)\n",
        "RC = pow_rank(test_ens_st.astype(np.float32), g_best)\n",
        "RPL = pow_rank(test_ens_pl_v2.astype(np.float32), g_best)\n",
        "RL  = pow_rank(test_lin_r.astype(np.float32), g_best)\n",
        "test_3 = wA_best * RA + wB_best * RB + wC_best * RC\n",
        "test_4 = (1.0 - wp_best) * test_3 + wp_best * RPL\n",
        "test_final = (1.0 - best_wl) * test_4 + best_wl * RL\n",
        "\n",
        "# Write submission\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_final[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (5th view LR added, w_lr={best_wl:.3f}, g={g_best})\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR View] Fold 0: macro=0.6257 | n_val=31 | time=22.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR View] Fold 1: macro=0.6528 | n_val=27 | time=24.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR View] Fold 2: macro=0.6595 | n_val=26 | time=0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR View] Fold 3: macro=0.6993 | n_val=31 | time=0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR View] Fold 4: macro=0.6095 | n_val=30 | time=0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5-view add LR] Best masked OOF macro=0.7702 at w_lr=0.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with 1216 rows (5th view LR added, w_lr=0.000, g=1.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.246032\n1  101     0.472355\n2  102     0.429147\n3  103     0.694682\n4  104     0.385245",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.246032</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.472355</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.429147</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.694682</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.385245</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "43a5241e-9cf3-4138-82c6-3363327946fd",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Station one-hot feature augmentation + compact CB bag (View A_stn) + re-run power+weights (swap A with A_stn) -> submission\n",
        "import re, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "def extract_station(stem):\n",
        "    if not isinstance(stem, str): return None\n",
        "    m = re.search(r'(PC\\d+)', stem); return m.group(1) if m else None\n",
        "\n",
        "def masked_auc_macro(y_true, y_pred, valid_mask):\n",
        "    C = y_true.shape[1]\n",
        "    aucs = []\n",
        "    for c in range(C):\n",
        "        m = valid_mask[:, c].astype(bool)\n",
        "        if m.sum() <= 1: continue\n",
        "        yt = y_true[m, c]; yp = y_pred[m, c]\n",
        "        if yt.max() == yt.min(): continue\n",
        "        try: aucs.append(roc_auc_score(yt, yp))\n",
        "        except Exception: pass\n",
        "    return float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "\n",
        "def rank_cols(mat):\n",
        "    N, C = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (N - 1) if N > 1 else r*0.0\n",
        "    return out.astype(np.float32)\n",
        "\n",
        "def pow_rank(mat, gamma):\n",
        "    return np.clip(np.power(mat, gamma, dtype=np.float64), 0.0, 1.0).astype(np.float32)\n",
        "\n",
        "# Preconditions for blending later\n",
        "assert 'OB_r' in globals() and 'OC_r' in globals() and 'oof_ens_pl_v2' in globals() and 'inter_mask_plv2' in globals(), 'Run cells 30/27/36 for OB_r/OC_r/PL V2.'\n",
        "assert 'TB' in globals() and 'test_ens_st' in globals() and 'test_ens_pl_v2' in globals(), 'Need test ranks for B/C/PL V2.'\n",
        "\n",
        "# 1) Build station one-hot dummies and augment features\n",
        "train_ids_sorted = pd.Index(sorted(train_rec_ids))\n",
        "test_ids_sorted = pd.Index(sorted(test_rec_ids))\n",
        "tr_stations = train_df.set_index('rec_id').loc[train_ids_sorted, 'filename'].map(extract_station).fillna('UNK')\n",
        "te_stations = pd.Series([id2fn[int(r)] for r in test_ids_sorted], index=test_ids_sorted).map(extract_station).fillna('UNK')\n",
        "d_tr = pd.get_dummies(tr_stations, prefix='stn').astype(np.float32)\n",
        "d_te = pd.get_dummies(te_stations, prefix='stn').astype(np.float32)\n",
        "d_all_cols = sorted(set(d_tr.columns).union(set(d_te.columns)))\n",
        "d_tr = d_tr.reindex(columns=d_all_cols, fill_value=0.0).astype(np.float32)\n",
        "d_te = d_te.reindex(columns=d_all_cols, fill_value=0.0).astype(np.float32)\n",
        "X_tr_aug = np.hstack([X_train_df.values.astype(np.float32), d_tr.values.astype(np.float32)])\n",
        "X_te_aug = np.hstack([X_test_df.values.astype(np.float32), d_te.values.astype(np.float32)])\n",
        "Y_full = pd.DataFrame(Y, index=train_rec_ids).loc[train_ids_sorted].values.astype(np.float32)\n",
        "N, C = X_tr_aug.shape[0], Y_full.shape[1]\n",
        "\n",
        "# Groups for GroupKFold by station aligned to train_ids_sorted\n",
        "groups = tr_stations.values\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "folds = list(gkf.split(X_tr_aug, Y_full, groups))\n",
        "print('Station one-hot columns:', len(d_all_cols), '| GroupKFold folds:', len(folds), flush=True)\n",
        "\n",
        "# 2) Compact CB bag (3 variants) on augmented matrices -> OA_stn (ranked OOF), MA_stn (mask), TA_stn (ranked test)\n",
        "variants = [\n",
        "    dict(name='cb_stn_d3_lr002_l2_50_rsm06_sub085_s42', depth=3, learning_rate=0.02,  l2_leaf_reg=50.0, rsm=0.6, subsample=0.85, random_strength=0.8, seed=42),\n",
        "    dict(name='cb_stn_d4_lr0015_l2_60_rsm05_sub08_s123', depth=4, learning_rate=0.015, l2_leaf_reg=60.0, rsm=0.5, subsample=0.80, random_strength=1.0, seed=123),\n",
        "    dict(name='cb_stn_d3_lr002_l2_70_rsm06_sub085_s456', depth=3, learning_rate=0.02,  l2_leaf_reg=70.0, rsm=0.6, subsample=0.85, random_strength=0.8, seed=456),\n",
        "]\n",
        "oofs = []; masks = []; tests = []\n",
        "for v in variants:\n",
        "    print(f\"=== Station-aug Variant {v['name']} ===\", flush=True)\n",
        "    oof = np.zeros((N, C), dtype=np.float32)\n",
        "    vmask = np.zeros((N, C), dtype=np.uint8)\n",
        "    test_folds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds):\n",
        "        t0 = time.time()\n",
        "        Xtr, Xva = X_tr_aug[tr_idx], X_tr_aug[va_idx]\n",
        "        ytr, yva = Y_full[tr_idx], Y_full[va_idx]\n",
        "        te_pred = np.zeros((X_te_aug.shape[0], C), dtype=np.float32)\n",
        "        for c in range(C):\n",
        "            ytr_c = ytr[:, c]; yva_c = yva[:, c]\n",
        "            if ytr_c.max() == ytr_c.min() or yva_c.max() == yva_c.min():\n",
        "                continue\n",
        "            tr_pool = Pool(Xtr, label=ytr_c)\n",
        "            va_pool = Pool(Xva, label=yva_c)\n",
        "            cb = CatBoostClassifier(\n",
        "                loss_function='Logloss',\n",
        "                depth=v['depth'], learning_rate=v['learning_rate'],\n",
        "                iterations=5000, l2_leaf_reg=v['l2_leaf_reg'],\n",
        "                rsm=v['rsm'], subsample=v['subsample'], random_strength=v['random_strength'],\n",
        "                auto_class_weights='Balanced', random_seed=v['seed'],\n",
        "                early_stopping_rounds=200, verbose=False, task_type='CPU', thread_count=-1\n",
        "            )\n",
        "            cb.fit(tr_pool, eval_set=va_pool)\n",
        "            oof[va_idx, c] = cb.predict_proba(Xva)[:, 1]\n",
        "            te_pred[:, c] = cb.predict_proba(X_te_aug)[:, 1]\n",
        "            vmask[va_idx, c] = 1\n",
        "        fold_macro = masked_auc_macro(yva, oof[va_idx], vmask[va_idx])\n",
        "        print(f\"  Fold {fi}: masked macro={fold_macro:.4f} | n_val={len(va_idx)} | time={time.time()-t0:.1f}s\", flush=True)\n",
        "        test_folds.append(te_pred)\n",
        "    oofs.append(oof); masks.append(vmask); tests.append(np.mean(test_folds, axis=0))\n",
        "    macro_v = masked_auc_macro(Y_full, oof, vmask)\n",
        "    print(f\"Variant OOF masked macro: {macro_v:.4f}\", flush=True)\n",
        "\n",
        "# Rank-avg within view A_stn\n",
        "inter_mask_A = masks[0].astype(bool)\n",
        "for m in masks[1:]: inter_mask_A &= m.astype(bool)\n",
        "MA_stn = inter_mask_A.astype(np.uint8)\n",
        "ranked_oofs = []\n",
        "for o in oofs:\n",
        "    Nn, Cc = o.shape\n",
        "    R = np.full_like(o, np.nan, dtype=np.float32)\n",
        "    for c in range(Cc):\n",
        "        m = MA_stn[:, c].astype(bool)\n",
        "        if not m.any(): continue\n",
        "        col = o[m, c]; r = rankdata(col, method='average')\n",
        "        R[m, c] = ((r - 1) / (len(r) - 1) if len(r) > 1 else r*0.0).astype(np.float32)\n",
        "    ranked_oofs.append(R)\n",
        "OA_stn = np.nanmean(ranked_oofs, axis=0).astype(np.float32)\n",
        "TA_stn = np.mean([rank_cols(tp) for tp in tests], axis=0).astype(np.float32)\n",
        "macro_Astn = masked_auc_macro(Y_full, OA_stn, MA_stn)\n",
        "print(f'[View A_stn] OOF masked macro={macro_Astn:.4f}', flush=True)\n",
        "\n",
        "# 3) Re-run power+weights tuning with A_stn replacing A\n",
        "assert 'MB' in globals(), 'Need View B mask (from cell 30)'\n",
        "MC = np.isfinite(OC_r).astype(np.uint8)\n",
        "inter_mask_new = (MA_stn.astype(bool) & MB.astype(bool) & MC.astype(bool)).astype(np.uint8)\n",
        "gammas = [1.0, 1.1, 1.2, 1.3]\n",
        "w_grid = np.linspace(0.0, 1.0, 21)\n",
        "wp_grid = np.linspace(0.0, 0.3, 16)\n",
        "best_cfg = None; best_macro = -1.0\n",
        "for g in gammas:\n",
        "    A_o = pow_rank(OA_stn, g); B_o = pow_rank(OB_r, g); C_o = pow_rank(OC_r, g); PL_o = pow_rank(oof_ens_pl_v2, g)\n",
        "    for wB in w_grid:\n",
        "        for wC in w_grid:\n",
        "            wA = 1.0 - wB - wC\n",
        "            if wA < 0 or wA > 1: continue\n",
        "            oof_3 = wA * A_o + wB * B_o + wC * C_o\n",
        "            for wp in wp_grid:\n",
        "                blend_oof = (1.0 - wp) * oof_3 + wp * PL_o\n",
        "                macro = masked_auc_macro(Y_full, blend_oof, inter_mask_new)\n",
        "                if macro > best_macro:\n",
        "                    best_macro = macro; best_cfg = (float(g), float(wA), float(wB), float(wC), float(wp))\n",
        "g_best_astn, wA_best_astn, wB_best_astn, wC_best_astn, wp_best_astn = best_cfg\n",
        "print(f\"[Power+Weights (A_stn)] Best masked OOF={best_macro:.4f} | gamma={g_best_astn} | wA={wA_best_astn:.2f} wB={wB_best_astn:.2f} wC={wC_best_astn:.2f} | wp={wp_best_astn:.2f}\", flush=True)\n",
        "\n",
        "# 4) Build test with A_stn best config and write submission\n",
        "RA = pow_rank(TA_stn.astype(np.float32), g_best_astn)\n",
        "RB = pow_rank(TB.astype(np.float32), g_best_astn)\n",
        "RC = pow_rank(test_ens_st.astype(np.float32), g_best_astn)\n",
        "RPL = pow_rank(test_ens_pl_v2.astype(np.float32), g_best_astn)\n",
        "test_3 = wA_best_astn * RA + wB_best_astn * RB + wC_best_astn * RC\n",
        "test_final = (1.0 - wp_best_astn) * test_3 + wp_best_astn * RPL\n",
        "\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_final[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (Station one-hot View A_stn + power+weights)\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Station one-hot columns: 13 | GroupKFold folds: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Station-aug Variant cb_stn_d3_lr002_l2_50_rsm06_sub085_s42 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7189 | n_val=31 | time=25.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7417 | n_val=27 | time=26.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7262 | n_val=26 | time=18.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7574 | n_val=31 | time=33.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7250 | n_val=30 | time=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Station-aug Variant cb_stn_d4_lr0015_l2_60_rsm05_sub08_s123 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7178 | n_val=31 | time=35.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7562 | n_val=27 | time=54.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7142 | n_val=26 | time=35.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7459 | n_val=31 | time=58.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7338 | n_val=30 | time=22.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Station-aug Variant cb_stn_d3_lr002_l2_70_rsm06_sub085_s456 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7103 | n_val=31 | time=26.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7587 | n_val=27 | time=31.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7165 | n_val=26 | time=16.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7231 | n_val=31 | time=36.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.6913 | n_val=30 | time=14.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7349\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View A_stn] OOF masked macro=0.7514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Power+Weights (A_stn)] Best masked OOF=0.7787 | gamma=1.0 | wA=0.55 wB=0.00 wC=0.45 | wp=0.22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with 1216 rows (Station one-hot View A_stn + power+weights)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.177937\n1  101     0.414608\n2  102     0.374345\n3  103     0.694923\n4  104     0.370348",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.177937</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.414608</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.374345</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.694923</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.370348</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "400f650c-80a8-4f90-8732-b0f6b9d2b01f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Station + Time feature augmentation (cyclical time) -> compact CB bag (View A_stn_time) -> re-run power+weights blend\n",
        "import re, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "def extract_station(stem):\n",
        "    if not isinstance(stem, str): return None\n",
        "    m = re.search(r'(PC\\d+)', stem); return m.group(1) if m else None\n",
        "\n",
        "def parse_datetime_from_stem(stem):\n",
        "    # Examples: PC10_20090513_054500_0020 -> date 2009-05-13, time 05:45:00\n",
        "    # Returns: year, month, day, hour, minute (ints); if fails, None\n",
        "    if not isinstance(stem, str):\n",
        "        return None\n",
        "    m = re.search(r'_(\\d{8})_(\\d{6})_', stem + '_')  # ensure trailing underscore for regex\n",
        "    if not m:\n",
        "        return None\n",
        "    d8 = m.group(1); t6 = m.group(2)\n",
        "    try:\n",
        "        year = int(d8[0:4]); month = int(d8[4:6]); day = int(d8[6:8])\n",
        "        hour = int(t6[0:2]); minute = int(t6[2:4]);\n",
        "        return year, month, day, hour, minute\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def masked_auc_macro(y_true, y_pred, valid_mask):\n",
        "    C = y_true.shape[1]\n",
        "    aucs = []\n",
        "    for c in range(C):\n",
        "        m = valid_mask[:, c].astype(bool)\n",
        "        if m.sum() <= 1: continue\n",
        "        yt = y_true[m, c]; yp = y_pred[m, c]\n",
        "        if yt.max() == yt.min(): continue\n",
        "        try: aucs.append(roc_auc_score(yt, yp))\n",
        "        except Exception: pass\n",
        "    return float(np.mean(aucs)) if len(aucs) else float('nan')\n",
        "\n",
        "def rank_cols(mat):\n",
        "    N, C = mat.shape\n",
        "    out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = (r - 1) / (N - 1) if N > 1 else r*0.0\n",
        "    return out.astype(np.float32)\n",
        "\n",
        "def pow_rank(mat, gamma):\n",
        "    return np.clip(np.power(mat, gamma, dtype=np.float64), 0.0, 1.0).astype(np.float32)\n",
        "\n",
        "# Preconditions: existing views B/C/PL V2 for blending\n",
        "assert 'OB_r' in globals() and 'OC_r' in globals() and 'oof_ens_pl_v2' in globals() and 'inter_mask_plv2' in globals(), 'Need OB_r/OC_r/PL V2 (run cells 30/27/36)'\n",
        "assert 'TB' in globals() and 'test_ens_st' in globals() and 'test_ens_pl_v2' in globals(), 'Need TB/test_ens_st/test_ens_pl_v2 (run cells 30/27/36)'\n",
        "\n",
        "# 1) Build station OHE + time cyclical features\n",
        "train_ids_sorted = pd.Index(sorted(train_rec_ids))\n",
        "test_ids_sorted = pd.Index(sorted(test_rec_ids))\n",
        "tr_stems = train_df.set_index('rec_id').loc[train_ids_sorted, 'filename']\n",
        "te_stems = pd.Series([id2fn[int(r)] for r in test_ids_sorted], index=test_ids_sorted)\n",
        "tr_stations = tr_stems.map(extract_station).fillna('UNK')\n",
        "te_stations = te_stems.map(extract_station).fillna('UNK')\n",
        "d_tr = pd.get_dummies(tr_stations, prefix='stn').astype(np.float32)\n",
        "d_te = pd.get_dummies(te_stations, prefix='stn').astype(np.float32)\n",
        "d_all_cols = sorted(set(d_tr.columns).union(set(d_te.columns)))\n",
        "d_tr = d_tr.reindex(columns=d_all_cols, fill_value=0.0).astype(np.float32)\n",
        "d_te = d_te.reindex(columns=d_all_cols, fill_value=0.0).astype(np.float32)\n",
        "\n",
        "# Parse time\n",
        "tr_dt = tr_stems.map(parse_datetime_from_stem)\n",
        "te_dt = te_stems.map(parse_datetime_from_stem)\n",
        "def dt_to_feats(dt_series):\n",
        "    # Build month, day, hour, minute and cyclical encodings\n",
        "    df = pd.DataFrame(index=dt_series.index)\n",
        "    df['month'] = dt_series.map(lambda x: x[1] if x else np.nan).astype('float32')\n",
        "    df['day'] = dt_series.map(lambda x: x[2] if x else np.nan).astype('float32')\n",
        "    df['hour'] = dt_series.map(lambda x: x[3] if x else np.nan).astype('float32')\n",
        "    df['minute'] = dt_series.map(lambda x: x[4] if x else np.nan).astype('float32')\n",
        "    # fill missings with -1 then zero out cyclical\n",
        "    df = df.fillna(-1.0).astype('float32')\n",
        "    # cyclical encodings (month in [1..12] -> [0..1) ; hour in [0..23])\n",
        "    m_norm = (df['month'] - 1.0) / 12.0\n",
        "    h_norm = (df['hour']) / 24.0\n",
        "    df['month_sin'] = np.sin(2*np.pi*m_norm).astype('float32')\n",
        "    df['month_cos'] = np.cos(2*np.pi*m_norm).astype('float32')\n",
        "    df['hour_sin'] = np.sin(2*np.pi*h_norm).astype('float32')\n",
        "    df['hour_cos'] = np.cos(2*np.pi*h_norm).astype('float32')\n",
        "    # Zero out cyclical for missing month/hour (marked -1)\n",
        "    df.loc[df['month'] < 0, ['month_sin','month_cos']] = 0.0\n",
        "    df.loc[df['hour'] < 0, ['hour_sin','hour_cos']] = 0.0\n",
        "    return df[['month','day','hour','minute','month_sin','month_cos','hour_sin','hour_cos']].astype('float32')\n",
        "\n",
        "tr_time = dt_to_feats(tr_dt)\n",
        "te_time = dt_to_feats(te_dt)\n",
        "\n",
        "# Assemble augmented matrices\n",
        "X_tr_base = X_train_df.set_index(pd.Index(sorted(train_rec_ids))).loc[train_ids_sorted].values.astype(np.float32)\n",
        "X_te_base = X_test_df.set_index(pd.Index(sorted(test_rec_ids))).loc[test_ids_sorted].values.astype(np.float32)\n",
        "X_tr_aug = np.hstack([X_tr_base, d_tr.values.astype(np.float32), tr_time.values.astype(np.float32)])\n",
        "X_te_aug = np.hstack([X_te_base, d_te.values.astype(np.float32), te_time.values.astype(np.float32)])\n",
        "Y_full = pd.DataFrame(Y, index=train_rec_ids).loc[train_ids_sorted].values.astype(np.float32)\n",
        "N, C = X_tr_aug.shape[0], Y_full.shape[1]\n",
        "\n",
        "# Groups for GroupKFold by station\n",
        "groups = tr_stations.values\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "folds = list(gkf.split(X_tr_aug, Y_full, groups))\n",
        "print('Station OHE:', len(d_all_cols), '| Time cols:', tr_time.shape[1], '| Aug dims:', X_tr_aug.shape[1], '| folds:', len(folds), flush=True)\n",
        "\n",
        "# 2) Compact CB bag (3 variants) on augmented matrices -> OA_stn_time, MA_stn_time, TA_stn_time\n",
        "variants = [\n",
        "    dict(name='cb_stn_time_d3_lr002_l2_50_rsm06_sub085_s42', depth=3, learning_rate=0.02,  l2_leaf_reg=50.0, rsm=0.6, subsample=0.85, random_strength=0.8, seed=42),\n",
        "    dict(name='cb_stn_time_d4_lr0015_l2_60_rsm05_sub08_s123', depth=4, learning_rate=0.015, l2_leaf_reg=60.0, rsm=0.5, subsample=0.80, random_strength=1.0, seed=123),\n",
        "    dict(name='cb_stn_time_d3_lr002_l2_70_rsm06_sub085_s456', depth=3, learning_rate=0.02,  l2_leaf_reg=70.0, rsm=0.6, subsample=0.85, random_strength=0.8, seed=456),\n",
        "]\n",
        "oofs, masks, tests = [], [], []\n",
        "for v in variants:\n",
        "    print(f\"=== Station+Time Variant {v['name']} ===\", flush=True)\n",
        "    oof = np.zeros((N, C), dtype=np.float32)\n",
        "    vmask = np.zeros((N, C), dtype=np.uint8)\n",
        "    test_folds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds):\n",
        "        t0 = time.time()\n",
        "        Xtr, Xva = X_tr_aug[tr_idx], X_tr_aug[va_idx]\n",
        "        ytr, yva = Y_full[tr_idx], Y_full[va_idx]\n",
        "        te_pred = np.zeros((X_te_aug.shape[0], C), dtype=np.float32)\n",
        "        for c in range(C):\n",
        "            ytr_c = ytr[:, c]; yva_c = yva[:, c]\n",
        "            if ytr_c.max() == ytr_c.min() or yva_c.max() == yva_c.min():\n",
        "                continue\n",
        "            tr_pool = Pool(Xtr, label=ytr_c)\n",
        "            va_pool = Pool(Xva, label=yva_c)\n",
        "            cb = CatBoostClassifier(\n",
        "                loss_function='Logloss',\n",
        "                depth=v['depth'], learning_rate=v['learning_rate'],\n",
        "                iterations=5000, l2_leaf_reg=v['l2_leaf_reg'],\n",
        "                rsm=v['rsm'], subsample=v['subsample'], random_strength=v['random_strength'],\n",
        "                auto_class_weights='Balanced', random_seed=v['seed'],\n",
        "                early_stopping_rounds=200, verbose=False, task_type='CPU', thread_count=-1\n",
        "            )\n",
        "            cb.fit(tr_pool, eval_set=va_pool)\n",
        "            oof[va_idx, c] = cb.predict_proba(Xva)[:, 1]\n",
        "            te_pred[:, c] = cb.predict_proba(X_te_aug)[:, 1]\n",
        "            vmask[va_idx, c] = 1\n",
        "        fold_macro = masked_auc_macro(yva, oof[va_idx], vmask[va_idx])\n",
        "        print(f\"  Fold {fi}: masked macro={fold_macro:.4f} | n_val={len(va_idx)} | time={time.time()-t0:.1f}s\", flush=True)\n",
        "        test_folds.append(te_pred)\n",
        "    oofs.append(oof); masks.append(vmask); tests.append(np.mean(test_folds, axis=0))\n",
        "    macro_v = masked_auc_macro(Y_full, oof, vmask)\n",
        "    print(f\"Variant OOF masked macro: {macro_v:.4f}\", flush=True)\n",
        "\n",
        "# Rank-avg within view\n",
        "inter_mask_time = masks[0].astype(bool)\n",
        "for m in masks[1:]: inter_mask_time &= m.astype(bool)\n",
        "MA_stn_time = inter_mask_time.astype(np.uint8)\n",
        "ranked_oofs = []\n",
        "for o in oofs:\n",
        "    Nn, Cc = o.shape\n",
        "    R = np.full_like(o, np.nan, dtype=np.float32)\n",
        "    for c in range(Cc):\n",
        "        m = MA_stn_time[:, c].astype(bool)\n",
        "        if not m.any(): continue\n",
        "        col = o[m, c]; r = rankdata(col, method='average')\n",
        "        R[m, c] = ((r - 1) / (len(r) - 1) if len(r) > 1 else r*0.0).astype(np.float32)\n",
        "    ranked_oofs.append(R)\n",
        "OA_stn_time = np.nanmean(ranked_oofs, axis=0).astype(np.float32)\n",
        "TA_stn_time = np.mean([rank_cols(tp) for tp in tests], axis=0).astype(np.float32)\n",
        "macro_Astn_time = masked_auc_macro(Y_full, OA_stn_time, MA_stn_time)\n",
        "print(f'[View A_stn_time] OOF masked macro={macro_Astn_time:.4f}')\n",
        "\n",
        "# 3) Power+weights tuning with A_stn_time replacing A\n",
        "MC = np.isfinite(OC_r).astype(np.uint8)\n",
        "inter_mask_new = (MA_stn_time.astype(bool) & MB.astype(bool) & MC.astype(bool)).astype(np.uint8)\n",
        "gammas = [1.0, 1.05, 1.1]\n",
        "w_grid = np.linspace(0.0, 1.0, 21)\n",
        "wp_grid = np.linspace(0.0, 0.3, 16)\n",
        "best_cfg = None; best_macro = -1.0\n",
        "for g in gammas:\n",
        "    A_o = pow_rank(OA_stn_time, g); B_o = pow_rank(OB_r, g); C_o = pow_rank(OC_r, g); PL_o = pow_rank(oof_ens_pl_v2, g)\n",
        "    for wB in w_grid:\n",
        "        for wC in w_grid:\n",
        "            wA = 1.0 - wB - wC\n",
        "            if wA < 0 or wA > 1: continue\n",
        "            oof_3 = wA * A_o + wB * B_o + wC * C_o\n",
        "            for wp in wp_grid:\n",
        "                blend_oof = (1.0 - wp) * oof_3 + wp * PL_o\n",
        "                macro = masked_auc_macro(Y_full, blend_oof, inter_mask_new)\n",
        "                if macro > best_macro:\n",
        "                    best_macro = macro; best_cfg = (float(g), float(wA), float(wB), float(wC), float(wp))\n",
        "g_best_ast, wA_best_ast, wB_best_ast, wC_best_ast, wp_best_ast = best_cfg\n",
        "print(f\"[Power+Weights (A_stn_time)] Best masked OOF={best_macro:.4f} | gamma={g_best_ast} | wA={wA_best_ast:.2f} wB={wB_best_ast:.2f} wC={wC_best_ast:.2f} | wp={wp_best_ast:.2f}\", flush=True)\n",
        "\n",
        "# 4) Build test and write submission\n",
        "RA = pow_rank(TA_stn_time.astype(np.float32), g_best_ast)\n",
        "RB = pow_rank(TB.astype(np.float32), g_best_ast)\n",
        "RC = pow_rank(test_ens_st.astype(np.float32), g_best_ast)\n",
        "RPL = pow_rank(test_ens_pl_v2.astype(np.float32), g_best_ast)\n",
        "test_3 = wA_best_ast * RA + wB_best_ast * RB + wC_best_ast * RC\n",
        "test_final = (1.0 - wp_best_ast) * test_3 + wp_best_ast * RPL\n",
        "\n",
        "test_id_order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, row in test_id_order.iterrows():\n",
        "    rid = int(row['rec_id']); cid = int(row['class_id'])\n",
        "    ti = rec_to_idx[rid]\n",
        "    p = float(test_final[ti, cid])\n",
        "    probs.append(min(max(p, 0.0), 1.0))\n",
        "sub_out = pd.DataFrame({'Id': test_id_order['Id'].astype(int), 'Probability': np.round(probs, 6)})\n",
        "sub_out.to_csv('submission.csv', index=False)\n",
        "print(f\"Wrote submission.csv with {len(sub_out)} rows (Station+Time View A_stn_time + power+weights)\")\n",
        "display(sub_out.head())"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Station OHE: 13 | Time cols: 8 | Aug dims: 718 | folds: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Station+Time Variant cb_stn_time_d3_lr002_l2_50_rsm06_sub085_s42 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7321 | n_val=31 | time=24.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7736 | n_val=27 | time=36.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7136 | n_val=26 | time=13.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7336 | n_val=31 | time=33.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.6664 | n_val=30 | time=13.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Station+Time Variant cb_stn_time_d4_lr0015_l2_60_rsm05_sub08_s123 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7097 | n_val=31 | time=50.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7833 | n_val=27 | time=80.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.7273 | n_val=26 | time=19.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7344 | n_val=31 | time=55.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.6805 | n_val=30 | time=21.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Station+Time Variant cb_stn_time_d3_lr002_l2_70_rsm06_sub085_s456 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: masked macro=0.7247 | n_val=31 | time=21.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: masked macro=0.7838 | n_val=27 | time=46.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: masked macro=0.6649 | n_val=26 | time=13.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: masked macro=0.7487 | n_val=31 | time=25.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: masked macro=0.7035 | n_val=30 | time=15.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF masked macro: 0.7567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[View A_stn_time] OOF masked macro=0.7606\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Power+Weights (A_stn_time)] Best masked OOF=0.7748 | gamma=1.0 | wA=0.65 wB=0.00 wC=0.35 | wp=0.24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with 1216 rows (Station+Time View A_stn_time + power+weights)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "    Id  Probability\n0  100     0.189836\n1  101     0.305042\n2  102     0.313410\n3  103     0.685730\n4  104     0.420749",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.189836</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>0.305042</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>102</td>\n      <td>0.313410</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>103</td>\n      <td>0.685730</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104</td>\n      <td>0.420749</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "5684218d-da75-40e7-9514-54ac27ff26a2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# A_stn_cat: station as single categorical feature (CatBoost) + compact blend\n",
        "import re, time, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "def extract_station(s):\n",
        "    m = re.search(r'(PC\\d+)', s) if isinstance(s, str) else None\n",
        "    return m.group(1) if m else 'UNK'\n",
        "\n",
        "def masked_auc_macro(y_true, y_pred, valid_mask):\n",
        "    C = y_true.shape[1]; aucs = []\n",
        "    for c in range(C):\n",
        "        m = valid_mask[:, c].astype(bool)\n",
        "        if m.sum() <= 1: continue\n",
        "        yt, yp = y_true[m, c], y_pred[m, c]\n",
        "        if yt.max() == yt.min(): continue\n",
        "        try: aucs.append(roc_auc_score(yt, yp))\n",
        "        except: pass\n",
        "    return float(np.mean(aucs)) if aucs else float('nan')\n",
        "\n",
        "def rank_cols(mat):\n",
        "    N, C = mat.shape; out = np.zeros_like(mat, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        r = rankdata(mat[:, c], method='average')\n",
        "        out[:, c] = ((r - 1) / (N - 1) if N > 1 else 0).astype(np.float32)\n",
        "    return out\n",
        "\n",
        "def pow_rank(mat, g):\n",
        "    return np.clip(np.power(mat, g, dtype=np.float64), 0.0, 1.0).astype(np.float32)\n",
        "\n",
        "# Preconditions\n",
        "assert 'OB_r' in globals() and 'OC_r' in globals() and 'oof_ens_pl_v2' in globals() and 'inter_mask_plv2' in globals(), 'Need OB_r/OC_r/PLV2'\n",
        "assert 'TB' in globals() and 'test_ens_st' in globals() and 'test_ens_pl_v2' in globals(), 'Need TB/test_ens_st/test_ens_pl_v2'\n",
        "assert 'MB' in globals(), 'Need MB mask from view B'\n",
        "\n",
        "# Build station categorical DF\n",
        "train_ids_sorted = pd.Index(sorted(train_rec_ids)); test_ids_sorted = pd.Index(sorted(test_rec_ids))\n",
        "tr_stems = train_df.set_index('rec_id').loc[train_ids_sorted, 'filename']\n",
        "te_stems = pd.Series([id2fn[int(r)] for r in test_ids_sorted], index=test_ids_sorted)\n",
        "tr_station = tr_stems.map(extract_station); te_station = te_stems.map(extract_station)\n",
        "X_tr = X_train_df.set_index(pd.Index(sorted(train_rec_ids))).loc[train_ids_sorted].reset_index(drop=True).astype(np.float32)\n",
        "X_te = X_test_df.set_index(pd.Index(sorted(test_rec_ids))).loc[test_ids_sorted].reset_index(drop=True).astype(np.float32)\n",
        "X_tr['station_id'] = tr_station.values.astype(str); X_te['station_id'] = te_station.values.astype(str)\n",
        "cat_idx = [X_tr.columns.get_loc('station_id')]\n",
        "Y_full = pd.DataFrame(Y, index=train_rec_ids).loc[train_ids_sorted].values.astype(np.float32)\n",
        "N, C = X_tr.shape[0], Y_full.shape[1]\n",
        "groups = tr_station.values\n",
        "folds = list(GroupKFold(n_splits=5).split(X_tr, Y_full, groups))\n",
        "print('Station categorical idx:', cat_idx[0], '| dims:', X_tr.shape, flush=True)\n",
        "\n",
        "# Compact 3-variant CatBoost bag\n",
        "variants = [\n",
        "    dict(name='stncat_d3_s42', depth=3, learning_rate=0.02,  l2_leaf_reg=50.0, rsm=0.6, subsample=0.85, random_strength=0.8, seed=42),\n",
        "    dict(name='stncat_d4_s123',depth=4, learning_rate=0.015, l2_leaf_reg=60.0, rsm=0.5, subsample=0.80, random_strength=1.0, seed=123),\n",
        "    dict(name='stncat_d3_s456',depth=3, learning_rate=0.02,  l2_leaf_reg=70.0, rsm=0.6, subsample=0.85, random_strength=0.8, seed=456),\n",
        "]\n",
        "oofs, masks, tests = [], [], []\n",
        "for v in variants:\n",
        "    print('===', v['name'], '===', flush=True)\n",
        "    oof = np.zeros((N, C), dtype=np.float32); vmask = np.zeros((N, C), dtype=np.uint8); test_folds = []\n",
        "    for fi, (tr_idx, va_idx) in enumerate(folds):\n",
        "        t0 = time.time(); Xtr_df, Xva_df = X_tr.iloc[tr_idx], X_tr.iloc[va_idx]; ytr, yva = Y_full[tr_idx], Y_full[va_idx]\n",
        "        te_pred = np.zeros((X_te.shape[0], C), dtype=np.float32)\n",
        "        for c in range(C):\n",
        "            ytr_c, yva_c = ytr[:, c], yva[:, c]\n",
        "            if ytr_c.max() == ytr_c.min() or yva_c.max() == yva_c.min(): continue\n",
        "            tr_pool = Pool(Xtr_df, label=ytr_c, cat_features=cat_idx); va_pool = Pool(Xva_df, label=yva_c, cat_features=cat_idx)\n",
        "            cb = CatBoostClassifier(loss_function='Logloss', depth=v['depth'], learning_rate=v['learning_rate'], iterations=5000,\n",
        "                                 l2_leaf_reg=v['l2_leaf_reg'], rsm=v['rsm'], subsample=v['subsample'], random_strength=v['random_strength'],\n",
        "                                 auto_class_weights='Balanced', random_seed=v['seed'], early_stopping_rounds=200, verbose=False, task_type='CPU', thread_count=-1)\n",
        "            cb.fit(tr_pool, eval_set=va_pool)\n",
        "            oof[va_idx, c] = cb.predict_proba(Xva_df)[:, 1]; te_pred[:, c] = cb.predict_proba(X_te)[:, 1]; vmask[va_idx, c] = 1\n",
        "        print(f'  Fold {fi}: macro={masked_auc_macro(yva, oof[va_idx], vmask[va_idx]):.4f} | n_val={len(va_idx)} | t={time.time()-t0:.1f}s', flush=True)\n",
        "        test_folds.append(te_pred)\n",
        "    oofs.append(oof); masks.append(vmask); tests.append(np.mean(test_folds, axis=0))\n",
        "    print('Variant OOF:', f'{masked_auc_macro(Y_full, oof, vmask):.4f}', flush=True)\n",
        "\n",
        "# Rank-avg within A_stn_cat\n",
        "inter_mask = masks[0].astype(bool)\n",
        "for m in masks[1:]: inter_mask &= m.astype(bool)\n",
        "MA_stn_cat = inter_mask.astype(np.uint8)\n",
        "ranked_oofs = []\n",
        "for o in oofs:\n",
        "    R = np.full_like(o, np.nan, dtype=np.float32)\n",
        "    for c in range(C):\n",
        "        m = MA_stn_cat[:, c].astype(bool)\n",
        "        if not m.any(): continue\n",
        "        col = o[m, c]; r = rankdata(col, method='average')\n",
        "        R[m, c] = ((r - 1) / (len(r) - 1) if len(r) > 1 else 0).astype(np.float32)\n",
        "    ranked_oofs.append(R)\n",
        "OA_stn_cat = np.nanmean(ranked_oofs, axis=0).astype(np.float32)\n",
        "TA_stn_cat = np.mean([rank_cols(tp) for tp in tests], axis=0).astype(np.float32)\n",
        "print('[A_stn_cat] OOF macro=', f'{masked_auc_macro(Y_full, OA_stn_cat, MA_stn_cat):.4f}', flush=True)\n",
        "\n",
        "# Power+weights with A_stn_cat\n",
        "MC = np.isfinite(OC_r).astype(np.uint8)\n",
        "inter_mask_new = (MA_stn_cat.astype(bool) & MB.astype(bool) & MC.astype(bool) & inter_mask_plv2.astype(bool)).astype(np.uint8)\n",
        "gammas = [1.0, 1.1]; w_grid = np.linspace(0.0, 1.0, 21); wp_grid = np.linspace(0.0, 0.3, 16)\n",
        "best = None; best_macro = -1.0\n",
        "for g in gammas:\n",
        "    A_o = pow_rank(OA_stn_cat, g); B_o = pow_rank(OB_r, g); C_o = pow_rank(OC_r, g); P_o = pow_rank(oof_ens_pl_v2, g)\n",
        "    for wB in w_grid:\n",
        "        for wC in w_grid:\n",
        "            wA = 1.0 - wB - wC\n",
        "            if wA < 0 or wA > 1: continue\n",
        "            o3 = wA*A_o + wB*B_o + wC*C_o\n",
        "            for wp in wp_grid:\n",
        "                blend = (1.0 - wp)*o3 + wp*P_o\n",
        "                m = masked_auc_macro(Y_full, blend, inter_mask_new)\n",
        "                if m > best_macro: best_macro = m; best = (float(g), float(wA), float(wB), float(wC), float(wp))\n",
        "gA, wA, wB, wC, wp = best\n",
        "print(f\"[Blend A_stn_cat] Best OOF={best_macro:.4f} | g={gA} | wA={wA:.2f} wB={wB:.2f} wC={wC:.2f} | wp={wp:.2f}\", flush=True)\n",
        "\n",
        "# Build test and write submission\n",
        "RA = pow_rank(TA_stn_cat.astype(np.float32), gA); RB = pow_rank(TB.astype(np.float32), gA)\n",
        "RC = pow_rank(test_ens_st.astype(np.float32), gA); RPL = pow_rank(test_ens_pl_v2.astype(np.float32), gA)\n",
        "test_3 = wA*RA + wB*RB + wC*RC; test_final = (1.0 - wp)*test_3 + wp*RPL\n",
        "order = sub_df[['Id','rec_id','class_id']].copy(); rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, r in order.iterrows():\n",
        "    ti = rec_to_idx[int(r['rec_id'])]; cid = int(r['class_id'])\n",
        "    p = float(test_final[ti, cid]); probs.append(min(max(p, 0.0), 1.0))\n",
        "pd.DataFrame({'Id': order['Id'].astype(int), 'Probability': np.round(probs, 6)}).to_csv('submission.csv', index=False)\n",
        "print('Wrote submission.csv (A_stn_cat + power+weights)')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Station categorical idx: 697 | dims: (145, 698)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== stncat_d3_s42 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: macro=0.7520 | n_val=31 | t=35.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: macro=0.7644 | n_val=27 | t=49.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: macro=0.6944 | n_val=26 | t=14.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: macro=0.7403 | n_val=31 | t=26.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: macro=0.7066 | n_val=30 | t=20.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF: 0.7427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== stncat_d4_s123 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: macro=0.7289 | n_val=31 | t=55.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: macro=0.7952 | n_val=27 | t=104.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: macro=0.6866 | n_val=26 | t=49.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: macro=0.7218 | n_val=31 | t=66.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: macro=0.7068 | n_val=30 | t=44.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF: 0.7310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== stncat_d3_s456 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0: macro=0.6949 | n_val=31 | t=34.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: macro=0.7650 | n_val=27 | t=50.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2: macro=0.6663 | n_val=26 | t=20.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3: macro=0.7254 | n_val=31 | t=38.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4: macro=0.7078 | n_val=30 | t=19.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variant OOF: 0.7261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[A_stn_cat] OOF macro= 0.7306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Blend A_stn_cat] Best OOF=0.7679 | g=1.0 | wA=0.25 wB=0.15 wC=0.60 | wp=0.30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv (A_stn_cat + power+weights)\n"
          ]
        }
      ]
    },
    {
      "id": "195bd14c-7915-48c9-8fd1-d68089e217ea",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Hedge submission: reuse A_stn view but with gamma=1.1 and same weights as best A_stn config\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "def pow_rank(mat, gamma):\n",
        "    return np.clip(np.power(mat, gamma, dtype=np.float64), 0.0, 1.0).astype(np.float32)\n",
        "\n",
        "assert 'TA_stn' in globals() and 'TB' in globals() and 'test_ens_st' in globals() and 'test_ens_pl_v2' in globals(), 'Run cell 42 first (A_stn) and have B/C/PL test ranks ready'\n",
        "assert 'wA_best_astn' in globals() and 'wB_best_astn' in globals() and 'wC_best_astn' in globals() and 'wp_best_astn' in globals(), 'Run cell 42 to set best weights for A_stn'\n",
        "\n",
        "g_hedge = 1.1\n",
        "RA = pow_rank(TA_stn.astype(np.float32), g_hedge)\n",
        "RB = pow_rank(TB.astype(np.float32), g_hedge)\n",
        "RC = pow_rank(test_ens_st.astype(np.float32), g_hedge)\n",
        "RPL = pow_rank(test_ens_pl_v2.astype(np.float32), g_hedge)\n",
        "test_3 = wA_best_astn * RA + wB_best_astn * RB + wC_best_astn * RC\n",
        "test_final = (1.0 - wp_best_astn) * test_3 + wp_best_astn * RPL\n",
        "\n",
        "order = sub_df[['Id','rec_id','class_id']].copy()\n",
        "rec_to_idx = {rid:i for i, rid in enumerate(sorted(test_rec_ids))}\n",
        "probs = []\n",
        "for _, r in order.iterrows():\n",
        "    ti = rec_to_idx[int(r['rec_id'])]; cid = int(r['class_id'])\n",
        "    p = float(test_final[ti, cid]); probs.append(min(max(p, 0.0), 1.0))\n",
        "pd.DataFrame({'Id': order['Id'].astype(int), 'Probability': np.round(probs, 6)}).to_csv('submission.csv', index=False)\n",
        "print('Wrote submission.csv (A_stn weights hedge, gamma=1.1)')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv (A_stn weights hedge, gamma=1.1)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}