{
  "cells": [
    {
      "id": "273f84d9-c651-4c75-960f-c598ad7ea13e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Tiny PANNs kNN sweep with LOSO and station-aware prior fusion\n",
        "import os, sys, gc, time, re, json, math, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "DATA_DIR = Path('essential_data')\n",
        "\n",
        "def load_species_list(path: Path):\n",
        "    df = pd.read_csv(path)\n",
        "    if {'class_id','code'}.issubset(df.columns):\n",
        "        return df.sort_values('class_id')['code'].tolist()\n",
        "    with open(path, 'r') as f:\n",
        "        f.readline()\n",
        "        out = []\n",
        "        for line in f:\n",
        "            s = line.strip()\n",
        "            if not s: continue\n",
        "            parts = s.split(',')\n",
        "            out.append(parts[1] if len(parts)>1 else s)\n",
        "    return out\n",
        "\n",
        "def parse_rec_id2filename(path: Path):\n",
        "    df = pd.read_csv(path)\n",
        "    df = df.rename(columns={df.columns[0]:'rec_id', df.columns[1]:'filename'})\n",
        "    df['rec_id'] = df['rec_id'].astype(int)\n",
        "    df['station'] = df['filename'].str.extract(r'^(PC\\d+)')\n",
        "    return df[['rec_id','filename','station']]\n",
        "\n",
        "def parse_labels(path: Path, C: int):\n",
        "    rec_ids, flags, Y = [], [], []\n",
        "    with open(path, 'r') as f:\n",
        "        f.readline()\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line: continue\n",
        "            parts = [tok.strip() for tok in line.split(',')]\n",
        "            try: rid = int(parts[0])\n",
        "            except: continue\n",
        "            tokens = parts[1:] if len(parts)>1 else []\n",
        "            is_test = any(tok=='?' for tok in tokens)\n",
        "            y = np.zeros(C, dtype=int)\n",
        "            if not is_test and tokens:\n",
        "                for tok in tokens:\n",
        "                    if tok in ('','?'): continue\n",
        "                    try: idx = int(tok)\n",
        "                    except: continue\n",
        "                    if 0 <= idx < C: y[idx] = 1\n",
        "            rec_ids.append(rid); flags.append(is_test); Y.append(y)\n",
        "    lab_cols = [f'label_{i}' for i in range(C)]\n",
        "    ydf = pd.DataFrame(np.vstack(Y), columns=lab_cols)\n",
        "    df = pd.DataFrame({'rec_id': rec_ids, 'is_test': flags})\n",
        "    return df.join(ydf), lab_cols\n",
        "\n",
        "def macro_auc_np(P, Y):\n",
        "    C = Y.shape[1]\n",
        "    aucs = []\n",
        "    for c in range(C):\n",
        "        yt = Y[:,c]; yp = P[:,c]\n",
        "        if yt.sum()==0 or yt.sum()==len(yt): continue\n",
        "        try: aucs.append(roc_auc_score(yt, yp))\n",
        "        except: pass\n",
        "    return float(np.mean(aucs)) if aucs else np.nan\n",
        "\n",
        "def l2_normalize_rows(A):\n",
        "    A = A.astype(np.float32, copy=False)\n",
        "    n = np.linalg.norm(A, axis=1, keepdims=True) + 1e-12\n",
        "    return A / n\n",
        "\n",
        "def load_panns_emb(path: Path, mask_train=None, mask_test=None):\n",
        "    arr = np.load(path, allow_pickle=True)\n",
        "    # Case 1: npz-like\n",
        "    if hasattr(arr, 'files'):\n",
        "        Xtr = arr['X_train'] if 'X_train' in arr.files else arr['train']\n",
        "        Xte = arr['X_test'] if 'X_test' in arr.files else arr['test']\n",
        "        id_tr = arr['train_ids'] if 'train_ids' in arr.files else None\n",
        "        id_te = arr['test_ids'] if 'test_ids' in arr.files else None\n",
        "        return Xtr, Xte, id_tr, id_te\n",
        "    # Case 2: object ndarray storing dict\n",
        "    if isinstance(arr, np.ndarray) and arr.dtype == object:\n",
        "        try:\n",
        "            obj = arr.item()\n",
        "        except Exception:\n",
        "            obj = arr.tolist() if hasattr(arr, 'tolist') else None\n",
        "        if isinstance(obj, dict):\n",
        "            Xtr = obj.get('X_train') or obj.get('train') or obj.get('train_X')\n",
        "            Xte = obj.get('X_test') or obj.get('test') or obj.get('test_X')\n",
        "            id_tr = obj.get('train_ids') or obj.get('ids_train') or obj.get('rec_ids_train')\n",
        "            id_te = obj.get('test_ids') or obj.get('ids_test') or obj.get('rec_ids_test')\n",
        "            return Xtr, Xte, id_tr, id_te\n",
        "    # Case 3: single 2D array of shape (N_total, D) aligned to labels order\n",
        "    if isinstance(arr, np.ndarray) and arr.ndim == 2 and mask_train is not None and mask_test is not None:\n",
        "        assert arr.shape[0] == int(mask_train.sum() + mask_test.sum()), 'Embedding rows != N_total'\n",
        "        return arr[mask_train], arr[mask_test], None, None\n",
        "    raise RuntimeError('Unsupported panns_cnn14_emb.npy format')\n",
        "\n",
        "def align_by_rec_ids(Xtr, Xte, id_tr, id_te, meta_train, meta_test):\n",
        "    # Align embeddings rows to meta_train/meta_test rec_id order\n",
        "    tr_order = meta_train['rec_id'].values.tolist()\n",
        "    te_order = meta_test['rec_id'].values.tolist()\n",
        "    def reindex(X, ids, order):\n",
        "        if ids is None:\n",
        "            # assume already aligned\n",
        "            return X\n",
        "        mp = {int(r): i for i, r in enumerate(ids)}\n",
        "        idx = [mp[r] for r in order]\n",
        "        return X[idx]\n",
        "    return reindex(Xtr, id_tr, tr_order), reindex(Xte, id_te, te_order)\n",
        "\n",
        "def knn_fold_predict(X_tr_emb, y_tr, X_va_emb, k=11, metric='cosine', weights='distance'):\n",
        "    nn = NearestNeighbors(n_neighbors=min(k, len(X_tr_emb)), metric=metric, algorithm='auto')\n",
        "    nn.fit(X_tr_emb)\n",
        "    dists, idxs = nn.kneighbors(X_va_emb, return_distance=True)\n",
        "    if weights == 'distance':\n",
        "        w = 1.0 / (dists + 1e-6)\n",
        "    else:\n",
        "        w = np.ones_like(dists)\n",
        "    Ytr = y_tr.astype(np.float32)\n",
        "    C = Ytr.shape[1]\n",
        "    P = np.zeros((len(X_va_emb), C), dtype=np.float32)\n",
        "    for i in range(len(X_va_emb)):\n",
        "        nbr_idx = idxs[i]\n",
        "        wi = w[i][:, None]\n",
        "        votes = (Ytr[nbr_idx] * wi).sum(axis=0)\n",
        "        denom = wi.sum() + 1e-8\n",
        "        P[i] = votes / denom\n",
        "    return np.clip(P, 0.0, 1.0)\n",
        "\n",
        "def knn_loso_oof_and_test(X_emb_tr, X_emb_te, y_train_df, groups, k=11, metric='cosine', weights='distance'):\n",
        "    logo = LeaveOneGroupOut()\n",
        "    idx = np.arange(len(groups))\n",
        "    Y = y_train_df.values.astype(np.uint8)\n",
        "    C = Y.shape[1]\n",
        "    P_oof = np.zeros((len(Y), C), dtype=np.float32)\n",
        "    fold = 0\n",
        "    t0 = time.time()\n",
        "    for tr, va in logo.split(idx, groups=groups):\n",
        "        fold += 1\n",
        "        print(f'[LOSO] fold {fold:02d} | tr={len(tr)} va={len(va)} | k={k} metric={metric} weights={weights} | elapsed={time.time()-t0:.1f}s')\n",
        "        sys.stdout.flush()\n",
        "        P_oof[va] = knn_fold_predict(X_emb_tr[tr], Y[tr], X_emb_tr[va], k=k, metric=metric, weights=weights)\n",
        "    print('[Full-train NN] fitting for test...')\n",
        "    P_test = knn_fold_predict(X_emb_tr, Y, X_emb_te, k=k, metric=metric, weights=weights)\n",
        "    auc = macro_auc_np(P_oof, Y)\n",
        "    return P_oof, P_test, auc\n",
        "\n",
        "def compute_fulltrain_station_priors(meta_train, y_train_df, alpha=30.0):\n",
        "    Y = y_train_df.values.astype(float)\n",
        "    C = Y.shape[1]\n",
        "    p_global = Y.mean(axis=0)\n",
        "    df = pd.DataFrame(Y, columns=[f'c{i}' for i in range(C)])\n",
        "    df['station'] = meta_train['station'].values\n",
        "    grp = df.groupby('station')\n",
        "    n = grp.size()\n",
        "    pos = grp[[f'c{i}' for i in range(C)]].sum()\n",
        "    eb = {}\n",
        "    for st, cnt in n.items():\n",
        "        pos_st = pos.loc[st].values\n",
        "        eb[st] = (pos_st + alpha * p_global) / (cnt + alpha)\n",
        "    st_arr = meta_train['station'].values\n",
        "    prior_train = np.vstack([eb.get(s, p_global) for s in st_arr])\n",
        "    return eb, p_global, prior_train\n",
        "\n",
        "def logit_zscore_full(prior_train):\n",
        "    P = np.clip(prior_train, 1e-6, 1-1e-6)\n",
        "    L = np.log(P/(1-P))\n",
        "    L = np.clip(L, -6, 6)\n",
        "    mu = L.mean(axis=0)\n",
        "    sd = L.std(axis=0) + 1e-6\n",
        "    Z = (L - mu) / sd\n",
        "    return Z, mu, sd\n",
        "\n",
        "def build_test_Z_station(meta_test, eb_map, p_global, mu, sd):\n",
        "    T = len(meta_test)\n",
        "    C = len(mu)\n",
        "    P = np.tile(p_global, (T, 1))\n",
        "    st_vals = meta_test['station'].values\n",
        "    for i, s in enumerate(st_vals):\n",
        "        if s in eb_map:\n",
        "            P[i] = eb_map[s]\n",
        "    L = np.log(np.clip(P,1e-6,1-1e-6)/np.clip(1-P,1e-6,1))\n",
        "    L = np.clip(L, -6, 6)\n",
        "    return (L - mu)/sd\n",
        "\n",
        "def logit(p):\n",
        "    p = np.clip(p, 1e-6, 1-1e-6)\n",
        "    return np.log(p/(1-p))\n",
        "def sigmoid(x):\n",
        "    return 1.0/(1.0+np.exp(-x))\n",
        "\n",
        "# 1) Load core metadata and labels\n",
        "species = load_species_list(DATA_DIR/'species_list.txt')\n",
        "rec_map = parse_rec_id2filename(DATA_DIR/'rec_id2filename.txt')\n",
        "labels_df, lab_cols_tmp = parse_labels(DATA_DIR/'rec_labels_test_hidden.txt', len(species))\n",
        "df = rec_map.merge(labels_df, on='rec_id', how='right')\n",
        "train_df = df[~df['is_test']].copy()\n",
        "test_df = df[df['is_test']].copy()\n",
        "y_train = train_df[[c for c in train_df.columns if c.startswith('label_')]].copy()\n",
        "y_train.columns = [f'label_{s}' for s in species]\n",
        "groups = train_df['station'].values\n",
        "meta_train = train_df[['rec_id','filename','station']].copy()\n",
        "meta_test = test_df[['rec_id','filename','station']].copy()\n",
        "print(f'Train N={len(train_df)} Test N={len(test_df)} Classes={y_train.shape[1]} Stations={len(pd.unique(groups))}')\n",
        "sys.stdout.flush()\n",
        "\n",
        "# 2) Load embeddings and align (supports single 2D array of all rows)\n",
        "mask_train = (~df['is_test']).values\n",
        "mask_test = (df['is_test']).values\n",
        "Xtr, Xte, id_tr, id_te = load_panns_emb(Path('panns_cnn14_emb.npy'), mask_train=mask_train, mask_test=mask_test)\n",
        "Xtr, Xte = align_by_rec_ids(Xtr, Xte, id_tr, id_te, meta_train, meta_test)\n",
        "Xtr = l2_normalize_rows(Xtr); Xte = l2_normalize_rows(Xte)\n",
        "print('Embeddings:', Xtr.shape, Xte.shape)\n",
        "sys.stdout.flush()\n",
        "\n",
        "# 3) Sweep k and metrics\n",
        "sweep = [\n",
        "    {'k':7, 'metric':'cosine', 'weights':'distance'},\n",
        "    {'k':9, 'metric':'cosine', 'weights':'distance'},\n",
        "    {'k':13, 'metric':'cosine', 'weights':'distance'},\n",
        "    {'k':11, 'metric':'euclidean', 'weights':'distance'},\n",
        "]\n",
        "best = {'auc': -1.0, 'cfg': None, 'P_oof': None, 'P_test': None}\n",
        "for i, cfg in enumerate(sweep):\n",
        "    t0 = time.time()\n",
        "    P_oof, P_test, auc = knn_loso_oof_and_test(Xtr, Xte, y_train, groups, **cfg)\n",
        "    print(f\"[Sweep {i+1}/{len(sweep)}] k={cfg['k']} metric={cfg['metric']} -> pooled macro AUC={auc:.4f} | dt={time.time()-t0:.1f}s\")\n",
        "    sys.stdout.flush()\n",
        "    if auc > best['auc']:\n",
        "        best.update({'auc': auc, 'cfg': cfg.copy(), 'P_oof': P_oof, 'P_test': P_test})\n",
        "\n",
        "print('Best sweep result:', best['cfg'], 'AUC=', f\"{best['auc']:.4f}\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "# 4) Save plain kNN submission for best config\n",
        "P_test_best = best['P_test']\n",
        "rows = []\n",
        "for i, rec_id in enumerate(meta_test['rec_id'].values.tolist()):\n",
        "    for c in range(y_train.shape[1]):\n",
        "        rows.append((rec_id*100 + c, float(P_test_best[i, c])))\n",
        "sub_knn = pd.DataFrame(rows, columns=['Id','Probability']).sort_values('Id').reset_index(drop=True)\n",
        "sub_knn.to_csv('submission_knn_sweep.csv', index=False)\n",
        "print('Saved submission_knn_sweep.csv | rows=', len(sub_knn))\n",
        "\n",
        "# 5) Station-aware prior fusion with lambda=0.25 (from weighted blend sweep)\n",
        "eb_map, p_global, prior_train = compute_fulltrain_station_priors(meta_train, y_train, alpha=30.0)\n",
        "prior_train_z, mu, sd = logit_zscore_full(prior_train)\n",
        "Z_test = build_test_Z_station(meta_test, eb_map, p_global, mu, sd)\n",
        "lam = 0.25\n",
        "P_test_fused = sigmoid(np.clip(logit(P_test_best) + lam*Z_test, -12, 12))\n",
        "rows2 = []\n",
        "for i, rec_id in enumerate(meta_test['rec_id'].values.tolist()):\n",
        "    for c in range(y_train.shape[1]):\n",
        "        rows2.append((rec_id*100 + c, float(P_test_fused[i, c])))\n",
        "sub_knn_pf = pd.DataFrame(rows2, columns=['Id','Probability']).sort_values('Id').reset_index(drop=True)\n",
        "sub_knn_pf.to_csv('submission_knn_sweep_priorfusion_station.csv', index=False)\n",
        "print('Saved submission_knn_sweep_priorfusion_station.csv | rows=', len(sub_knn_pf))\n",
        "\n",
        "gc.collect();"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train N=258 Test N=64 Classes=19 Stations=13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings: (258, 2048) (64, 2048)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 01 | tr=231 va=27 | k=7 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 02 | tr=234 va=24 | k=7 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 03 | tr=232 va=26 | k=7 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 04 | tr=244 va=14 | k=7 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 05 | tr=233 va=25 | k=7 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 06 | tr=233 va=25 | k=7 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 07 | tr=236 va=22 | k=7 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 08 | tr=247 va=11 | k=7 metric=cosine weights=distance | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 09 | tr=243 va=15 | k=7 metric=cosine weights=distance | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 10 | tr=243 va=15 | k=7 metric=cosine weights=distance | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 11 | tr=238 va=20 | k=7 metric=cosine weights=distance | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 12 | tr=234 va=24 | k=7 metric=cosine weights=distance | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 13 | tr=248 va=10 | k=7 metric=cosine weights=distance | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Full-train NN] fitting for test...\n[Sweep 1/4] k=7 metric=cosine -> pooled macro AUC=0.6317 | dt=0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 01 | tr=231 va=27 | k=9 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 02 | tr=234 va=24 | k=9 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 03 | tr=232 va=26 | k=9 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 04 | tr=244 va=14 | k=9 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 05 | tr=233 va=25 | k=9 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 06 | tr=233 va=25 | k=9 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 07 | tr=236 va=22 | k=9 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 08 | tr=247 va=11 | k=9 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 09 | tr=243 va=15 | k=9 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 10 | tr=243 va=15 | k=9 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 11 | tr=238 va=20 | k=9 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 12 | tr=234 va=24 | k=9 metric=cosine weights=distance | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 13 | tr=248 va=10 | k=9 metric=cosine weights=distance | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Full-train NN] fitting for test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sweep 2/4] k=9 metric=cosine -> pooled macro AUC=0.6496 | dt=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 01 | tr=231 va=27 | k=13 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 02 | tr=234 va=24 | k=13 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 03 | tr=232 va=26 | k=13 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 04 | tr=244 va=14 | k=13 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 05 | tr=233 va=25 | k=13 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 06 | tr=233 va=25 | k=13 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 07 | tr=236 va=22 | k=13 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 08 | tr=247 va=11 | k=13 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 09 | tr=243 va=15 | k=13 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 10 | tr=243 va=15 | k=13 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 11 | tr=238 va=20 | k=13 metric=cosine weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 12 | tr=234 va=24 | k=13 metric=cosine weights=distance | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 13 | tr=248 va=10 | k=13 metric=cosine weights=distance | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Full-train NN] fitting for test...\n[Sweep 3/4] k=13 metric=cosine -> pooled macro AUC=0.6615 | dt=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 01 | tr=231 va=27 | k=11 metric=euclidean weights=distance | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 02 | tr=234 va=24 | k=11 metric=euclidean weights=distance | elapsed=0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 03 | tr=232 va=26 | k=11 metric=euclidean weights=distance | elapsed=0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 04 | tr=244 va=14 | k=11 metric=euclidean weights=distance | elapsed=0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 05 | tr=233 va=25 | k=11 metric=euclidean weights=distance | elapsed=0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 06 | tr=233 va=25 | k=11 metric=euclidean weights=distance | elapsed=0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 07 | tr=236 va=22 | k=11 metric=euclidean weights=distance | elapsed=0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 08 | tr=247 va=11 | k=11 metric=euclidean weights=distance | elapsed=0.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 09 | tr=243 va=15 | k=11 metric=euclidean weights=distance | elapsed=1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 10 | tr=243 va=15 | k=11 metric=euclidean weights=distance | elapsed=1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 11 | tr=238 va=20 | k=11 metric=euclidean weights=distance | elapsed=1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 12 | tr=234 va=24 | k=11 metric=euclidean weights=distance | elapsed=1.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO] fold 13 | tr=248 va=10 | k=11 metric=euclidean weights=distance | elapsed=1.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Full-train NN] fitting for test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sweep 4/4] k=11 metric=euclidean -> pooled macro AUC=0.6549 | dt=1.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best sweep result: {'k': 13, 'metric': 'cosine', 'weights': 'distance'} AUC= 0.6615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_knn_sweep.csv | rows= 1216\nSaved submission_knn_sweep_priorfusion_station.csv | rows= 1216\n"
          ]
        }
      ]
    },
    {
      "id": "074211ba-db01-431d-b13c-922ecd4d4922",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inspect panns_cnn14_emb.npy format to adapt loader\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "p = Path('panns_cnn14_emb.npy')\n",
        "arr = np.load(p, allow_pickle=True)\n",
        "print('Loaded type:', type(arr))\n",
        "if hasattr(arr, 'files'):\n",
        "    print('npz-like files:', arr.files)\n",
        "else:\n",
        "    print('No .files attribute; dtype:', getattr(arr, 'dtype', None), 'shape:', getattr(arr, 'shape', None))\n",
        "    if isinstance(arr, np.ndarray) and arr.dtype == object:\n",
        "        try:\n",
        "            obj = arr.item()\n",
        "            print('Top-level keys:', list(obj.keys()))\n",
        "            for k, v in obj.items():\n",
        "                if hasattr(v, 'shape'):\n",
        "                    print(' key', k, '-> shape', v.shape, 'dtype', getattr(v, 'dtype', None))\n",
        "                else:\n",
        "                    print(' key', k, '-> type', type(v))\n",
        "        except Exception as e:\n",
        "            print('arr.item() failed:', e)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded type: <class 'numpy.ndarray'>\nNo .files attribute; dtype: float32 shape: (322, 2048)\n"
          ]
        }
      ]
    },
    {
      "id": "3636bb47-2503-4a70-b6e9-50239c2a0547",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Regenerate kNN station-aware prior fusion with safer lambda=0.20 and set as submission.csv\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Rebuild minimal metadata and priors\n",
        "DATA_DIR = Path('essential_data')\n",
        "def load_species_list(path: Path):\n",
        "    df = pd.read_csv(path)\n",
        "    if {'class_id','code'}.issubset(df.columns):\n",
        "        return df.sort_values('class_id')['code'].tolist()\n",
        "    with open(path, 'r') as f:\n",
        "        f.readline()\n",
        "        out = []\n",
        "        for line in f:\n",
        "            s = line.strip()\n",
        "            if not s: continue\n",
        "            parts = s.split(',')\n",
        "            out.append(parts[1] if len(parts)>1 else s)\n",
        "    return out\n",
        "def parse_rec_id2filename(path: Path):\n",
        "    df = pd.read_csv(path)\n",
        "    df = df.rename(columns={df.columns[0]:'rec_id', df.columns[1]:'filename'})\n",
        "    df['rec_id'] = df['rec_id'].astype(int)\n",
        "    df['station'] = df['filename'].str.extract(r'^(PC\\d+)')\n",
        "    return df[['rec_id','filename','station']]\n",
        "def parse_labels(path: Path, C: int):\n",
        "    rec_ids, flags, Y = [], [], []\n",
        "    with open(path, 'r') as f:\n",
        "        f.readline()\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line: continue\n",
        "            parts = [tok.strip() for tok in line.split(',')]\n",
        "            try: rid = int(parts[0])\n",
        "            except: continue\n",
        "            tokens = parts[1:] if len(parts)>1 else []\n",
        "            is_test = any(tok=='?' for tok in tokens)\n",
        "            y = np.zeros(C, dtype=int)\n",
        "            if not is_test and tokens:\n",
        "                for tok in tokens:\n",
        "                    if tok in ('','?'): continue\n",
        "                    try: idx = int(tok)\n",
        "                    except: continue\n",
        "                    if 0 <= idx < C: y[idx] = 1\n",
        "            rec_ids.append(rid); flags.append(is_test); Y.append(y)\n",
        "    lab_cols = [f'label_{i}' for i in range(C)]\n",
        "    ydf = pd.DataFrame(np.vstack(Y), columns=lab_cols)\n",
        "    df = pd.DataFrame({'rec_id': rec_ids, 'is_test': flags})\n",
        "    return df.join(ydf), lab_cols\n",
        "def compute_fulltrain_station_priors(meta_train, y_train_df, alpha=30.0):\n",
        "    Y = y_train_df.values.astype(float)\n",
        "    C = Y.shape[1]\n",
        "    p_global = Y.mean(axis=0)\n",
        "    df = pd.DataFrame(Y, columns=[f'c{i}' for i in range(C)])\n",
        "    df['station'] = meta_train['station'].values\n",
        "    grp = df.groupby('station')\n",
        "    n = grp.size()\n",
        "    pos = grp[[f'c{i}' for i in range(C)]].sum()\n",
        "    eb = {}\n",
        "    for st, cnt in n.items():\n",
        "        pos_st = pos.loc[st].values\n",
        "        eb[st] = (pos_st + alpha * p_global) / (cnt + alpha)\n",
        "    st_arr = meta_train['station'].values\n",
        "    prior_train = np.vstack([eb.get(s, p_global) for s in st_arr])\n",
        "    return eb, p_global, prior_train\n",
        "def logit_zscore_full(prior_train):\n",
        "    P = np.clip(prior_train, 1e-6, 1-1e-6)\n",
        "    L = np.log(P/(1-P))\n",
        "    L = np.clip(L, -6, 6)\n",
        "    mu = L.mean(axis=0)\n",
        "    sd = L.std(axis=0) + 1e-6\n",
        "    Z = (L - mu) / sd\n",
        "    return Z, mu, sd\n",
        "def build_test_Z_station(meta_test, eb_map, p_global, mu, sd):\n",
        "    T = len(meta_test)\n",
        "    P = np.tile(p_global, (T, 1))\n",
        "    st_vals = meta_test['station'].values\n",
        "    for i, s in enumerate(st_vals):\n",
        "        if s in eb_map:\n",
        "            P[i] = eb_map[s]\n",
        "    L = np.log(np.clip(P,1e-6,1-1e-6)/np.clip(1-P,1e-6,1))\n",
        "    L = np.clip(L, -6, 6)\n",
        "    return (L - mu)/sd\n",
        "def logit(p):\n",
        "    p = np.clip(p, 1e-6, 1-1e-6)\n",
        "    return np.log(p/(1-p))\n",
        "def sigmoid(x):\n",
        "    return 1.0/(1.0+np.exp(-x))\n",
        "\n",
        "species = load_species_list(DATA_DIR/'species_list.txt')\n",
        "rec_map = parse_rec_id2filename(DATA_DIR/'rec_id2filename.txt')\n",
        "labels_df, _ = parse_labels(DATA_DIR/'rec_labels_test_hidden.txt', len(species))\n",
        "df_all = rec_map.merge(labels_df, on='rec_id', how='right')\n",
        "train_df = df_all[~df_all['is_test']].copy()\n",
        "test_df = df_all[df_all['is_test']].copy()\n",
        "y_train = train_df[[c for c in train_df.columns if c.startswith('label_')]].copy()\n",
        "y_train.columns = [f'label_{s}' for s in species]\n",
        "meta_train = train_df[['rec_id','filename','station']].copy()\n",
        "meta_test = test_df[['rec_id','filename','station']].copy()\n",
        "\n",
        "# Build station-aware Z_test\n",
        "eb_map, p_global, prior_train = compute_fulltrain_station_priors(meta_train, y_train, alpha=30.0)\n",
        "prior_train_z, mu, sd = logit_zscore_full(prior_train)\n",
        "Z_test = build_test_Z_station(meta_test, eb_map, p_global, mu, sd)\n",
        "\n",
        "# Load existing kNN submission and apply lambda=0.20 fusion\n",
        "sub_knn = pd.read_csv('submission_knn.csv').sort_values('Id').reset_index(drop=True)\n",
        "ids = sub_knn['Id'].values; probs = sub_knn['Probability'].values\n",
        "rec_ids_order = meta_test['rec_id'].values.tolist()\n",
        "T = len(rec_ids_order); C = y_train.shape[1]\n",
        "P_knn = np.zeros((T, C), dtype=float)\n",
        "id_to_prob = dict(zip(ids, probs))\n",
        "for i, rid in enumerate(rec_ids_order):\n",
        "    base = rid*100\n",
        "    for cls in range(C):\n",
        "        P_knn[i, cls] = float(id_to_prob.get(base+cls, 0.5))\n",
        "lam = 0.20\n",
        "P_knn_f = sigmoid(np.clip(logit(P_knn) + lam*Z_test, -12, 12))\n",
        "rows = []\n",
        "for i, rid in enumerate(rec_ids_order):\n",
        "    for cls in range(C):\n",
        "        rows.append((rid*100 + cls, float(P_knn_f[i, cls])))\n",
        "out = pd.DataFrame(rows, columns=['Id','Probability']).sort_values('Id').reset_index(drop=True)\n",
        "out.to_csv('submission_knn_priorfusion_station_lam020.csv', index=False)\n",
        "out.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_knn_priorfusion_station_lam020.csv and overwrote submission.csv | rows=', len(out))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_knn_priorfusion_station_lam020.csv and overwrote submission.csv | rows= 1216\n"
          ]
        }
      ]
    },
    {
      "id": "b4a52505-c3e8-4df1-aabb-069c56e22d78",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set submission.csv to robust hedge: submission_sktrees.csv\n",
        "import pandas as pd, os, numpy as np\n",
        "path = 'submission_sktrees.csv'\n",
        "assert os.path.exists(path), 'Missing submission_sktrees.csv'\n",
        "df = pd.read_csv(path).sort_values('Id').reset_index(drop=True)\n",
        "assert len(df)==1216 and {'Id','Probability'}.issubset(df.columns), 'Bad format/row count'\n",
        "assert np.isfinite(df['Probability']).all() and df['Probability'].between(0,1).all(), 'Prob out of [0,1] or non-finite'\n",
        "df.to_csv('submission.csv', index=False)\n",
        "print('Overwrote submission.csv with submission_sktrees.csv | rows=', len(df))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwrote submission.csv with submission_sktrees.csv | rows= 1216\n"
          ]
        }
      ]
    },
    {
      "id": "8b7fb10d-0bb2-4589-8d4c-88bd2ffc277e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set submission.csv to plain weighted blend hedge\n",
        "import pandas as pd, os, numpy as np\n",
        "path = 'submission_weighted.csv'\n",
        "assert os.path.exists(path), 'Missing submission_weighted.csv'\n",
        "df = pd.read_csv(path).sort_values('Id').reset_index(drop=True)\n",
        "assert len(df)==1216 and {'Id','Probability'}.issubset(df.columns), 'Bad format/row count'\n",
        "assert np.isfinite(df['Probability']).all() and df['Probability'].between(0,1).all(), 'Probs out of [0,1] or non-finite'\n",
        "df.to_csv('submission.csv', index=False)\n",
        "print('Overwrote submission.csv with submission_weighted.csv | rows=', len(df))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwrote submission.csv with submission_weighted.csv | rows= 1216\n"
          ]
        }
      ]
    },
    {
      "id": "7526b05d-3509-476e-a2fd-354d1af2e15e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set submission.csv to plain kNN (no prior fusion) per expert fallback\n",
        "import pandas as pd, os, numpy as np\n",
        "path = 'submission_knn.csv'\n",
        "assert os.path.exists(path), 'Missing submission_knn.csv'\n",
        "df = pd.read_csv(path).sort_values('Id').reset_index(drop=True)\n",
        "assert len(df)==1216 and {'Id','Probability'}.issubset(df.columns), 'Bad format/row count'\n",
        "assert np.isfinite(df['Probability']).all() and df['Probability'].between(0,1).all(), 'Prob out of [0,1] or non-finite'\n",
        "df.to_csv('submission.csv', index=False)\n",
        "print('Overwrote submission.csv with submission_knn.csv | rows=', len(df))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwrote submission.csv with submission_knn.csv | rows= 1216\n"
          ]
        }
      ]
    },
    {
      "id": "94db5394-fd58-4ee9-9682-d3f0992199ea",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set submission.csv to FS/PANNs blend per expert order (second submission)\n",
        "import pandas as pd, os, numpy as np\n",
        "path = 'submission_fs.csv'\n",
        "assert os.path.exists(path), 'Missing submission_fs.csv'\n",
        "df = pd.read_csv(path).sort_values('Id').reset_index(drop=True)\n",
        "assert len(df)==1216 and {'Id','Probability'}.issubset(df.columns), 'Bad format/row count'\n",
        "assert np.isfinite(df['Probability']).all() and df['Probability'].between(0,1).all(), 'Prob out of [0,1] or non-finite'\n",
        "df.to_csv('submission.csv', index=False)\n",
        "print('Overwrote submission.csv with submission_fs.csv | rows=', len(df))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwrote submission.csv with submission_fs.csv | rows= 1216\n"
          ]
        }
      ]
    },
    {
      "id": "6848b159-bc71-46dc-80e0-990b1829406b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set submission.csv to 50/50 average hedge (submission_avg_skt_fs.csv)\n",
        "import pandas as pd, os, numpy as np\n",
        "path = 'submission_avg_skt_fs.csv'\n",
        "assert os.path.exists(path), 'Missing submission_avg_skt_fs.csv'\n",
        "df = pd.read_csv(path).sort_values('Id').reset_index(drop=True)\n",
        "assert len(df)==1216 and {'Id','Probability'}.issubset(df.columns), 'Bad format/row count'\n",
        "assert np.isfinite(df['Probability']).all() and df['Probability'].between(0,1).all(), 'Prob out of [0,1] or non-finite'\n",
        "df.to_csv('submission.csv', index=False)\n",
        "print('Overwrote submission.csv with submission_avg_skt_fs.csv | rows=', len(df))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwrote submission.csv with submission_avg_skt_fs.csv | rows= 1216\n"
          ]
        }
      ]
    },
    {
      "id": "d94aec97-b5e0-482b-ad76-c690dbd654a8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd, hashlib, os\n",
        "target = 'submission_rankmean.csv'\n",
        "assert os.path.exists(target), f'Missing {target}'\n",
        "sub = pd.read_csv(target)\n",
        "print('Loaded', target, 'shape=', sub.shape)\n",
        "sample = pd.read_csv('sample_submission.csv')\n",
        "print('Sample shape=', sample.shape, 'cols=', list(sample.columns))\n",
        "assert list(sub.columns) == list(sample.columns), f'Columns do not match sample_submission: {list(sub.columns)} vs {list(sample.columns)}'\n",
        "assert len(sub) == len(sample), f'Row count mismatch: {len(sub)} vs {len(sample)}'\n",
        "sub = sub.sort_values('Id').reset_index(drop=True)\n",
        "sample_sorted = sample.sort_values('Id').reset_index(drop=True)\n",
        "assert (sub['Id'] == sample_sorted['Id']).all(), 'Id values do not match sample_submission after sort'\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "h = hashlib.sha256(open('submission.csv','rb').read()).hexdigest()\n",
        "print('Wrote submission.csv, sha256=', h)\n",
        "print(sub.head(3))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded submission_rankmean.csv shape= (1216, 2)\nSample shape= (1216, 2) cols= ['Id', 'Probability']\nWrote submission.csv, sha256= 99d8c93f92dfaffb0f851347f47b0c264540b1d00801b0b003c2667e2988febb\n    Id  Probability\n0  100     0.212500\n1  101     0.346875\n2  102     0.265625\n"
          ]
        }
      ]
    },
    {
      "id": "ab5586bd-2b4f-48a3-ba1f-a6f1bc901940",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set submission.csv to rebuild equal-weight blend with validation\n",
        "import pandas as pd, os, hashlib, numpy as np\n",
        "path = 'submission_rebuild_blend.csv'\n",
        "assert os.path.exists(path), f'Missing {path}'\n",
        "sub = pd.read_csv(path).sort_values('Id').reset_index(drop=True)\n",
        "sample = pd.read_csv('sample_submission.csv').sort_values('Id').reset_index(drop=True)\n",
        "assert list(sub.columns) == list(sample.columns), f'Bad columns: {list(sub.columns)}'\n",
        "assert len(sub) == len(sample) == 1216, f'Row count mismatch: {len(sub)} vs {len(sample)}'\n",
        "assert (sub['Id'] == sample['Id']).all(), 'Id alignment mismatch vs sample'\n",
        "assert np.isfinite(sub['Probability']).all() and sub['Probability'].between(0,1).all(), 'Probabilities invalid'\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Overwrote submission.csv with', path, '| rows=', len(sub))\n",
        "print('sha256=', hashlib.sha256(open('submission.csv','rb').read()).hexdigest())\n",
        "print(sub.head(3))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwrote submission.csv with submission_rebuild_blend.csv | rows= 1216\nsha256= dc08e95be74bc84c31703a168bdd58b9b2cb244bce16c2e76bd876ef419c9b77\n    Id  Probability\n0  100     0.000641\n1  101     0.003674\n2  102     0.004824\n"
          ]
        }
      ]
    },
    {
      "id": "58c2b266-944d-4642-aebd-79d2003ab61c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Alignment rescue for PANNs kNN: try multiple plausible row orders and pick best by OOF\n",
        "import numpy as np, pandas as pd, time, sys, gc\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def macro_auc_np(P, Y):\n",
        "    C = Y.shape[1]\n",
        "    aucs = []\n",
        "    for c in range(C):\n",
        "        yt = Y[:,c]; yp = P[:,c]\n",
        "        if yt.sum()==0 or yt.sum()==len(yt):\n",
        "            continue\n",
        "        try: aucs.append(roc_auc_score(yt, yp))\n",
        "        except: pass\n",
        "    return float(np.mean(aucs)) if aucs else np.nan\n",
        "\n",
        "def l2_normalize_rows(A):\n",
        "    A = A.astype(np.float32, copy=False)\n",
        "    n = np.linalg.norm(A, axis=1, keepdims=True) + 1e-12\n",
        "    return A / n\n",
        "\n",
        "def knn_predict_block(X_tr_emb, Y_tr, X_va_emb, k=11, metric='cosine'):\n",
        "    nn = NearestNeighbors(n_neighbors=min(k, len(X_tr_emb)), metric=metric)\n",
        "    nn.fit(X_tr_emb)\n",
        "    dists, idxs = nn.kneighbors(X_va_emb, return_distance=True)\n",
        "    w = 1.0 / (dists + 1e-6)\n",
        "    C = Y_tr.shape[1]\n",
        "    P = np.zeros((len(X_va_emb), C), dtype=np.float32)\n",
        "    for i in range(len(X_va_emb)):\n",
        "        nbr = idxs[i]; wi = w[i][:, None]\n",
        "        votes = (Y_tr[nbr] * wi).sum(axis=0)\n",
        "        P[i] = votes / (wi.sum() + 1e-8)\n",
        "    return np.clip(P, 0, 1)\n",
        "\n",
        "def knn_loso_oof_and_test(X_tr, X_te, y_train_df, groups, k=11, metric='cosine'):\n",
        "    logo = LeaveOneGroupOut()\n",
        "    idx = np.arange(len(groups))\n",
        "    Y = y_train_df.values.astype(np.uint8)\n",
        "    C = Y.shape[1]\n",
        "    P_oof = np.zeros((len(Y), C), dtype=np.float32)\n",
        "    fold = 0; t0 = time.time()\n",
        "    for tr, va in logo.split(idx, groups=groups):\n",
        "        fold += 1\n",
        "        print(f'[ALIGN] fold {fold:02d} tr={len(tr)} va={len(va)} | elapsed={time.time()-t0:.1f}s'); sys.stdout.flush()\n",
        "        P_oof[va] = knn_predict_block(X_tr[tr], Y[tr], X_tr[va], k=k, metric=metric)\n",
        "    P_test = knn_predict_block(X_tr, Y, X_te, k=k, metric=metric)\n",
        "    auc = macro_auc_np(P_oof, Y)\n",
        "    return P_oof, P_test, auc\n",
        "\n",
        "# 1) Load core meta/labels\n",
        "DATA_DIR = Path('essential_data')\n",
        "def load_species_list(path: Path):\n",
        "    df = pd.read_csv(path)\n",
        "    if {'class_id','code'}.issubset(df.columns):\n",
        "        return df.sort_values('class_id')['code'].tolist()\n",
        "    with open(path, 'r') as f:\n",
        "        f.readline()\n",
        "        out = []\n",
        "        for line in f:\n",
        "            s = line.strip()\n",
        "            if not s: continue\n",
        "            parts = s.split(',')\n",
        "            out.append(parts[1] if len(parts)>1 else s)\n",
        "    return out\n",
        "def parse_rec_id2filename(path: Path):\n",
        "    df = pd.read_csv(path)\n",
        "    df = df.rename(columns={df.columns[0]:'rec_id', df.columns[1]:'filename'})\n",
        "    df['rec_id'] = df['rec_id'].astype(int)\n",
        "    df['station'] = df['filename'].str.extract(r'^(PC\\d+)')\n",
        "    return df[['rec_id','filename','station']]\n",
        "def parse_labels(path: Path, C: int):\n",
        "    rec_ids, flags, Y = [], [], []\n",
        "    with open(path, 'r') as f:\n",
        "        f.readline()\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line: continue\n",
        "            parts = [tok.strip() for tok in line.split(',')]\n",
        "            try: rid = int(parts[0])\n",
        "            except: continue\n",
        "            tokens = parts[1:] if len(parts)>1 else []\n",
        "            is_test = any(tok=='?' for tok in tokens)\n",
        "            y = np.zeros(C, dtype=int)\n",
        "            if not is_test and tokens:\n",
        "                for tok in tokens:\n",
        "                    if tok in ('','?'): continue\n",
        "                    try: idx = int(tok)\n",
        "                    except: continue\n",
        "                    if 0 <= idx < C: y[idx] = 1\n",
        "            rec_ids.append(rid); flags.append(is_test); Y.append(y)\n",
        "    lab_cols = [f'label_{i}' for i in range(C)]\n",
        "    ydf = pd.DataFrame(np.vstack(Y), columns=lab_cols)\n",
        "    df = pd.DataFrame({'rec_id': rec_ids, 'is_test': flags})\n",
        "    return df.join(ydf), lab_cols\n",
        "\n",
        "species = load_species_list(DATA_DIR/'species_list.txt')\n",
        "assert len(species)==19, f'species count {len(species)} != 19'\n",
        "rec_map = parse_rec_id2filename(DATA_DIR/'rec_id2filename.txt')\n",
        "labels_df, _ = parse_labels(DATA_DIR/'rec_labels_test_hidden.txt', len(species))\n",
        "df_all = rec_map.merge(labels_df, on='rec_id', how='right')\n",
        "train_df = df_all[~df_all['is_test']].copy()\n",
        "test_df = df_all[df_all['is_test']].copy()\n",
        "y_train = train_df[[c for c in train_df.columns if c.startswith('label_')]].copy()\n",
        "y_train.columns = [f'label_{s}' for s in species]\n",
        "groups = train_df['station'].values\n",
        "meta_train = train_df[['rec_id','filename','station']].copy()\n",
        "meta_test = test_df[['rec_id','filename','station']].copy()\n",
        "print(f'Meta: Ntr={len(train_df)} Nte={len(test_df)} C={y_train.shape[1]} stations={len(pd.unique(groups))}')\n",
        "sys.stdout.flush()\n",
        "\n",
        "# 2) Load raw embeddings array\n",
        "E_all = np.load('panns_cnn14_emb.npy', allow_pickle=True)\n",
        "assert isinstance(E_all, np.ndarray) and E_all.ndim==2 and E_all.shape[1]==2048, f'Bad emb shape {getattr(E_all,\"shape\",None)}'\n",
        "assert E_all.shape[0] == len(df_all), f'Embedding rows {E_all.shape[0]} != N_all {len(df_all)}'\n",
        "\n",
        "# Build three candidate alignments:\n",
        "# A) Current df_all row order\n",
        "order_A = df_all.index.values\n",
        "E_A = E_all.copy()  # as-is\n",
        "# B) Sort by rec_id ascending\n",
        "df_sorted = df_all.sort_values('rec_id').reset_index(drop=True)\n",
        "mpB = {rid:i for i, rid in enumerate(df_sorted['rec_id'].values.tolist())}\n",
        "idxB = [mpB[rid] for rid in df_all['rec_id'].values.tolist()]\n",
        "E_B = E_all[idxB]\n",
        "# C) Use rec_map order (as read from file) to assign, then reindex to df_all\n",
        "mpC_assign = {rid:i for i, rid in enumerate(rec_map['rec_id'].values.tolist())}\n",
        "idxC_assign = [mpC_assign[rid] for rid in df_all['rec_id'].values.tolist()]\n",
        "E_C = E_all[idxC_assign]\n",
        "\n",
        "cands = {'A_df_order': E_A, 'B_sort_recid': E_B, 'C_recmap_order': E_C}\n",
        "results = {}\n",
        "best_key, best_auc = None, -1.0\n",
        "for key, E in cands.items():\n",
        "    # Split to train/test via df_all masks\n",
        "    mask_tr = (~df_all['is_test']).values\n",
        "    mask_te = (df_all['is_test']).values\n",
        "    Xtr = l2_normalize_rows(E[mask_tr])\n",
        "    Xte = l2_normalize_rows(E[mask_te])\n",
        "    print(f'Trying alignment {key}: Xtr {Xtr.shape} Xte {Xte.shape}'); sys.stdout.flush()\n",
        "    P_oof, P_test, auc = knn_loso_oof_and_test(Xtr, Xte, y_train, groups, k=11, metric='cosine')\n",
        "    print(f'  -> AUC {auc:.4f}'); sys.stdout.flush()\n",
        "    results[key] = (auc, P_oof, P_test)\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_key = auc, key\n",
        "\n",
        "print('Best alignment:', best_key, 'AUC=', f'{best_auc:.4f}')\n",
        "P_test_best = results[best_key][2]\n",
        "\n",
        "# 3) Build submission from best\n",
        "rows = []\n",
        "for i, rec_id in enumerate(meta_test['rec_id'].values.tolist()):\n",
        "    for c in range(y_train.shape[1]):\n",
        "        rows.append((rec_id*100 + c, float(P_test_best[i, c])))\n",
        "sub = pd.DataFrame(rows, columns=['Id','Probability']).sort_values('Id').reset_index(drop=True)\n",
        "sub.to_csv('submission_knn_alignfix.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_knn_alignfix.csv and overwrote submission.csv | rows=', len(sub))\n",
        "print(sub.head(3))\n",
        "gc.collect();"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta: Ntr=258 Nte=64 C=19 stations=13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying alignment A_df_order: Xtr (258, 2048) Xte (64, 2048)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 01 tr=231 va=27 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 02 tr=234 va=24 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 03 tr=232 va=26 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 04 tr=244 va=14 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 05 tr=233 va=25 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 06 tr=233 va=25 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 07 tr=236 va=22 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 08 tr=247 va=11 | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 09 tr=243 va=15 | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 10 tr=243 va=15 | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 11 tr=238 va=20 | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 12 tr=234 va=24 | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 13 tr=248 va=10 | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> AUC 0.6547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying alignment B_sort_recid: Xtr (258, 2048) Xte (64, 2048)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 01 tr=231 va=27 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 02 tr=234 va=24 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 03 tr=232 va=26 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 04 tr=244 va=14 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 05 tr=233 va=25 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 06 tr=233 va=25 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 07 tr=236 va=22 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 08 tr=247 va=11 | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 09 tr=243 va=15 | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 10 tr=243 va=15 | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 11 tr=238 va=20 | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 12 tr=234 va=24 | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 13 tr=248 va=10 | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> AUC 0.6547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying alignment C_recmap_order: Xtr (258, 2048) Xte (64, 2048)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 01 tr=231 va=27 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 02 tr=234 va=24 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 03 tr=232 va=26 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 04 tr=244 va=14 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 05 tr=233 va=25 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 06 tr=233 va=25 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 07 tr=236 va=22 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 08 tr=247 va=11 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 09 tr=243 va=15 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 10 tr=243 va=15 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 11 tr=238 va=20 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 12 tr=234 va=24 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] fold 13 tr=248 va=10 | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> AUC 0.6547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best alignment: A_df_order AUC= 0.6547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_knn_alignfix.csv and overwrote submission.csv | rows= 1216\n    Id  Probability\n0  100          0.0\n1  101          0.0\n2  102          0.0\n"
          ]
        }
      ]
    },
    {
      "id": "201254dd-3413-4e0e-bf1a-748c4b21c92a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Re-run kNN with strict settings from production: k=11, cosine, uniform weights\n",
        "import numpy as np, pandas as pd, sys, time, gc\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def macro_auc_np(P, Y):\n",
        "    C = Y.shape[1]; aucs = []\n",
        "    for c in range(C):\n",
        "        yt = Y[:,c]; yp = P[:,c]\n",
        "        if yt.sum()==0 or yt.sum()==len(yt):\n",
        "            continue\n",
        "        try: aucs.append(roc_auc_score(yt, yp))\n",
        "        except: pass\n",
        "    return float(np.mean(aucs)) if aucs else np.nan\n",
        "\n",
        "def l2_normalize_rows(A):\n",
        "    A = A.astype(np.float32, copy=False)\n",
        "    n = np.linalg.norm(A, axis=1, keepdims=True) + 1e-12\n",
        "    return A / n\n",
        "\n",
        "def load_species_list(path: Path):\n",
        "    df = pd.read_csv(path)\n",
        "    if {'class_id','code'}.issubset(df.columns):\n",
        "        return df.sort_values('class_id')['code'].tolist()\n",
        "    with open(path, 'r') as f:\n",
        "        f.readline()\n",
        "        out = []\n",
        "        for line in f:\n",
        "            s = line.strip()\n",
        "            if not s: continue\n",
        "            parts = s.split(',')\n",
        "            out.append(parts[1] if len(parts)>1 else s)\n",
        "    return out\n",
        "\n",
        "def parse_rec_id2filename(path: Path):\n",
        "    df = pd.read_csv(path)\n",
        "    df = df.rename(columns={df.columns[0]:'rec_id', df.columns[1]:'filename'})\n",
        "    df['rec_id'] = df['rec_id'].astype(int)\n",
        "    df['station'] = df['filename'].str.extract(r'^(PC\\d+)')\n",
        "    return df[['rec_id','filename','station']]\n",
        "\n",
        "def parse_labels(path: Path, C: int):\n",
        "    rec_ids, flags, Y = [], [], []\n",
        "    with open(path, 'r') as f:\n",
        "        f.readline()\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line: continue\n",
        "            parts = [tok.strip() for tok in line.split(',')]\n",
        "            try: rid = int(parts[0])\n",
        "            except: continue\n",
        "            tokens = parts[1:] if len(parts)>1 else []\n",
        "            is_test = any(tok=='?' for tok in tokens)\n",
        "            y = np.zeros(C, dtype=int)\n",
        "            if not is_test and tokens:\n",
        "                for tok in tokens:\n",
        "                    if tok in ('','?'): continue\n",
        "                    try: idx = int(tok)\n",
        "                    except: continue\n",
        "                    if 0 <= idx < C: y[idx] = 1\n",
        "            rec_ids.append(rid); flags.append(is_test); Y.append(y)\n",
        "    lab_cols = [f'label_{i}' for i in range(C)]\n",
        "    ydf = pd.DataFrame(np.vstack(Y), columns=lab_cols)\n",
        "    df = pd.DataFrame({'rec_id': rec_ids, 'is_test': flags})\n",
        "    return df.join(ydf), lab_cols\n",
        "\n",
        "DATA_DIR = Path('essential_data')\n",
        "species = load_species_list(DATA_DIR/'species_list.txt')\n",
        "assert len(species)==19, 'species != 19'\n",
        "rec_map = parse_rec_id2filename(DATA_DIR/'rec_id2filename.txt')\n",
        "labels_df, _ = parse_labels(DATA_DIR/'rec_labels_test_hidden.txt', len(species))\n",
        "df_all = rec_map.merge(labels_df, on='rec_id', how='right')\n",
        "train_df = df_all[~df_all['is_test']].copy()\n",
        "test_df = df_all[df_all['is_test']].copy()\n",
        "y_train = train_df[[c for c in train_df.columns if c.startswith('label_')]].copy()\n",
        "y_train.columns = [f'label_{s}' for s in species]\n",
        "groups = train_df['station'].values\n",
        "meta_test = test_df[['rec_id','filename','station']].copy()\n",
        "print(f'Ntr={len(train_df)} Nte={len(test_df)} C={y_train.shape[1]} stations={len(pd.unique(groups))}')\n",
        "sys.stdout.flush()\n",
        "\n",
        "# Use as-is embedding row order split by is_test mask (previous best among tried alignments)\n",
        "E_all = np.load('panns_cnn14_emb.npy', allow_pickle=True)\n",
        "assert isinstance(E_all, np.ndarray) and E_all.ndim==2 and E_all.shape[1]==2048, 'Bad emb array'\n",
        "mask_tr = (~df_all['is_test']).values\n",
        "mask_te = (df_all['is_test']).values\n",
        "Xtr = l2_normalize_rows(E_all[mask_tr])\n",
        "Xte = l2_normalize_rows(E_all[mask_te])\n",
        "assert np.allclose(np.linalg.norm(Xtr[:5], axis=1), 1.0, atol=1e-5)\n",
        "print('Emb shapes:', Xtr.shape, Xte.shape)\n",
        "\n",
        "def knn_predict_uniform(X_tr, Y_tr, X_va, k=11, metric='cosine'):\n",
        "    nn = NearestNeighbors(n_neighbors=min(k, len(X_tr)), metric=metric)\n",
        "    nn.fit(X_tr)\n",
        "    _, idxs = nn.kneighbors(X_va, return_distance=True)\n",
        "    C = Y_tr.shape[1]\n",
        "    P = np.zeros((len(X_va), C), dtype=np.float32)\n",
        "    for i in range(len(X_va)):\n",
        "        nbr = idxs[i]\n",
        "        P[i] = Y_tr[nbr].mean(axis=0)\n",
        "    return np.clip(P, 0, 1)\n",
        "\n",
        "logo = LeaveOneGroupOut()\n",
        "idx = np.arange(len(groups))\n",
        "Y = y_train.values.astype(np.uint8)\n",
        "P_oof = np.zeros_like(Y, dtype=np.float32)\n",
        "t0 = time.time()\n",
        "for f, (tr, va) in enumerate(logo.split(idx, groups=groups), 1):\n",
        "    print(f'[UNIF] fold {f:02d} tr={len(tr)} va={len(va)} | elapsed={time.time()-t0:.1f}s'); sys.stdout.flush()\n",
        "    P_oof[va] = knn_predict_uniform(Xtr[tr], Y[tr], Xtr[va], k=11, metric='cosine')\n",
        "auc = macro_auc_np(P_oof, Y)\n",
        "print('Pooled macro AUC (k=11, cosine, uniform):', f'{auc:.4f}')\n",
        "\n",
        "# Full-train for test\n",
        "P_test = knn_predict_uniform(Xtr, Y, Xte, k=11, metric='cosine')\n",
        "rows = []\n",
        "rec_ids = meta_test['rec_id'].values.tolist()\n",
        "C = Y.shape[1]\n",
        "for i, rid in enumerate(rec_ids):\n",
        "    for c in range(C):\n",
        "        rows.append((rid*100 + c, float(P_test[i, c])))\n",
        "sub = pd.DataFrame(rows, columns=['Id','Probability']).sort_values('Id').reset_index(drop=True)\n",
        "sub.to_csv('submission_knn_uniform.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_knn_uniform.csv and overwrote submission.csv | rows=', len(sub))\n",
        "print(sub.head(3))\n",
        "gc.collect();"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ntr=258 Nte=64 C=19 stations=13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emb shapes: (258, 2048) (64, 2048)\n[UNIF] fold 01 tr=231 va=27 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UNIF] fold 02 tr=234 va=24 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UNIF] fold 03 tr=232 va=26 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UNIF] fold 04 tr=244 va=14 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UNIF] fold 05 tr=233 va=25 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UNIF] fold 06 tr=233 va=25 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UNIF] fold 07 tr=236 va=22 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UNIF] fold 08 tr=247 va=11 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UNIF] fold 09 tr=243 va=15 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UNIF] fold 10 tr=243 va=15 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UNIF] fold 11 tr=238 va=20 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UNIF] fold 12 tr=234 va=24 | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UNIF] fold 13 tr=248 va=10 | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled macro AUC (k=11, cosine, uniform): 0.6523\nSaved submission_knn_uniform.csv and overwrote submission.csv | rows= 1216\n    Id  Probability\n0  100          0.0\n1  101          0.0\n2  102          0.0\n"
          ]
        }
      ]
    },
    {
      "id": "bcbeb21c-8416-428e-bf0e-34e5e17291ee",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Regenerate PANNs CNN14 embeddings with explicit rec_id mapping and save as NPZ\n",
        "import os, sys, time, gc, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "DATA_DIR = Path('essential_data')\n",
        "WAV_DIR = DATA_DIR/'src_wavs'\n",
        "\n",
        "def log(msg):\n",
        "    print(time.strftime('%H:%M:%S'), msg, flush=True)\n",
        "\n",
        "# Robust loaders\n",
        "def load_species_list(path: Path):\n",
        "    try:\n",
        "        df = pd.read_csv(path)\n",
        "        if {'class_id','code'}.issubset(df.columns):\n",
        "            return df.sort_values('class_id')['code'].tolist()\n",
        "    except Exception:\n",
        "        pass\n",
        "    lines = []\n",
        "    with open(path, 'r') as f:\n",
        "        for ln in f:\n",
        "            s = ln.strip()\n",
        "            if not s: continue\n",
        "            if s.lower().startswith('species') or ',' in s or '\\t' in s:\n",
        "                continue\n",
        "            lines.append(s)\n",
        "    return lines\n",
        "\n",
        "def parse_rec_id2filename(path: Path):\n",
        "    df = pd.read_csv(path)\n",
        "    df = df.rename(columns={df.columns[0]:'rec_id', df.columns[1]:'filename'})\n",
        "    df['rec_id'] = df['rec_id'].astype(int)\n",
        "    df['station'] = df['filename'].str.extract(r'^(PC\\d+)')\n",
        "    return df[['rec_id','filename','station']]\n",
        "\n",
        "def parse_labels(path: Path, C: int):\n",
        "    rec_ids, flags, Y = [], [], []\n",
        "    with open(path, 'r') as f:\n",
        "        _ = f.readline()\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line: continue\n",
        "            parts = [tok.strip() for tok in line.split(',')]\n",
        "            try: rid = int(parts[0])\n",
        "            except: continue\n",
        "            tokens = parts[1:] if len(parts)>1 else []\n",
        "            is_test = any(tok=='?' for tok in tokens)\n",
        "            y = np.zeros(C, dtype=np.uint8)\n",
        "            if not is_test:\n",
        "                for tok in tokens:\n",
        "                    if tok in ('','?'): continue\n",
        "                    try: idx = int(tok)\n",
        "                    except: continue\n",
        "                    if 0 <= idx < C: y[idx] = 1\n",
        "            rec_ids.append(rid); flags.append(is_test); Y.append(y)\n",
        "    lab_cols = [f'label_{i}' for i in range(C)]\n",
        "    ydf = pd.DataFrame(np.vstack(Y), columns=lab_cols)\n",
        "    df = pd.DataFrame({'rec_id': rec_ids, 'is_test': flags})\n",
        "    return df.join(ydf), lab_cols\n",
        "\n",
        "# Build meta\n",
        "species = load_species_list(DATA_DIR/'species_list.txt')\n",
        "assert len(species)==19, f'species {len(species)} != 19'\n",
        "rec_map = parse_rec_id2filename(DATA_DIR/'rec_id2filename.txt')\n",
        "labels_df, _ = parse_labels(DATA_DIR/'rec_labels_test_hidden.txt', len(species))\n",
        "df_all = rec_map.merge(labels_df, on='rec_id', how='right')\n",
        "train_df = df_all[~df_all['is_test']].copy()\n",
        "test_df = df_all[df_all['is_test']].copy()\n",
        "log(f'Meta ready: N_all={len(df_all)} Ntr={len(train_df)} Nte={len(test_df)} stations={df_all.station.nunique()}')\n",
        "\n",
        "# Ensure deps\n",
        "def ensure(pkgs):\n",
        "    import importlib\n",
        "    for name, pipname in pkgs:\n",
        "        try:\n",
        "            importlib.import_module(name)\n",
        "        except Exception:\n",
        "            log(f'Installing {pipname} ...')\n",
        "            import subprocess\n",
        "            subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', pipname], check=True)\n",
        "ensure([('librosa','librosa==0.10.1'), ('soundfile','soundfile==0.12.1')])\n",
        "try:\n",
        "    import torch\n",
        "except Exception:\n",
        "    # Fallback to CPU torch if missing\n",
        "    ensure([('torch','torch==2.2.2')])\n",
        "    import torch\n",
        "\n",
        "import librosa, soundfile as sf\n",
        "sys.path.insert(0, str(Path('panns_repo')/'pytorch'))\n",
        "from models import Cnn14\n",
        "\n",
        "# Load PANNs CNN14 model and weights\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "log(f'Using device: {device}')\n",
        "# IMPORTANT: Use window_size=512 and hop_size=160 to match Cnn14_32k.pth (257 FFT bins)\n",
        "model = Cnn14(sample_rate=32000, window_size=512, hop_size=160, mel_bins=64, fmin=50, fmax=14000, classes_num=527)\n",
        "ckpt = torch.load('Cnn14_32k.pth', map_location='cpu')\n",
        "state = ckpt.get('model', ckpt)\n",
        "missing, unexpected = model.load_state_dict(state, strict=False)\n",
        "log(f'Loaded Cnn14 weights | missing={len(missing)} unexpected={len(unexpected)}')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Helper to load and prepare audio\n",
        "SR = 32000\n",
        "FIX_LEN = SR * 10  # 10 seconds\n",
        "def load_audio(filepath: Path):\n",
        "    y, sr = librosa.load(filepath, sr=SR, mono=True)\n",
        "    if len(y) < FIX_LEN:\n",
        "        y = np.pad(y, (0, FIX_LEN - len(y)))\n",
        "    elif len(y) > FIX_LEN:\n",
        "        y = y[:FIX_LEN]\n",
        "    return y.astype(np.float32)\n",
        "\n",
        "# Compute embeddings in df_all row order\n",
        "N = len(df_all)\n",
        "emb = np.zeros((N, 2048), dtype=np.float32)\n",
        "t0 = time.time()\n",
        "bs = 16\n",
        "buf = []\n",
        "buf_idx = []\n",
        "def flush_batch():\n",
        "    if not buf: return\n",
        "    x = np.stack(buf, axis=0)\n",
        "    with torch.no_grad():\n",
        "        xt = torch.from_numpy(x).to(device)\n",
        "        out = model(xt)  # expect dict with 'embedding'\n",
        "        if isinstance(out, dict) and 'embedding' in out:\n",
        "            z = out['embedding']\n",
        "        elif hasattr(out, 'embedding'):\n",
        "            z = out.embedding\n",
        "        else:\n",
        "            # Some versions return tuple (clipwise_output, embedding)\n",
        "            try:\n",
        "                z = out[1]\n",
        "            except Exception:\n",
        "                raise RuntimeError('Unexpected Cnn14 output structure')\n",
        "        z = z.detach().cpu().numpy().astype(np.float32)\n",
        "    for j, idx in enumerate(buf_idx):\n",
        "        emb[idx] = z[j]\n",
        "    buf.clear(); buf_idx.clear()\n",
        "\n",
        "for i, row in enumerate(df_all.itertuples(index=False)):\n",
        "    if (i % 25)==0 and i>0:\n",
        "        log(f'Processed {i}/{N} | dt={time.time()-t0:.1f}s')\n",
        "    fname = getattr(row, 'filename')\n",
        "    fname = str(fname)\n",
        "    if not fname.lower().endswith('.wav'):\n",
        "        fname = fname + '.wav'\n",
        "    wav_path = WAV_DIR / fname\n",
        "    if not wav_path.exists():\n",
        "        raise FileNotFoundError(f'Missing audio file: {wav_path}')\n",
        "    y = load_audio(wav_path)\n",
        "    buf.append(y)\n",
        "    buf_idx.append(i)\n",
        "    if len(buf) >= bs:\n",
        "        flush_batch()\n",
        "flush_batch()\n",
        "log(f'Embeddings computed: shape={emb.shape} | total dt={time.time()-t0:.1f}s')\n",
        "\n",
        "# Split to train/test aligned to meta order\n",
        "mask_tr = (~df_all['is_test']).values\n",
        "mask_te = (df_all['is_test']).values\n",
        "X_train_emb = emb[mask_tr]\n",
        "X_test_emb = emb[mask_te]\n",
        "ids_train = train_df['rec_id'].values.astype(np.int64)\n",
        "ids_test = test_df['rec_id'].values.astype(np.int64)\n",
        "assert X_train_emb.shape[0]==len(ids_train) and X_test_emb.shape[0]==len(ids_test)\n",
        "\n",
        "# Save NPZ with explicit ids\n",
        "out_path = 'panns_cnn14_emb_v2.npz'\n",
        "np.savez_compressed(out_path, X_train=X_train_emb, X_test=X_test_emb, train_ids=ids_train, test_ids=ids_test)\n",
        "log(f'Saved {out_path} with shapes tr={X_train_emb.shape} te={X_test_emb.shape}')\n",
        "gc.collect();"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23:17:24 Meta ready: N_all=322 Ntr=258 Nte=64 stations=13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23:17:24 Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23:17:26 Loaded Cnn14 weights | missing=0 unexpected=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23:17:26 Processed 25/322 | dt=0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23:17:27 Processed 50/322 | dt=1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23:17:27 Processed 75/322 | dt=1.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23:17:28 Processed 100/322 | dt=1.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23:17:28 Processed 125/322 | dt=2.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23:17:28 Processed 150/322 | dt=2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23:17:29 Processed 175/322 | dt=2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23:17:29 Processed 200/322 | dt=3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23:17:30 Processed 225/322 | dt=3.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23:17:30 Processed 250/322 | dt=4.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23:17:30 Processed 275/322 | dt=4.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23:17:31 Processed 300/322 | dt=4.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23:17:31 Embeddings computed: shape=(322, 2048) | total dt=5.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23:17:31 Saved panns_cnn14_emb_v2.npz with shapes tr=(258, 2048) te=(64, 2048)\n"
          ]
        }
      ]
    },
    {
      "id": "5a726d38-7bb5-4088-81a3-1a44f1e40e12",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Use regenerated embeddings (panns_cnn14_emb_v2.npz) with explicit ids for LOSO kNN and submission\n",
        "import numpy as np, pandas as pd, time, sys, gc\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def macro_auc_np(P, Y):\n",
        "    C = Y.shape[1]; aucs = []\n",
        "    for c in range(C):\n",
        "        yt = Y[:,c]; yp = P[:,c]\n",
        "        if yt.sum()==0 or yt.sum()==len(yt):\n",
        "            continue\n",
        "        try: aucs.append(roc_auc_score(yt, yp))\n",
        "        except: pass\n",
        "    return float(np.mean(aucs)) if aucs else np.nan\n",
        "\n",
        "def l2_normalize_rows(A):\n",
        "    A = A.astype(np.float32, copy=False)\n",
        "    n = np.linalg.norm(A, axis=1, keepdims=True) + 1e-12\n",
        "    return A / n\n",
        "\n",
        "DATA_DIR = Path('essential_data')\n",
        "def load_species_list(path: Path):\n",
        "    df = pd.read_csv(path)\n",
        "    if {'class_id','code'}.issubset(df.columns):\n",
        "        return df.sort_values('class_id')['code'].tolist()\n",
        "    with open(path, 'r') as f:\n",
        "        f.readline()\n",
        "        out = []\n",
        "        for line in f:\n",
        "            s = line.strip()\n",
        "            if not s: continue\n",
        "            parts = s.split(',')\n",
        "            out.append(parts[1] if len(parts)>1 else s)\n",
        "    return out\n",
        "def parse_rec_id2filename(path: Path):\n",
        "    df = pd.read_csv(path)\n",
        "    df = df.rename(columns={df.columns[0]:'rec_id', df.columns[1]:'filename'})\n",
        "    df['rec_id'] = df['rec_id'].astype(int)\n",
        "    df['station'] = df['filename'].str.extract(r'^(PC\\d+)')\n",
        "    return df[['rec_id','filename','station']]\n",
        "def parse_labels(path: Path, C: int):\n",
        "    rec_ids, flags, Y = [], [], []\n",
        "    with open(path, 'r') as f:\n",
        "        f.readline()\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line: continue\n",
        "            parts = [tok.strip() for tok in line.split(',')]\n",
        "            try: rid = int(parts[0])\n",
        "            except: continue\n",
        "            tokens = parts[1:] if len(parts)>1 else []\n",
        "            is_test = any(tok=='?' for tok in tokens)\n",
        "            y = np.zeros(C, dtype=int)\n",
        "            if not is_test and tokens:\n",
        "                for tok in tokens:\n",
        "                    if tok in ('','?'): continue\n",
        "                    try: idx = int(tok)\n",
        "                    except: continue\n",
        "                    if 0 <= idx < C: y[idx] = 1\n",
        "            rec_ids.append(rid); flags.append(is_test); Y.append(y)\n",
        "    lab_cols = [f'label_{i}' for i in range(C)]\n",
        "    ydf = pd.DataFrame(np.vstack(Y), columns=lab_cols)\n",
        "    df = pd.DataFrame({'rec_id': rec_ids, 'is_test': flags})\n",
        "    return df.join(ydf), lab_cols\n",
        "\n",
        "species = load_species_list(DATA_DIR/'species_list.txt')\n",
        "assert len(species)==19, 'species != 19'\n",
        "rec_map = parse_rec_id2filename(DATA_DIR/'rec_id2filename.txt')\n",
        "labels_df, _ = parse_labels(DATA_DIR/'rec_labels_test_hidden.txt', len(species))\n",
        "df_all = rec_map.merge(labels_df, on='rec_id', how='right')\n",
        "train_df = df_all[~df_all['is_test']].copy()\n",
        "test_df = df_all[df_all['is_test']].copy()\n",
        "y_train = train_df[[c for c in train_df.columns if c.startswith('label_')]].copy()\n",
        "y_train.columns = [f'label_{s}' for s in species]\n",
        "groups = train_df['station'].values\n",
        "meta_train = train_df[['rec_id','filename','station']].copy()\n",
        "meta_test = test_df[['rec_id','filename','station']].copy()\n",
        "print(f'Ntr={len(train_df)} Nte={len(test_df)} C={y_train.shape[1]} stations={len(pd.unique(groups))}')\n",
        "sys.stdout.flush()\n",
        "\n",
        "# Load regenerated embeddings with explicit ids\n",
        "npz = np.load('panns_cnn14_emb_v2.npz')\n",
        "Xtr_raw = npz['X_train']; Xte_raw = npz['X_test']\n",
        "ids_tr = npz['train_ids'].astype(int); ids_te = npz['test_ids'].astype(int)\n",
        "\n",
        "# Align to meta order\n",
        "mp_tr = {int(r): i for i, r in enumerate(ids_tr.tolist())}\n",
        "mp_te = {int(r): i for i, r in enumerate(ids_te.tolist())}\n",
        "idx_tr = [mp_tr[int(r)] for r in meta_train['rec_id'].values.tolist()]\n",
        "idx_te = [mp_te[int(r)] for r in meta_test['rec_id'].values.tolist()]\n",
        "Xtr = l2_normalize_rows(Xtr_raw[idx_tr]); Xte = l2_normalize_rows(Xte_raw[idx_te])\n",
        "assert Xtr.shape==(258,2048) and Xte.shape==(64,2048), f'Shapes mismatch: {Xtr.shape}, {Xte.shape}'\n",
        "assert np.allclose(np.linalg.norm(Xtr[:5], axis=1), 1.0, atol=1e-5)\n",
        "print('Embeddings aligned:', Xtr.shape, Xte.shape); sys.stdout.flush()\n",
        "\n",
        "def knn_predict_uniform(X_tr, Y_tr, X_va, k=11, metric='cosine'):\n",
        "    nn = NearestNeighbors(n_neighbors=min(k, len(X_tr)), metric=metric)\n",
        "    nn.fit(X_tr)\n",
        "    _, idxs = nn.kneighbors(X_va, return_distance=True)\n",
        "    C = Y_tr.shape[1]\n",
        "    P = np.zeros((len(X_va), C), dtype=np.float32)\n",
        "    for i in range(len(X_va)):\n",
        "        nbr = idxs[i]\n",
        "        P[i] = Y_tr[nbr].mean(axis=0)\n",
        "    return np.clip(P, 0, 1)\n",
        "\n",
        "logo = LeaveOneGroupOut()\n",
        "idx = np.arange(len(groups))\n",
        "Y = y_train.values.astype(np.uint8)\n",
        "P_oof = np.zeros_like(Y, dtype=np.float32)\n",
        "t0 = time.time()\n",
        "for f, (tr, va) in enumerate(logo.split(idx, groups=groups), 1):\n",
        "    print(f'[LOSO v2] fold {f:02d} tr={len(tr)} va={len(va)} | elapsed={time.time()-t0:.1f}s'); sys.stdout.flush()\n",
        "    P_oof[va] = knn_predict_uniform(Xtr[tr], Y[tr], Xtr[va], k=11, metric='cosine')\n",
        "auc = macro_auc_np(P_oof, Y)\n",
        "print('Pooled macro AUC (v2, k=11, cosine, uniform):', f'{auc:.4f}')\n",
        "\n",
        "# Full-train for test\n",
        "P_test = knn_predict_uniform(Xtr, Y, Xte, k=11, metric='cosine')\n",
        "rows = []\n",
        "rec_ids = meta_test['rec_id'].values.tolist()\n",
        "C = Y.shape[1]\n",
        "for i, rid in enumerate(rec_ids):\n",
        "    for c in range(C):\n",
        "        rows.append((rid*100 + c, float(P_test[i, c])))\n",
        "sub = pd.DataFrame(rows, columns=['Id','Probability']).sort_values('Id').reset_index(drop=True)\n",
        "sub.to_csv('submission_knn_v2.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_knn_v2.csv and overwrote submission.csv | rows=', len(sub))\n",
        "print(sub.head(3))\n",
        "gc.collect();"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ntr=258 Nte=64 C=19 stations=13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings aligned: (258, 2048) (64, 2048)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO v2] fold 01 tr=231 va=27 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO v2] fold 02 tr=234 va=24 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO v2] fold 03 tr=232 va=26 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO v2] fold 04 tr=244 va=14 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO v2] fold 05 tr=233 va=25 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO v2] fold 06 tr=233 va=25 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO v2] fold 07 tr=236 va=22 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO v2] fold 08 tr=247 va=11 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO v2] fold 09 tr=243 va=15 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO v2] fold 10 tr=243 va=15 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO v2] fold 11 tr=238 va=20 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO v2] fold 12 tr=234 va=24 | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSO v2] fold 13 tr=248 va=10 | elapsed=0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled macro AUC (v2, k=11, cosine, uniform): 0.6266\nSaved submission_knn_v2.csv and overwrote submission.csv | rows= 1216\n    Id  Probability\n0  100     0.000000\n1  101     0.181818\n2  102     0.000000\n"
          ]
        }
      ]
    },
    {
      "id": "7b6b65fb-97ea-4bf6-89e5-e56501c03d1c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# LOSO Logistic Regression on regenerated PANNs embeddings (Standardized per fold)\n",
        "import numpy as np, pandas as pd, time, sys, gc\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def macro_auc_np(P, Y):\n",
        "    C = Y.shape[1]; aucs = []\n",
        "    for c in range(C):\n",
        "        yt = Y[:,c]; yp = P[:,c]\n",
        "        if yt.sum()==0 or yt.sum()==len(yt):\n",
        "            continue\n",
        "        try: aucs.append(roc_auc_score(yt, yp))\n",
        "        except: pass\n",
        "    return float(np.mean(aucs)) if aucs else np.nan\n",
        "\n",
        "DATA_DIR = Path('essential_data')\n",
        "def load_species_list(path: Path):\n",
        "    df = pd.read_csv(path)\n",
        "    if {'class_id','code'}.issubset(df.columns):\n",
        "        return df.sort_values('class_id')['code'].tolist()\n",
        "    with open(path, 'r') as f:\n",
        "        f.readline()\n",
        "        out = []\n",
        "        for line in f:\n",
        "            s = line.strip()\n",
        "            if not s: continue\n",
        "            parts = s.split(',')\n",
        "            out.append(parts[1] if len(parts)>1 else s)\n",
        "    return out\n",
        "def parse_rec_id2filename(path: Path):\n",
        "    df = pd.read_csv(path)\n",
        "    df = df.rename(columns={df.columns[0]:'rec_id', df.columns[1]:'filename'})\n",
        "    df['rec_id'] = df['rec_id'].astype(int)\n",
        "    df['station'] = df['filename'].str.extract(r'^(PC\\d+)')\n",
        "    return df[['rec_id','filename','station']]\n",
        "def parse_labels(path: Path, C: int):\n",
        "    rec_ids, flags, Y = [], [], []\n",
        "    with open(path, 'r') as f:\n",
        "        f.readline()\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line: continue\n",
        "            parts = [tok.strip() for tok in line.split(',')]\n",
        "            try: rid = int(parts[0])\n",
        "            except: continue\n",
        "            tokens = parts[1:] if len(parts)>1 else []\n",
        "            is_test = any(tok=='?' for tok in tokens)\n",
        "            y = np.zeros(C, dtype=int)\n",
        "            if not is_test and tokens:\n",
        "                for tok in tokens:\n",
        "                    if tok in ('','?'): continue\n",
        "                    try: idx = int(tok)\n",
        "                    except: continue\n",
        "                    if 0 <= idx < C: y[idx] = 1\n",
        "            rec_ids.append(rid); flags.append(is_test); Y.append(y)\n",
        "    lab_cols = [f'label_{i}' for i in range(C)]\n",
        "    ydf = pd.DataFrame(np.vstack(Y), columns=lab_cols)\n",
        "    df = pd.DataFrame({'rec_id': rec_ids, 'is_test': flags})\n",
        "    return df.join(ydf), lab_cols\n",
        "\n",
        "species = load_species_list(DATA_DIR/'species_list.txt')\n",
        "assert len(species)==19, 'species != 19'\n",
        "rec_map = parse_rec_id2filename(DATA_DIR/'rec_id2filename.txt')\n",
        "labels_df, _ = parse_labels(DATA_DIR/'rec_labels_test_hidden.txt', len(species))\n",
        "df_all = rec_map.merge(labels_df, on='rec_id', how='right')\n",
        "train_df = df_all[~df_all['is_test']].copy()\n",
        "test_df = df_all[df_all['is_test']].copy()\n",
        "y_train = train_df[[c for c in train_df.columns if c.startswith('label_')]].copy()\n",
        "y_train.columns = [f'label_{s}' for s in species]\n",
        "groups = train_df['station'].values\n",
        "meta_train = train_df[['rec_id','filename','station']].copy()\n",
        "meta_test = test_df[['rec_id','filename','station']].copy()\n",
        "print(f'Ntr={len(train_df)} Nte={len(test_df)} C={y_train.shape[1]} stations={len(pd.unique(groups))}')\n",
        "sys.stdout.flush()\n",
        "\n",
        "# Load regenerated embeddings with explicit ids and align to meta order\n",
        "npz = np.load('panns_cnn14_emb_v2.npz')\n",
        "Xtr_raw = npz['X_train']; Xte_raw = npz['X_test']\n",
        "ids_tr = npz['train_ids'].astype(int); ids_te = npz['test_ids'].astype(int)\n",
        "mp_tr = {int(r): i for i, r in enumerate(ids_tr.tolist())}\n",
        "mp_te = {int(r): i for i, r in enumerate(ids_te.tolist())}\n",
        "idx_tr = [mp_tr[int(r)] for r in meta_train['rec_id'].values.tolist()]\n",
        "idx_te = [mp_te[int(r)] for r in meta_test['rec_id'].values.tolist()]\n",
        "Xtr = Xtr_raw[idx_tr].astype(np.float32); Xte = Xte_raw[idx_te].astype(np.float32)\n",
        "assert Xtr.shape==(258,2048) and Xte.shape==(64,2048)\n",
        "\n",
        "# LOSO with per-fold StandardScaler and OvR LogisticRegression\n",
        "logo = LeaveOneGroupOut()\n",
        "idx = np.arange(len(groups))\n",
        "Y = y_train.values.astype(np.uint8)\n",
        "C = Y.shape[1]\n",
        "P_oof = np.zeros((len(Y), C), dtype=np.float32)\n",
        "t0 = time.time()\n",
        "for f, (tr, va) in enumerate(logo.split(idx, groups=groups), 1):\n",
        "    print(f'[LR] fold {f:02d} tr={len(tr)} va={len(va)} | elapsed={time.time()-t0:.1f}s'); sys.stdout.flush()\n",
        "    ss = StandardScaler(with_mean=True, with_std=True)\n",
        "    Xtr_s = ss.fit_transform(Xtr[tr])\n",
        "    Xva_s = ss.transform(Xtr[va])\n",
        "    for c in range(C):\n",
        "        ytr_c = Y[tr, c]\n",
        "        if ytr_c.min() == ytr_c.max():\n",
        "            P_oof[va, c] = Y[:, c].mean()\n",
        "            continue\n",
        "        lr = LogisticRegression(C=1.0, penalty='l2', solver='lbfgs', class_weight='balanced', max_iter=2000, n_jobs=-1, random_state=42)\n",
        "        lr.fit(Xtr_s, ytr_c)\n",
        "        P_oof[va, c] = lr.predict_proba(Xva_s)[:, 1]\n",
        "auc = macro_auc_np(P_oof, Y)\n",
        "print('Pooled macro AUC (LR on PANNs):', f'{auc:.4f}')\n",
        "\n",
        "# Full train for test predictions\n",
        "ss_full = StandardScaler(with_mean=True, with_std=True)\n",
        "Xtr_full_s = ss_full.fit_transform(Xtr)\n",
        "Xte_full_s = ss_full.transform(Xte)\n",
        "P_test = np.zeros((len(Xte_full_s), C), dtype=np.float32)\n",
        "for c in range(C):\n",
        "    y_c = Y[:, c]\n",
        "    if y_c.min() == y_c.max():\n",
        "        P_test[:, c] = Y[:, c].mean()\n",
        "        continue\n",
        "    lr = LogisticRegression(C=1.0, penalty='l2', solver='lbfgs', class_weight='balanced', max_iter=2000, n_jobs=-1, random_state=42)\n",
        "    lr.fit(Xtr_full_s, y_c)\n",
        "    P_test[:, c] = lr.predict_proba(Xte_full_s)[:, 1]\n",
        "\n",
        "# Build submission\n",
        "rows = []\n",
        "rec_ids = meta_test['rec_id'].values.tolist()\n",
        "for i, rid in enumerate(rec_ids):\n",
        "    for c in range(C):\n",
        "        rows.append((rid*100 + c, float(P_test[i, c])))\n",
        "sub = pd.DataFrame(rows, columns=['Id','Probability']).sort_values('Id').reset_index(drop=True)\n",
        "sub.to_csv('submission_lr_panns.csv', index=False)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission_lr_panns.csv and overwrote submission.csv | rows=', len(sub))\n",
        "print(sub.head(3))\n",
        "gc.collect();"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ntr=258 Nte=64 C=19 stations=13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR] fold 01 tr=231 va=27 | elapsed=0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR] fold 02 tr=234 va=24 | elapsed=54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/multiprocessing/queues.py:122: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  return _ForkingPickler.loads(res)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR] fold 03 tr=232 va=26 | elapsed=104.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR] fold 04 tr=244 va=14 | elapsed=122.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR] fold 05 tr=233 va=25 | elapsed=143.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR] fold 06 tr=233 va=25 | elapsed=163.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR] fold 07 tr=236 va=22 | elapsed=183.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR] fold 08 tr=247 va=11 | elapsed=203.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR] fold 09 tr=243 va=15 | elapsed=223.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR] fold 10 tr=243 va=15 | elapsed=243.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR] fold 11 tr=238 va=20 | elapsed=263.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR] fold 12 tr=234 va=24 | elapsed=284.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR] fold 13 tr=248 va=10 | elapsed=304.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled macro AUC (LR on PANNs): 0.6780\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_lr_panns.csv and overwrote submission.csv | rows= 1216\n    Id  Probability\n0  100     0.017779\n1  101     0.004862\n2  102     0.001935\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}