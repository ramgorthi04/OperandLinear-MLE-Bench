{
  "cells": [
    {
      "id": "fe6c58df-1061-430e-af6e-8217f89bea71",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Force pure eager mode (disable Torch compile/Inductor/Triton) BEFORE importing torch/transformers\n",
        "%env TORCHDYNAMO_DISABLE=1\n",
        "%env TORCH_COMPILE_DISABLE=1\n",
        "%env TORCHINDUCTOR_DISABLE=1\n",
        "%env TRITON_DISABLE=1\n",
        "%env XFORMERS_FORCE_DISABLE_TRITON=1\n",
        "%env TOKENIZERS_PARALLELISM=false\n",
        "\n",
        "import os\n",
        "os.environ['TORCHDYNAMO_DISABLE'] = '1'\n",
        "os.environ['TORCH_COMPILE_DISABLE'] = '1'\n",
        "os.environ['TORCHINDUCTOR_DISABLE'] = '1'\n",
        "os.environ['TRITON_DISABLE'] = '1'\n",
        "os.environ['XFORMERS_FORCE_DISABLE_TRITON'] = '1'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "\n",
        "import torch\n",
        "try:\n",
        "    import torch._dynamo as dynamo\n",
        "    dynamo.config.suppress_errors = True\n",
        "    torch._dynamo.reset()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Prefer plain math attention (avoid Triton-backed flash/sdpa kernels)\n",
        "try:\n",
        "    from torch.backends.cuda import sdp_kernel\n",
        "    sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print('Eager mode enforced. Ready to import transformers/accelerate safely.')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TORCHDYNAMO_DISABLE=1\nenv: TORCH_COMPILE_DISABLE=1\nenv: TORCHINDUCTOR_DISABLE=1\nenv: TRITON_DISABLE=1\nenv: XFORMERS_FORCE_DISABLE_TRITON=1\nenv: TOKENIZERS_PARALLELISM=false\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eager mode enforced. Ready to import transformers/accelerate safely.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n  self.gen = func(*args, **kwds)\n"
          ]
        }
      ]
    },
    {
      "id": "359a1cfe-f867-4803-bf28-58301c7f8b46",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan: AES 2.0 Medal Strategy\n",
        "\n",
        "Objectives:\n",
        "- Establish fast, reliable CV and a working baseline ASAP.\n",
        "- Leverage GPU for transformer fine-tuning; cache features and OOF.\n",
        "- Optimize QWK via post-processing (threshold search) and robust CV.\n",
        "\n",
        "Milestone 1: Environment & Data sanity\n",
        "- Verify GPU (nvidia-smi).\n",
        "- Load train/test, inspect schema, target distribution, text fields, lengths.\n",
        "- Decide CV: stratified KFold by score and prompt_id (if present), group-aware if necessary.\n",
        "\n",
        "Milestone 2: Baseline (fast)\n",
        "- TF-IDF (char+word) + Ridge/LinearSVR/XGBoost (GPU) regression.\n",
        "- Round-to-integers and optimized thresholds for QWK.\n",
        "- Save OOF, test preds; establish CV ~0.80+ quickly.\n",
        "\n",
        "Milestone 3: Transformer models\n",
        "- Install torch cu121 stack + Transformers.\n",
        "- Fine-tune DeBERTa-v3-base or RoBERTa-large (sequence regression).\n",
        "- Use max_length ~1024 with Longformer/DeBERTa-v3-large if feasible; else chunking + mean/max pooling.\n",
        "- CV with 5 folds, early stopping; log time per fold.\n",
        "- Optimize prediction-to-label mapping (isotonic or threshold search).\n",
        "\n",
        "Milestone 4: Ensembling\n",
        "- Blend TF-IDF model with transformer OOF (weighted).\n",
        "- Try multiple seeds/models; weight by OOF.\n",
        "\n",
        "Milestone 5: Error Analysis & Refinements\n",
        "- Bucket by prompt/length/score; address calibration.\n",
        "- Feature augments: readability, basic counts, prompt_id embeddings.\n",
        "\n",
        "Validation Discipline:\n",
        "- Single, deterministic folds saved to disk and reused.\n",
        "- All preprocessors fit inside folds.\n",
        "- Multiple seeds; track \u0394OOF per change.\n",
        "\n",
        "Next: Run GPU check and basic EDA, then request expert review of plan and CV setup."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "12cbd740-eb5e-499b-b76b-99782e85e3e3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, time, subprocess, textwrap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def run(cmd):\n",
        "    print(f\"$ {' '.join(cmd)}\", flush=True)\n",
        "    try:\n",
        "        out = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, check=False, text=True)\n",
        "        print(out.stdout)\n",
        "    except Exception as e:\n",
        "        print(f\"Command failed: {e}\")\n",
        "\n",
        "print('=== Environment: GPU check (nvidia-smi) ===', flush=True)\n",
        "run(['bash','-lc','nvidia-smi || true'])\n",
        "\n",
        "t0 = time.time()\n",
        "print('=== Loading data ===', flush=True)\n",
        "train_path = 'train.csv'\n",
        "test_path = 'test.csv'\n",
        "train = pd.read_csv(train_path)\n",
        "test = pd.read_csv(test_path)\n",
        "print(f\"train shape: {train.shape}\")\n",
        "print(f\"test shape: {test.shape}\")\n",
        "print('train columns:', list(train.columns))\n",
        "print('test columns:', list(test.columns))\n",
        "\n",
        "# Identify key columns (hardcode text_col fix)\n",
        "target_col = 'score' if 'score' in train.columns else None\n",
        "id_cols = [c for c in train.columns if 'id' in c.lower()]\n",
        "text_col = 'full_text' if 'full_text' in train.columns else None\n",
        "prompt_candidates = [c for c in train.columns if 'prompt' in c.lower()]\n",
        "prompt_col = prompt_candidates[0] if prompt_candidates else None\n",
        "print('Detected columns -> id:', id_cols, ' text:', text_col, ' prompt:', prompt_col, ' target:', target_col)\n",
        "\n",
        "print('\\n=== Head(train) ===')\n",
        "print(train.head(3))\n",
        "print('\\n=== Head(test) ===')\n",
        "print(test.head(3))\n",
        "\n",
        "if target_col:\n",
        "    print('\\nTarget stats:')\n",
        "    print(train[target_col].describe())\n",
        "    vc = train[target_col].value_counts().sort_index()\n",
        "    print('value_counts:', vc.to_dict())\n",
        "\n",
        "if text_col:\n",
        "    print('\\nText length stats (chars) on train:')\n",
        "    lens = train[text_col].astype(str).str.len()\n",
        "    print(lens.describe())\n",
        "    print('Word count stats on train:')\n",
        "    wcnt = train[text_col].astype(str).str.split().map(len)\n",
        "    print(wcnt.describe())\n",
        "    print('Test text length stats (chars):')\n",
        "    lens_te = test[text_col].astype(str).str.len()\n",
        "    print(lens_te.describe())\n",
        "    print('Test word count stats:')\n",
        "    wcnt_te = test[text_col].astype(str).str.split().map(len)\n",
        "    print(wcnt_te.describe())\n",
        "\n",
        "elapsed = time.time() - t0\n",
        "print(f'=== EDA setup done in {elapsed:.2f}s ===', flush=True)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Environment: GPU check (nvidia-smi) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$ bash -lc nvidia-smi || true\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to initialize NVML: Unknown Error\n\n=== Loading data ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train shape: (15576, 3)\ntest shape: (1731, 2)\ntrain columns: ['essay_id', 'full_text', 'score']\ntest columns: ['essay_id', 'full_text']\nDetected columns -> id: ['essay_id']  text: full_text  prompt: None  target: score\n\n=== Head(train) ===\n  essay_id                                          full_text  score\n0  663d2cf  Dear State Senator,\\n\\nI am arguing in favor o...      3\n1  3a20bfb  In \" The Challenge of Exploring Venus\" The aut...      2\n2  6adae64  Teachers can have a hard time telling if their...      3\n\n=== Head(test) ===\n  essay_id                                          full_text\n0  d550b2d  The face was not created by aliens because the...\n1  0c10954  Hello my name is Luke Bomberger and I was seag...\n2  ef04816  The technology to read the emotional expressio...\n\nTarget stats:\ncount    15576.000000\nmean         2.950116\nstd          1.044384\nmin          1.000000\n25%          2.000000\n50%          3.000000\n75%          4.000000\nmax          6.000000\nName: score, dtype: float64\nvalue_counts: {1: 1124, 2: 4249, 3: 5629, 4: 3563, 5: 876, 6: 135}\n\nText length stats (chars) on train:\ncount    15576.000000\nmean      2073.452748\nstd        930.115826\nmin        712.000000\n25%       1396.000000\n50%       1925.000000\n75%       2547.000000\nmax      20459.000000\nName: full_text, dtype: float64\nWord count stats on train:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    15576.000000\nmean       368.705252\nstd        150.747833\nmin        150.000000\n25%        253.000000\n50%        345.000000\n75%        453.000000\nmax       1656.000000\nName: full_text, dtype: float64\nTest text length stats (chars):\ncount     1731.000000\nmean      2055.101098\nstd        887.263737\nmin        800.000000\n25%       1402.500000\n50%       1912.000000\n75%       2497.500000\nmax      10309.000000\nName: full_text, dtype: float64\nTest word count stats:\ncount    1731.000000\nmean      365.135760\nstd       147.183553\nmin       152.000000\n25%       255.000000\n50%       340.000000\n75%       443.500000\nmax      1367.000000\nName: full_text, dtype: float64\n=== EDA setup done in 0.68s ===\n"
          ]
        }
      ]
    },
    {
      "id": "2ae03f99-5a4c-4632-a436-20280ad74533",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time, math, json, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from scipy import sparse\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "def qwk(y_true, y_pred_int):\n",
        "    return cohen_kappa_score(y_true, y_pred_int, weights='quadratic')\n",
        "\n",
        "class ThresholdOptimizer:\n",
        "    def __init__(self, init_thresholds=None):\n",
        "        self.thresholds = np.array(init_thresholds if init_thresholds is not None else [1.5,2.5,3.5,4.5,5.5], dtype=float)\n",
        "\n",
        "    def _apply(self, preds):\n",
        "        th = self.thresholds\n",
        "        return np.digitize(preds, th) + 1  # maps to 1..6\n",
        "\n",
        "    def score(self, y_true, preds):\n",
        "        return qwk(y_true, self._apply(preds))\n",
        "\n",
        "    def fit(self, y_true, preds, iters=200, step=0.02):\n",
        "        best = self.thresholds.copy()\n",
        "        best_score = self.score(y_true, preds)\n",
        "        for it in range(iters):\n",
        "            improved = False\n",
        "            for i in range(len(best)):\n",
        "                for delta in (-step, step):\n",
        "                    cand = best.copy()\n",
        "                    cand[i] += delta\n",
        "                    cand = np.sort(cand)\n",
        "                    # Enforce bounds\n",
        "                    if not (0.5 < cand[0] < cand[1] < cand[2] < cand[3] < cand[4] < 6.5):\n",
        "                        continue\n",
        "                    s = qwk(y_true, np.digitize(preds, cand) + 1)\n",
        "                    if s > best_score:\n",
        "                        best_score, best = s, cand\n",
        "                        improved = True\n",
        "            if not improved:\n",
        "                step *= 0.5\n",
        "                if step < 1e-4:\n",
        "                    break\n",
        "        self.thresholds = best\n",
        "        return best, best_score\n",
        "\n",
        "print('=== Building CV folds and TF-IDF baseline ===', flush=True)\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "text_col = 'full_text'\n",
        "target_col = 'score'\n",
        "\n",
        "# Length bins to stabilize stratification\n",
        "len_bins = pd.qcut(train[text_col].astype(str).str.len(), q=5, duplicates='drop', labels=False)\n",
        "strat_labels = train[target_col].astype(int).astype(str) + '_' + len_bins.astype(int).astype(str)\n",
        "\n",
        "n_folds = 5\n",
        "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "folds = np.full(len(train), -1, dtype=int)\n",
        "for fold, (_, val_idx) in enumerate(skf.split(train, strat_labels)):\n",
        "    folds[val_idx] = fold\n",
        "pd.DataFrame({'essay_id': train['essay_id'], 'fold': folds}).to_csv('folds.csv', index=False)\n",
        "print('Saved folds.csv')\n",
        "\n",
        "# Placeholders\n",
        "oof = np.zeros(len(train), dtype=float)\n",
        "test_preds_folds = []\n",
        "\n",
        "t_start = time.time()\n",
        "for fold in range(n_folds):\n",
        "    f0 = time.time()\n",
        "    tr_idx = np.where(folds != fold)[0]\n",
        "    va_idx = np.where(folds == fold)[0]\n",
        "    X_tr_text = train.loc[tr_idx, text_col].astype(str)\n",
        "    X_va_text = train.loc[va_idx, text_col].astype(str)\n",
        "    y_tr = train.loc[tr_idx, target_col].values.astype(float)\n",
        "\n",
        "    # Vectorizers fit INSIDE fold\n",
        "    word_tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=3, sublinear_tf=True, max_features=80000)\n",
        "    char_tfidf = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,5), min_df=3, sublinear_tf=True, max_features=120000)\n",
        "\n",
        "    Xw_tr = word_tfidf.fit_transform(X_tr_text)\n",
        "    Xc_tr = char_tfidf.fit_transform(X_tr_text)\n",
        "    X_tr = sparse.hstack([Xw_tr, Xc_tr]).tocsr()\n",
        "\n",
        "    Xw_va = word_tfidf.transform(X_va_text)\n",
        "    Xc_va = char_tfidf.transform(X_va_text)\n",
        "    X_va = sparse.hstack([Xw_va, Xc_va]).tocsr()\n",
        "\n",
        "    # Model\n",
        "    model = Ridge(alpha=4.0, random_state=42)\n",
        "    model.fit(X_tr, y_tr)\n",
        "    oof[va_idx] = model.predict(X_va)\n",
        "\n",
        "    # Test transform and preds for this fold\n",
        "    Xw_te = word_tfidf.transform(test[text_col].astype(str))\n",
        "    Xc_te = char_tfidf.transform(test[text_col].astype(str))\n",
        "    X_te = sparse.hstack([Xw_te, Xc_te]).tocsr()\n",
        "    te_pred = model.predict(X_te)\n",
        "    test_preds_folds.append(te_pred.astype(float))\n",
        "\n",
        "    # Fold logging\n",
        "    # Quick rounded QWK for sanity per fold\n",
        "    va_true = train.loc[va_idx, target_col].values.astype(int)\n",
        "    va_round = np.clip(np.rint(oof[va_idx]), 1, 6).astype(int)\n",
        "    fold_qwk_round = qwk(va_true, va_round)\n",
        "    print(f'Fold {fold}: n_tr={len(tr_idx)} n_va={len(va_idx)} round-QWK={fold_qwk_round:.4f} elapsed={time.time()-f0:.1f}s', flush=True)\n",
        "\n",
        "elapsed = time.time() - t_start\n",
        "print(f'All folds done in {elapsed/60:.1f} min', flush=True)\n",
        "\n",
        "# Threshold optimization on global OOF\n",
        "oof_clipped = np.clip(oof, 0.5, 6.5)\n",
        "opt = ThresholdOptimizer()\n",
        "init_th = [1.5,2.5,3.5,4.5,5.5]\n",
        "best_th, best_oof_qwk = opt.fit(train[target_col].values.astype(int), oof_clipped, iters=200, step=0.05)\n",
        "oof_labels = opt._apply(oof_clipped)\n",
        "round_qwk = qwk(train[target_col].values.astype(int), np.clip(np.rint(oof),1,6).astype(int))\n",
        "print(f'OOF round-QWK={round_qwk:.5f}  OOF thresh-QWK={best_oof_qwk:.5f}  thresholds={best_th}', flush=True)\n",
        "\n",
        "# Blend test predictions across folds (mean), then apply thresholds\n",
        "test_pred_mean = np.mean(np.vstack(test_preds_folds), axis=0)\n",
        "test_pred_mean = np.clip(test_pred_mean, 0.5, 6.5)\n",
        "test_labels = np.digitize(test_pred_mean, best_th) + 1\n",
        "test_labels = np.clip(test_labels, 1, 6).astype(int)\n",
        "\n",
        "# Save artifacts\n",
        "np.save('oof_tfidf.npy', oof)\n",
        "np.save('test_tfidf.npy', test_pred_mean)\n",
        "with open('thresholds_tfidf.json','w') as f:\n",
        "    json.dump({'thresholds': best_th.tolist(), 'oof_qwk': float(best_oof_qwk), 'round_oof_qwk': float(round_qwk)}, f)\n",
        "\n",
        "sub = pd.DataFrame({'essay_id': test['essay_id'], 'score': test_labels})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv, oof_tfidf.npy, test_tfidf.npy, thresholds_tfidf.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "3835df5a-3ec4-4d68-9634-e342da17cd32",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, subprocess, shutil, time\n",
        "from pathlib import Path\n",
        "\n",
        "def pip(*args):\n",
        "    print(\">\", *args, flush=True)\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", *args], check=True)\n",
        "\n",
        "print(\"=== Install PyTorch cu121 + NLP stack ===\", flush=True)\n",
        "# Uninstall any stray stacks (best-effort)\n",
        "for pkg in (\"torch\",\"torchvision\",\"torchaudio\"):\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", pkg], check=False)\n",
        "\n",
        "for d in (\n",
        "    \"/app/.pip-target/torch\",\n",
        "    \"/app/.pip-target/torch-2.8.0.dist-info\",\n",
        "    \"/app/.pip-target/torch-2.4.1.dist-info\",\n",
        "    \"/app/.pip-target/torchvision\",\n",
        "    \"/app/.pip-target/torchvision-0.23.0.dist-info\",\n",
        "    \"/app/.pip-target/torchvision-0.19.1.dist-info\",\n",
        "    \"/app/.pip-target/torchaudio\",\n",
        "    \"/app/.pip-target/torchaudio-2.8.0.dist-info\",\n",
        "    \"/app/.pip-target/torchaudio-2.4.1.dist-info\",\n",
        "    \"/app/.pip-target/torchgen\",\n",
        "    \"/app/.pip-target/functorch\",\n",
        "):\n",
        "    if os.path.exists(d):\n",
        "        print(\"Removing\", d)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "# 1) Install exact cu121 stack\n",
        "pip(\"install\",\n",
        "    \"--index-url\", \"https://download.pytorch.org/whl/cu121\",\n",
        "    \"--extra-index-url\", \"https://pypi.org/simple\",\n",
        "    \"torch==2.4.1\", \"torchvision==0.19.1\", \"torchaudio==2.4.1\")\n",
        "\n",
        "# 2) Freeze torch versions\n",
        "Path(\"constraints.txt\").write_text(\"torch==2.4.1\\ntorchvision==0.19.1\\ntorchaudio==2.4.1\\n\")\n",
        "\n",
        "# 3) Install Transformers stack honoring constraints\n",
        "pip(\"install\", \"-c\", \"constraints.txt\",\n",
        "    \"transformers==4.44.2\", \"accelerate==0.34.2\",\n",
        "    \"datasets==2.21.0\", \"evaluate==0.4.2\",\n",
        "    \"sentencepiece\", \"scikit-learn\", \"torchmetrics\",\n",
        "    \"--upgrade-strategy\", \"only-if-needed\")\n",
        "\n",
        "import torch\n",
        "print(\"torch:\", torch.__version__, \"built CUDA:\", getattr(torch.version, \"cuda\", None))\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "assert str(getattr(torch.version, \"cuda\", \"\")).startswith(\"12.1\"), f\"Wrong CUDA build: {torch.version.cuda}\"\n",
        "assert torch.cuda.is_available(), \"CUDA not available\"\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "print(\"=== Install & GPU sanity OK ===\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "c6e7f3b2-ece1-455e-9ed9-363e6be55c22",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "\n",
        "print('=== Building prompt clusters and grouped folds ===', flush=True)\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "text_col = 'full_text'\n",
        "target_col = 'score'\n",
        "\n",
        "# TF-IDF -> SVD on TRAIN only\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5, max_features=100000, sublinear_tf=True)\n",
        "X = tfidf.fit_transform(train[text_col].astype(str))\n",
        "print(f'TFIDF shape: {X.shape}', flush=True)\n",
        "\n",
        "svd = TruncatedSVD(n_components=50, random_state=42)\n",
        "X_svd = svd.fit_transform(X)\n",
        "print('SVD done.', flush=True)\n",
        "\n",
        "k = 12\n",
        "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "clusters = kmeans.fit_predict(X_svd)\n",
        "print('KMeans done.', flush=True)\n",
        "\n",
        "train['cluster'] = clusters.astype(int)\n",
        "\n",
        "# StratifiedGroupKFold: stratify by score, group by cluster\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds_g = np.full(len(train), -1, dtype=int)\n",
        "for fold, (_, va_idx) in enumerate(sgkf.split(X_svd, train[target_col].astype(int), groups=train['cluster'])):\n",
        "    folds_g[va_idx] = fold\n",
        "\n",
        "assert (folds_g >= 0).all(), 'Some rows not assigned a fold'\n",
        "fold_df = pd.DataFrame({'essay_id': train['essay_id'], 'fold_grouped': folds_g, 'cluster': train['cluster']})\n",
        "fold_df.to_csv('folds_grouped.csv', index=False)\n",
        "print('Saved folds_grouped.csv with grouped folds and clusters')\n",
        "print('Cluster distribution:', train['cluster'].value_counts().sort_index().to_dict())\n",
        "print('Fold sizes:', pd.Series(folds_g).value_counts().sort_index().to_dict())\n",
        "print(f'=== Done in {(time.time()-t0):.1f}s ===', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "f55a6118-1850-4264-9eae-1a0a7faf5d20",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, time, json, math, random, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, Trainer, TrainingArguments\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision('high')\n",
        "SEED = 42\n",
        "def seed_everything(seed=SEED):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "seed_everything()\n",
        "\n",
        "def qwk(y_true, y_pred_int):\n",
        "    return cohen_kappa_score(y_true, y_pred_int, weights='quadratic')\n",
        "\n",
        "class ThresholdOptimizer:\n",
        "    def __init__(self, init_thresholds=None):\n",
        "        self.thresholds = np.array(init_thresholds if init_thresholds is not None else [1.5,2.5,3.5,4.5,5.5], dtype=float)\n",
        "    def _apply(self, preds):\n",
        "        return np.digitize(preds, self.thresholds) + 1\n",
        "    def fit(self, y_true, preds, iters=200, step=0.05):\n",
        "        best = self.thresholds.copy(); best_score = qwk(y_true, self._apply(preds))\n",
        "        for _ in range(iters):\n",
        "            improved = False\n",
        "            for i in range(5):\n",
        "                for d in (-step, step):\n",
        "                    cand = np.sort(np.clip(best + (np.arange(5)==i)*d, 0.5, 6.5))\n",
        "                    if not (0.5 < cand[0] < cand[1] < cand[2] < cand[3] < cand[4] < 6.5):\n",
        "                        continue\n",
        "                    s = qwk(y_true, np.digitize(preds, cand) + 1)\n",
        "                    if s > best_score:\n",
        "                        best_score, best, improved = s, cand, True\n",
        "            if not improved:\n",
        "                step *= 0.5\n",
        "                if step < 1e-4: break\n",
        "        self.thresholds = best\n",
        "        return best, best_score\n",
        "\n",
        "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
        "MAX_LEN = 1024\n",
        "HEAD_FRAC = 0.88  # dynamic head emphasis\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "def encode_head_tail(text):\n",
        "    ids = tokenizer(text, add_special_tokens=False)['input_ids']\n",
        "    keep_total = MAX_LEN - 3  # [CLS], mid [SEP], last [SEP]\n",
        "    if len(ids) <= MAX_LEN - 2:\n",
        "        out = [tokenizer.cls_token_id] + ids + [tokenizer.sep_token_id]\n",
        "    else:\n",
        "        keep_head = int(HEAD_FRAC * keep_total)\n",
        "        keep_tail = keep_total - keep_head\n",
        "        head = ids[:keep_head]\n",
        "        tail = ids[-keep_tail:] if keep_tail > 0 else []\n",
        "        out = [tokenizer.cls_token_id] + head + [tokenizer.sep_token_id] + tail + [tokenizer.sep_token_id]\n",
        "    attn = [1]*len(out)\n",
        "    return {'input_ids': out, 'attention_mask': attn}\n",
        "\n",
        "class EssayDataset(Dataset):\n",
        "    def __init__(self, df, text_col='full_text', targets=None):\n",
        "        self.texts = df[text_col].astype(str).tolist()\n",
        "        self.targets = None if targets is None else targets.astype(np.float32)\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        enc = encode_head_tail(self.texts[idx])\n",
        "        item = {k: torch.tensor(v, dtype=torch.long) for k, v in enc.items()}\n",
        "        if self.targets is not None:\n",
        "            item['labels'] = torch.tensor(self.targets[idx], dtype=torch.float32)  # shape [ ] scalar\n",
        "        return item\n",
        "\n",
        "class PadCollator:\n",
        "    def __init__(self):\n",
        "        self.pad = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n",
        "    def __call__(self, features):\n",
        "        labels = None\n",
        "        if 'labels' in features[0]:\n",
        "            labels = torch.stack([f['labels'] for f in features]).view(-1)  # shape [B]\n",
        "            for f in features: f.pop('labels')\n",
        "        batch = self.pad(features)\n",
        "        if labels is not None: batch['labels'] = labels\n",
        "        return batch\n",
        "\n",
        "class SmoothL1Trainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop('labels')\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits.squeeze(-1)\n",
        "        loss = torch.nn.functional.smooth_l1_loss(logits, labels, beta=1.0, reduction='mean')\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "def train_fold(fold, df, folds, out_dir='deberta_base_1024'):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    tr_idx = np.where(folds != fold)[0]\n",
        "    va_idx = np.where(folds == fold)[0]\n",
        "    dtrain = EssayDataset(df.iloc[tr_idx], targets=df.iloc[tr_idx]['score'].values.astype(np.float32))\n",
        "    dvalid = EssayDataset(df.iloc[va_idx], targets=df.iloc[va_idx]['score'].values.astype(np.float32))\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=1, problem_type='regression')\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"{out_dir}/fold{fold}\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        fp16=True,\n",
        "        evaluation_strategy='steps',\n",
        "        save_strategy='steps',\n",
        "        eval_steps=800,\n",
        "        save_steps=800,\n",
        "        save_total_limit=1,\n",
        "        logging_strategy='steps',\n",
        "        logging_steps=200,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='qwk_round',\n",
        "        greater_is_better=True,\n",
        "        report_to=[],\n",
        "        dataloader_num_workers=6,\n",
        "        dataloader_pin_memory=True,\n",
        "        dataloader_persistent_workers=True,\n",
        "        gradient_accumulation_steps=4,\n",
        "        lr_scheduler_type='cosine',\n",
        "        warmup_ratio=0.1,\n",
        "        optim='adamw_torch_fused',\n",
        "        eval_accumulation_steps=32,\n",
        "        seed=SEED\n",
        "    )\n",
        "    def compute_metrics(eval_pred):\n",
        "        preds = eval_pred.predictions.squeeze()\n",
        "        labels = eval_pred.label_ids.squeeze()\n",
        "        preds_clip = np.clip(preds, 0.5, 6.5)\n",
        "        q = qwk(labels.astype(int), np.clip(np.rint(preds_clip),1,6).astype(int))\n",
        "        return {'qwk_round': q}\n",
        "    trainer = SmoothL1Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=dtrain,\n",
        "        eval_dataset=dvalid,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=PadCollator(),\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "    t0 = time.time()\n",
        "    trainer.train()\n",
        "    print(f\"Fold {fold} train done in {(time.time()-t0)/60:.1f} min\", flush=True)\n",
        "    preds_val = trainer.predict(dvalid).predictions.squeeze()\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "    return va_idx, preds_val\n",
        "\n",
        "print('=== DeBERTa-v3-base 1024 head+tail 5-fold training (grouped folds) ===', flush=True)\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "folds_g = pd.read_csv('folds_grouped.csv')\n",
        "folds_map = dict(zip(folds_g['essay_id'], folds_g['fold_grouped']))\n",
        "folds = train_df['essay_id'].map(folds_map).values.astype(int)\n",
        "\n",
        "oof = np.zeros(len(train_df), dtype=float)\n",
        "for f in sorted(np.unique(folds)):\n",
        "    f_start = time.time()\n",
        "    va_idx, preds_val = train_fold(f, train_df, folds)\n",
        "    oof[va_idx] = preds_val\n",
        "    y_true = train_df.iloc[va_idx]['score'].values.astype(int)\n",
        "    fold_qwk_round = qwk(y_true, np.clip(np.rint(np.clip(preds_val,0.5,6.5)),1,6).astype(int))\n",
        "    print(f\"Fold {f} val round-QWK={fold_qwk_round:.4f} elapsed={(time.time()-f_start)/60:.1f} min\", flush=True)\n",
        "\n",
        "np.save('oof_deberta_base_1024.npy', oof)\n",
        "opt = ThresholdOptimizer()\n",
        "best_th, best_oof_qwk = opt.fit(train_df['score'].values.astype(int), np.clip(oof,0.5,6.5))\n",
        "round_q = qwk(train_df['score'].values.astype(int), np.clip(np.rint(oof),1,6).astype(int))\n",
        "print(f'OOF round-QWK={round_q:.5f}  OOF thresh-QWK={best_oof_qwk:.5f}  thresholds={best_th}', flush=True)\n",
        "with open('thresholds_deberta_base_1024.json','w') as f:\n",
        "    json.dump({'thresholds': best_th.tolist(), 'oof_qwk': float(best_oof_qwk), 'round_oof_qwk': float(round_q)}, f)\n",
        "\n",
        "print('=== Note === Next: add test-time inference and seeds; if OOF <0.835, pivot to 512 sliding windows. ===', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "c990e3f6-30d6-480c-8b44-5bb4b616242e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, time, json, math, random, gc\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModel,\n",
        "    DataCollatorWithPadding, Trainer, TrainingArguments\n",
        ")\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "print('=== DeBERTa-v3-base sliding windows + mean pooling (512/384) ===', flush=True)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision('high')\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "\n",
        "SEED = 42\n",
        "def seed_everything(seed=SEED):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "seed_everything()\n",
        "\n",
        "def qwk(y_true, y_pred_int):\n",
        "    return cohen_kappa_score(y_true, y_pred_int, weights='quadratic')\n",
        "\n",
        "class ThresholdOptimizer:\n",
        "    def __init__(self, init_thresholds=None):\n",
        "        self.thresholds = np.array(init_thresholds if init_thresholds is not None else [1.5,2.5,3.5,4.5,5.5], dtype=float)\n",
        "    def _apply(self, preds):\n",
        "        return np.digitize(preds, self.thresholds) + 1\n",
        "    def fit(self, y_true, preds, iters=200, step=0.05):\n",
        "        best = self.thresholds.copy(); best_score = qwk(y_true, self._apply(preds))\n",
        "        for _ in range(iters):\n",
        "            improved = False\n",
        "            for i in range(5):\n",
        "                for d in (-step, step):\n",
        "                    cand = np.sort(np.clip(best + (np.arange(5)==i)*d, 0.5, 6.5))\n",
        "                    if not (0.5 < cand[0] < cand[1] < cand[2] < cand[3] < cand[4] < 6.5):\n",
        "                        continue\n",
        "                    s = qwk(y_true, np.digitize(preds, cand) + 1)\n",
        "                    if s > best_score:\n",
        "                        best_score, best, improved = s, cand, True\n",
        "            if not improved:\n",
        "                step *= 0.5\n",
        "                if step < 1e-4: break\n",
        "        self.thresholds = best\n",
        "        return best, best_score\n",
        "\n",
        "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
        "MAX_LEN = 512\n",
        "STRIDE = 128  # overlap tokens between chunks\n",
        "BATCH_TRAIN = 8\n",
        "BATCH_EVAL = 32\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "class WindowDataset(Dataset):\n",
        "    def __init__(self, df, text_col='full_text', labels=None):\n",
        "        self.essay_ids = []\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        self.labels = [] if labels is not None else None\n",
        "        texts = df[text_col].astype(str).tolist()\n",
        "        eids = df['essay_id'].tolist()\n",
        "        lbls = None if labels is None else labels.astype(np.float32).tolist()\n",
        "        enc = tokenizer(texts,\n",
        "                        max_length=MAX_LEN,\n",
        "                        truncation=True,\n",
        "                        padding=False,\n",
        "                        return_overflowing_tokens=True,\n",
        "                        stride=STRIDE,\n",
        "                        return_attention_mask=True)\n",
        "        overflow_to_sample = enc.pop('overflow_to_sample_mapping')\n",
        "        for idx, sample_idx in enumerate(overflow_to_sample):\n",
        "            self.essay_ids.append(eids[sample_idx])\n",
        "            self.input_ids.append(enc['input_ids'][idx])\n",
        "            self.attn_masks.append(enc['attention_mask'][idx])\n",
        "            if lbls is not None:\n",
        "                self.labels.append(lbls[sample_idx])\n",
        "        if self.labels is not None:\n",
        "            self.labels = np.array(self.labels, dtype=np.float32)\n",
        "        # Keep as lists; collator will pad\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "    def __getitem__(self, i):\n",
        "        item = {\n",
        "            'input_ids': torch.tensor(self.input_ids[i], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.attn_masks[i], dtype=torch.long),\n",
        "        }\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(self.labels[i], dtype=torch.float32)\n",
        "        item['essay_id'] = self.essay_ids[i]\n",
        "        return item\n",
        "\n",
        "class PadCollator:\n",
        "    def __init__(self):\n",
        "        self.pad = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n",
        "    def __call__(self, features):\n",
        "        # Ensure any extra keys like essay_id are removed before padding\n",
        "        for f in features:\n",
        "            if 'essay_id' in f:\n",
        "                f.pop('essay_id')\n",
        "        labels = None\n",
        "        if 'labels' in features[0]:\n",
        "            labels = torch.stack([f['labels'] for f in features]).view(-1)\n",
        "            for f in features: f.pop('labels')\n",
        "        batch = self.pad(features)\n",
        "        if labels is not None:\n",
        "            batch['labels'] = labels\n",
        "        return batch\n",
        "\n",
        "class MeanPoolRegressor(nn.Module):\n",
        "    def __init__(self, model_name, hidden_size=768):\n",
        "        super().__init__()\n",
        "        self.backbone = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.head = nn.Linear(hidden_size, 1)\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
        "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last = out.last_hidden_state  # [B, T, H]\n",
        "        mask = attention_mask.unsqueeze(-1).float()  # [B, T, 1]\n",
        "        masked = last * mask\n",
        "        denom = mask.sum(dim=1).clamp(min=1e-6)\n",
        "        mean = masked.sum(dim=1) / denom  # [B, H]\n",
        "        mean = self.dropout(mean)\n",
        "        logits = self.head(mean).squeeze(-1)  # [B]\n",
        "        if labels is not None:\n",
        "            loss = torch.nn.functional.smooth_l1_loss(logits, labels, beta=1.0, reduction='mean')\n",
        "            return {'loss': loss, 'logits': logits.unsqueeze(-1)}\n",
        "        return {'logits': logits.unsqueeze(-1)}\n",
        "\n",
        "# Global var to let compute_metrics know current eval essay_ids order\n",
        "EVAL_ESSAY_IDS = None\n",
        "\n",
        "def make_compute_metrics():\n",
        "    def compute(eval_pred):\n",
        "        preds = eval_pred.predictions.squeeze()  # per-window\n",
        "        labels = eval_pred.label_ids.squeeze()   # per-window\n",
        "        # Aggregate by essay\n",
        "        ids = np.array(EVAL_ESSAY_IDS)\n",
        "        by_id = defaultdict(list)\n",
        "        by_id_true = {}\n",
        "        for p, y, i in zip(preds, labels, ids):\n",
        "            by_id[i].append(float(p))\n",
        "            by_id_true[i] = int(y)\n",
        "        agg_preds = []\n",
        "        agg_true = []\n",
        "        for i, vals in by_id.items():\n",
        "            agg_preds.append(np.mean(vals))\n",
        "            agg_true.append(by_id_true[i])\n",
        "        agg_preds = np.clip(np.array(agg_preds), 0.5, 6.5)\n",
        "        agg_labels = np.array(agg_true, dtype=int)\n",
        "        q = qwk(agg_labels, np.clip(np.rint(agg_preds), 1, 6).astype(int))\n",
        "        return {'qwk_round': q}\n",
        "    return compute\n",
        "\n",
        "def train_fold_windows(fold, df, folds, out_dir='deberta_v3_base_win512'):\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        tr_idx = np.where(folds != fold)[0]\n",
        "        va_idx = np.where(folds == fold)[0]\n",
        "        dtrain = WindowDataset(df.iloc[tr_idx], labels=df.iloc[tr_idx]['score'].values.astype(np.float32))\n",
        "        dvalid = WindowDataset(df.iloc[va_idx], labels=df.iloc[va_idx]['score'].values.astype(np.float32))\n",
        "        model = MeanPoolRegressor(MODEL_NAME, hidden_size=768)\n",
        "        args = TrainingArguments(\n",
        "            output_dir=f\"{out_dir}/fold{fold}\",\n",
        "            learning_rate=2e-5,\n",
        "            per_device_train_batch_size=BATCH_TRAIN,\n",
        "            per_device_eval_batch_size=BATCH_EVAL,\n",
        "            num_train_epochs=3,\n",
        "            weight_decay=0.01,\n",
        "            bf16=True,\n",
        "            evaluation_strategy='steps',\n",
        "            save_strategy='steps',\n",
        "            eval_steps=1000,\n",
        "            save_steps=1000,\n",
        "            save_total_limit=1,\n",
        "            logging_strategy='steps',\n",
        "            logging_steps=200,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model='qwk_round',\n",
        "            greater_is_better=True,\n",
        "            report_to=[],\n",
        "            dataloader_num_workers=6,\n",
        "            dataloader_pin_memory=True,\n",
        "            dataloader_persistent_workers=True,\n",
        "            gradient_accumulation_steps=4,\n",
        "            lr_scheduler_type='cosine',\n",
        "            warmup_ratio=0.1,\n",
        "            optim='adamw_torch_fused',\n",
        "            eval_accumulation_steps=32,\n",
        "            seed=SEED,\n",
        "            remove_unused_columns=False\n",
        "        )\n",
        "        global EVAL_ESSAY_IDS\n",
        "        EVAL_ESSAY_IDS = dvalid.essay_ids\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=args,\n",
        "            train_dataset=dtrain,\n",
        "            eval_dataset=dvalid,\n",
        "            tokenizer=tokenizer,\n",
        "            data_collator=PadCollator(),\n",
        "            compute_metrics=make_compute_metrics()\n",
        "        )\n",
        "        t0 = time.time()\n",
        "        trainer.train()\n",
        "        print(f\"Fold {fold} train done in {(time.time()-t0)/60:.1f} min\", flush=True)\n",
        "        # Predict on valid windows and aggregate\n",
        "        preds_val = trainer.predict(dvalid).predictions.squeeze()\n",
        "        ids = np.array(dvalid.essay_ids)\n",
        "        by_id = defaultdict(list)\n",
        "        for p, i in zip(preds_val, ids):\n",
        "            by_id[i].append(float(p))\n",
        "        agg = {i: float(np.mean(v)) for i, v in by_id.items()}\n",
        "        # Map back to essay order\n",
        "        va_eids = df.iloc[va_idx]['essay_id'].values.tolist()\n",
        "        agg_vec = np.array([agg[e] for e in va_eids], dtype=float)\n",
        "        torch.cuda.empty_cache(); gc.collect()\n",
        "        return va_idx, agg_vec\n",
        "\n",
        "train_df = pd.read_csv('train.csv')\n",
        "folds_g = pd.read_csv('folds_grouped.csv')\n",
        "folds_map = dict(zip(folds_g['essay_id'], folds_g['fold_grouped']))\n",
        "folds = train_df['essay_id'].map(folds_map).values.astype(int)\n",
        "\n",
        "oof = np.zeros(len(train_df), dtype=float)\n",
        "for f in sorted(np.unique(folds)):\n",
        "    f_start = time.time()\n",
        "    va_idx, agg_preds = train_fold_windows(f, train_df, folds)\n",
        "    oof[va_idx] = agg_preds\n",
        "    y_true = train_df.iloc[va_idx]['score'].values.astype(int)\n",
        "    q = qwk(y_true, np.clip(np.rint(np.clip(agg_preds, 0.5, 6.5)), 1, 6).astype(int))\n",
        "    print(f\"Fold {f} val round-QWK={q:.4f} elapsed={(time.time()-f_start)/60:.1f} min\", flush=True)\n",
        "\n",
        "np.save('oof_deberta_v3_base_win512.npy', oof)\n",
        "opt = ThresholdOptimizer()\n",
        "best_th, best_oof_qwk = opt.fit(train_df['score'].values.astype(int), np.clip(oof, 0.5, 6.5))\n",
        "round_q = qwk(train_df['score'].values.astype(int), np.clip(np.rint(oof), 1, 6).astype(int))\n",
        "print(f'OOF round-QWK={round_q:.5f}  OOF thresh-QWK={best_oof_qwk:.5f}  thresholds={best_th}', flush=True)\n",
        "with open('thresholds_deberta_v3_base_win512.json','w') as f:\n",
        "    json.dump({'thresholds': best_th.tolist(), 'oof_qwk': float(best_oof_qwk), 'round_oof_qwk': float(round_q)}, f)\n",
        "print('=== Next: add test-time window inference + second seed; then consider v3-large ===', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "0784745f-3d74-4c13-b08d-d121c7373ac8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "\n",
        "print('=== Rebuilding grouped folds: TF-IDF -> SVD(100) -> KMeans(k=16) -> StratifiedGroupKFold ===', flush=True)\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "text_col = 'full_text'\n",
        "target_col = 'score'\n",
        "\n",
        "# Fit TF-IDF on TRAIN only\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5, max_features=120000, sublinear_tf=True)\n",
        "X = tfidf.fit_transform(train[text_col].astype(str))\n",
        "print(f'TFIDF shape: {X.shape}', flush=True)\n",
        "\n",
        "# SVD to 100 components\n",
        "svd = TruncatedSVD(n_components=100, random_state=42)\n",
        "X_svd = svd.fit_transform(X)\n",
        "print('SVD done.', flush=True)\n",
        "\n",
        "# KMeans k=16\n",
        "k = 16\n",
        "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "clusters = kmeans.fit_predict(X_svd)\n",
        "train['cluster_k16'] = clusters.astype(int)\n",
        "print('KMeans done.', flush=True)\n",
        "\n",
        "# StratifiedGroupKFold: stratify by score, group by new clusters\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds_g = np.full(len(train), -1, dtype=int)\n",
        "for fold, (_, va_idx) in enumerate(sgkf.split(X_svd, train[target_col].astype(int), groups=train['cluster_k16'])):\n",
        "    folds_g[va_idx] = fold\n",
        "assert (folds_g >= 0).all(), 'Some rows not assigned a fold'\n",
        "\n",
        "fold_df = pd.DataFrame({\n",
        "    'essay_id': train['essay_id'],\n",
        "    'fold_grouped_k16': folds_g,\n",
        "    'cluster_k16': train['cluster_k16']\n",
        "})\n",
        "fold_df.to_csv('folds_grouped_k16.csv', index=False)\n",
        "print('Saved folds_grouped_k16.csv')\n",
        "print('Cluster (k=16) distribution:', train['cluster_k16'].value_counts().sort_index().to_dict())\n",
        "print('Fold sizes:', pd.Series(folds_g).value_counts().sort_index().to_dict())\n",
        "print(f'=== Done in {(time.time()-t0):.1f}s ===', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "66fb9ccd-695b-4220-b313-93d3e625a3e6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, time, json, math, random, gc\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModel,\n",
        "    DataCollatorWithPadding, Trainer, TrainingArguments, PrinterCallback\n",
        ")\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "print('=== Seed 2 prep: DeBERTa-v3-base windows + mean pooling + Multi-Sample Dropout (512/384) ===', flush=True)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision('high')\n",
        "torch.backends.cudnn.benchmark = True\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "# Disable any implicit torch.compile/inductor paths to avoid Triton build (Python.h) issues\n",
        "os.environ['TORCH_COMPILE_DISABLE'] = '1'\n",
        "os.environ['TORCHINDUCTOR_DISABLE'] = '1'\n",
        "try:\n",
        "    import torch._dynamo as dynamo\n",
        "    dynamo.config.suppress_errors = True\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "SEED2 = 2025\n",
        "def seed_everything(seed=SEED2):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "seed_everything()\n",
        "\n",
        "def qwk(y_true, y_pred_int):\n",
        "    return cohen_kappa_score(y_true, y_pred_int, weights='quadratic')\n",
        "\n",
        "class ThresholdOptimizer:\n",
        "    def __init__(self, init_thresholds=None):\n",
        "        self.thresholds = np.array(init_thresholds if init_thresholds is not None else [1.5,2.5,3.5,4.5,5.5], dtype=float)\n",
        "    def _apply(self, preds):\n",
        "        return np.digitize(preds, self.thresholds) + 1\n",
        "    def fit(self, y_true, preds, iters=200, step=0.05):\n",
        "        best = self.thresholds.copy(); best_score = qwk(y_true, self._apply(preds))\n",
        "        for _ in range(iters):\n",
        "            improved = False; \n",
        "            for i in range(5):\n",
        "                for d in (-step, step):\n",
        "                    cand = np.sort(np.clip(best + (np.arange(5)==i)*d, 0.5, 6.5))\n",
        "                    if not (0.5 < cand[0] < cand[1] < cand[2] < cand[3] < cand[4] < 6.5):\n",
        "                        continue\n",
        "                    s = qwk(y_true, np.digitize(preds, cand) + 1)\n",
        "                    if s > best_score:\n",
        "                        best_score, best, improved = s, cand, True\n",
        "            if not improved:\n",
        "                step *= 0.5\n",
        "                if step < 1e-4: break\n",
        "        self.thresholds = best\n",
        "        return best, best_score\n",
        "\n",
        "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
        "MAX_LEN = 512\n",
        "STRIDE = 128  # overlap tokens between chunks\n",
        "BATCH_TRAIN = 4  # reduce to avoid OOM without grad ckpt\n",
        "BATCH_EVAL = 32\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "class WindowDataset(Dataset):\n",
        "    def __init__(self, df, text_col='full_text', labels=None):\n",
        "        self.essay_ids = []\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        self.lengths = []  # token counts per window\n",
        "        self.labels = [] if labels is not None else None\n",
        "        texts = df[text_col].astype(str).tolist()\n",
        "        eids = df['essay_id'].tolist()\n",
        "        lbls = None if labels is None else labels.astype(np.float32).tolist()\n",
        "        enc = tokenizer(texts,\n",
        "                        max_length=MAX_LEN,\n",
        "                        truncation=True,\n",
        "                        padding=False,\n",
        "                        return_overflowing_tokens=True,\n",
        "                        stride=STRIDE,\n",
        "                        return_attention_mask=True)\n",
        "        overflow_to_sample = enc.pop('overflow_to_sample_mapping')\n",
        "        for idx, sample_idx in enumerate(overflow_to_sample):\n",
        "            self.essay_ids.append(eids[sample_idx])\n",
        "            ids_i = enc['input_ids'][idx]\n",
        "            attn_i = enc['attention_mask'][idx]\n",
        "            self.input_ids.append(ids_i)\n",
        "            self.attn_masks.append(attn_i)\n",
        "            self.lengths.append(int(sum(attn_i)))\n",
        "            if lbls is not None:\n",
        "                self.labels.append(lbls[sample_idx])\n",
        "        if self.labels is not None:\n",
        "            self.labels = np.array(self.labels, dtype=np.float32)\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "    def __getitem__(self, i):\n",
        "        item = {\n",
        "            'input_ids': torch.tensor(self.input_ids[i], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.attn_masks[i], dtype=torch.long),\n",
        "        }\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(self.labels[i], dtype=torch.float32)\n",
        "        item['essay_id'] = self.essay_ids[i]\n",
        "        return item\n",
        "\n",
        "class PadCollator:\n",
        "    def __init__(self):\n",
        "        self.pad = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n",
        "    def __call__(self, features):\n",
        "        for f in features:\n",
        "            f.pop('essay_id', None)\n",
        "        labels = None\n",
        "        if 'labels' in features[0]:\n",
        "            labels = torch.stack([f['labels'] for f in features]).view(-1)\n",
        "            for f in features: f.pop('labels')\n",
        "        batch = self.pad(features)\n",
        "        if labels is not None:\n",
        "            batch['labels'] = labels\n",
        "        return batch\n",
        "\n",
        "class MSDMeanPoolRegressor(nn.Module):\n",
        "    def __init__(self, model_name, hidden_size=768, msd=5, p=0.2):\n",
        "        super().__init__()\n",
        "        self.backbone = AutoModel.from_pretrained(model_name)\n",
        "        self.dropouts = nn.ModuleList([nn.Dropout(p) for _ in range(msd)])\n",
        "        self.head = nn.Linear(hidden_size, 1)\n",
        "        self.msd = msd\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
        "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last = out.last_hidden_state  # [B, T, H]\n",
        "        mask = attention_mask.unsqueeze(-1).float()\n",
        "        mean = (last * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-6)\n",
        "        logits_list = []\n",
        "        for dp in self.dropouts:\n",
        "            logits_list.append(self.head(dp(mean)).squeeze(-1))\n",
        "        logits = torch.stack(logits_list, dim=0).mean(dim=0)  # [B]\n",
        "        if labels is not None:\n",
        "            loss = torch.nn.functional.smooth_l1_loss(logits, labels, beta=1.0, reduction='mean')\n",
        "            return {'loss': loss, 'logits': logits.unsqueeze(-1)}\n",
        "        return {'logits': logits.unsqueeze(-1)}\n",
        "\n",
        "def compute_metrics_factory(eval_ids):\n",
        "    # Note: eval doesn't expose per-window lengths; use simple mean for metrics.\n",
        "    def compute(eval_pred):\n",
        "        preds = eval_pred.predictions.squeeze()\n",
        "        labels = eval_pred.label_ids.squeeze()\n",
        "        ids = np.array(eval_ids)\n",
        "        by_id = defaultdict(list)\n",
        "        by_id_true = {}\n",
        "        for p, y, i in zip(preds, labels, ids):\n",
        "            by_id[i].append(float(p))\n",
        "            by_id_true[i] = int(y)\n",
        "        agg_preds = np.array([np.mean(v) for i, v in by_id.items()])\n",
        "        agg_true = np.array([by_id_true[i] for i in by_id.keys()])\n",
        "        agg_preds = np.clip(agg_preds, 0.5, 6.5)\n",
        "        q = qwk(agg_true, np.clip(np.rint(agg_preds), 1, 6).astype(int))\n",
        "        return {'qwk_round': q}\n",
        "    return compute\n",
        "\n",
        "def _log_windows_stats(name, ds):\n",
        "    n_win = len(ds)\n",
        "    uniq = len(set(ds.essay_ids))\n",
        "    avg_w = n_win / max(uniq, 1)\n",
        "    print(f'{name}: essays={uniq} windows={n_win} avg_windows_per_essay={avg_w:.2f}', flush=True)\n",
        "\n",
        "def train_fold_seed2(fold, df, folds, out_dir='deberta_v3_base_win512_seed2025'):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    tr_idx = np.where(folds != fold)[0]\n",
        "    va_idx = np.where(folds == fold)[0]\n",
        "    print(f'[Fold {fold}] Building datasets...', flush=True)\n",
        "    dtrain = WindowDataset(df.iloc[tr_idx], labels=df.iloc[tr_idx]['score'].values.astype(np.float32))\n",
        "    dvalid = WindowDataset(df.iloc[va_idx], labels=df.iloc[va_idx]['score'].values.astype(np.float32))\n",
        "    _log_windows_stats(f'[Fold {fold}] Train', dtrain)\n",
        "    _log_windows_stats(f'[Fold {fold}] Valid', dvalid)\n",
        "    model = MSDMeanPoolRegressor(MODEL_NAME, hidden_size=768, msd=5, p=0.2)\n",
        "    # Disable gradient checkpointing to avoid potential stalls\n",
        "    # Eager mode only (no torch.compile) to avoid Triton build issues\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"{out_dir}/fold{fold}\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=BATCH_TRAIN,\n",
        "        per_device_eval_batch_size=BATCH_EVAL,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        fp16=True,\n",
        "        bf16=False,\n",
        "        evaluation_strategy='steps',\n",
        "        save_strategy='steps',\n",
        "        eval_steps=400,\n",
        "        save_steps=400,\n",
        "        save_total_limit=1,\n",
        "        logging_strategy='steps',\n",
        "        logging_steps=50,\n",
        "        logging_first_step=True,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='qwk_round',\n",
        "        greater_is_better=True,\n",
        "        report_to=[],\n",
        "        disable_tqdm=False,\n",
        "        dataloader_num_workers=0,\n",
        "        dataloader_pin_memory=True,\n",
        "        dataloader_persistent_workers=False,\n",
        "        group_by_length=False,\n",
        "        gradient_accumulation_steps=4,\n",
        "        lr_scheduler_type='cosine',\n",
        "        warmup_ratio=0.1,\n",
        "        optim='adamw_torch_fused',\n",
        "        eval_accumulation_steps=32,\n",
        "        seed=SEED2,\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=dtrain,\n",
        "        eval_dataset=dvalid,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=PadCollator(),\n",
        "        compute_metrics=compute_metrics_factory(dvalid.essay_ids),\n",
        "        callbacks=[PrinterCallback()]\n",
        "    )\n",
        "    t0 = time.time()\n",
        "    print(f'[Fold {fold}] Start training...', flush=True)\n",
        "    trainer.train()\n",
        "    print(f\"[Fold {fold}] Train done in {(time.time()-t0)/60:.1f} min\", flush=True)\n",
        "    # Predict on valid windows and aggregate (token-count weighted mean)\n",
        "    preds_val = trainer.predict(dvalid).predictions.squeeze()\n",
        "    ids = np.array(dvalid.essay_ids)\n",
        "    lens = np.array(dvalid.lengths, dtype=float)\n",
        "    by_sum = defaultdict(float)\n",
        "    by_w = defaultdict(float)\n",
        "    for p, i, w in zip(preds_val, ids, lens):\n",
        "        by_sum[i] += float(p) * float(w)\n",
        "        by_w[i] += float(w)\n",
        "    agg = {i: (by_sum[i] / max(by_w[i], 1e-6)) for i in by_sum.keys()}\n",
        "    va_eids = df.iloc[va_idx]['essay_id'].values.tolist()\n",
        "    agg_vec = np.array([agg[e] for e in va_eids], dtype=float)\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "    return va_idx, agg_vec\n",
        "\n",
        "# Driver for seed 2 (will execute after current run finishes):\n",
        "print('Prepared seed-2 training function with k16 folds, MSD, eval_steps=500. To run: set folds from folds_grouped_k16.csv and loop folds.', flush=True)\n",
        "if os.path.exists('folds_grouped_k16.csv'):\n",
        "    print('Found folds_grouped_k16.csv. Example run snippet (not executing now):', flush=True)\n",
        "    print(\"\"\"\n",
        "train_df = pd.read_csv('train.csv')\n",
        "folds_g2 = pd.read_csv('folds_grouped_k16.csv')\n",
        "folds_map2 = dict(zip(folds_g2['essay_id'], folds_g2['fold_grouped_k16']))\n",
        "folds2 = train_df['essay_id'].map(folds_map2).values.astype(int)\n",
        "oof2 = np.zeros(len(train_df), dtype=float)\n",
        "for f in sorted(np.unique(folds2)):\n",
        "    f_start = time.time()\n",
        "    va_idx, agg_preds = train_fold_seed2(f, train_df, folds2)\n",
        "    oof2[va_idx] = agg_preds\n",
        "    y_true = train_df.iloc[va_idx]['score'].values.astype(int)\n",
        "    q = qwk(y_true, np.clip(np.rint(np.clip(agg_preds, 0.5, 6.5)), 1, 6).astype(int))\n",
        "    print(f'Fold {f} val round-QWK={q:.4f} elapsed={(time.time()-f_start)/60:.1f} min', flush=True)\n",
        "np.save('oof_deberta_v3_base_win512_seed2025.npy', oof2)\n",
        "opt = ThresholdOptimizer()\n",
        "best_th, best_oof_qwk = opt.fit(train_df['score'].values.astype(int), np.clip(oof2, 0.5, 6.5))\n",
        "round_q = qwk(train_df['score'].values.astype(int), np.clip(np.rint(oof2), 1, 6).astype(int))\n",
        "print(f'OOF round-QWK={round_q:.5f}  OOF thresh-QWK={best_oof_qwk:.5f}  thresholds={best_th}', flush=True)\n",
        "with open('thresholds_deberta_v3_base_win512_seed2025.json','w') as f:\n",
        "    json.dump({'thresholds': best_th.tolist(), 'oof_qwk': float(best_oof_qwk), 'round_oof_qwk': float(round_q)}, f)\n",
        "    \"\"\")\n",
        "else:\n",
        "    print('folds_grouped_k16.csv not found yet. Build it with cell 7 first.', flush=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Seed 2 prep: DeBERTa-v3-base windows + mean pooling + Multi-Sample Dropout (512/384) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared seed-2 training function with k16 folds, MSD, eval_steps=500. To run: set folds from folds_grouped_k16.csv and loop folds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found folds_grouped_k16.csv. Example run snippet (not executing now):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ntrain_df = pd.read_csv('train.csv')\nfolds_g2 = pd.read_csv('folds_grouped_k16.csv')\nfolds_map2 = dict(zip(folds_g2['essay_id'], folds_g2['fold_grouped_k16']))\nfolds2 = train_df['essay_id'].map(folds_map2).values.astype(int)\noof2 = np.zeros(len(train_df), dtype=float)\nfor f in sorted(np.unique(folds2)):\n    f_start = time.time()\n    va_idx, agg_preds = train_fold_seed2(f, train_df, folds2)\n    oof2[va_idx] = agg_preds\n    y_true = train_df.iloc[va_idx]['score'].values.astype(int)\n    q = qwk(y_true, np.clip(np.rint(np.clip(agg_preds, 0.5, 6.5)), 1, 6).astype(int))\n    print(f'Fold {f} val round-QWK={q:.4f} elapsed={(time.time()-f_start)/60:.1f} min', flush=True)\nnp.save('oof_deberta_v3_base_win512_seed2025.npy', oof2)\nopt = ThresholdOptimizer()\nbest_th, best_oof_qwk = opt.fit(train_df['score'].values.astype(int), np.clip(oof2, 0.5, 6.5))\nround_q = qwk(train_df['score'].values.astype(int), np.clip(np.rint(oof2), 1, 6).astype(int))\nprint(f'OOF round-QWK={round_q:.5f}  OOF thresh-QWK={best_oof_qwk:.5f}  thresholds={best_th}', flush=True)\nwith open('thresholds_deberta_v3_base_win512_seed2025.json','w') as f:\n    json.dump({'thresholds': best_th.tolist(), 'oof_qwk': float(best_oof_qwk), 'round_oof_qwk': float(round_q)}, f)\n    \n"
          ]
        }
      ]
    },
    {
      "id": "73c05080-2734-44be-b572-38c3cb720a02",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, glob, json, time, gc\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding, Trainer, TrainingArguments\n",
        "\n",
        "print('=== Test-time inference (windows mean) for deberta_v3_base_win512 ===', flush=True)\n",
        "\n",
        "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
        "MAX_LEN = 512\n",
        "STRIDE = 128\n",
        "BATCH_EVAL = 32\n",
        "OUT_DIR = 'deberta_v3_base_win512'\n",
        "\n",
        "tokenizer_tt = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "class WindowDatasetTest(Dataset):\n",
        "    def __init__(self, df, text_col='full_text'):\n",
        "        self.essay_ids = []\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        texts = df[text_col].astype(str).tolist()\n",
        "        eids = df['essay_id'].tolist()\n",
        "        enc = tokenizer_tt(texts,\n",
        "                           max_length=MAX_LEN,\n",
        "                           truncation=True,\n",
        "                           padding=False,\n",
        "                           return_overflowing_tokens=True,\n",
        "                           stride=STRIDE,\n",
        "                           return_attention_mask=True)\n",
        "        overflow_to_sample = enc.pop('overflow_to_sample_mapping')\n",
        "        for idx, sample_idx in enumerate(overflow_to_sample):\n",
        "            self.essay_ids.append(eids[sample_idx])\n",
        "            self.input_ids.append(enc['input_ids'][idx])\n",
        "            self.attn_masks.append(enc['attention_mask'][idx])\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "    def __getitem__(self, i):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.input_ids[i], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.attn_masks[i], dtype=torch.long),\n",
        "            'essay_id': self.essay_ids[i]\n",
        "        }\n",
        "\n",
        "class PadCollatorTT:\n",
        "    def __init__(self):\n",
        "        self.pad = DataCollatorWithPadding(tokenizer=tokenizer_tt, pad_to_multiple_of=8)\n",
        "    def __call__(self, features):\n",
        "        for f in features:\n",
        "            f.pop('essay_id', None)\n",
        "        batch = self.pad(features)\n",
        "        return batch\n",
        "\n",
        "class MeanPoolRegressor(nn.Module):\n",
        "    def __init__(self, model_name, hidden_size=768):\n",
        "        super().__init__()\n",
        "        self.backbone = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.head = nn.Linear(hidden_size, 1)\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
        "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last = out.last_hidden_state\n",
        "        mask = attention_mask.unsqueeze(-1).float()\n",
        "        mean = (last * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-6)\n",
        "        mean = self.dropout(mean)\n",
        "        logits = self.head(mean).squeeze(-1)\n",
        "        return {'logits': logits.unsqueeze(-1)}\n",
        "\n",
        "def load_best_subdir(folder):\n",
        "        cks = sorted(glob.glob(os.path.join(folder, 'checkpoint-*')), key=lambda p: int(p.split('-')[-1]))\n",
        "        return cks[-1] if cks else folder\n",
        "\n",
        "def _find_weight_file(path_dir):\n",
        "    cand = [\n",
        "        os.path.join(path_dir, 'pytorch_model.bin'),\n",
        "        os.path.join(path_dir, 'model.safetensors'),\n",
        "        os.path.join(path_dir, 'pytorch_model.bin.index.json')\n",
        "    ]\n",
        "    for p in cand:\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "def predict_fold(folder, dtest):\n",
        "    best_dir = load_best_subdir(folder)\n",
        "    # If checkpoint dir chosen but missing weights, fallback to fold root\n",
        "    wt = _find_weight_file(best_dir)\n",
        "    if wt is None and 'checkpoint-' in best_dir:\n",
        "        root_dir = folder\n",
        "        wt = _find_weight_file(root_dir)\n",
        "        best_dir = root_dir if wt is not None else best_dir\n",
        "    # Init model and load weights\n",
        "    model = MeanPoolRegressor(MODEL_NAME)\n",
        "    if wt is not None and wt.endswith('.safetensors'):\n",
        "        from safetensors.torch import load_file\n",
        "        sd = load_file(wt)\n",
        "        model.load_state_dict(sd, strict=False)\n",
        "    elif wt is not None:\n",
        "        model.load_state_dict(torch.load(wt, map_location='cpu'), strict=False)\n",
        "    else:\n",
        "        # As a last resort, try HF format dir\n",
        "        try:\n",
        "            model = MeanPoolRegressor.from_pretrained(best_dir)  # may fail for custom head\n",
        "        except Exception:\n",
        "            pass\n",
        "    args = TrainingArguments(output_dir=os.path.join(folder, 'tmp_infer'), per_device_eval_batch_size=BATCH_EVAL,\n",
        "                             dataloader_num_workers=0, dataloader_pin_memory=True, report_to=[], fp16=True, bf16=False)\n",
        "    trainer = Trainer(model=model, args=args, tokenizer=tokenizer_tt, data_collator=PadCollatorTT())\n",
        "    preds = trainer.predict(dtest).predictions.squeeze()\n",
        "    ids = np.array(dtest.essay_ids)\n",
        "    by_id = defaultdict(list)\n",
        "    for p, i in zip(preds, ids):\n",
        "        by_id[i].append(float(p))\n",
        "    # Aggregate per essay-id\n",
        "    agg = {}\n",
        "    for i, v in by_id.items():\n",
        "        agg[i] = float(np.mean(v))\n",
        "    return agg\n",
        "\n",
        "def run_test_inference():\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    dtest = WindowDatasetTest(test_df)\n",
        "    fold_dirs = sorted([p for p in glob.glob(os.path.join(OUT_DIR, 'fold*')) if os.path.isdir(p)])\n",
        "    all_fold_preds = []\n",
        "    t0 = time.time()\n",
        "    for fd in fold_dirs:\n",
        "        t1 = time.time()\n",
        "        agg = predict_fold(fd, dtest)\n",
        "        all_fold_preds.append(agg)\n",
        "        print(f'Predicted {fd} in {(time.time()-t1)/60:.1f} min', flush=True)\n",
        "        torch.cuda.empty_cache(); gc.collect()\n",
        "    # Average across folds\n",
        "    test_ids_order = test_df['essay_id'].tolist()\n",
        "    preds_mat = []\n",
        "    for agg in all_fold_preds:\n",
        "        preds_mat.append([agg[e] for e in test_ids_order])\n",
        "    preds_mean = np.mean(np.array(preds_mat), axis=0)\n",
        "    preds_mean = np.clip(preds_mean, 0.5, 6.5)\n",
        "    np.save('test_deberta_v3_base_win512.npy', preds_mean)\n",
        "    th_path = 'thresholds_deberta_v3_base_win512.json'\n",
        "    if os.path.exists(th_path):\n",
        "        with open(th_path, 'r') as f:\n",
        "            th = np.array(json.load(f)['thresholds'], dtype=float)\n",
        "    else:\n",
        "        th = np.array([1.5,2.5,3.5,4.5,5.5], dtype=float)\n",
        "    labels = np.digitize(preds_mean, th) + 1\n",
        "    labels = np.clip(labels, 1, 6).astype(int)\n",
        "    sub = pd.DataFrame({'essay_id': test_df['essay_id'], 'score': labels})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv and test_deberta_v3_base_win512.npy')\n",
        "    print(f'=== Test inference done in {(time.time()-t0)/60:.1f} min ===', flush=True)\n",
        "\n",
        "print('Inference cell ready. Run after training artifacts exist in deberta_v3_base_win512/fold*/')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Test-time inference (windows mean) for deberta_v3_base_win512 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference cell ready. Run after training artifacts exist in deberta_v3_base_win512/fold*/\n"
          ]
        }
      ]
    },
    {
      "id": "30c75c12-69ef-4c55-b940-0ba8239c1d00",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time, json, numpy as np, pandas as pd\n",
        "print('=== Running seed-2 training with k=16 grouped folds (MSD, eval_steps=500) ===', flush=True)\n",
        "t0 = time.time()\n",
        "train_df = pd.read_csv('train.csv')\n",
        "folds_g2 = pd.read_csv('folds_grouped_k16.csv')\n",
        "folds_map2 = dict(zip(folds_g2['essay_id'], folds_g2['fold_grouped_k16']))\n",
        "folds2 = train_df['essay_id'].map(folds_map2).values.astype(int)\n",
        "oof2 = np.zeros(len(train_df), dtype=float)\n",
        "for f in sorted(np.unique(folds2)):\n",
        "    f_start = time.time()\n",
        "    va_idx, agg_preds = train_fold_seed2(f, train_df, folds2)\n",
        "    oof2[va_idx] = agg_preds\n",
        "    y_true = train_df.iloc[va_idx]['score'].values.astype(int)\n",
        "    q = qwk(y_true, np.clip(np.rint(np.clip(agg_preds, 0.5, 6.5)), 1, 6).astype(int))\n",
        "    print(f'Fold {f} val round-QWK={q:.4f} elapsed={(time.time()-f_start)/60:.1f} min', flush=True)\n",
        "np.save('oof_deberta_v3_base_win512_seed2025.npy', oof2)\n",
        "opt = ThresholdOptimizer()\n",
        "best_th, best_oof_qwk = opt.fit(train_df['score'].values.astype(int), np.clip(oof2, 0.5, 6.5))\n",
        "round_q = qwk(train_df['score'].values.astype(int), np.clip(np.rint(oof2), 1, 6).astype(int))\n",
        "print(f'OOF round-QWK={round_q:.5f}  OOF thresh-QWK={best_oof_qwk:.5f}  thresholds={best_th}', flush=True)\n",
        "with open('thresholds_deberta_v3_base_win512_seed2025.json','w') as f:\n",
        "    json.dump({'thresholds': best_th.tolist(), 'oof_qwk': float(best_oof_qwk), 'round_oof_qwk': float(round_q)}, f)\n",
        "print(f'=== Seed-2 done in {(time.time()-t0)/60:.1f} min ===', flush=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Running seed-2 training with k=16 grouped folds (MSD, eval_steps=500) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Building datasets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Train: essays=13166 windows=17322 avg_windows_per_essay=1.32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Valid: essays=2410 windows=2961 avg_windows_per_essay=1.23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Start training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='3246' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/3246 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 2.6829, 'grad_norm': 29.283714294433594, 'learning_rate': 6.153846153846154e-08, 'epoch': 0.0009235742322789194}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 2.1801, 'grad_norm': 11.049778938293457, 'learning_rate': 3.0769230769230774e-06, 'epoch': 0.04617871161394597}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.4359, 'grad_norm': 6.179534912109375, 'learning_rate': 6.153846153846155e-06, 'epoch': 0.09235742322789194}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.3749, 'grad_norm': 2.8593878746032715, 'learning_rate': 9.230769230769232e-06, 'epoch': 0.1385361348418379}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.3704, 'grad_norm': 5.08953332901001, 'learning_rate': 1.230769230769231e-05, 'epoch': 0.18471484645578387}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.3555, 'grad_norm': 10.21146011352539, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.23089355806972986}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.4006, 'grad_norm': 22.631258010864258, 'learning_rate': 1.8461538461538465e-05, 'epoch': 0.2770722696836758}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.3442, 'grad_norm': 9.150673866271973, 'learning_rate': 1.999638539797114e-05, 'epoch': 0.3232509812976218}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.3205, 'grad_norm': 2.66862154006958, 'learning_rate': 1.9967484258268576e-05, 'epoch': 0.36942969291156774}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.23827728629112244, 'eval_qwk_round': 0.6857860931176535, 'eval_runtime': 392.0781, 'eval_samples_per_second': 7.552, 'eval_steps_per_second': 0.237, 'epoch': 0.36942969291156774}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.2365, 'grad_norm': 7.990867614746094, 'learning_rate': 1.990976553665388e-05, 'epoch': 0.41560840452551373}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.2587, 'grad_norm': 2.2882888317108154, 'learning_rate': 1.9823396107129044e-05, 'epoch': 0.4617871161394597}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.2407, 'grad_norm': 8.910479545593262, 'learning_rate': 1.9708625677448357e-05, 'epoch': 0.5079658277534057}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.2606, 'grad_norm': 4.122722148895264, 'learning_rate': 1.9565786067173572e-05, 'epoch': 0.5541445393673516}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.2462, 'grad_norm': 8.829679489135742, 'learning_rate': 1.9395290248330815e-05, 'epoch': 0.6003232509812976}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.2661, 'grad_norm': 10.581090927124023, 'learning_rate': 1.9197631151442747e-05, 'epoch': 0.6465019625952436}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.2184, 'grad_norm': 10.760598182678223, 'learning_rate': 1.8973380240388088e-05, 'epoch': 0.6926806742091896}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.1962, 'grad_norm': 6.411953926086426, 'learning_rate': 1.8723185860208653e-05, 'epoch': 0.7388593858231355}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.21167442202568054, 'eval_qwk_round': 0.7572089675233813, 'eval_runtime': 391.8354, 'eval_samples_per_second': 7.557, 'eval_steps_per_second': 0.237, 'epoch': 0.7388593858231355}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.2251, 'grad_norm': 3.406346082687378, 'learning_rate': 1.8447771362640735e-05, 'epoch': 0.7850380974370815}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.226, 'grad_norm': 3.741476535797119, 'learning_rate': 1.8147933014790245e-05, 'epoch': 0.8312168090510275}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.2338, 'grad_norm': 10.01356029510498, 'learning_rate': 1.7824537696997862e-05, 'epoch': 0.8773955206649734}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.2017, 'grad_norm': 6.351576805114746, 'learning_rate': 1.747852039655015e-05, 'epoch': 0.9235742322789194}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.2321, 'grad_norm': 3.014822483062744, 'learning_rate': 1.7110881504482632e-05, 'epoch': 0.9697529438928654}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.2261, 'grad_norm': 3.3823678493499756, 'learning_rate': 1.6722683923290228e-05, 'epoch': 1.0159316555068114}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.1977, 'grad_norm': 6.635529518127441, 'learning_rate': 1.6315049993907145e-05, 'epoch': 1.0621103671207572}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.1813, 'grad_norm': 4.952932357788086, 'learning_rate': 1.588915825084077e-05, 'epoch': 1.1082890787347033}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.1851348578929901, 'eval_qwk_round': 0.7460734389033399, 'eval_runtime': 392.1869, 'eval_samples_per_second': 7.55, 'eval_steps_per_second': 0.237, 'epoch': 1.1082890787347033}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.1968, 'grad_norm': 4.973823070526123, 'learning_rate': 1.5446240014840997e-05, 'epoch': 1.1544677903486493}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.2151, 'grad_norm': 6.858154773712158, 'learning_rate': 1.4987575832956173e-05, 'epoch': 1.2006465019625951}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.1963, 'grad_norm': 9.147194862365723, 'learning_rate': 1.4514491776267939e-05, 'epoch': 1.2468252135765412}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.1882, 'grad_norm': 6.684695243835449, 'learning_rate': 1.4028355606008888e-05, 'epoch': 1.2930039251904872}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.1631, 'grad_norm': 5.729094505310059, 'learning_rate': 1.3530572819147346e-05, 'epoch': 1.3391826368044333}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.1772, 'grad_norm': 3.558868646621704, 'learning_rate': 1.302258258487217e-05, 'epoch': 1.385361348418379}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.1815, 'grad_norm': 5.575219631195068, 'learning_rate': 1.2505853583725754e-05, 'epoch': 1.4315400600323251}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.1777, 'grad_norm': 7.146465301513672, 'learning_rate': 1.198187976141507e-05, 'epoch': 1.4777187716462712}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.2381138652563095, 'eval_qwk_round': 0.7581001178991479, 'eval_runtime': 392.8363, 'eval_samples_per_second': 7.537, 'eval_steps_per_second': 0.237, 'epoch': 1.4777187716462712}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.171, 'grad_norm': 2.7038567066192627, 'learning_rate': 1.1452176009577062e-05, 'epoch': 1.523897483260217}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.1704, 'grad_norm': 10.400626182556152, 'learning_rate': 1.0918273785986052e-05, 'epoch': 1.570076194874163}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.1994, 'grad_norm': 11.964150428771973, 'learning_rate': 1.03817166868658e-05, 'epoch': 1.616254906488109}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.1754, 'grad_norm': 5.145857810974121, 'learning_rate': 9.844055984107359e-06, 'epoch': 1.662433618102055}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.1844, 'grad_norm': 4.400597095489502, 'learning_rate': 9.306846140295395e-06, 'epoch': 1.708612329716001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.1806, 'grad_norm': 3.624797821044922, 'learning_rate': 8.77164031450966e-06, 'epoch': 1.754791041329947}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.1665, 'grad_norm': 2.577631711959839, 'learning_rate': 8.239985871895144e-06, 'epoch': 1.8009697529438928}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(np.unique(folds2)):\n\u001b[32m     10\u001b[39m     f_start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     va_idx, agg_preds = \u001b[43mtrain_fold_seed2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolds2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     oof2[va_idx] = agg_preds\n\u001b[32m     13\u001b[39m     y_true = train_df.iloc[va_idx][\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m].values.astype(\u001b[38;5;28mint\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 227\u001b[39m, in \u001b[36mtrain_fold_seed2\u001b[39m\u001b[34m(fold, df, folds, out_dir)\u001b[39m\n\u001b[32m    225\u001b[39m t0 = time.time()\n\u001b[32m    226\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m[Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Start training...\u001b[39m\u001b[33m'\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Train done in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time.time()-t0)/\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m min\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    229\u001b[39m \u001b[38;5;66;03m# Predict on valid windows and aggregate (token-count weighted mean)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/transformers/trainer.py:1938\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1937\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1938\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/transformers/trainer.py:2279\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2276\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_begin(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2278\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.accumulate(model):\n\u001b[32m-> \u001b[39m\u001b[32m2279\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2281\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2282\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2283\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2284\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2285\u001b[39m ):\n\u001b[32m   2286\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2287\u001b[39m     tr_loss += tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/transformers/trainer.py:3349\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3347\u001b[39m         scaled_loss.backward()\n\u001b[32m   3348\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3349\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3351\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach() / \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/accelerate/accelerator.py:2196\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2194\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2195\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2196\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/_tensor.py:521\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    513\u001b[39m         Tensor.backward,\n\u001b[32m    514\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    519\u001b[39m         inputs=inputs,\n\u001b[32m    520\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/autograd/__init__.py:289\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    284\u001b[39m     retain_graph = create_graph\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/autograd/graph.py:769\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    767\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    768\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    770\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/autograd/function.py:291\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBackwardCFunction\u001b[39;00m(_C._FunctionBase, FunctionCtx, _HookMixin):\n\u001b[32m    287\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[33;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m    292\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[33;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[33;03m        \"\"\"\u001b[39;00m\n\u001b[32m    295\u001b[39m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[32m    296\u001b[39m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "74d9787a-1f62-413c-a2cc-198673c9316e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess, sys, time\n",
        "print('=== Installing python dev headers to fix Triton build (Python.h) ===', flush=True)\n",
        "t0 = time.time()\n",
        "cmd = \"apt-get update -y && apt-get install -y python3-dev python3.11-dev build-essential\"\n",
        "print(cmd, flush=True)\n",
        "ret = subprocess.run(['bash','-lc', cmd], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "print(ret.stdout)\n",
        "print(f'=== Done in {(time.time()-t0)/60:.1f} min ===', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "55d59d69-69cb-4791-a881-66544ce5b82d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import math, copy, os, time, gc\n",
        "import numpy as np\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from transformers import TrainerCallback, Trainer, TrainingArguments, PrinterCallback\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "\n",
        "print('=== Seed-3 prep: LLRD + EMA (READY) | fixes: token-weighted eval, dynamic head, group_by_length, unique class name ===', flush=True)\n",
        "\n",
        "# Ensure base backbone is used regardless of previous globals\n",
        "MODEL_NAME_SEED3 = 'microsoft/deberta-v3-base'\n",
        "\n",
        "def build_llrd_param_groups(model, base_lr=2e-5, head_lr_mult=2.0, decay=0.9, weight_decay=0.01):\n",
        "    backbone = model.backbone\n",
        "    layers = []\n",
        "    if hasattr(backbone, 'encoder') and hasattr(backbone.encoder, 'layer'):\n",
        "        layers = list(backbone.encoder.layer)\n",
        "    elif hasattr(backbone, 'deberta') and hasattr(backbone.deberta, 'encoder') and hasattr(backbone.deberta.encoder, 'layer'):\n",
        "        layers = list(backbone.deberta.encoder.layer)\n",
        "    elif hasattr(backbone, 'embeddings'):\n",
        "        layers = []\n",
        "    n = len(layers)\n",
        "    param_groups = []\n",
        "    no_decay = ('bias', 'LayerNorm.weight', 'layer_norm.weight', 'ln.weight')\n",
        "    def add_group(params, lr, wd):\n",
        "        if not params: return\n",
        "        param_groups.append({'params': params, 'lr': lr, 'weight_decay': wd})\n",
        "    # Embeddings (deepest)\n",
        "    emb, emb_nd = [], []\n",
        "    for n_, p in backbone.embeddings.named_parameters(recurse=True):\n",
        "        (emb_nd if any(nd in n_ for nd in no_decay) else emb).append(p)\n",
        "    add_group(emb, base_lr * (decay ** (n+1)), weight_decay)\n",
        "    add_group(emb_nd, base_lr * (decay ** (n+1)), 0.0)\n",
        "    # Encoder layers\n",
        "    for i, layer in enumerate(layers):\n",
        "        depth = i + 1\n",
        "        lr_i = base_lr * (decay ** (n - depth + 1))\n",
        "        pg, pg_nd = [], []\n",
        "        for n_, p in layer.named_parameters(recurse=True):\n",
        "            (pg_nd if any(nd in n_ for nd in no_decay) else pg).append(p)\n",
        "        add_group(pg, lr_i, weight_decay)\n",
        "        add_group(pg_nd, lr_i, 0.0)\n",
        "    # Pooler\n",
        "    if hasattr(backbone, 'pooler'):\n",
        "        pl, pl_nd = [], []\n",
        "        for n_, p in backbone.pooler.named_parameters(recurse=True):\n",
        "            (pl_nd if any(nd in n_ for nd in no_decay) else pl).append(p)\n",
        "        add_group(pl, base_lr, weight_decay)\n",
        "        add_group(pl_nd, base_lr, 0.0)\n",
        "    # Head (higher LR)\n",
        "    head_lr = base_lr * head_lr_mult\n",
        "    head_wd, head_nd = [], []\n",
        "    for n_, p in model.head.named_parameters(recurse=True):\n",
        "        (head_nd if any(nd in n_ for nd in no_decay) else head_wd).append(p)\n",
        "    add_group(head_wd, head_lr, weight_decay)\n",
        "    add_group(head_nd, head_lr, 0.0)\n",
        "    return param_groups\n",
        "\n",
        "class EMACallback(TrainerCallback):\n",
        "    def __init__(self, ema_decay=0.995):\n",
        "        self.decay = ema_decay\n",
        "        self.shadow = {}\n",
        "        self.backup = {}\n",
        "    def on_train_begin(self, args, state, control, **kwargs):\n",
        "        model = kwargs['model']\n",
        "        self.shadow = {name: p.detach().clone() for name, p in model.named_parameters() if p.requires_grad}\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        model = kwargs['model']\n",
        "        with torch.no_grad():\n",
        "            for name, p in model.named_parameters():\n",
        "                if p.requires_grad and name in self.shadow:\n",
        "                    self.shadow[name].mul_(self.decay).add_(p.detach(), alpha=(1.0 - self.decay))\n",
        "    def apply_shadow(self, model):\n",
        "        self.backup = {}\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.requires_grad and name in self.shadow:\n",
        "                self.backup[name] = p.detach().clone()\n",
        "                p.data.copy_(self.shadow[name].data)\n",
        "    def restore(self, model):\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.requires_grad and name in self.backup:\n",
        "                p.data.copy_(self.backup[name].data)\n",
        "        self.backup = {}\n",
        "    def on_evaluate(self, args, state, control, **kwargs):\n",
        "        self.apply_shadow(kwargs['model'])\n",
        "    def on_evaluate_end(self, args, state, control, **kwargs):\n",
        "        self.restore(kwargs['model'])\n",
        "\n",
        "# Unique class name to avoid clashes with earlier definitions\n",
        "from transformers import AutoModel\n",
        "class MSDMeanPoolRegressorSeed3(nn.Module):\n",
        "    def __init__(self, model_name, msd=5, p=0.2):\n",
        "        super().__init__()\n",
        "        self.backbone = AutoModel.from_pretrained(model_name)\n",
        "        hidden = int(getattr(self.backbone.config, 'hidden_size', 768))\n",
        "        self.dropouts = nn.ModuleList([nn.Dropout(p) for _ in range(msd)])\n",
        "        self.head = nn.Linear(hidden, 1)\n",
        "        self.msd = msd\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
        "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last = out.last_hidden_state  # [B, T, H]\n",
        "        mask = attention_mask.unsqueeze(-1).float()\n",
        "        mean = (last * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-6)\n",
        "        logits_list = []\n",
        "        for dp in self.dropouts:\n",
        "            logits_list.append(self.head(dp(mean)).squeeze(-1))\n",
        "        logits = torch.stack(logits_list, dim=0).mean(dim=0)  # [B]\n",
        "        if labels is not None:\n",
        "            loss = torch.nn.functional.smooth_l1_loss(logits, labels, beta=1.0, reduction='mean')\n",
        "            return {'loss': loss, 'logits': logits.unsqueeze(-1)}\n",
        "        return {'logits': logits.unsqueeze(-1)}\n",
        "\n",
        "def compute_metrics_factory_token_weighted(eval_ids, eval_lengths):\n",
        "    def compute(eval_pred):\n",
        "        preds = eval_pred.predictions.squeeze()\n",
        "        labels = eval_pred.label_ids.squeeze()\n",
        "        ids = np.array(eval_ids)\n",
        "        wts = np.array(eval_lengths, dtype=float)\n",
        "        by_sum, by_w, by_true = defaultdict(float), defaultdict(float), {}\n",
        "        for p, y, i, w in zip(preds, labels, ids, wts):\n",
        "            by_sum[i] += float(p) * float(w)\n",
        "            by_w[i] += float(w)\n",
        "            by_true[i] = int(y)\n",
        "        agg_preds, agg_true = [], []\n",
        "        for i in by_sum.keys():\n",
        "            agg_preds.append(by_sum[i] / max(by_w[i], 1e-6))\n",
        "            agg_true.append(by_true[i])\n",
        "        agg_preds = np.clip(np.array(agg_preds), 0.5, 6.5)\n",
        "        agg_true = np.array(agg_true, dtype=int)\n",
        "        q = cohen_kappa_score(agg_true, np.clip(np.rint(agg_preds), 1, 6).astype(int), weights='quadratic')\n",
        "        return {'qwk_round': q}\n",
        "    return compute\n",
        "\n",
        "def train_fold_seed3(fold, df, folds, out_dir='deberta_v3_base_win512_seed3_llrd_ema',\n",
        "                     base_lr=2e-5, head_lr_mult=2.0, decay=0.9, ema_decay=0.995):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    tr_idx = np.where(folds != fold)[0]\n",
        "    va_idx = np.where(folds == fold)[0]\n",
        "    print(f'[Seed3 Fold {fold}] Building datasets...', flush=True)\n",
        "    dtrain = WindowDataset(df.iloc[tr_idx], labels=df.iloc[tr_idx]['score'].values.astype(np.float32))\n",
        "    dvalid = WindowDataset(df.iloc[va_idx], labels=df.iloc[va_idx]['score'].values.astype(np.float32))\n",
        "    _log_windows_stats(f'[Seed3 Fold {fold}] Train', dtrain)\n",
        "    _log_windows_stats(f'[Seed3 Fold {fold}] Valid', dvalid)\n",
        "    model = MSDMeanPoolRegressorSeed3(MODEL_NAME_SEED3, msd=5, p=0.2)\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"{out_dir}/fold{fold}\",\n",
        "        learning_rate=base_lr,\n",
        "        per_device_train_batch_size=max(4, BATCH_TRAIN),\n",
        "        per_device_eval_batch_size=BATCH_EVAL,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        fp16=True,\n",
        "        bf16=False,\n",
        "        evaluation_strategy='steps',\n",
        "        save_strategy='steps',\n",
        "        eval_steps=400,\n",
        "        save_steps=400,\n",
        "        save_total_limit=1,\n",
        "        logging_strategy='steps',\n",
        "        logging_steps=50,\n",
        "        logging_first_step=True,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='qwk_round',\n",
        "        greater_is_better=True,\n",
        "        report_to=[],\n",
        "        disable_tqdm=False,\n",
        "        dataloader_num_workers=0,\n",
        "        dataloader_pin_memory=True,\n",
        "        dataloader_persistent_workers=False,\n",
        "        group_by_length=True,\n",
        "        gradient_accumulation_steps=4,\n",
        "        lr_scheduler_type='cosine',\n",
        "        warmup_ratio=0.1,\n",
        "        optim='adamw_torch_fused',\n",
        "        eval_accumulation_steps=32,\n",
        "        seed=SEED2+1,\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "    # Custom optimizer with LLRD\n",
        "    pg = build_llrd_param_groups(model, base_lr=base_lr, head_lr_mult=head_lr_mult, decay=decay, weight_decay=0.01)\n",
        "    class LLRDTrainer(Trainer):\n",
        "        def create_optimizer(self):\n",
        "            if self.optimizer is not None:\n",
        "                return\n",
        "            optim_kwargs = {'lr': base_lr, 'betas': (0.9, 0.999), 'eps': 1e-8, 'weight_decay': 0.01}\n",
        "            try:\n",
        "                from torch.optim import AdamW\n",
        "                self.optimizer = AdamW(pg, **optim_kwargs)\n",
        "            except Exception:\n",
        "                self.optimizer = torch.optim.AdamW(pg, **optim_kwargs)\n",
        "    ema_cb = EMACallback(ema_decay=ema_decay)\n",
        "    trainer = LLRDTrainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=dtrain,\n",
        "        eval_dataset=dvalid,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=PadCollator(),\n",
        "        compute_metrics=compute_metrics_factory_token_weighted(dvalid.essay_ids, dvalid.lengths),\n",
        "        callbacks=[PrinterCallback(), ema_cb]\n",
        "    )\n",
        "    print(f'[Seed3 Fold {fold}] Start training...', flush=True)\n",
        "    t0 = time.time()\n",
        "    trainer.train()\n",
        "    print(f\"[Seed3 Fold {fold}] Train done in {(time.time()-t0)/60:.1f} min\", flush=True)\n",
        "    # EMA weights applied automatically during eval; do final predict with shadow applied\n",
        "    ema_cb.apply_shadow(trainer.model)\n",
        "    preds_val = trainer.predict(dvalid).predictions.squeeze()\n",
        "    ema_cb.restore(trainer.model)\n",
        "    ids = np.array(dvalid.essay_ids)\n",
        "    lens = np.array(dvalid.lengths, dtype=float)\n",
        "    by_sum = defaultdict(float); by_w = defaultdict(float)\n",
        "    for p, i, w in zip(preds_val, ids, lens):\n",
        "        by_sum[i] += float(p) * float(w); by_w[i] += float(w)\n",
        "    agg = {i: (by_sum[i] / max(by_w[i], 1e-6)) for i in by_sum.keys()}\n",
        "    va_eids = df.iloc[va_idx]['essay_id'].values.tolist()\n",
        "    agg_vec = np.array([agg[e] for e in va_eids], dtype=float)\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "    return va_idx, agg_vec\n",
        "\n",
        "print('Seed-3 training function ready (LLRD+EMA, token-weighted eval). Execute the driver cell next.', flush=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Seed-3 prep: LLRD + EMA (READY) | fixes: token-weighted eval, dynamic head, group_by_length, unique class name ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed-3 training function ready (LLRD+EMA, token-weighted eval). Execute the driver cell next.\n"
          ]
        }
      ]
    },
    {
      "id": "c0603dc8-d7d8-4353-8bf9-5d79a1ac2d31",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, time, json, math, random, gc, glob\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModel, Trainer, TrainingArguments, DataCollatorWithPadding, EarlyStoppingCallback, PrinterCallback\n",
        ")\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "print('=== Pivot: DeBERTa-v3-LARGE sliding windows (512/128) + MSD CLS+Mean, 3-fold, token-weighted eval ===', flush=True)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision('high')\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "\n",
        "SEED_L = 1337\n",
        "def seed_everything(seed=SEED_L):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "seed_everything()\n",
        "\n",
        "def qwk(y_true, y_pred_int):\n",
        "    return cohen_kappa_score(y_true, y_pred_int, weights='quadratic')\n",
        "\n",
        "class ThresholdOptimizer:\n",
        "    def __init__(self, init_thresholds=None):\n",
        "        self.thresholds = np.array(init_thresholds if init_thresholds is not None else [1.5,2.5,3.5,4.5,5.5], dtype=float)\n",
        "    def _apply(self, preds):\n",
        "        return np.digitize(preds, self.thresholds) + 1\n",
        "    def fit(self, y_true, preds, iters=200, step=0.05):\n",
        "        best = self.thresholds.copy(); best_score = qwk(y_true, self._apply(preds))\n",
        "        for _ in range(iters):\n",
        "            improved = False\n",
        "            for i in range(5):\n",
        "                for d in (-step, step):\n",
        "                    cand = np.sort(np.clip(best + (np.arange(5)==i)*d, 0.5, 6.5))\n",
        "                    if not (0.5 < cand[0] < cand[1] < cand[2] < cand[3] < cand[4] < 6.5):\n",
        "                        continue\n",
        "                    s = qwk(y_true, np.digitize(preds, cand) + 1)\n",
        "                    if s > best_score:\n",
        "                        best_score, best, improved = s, cand, True\n",
        "            if not improved:\n",
        "                step *= 0.5\n",
        "                if step < 1e-4: break\n",
        "        self.thresholds = best\n",
        "        return best, best_score\n",
        "\n",
        "MODEL_NAME = 'microsoft/deberta-v3-large'\n",
        "MAX_LEN = 512\n",
        "STRIDE = 128\n",
        "BATCH_TRAIN = 2\n",
        "GRAD_ACCUM = 8\n",
        "BATCH_EVAL = 8\n",
        "\n",
        "tokenizer_large = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "class WindowDatasetL(Dataset):\n",
        "    def __init__(self, df, text_col='full_text', labels=None):\n",
        "        self.essay_ids = []\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        self.lengths = []  # valid token counts per window\n",
        "        self.labels = [] if labels is not None else None\n",
        "        texts = df[text_col].astype(str).tolist()\n",
        "        eids = df['essay_id'].tolist()\n",
        "        lbls = None if labels is None else labels.astype(np.float32).tolist()\n",
        "        enc = tokenizer_large(texts,\n",
        "                              max_length=MAX_LEN,\n",
        "                              truncation=True,\n",
        "                              padding=False,\n",
        "                              return_overflowing_tokens=True,\n",
        "                              stride=STRIDE,\n",
        "                              return_attention_mask=True)\n",
        "        overflow_to_sample = enc.pop('overflow_to_sample_mapping')\n",
        "        for idx, sample_idx in enumerate(overflow_to_sample):\n",
        "            self.essay_ids.append(eids[sample_idx])\n",
        "            ids_i = enc['input_ids'][idx]\n",
        "            attn_i = enc['attention_mask'][idx]\n",
        "            self.input_ids.append(ids_i)\n",
        "            self.attn_masks.append(attn_i)\n",
        "            self.lengths.append(int(sum(attn_i)))\n",
        "            if lbls is not None:\n",
        "                self.labels.append(lbls[sample_idx])\n",
        "        if self.labels is not None:\n",
        "            self.labels = np.array(self.labels, dtype=np.float32)\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "    def __getitem__(self, i):\n",
        "        item = {\n",
        "            'input_ids': torch.tensor(self.input_ids[i], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.attn_masks[i], dtype=torch.long),\n",
        "        }\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(self.labels[i], dtype=torch.float32)\n",
        "        item['essay_id'] = self.essay_ids[i]\n",
        "        item['length'] = self.lengths[i]\n",
        "        return item\n",
        "\n",
        "class PadCollatorL:\n",
        "    def __init__(self):\n",
        "        self.pad = DataCollatorWithPadding(tokenizer=tokenizer_large, pad_to_multiple_of=8)\n",
        "    def __call__(self, features):\n",
        "        for f in features:\n",
        "            f.pop('essay_id', None); f.pop('length', None)\n",
        "        labels = None\n",
        "        if 'labels' in features[0]:\n",
        "            labels = torch.stack([f['labels'] for f in features]).view(-1)\n",
        "            for f in features: f.pop('labels')\n",
        "        batch = self.pad(features)\n",
        "        if labels is not None:\n",
        "            batch['labels'] = labels\n",
        "        return batch\n",
        "\n",
        "class MSDCLSMeanRegressor(nn.Module):\n",
        "    def __init__(self, model_name, hidden_size=1024, msd=5, p=0.2):\n",
        "        super().__init__()\n",
        "        self.backbone = AutoModel.from_pretrained(model_name)\n",
        "        self.dropouts = nn.ModuleList([nn.Dropout(p) for _ in range(msd)])\n",
        "        self.head = nn.Linear(hidden_size*2, 1)  # CLS + mean concat\n",
        "        self.msd = msd\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
        "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last = out.last_hidden_state  # [B, T, H]\n",
        "        cls = last[:, 0, :]  # [B, H]\n",
        "        mask = attention_mask.unsqueeze(-1).float()\n",
        "        mean = (last * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-6)  # [B, H]\n",
        "        feat = torch.cat([cls, mean], dim=-1)  # [B, 2H]\n",
        "        logits_list = []\n",
        "        for dp in self.dropouts:\n",
        "            logits_list.append(self.head(dp(feat)).squeeze(-1))\n",
        "        logits = torch.stack(logits_list, dim=0).mean(dim=0)  # [B]\n",
        "        if labels is not None:\n",
        "            loss = torch.nn.functional.smooth_l1_loss(logits, labels, beta=1.0, reduction='mean')\n",
        "            return {'loss': loss, 'logits': logits.unsqueeze(-1)}\n",
        "        return {'logits': logits.unsqueeze(-1)}\n",
        "\n",
        "# Globals to align eval aggregation to inference: token-length weighted mean\n",
        "EVAL_IDS = None\n",
        "EVAL_WTS = None\n",
        "\n",
        "def make_compute_metrics_token_weighted():\n",
        "    def compute(eval_pred):\n",
        "        preds = eval_pred.predictions.squeeze()\n",
        "        labels = eval_pred.label_ids.squeeze()\n",
        "        ids = np.array(EVAL_IDS)\n",
        "        wts = np.array(EVAL_WTS, dtype=float)\n",
        "        by_sum, by_w = defaultdict(float), defaultdict(float)\n",
        "        by_true = {}\n",
        "        for p, y, i, w in zip(preds, labels, ids, wts):\n",
        "            by_sum[i] += float(p) * float(w)\n",
        "            by_w[i] += float(w)\n",
        "            by_true[i] = int(y)\n",
        "        agg_preds, agg_true = [], []\n",
        "        for i in by_sum.keys():\n",
        "            agg_preds.append(by_sum[i] / max(by_w[i], 1e-6))\n",
        "            agg_true.append(by_true[i])\n",
        "        agg_preds = np.clip(np.array(agg_preds), 0.5, 6.5)\n",
        "        agg_true = np.array(agg_true, dtype=int)\n",
        "        q = qwk(agg_true, np.clip(np.rint(agg_preds), 1, 6).astype(int))\n",
        "        return {'qwk_round': q}\n",
        "    return compute\n",
        "\n",
        "def train_fold_v3large(fold, df, folds, out_dir='deberta_v3_large_win512'):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    tr_idx = np.where(folds != fold)[0]\n",
        "    va_idx = np.where(folds == fold)[0]\n",
        "    print(f'[v3-large Fold {fold}] Build datasets...', flush=True)\n",
        "    dtrain = WindowDatasetL(df.iloc[tr_idx], labels=df.iloc[tr_idx]['score'].values.astype(np.float32))\n",
        "    dvalid = WindowDatasetL(df.iloc[va_idx], labels=df.iloc[va_idx]['score'].values.astype(np.float32))\n",
        "    print(f'[v3-large Fold {fold}] Train: essays={len(set(dtrain.essay_ids))} windows={len(dtrain)} avg_w/E={len(dtrain)/max(1,len(set(dtrain.essay_ids))):.2f}', flush=True)\n",
        "    print(f'[v3-large Fold {fold}] Valid: essays={len(set(dvalid.essay_ids))} windows={len(dvalid)} avg_w/E={len(dvalid)/max(1,len(set(dvalid.essay_ids))):.2f}', flush=True)\n",
        "    model = MSDCLSMeanRegressor(MODEL_NAME, hidden_size=1024, msd=5, p=0.2)\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"{out_dir}/fold{fold}\",\n",
        "        learning_rate=1.5e-5,\n",
        "        per_device_train_batch_size=BATCH_TRAIN,\n",
        "        per_device_eval_batch_size=BATCH_EVAL,\n",
        "        num_train_epochs=3,  # early stopping will cut to ~2-2.5\n",
        "        weight_decay=0.05,\n",
        "        fp16=True,\n",
        "        bf16=False,\n",
        "        evaluation_strategy='steps',\n",
        "        save_strategy='steps',\n",
        "        eval_steps=250,\n",
        "        save_steps=250,\n",
        "        save_total_limit=1,\n",
        "        logging_strategy='steps',\n",
        "        logging_steps=50,\n",
        "        logging_first_step=True,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='qwk_round',\n",
        "        greater_is_better=True,\n",
        "        report_to=[],\n",
        "        disable_tqdm=False,\n",
        "        dataloader_num_workers=0,\n",
        "        dataloader_pin_memory=True,\n",
        "        dataloader_persistent_workers=False,\n",
        "        group_by_length=False,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM,\n",
        "        lr_scheduler_type='cosine',\n",
        "        warmup_ratio=0.08,\n",
        "        optim='adamw_torch_fused',\n",
        "        eval_accumulation_steps=16,\n",
        "        seed=SEED_L,\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "    global EVAL_IDS, EVAL_WTS\n",
        "    EVAL_IDS = dvalid.essay_ids\n",
        "    EVAL_WTS = dvalid.lengths\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=dtrain,\n",
        "        eval_dataset=dvalid,\n",
        "        tokenizer=tokenizer_large,\n",
        "        data_collator=PadCollatorL(),\n",
        "        compute_metrics=make_compute_metrics_token_weighted(),\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.0), PrinterCallback()]\n",
        "    )\n",
        "    t0 = time.time()\n",
        "    print(f'[v3-large Fold {fold}] Start training...', flush=True)\n",
        "    trainer.train()\n",
        "    print(f\"[v3-large Fold {fold}] Train done in {(time.time()-t0)/60:.1f} min\", flush=True)\n",
        "    # Predict on valid windows and aggregate with token-length weights\n",
        "    preds_val = trainer.predict(dvalid).predictions.squeeze()\n",
        "    ids = np.array(dvalid.essay_ids)\n",
        "    lens = np.array(dvalid.lengths, dtype=float)\n",
        "    by_sum, by_w = defaultdict(float), defaultdict(float)\n",
        "    for p, i, w in zip(preds_val, ids, lens):\n",
        "        by_sum[i] += float(p) * float(w)\n",
        "        by_w[i] += float(w)\n",
        "    agg = {i: (by_sum[i] / max(by_w[i], 1e-6)) for i in by_sum.keys()}\n",
        "    va_eids = df.iloc[va_idx]['essay_id'].values.tolist()\n",
        "    agg_vec = np.array([agg[e] for e in va_eids], dtype=float)\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "    return va_idx, agg_vec\n",
        "\n",
        "# Driver: run only 3 folds to fit time budget\n",
        "t0 = time.time()\n",
        "train_df = pd.read_csv('train.csv')\n",
        "folds_g2 = pd.read_csv('folds_grouped_k16.csv') if os.path.exists('folds_grouped_k16.csv') else pd.read_csv('folds_grouped.csv').rename(columns={'fold_grouped':'fold_grouped_k16'})\n",
        "folds_map2 = dict(zip(folds_g2['essay_id'], folds_g2[[c for c in folds_g2.columns if 'fold_grouped' in c][0]]))\n",
        "folds_arr = train_df['essay_id'].map(folds_map2).values.astype(int)\n",
        "\n",
        "unique_folds = sorted(np.unique(folds_arr))[:3]\n",
        "print('Using folds:', unique_folds, flush=True)\n",
        "oof_l = np.zeros(len(train_df), dtype=float)\n",
        "for f in unique_folds:\n",
        "    f_start = time.time()\n",
        "    va_idx, agg_preds = train_fold_v3large(f, train_df, folds_arr, out_dir='deberta_v3_large_win512')\n",
        "    oof_l[va_idx] = agg_preds\n",
        "    y_true = train_df.iloc[va_idx]['score'].values.astype(int)\n",
        "    q = qwk(y_true, np.clip(np.rint(np.clip(agg_preds, 0.5, 6.5)), 1, 6).astype(int))\n",
        "    print(f\"[v3-large Fold {f}] val round-QWK={q:.4f} elapsed={(time.time()-f_start)/60:.1f} min\", flush=True)\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "np.save('oof_deberta_v3_large_win512.npy', oof_l)\n",
        "opt = ThresholdOptimizer()\n",
        "best_th, best_oof_qwk = opt.fit(train_df['score'].values.astype(int), np.clip(oof_l, 0.5, 6.5))\n",
        "round_q = qwk(train_df['score'].values.astype(int), np.clip(np.rint(oof_l), 1, 6).astype(int))\n",
        "print(f'OOF (partial 3-fold) round-QWK={round_q:.5f}  thresh-QWK={best_oof_qwk:.5f} thresholds={best_th}', flush=True)\n",
        "with open('thresholds_deberta_v3_large_win512.json','w') as f:\n",
        "    json.dump({'thresholds': best_th.tolist(), 'oof_qwk': float(best_oof_qwk), 'round_oof_qwk': float(round_q)}, f)\n",
        "print(f'=== v3-large 3-fold run done in {(time.time()-t0)/60:.1f} min ===', flush=True)\n",
        "print('Next: run inference and blend with TF-IDF; then calibrate global thresholds.', flush=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Pivot: DeBERTa-v3-LARGE sliding windows (512/128) + MSD CLS+Mean, 3-fold, token-weighted eval ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using folds: [0, 1, 2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v3-large Fold 0] Build datasets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v3-large Fold 0] Train: essays=13166 windows=17322 avg_w/E=1.32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v3-large Fold 0] Valid: essays=2410 windows=2961 avg_w/E=1.23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[v3-large Fold 0] Start training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='3246' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/3246 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 2.4715, 'grad_norm': 36.33293914794922, 'learning_rate': 5.7692307692307695e-08, 'epoch': 0.0009236808682600162}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 1.9256, 'grad_norm': 6.323685646057129, 'learning_rate': 2.884615384615385e-06, 'epoch': 0.04618404341300081}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.4093, 'grad_norm': 11.742685317993164, 'learning_rate': 5.76923076923077e-06, 'epoch': 0.09236808682600162}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.3503, 'grad_norm': 5.3757452964782715, 'learning_rate': 8.653846153846153e-06, 'epoch': 0.13855213023900242}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.3012, 'grad_norm': 5.670287609100342, 'learning_rate': 1.153846153846154e-05, 'epoch': 0.18473617365200323}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.2853, 'grad_norm': 12.33273696899414, 'learning_rate': 1.4423076923076924e-05, 'epoch': 0.23092021706500404}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.24509067833423615, 'eval_qwk_round': 0.6841542448418836, 'eval_runtime': 1161.5732, 'eval_samples_per_second': 2.549, 'eval_steps_per_second': 0.319, 'epoch': 0.23092021706500404}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.2538, 'grad_norm': 5.509453296661377, 'learning_rate': 1.4993359400471464e-05, 'epoch': 0.27710426047800485}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 252\u001b[39m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m unique_folds:\n\u001b[32m    251\u001b[39m     f_start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     va_idx, agg_preds = \u001b[43mtrain_fold_v3large\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolds_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdeberta_v3_large_win512\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m     oof_l[va_idx] = agg_preds\n\u001b[32m    254\u001b[39m     y_true = train_df.iloc[va_idx][\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m].values.astype(\u001b[38;5;28mint\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 224\u001b[39m, in \u001b[36mtrain_fold_v3large\u001b[39m\u001b[34m(fold, df, folds, out_dir)\u001b[39m\n\u001b[32m    222\u001b[39m t0 = time.time()\n\u001b[32m    223\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m[v3-large Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Start training...\u001b[39m\u001b[33m'\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[v3-large Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Train done in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time.time()-t0)/\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m min\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# Predict on valid windows and aggregate with token-length weights\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/transformers/trainer.py:1938\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1937\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1938\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/transformers/trainer.py:2279\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2276\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_begin(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2278\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.accumulate(model):\n\u001b[32m-> \u001b[39m\u001b[32m2279\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2281\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2282\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2283\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2284\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2285\u001b[39m ):\n\u001b[32m   2286\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2287\u001b[39m     tr_loss += tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/transformers/trainer.py:3349\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3347\u001b[39m         scaled_loss.backward()\n\u001b[32m   3348\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3349\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3351\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach() / \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/accelerate/accelerator.py:2196\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2194\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2195\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2196\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/_tensor.py:521\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    513\u001b[39m         Tensor.backward,\n\u001b[32m    514\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    519\u001b[39m         inputs=inputs,\n\u001b[32m    520\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/autograd/__init__.py:289\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    284\u001b[39m     retain_graph = create_graph\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/autograd/graph.py:769\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    767\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    768\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    770\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/autograd/function.py:291\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBackwardCFunction\u001b[39;00m(_C._FunctionBase, FunctionCtx, _HookMixin):\n\u001b[32m    287\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[33;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m    292\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[33;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[33;03m        \"\"\"\u001b[39;00m\n\u001b[32m    295\u001b[39m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[32m    296\u001b[39m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "fdb7eedd-be89-4005-b468-7c834c0da009",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, glob, json, gc, time\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "print('=== Inference + Blending: v3-large + TF-IDF ===', flush=True)\n",
        "\n",
        "def qwk(y_true, y_pred_int):\n",
        "    return cohen_kappa_score(y_true, y_pred_int, weights='quadratic')\n",
        "\n",
        "class ThresholdOptimizer:\n",
        "    def __init__(self, init_thresholds=None):\n",
        "        self.thresholds = np.array(init_thresholds if init_thresholds is not None else [1.5,2.5,3.5,4.5,5.5], dtype=float)\n",
        "    def _apply(self, preds):\n",
        "        return np.digitize(preds, self.thresholds) + 1\n",
        "    def fit(self, y_true, preds, iters=200, step=0.05):\n",
        "        best = self.thresholds.copy(); best_score = qwk(y_true, self._apply(preds))\n",
        "        for _ in range(iters):\n",
        "            improved = False\n",
        "            for i in range(5):\n",
        "                for d in (-step, step):\n",
        "                    cand = np.sort(np.clip(best + (np.arange(5)==i)*d, 0.5, 6.5))\n",
        "                    if not (0.5 < cand[0] < cand[1] < cand[2] < cand[3] < cand[4] < 6.5):\n",
        "                        continue\n",
        "                    s = qwk(y_true, np.digitize(preds, cand) + 1)\n",
        "                    if s > best_score:\n",
        "                        best_score, best, improved = s, cand, True\n",
        "            if not improved:\n",
        "                step *= 0.5\n",
        "                if step < 1e-4: break\n",
        "        self.thresholds = best\n",
        "        return best, best_score\n",
        "\n",
        "# Test-time inference for v3-large CLS+Mean MSD model\n",
        "MODEL_NAME_L = 'microsoft/deberta-v3-large'\n",
        "MAX_LEN_L = 512\n",
        "STRIDE_L = 128\n",
        "BATCH_EVAL_L = 8\n",
        "OUT_DIR_L = 'deberta_v3_large_win512'\n",
        "tokenizer_inf_l = AutoTokenizer.from_pretrained(MODEL_NAME_L, use_fast=True)\n",
        "\n",
        "class WindowDatasetTestL(Dataset):\n",
        "    def __init__(self, df, text_col='full_text'):\n",
        "        self.essay_ids = []\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        self.lengths = []\n",
        "        texts = df[text_col].astype(str).tolist()\n",
        "        eids = df['essay_id'].tolist()\n",
        "        enc = tokenizer_inf_l(texts,\n",
        "                              max_length=MAX_LEN_L,\n",
        "                              truncation=True,\n",
        "                              padding=False,\n",
        "                              return_overflowing_tokens=True,\n",
        "                              stride=STRIDE_L,\n",
        "                              return_attention_mask=True)\n",
        "        overflow_to_sample = enc.pop('overflow_to_sample_mapping')\n",
        "        for idx, sample_idx in enumerate(overflow_to_sample):\n",
        "            self.essay_ids.append(eids[sample_idx])\n",
        "            ids_i = enc['input_ids'][idx]\n",
        "            attn_i = enc['attention_mask'][idx]\n",
        "            self.input_ids.append(ids_i)\n",
        "            self.attn_masks.append(attn_i)\n",
        "            self.lengths.append(int(sum(attn_i)))\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "    def __getitem__(self, i):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.input_ids[i], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.attn_masks[i], dtype=torch.long),\n",
        "            'essay_id': self.essay_ids[i],\n",
        "            'length': self.lengths[i],\n",
        "        }\n",
        "\n",
        "class PadCollatorTTL:\n",
        "    def __init__(self):\n",
        "        self.pad = DataCollatorWithPadding(tokenizer=tokenizer_inf_l, pad_to_multiple_of=8)\n",
        "    def __call__(self, features):\n",
        "        for f in features:\n",
        "            f.pop('essay_id', None); f.pop('length', None)\n",
        "        return self.pad(features)\n",
        "\n",
        "class MSDCLSMeanRegressor(nn.Module):\n",
        "    def __init__(self, model_name, hidden_size=1024, msd=5, p=0.2):\n",
        "        super().__init__()\n",
        "        self.backbone = AutoModel.from_pretrained(model_name)\n",
        "        self.dropouts = nn.ModuleList([nn.Dropout(p) for _ in range(msd)])\n",
        "        self.head = nn.Linear(hidden_size*2, 1)\n",
        "        self.msd = msd\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
        "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last = out.last_hidden_state\n",
        "        cls = last[:, 0, :]\n",
        "        mask = attention_mask.unsqueeze(-1).float()\n",
        "        mean = (last * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-6)\n",
        "        feat = torch.cat([cls, mean], dim=-1)\n",
        "        logits = 0.0\n",
        "        for dp in self.dropouts:\n",
        "            logits = logits + self.head(dp(feat)).squeeze(-1)\n",
        "        logits = logits / float(self.msd)\n",
        "        return {'logits': logits.unsqueeze(-1)}\n",
        "\n",
        "def load_best_subdir(folder):\n",
        "    cks = sorted(glob.glob(os.path.join(folder, 'checkpoint-*')), key=lambda p: int(p.split('-')[-1]))\n",
        "    return cks[-1] if cks else folder\n",
        "\n",
        "def predict_fold_large(folder, dtest):\n",
        "    best_dir = load_best_subdir(folder)\n",
        "    model = MSDCLSMeanRegressor(MODEL_NAME_L, hidden_size=1024, msd=5, p=0.2)\n",
        "    sd_path = os.path.join(best_dir, 'pytorch_model.bin')\n",
        "    model.load_state_dict(torch.load(sd_path, map_location='cpu'))\n",
        "    args = TrainingArguments(output_dir=os.path.join(folder, 'tmp_infer'), per_device_eval_batch_size=BATCH_EVAL_L,\n",
        "                             dataloader_num_workers=2, dataloader_pin_memory=True, report_to=[], fp16=True)\n",
        "    trainer = Trainer(model=model, args=args, tokenizer=tokenizer_inf_l, data_collator=PadCollatorTTL())\n",
        "    preds = trainer.predict(dtest).predictions.squeeze()\n",
        "    ids = np.array(dtest.essay_ids)\n",
        "    lens = np.array(dtest.lengths, dtype=float)\n",
        "    by_sum, by_w = defaultdict(float), defaultdict(float)\n",
        "    for p, i, w in zip(preds, ids, lens):\n",
        "        by_sum[i] += float(p) * float(w)\n",
        "        by_w[i] += float(w)\n",
        "    agg = {i: (by_sum[i] / max(by_w[i], 1e-6)) for i in by_sum.keys()}\n",
        "    return agg\n",
        "\n",
        "def run_test_inference_large():\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    dtest = WindowDatasetTestL(test_df)\n",
        "    fold_dirs = sorted([p for p in glob.glob(os.path.join(OUT_DIR_L, 'fold*')) if os.path.isdir(p)])\n",
        "    all_fold_preds = []\n",
        "    t0 = time.time()\n",
        "    for fd in fold_dirs:\n",
        "        t1 = time.time()\n",
        "        agg = predict_fold_large(fd, dtest)\n",
        "        all_fold_preds.append(agg)\n",
        "        print(f'[v3-large inference] {fd} done in {(time.time()-t1)/60:.1f} min', flush=True)\n",
        "        torch.cuda.empty_cache(); gc.collect()\n",
        "    test_ids_order = test_df['essay_id'].tolist()\n",
        "    preds_mat = []\n",
        "    for agg in all_fold_preds:\n",
        "        preds_mat.append([agg[e] for e in test_ids_order])\n",
        "    preds_mean = np.mean(np.array(preds_mat), axis=0)\n",
        "    preds_mean = np.clip(preds_mean, 0.5, 6.5)\n",
        "    np.save('test_deberta_v3_large_win512.npy', preds_mean)\n",
        "    print('[v3-large inference] Saved test_deberta_v3_large_win512.npy', flush=True)\n",
        "    return preds_mean\n",
        "\n",
        "def optimize_blend_and_submit():\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    y_true = train_df['score'].values.astype(int)\n",
        "    # Load OOFs\n",
        "    oof_large = np.load('oof_deberta_v3_large_win512.npy') if os.path.exists('oof_deberta_v3_large_win512.npy') else None\n",
        "    oof_tfidf = np.load('oof_tfidf.npy')\n",
        "    assert oof_large is not None, 'oof_deberta_v3_large_win512.npy not found'\n",
        "    oof_large = np.clip(oof_large, 0.5, 6.5)\n",
        "    oof_tfidf = np.clip(oof_tfidf, 0.5, 6.5)\n",
        "    # Grid search blend weight\n",
        "    best = (-1.0, 0.0, [1.5,2.5,3.5,4.5,5.5])  # (qwk, w, th)\n",
        "    for w in np.linspace(0.6, 0.95, 15):\n",
        "        blend = w * oof_large + (1.0 - w) * oof_tfidf\n",
        "        opt = ThresholdOptimizer()\n",
        "        th, q = opt.fit(y_true, blend.copy(), iters=200, step=0.05)\n",
        "        if q > best[0]:\n",
        "            best = (q, float(w), th)\n",
        "    best_q, best_w, best_th = best\n",
        "    print(f'[Blend] Best OOF thresh-QWK={best_q:.5f} at w={best_w:.3f} thresholds={best_th}', flush=True)\n",
        "    with open('blend_params_large_tfidf.json','w') as f:\n",
        "        json.dump({'weight_large': best_w, 'thresholds': [float(x) for x in best_th], 'oof_qwk': float(best_q)}, f)\n",
        "    # Test preds\n",
        "    if os.path.exists('test_deberta_v3_large_win512.npy'):\n",
        "        test_large = np.load('test_deberta_v3_large_win512.npy')\n",
        "    else:\n",
        "        test_large = run_test_inference_large()\n",
        "    test_tfidf = np.load('test_tfidf.npy')\n",
        "    test_blend = best_w * test_large + (1.0 - best_w) * test_tfidf\n",
        "    test_blend = np.clip(test_blend, 0.5, 6.5)\n",
        "    th = np.array(best_th, dtype=float)\n",
        "    labels = np.digitize(test_blend, th) + 1\n",
        "    labels = np.clip(labels, 1, 6).astype(int)\n",
        "    sub = pd.DataFrame({'essay_id': pd.read_csv('test.csv')['essay_id'], 'score': labels})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (blended v3-large + TF-IDF)', flush=True)\n",
        "\n",
        "print('=== Inference+Blend cell ready. After training finishes, run optimize_blend_and_submit() ===', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "9dddd1fc-103b-41e7-90ae-67bd668dd4a2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time, json, numpy as np, pandas as pd\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from collections import defaultdict\n",
        "\n",
        "print('=== Seed-3 RUN: DeBERTa-v3-base (LLRD+EMA), 3 folds (k16), sliding 512/128 ===', flush=True)\n",
        "\n",
        "def qwk(y_true, y_pred_int):\n",
        "    return cohen_kappa_score(y_true, y_pred_int, weights='quadratic')\n",
        "\n",
        "class ThresholdOptimizer:\n",
        "    def __init__(self, init_thresholds=None):\n",
        "        self.thresholds = np.array(init_thresholds if init_thresholds is not None else [1.5,2.5,3.5,4.5,5.5], dtype=float)\n",
        "    def _apply(self, preds):\n",
        "        return np.digitize(preds, self.thresholds) + 1\n",
        "    def fit(self, y_true, preds, iters=200, step=0.05):\n",
        "        best = self.thresholds.copy(); best_score = qwk(y_true, self._apply(preds))\n",
        "        for _ in range(iters):\n",
        "            improved = False\n",
        "            for i in range(5):\n",
        "                for d in (-step, step):\n",
        "                    cand = np.sort(np.clip(best + (np.arange(5)==i)*d, 0.5, 6.5))\n",
        "                    if not (0.5 < cand[0] < cand[1] < cand[2] < cand[3] < cand[4] < 6.5):\n",
        "                        continue\n",
        "                    s = qwk(y_true, np.digitize(preds, cand) + 1)\n",
        "                    if s > best_score:\n",
        "                        best_score, best, improved = s, cand, True\n",
        "            if not improved:\n",
        "                step *= 0.5\n",
        "                if step < 1e-4: break\n",
        "        self.thresholds = best\n",
        "        return best, best_score\n",
        "\n",
        "# Local, unambiguous version of seed-3 fold trainer to avoid class collisions\n",
        "def train_fold_seed3_local(fold, df, folds, out_dir='deberta_v3_base_win512_seed3_llrd_ema',\n",
        "                           base_lr=2e-5, head_lr_mult=2.5, decay=0.9, ema_decay=0.995):\n",
        "    import os, time, gc, torch\n",
        "    from transformers import Trainer, TrainingArguments, PrinterCallback, EarlyStoppingCallback\n",
        "    # Build datasets from global WindowDataset\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    tr_idx = np.where(folds != fold)[0]\n",
        "    va_idx = np.where(folds == fold)[0]\n",
        "    print(f'[Seed3 Fold {fold}] Building datasets...', flush=True)\n",
        "    dtrain = WindowDataset(df.iloc[tr_idx], labels=df.iloc[tr_idx]['score'].values.astype(np.float32))\n",
        "    dvalid = WindowDataset(df.iloc[va_idx], labels=df.iloc[va_idx]['score'].values.astype(np.float32))\n",
        "    _log_windows_stats(f'[Seed3 Fold {fold}] Train', dtrain)\n",
        "    _log_windows_stats(f'[Seed3 Fold {fold}] Valid', dvalid)\n",
        "    # Build model using the unique Seed3 class defined in cell 13\n",
        "    model = MSDMeanPoolRegressorSeed3('microsoft/deberta-v3-base', msd=5, p=0.2)\n",
        "    # LLRD param groups from global function\n",
        "    pg = build_llrd_param_groups(model, base_lr=base_lr, head_lr_mult=head_lr_mult, decay=decay, weight_decay=0.01)\n",
        "    class LLRDTrainer(Trainer):\n",
        "        def create_optimizer(self):\n",
        "            if self.optimizer is not None:\n",
        "                return\n",
        "            optim_kwargs = {'lr': base_lr, 'betas': (0.9, 0.999), 'eps': 1e-8, 'weight_decay': 0.01}\n",
        "            try:\n",
        "                from torch.optim import AdamW\n",
        "                self.optimizer = AdamW(pg, **optim_kwargs)\n",
        "            except Exception:\n",
        "                self.optimizer = torch.optim.AdamW(pg, **optim_kwargs)\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"{out_dir}/fold{fold}\",\n",
        "        learning_rate=base_lr,\n",
        "        per_device_train_batch_size=max(4, BATCH_TRAIN),\n",
        "        per_device_eval_batch_size=BATCH_EVAL,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        fp16=True,\n",
        "        bf16=False,\n",
        "        evaluation_strategy='steps',\n",
        "        save_strategy='steps',\n",
        "        eval_steps=400,\n",
        "        save_steps=400,\n",
        "        save_total_limit=1,\n",
        "        logging_strategy='steps',\n",
        "        logging_steps=50,\n",
        "        logging_first_step=True,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='qwk_round',\n",
        "        greater_is_better=True,\n",
        "        report_to=[],\n",
        "        disable_tqdm=False,\n",
        "        dataloader_num_workers=0,\n",
        "        dataloader_pin_memory=True,\n",
        "        dataloader_persistent_workers=False,\n",
        "        group_by_length=True,\n",
        "        gradient_accumulation_steps=4,\n",
        "        lr_scheduler_type='cosine',\n",
        "        warmup_ratio=0.1,\n",
        "        optim='adamw_torch_fused',\n",
        "        eval_accumulation_steps=32,\n",
        "        seed=SEED2+1,\n",
        "        remove_unused_columns=False,\n",
        "        max_grad_norm=1.0,\n",
        "    )\n",
        "    ema_cb = EMACallback(ema_decay=ema_decay)\n",
        "    trainer = LLRDTrainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=dtrain,\n",
        "        eval_dataset=dvalid,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=PadCollator(),\n",
        "        compute_metrics=compute_metrics_factory_token_weighted(dvalid.essay_ids, dvalid.lengths),\n",
        "        callbacks=[PrinterCallback(), ema_cb, EarlyStoppingCallback(early_stopping_patience=1, early_stopping_threshold=0.0)]\n",
        "    )\n",
        "    print(f'[Seed3 Fold {fold}] Start training...', flush=True)\n",
        "    t0 = time.time()\n",
        "    trainer.train()\n",
        "    print(f\"[Seed3 Fold {fold}] Train done in {(time.time()-t0)/60:.1f} min\", flush=True)\n",
        "    # EMA shadow for final predict\n",
        "    ema_cb.apply_shadow(trainer.model)\n",
        "    preds_val = trainer.predict(dvalid).predictions.squeeze()\n",
        "    ema_cb.restore(trainer.model)\n",
        "    ids = np.array(dvalid.essay_ids)\n",
        "    lens = np.array(dvalid.lengths, dtype=float)\n",
        "    by_sum = defaultdict(float); by_w = defaultdict(float)\n",
        "    for p, i, w in zip(preds_val, ids, lens):\n",
        "        by_sum[i] += float(p) * float(w); by_w[i] += float(w)\n",
        "    agg = {i: (by_sum[i] / max(by_w[i], 1e-6)) for i in by_sum.keys()}\n",
        "    va_idx_list = df.iloc[va_idx]['essay_id'].values.tolist()\n",
        "    agg_vec = np.array([agg[e] for e in va_idx_list], dtype=float)\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "    return va_idx, agg_vec\n",
        "\n",
        "# Use existing folds (k=16 grouped), run only fold 2 with higher LR after poor fold 1\n",
        "t0 = time.time()\n",
        "train_df = pd.read_csv('train.csv')\n",
        "folds_g2 = pd.read_csv('folds_grouped_k16.csv')\n",
        "folds_map2 = dict(zip(folds_g2['essay_id'], folds_g2['fold_grouped_k16']))\n",
        "folds_arr = train_df['essay_id'].map(folds_map2).values.astype(int)\n",
        "# Run only fold 2 now, with base_lr bumped to 2.8e-5\n",
        "use_folds = [2]\n",
        "print('Using folds:', use_folds, '(base_lr=2.8e-5 for this run)', flush=True)\n",
        "\n",
        "oof_seed3 = np.zeros(len(train_df), dtype=float)\n",
        "for f in use_folds:\n",
        "    f_start = time.time()\n",
        "    va_idx, agg_preds = train_fold_seed3_local(f, train_df, folds_arr, out_dir='deberta_v3_base_win512_seed3_llrd_ema',\n",
        "                                               base_lr=2.8e-5, head_lr_mult=2.5, decay=0.9, ema_decay=0.995)\n",
        "    oof_seed3[va_idx] = agg_preds\n",
        "    y_true = train_df.iloc[va_idx]['score'].values.astype(int)\n",
        "    q = qwk(y_true, np.clip(np.rint(np.clip(agg_preds, 0.5, 6.5)), 1, 6).astype(int))\n",
        "    print(f\"[Seed3 Fold {f}] val round-QWK={q:.4f} elapsed={(time.time()-f_start)/60:.1f} min\", flush=True)\n",
        "\n",
        "np.save('oof_deberta_v3_base_win512_seed3_llrd_ema.npy', oof_seed3)\n",
        "opt = ThresholdOptimizer()\n",
        "best_th, best_oof_qwk = opt.fit(train_df['score'].values.astype(int), np.clip(oof_seed3, 0.5, 6.5))\n",
        "round_q = qwk(train_df['score'].values.astype(int), np.clip(np.rint(oof_seed3), 1, 6).astype(int))\n",
        "print(f'OOF (partial) round-QWK={round_q:.5f}  thresh-QWK={best_oof_qwk:.5f} thresholds={best_th}', flush=True)\n",
        "with open('thresholds_deberta_v3_base_win512_seed3_llrd_ema.json','w') as f:\n",
        "    json.dump({'thresholds': best_th.tolist(), 'oof_qwk': float(best_oof_qwk), 'round_oof_qwk': float(round_q)}, f)\n",
        "print(f'=== Seed-3 partial run done in {(time.time()-t0)/60:.1f} min ===', flush=True)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Seed-3 RUN: DeBERTa-v3-base (LLRD+EMA), 3 folds (k16), sliding 512/128 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using folds: [2] (base_lr=2.8e-5 for this run)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed3 Fold 2] Building datasets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed3 Fold 2] Train: essays=13306 windows=17042 avg_windows_per_essay=1.28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed3 Fold 2] Valid: essays=2270 windows=3241 avg_windows_per_essay=1.43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Seed3 Fold 2] Start training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='3195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/3195 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 4.4972, 'grad_norm': 28.20829200744629, 'learning_rate': 2.2241325997878757e-08, 'epoch': 0.0009387467730579676}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 2.8793, 'grad_norm': 28.798147201538086, 'learning_rate': 1.1120662998939378e-06, 'epoch': 0.04693733865289838}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6369, 'grad_norm': 23.43297576904297, 'learning_rate': 2.2241325997878755e-06, 'epoch': 0.09387467730579677}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.3431, 'grad_norm': 6.04536247253418, 'learning_rate': 3.336198899681813e-06, 'epoch': 0.14081201595869514}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.361, 'grad_norm': 3.446148633956909, 'learning_rate': 4.448265199575751e-06, 'epoch': 0.18774935461159353}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.2832, 'grad_norm': 4.831424713134766, 'learning_rate': 5.560331499469689e-06, 'epoch': 0.2346866932644919}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.2854, 'grad_norm': 13.580657005310059, 'learning_rate': 6.672397799363626e-06, 'epoch': 0.28162403191739027}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.3858, 'grad_norm': 17.343822479248047, 'learning_rate': 7.115312361416194e-06, 'epoch': 0.32856137057028867}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.3865, 'grad_norm': 10.519705772399902, 'learning_rate': 7.1036356135517416e-06, 'epoch': 0.37549870922318707}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4877200126647949, 'eval_qwk_round': 0.38282240052751304, 'eval_runtime': 402.4138, 'eval_samples_per_second': 8.054, 'eval_steps_per_second': 1.009, 'epoch': 0.37549870922318707}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 139\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m use_folds:\n\u001b[32m    138\u001b[39m     f_start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     va_idx, agg_preds = \u001b[43mtrain_fold_seed3_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolds_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdeberta_v3_base_win512_seed3_llrd_ema\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mbase_lr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2.8e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_lr_mult\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mema_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.995\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m     oof_seed3[va_idx] = agg_preds\n\u001b[32m    142\u001b[39m     y_true = train_df.iloc[va_idx][\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m].values.astype(\u001b[38;5;28mint\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mtrain_fold_seed3_local\u001b[39m\u001b[34m(fold, df, folds, out_dir, base_lr, head_lr_mult, decay, ema_decay)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m[Seed3 Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Start training...\u001b[39m\u001b[33m'\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    108\u001b[39m t0 = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Seed3 Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Train done in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time.time()-t0)/\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m min\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# EMA shadow for final predict\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/transformers/trainer.py:1938\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1937\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1938\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/transformers/trainer.py:2279\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2276\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_begin(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2278\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.accumulate(model):\n\u001b[32m-> \u001b[39m\u001b[32m2279\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2281\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2282\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2283\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2284\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2285\u001b[39m ):\n\u001b[32m   2286\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2287\u001b[39m     tr_loss += tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/transformers/trainer.py:3349\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3347\u001b[39m         scaled_loss.backward()\n\u001b[32m   3348\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3349\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3351\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach() / \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/accelerate/accelerator.py:2196\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2194\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2195\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2196\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/_tensor.py:521\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    513\u001b[39m         Tensor.backward,\n\u001b[32m    514\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    519\u001b[39m         inputs=inputs,\n\u001b[32m    520\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/autograd/__init__.py:289\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    284\u001b[39m     retain_graph = create_graph\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/autograd/graph.py:769\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    767\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    768\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    770\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/autograd/function.py:291\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBackwardCFunction\u001b[39;00m(_C._FunctionBase, FunctionCtx, _HookMixin):\n\u001b[32m    287\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[33;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m    292\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[33;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[33;03m        \"\"\"\u001b[39;00m\n\u001b[32m    295\u001b[39m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[32m    296\u001b[39m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "56268b6b-8c36-4024-97af-620fcc6edc3a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "STRIDE = 128\n",
        "print('Sliding-window STRIDE reset to 128 for subsequent dataset builds.', flush=True)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sliding-window STRIDE reset to 128 for subsequent dataset builds.\n"
          ]
        }
      ]
    },
    {
      "id": "705c0fbf-7e12-4c44-8948-0fa0a4a91ebb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, glob, json, time, gc\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "print('=== Seed-3 Inference (token-weighted) + Blend with TF-IDF ===', flush=True)\n",
        "\n",
        "def qwk(y_true, y_pred_int):\n",
        "    return cohen_kappa_score(y_true, y_pred_int, weights='quadratic')\n",
        "\n",
        "class ThresholdOptimizer:\n",
        "    def __init__(self, init_thresholds=None):\n",
        "        self.thresholds = np.array(init_thresholds if init_thresholds is not None else [1.5,2.5,3.5,4.5,5.5], dtype=float)\n",
        "    def _apply(self, preds):\n",
        "        return np.digitize(preds, self.thresholds) + 1\n",
        "    def fit(self, y_true, preds, iters=200, step=0.05):\n",
        "        best = self.thresholds.copy(); best_score = qwk(y_true, self._apply(preds))\n",
        "        for _ in range(iters):\n",
        "            improved = False\n",
        "            for i in range(5):\n",
        "                for d in (-step, step):\n",
        "                    cand = np.sort(np.clip(best + (np.arange(5)==i)*d, 0.5, 6.5))\n",
        "                    if not (0.5 < cand[0] < cand[1] < cand[2] < cand[3] < cand[4] < 6.5):\n",
        "                        continue\n",
        "                    s = qwk(y_true, np.digitize(preds, cand) + 1)\n",
        "                    if s > best_score:\n",
        "                        best_score, best, improved = s, cand, True\n",
        "            if not improved:\n",
        "                step *= 0.5\n",
        "                if step < 1e-4: break\n",
        "        self.thresholds = best\n",
        "        return best, best_score\n",
        "\n",
        "# Consistent params with training\n",
        "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
        "MAX_LEN = 512\n",
        "STRIDE = 128\n",
        "BATCH_EVAL = 32\n",
        "OUT_DIR = 'deberta_v3_base_win512_seed3_llrd_ema'\n",
        "\n",
        "tokenizer_seed3 = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "class WindowDatasetTestSeed3(Dataset):\n",
        "    def __init__(self, df, text_col='full_text'):\n",
        "        self.essay_ids = []\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        self.lengths = []  # valid token counts per window\n",
        "        texts = df[text_col].astype(str).tolist()\n",
        "        eids = df['essay_id'].tolist()\n",
        "        enc = tokenizer_seed3(texts, max_length=MAX_LEN, truncation=True, padding=False,\n",
        "                              return_overflowing_tokens=True, stride=STRIDE, return_attention_mask=True)\n",
        "        overflow_to_sample = enc.pop('overflow_to_sample_mapping')\n",
        "        for idx, sample_idx in enumerate(overflow_to_sample):\n",
        "            self.essay_ids.append(eids[sample_idx])\n",
        "            ids_i = enc['input_ids'][idx]\n",
        "            attn_i = enc['attention_mask'][idx]\n",
        "            self.input_ids.append(ids_i)\n",
        "            self.attn_masks.append(attn_i)\n",
        "            self.lengths.append(int(sum(attn_i)))\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "    def __getitem__(self, i):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.input_ids[i], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.attn_masks[i], dtype=torch.long),\n",
        "            'essay_id': self.essay_ids[i],\n",
        "            'length': self.lengths[i],\n",
        "        }\n",
        "\n",
        "class PadCollatorTTSeed3:\n",
        "    def __init__(self):\n",
        "        self.pad = DataCollatorWithPadding(tokenizer=tokenizer_seed3, pad_to_multiple_of=8)\n",
        "    def __call__(self, features):\n",
        "        for f in features:\n",
        "            f.pop('essay_id', None); f.pop('length', None)\n",
        "        return self.pad(features)\n",
        "\n",
        "class MSDMeanPoolRegressorSeed3(nn.Module):\n",
        "    def __init__(self, model_name, msd=5, p=0.2):\n",
        "        super().__init__()\n",
        "        self.backbone = AutoModel.from_pretrained(model_name)\n",
        "        hidden = int(getattr(self.backbone.config, 'hidden_size', 768))\n",
        "        self.dropouts = nn.ModuleList([nn.Dropout(p) for _ in range(msd)])\n",
        "        self.head = nn.Linear(hidden, 1)\n",
        "        self.msd = msd\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
        "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last = out.last_hidden_state\n",
        "        mask = attention_mask.unsqueeze(-1).float()\n",
        "        mean = (last * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-6)\n",
        "        logits = 0.0\n",
        "        for dp in self.dropouts:\n",
        "            logits = logits + self.head(dp(mean)).squeeze(-1)\n",
        "        logits = logits / float(self.msd)\n",
        "        return {'logits': logits.unsqueeze(-1)}\n",
        "\n",
        "def load_best_subdir(folder):\n",
        "    cks = sorted(glob.glob(os.path.join(folder, 'checkpoint-*')), key=lambda p: int(p.split('-')[-1]))\n",
        "    return cks[-1] if cks else folder\n",
        "\n",
        "def predict_fold_seed3(folder, dtest):\n",
        "    best_dir = load_best_subdir(folder)\n",
        "    # Load model weights saved by Trainer\n",
        "    model = MSDMeanPoolRegressorSeed3(MODEL_NAME, msd=5, p=0.2)\n",
        "    sd_path = os.path.join(best_dir, 'pytorch_model.bin')\n",
        "    model.load_state_dict(torch.load(sd_path, map_location='cpu'))\n",
        "    args = TrainingArguments(output_dir=os.path.join(folder, 'tmp_infer'), per_device_eval_batch_size=BATCH_EVAL,\n",
        "                             dataloader_num_workers=0, dataloader_pin_memory=True, report_to=[], fp16=True)\n",
        "    trainer = Trainer(model=model, args=args, tokenizer=tokenizer_seed3, data_collator=PadCollatorTTSeed3())\n",
        "    preds = trainer.predict(dtest).predictions.squeeze()\n",
        "    ids = np.array(dtest.essay_ids)\n",
        "    lens = np.array(dtest.lengths, dtype=float)\n",
        "    by_sum, by_w = defaultdict(float), defaultdict(float)\n",
        "    for p, i, w in zip(preds, ids, lens):\n",
        "        by_sum[i] += float(p) * float(w)\n",
        "        by_w[i] += float(w)\n",
        "    agg = {i: (by_sum[i] / max(by_w[i], 1e-6)) for i in by_sum.keys()}\n",
        "    return agg\n",
        "\n",
        "def run_test_inference_seed3():\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    dtest = WindowDatasetTestSeed3(test_df)\n",
        "    fold_dirs = sorted([p for p in glob.glob(os.path.join(OUT_DIR, 'fold*')) if os.path.isdir(p)])\n",
        "    assert fold_dirs, f'No fold dirs found in {OUT_DIR}'\n",
        "    all_fold_preds = []\n",
        "    t0 = time.time()\n",
        "    for fd in fold_dirs:\n",
        "        t1 = time.time()\n",
        "        agg = predict_fold_seed3(fd, dtest)\n",
        "        all_fold_preds.append(agg)\n",
        "        print(f'[seed3 inference] {fd} done in {(time.time()-t1)/60:.1f} min', flush=True)\n",
        "        torch.cuda.empty_cache(); gc.collect()\n",
        "    test_ids_order = test_df['essay_id'].tolist()\n",
        "    preds_mat = []\n",
        "    for agg in all_fold_preds:\n",
        "        preds_mat.append([agg[e] for e in test_ids_order])\n",
        "    preds_mean = np.mean(np.array(preds_mat), axis=0)\n",
        "    preds_mean = np.clip(preds_mean, 0.5, 6.5)\n",
        "    np.save('test_deberta_v3_base_win512_seed3_llrd_ema.npy', preds_mean)\n",
        "    print('[seed3 inference] Saved test_deberta_v3_base_win512_seed3_llrd_ema.npy', flush=True)\n",
        "    return preds_mean\n",
        "\n",
        "def optimize_blend_and_submit_seed3_tfidf():\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    y_true = train_df['score'].values.astype(int)\n",
        "    # Load OOFs\n",
        "    oof_seed3 = np.load('oof_deberta_v3_base_win512_seed3_llrd_ema.npy')\n",
        "    oof_tfidf = np.load('oof_tfidf.npy')\n",
        "    oof_seed3 = np.clip(oof_seed3, 0.5, 6.5)\n",
        "    oof_tfidf = np.clip(oof_tfidf, 0.5, 6.5)\n",
        "    # Handle partial OOF: optimize on entries with seed3 predictions present\n",
        "    mask = oof_seed3 != 0.0\n",
        "    if mask.sum() == 0:\n",
        "        raise RuntimeError('No non-zero entries in oof_seed3; ensure at least one fold finished.')\n",
        "    y_sub = y_true[mask]\n",
        "    s3_sub = oof_seed3[mask]\n",
        "    tf_sub = oof_tfidf[mask]\n",
        "    print(f'[Blend seed3+tfidf] optimizing on {mask.sum()} / {len(mask)} train rows with seed3 OOF', flush=True)\n",
        "    # Blend weight grid per expert advice\n",
        "    best = (-1.0, 0.0, [1.5,2.5,3.5,4.5,5.5])\n",
        "    for w in np.linspace(0.75, 0.92, 18):\n",
        "        blend = w * s3_sub + (1.0 - w) * tf_sub\n",
        "        opt = ThresholdOptimizer()\n",
        "        th, q = opt.fit(y_sub, blend.copy(), iters=200, step=0.05)\n",
        "        if q > best[0]:\n",
        "            best = (q, float(w), th)\n",
        "    best_q, best_w, best_th = best\n",
        "    print(f'[Blend seed3+tfidf] Best OOF thresh-QWK={best_q:.5f} at w={best_w:.3f} thresholds={best_th}', flush=True)\n",
        "    with open('blend_params_seed3_tfidf.json','w') as f:\n",
        "        json.dump({'weight_seed3': best_w, 'thresholds': [float(x) for x in best_th], 'oof_qwk': float(best_q)}, f)\n",
        "    # Test preds\n",
        "    if os.path.exists('test_deberta_v3_base_win512_seed3_llrd_ema.npy'):\n",
        "        test_seed3 = np.load('test_deberta_v3_base_win512_seed3_llrd_ema.npy')\n",
        "    else:\n",
        "        test_seed3 = run_test_inference_seed3()\n",
        "    test_tfidf = np.load('test_tfidf.npy')\n",
        "    test_blend = best_w * test_seed3 + (1.0 - best_w) * test_tfidf\n",
        "    test_blend = np.clip(test_blend, 0.5, 6.5)\n",
        "    th = np.array(best_th, dtype=float)\n",
        "    labels = np.digitize(test_blend, th) + 1\n",
        "    labels = np.clip(labels, 1, 6).astype(int)\n",
        "    sub = pd.DataFrame({'essay_id': pd.read_csv('test.csv')['essay_id'], 'score': labels})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (blended seed3 + TF-IDF)', flush=True)\n",
        "\n",
        "print('Inference+Blend (seed3+tfidf) cell ready. After training finishes, run:')\n",
        "print(' - run_test_inference_seed3()')\n",
        "print(' - optimize_blend_and_submit_seed3_tfidf()')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "3706ea4a-fd71-4623-8497-36fdca6463a4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, glob, json, time, gc\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "print('=== Blend existing OOFs (TF-IDF + DeBERTa base 1024 + optional v3-base win512 if test preds exist) ===', flush=True)\n",
        "\n",
        "def qwk(y_true, y_pred_int):\n",
        "    return cohen_kappa_score(y_true, y_pred_int, weights='quadratic')\n",
        "\n",
        "class ThresholdOptimizer:\n",
        "    def __init__(self, init_thresholds=None):\n",
        "        self.thresholds = np.array(init_thresholds if init_thresholds is not None else [1.5,2.5,3.5,4.5,5.5], dtype=float)\n",
        "    def _apply(self, preds):\n",
        "        return np.digitize(preds, self.thresholds) + 1\n",
        "    def fit(self, y_true, preds, iters=200, step=0.05):\n",
        "        best = self.thresholds.copy(); best_score = qwk(y_true, self._apply(preds))\n",
        "        for _ in range(iters):\n",
        "            improved = False\n",
        "            for i in range(5):\n",
        "                for d in (-step, step):\n",
        "                    cand = np.sort(np.clip(best + (np.arange(5)==i)*d, 0.5, 6.5))\n",
        "                    if not (0.5 < cand[0] < cand[1] < cand[2] < cand[3] < cand[4] < 6.5):\n",
        "                        continue\n",
        "                    s = qwk(y_true, np.digitize(preds, cand) + 1)\n",
        "                    if s > best_score:\n",
        "                        best_score, best, improved = s, cand, True\n",
        "            if not improved:\n",
        "                step *= 0.5\n",
        "                if step < 1e-4: break\n",
        "        self.thresholds = best\n",
        "        return best, best_score\n",
        "\n",
        "# ---------- Inference helpers for DeBERTa-base 1024 head+tail ----------\n",
        "MODEL_1024 = 'microsoft/deberta-v3-base'\n",
        "MAX_LEN_1024 = 1024\n",
        "HEAD_FRAC = 0.88\n",
        "BATCH_EVAL_1024 = 16\n",
        "OUT_DIR_1024 = 'deberta_base_1024'\n",
        "tok_1024 = AutoTokenizer.from_pretrained(MODEL_1024, use_fast=True)\n",
        "\n",
        "def encode_head_tail(text):\n",
        "    ids = tok_1024(text, add_special_tokens=False)['input_ids']\n",
        "    keep_total = MAX_LEN_1024 - 3\n",
        "    if len(ids) <= MAX_LEN_1024 - 2:\n",
        "        out = [tok_1024.cls_token_id] + ids + [tok_1024.sep_token_id]\n",
        "    else:\n",
        "        keep_head = int(HEAD_FRAC * keep_total)\n",
        "        keep_tail = keep_total - keep_head\n",
        "        head = ids[:keep_head]\n",
        "        tail = ids[-keep_tail:] if keep_tail > 0 else []\n",
        "        out = [tok_1024.cls_token_id] + head + [tok_1024.sep_token_id] + tail + [tok_1024.sep_token_id]\n",
        "    attn = [1]*len(out)\n",
        "    return {'input_ids': out, 'attention_mask': attn}\n",
        "\n",
        "class HeadTailTestDS(Dataset):\n",
        "    def __init__(self, df, text_col='full_text'):\n",
        "        self.ids = df['essay_id'].tolist()\n",
        "        self.encs = [encode_head_tail(t) for t in df[text_col].astype(str).tolist()]\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, i):\n",
        "        e = self.encs[i]\n",
        "        return {'input_ids': torch.tensor(e['input_ids'], dtype=torch.long),\n",
        "                'attention_mask': torch.tensor(e['attention_mask'], dtype=torch.long),\n",
        "                'essay_id': self.ids[i]}\n",
        "\n",
        "class PadCollator1024:\n",
        "    def __init__(self):\n",
        "        self.pad = DataCollatorWithPadding(tokenizer=tok_1024, pad_to_multiple_of=8)\n",
        "    def __call__(self, features):\n",
        "        for f in features: f.pop('essay_id', None)\n",
        "        return self.pad(features)\n",
        "\n",
        "def load_best_subdir(path_dir):\n",
        "    cks = sorted(glob.glob(os.path.join(path_dir, 'checkpoint-*')), key=lambda p: int(p.split('-')[-1]))\n",
        "    return cks[-1] if cks else path_dir\n",
        "\n",
        "def infer_test_deberta_base_1024():\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    dtest = HeadTailTestDS(test_df)\n",
        "    fold_dirs = sorted([p for p in glob.glob(os.path.join(OUT_DIR_1024, 'fold*')) if os.path.isdir(p)])\n",
        "    assert fold_dirs, f'No fold dirs found in {OUT_DIR_1024}'\n",
        "    preds_folds = []\n",
        "    for fd in fold_dirs:\n",
        "        best_dir = load_best_subdir(fd)\n",
        "        try:\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(best_dir)\n",
        "        except Exception:\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(MODEL_1024, num_labels=1, problem_type='regression')\n",
        "            sd = torch.load(os.path.join(best_dir, 'pytorch_model.bin'), map_location='cpu')\n",
        "            model.load_state_dict(sd)\n",
        "        args = TrainingArguments(output_dir=os.path.join(fd, 'tmp_infer'), per_device_eval_batch_size=BATCH_EVAL_1024,\n",
        "                                 dataloader_num_workers=0, dataloader_pin_memory=True, report_to=[], fp16=True)\n",
        "        trainer = Trainer(model=model, args=args, tokenizer=tok_1024, data_collator=PadCollator1024())\n",
        "        preds = trainer.predict(dtest).predictions.squeeze()\n",
        "        preds_folds.append(preds.astype(float))\n",
        "        torch.cuda.empty_cache(); gc.collect()\n",
        "    preds_mean = np.mean(np.vstack(preds_folds), axis=0)\n",
        "    preds_mean = np.clip(preds_mean, 0.5, 6.5)\n",
        "    np.save('test_deberta_base_1024.npy', preds_mean)\n",
        "    print('[1024 inference] Saved test_deberta_base_1024.npy', flush=True)\n",
        "    return preds_mean\n",
        "\n",
        "# ---------- Blending existing OOFs and producing submission ----------\n",
        "def blend_existing_and_submit():\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    y_true = train_df['score'].values.astype(int)\n",
        "    # Load available OOFs\n",
        "    oof_tfidf = np.load('oof_tfidf.npy')\n",
        "    oof_1024 = np.load('oof_deberta_base_1024.npy') if os.path.exists('oof_deberta_base_1024.npy') else None\n",
        "    # Only include v3 win512 if BOTH OOF and TEST preds exist to keep sources consistent\n",
        "    include_v3 = os.path.exists('oof_deberta_v3_base_win512.npy') and os.path.exists('test_deberta_v3_base_win512.npy')\n",
        "    oof_win512 = np.load('oof_deberta_v3_base_win512.npy') if include_v3 else None\n",
        "    mats = []\n",
        "    names = []\n",
        "    mats.append(np.clip(oof_tfidf, 0.5, 6.5)); names.append('tfidf')\n",
        "    if oof_1024 is not None:\n",
        "        mats.append(np.clip(oof_1024, 0.5, 6.5)); names.append('d1024')\n",
        "    if include_v3 and oof_win512 is not None:\n",
        "        mats.append(np.clip(oof_win512, 0.5, 6.5)); names.append('v3w512')\n",
        "    mats = [m.astype(float) for m in mats]\n",
        "    k = len(mats)\n",
        "    assert k >= 2, 'Need at least two OOF sources to blend'\n",
        "    print('[Blend] sources:', names, flush=True)\n",
        "    # Grid search weights\n",
        "    best = (-1.0, None, [1.5,2.5,3.5,4.5,5.5])\n",
        "    if k == 2:\n",
        "        A, B = mats[0], mats[1]\n",
        "        for w in np.linspace(0.6, 0.95, 36):\n",
        "            blend = w*A + (1.0-w)*B\n",
        "            opt = ThresholdOptimizer()\n",
        "            th, q = opt.fit(y_true, blend.copy(), iters=300, step=0.05)\n",
        "            if q > best[0]: best = (q, (float(w), 1.0-float(w)), th)\n",
        "    else:\n",
        "        A, B, C = mats[:3]\n",
        "        grid = np.linspace(0.1, 0.9, 41)\n",
        "        for w1 in grid[::4]:\n",
        "            for w2 in grid[::4]:\n",
        "                w3 = 1.0 - w1 - w2\n",
        "                if w3 <= 0 or w3 >= 0.9: continue\n",
        "                blend = w1*A + w2*B + w3*C\n",
        "                opt = ThresholdOptimizer()\n",
        "                th, q = opt.fit(y_true, blend.copy(), iters=300, step=0.05)\n",
        "                if q > best[0]: best = (q, (float(w1), float(w2), float(w3)), th)\n",
        "    best_q, best_w, best_th = best\n",
        "    print(f\"[Blend] Best OOF thresh-QWK={best_q:.5f} weights={best_w} thresholds={best_th}\", flush=True)\n",
        "    with open('blend_existing_params.json','w') as f:\n",
        "        json.dump({'weights': best_w, 'thresholds': [float(x) for x in best_th], 'oof_qwk': float(best_q), 'sources': names}, f)\n",
        "\n",
        "    # Test preds for each source\n",
        "    test_tfidf = np.load('test_tfidf.npy')\n",
        "    test_preds = []\n",
        "    test_preds.append(np.clip(test_tfidf, 0.5, 6.5))\n",
        "    te_1024 = np.load('test_deberta_base_1024.npy') if os.path.exists('test_deberta_base_1024.npy') else infer_test_deberta_base_1024()\n",
        "    if 'd1024' in names: test_preds.append(np.clip(te_1024, 0.5, 6.5))\n",
        "    if 'v3w512' in names:\n",
        "        te_w512 = np.load('test_deberta_v3_base_win512.npy')\n",
        "        test_preds.append(np.clip(te_w512, 0.5, 6.5))\n",
        "\n",
        "    # Combine with best weights\n",
        "    test_preds = [p.astype(float) for p in test_preds]\n",
        "    if len(best_w) == 2:\n",
        "        w1, w2 = best_w\n",
        "        test_blend = w1*test_preds[0] + w2*test_preds[1]\n",
        "    else:\n",
        "        w1, w2, w3 = best_w\n",
        "        test_blend = w1*test_preds[0] + w2*test_preds[1] + w3*test_preds[2]\n",
        "    test_blend = np.clip(test_blend, 0.5, 6.5)\n",
        "    th = np.array(best_th, dtype=float)\n",
        "    labels = np.digitize(test_blend, th) + 1\n",
        "    labels = np.clip(labels, 1, 6).astype(int)\n",
        "    sub = pd.DataFrame({'essay_id': pd.read_csv('test.csv')['essay_id'], 'score': labels})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (blend existing models)', flush=True)\n",
        "\n",
        "print('Blend cell ready. After training/inference, run: blend_existing_and_submit()', flush=True)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Blend existing OOFs (TF-IDF + DeBERTa base 1024 + optional v3-base win512 if test preds exist) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blend cell ready. After training/inference, run: blend_existing_and_submit()\n"
          ]
        }
      ]
    },
    {
      "id": "3c193770-0e13-435d-9ac6-562ca0ee77c4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('=== Running blend_existing_and_submit() ===', flush=True)\n",
        "blend_existing_and_submit()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Running blend_existing_and_submit() ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Blend] sources: ['tfidf', 'd1024', 'v3w512']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Blend] Best OOF thresh-QWK=0.80996 weights=(0.5, 0.33999999999999997, 0.16000000000000003) thresholds=[1.84960938 2.68085937 3.4953125  4.21953125 4.9375    ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (blend existing models)\n"
          ]
        }
      ]
    },
    {
      "id": "6068789a-b8ff-42f5-ab04-430196a10615",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('=== Generating test_deberta_v3_base_win512.npy via run_test_inference() ===', flush=True)\n",
        "run_test_inference()\n",
        "print('Done. You can re-run blend (cell 20) to refresh submission with proper v3 win512 test preds.', flush=True)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Generating test_deberta_v3_base_win512.npy via run_test_inference() ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/70 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted deberta_v3_base_win512/fold0 in 4.9 min\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/70 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted deberta_v3_base_win512/fold1 in 4.9 min\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/70 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted deberta_v3_base_win512/fold2 in 4.9 min\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/70 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted deberta_v3_base_win512/fold3 in 4.9 min\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/70 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted deberta_v3_base_win512/fold4 in 4.9 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv and test_deberta_v3_base_win512.npy\n=== Test inference done in 24.6 min ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. You can re-run blend (cell 20) to refresh submission with proper v3 win512 test preds.\n"
          ]
        }
      ]
    },
    {
      "id": "f42b34f0-c071-4bcb-a60f-b8ce0236834e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time, json, numpy as np, pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "print('=== Cluster-wise thresholds for blended preds (k=16 on train TF-IDF->SVD) ===', flush=True)\n",
        "\n",
        "def qwk(y_true, y_pred_int):\n",
        "    return cohen_kappa_score(y_true, y_pred_int, weights='quadratic')\n",
        "\n",
        "class ThresholdOptimizer:\n",
        "    def __init__(self, init_thresholds=None):\n",
        "        self.thresholds = np.array(init_thresholds if init_thresholds is not None else [1.5,2.5,3.5,4.5,5.5], dtype=float)\n",
        "    def _apply(self, preds):\n",
        "        return np.digitize(preds, self.thresholds) + 1\n",
        "    def fit(self, y_true, preds, iters=200, step=0.05):\n",
        "        best = self.thresholds.copy(); best_score = qwk(y_true, self._apply(np.clip(preds,0.5,6.5)))\n",
        "        for _ in range(iters):\n",
        "            improved = False\n",
        "            for i in range(5):\n",
        "                for d in (-step, step):\n",
        "                    cand = np.sort(np.clip(best + (np.arange(5)==i)*d, 0.5, 6.5))\n",
        "                    if not (0.5 < cand[0] < cand[1] < cand[2] < cand[3] < cand[4] < 6.5):\n",
        "                        continue\n",
        "                    s = qwk(y_true, np.digitize(np.clip(preds,0.5,6.5), cand) + 1)\n",
        "                    if s > best_score:\n",
        "                        best_score, best, improved = s, cand, True\n",
        "            if not improved:\n",
        "                step *= 0.5\n",
        "                if step < 1e-4: break\n",
        "        self.thresholds = best\n",
        "        return best, best_score\n",
        "\n",
        "def build_clusters(train_texts, k=16):\n",
        "    tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5, max_features=120000, sublinear_tf=True)\n",
        "    X = tfidf.fit_transform(train_texts.astype(str))\n",
        "    svd = TruncatedSVD(n_components=100, random_state=42)\n",
        "    X_svd = svd.fit_transform(X)\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    cl_train = kmeans.fit_predict(X_svd).astype(int)\n",
        "    return tfidf, svd, kmeans, cl_train\n",
        "\n",
        "def assign_clusters(tfidf, svd, kmeans, texts):\n",
        "    X = tfidf.transform(texts.astype(str))\n",
        "    Xs = svd.transform(X)\n",
        "    return kmeans.predict(Xs).astype(int)\n",
        "\n",
        "def blended_preds_and_weights(y_true, mats):\n",
        "    # mats: list of arrays [N] for OOF sources in fixed order [tfidf, d1024, v3w512?]\n",
        "    mats = [np.clip(m.astype(float), 0.5, 6.5) for m in mats]\n",
        "    k = len(mats)\n",
        "    best = (-1.0, None, np.array([1.5,2.5,3.5,4.5,5.5], dtype=float))\n",
        "    if k == 2:\n",
        "        A, B = mats\n",
        "        for w in np.linspace(0.6, 0.95, 36):\n",
        "            blend = w*A + (1.0-w)*B\n",
        "            opt = ThresholdOptimizer()\n",
        "            th, q = opt.fit(y_true, blend.copy(), iters=300, step=0.05)\n",
        "            if q > best[0]: best = (q, (float(w), 1.0-float(w)), th)\n",
        "        w = best[1]; A, B = mats\n",
        "        oof_blend = w[0]*A + w[1]*B\n",
        "    else:\n",
        "        A, B, C = mats[:3]\n",
        "        grid = np.linspace(0.1, 0.9, 41)\n",
        "        for w1 in grid[::4]:\n",
        "            for w2 in grid[::4]:\n",
        "                w3 = 1.0 - w1 - w2\n",
        "                if w3 <= 0 or w3 >= 0.9: continue\n",
        "                blend = w1*A + w2*B + w3*C\n",
        "                opt = ThresholdOptimizer()\n",
        "                th, q = opt.fit(y_true, blend.copy(), iters=300, step=0.05)\n",
        "                if q > best[0]: best = (q, (float(w1), float(w2), float(w3)), th)\n",
        "        w = best[1]\n",
        "        oof_blend = w[0]*A + w[1]*B + w[2]*C\n",
        "    return np.clip(oof_blend, 0.5, 6.5), best[1], np.array(best[2], dtype=float), float(best[0])\n",
        "\n",
        "def run_cluster_threshold_blend():\n",
        "    t0 = time.time()\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    y_true = train_df['score'].values.astype(int)\n",
        "    # Load OOF/test sources\n",
        "    oof_tfidf = np.load('oof_tfidf.npy')\n",
        "    te_tfidf = np.load('test_tfidf.npy')\n",
        "    mats_oof = [oof_tfidf]\n",
        "    mats_te = [te_tfidf]\n",
        "    if os.path.exists('oof_deberta_base_1024.npy') and os.path.exists('test_deberta_base_1024.npy'):\n",
        "        mats_oof.append(np.load('oof_deberta_base_1024.npy'))\n",
        "        mats_te.append(np.load('test_deberta_base_1024.npy'))\n",
        "    if os.path.exists('oof_deberta_v3_base_win512.npy') and os.path.exists('test_deberta_v3_base_win512.npy'):\n",
        "        mats_oof.append(np.load('oof_deberta_v3_base_win512.npy'))\n",
        "        mats_te.append(np.load('test_deberta_v3_base_win512.npy'))\n",
        "    assert len(mats_oof) >= 2, 'Need at least two sources'\n",
        "    # Optimize global weights on OOF\n",
        "    oof_blend, weights, th_global, q_global = blended_preds_and_weights(y_true, mats_oof)\n",
        "    print(f'[ClusterBlend] Global best OOF thresh-QWK={q_global:.5f}, weights={weights}, thr={th_global}', flush=True)\n",
        "    # Build clusters on train only\n",
        "    tfidf, svd, kmeans, cl_train = build_clusters(train_df['full_text'])\n",
        "    cl_test = assign_clusters(tfidf, svd, kmeans, test_df['full_text'])\n",
        "    # Fit per-cluster thresholds on blended OOF\n",
        "    th_per = {}\n",
        "    oof_lbls_cluster = np.zeros_like(y_true)\n",
        "    for c in range(kmeans.n_clusters):\n",
        "        idx = np.where(cl_train == c)[0]\n",
        "        if len(idx) < 50:\n",
        "            th_per[c] = th_global.copy()\n",
        "            continue\n",
        "        opt = ThresholdOptimizer(th_global.copy())\n",
        "        th_c, q_c = opt.fit(y_true[idx], oof_blend[idx], iters=200, step=0.05)\n",
        "        th_per[c] = th_c\n",
        "    # Evaluate OOF with per-cluster thresholds (sanity)\n",
        "    for c in range(kmeans.n_clusters):\n",
        "        idx = np.where(cl_train == c)[0]\n",
        "        if len(idx) == 0: continue\n",
        "        th = th_per[c]\n",
        "        oof_lbls_cluster[idx] = np.digitize(oof_blend[idx], th) + 1\n",
        "    oof_lbls_cluster = np.clip(oof_lbls_cluster, 1, 6).astype(int)\n",
        "    q_cluster = qwk(y_true, oof_lbls_cluster)\n",
        "    print(f'[ClusterBlend] OOF QWK with per-cluster thresholds={q_cluster:.5f} (vs global {q_global:.5f})', flush=True)\n",
        "    # Build blended test preds using same weights\n",
        "    mats_te = [np.clip(m.astype(float), 0.5, 6.5) for m in mats_te]\n",
        "    if len(weights) == 2:\n",
        "        te_blend = weights[0]*mats_te[0] + weights[1]*mats_te[1]\n",
        "    else:\n",
        "        te_blend = weights[0]*mats_te[0] + weights[1]*mats_te[1] + weights[2]*mats_te[2]\n",
        "    te_blend = np.clip(te_blend, 0.5, 6.5)\n",
        "    # Apply per-cluster thresholds to test\n",
        "    labels = np.zeros(len(test_df), dtype=int)\n",
        "    for c in range(kmeans.n_clusters):\n",
        "        idx = np.where(cl_test == c)[0]\n",
        "        if len(idx) == 0: continue\n",
        "        th = th_per.get(c, th_global)\n",
        "        labels[idx] = np.digitize(te_blend[idx], th) + 1\n",
        "    labels = np.clip(labels, 1, 6).astype(int)\n",
        "    sub = pd.DataFrame({'essay_id': test_df['essay_id'], 'score': labels})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    with open('blend_existing_params.json','r') as f:\n",
        "        prev = json.load(f) if os.path.exists('blend_existing_params.json') else {}\n",
        "    meta = {'weights': weights, 'global_thresholds': th_global.tolist(), 'cluster_qwk': float(q_cluster), 'global_qwk': float(q_global)}\n",
        "    with open('blend_clusterwise_params.json','w') as f:\n",
        "        json.dump(meta, f)\n",
        "    print('Saved submission.csv (cluster-wise thresholds) and blend_clusterwise_params.json', flush=True)\n",
        "    print(f'=== Cluster-wise blend done in {(time.time()-t0)/60:.1f} min ===', flush=True)\n",
        "\n",
        "run_cluster_threshold_blend()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Cluster-wise thresholds for blended preds (k=16 on train TF-IDF->SVD) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ClusterBlend] Global best OOF thresh-QWK=0.80996, weights=(0.5, 0.33999999999999997, 0.16000000000000003), thr=[1.84960938 2.68085937 3.4953125  4.21953125 4.9375    ]\n"
          ]
        }
      ]
    },
    {
      "id": "64895c87-48c6-4604-b85e-06d9dd1fb35b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time, json, numpy as np, pandas as pd\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "print('=== Length-bin threshold calibration for blended preds ===', flush=True)\n",
        "\n",
        "def qwk(y_true, y_pred_int):\n",
        "    return cohen_kappa_score(y_true, y_pred_int, weights='quadratic')\n",
        "\n",
        "class ThresholdOptimizer:\n",
        "    def __init__(self, init_thresholds=None):\n",
        "        self.thresholds = np.array(init_thresholds if init_thresholds is not None else [1.5,2.5,3.5,4.5,5.5], dtype=float)\n",
        "    def _apply(self, preds):\n",
        "        return np.digitize(preds, self.thresholds) + 1\n",
        "    def fit(self, y_true, preds, iters=200, step=0.05):\n",
        "        preds = np.clip(preds, 0.5, 6.5)\n",
        "        best = self.thresholds.copy(); best_score = qwk(y_true, self._apply(preds))\n",
        "        for _ in range(iters):\n",
        "            improved = False\n",
        "            for i in range(5):\n",
        "                for d in (-step, step):\n",
        "                    cand = np.sort(np.clip(best + (np.arange(5)==i)*d, 0.5, 6.5))\n",
        "                    if not (0.5 < cand[0] < cand[1] < cand[2] < cand[3] < cand[4] < 6.5):\n",
        "                        continue\n",
        "                    s = qwk(y_true, np.digitize(preds, cand) + 1)\n",
        "                    if s > best_score:\n",
        "                        best_score, best, improved = s, cand, True\n",
        "            if not improved:\n",
        "                step *= 0.5\n",
        "                if step < 1e-4: break\n",
        "        self.thresholds = best\n",
        "        return best, best_score\n",
        "\n",
        "def load_blend_weights_and_sources():\n",
        "    with open('blend_existing_params.json','r') as f:\n",
        "        meta = json.load(f)\n",
        "    weights = meta['weights']\n",
        "    sources = meta.get('sources', ['tfidf','d1024','v3w512'])\n",
        "    if isinstance(weights, list): weights = tuple(weights)\n",
        "    return weights, sources\n",
        "\n",
        "def load_oof_and_test_for_sources(sources):\n",
        "    oofs = []; tests = []\n",
        "    for s in sources:\n",
        "        if s == 'tfidf':\n",
        "            oofs.append(np.load('oof_tfidf.npy'))\n",
        "            tests.append(np.load('test_tfidf.npy'))\n",
        "        elif s == 'd1024':\n",
        "            oofs.append(np.load('oof_deberta_base_1024.npy'))\n",
        "            tests.append(np.load('test_deberta_base_1024.npy'))\n",
        "        elif s == 'v3w512':\n",
        "            oofs.append(np.load('oof_deberta_v3_base_win512.npy'))\n",
        "            tests.append(np.load('test_deberta_v3_base_win512.npy'))\n",
        "        else:\n",
        "            raise ValueError(f'Unknown source {s}')\n",
        "    oofs = [np.clip(x.astype(float), 0.5, 6.5) for x in oofs]\n",
        "    tests = [np.clip(x.astype(float), 0.5, 6.5) for x in tests]\n",
        "    return oofs, tests\n",
        "\n",
        "def blend_with_weights(arrs, weights):\n",
        "    arrs = [a.astype(float) for a in arrs]\n",
        "    if len(weights) == 2:\n",
        "        w1, w2 = weights\n",
        "        return np.clip(w1*arrs[0] + w2*arrs[1], 0.5, 6.5)\n",
        "    elif len(weights) == 3:\n",
        "        w1, w2, w3 = weights\n",
        "        return np.clip(w1*arrs[0] + w2*arrs[1] + w3*arrs[2], 0.5, 6.5)\n",
        "    else:\n",
        "        raise ValueError('weights must be len 2 or 3')\n",
        "\n",
        "def run_length_bin_thresholds(n_bins=6):\n",
        "    t0 = time.time()\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    y_true = train_df['score'].values.astype(int)\n",
        "    # Use character length bins (robust, quick)\n",
        "    len_tr = train_df['full_text'].astype(str).str.len().values.astype(float)\n",
        "    len_te = test_df['full_text'].astype(str).str.len().values.astype(float)\n",
        "    # Load blend config and preds\n",
        "    weights, sources = load_blend_weights_and_sources()\n",
        "    oofs, tests = load_oof_and_test_for_sources(sources)\n",
        "    oof_blend = blend_with_weights(oofs, weights)\n",
        "    te_blend = blend_with_weights(tests, weights)\n",
        "    # Global thresholds as fallback\n",
        "    opt_g = ThresholdOptimizer()\n",
        "    th_global, q_global = opt_g.fit(y_true, oof_blend.copy(), iters=300, step=0.05)\n",
        "    # Build bins on train only\n",
        "    qs = np.linspace(0, 1, n_bins+1)\n",
        "    edges = np.unique(np.quantile(len_tr, qs))\n",
        "    if len(edges) <= 2:\n",
        "        edges = np.unique(np.quantile(len_tr, [0, 0.33, 0.66, 1.0]))\n",
        "    # Fit per-bin thresholds\n",
        "    th_per = []  # list of (lo, hi, thresholds)\n",
        "    oof_labels = np.zeros_like(y_true)\n",
        "    for i in range(len(edges)-1):\n",
        "        lo, hi = edges[i], edges[i+1] + (1e-6 if i == len(edges)-2 else 0.0)\n",
        "        idx = np.where((len_tr >= lo) & (len_tr < hi))[0]\n",
        "        if len(idx) < 100:\n",
        "            th = th_global.copy()\n",
        "        else:\n",
        "            opt = ThresholdOptimizer(th_global.copy())\n",
        "            th, _ = opt.fit(y_true[idx], oof_blend[idx], iters=200, step=0.05)\n",
        "        th_per.append((lo, hi, th))\n",
        "        # assign oof labels for sanity check\n",
        "        if len(idx) > 0:\n",
        "            oof_labels[idx] = np.digitize(oof_blend[idx], th) + 1\n",
        "    oof_labels = np.clip(oof_labels, 1, 6).astype(int)\n",
        "    q_lenbin = qwk(y_true, oof_labels)\n",
        "    print(f'[LenBin] OOF QWK with per-bin thresholds={q_lenbin:.5f} (global={q_global:.5f}) bins={len(th_per)}', flush=True)\n",
        "    # Apply to test\n",
        "    labels = np.zeros(len(test_df), dtype=int)\n",
        "    for lo, hi, th in th_per:\n",
        "        idx = np.where((len_te >= lo) & (len_te < hi))[0]\n",
        "        if len(idx) == 0: continue\n",
        "        labels[idx] = np.digitize(te_blend[idx], th) + 1\n",
        "    labels = np.clip(labels, 1, 6).astype(int)\n",
        "    sub = pd.DataFrame({'essay_id': test_df['essay_id'], 'score': labels})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    meta = {\n",
        "        'weights': list(weights) if isinstance(weights, tuple) else weights,\n",
        "        'sources': sources,\n",
        "        'global_thresholds': opt_g.thresholds.tolist(),\n",
        "        'oof_qwk_global': float(q_global),\n",
        "        'oof_qwk_lenbin': float(q_lenbin),\n",
        "        'bins': [(float(lo), float(hi), [float(x) for x in th]) for (lo,hi,th) in th_per]\n",
        "    }\n",
        "    with open('blend_lenbin_params.json','w') as f:\n",
        "        json.dump(meta, f)\n",
        "    print('Saved submission.csv (length-bin thresholds) and blend_lenbin_params.json', flush=True)\n",
        "    print(f'=== Length-bin calibration done in {(time.time()-t0)/60:.1f} min ===', flush=True)\n",
        "\n",
        "run_length_bin_thresholds(n_bins=6)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "id": "c20d5816-5807-47dd-8016-b38c30004753",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time, json, numpy as np, pandas as pd\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "print('=== Fast affine calibration on blended preds (fixed thresholds) ===', flush=True)\n",
        "\n",
        "def qwk(y_true, y_pred_int):\n",
        "    return cohen_kappa_score(y_true, y_pred_int, weights='quadratic')\n",
        "\n",
        "def load_blend_meta():\n",
        "    with open('blend_existing_params.json','r') as f:\n",
        "        meta = json.load(f)\n",
        "    weights = meta['weights']\n",
        "    if isinstance(weights, list):\n",
        "        weights = tuple(weights)\n",
        "    th = np.array(meta['thresholds'], dtype=float)\n",
        "    sources = meta.get('sources', ['tfidf','d1024','v3w512'])\n",
        "    return weights, th, sources\n",
        "\n",
        "def load_oof_and_test(sources):\n",
        "    oofs, tests = [], []\n",
        "    for s in sources:\n",
        "        if s == 'tfidf':\n",
        "            oofs.append(np.load('oof_tfidf.npy')); tests.append(np.load('test_tfidf.npy'))\n",
        "        elif s == 'd1024':\n",
        "            oofs.append(np.load('oof_deberta_base_1024.npy')); tests.append(np.load('test_deberta_base_1024.npy'))\n",
        "        elif s == 'v3w512':\n",
        "            oofs.append(np.load('oof_deberta_v3_base_win512.npy')); tests.append(np.load('test_deberta_v3_base_win512.npy'))\n",
        "        else:\n",
        "            raise ValueError(f'Unknown source {s}')\n",
        "    oofs = [np.clip(x.astype(float), 0.5, 6.5) for x in oofs]\n",
        "    tests = [np.clip(x.astype(float), 0.5, 6.5) for x in tests]\n",
        "    return oofs, tests\n",
        "\n",
        "def blend(arrs, weights):\n",
        "    arrs = [a.astype(float) for a in arrs]\n",
        "    if len(weights) == 2:\n",
        "        w1, w2 = weights; out = w1*arrs[0] + w2*arrs[1]\n",
        "    else:\n",
        "        w1, w2, w3 = weights; out = w1*arrs[0] + w2*arrs[1] + w3*arrs[2]\n",
        "    return np.clip(out, 0.5, 6.5)\n",
        "\n",
        "def run_affine_calibration():\n",
        "    t0 = time.time()\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    y_true = train_df['score'].values.astype(int)\n",
        "    weights, th, sources = load_blend_meta()\n",
        "    oofs, tests = load_oof_and_test(sources)\n",
        "    oof_blend = blend(oofs, weights)\n",
        "    te_blend = blend(tests, weights)\n",
        "    # Grid over scale (a) and bias (b), keep thresholds fixed\n",
        "    best = (-1.0, 1.0, 0.0)  # (qwk, a, b)\n",
        "    A = np.linspace(0.95, 1.05, 41)\n",
        "    B = np.linspace(-0.10, 0.10, 81)\n",
        "    for a in A[::2]:\n",
        "        xb = np.clip(a*oof_blend, 0.5, 6.5)\n",
        "        for b in B[::2]:\n",
        "            z = np.clip(xb + b, 0.5, 6.5)\n",
        "            pred = np.digitize(z, th) + 1\n",
        "            q = qwk(y_true, pred.astype(int))\n",
        "            if q > best[0]:\n",
        "                best = (float(q), float(a), float(b))\n",
        "    q_best, a_best, b_best = best\n",
        "    print(f'[Affine] Best OOF QWK={q_best:.5f} with a={a_best:.5f}, b={b_best:.5f} (fixed thr)', flush=True)\n",
        "    # Apply to test\n",
        "    zt = np.clip(a_best*te_blend + b_best, 0.5, 6.5)\n",
        "    labels = np.digitize(zt, th) + 1\n",
        "    labels = np.clip(labels, 1, 6).astype(int)\n",
        "    sub = pd.DataFrame({'essay_id': test_df['essay_id'], 'score': labels})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    with open('blend_affine_params.json','w') as f:\n",
        "        json.dump({'a': a_best, 'b': b_best, 'oof_qwk_affine': q_best, 'thresholds': th.tolist(), 'weights': list(weights), 'sources': sources}, f)\n",
        "    print('Saved submission.csv (affine-calibrated blend) and blend_affine_params.json', flush=True)\n",
        "    print(f'=== Affine calibration done in {(time.time()-t0)/60:.1f} min ===', flush=True)\n",
        "\n",
        "run_affine_calibration()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "id": "bb5499a1-0172-4666-9524-1b7d8efdc61b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time, json, numpy as np, pandas as pd\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "print('=== Isotonic calibration on blended preds + threshold re-opt (iters<=80) ===', flush=True)\n",
        "\n",
        "def qwk(y_true, y_pred_int):\n",
        "    return cohen_kappa_score(y_true, y_pred_int, weights='quadratic')\n",
        "\n",
        "class ThresholdOptimizer:\n",
        "    def __init__(self, init_thresholds=None):\n",
        "        self.thresholds = np.array(init_thresholds if init_thresholds is not None else [1.5,2.5,3.5,4.5,5.5], dtype=float)\n",
        "    def _apply(self, preds):\n",
        "        return np.digitize(preds, self.thresholds) + 1\n",
        "    def fit(self, y_true, preds, iters=80, step=0.05):\n",
        "        preds = np.clip(preds, 0.5, 6.5)\n",
        "        best = self.thresholds.copy(); best_score = qwk(y_true, self._apply(preds))\n",
        "        for _ in range(iters):\n",
        "            improved = False\n",
        "            for i in range(5):\n",
        "                for d in (-step, step):\n",
        "                    cand = np.sort(np.clip(best + (np.arange(5)==i)*d, 0.5, 6.5))\n",
        "                    if not (0.5 < cand[0] < cand[1] < cand[2] < cand[3] < cand[4] < 6.5):\n",
        "                        continue\n",
        "                    s = qwk(y_true, np.digitize(preds, cand) + 1)\n",
        "                    if s > best_score:\n",
        "                        best_score, best, improved = s, cand, True\n",
        "            if not improved:\n",
        "                step *= 0.5\n",
        "                if step < 1e-4: break\n",
        "        self.thresholds = best\n",
        "        return best, best_score\n",
        "\n",
        "def load_blend_meta():\n",
        "    with open('blend_existing_params.json','r') as f:\n",
        "        meta = json.load(f)\n",
        "    weights = meta['weights']\n",
        "    if isinstance(weights, list): weights = tuple(weights)\n",
        "    th = np.array(meta['thresholds'], dtype=float)\n",
        "    sources = meta.get('sources', ['tfidf','d1024','v3w512'])\n",
        "    return weights, th, sources, float(meta.get('oof_qwk', 0.0))\n",
        "\n",
        "def load_oof_and_test(sources):\n",
        "    oofs, tests = [], []\n",
        "    for s in sources:\n",
        "        if s == 'tfidf':\n",
        "            oofs.append(np.load('oof_tfidf.npy')); tests.append(np.load('test_tfidf.npy'))\n",
        "        elif s == 'd1024':\n",
        "            oofs.append(np.load('oof_deberta_base_1024.npy')); tests.append(np.load('test_deberta_base_1024.npy'))\n",
        "        elif s == 'v3w512':\n",
        "            oofs.append(np.load('oof_deberta_v3_base_win512.npy')); tests.append(np.load('test_deberta_v3_base_win512.npy'))\n",
        "        else:\n",
        "            raise ValueError(f'Unknown source {s}')\n",
        "    oofs = [np.clip(x.astype(float), 0.5, 6.5) for x in oofs]\n",
        "    tests = [np.clip(x.astype(float), 0.5, 6.5) for x in tests]\n",
        "    return oofs, tests\n",
        "\n",
        "def blend(arrs, weights):\n",
        "    arrs = [a.astype(float) for a in arrs]\n",
        "    if len(weights) == 2:\n",
        "        w1, w2 = weights; out = w1*arrs[0] + w2*arrs[1]\n",
        "    else:\n",
        "        w1, w2, w3 = weights; out = w1*arrs[0] + w2*arrs[1] + w3*arrs[2]\n",
        "    return np.clip(out, 0.5, 6.5)\n",
        "\n",
        "def run_isotonic_calibration():\n",
        "    t0 = time.time()\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    y_true = train_df['score'].values.astype(int)\n",
        "    weights, th_init, sources, q_base = load_blend_meta()\n",
        "    oofs, tests = load_oof_and_test(sources)\n",
        "    oof_blend = blend(oofs, weights)\n",
        "    te_blend = blend(tests, weights)\n",
        "    # Fit isotonic regression (monotonic) mapping blended preds -> target\n",
        "    ir = IsotonicRegression(y_min=0.5, y_max=6.5, increasing=True, out_of_bounds='clip')\n",
        "    ir.fit(oof_blend, y_true.astype(float))\n",
        "    oof_iso = np.clip(ir.transform(oof_blend), 0.5, 6.5)\n",
        "    te_iso = np.clip(ir.transform(te_blend), 0.5, 6.5)\n",
        "    # Re-opt thresholds on isotonic-transformed OOF with iters<=80\n",
        "    opt = ThresholdOptimizer(th_init.copy())\n",
        "    th_best, q_iso = opt.fit(y_true, oof_iso.copy(), iters=80, step=0.05)\n",
        "    print(f'[Iso] OOF QWK after isotonic + re-threshold={q_iso:.5f} (base blend={q_base:.5f}) thr={th_best}', flush=True)\n",
        "    # Apply to test\n",
        "    labels = np.digitize(te_iso, th_best) + 1\n",
        "    labels = np.clip(labels, 1, 6).astype(int)\n",
        "    sub = pd.DataFrame({'essay_id': test_df['essay_id'], 'score': labels})\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    with open('blend_isotonic_params.json','w') as f:\n",
        "        json.dump({'oof_qwk_isotonic': float(q_iso), 'oof_qwk_base': float(q_base), 'thresholds': th_best.tolist()}, f)\n",
        "    print('Saved submission.csv (isotonic-calibrated blend) and blend_isotonic_params.json')\n",
        "    print(f'=== Isotonic calibration done in {(time.time()-t0)/60:.2f} min ===', flush=True)\n",
        "\n",
        "run_isotonic_calibration()"
      ],
      "execution_count": 44,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}