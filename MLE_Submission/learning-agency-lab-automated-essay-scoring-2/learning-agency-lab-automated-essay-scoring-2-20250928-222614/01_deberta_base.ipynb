{
  "cells": [
    {
      "id": "099852c2-aef7-490c-8736-039699616e2d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install CUDA 12.1 PyTorch stack and NLP deps; verify GPU\n",
        "import os, sys, subprocess, shutil, time\n",
        "from pathlib import Path\n",
        "\n",
        "def pip(*args):\n",
        "    print(\">\", *args, flush=True)\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", *args], check=True)\n",
        "\n",
        "# Show NVIDIA-SMI first\n",
        "print(\"=== NVIDIA-SMI ===\", flush=True)\n",
        "subprocess.run(['bash','-lc','nvidia-smi || true'], check=False)\n",
        "\n",
        "# Uninstall any preexisting torch stack to avoid duplicates\n",
        "for pkg in (\"torch\",\"torchvision\",\"torchaudio\"):\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", pkg], check=False)\n",
        "\n",
        "# Clean stray site dirs that can shadow correct wheels (idempotent)\n",
        "for d in (\n",
        "    \"/app/.pip-target/torch\",\n",
        "    \"/app/.pip-target/torchvision\",\n",
        "    \"/app/.pip-target/torchaudio\",\n",
        "    \"/app/.pip-target/torch-2.8.0.dist-info\",\n",
        "    \"/app/.pip-target/torch-2.4.1.dist-info\",\n",
        "    \"/app/.pip-target/torchvision-0.23.0.dist-info\",\n",
        "    \"/app/.pip-target/torchvision-0.19.1.dist-info\",\n",
        "    \"/app/.pip-target/torchaudio-2.8.0.dist-info\",\n",
        "    \"/app/.pip-target/torchaudio-2.4.1.dist-info\",\n",
        "    \"/app/.pip-target/torchgen\",\n",
        "    \"/app/.pip-target/functorch\",\n",
        "):\n",
        "    if os.path.exists(d):\n",
        "        print(\"Removing\", d, flush=True)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "# 1) Install EXACT cu121 torch stack\n",
        "pip(\"install\",\n",
        "    \"--index-url\", \"https://download.pytorch.org/whl/cu121\",\n",
        "    \"--extra-index-url\", \"https://pypi.org/simple\",\n",
        "    \"torch==2.4.1\", \"torchvision==0.19.1\", \"torchaudio==2.4.1\")\n",
        "\n",
        "# 2) Freeze torch versions\n",
        "Path(\"constraints.txt\").write_text(\n",
        "    \"torch==2.4.1\\n\"\n",
        "    \"torchvision==0.19.1\\n\"\n",
        "    \"torchaudio==2.4.1\\n\"\n",
        ")\n",
        "\n",
        "# 3) Install transformer deps without touching torch\n",
        "pip(\"install\", \"-c\", \"constraints.txt\",\n",
        "    \"transformers==4.44.2\", \"accelerate==0.34.2\",\n",
        "    \"datasets==2.21.0\", \"evaluate==0.4.2\",\n",
        "    \"sentencepiece\", \"scikit-learn\", \"numpy\", \"pandas\",\n",
        "    \"tqdm\", \"scipy\",\n",
        "    \"--upgrade-strategy\", \"only-if-needed\")\n",
        "\n",
        "# 4) Sanity gate\n",
        "import torch\n",
        "print(\"torch:\", torch.__version__, \"built CUDA:\", getattr(torch.version, \"cuda\", None))\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "assert str(getattr(torch.version, \"cuda\", \"\")).startswith(\"12.1\"), f\"Wrong CUDA build: {torch.version.cuda}\"\n",
        "assert torch.cuda.is_available(), \"CUDA not available\"\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "print(\"Environment ready.\", flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "d60496b5-a157-4054-b74b-3faa3c28e268",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DeBERTa-v3-base utilities: data loading, tokenizer (head+tail), dataset, collator, model factory\n",
        "import os, time, math, random, numpy as np, pandas as pd, torch\n",
        "from datasets import Dataset as HFDataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
        "MAX_LEN = 512  # model's native max length\n",
        "HEAD_TOKENS = 200\n",
        "TAIL_TOKENS = MAX_LEN - 2 - HEAD_TOKENS  # account for special tokens by tokenizer padding/truncation\n",
        "\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "folds_df = pd.read_csv('folds.csv')\n",
        "id_col, text_col, target_col = 'essay_id', 'full_text', 'score'\n",
        "assert {id_col, text_col, target_col}.issubset(train_df.columns)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def head_tail_encode(texts):\n",
        "    # Tokenize to ids first without truncation to slice head/tail by tokens\n",
        "    enc = tokenizer(texts, add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)\n",
        "    input_ids_list = []\n",
        "    for ids in enc['input_ids']:\n",
        "        if len(ids) <= MAX_LEN - 2:\n",
        "            input_ids_list.append(ids)\n",
        "        else:\n",
        "            head = ids[:HEAD_TOKENS]\n",
        "            tail = ids[-TAIL_TOKENS:] if TAIL_TOKENS > 0 else []\n",
        "            input_ids_list.append(head + tail)\n",
        "    # Now add special tokens and pad/truncate to MAX_LEN\n",
        "    out = tokenizer.pad({'input_ids': [tokenizer.build_inputs_with_special_tokens(ids) for ids in input_ids_list]},\n",
        "                        padding='max_length', max_length=MAX_LEN, return_tensors=None)\n",
        "    return out  # dict with input_ids, attention_mask\n",
        "\n",
        "class TextRegDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, targets=None):\n",
        "        self.texts = texts\n",
        "        self.targets = targets\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        tokenized = head_tail_encode([text])\n",
        "        item = {\n",
        "            'input_ids': torch.tensor(tokenized['input_ids'][0], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(tokenized['attention_mask'][0], dtype=torch.long),\n",
        "        }\n",
        "        if self.targets is not None:\n",
        "            item['labels'] = torch.tensor(float(self.targets[idx]), dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n",
        "\n",
        "def get_model():\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=1, problem_type='regression')\n",
        "    return model\n",
        "\n",
        "print('DeBERTa utilities ready. Next: add training loop with 5-fold CV, AMP, and OOF/test caching.', flush=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeBERTa utilities ready. Next: add training loop with 5-fold CV, AMP, and OOF/test caching.\n"
          ]
        }
      ]
    },
    {
      "id": "004b7e50-b2aa-409f-867c-fc8ec3c9af64",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DeBERTa-v3-base 5-fold with head-tail training + sliding-window (512, stride 128) eval/infer; QWK early stop\n",
        "import numpy as np, torch, time, math, os, pandas as pd, random\n",
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "n_splits = int(folds_df['fold'].max()) + 1\n",
        "y = train_df[target_col].astype(float).values\n",
        "min_score, max_score = 1.0, 6.0\n",
        "\n",
        "# Perf/stability flags\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False; torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def qwk_int(y_true, y_pred_int):\n",
        "    return cohen_kappa_score(y_true, y_pred_int, weights='quadratic')\n",
        "\n",
        "# Pre-tokenize ALL texts once to speed per-fold ops\n",
        "print('[DeBERTa] Pre-tokenizing train/test to raw token ids...', flush=True)\n",
        "tok_train = tokenizer(train_df[text_col].tolist(), add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)\n",
        "tok_test  = tokenizer(test_df[text_col].tolist(),  add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)\n",
        "train_ids_all = tok_train['input_ids']\n",
        "test_ids_all  = tok_test['input_ids']\n",
        "print('[DeBERTa] Pre-tokenization done.', flush=True)\n",
        "\n",
        "# Helpers for head+tail pack to 512\n",
        "def pack_head_tail(ids, max_len=512, head=200):\n",
        "    tail = max_len - 2 - head\n",
        "    if len(ids) <= max_len - 2:\n",
        "        core = ids\n",
        "    else:\n",
        "        core = ids[:head] + (ids[-tail:] if tail>0 else [])\n",
        "    built = tokenizer.build_inputs_with_special_tokens(core)\n",
        "    out = tokenizer.pad({'input_ids':[built]}, padding='max_length', max_length=max_len, return_tensors='pt')\n",
        "    return out['input_ids'][0], out['attention_mask'][0]\n",
        "\n",
        "# Sliding-window chunking (for eval/infer) with stride 128\n",
        "def chunkify_ids(ids, max_len=512, stride=128):\n",
        "    usable = max_len - 2\n",
        "    if len(ids) <= usable:\n",
        "        chunks = [ids]\n",
        "    else:\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        while start < len(ids):\n",
        "            end = min(start + usable, len(ids))\n",
        "            chunks.append(ids[start:end])\n",
        "            if end == len(ids):\n",
        "                break\n",
        "            # FIX: advance by stride (not usable - stride)\n",
        "            start += stride\n",
        "    # build tensors and weights (token count per chunk)\n",
        "    input_ids = []; attn = []; weights = []\n",
        "    for ch in chunks:\n",
        "        built = tokenizer.build_inputs_with_special_tokens(ch)\n",
        "        padded = tokenizer.pad({'input_ids':[built]}, padding='max_length', max_length=max_len, return_tensors='pt')\n",
        "        input_ids.append(padded['input_ids'][0]); attn.append(padded['attention_mask'][0]); weights.append(float(len(ch)))\n",
        "    return input_ids, attn, weights\n",
        "\n",
        "class HeadTailDataset(Dataset):  # for training (single segment per essay)\n",
        "    def __init__(self, ids_list, targets=None):\n",
        "        self.ids_list = ids_list\n",
        "        self.targets = targets\n",
        "    def __len__(self): return len(self.ids_list)\n",
        "    def __getitem__(self, idx):\n",
        "        ids = self.ids_list[idx]\n",
        "        input_ids, attention_mask = pack_head_tail(ids, MAX_LEN, HEAD_TOKENS)\n",
        "        item = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
        "        if self.targets is not None:\n",
        "            item['labels'] = torch.tensor(float(self.targets[idx]), dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "class ChunkDataset(Dataset):  # flat chunks for eval/infer\n",
        "    def __init__(self, ids_list):\n",
        "        self.inputs = []; self.attns = []; self.essay_idx = []; self.weights = []\n",
        "        for i, ids in enumerate(ids_list):\n",
        "            inp, att, w = chunkify_ids(ids, MAX_LEN, stride=128)\n",
        "            self.inputs.extend(inp); self.attns.extend(att);\n",
        "            self.essay_idx.extend([i]*len(inp)); self.weights.extend(w)\n",
        "    def __len__(self): return len(self.inputs)\n",
        "    def __getitem__(self, idx):\n",
        "        return {'input_ids': self.inputs[idx], 'attention_mask': self.attns[idx]}\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {k: torch.stack([x[k] for x in batch]) for k in batch[0].keys()}\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds = eval_pred.predictions.reshape(-1)\n",
        "    labels = eval_pred.label_ids.reshape(-1)\n",
        "    preds = np.clip(preds, min_score, max_score)\n",
        "    base_th = np.array([1.5,2.5,3.5,4.5,5.5])\n",
        "    bins = [-np.inf] + base_th.tolist() + [np.inf]\n",
        "    pred_int = np.digitize(preds, bins)\n",
        "    labels_int = labels.astype(int)\n",
        "    return {'qwk': qwk_int(labels_int, pred_int)}\n",
        "\n",
        "def length_weighted_aggregate(flat_preds, essay_idx, weights, n_items):\n",
        "    agg = np.zeros(n_items, dtype=np.float32); wsum = np.zeros(n_items, dtype=np.float32)\n",
        "    np.add.at(agg, essay_idx, flat_preds * weights); np.add.at(wsum, essay_idx, weights)\n",
        "    return agg / np.clip(wsum, 1e-6, None)\n",
        "\n",
        "# Build test chunks once\n",
        "test_chunk_ds_global = ChunkDataset(test_ids_all)\n",
        "\n",
        "oof = np.zeros(len(train_df), dtype=np.float32)\n",
        "test_pred_f = np.zeros((len(test_df), n_splits), dtype=np.float32)\n",
        "\n",
        "for f in range(n_splits):\n",
        "    fold_t0 = time.time()\n",
        "    tr_idx = folds_df.index[folds_df['fold']!=f].to_numpy()\n",
        "    va_idx = folds_df.index[folds_df['fold']==f].to_numpy()\n",
        "    print(f'[DeBERTa] Fold {f} start: tr={len(tr_idx)} va={len(va_idx)}', flush=True)\n",
        "\n",
        "    seed_everything(42 + f)\n",
        "    model = get_model()\n",
        "    # mild dropout\n",
        "    if hasattr(model.config, 'hidden_dropout_prob'): model.config.hidden_dropout_prob = 0.1\n",
        "    if hasattr(model.config, 'attention_probs_dropout_prob'): model.config.attention_probs_dropout_prob = 0.1\n",
        "    model.gradient_checkpointing_enable(); model.to(device)\n",
        "\n",
        "    # Datasets\n",
        "    train_ds = HeadTailDataset([train_ids_all[i] for i in tr_idx], train_df.loc[tr_idx, target_col].tolist())\n",
        "    valid_ds_ht = HeadTailDataset([train_ids_all[i] for i in va_idx], train_df.loc[va_idx, target_col].tolist())  # for ES/QWK monitor\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'outputs_fold{f}',\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=64,\n",
        "        gradient_accumulation_steps=1,\n",
        "        evaluation_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        learning_rate=2e-5,\n",
        "        num_train_epochs=5,\n",
        "        weight_decay=0.02,\n",
        "        lr_scheduler_type='cosine',\n",
        "        warmup_ratio=0.1,\n",
        "        bf16=True,\n",
        "        bf16_full_eval=True,\n",
        "        logging_steps=50,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='qwk',\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=5,\n",
        "        dataloader_pin_memory=True,\n",
        "        dataloader_num_workers=2,\n",
        "        report_to=[]\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=valid_ds_ht,\n",
        "        data_collator=collate_fn,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Prepare sliding-window validation chunks\n",
        "    val_chunk_ds = ChunkDataset([train_ids_all[i] for i in va_idx])\n",
        "    essay_idx_val = np.array(val_chunk_ds.essay_idx, dtype=np.int64)\n",
        "    weights_val = np.array(val_chunk_ds.weights, dtype=np.float32)\n",
        "\n",
        "    # Post-hoc checkpoint selection via sliding-window QWK\n",
        "    chk_dir = args.output_dir\n",
        "    ckpts = []\n",
        "    if os.path.isdir(chk_dir):\n",
        "        for d in os.listdir(chk_dir):\n",
        "            if d.startswith('checkpoint-'):\n",
        "                try:\n",
        "                    step = int(d.split('-')[-1])\n",
        "                except:\n",
        "                    step = -1\n",
        "                ckpts.append((step, os.path.join(chk_dir, d)))\n",
        "    ckpts.sort()\n",
        "    # evaluate last up to 3 checkpoints for speed; if none, fall back to current model\n",
        "    candidates = [p for _, p in ckpts[-3:]] if ckpts else []\n",
        "    best_q = -1.0; best_path = None; best_val_pred = None\n",
        "    for path in candidates if candidates else [None]:\n",
        "        if path is not None:\n",
        "            cand_model = AutoModelForSequenceClassification.from_pretrained(path, num_labels=1, problem_type='regression').to(device)\n",
        "        else:\n",
        "            cand_model = trainer.model\n",
        "        cand_trainer = Trainer(model=cand_model, args=args, data_collator=collate_fn)\n",
        "        with torch.no_grad():\n",
        "            preds_flat = cand_trainer.predict(val_chunk_ds).predictions.reshape(-1)\n",
        "        preds_flat = np.clip(preds_flat, min_score, max_score)\n",
        "        val_pred = length_weighted_aggregate(preds_flat, essay_idx_val, weights_val, len(va_idx))\n",
        "        # compute QWK with base thresholds\n",
        "        base_th = np.array([1.5,2.5,3.5,4.5,5.5])\n",
        "        bins = [-np.inf] + base_th.tolist() + [np.inf]\n",
        "        val_int = np.digitize(val_pred, bins)\n",
        "        score = qwk_int(train_df.loc[va_idx, target_col].astype(int).values, val_int)\n",
        "        if score > best_q:\n",
        "            best_q = score; best_path = path; best_val_pred = val_pred.astype(np.float32)\n",
        "\n",
        "    # Save OOF for this fold using the best checkpoint\n",
        "    oof[va_idx] = np.clip(best_val_pred, min_score, max_score)\n",
        "\n",
        "    # Load best checkpoint (if different) for test inference\n",
        "    if best_path is not None:\n",
        "        best_model = AutoModelForSequenceClassification.from_pretrained(best_path, num_labels=1, problem_type='regression').to(device)\n",
        "    else:\n",
        "        best_model = trainer.model\n",
        "    best_trainer = Trainer(model=best_model, args=args, data_collator=collate_fn)\n",
        "    with torch.no_grad():\n",
        "        test_preds_flat = best_trainer.predict(test_chunk_ds_global).predictions.reshape(-1)\n",
        "    test_preds_flat = np.clip(test_preds_flat, min_score, max_score)\n",
        "    essay_idx_t = np.array(test_chunk_ds_global.essay_idx, dtype=np.int64); weights_t = np.array(test_chunk_ds_global.weights, dtype=np.float32)\n",
        "    test_pred_f[:, f] = length_weighted_aggregate(test_preds_flat, essay_idx_t, weights_t, len(test_df)).astype(np.float32)\n",
        "\n",
        "    del trainer, model, train_ds, valid_ds_ht, val_chunk_ds, best_trainer, best_model\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f'[DeBERTa] Fold {f} done in {time.time()-fold_t0:.1f}s (best SW QWK={best_q:.5f})', flush=True)\n",
        "\n",
        "# Save artifacts\n",
        "pd.DataFrame({'essay_id': train_df[id_col], 'oof_deberta': oof, 'y': y}).to_csv('oof_deberta_base.csv', index=False)\n",
        "np.save('test_deberta_base.npy', test_pred_f.mean(axis=1))\n",
        "print('Saved oof_deberta_base.csv and test_deberta_base.npy', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "e472aeb7-ef51-4f63-9c54-ff12b531f0f3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Post-hoc sliding-window re-eval at stride=64 (no retrain); save new OOF/test artifacts\n",
        "import os, time, numpy as np, pandas as pd, torch\n",
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "assert 'tokenizer' in globals() and 'train_df' in globals() and 'test_df' in globals() and 'folds_df' in globals(), 'Run setup cells first.'\n",
        "\n",
        "min_score, max_score = 1.0, 6.0\n",
        "n_splits = int(folds_df['fold'].max()) + 1\n",
        "\n",
        "def qwk_int(y_true, y_pred_int):\n",
        "    return cohen_kappa_score(y_true, y_pred_int, weights='quadratic')\n",
        "\n",
        "def chunkify_ids_stride(ids, max_len=512, stride=64):\n",
        "    usable = max_len - 2\n",
        "    if len(ids) <= usable:\n",
        "        chunks = [ids]\n",
        "    else:\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        while start < len(ids):\n",
        "            end = min(start + usable, len(ids))\n",
        "            chunks.append(ids[start:end])\n",
        "            if end == len(ids):\n",
        "                break\n",
        "            start += stride\n",
        "    input_ids = []; attn = []; weights = []\n",
        "    for ch in chunks:\n",
        "        built = tokenizer.build_inputs_with_special_tokens(ch)\n",
        "        padded = tokenizer.pad({'input_ids':[built]}, padding='max_length', max_length=max_len, return_tensors='pt')\n",
        "        input_ids.append(padded['input_ids'][0]); attn.append(padded['attention_mask'][0]); weights.append(float(len(ch)))\n",
        "    return input_ids, attn, weights\n",
        "\n",
        "class ChunkDataset64(Dataset):\n",
        "    def __init__(self, ids_list):\n",
        "        self.inputs=[]; self.attns=[]; self.essay_idx=[]; self.weights=[]\n",
        "        for i, ids in enumerate(ids_list):\n",
        "            inp, att, w = chunkify_ids_stride(ids, MAX_LEN, stride=64)\n",
        "            self.inputs.extend(inp); self.attns.extend(att);\n",
        "            self.essay_idx.extend([i]*len(inp)); self.weights.extend(w)\n",
        "    def __len__(self): return len(self.inputs)\n",
        "    def __getitem__(self, idx):\n",
        "        return {'input_ids': self.inputs[idx], 'attention_mask': self.attns[idx]}\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {k: torch.stack([x[k] for x in batch]) for k in batch[0].keys()}\n",
        "\n",
        "def length_weighted_aggregate(flat_preds, essay_idx, weights, n_items):\n",
        "    agg = np.zeros(n_items, dtype=np.float32); wsum = np.zeros(n_items, dtype=np.float32)\n",
        "    np.add.at(agg, essay_idx, flat_preds * weights); np.add.at(wsum, essay_idx, weights)\n",
        "    return agg / np.clip(wsum, 1e-6, None)\n",
        "\n",
        "# Build ids once (use same pretokenization as training cell if present)\n",
        "if 'train_ids_all' not in globals() or 'test_ids_all' not in globals():\n",
        "    tok_train = tokenizer(train_df[text_col].tolist(), add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)\n",
        "    tok_test  = tokenizer(test_df[text_col].tolist(),  add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)\n",
        "    train_ids_all = tok_train['input_ids']\n",
        "    test_ids_all  = tok_test['input_ids']\n",
        "\n",
        "test_chunk_ds64 = ChunkDataset64(test_ids_all)\n",
        "essay_idx_t = np.array(test_chunk_ds64.essay_idx, dtype=np.int64)\n",
        "weights_t = np.array(test_chunk_ds64.weights, dtype=np.float32)\n",
        "\n",
        "oof = np.zeros(len(train_df), dtype=np.float32)\n",
        "test_pred_f = np.zeros((len(test_df), n_splits), dtype=np.float32)\n",
        "y_int = train_df[target_col].astype(int).values\n",
        "\n",
        "t0 = time.time()\n",
        "for f in range(n_splits):\n",
        "    f_t = time.time()\n",
        "    va_idx = folds_df.index[folds_df['fold']==f].to_numpy()\n",
        "    chk_dir = f'outputs_fold{f}'\n",
        "    assert os.path.isdir(chk_dir), f'Missing {chk_dir}; run training first.'\n",
        "    ckpts = []\n",
        "    for d in os.listdir(chk_dir):\n",
        "        if d.startswith('checkpoint-'):\n",
        "            try:\n",
        "                step = int(d.split('-')[-1])\n",
        "            except:\n",
        "                step = -1\n",
        "            ckpts.append((step, os.path.join(chk_dir, d)))\n",
        "    ckpts.sort()\n",
        "    candidates = [p for _, p in ckpts[-3:]] if ckpts else []\n",
        "    val_chunk_ds64 = ChunkDataset64([train_ids_all[i] for i in va_idx])\n",
        "    essay_idx_val = np.array(val_chunk_ds64.essay_idx, dtype=np.int64)\n",
        "    weights_val = np.array(val_chunk_ds64.weights, dtype=np.float32)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=chk_dir,\n",
        "        per_device_eval_batch_size=64,\n",
        "        dataloader_num_workers=2,\n",
        "        bf16_full_eval=True,\n",
        "        report_to=[]\n",
        "    )\n",
        "\n",
        "    best_q = -1.0; best_path = None; best_val_pred=None\n",
        "    for path in candidates if candidates else [None]:\n",
        "        if path is None:\n",
        "            continue  # require explicit checkpoints\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(path, num_labels=1, problem_type='regression')\n",
        "        trainer = Trainer(model=model, args=args, data_collator=collate_fn)\n",
        "        with torch.no_grad():\n",
        "            preds_flat = trainer.predict(val_chunk_ds64).predictions.reshape(-1)\n",
        "        preds_flat = np.clip(preds_flat, min_score, max_score)\n",
        "        val_pred = length_weighted_aggregate(preds_flat, essay_idx_val, weights_val, len(va_idx))\n",
        "        base_th = np.array([1.5,2.5,3.5,4.5,5.5])\n",
        "        bins = [-np.inf] + base_th.tolist() + [np.inf]\n",
        "        val_int = np.digitize(val_pred, bins)\n",
        "        score = qwk_int(y_int[va_idx], val_int)\n",
        "        if score > best_q:\n",
        "            best_q = score; best_path = path; best_val_pred = val_pred.astype(np.float32)\n",
        "\n",
        "    assert best_val_pred is not None, 'No valid checkpoint selected.'\n",
        "    oof[va_idx] = np.clip(best_val_pred, min_score, max_score)\n",
        "\n",
        "    # Test inference with best checkpoint\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(best_path, num_labels=1, problem_type='regression')\n",
        "    trainer = Trainer(model=model, args=args, data_collator=collate_fn)\n",
        "    with torch.no_grad():\n",
        "        test_flat = trainer.predict(test_chunk_ds64).predictions.reshape(-1)\n",
        "    test_flat = np.clip(test_flat, min_score, max_score)\n",
        "    test_pred_f[:, f] = length_weighted_aggregate(test_flat, essay_idx_t, weights_t, len(test_df)).astype(np.float32)\n",
        "    print(f'[SW64] Fold {f} done in {time.time()-f_t:.1f}s (best QWK={best_q:.5f})', flush=True)\n",
        "\n",
        "pd.DataFrame({'essay_id': train_df[id_col], 'oof_deberta': oof, 'y': y_int}).to_csv('oof_deberta_base_sw64.csv', index=False)\n",
        "np.save('test_deberta_base_sw64.npy', test_pred_f.mean(axis=1))\n",
        "print(f'[SW64] Saved oof_deberta_base_sw64.csv and test_deberta_base_sw64.npy in {time.time()-t0:.1f}s', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "b754e383-523d-462b-b3f9-381887ce3561",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Seed 777 full 5-fold train with SW64 checkpoint selection and TTA (SW64, SW128, Head+Tail); save per-view and combined artifacts\n",
        "import os, time, math, random, numpy as np, pandas as pd, torch\n",
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals() and 'folds_df' in globals() and 'tokenizer' in globals(), 'Run setup cells first.'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BASE_SEED = 777\n",
        "n_splits = int(folds_df['fold'].max()) + 1\n",
        "y = train_df[target_col].astype(float).values\n",
        "min_score, max_score = 1.0, 6.0\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False; torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def qwk_int(y_true, y_pred_int):\n",
        "    return cohen_kappa_score(y_true, y_pred_int, weights='quadratic')\n",
        "\n",
        "# Pre-tokenize if not present\n",
        "if 'train_ids_all' not in globals() or 'test_ids_all' not in globals():\n",
        "    print('[s777] Pre-tokenizing...', flush=True)\n",
        "    tok_train = tokenizer(train_df[text_col].tolist(), add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)\n",
        "    tok_test  = tokenizer(test_df[text_col].tolist(),  add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)\n",
        "    train_ids_all = tok_train['input_ids']\n",
        "    test_ids_all  = tok_test['input_ids']\n",
        "\n",
        "def pack_head_tail(ids, max_len=512, head=HEAD_TOKENS):\n",
        "    tail = max_len - 2 - head\n",
        "    if len(ids) <= max_len - 2:\n",
        "        core = ids\n",
        "    else:\n",
        "        core = ids[:head] + (ids[-tail:] if tail>0 else [])\n",
        "    built = tokenizer.build_inputs_with_special_tokens(core)\n",
        "    out = tokenizer.pad({'input_ids':[built]}, padding='max_length', max_length=max_len, return_tensors='pt')\n",
        "    return out['input_ids'][0], out['attention_mask'][0]\n",
        "\n",
        "def chunkify_stride(ids, max_len=512, stride=128):\n",
        "    usable = max_len - 2\n",
        "    if len(ids) <= usable:\n",
        "        chunks = [ids]\n",
        "    else:\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        while start < len(ids):\n",
        "            end = min(start + usable, len(ids))\n",
        "            chunks.append(ids[start:end])\n",
        "            if end == len(ids):\n",
        "                break\n",
        "            start += stride\n",
        "    input_ids = []; attn = []; weights = []\n",
        "    for ch in chunks:\n",
        "        built = tokenizer.build_inputs_with_special_tokens(ch)\n",
        "        padded = tokenizer.pad({'input_ids':[built]}, padding='max_length', max_length=max_len, return_tensors='pt')\n",
        "        input_ids.append(padded['input_ids'][0]); attn.append(padded['attention_mask'][0]); weights.append(float(len(ch)))\n",
        "    return input_ids, attn, weights\n",
        "\n",
        "class TrainHTDataset(Dataset):\n",
        "    def __init__(self, ids_list, targets=None):\n",
        "        self.ids_list = ids_list; self.targets = targets\n",
        "    def __len__(self): return len(self.ids_list)\n",
        "    def __getitem__(self, idx):\n",
        "        ids = self.ids_list[idx]\n",
        "        input_ids, attention_mask = pack_head_tail(ids, MAX_LEN, HEAD_TOKENS)\n",
        "        item = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
        "        if self.targets is not None:\n",
        "            item['labels'] = torch.tensor(float(self.targets[idx]), dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "class ChunkDataset(Dataset):\n",
        "    def __init__(self, ids_list, stride):\n",
        "        self.inputs=[]; self.attns=[]; self.essay_idx=[]; self.weights=[]\n",
        "        for i, ids in enumerate(ids_list):\n",
        "            inp, att, w = chunkify_stride(ids, MAX_LEN, stride=stride)\n",
        "            self.inputs.extend(inp); self.attns.extend(att);\n",
        "            self.essay_idx.extend([i]*len(inp)); self.weights.extend(w)\n",
        "    def __len__(self): return len(self.inputs)\n",
        "    def __getitem__(self, idx):\n",
        "        return {'input_ids': self.inputs[idx], 'attention_mask': self.attns[idx]}\n",
        "\n",
        "class HTInferDataset(Dataset):\n",
        "    def __init__(self, ids_list):\n",
        "        self.inputs=[]; self.attns=[]; self.essay_idx=[]\n",
        "        for i, ids in enumerate(ids_list):\n",
        "            inp, att = pack_head_tail(ids, MAX_LEN, HEAD_TOKENS)\n",
        "            self.inputs.append(inp); self.attns.append(att); self.essay_idx.append(i)\n",
        "    def __len__(self): return len(self.inputs)\n",
        "    def __getitem__(self, idx):\n",
        "        return {'input_ids': self.inputs[idx], 'attention_mask': self.attns[idx]}\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {k: torch.stack([x[k] for x in batch]) for k in batch[0].keys()}\n",
        "\n",
        "def length_weighted_aggregate(flat_preds, essay_idx, weights, n_items):\n",
        "    agg = np.zeros(n_items, dtype=np.float32); wsum = np.zeros(n_items, dtype=np.float32)\n",
        "    np.add.at(agg, essay_idx, flat_preds * weights); np.add.at(wsum, essay_idx, weights)\n",
        "    return agg / np.clip(wsum, 1e-6, None)\n",
        "\n",
        "# Prebuild test datasets for TTA\n",
        "test_chunks_64 = ChunkDataset(test_ids_all, stride=64)\n",
        "test_chunks_128 = ChunkDataset(test_ids_all, stride=128)\n",
        "test_ht = HTInferDataset(test_ids_all)\n",
        "essay_idx_t64 = np.array(test_chunks_64.essay_idx, dtype=np.int64); weights_t64 = np.array(test_chunks_64.weights, dtype=np.float32)\n",
        "essay_idx_t128 = np.array(test_chunks_128.essay_idx, dtype=np.int64); weights_t128 = np.array(test_chunks_128.weights, dtype=np.float32)\n",
        "\n",
        "oof_64 = np.zeros(len(train_df), dtype=np.float32)\n",
        "oof_128 = np.zeros(len(train_df), dtype=np.float32)\n",
        "oof_ht = np.zeros(len(train_df), dtype=np.float32)\n",
        "test_pred_f64 = np.zeros((len(test_df), n_splits), dtype=np.float32)\n",
        "test_pred_f128 = np.zeros((len(test_df), n_splits), dtype=np.float32)\n",
        "test_pred_fht = np.zeros((len(test_df), n_splits), dtype=np.float32)\n",
        "\n",
        "for f in range(n_splits):\n",
        "    fold_t0 = time.time()\n",
        "    tr_idx = folds_df.index[folds_df['fold']!=f].to_numpy()\n",
        "    va_idx = folds_df.index[folds_df['fold']==f].to_numpy()\n",
        "    print(f'[s777] Fold {f} start: tr={len(tr_idx)} va={len(va_idx)}', flush=True)\n",
        "\n",
        "    seed_everything(BASE_SEED + f)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=1, problem_type='regression')\n",
        "    if hasattr(model.config, 'hidden_dropout_prob'): model.config.hidden_dropout_prob = 0.10\n",
        "    if hasattr(model.config, 'attention_probs_dropout_prob'): model.config.attention_probs_dropout_prob = 0.10\n",
        "    model.gradient_checkpointing_enable(); model.to(device)\n",
        "\n",
        "    train_ds = TrainHTDataset([train_ids_all[i] for i in tr_idx], train_df.loc[tr_idx, target_col].tolist())\n",
        "    valid_ds_ht = TrainHTDataset([train_ids_all[i] for i in va_idx], train_df.loc[va_idx, target_col].tolist())\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'outputs_fold{f}',\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=64,\n",
        "        gradient_accumulation_steps=1,\n",
        "        evaluation_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        learning_rate=2e-5,\n",
        "        num_train_epochs=5,\n",
        "        weight_decay=0.02,\n",
        "        lr_scheduler_type='cosine',\n",
        "        warmup_ratio=0.1,\n",
        "        bf16=True,\n",
        "        bf16_full_eval=True,\n",
        "        logging_steps=50,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='qwk',\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=5,\n",
        "        dataloader_pin_memory=True,\n",
        "        dataloader_num_workers=2,\n",
        "        report_to=[]\n",
        "    )\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        preds = eval_pred.predictions.reshape(-1)\n",
        "        labels = eval_pred.label_ids.reshape(-1)\n",
        "        preds = np.clip(preds, min_score, max_score)\n",
        "        base_th = np.array([1.5,2.5,3.5,4.5,5.5])\n",
        "        bins = [-np.inf] + base_th.tolist() + [np.inf]\n",
        "        pred_int = np.digitize(preds, bins)\n",
        "        labels_int = labels.astype(int)\n",
        "        return {'qwk': qwk_int(labels_int, pred_int)}\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=valid_ds_ht,\n",
        "        data_collator=collate_fn,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "    # Build validation datasets for TTA\n",
        "    val_chunks_64 = ChunkDataset([train_ids_all[i] for i in va_idx], stride=64)\n",
        "    val_chunks_128 = ChunkDataset([train_ids_all[i] for i in va_idx], stride=128)\n",
        "    val_ht = HTInferDataset([train_ids_all[i] for i in va_idx])\n",
        "    essay_idx_v64 = np.array(val_chunks_64.essay_idx, dtype=np.int64); weights_v64 = np.array(val_chunks_64.weights, dtype=np.float32)\n",
        "    essay_idx_v128 = np.array(val_chunks_128.essay_idx, dtype=np.int64); weights_v128 = np.array(val_chunks_128.weights, dtype=np.float32)\n",
        "\n",
        "    # Post-hoc checkpoint selection using SW64 QWK\n",
        "    chk_dir = args.output_dir\n",
        "    ckpts = []\n",
        "    if os.path.isdir(chk_dir):\n",
        "        for d in os.listdir(chk_dir):\n",
        "            if d.startswith('checkpoint-'):\n",
        "                try: step = int(d.split('-')[-1])\n",
        "                except: step = -1\n",
        "                ckpts.append((step, os.path.join(chk_dir, d)))\n",
        "    ckpts.sort()\n",
        "    candidates = [p for _, p in ckpts[-3:]] if ckpts else []\n",
        "    best_q = -1.0; best_path = None; best_val_pred64 = None; best_val_pred128 = None; best_val_pred_ht = None\n",
        "\n",
        "    def eval_view(model_path, ds, aggregate, essay_idx, weights=None, n_items=None):\n",
        "        m = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=1, problem_type='regression').to(device)\n",
        "        t = Trainer(model=m, args=args, data_collator=collate_fn)\n",
        "        with torch.no_grad():\n",
        "            flat = t.predict(ds).predictions.reshape(-1)\n",
        "        flat = np.clip(flat, min_score, max_score)\n",
        "        if aggregate:\n",
        "            return length_weighted_aggregate(flat, essay_idx, weights, n_items).astype(np.float32)\n",
        "        else:\n",
        "            return flat.astype(np.float32)\n",
        "\n",
        "    for path in candidates if candidates else [None]:\n",
        "        if path is None:\n",
        "            continue\n",
        "        # Evaluate SW64 for selection\n",
        "        val_pred64 = eval_view(path, val_chunks_64, True, essay_idx_v64, weights_v64, len(va_idx))\n",
        "        base_th = np.array([1.5,2.5,3.5,4.5,5.5])\n",
        "        bins = [-np.inf] + base_th.tolist() + [np.inf]\n",
        "        q = qwk_int(train_df.loc[va_idx, target_col].astype(int).values, np.digitize(val_pred64, bins))\n",
        "        if q > best_q:\n",
        "            best_q = q; best_path = path; best_val_pred64 = val_pred64\n",
        "            # Also compute companion views for the same checkpoint\n",
        "            best_val_pred128 = eval_view(path, val_chunks_128, True, essay_idx_v128, weights_v128, len(va_idx))\n",
        "            # Head+Tail single view\n",
        "            m_ht = AutoModelForSequenceClassification.from_pretrained(path, num_labels=1, problem_type='regression').to(device)\n",
        "            t_ht = Trainer(model=m_ht, args=args, data_collator=collate_fn)\n",
        "            with torch.no_grad():\n",
        "                flat_ht = t_ht.predict(val_ht).predictions.reshape(-1)\n",
        "            best_val_pred_ht = np.clip(flat_ht, min_score, max_score).astype(np.float32)\n",
        "\n",
        "    assert best_path is not None and best_val_pred64 is not None, '[s777] No valid checkpoint found for fold %d' % f\n",
        "\n",
        "    # Save OOF per view\n",
        "    oof_64[va_idx] = np.clip(best_val_pred64, min_score, max_score)\n",
        "    oof_128[va_idx] = np.clip(best_val_pred128, min_score, max_score)\n",
        "    oof_ht[va_idx] = np.clip(best_val_pred_ht, min_score, max_score)\n",
        "\n",
        "    # Test inference for all TTA views with best checkpoint\n",
        "    best_model = AutoModelForSequenceClassification.from_pretrained(best_path, num_labels=1, problem_type='regression').to(device)\n",
        "    t_common = Trainer(model=best_model, args=args, data_collator=collate_fn)\n",
        "    with torch.no_grad():\n",
        "        flat64 = t_common.predict(test_chunks_64).predictions.reshape(-1)\n",
        "        flat128 = t_common.predict(test_chunks_128).predictions.reshape(-1)\n",
        "        flat_ht = t_common.predict(test_ht).predictions.reshape(-1)\n",
        "    flat64 = np.clip(flat64, min_score, max_score); flat128 = np.clip(flat128, min_score, max_score); flat_ht = np.clip(flat_ht, min_score, max_score)\n",
        "    test_pred_f64[:, f] = length_weighted_aggregate(flat64, essay_idx_t64, weights_t64, len(test_df)).astype(np.float32)\n",
        "    test_pred_f128[:, f] = length_weighted_aggregate(flat128, essay_idx_t128, weights_t128, len(test_df)).astype(np.float32)\n",
        "    # HT is one per essay already\n",
        "    test_pred_fht[:, f] = flat_ht.astype(np.float32)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f'[s777] Fold {f} done in {time.time()-fold_t0:.1f}s (best SW64 QWK={best_q:.5f})', flush=True)\n",
        "\n",
        "# Save per-view OOF and test\n",
        "pd.DataFrame({'essay_id': train_df[id_col], 'oof_deberta': oof_64, 'y': train_df[target_col].astype(int).values}).to_csv('oof_deberta_base_s777_sw64.csv', index=False)\n",
        "pd.DataFrame({'essay_id': train_df[id_col], 'oof_deberta': oof_128, 'y': train_df[target_col].astype(int).values}).to_csv('oof_deberta_base_s777_sw128.csv', index=False)\n",
        "pd.DataFrame({'essay_id': train_df[id_col], 'oof_deberta': oof_ht, 'y': train_df[target_col].astype(int).values}).to_csv('oof_deberta_base_s777_ht.csv', index=False)\n",
        "np.save('test_deberta_base_s777_sw64.npy', test_pred_f64.mean(axis=1))\n",
        "np.save('test_deberta_base_s777_sw128.npy', test_pred_f128.mean(axis=1))\n",
        "np.save('test_deberta_base_s777_ht.npy', test_pred_fht.mean(axis=1))\n",
        "\n",
        "# Also save the TTA-combined view (0.4*SW64 + 0.4*SW128 + 0.2*HT)\n",
        "oof_tta = 0.4*oof_64 + 0.4*oof_128 + 0.2*oof_ht\n",
        "test_tta = 0.4*test_pred_f64.mean(axis=1) + 0.4*test_pred_f128.mean(axis=1) + 0.2*test_pred_fht.mean(axis=1)\n",
        "pd.DataFrame({'essay_id': train_df[id_col], 'oof_deberta': oof_tta.astype(np.float32), 'y': train_df[target_col].astype(int).values}).to_csv('oof_deberta_base_s777.csv', index=False)\n",
        "np.save('test_deberta_base_s777.npy', test_tta.astype(np.float32))\n",
        "print('[s777] Saved per-view and combined TTA artifacts for seed 777.', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "f1604b3c-c3c7-45ba-815a-fe241858782e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Seed 2025 partial folds with diversity (lr=1.8e-5, HEAD_TOKENS=256, dropout=0.12) + TTA; configurable folds_to_run\n",
        "import os, time, math, random, numpy as np, pandas as pd, torch\n",
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals() and 'folds_df' in globals() and 'tokenizer' in globals(), 'Run setup cells first.'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BASE_SEED = 2025\n",
        "LOCAL_HEAD_TOKENS = 256\n",
        "n_splits = int(folds_df['fold'].max()) + 1\n",
        "y = train_df[target_col].astype(float).values\n",
        "min_score, max_score = 1.0, 6.0\n",
        "\n",
        "# Choose weakest folds first; adjust list as needed (run 2 first, add third if time permits)\n",
        "FOLDS_TO_RUN = [3]  # run fold 3 now to complete 5/5 coverage for s2025\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False; torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def qwk_int(y_true, y_pred_int):\n",
        "    return cohen_kappa_score(y_true, y_pred_int, weights='quadratic')\n",
        "\n",
        "# Reuse pretokenized ids if present; otherwise build now\n",
        "if 'train_ids_all' not in globals() or 'test_ids_all' not in globals():\n",
        "    print('[s2025] Pre-tokenizing...', flush=True)\n",
        "    tok_train = tokenizer(train_df[text_col].tolist(), add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)\n",
        "    tok_test  = tokenizer(test_df[text_col].tolist(),  add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)\n",
        "    train_ids_all = tok_train['input_ids']\n",
        "    test_ids_all  = tok_test['input_ids']\n",
        "\n",
        "def pack_head_tail_local(ids, max_len=512, head=LOCAL_HEAD_TOKENS):\n",
        "    tail = max_len - 2 - head\n",
        "    if len(ids) <= max_len - 2:\n",
        "        core = ids\n",
        "    else:\n",
        "        core = ids[:head] + (ids[-tail:] if tail>0 else [])\n",
        "    built = tokenizer.build_inputs_with_special_tokens(core)\n",
        "    out = tokenizer.pad({'input_ids':[built]}, padding='max_length', max_length=max_len, return_tensors='pt')\n",
        "    return out['input_ids'][0], out['attention_mask'][0]\n",
        "\n",
        "def chunkify_stride(ids, max_len=512, stride=128):\n",
        "    usable = max_len - 2\n",
        "    if len(ids) <= usable:\n",
        "        chunks = [ids]\n",
        "    else:\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        while start < len(ids):\n",
        "            end = min(start + usable, len(ids))\n",
        "            chunks.append(ids[start:end])\n",
        "            if end == len(ids):\n",
        "                break\n",
        "            start += stride\n",
        "    input_ids = []; attn = []; weights = []\n",
        "    for ch in chunks:\n",
        "        built = tokenizer.build_inputs_with_special_tokens(ch)\n",
        "        padded = tokenizer.pad({'input_ids':[built]}, padding='max_length', max_length=max_len, return_tensors='pt')\n",
        "        input_ids.append(padded['input_ids'][0]); attn.append(padded['attention_mask'][0]); weights.append(float(len(ch)))\n",
        "    return input_ids, attn, weights\n",
        "\n",
        "class TrainHTDatasetLocal(Dataset):\n",
        "    def __init__(self, ids_list, targets=None):\n",
        "        self.ids_list = ids_list; self.targets = targets\n",
        "    def __len__(self): return len(self.ids_list)\n",
        "    def __getitem__(self, idx):\n",
        "        ids = self.ids_list[idx]\n",
        "        input_ids, attention_mask = pack_head_tail_local(ids, MAX_LEN, LOCAL_HEAD_TOKENS)\n",
        "        item = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
        "        if self.targets is not None:\n",
        "            item['labels'] = torch.tensor(float(self.targets[idx]), dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "class ChunkDataset(Dataset):\n",
        "    def __init__(self, ids_list, stride):\n",
        "        self.inputs=[]; self.attns=[]; self.essay_idx=[]; self.weights=[]\n",
        "        for i, ids in enumerate(ids_list):\n",
        "            inp, att, w = chunkify_stride(ids, MAX_LEN, stride=stride)\n",
        "            self.inputs.extend(inp); self.attns.extend(att);\n",
        "            self.essay_idx.extend([i]*len(inp)); self.weights.extend(w)\n",
        "    def __len__(self): return len(self.inputs)\n",
        "    def __getitem__(self, idx):\n",
        "        return {'input_ids': self.inputs[idx], 'attention_mask': self.attns[idx]}\n",
        "\n",
        "class HTInferDatasetLocal(Dataset):\n",
        "    def __init__(self, ids_list):\n",
        "        self.inputs=[]; self.attns=[]; self.essay_idx=[]\n",
        "        for i, ids in enumerate(ids_list):\n",
        "            inp, att = pack_head_tail_local(ids, MAX_LEN, LOCAL_HEAD_TOKENS)\n",
        "            self.inputs.append(inp); self.attns.append(att); self.essay_idx.append(i)\n",
        "    def __len__(self): return len(self.inputs)\n",
        "    def __getitem__(self, idx):\n",
        "        return {'input_ids': self.inputs[idx], 'attention_mask': self.attns[idx]}\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {k: torch.stack([x[k] for x in batch]) for k in batch[0].keys()}\n",
        "\n",
        "def length_weighted_aggregate(flat_preds, essay_idx, weights, n_items):\n",
        "    agg = np.zeros(n_items, dtype=np.float32); wsum = np.zeros(n_items, dtype=np.float32)\n",
        "    np.add.at(agg, essay_idx, flat_preds * weights); np.add.at(wsum, essay_idx, weights)\n",
        "    return agg / np.clip(wsum, 1e-6, None)\n",
        "\n",
        "# Prebuild test datasets for TTA\n",
        "test_chunks_64 = ChunkDataset(test_ids_all, stride=64)\n",
        "test_chunks_128 = ChunkDataset(test_ids_all, stride=128)\n",
        "test_ht = HTInferDatasetLocal(test_ids_all)\n",
        "essay_idx_t64 = np.array(test_chunks_64.essay_idx, dtype=np.int64); weights_t64 = np.array(test_chunks_64.weights, dtype=np.float32)\n",
        "essay_idx_t128 = np.array(test_chunks_128.essay_idx, dtype=np.int64); weights_t128 = np.array(test_chunks_128.weights, dtype=np.float32)\n",
        "\n",
        "# Allocate OOF/test holders only for folds we run; fill others with zeros (ignored in bagging by availability)\n",
        "oof_64 = np.zeros(len(train_df), dtype=np.float32)\n",
        "oof_128 = np.zeros(len(train_df), dtype=np.float32)\n",
        "oof_ht = np.zeros(len(train_df), dtype=np.float32)\n",
        "test_pred_f64 = np.zeros((len(test_df), n_splits), dtype=np.float32)\n",
        "test_pred_f128 = np.zeros((len(test_df), n_splits), dtype=np.float32)\n",
        "test_pred_fht = np.zeros((len(test_df), n_splits), dtype=np.float32)\n",
        "\n",
        "for f in FOLDS_TO_RUN:\n",
        "    fold_t0 = time.time()\n",
        "    tr_idx = folds_df.index[folds_df['fold']!=f].to_numpy()\n",
        "    va_idx = folds_df.index[folds_df['fold']==f].to_numpy()\n",
        "    print(f'[s2025] Fold {f} start: tr={len(tr_idx)} va={len(va_idx)}', flush=True)\n",
        "\n",
        "    seed_everything(BASE_SEED + f)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=1, problem_type='regression')\n",
        "    if hasattr(model.config, 'hidden_dropout_prob'): model.config.hidden_dropout_prob = 0.12\n",
        "    if hasattr(model.config, 'attention_probs_dropout_prob'): model.config.attention_probs_dropout_prob = 0.12\n",
        "    model.gradient_checkpointing_enable(); model.to(device)\n",
        "\n",
        "    train_ds = TrainHTDatasetLocal([train_ids_all[i] for i in tr_idx], train_df.loc[tr_idx, target_col].tolist())\n",
        "    valid_ds_ht = TrainHTDatasetLocal([train_ids_all[i] for i in va_idx], train_df.loc[va_idx, target_col].tolist())\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'outputs_fold{f}',\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=64,\n",
        "        gradient_accumulation_steps=1,\n",
        "        evaluation_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        learning_rate=1.8e-5,\n",
        "        num_train_epochs=5,\n",
        "        weight_decay=0.02,\n",
        "        lr_scheduler_type='cosine',\n",
        "        warmup_ratio=0.1,\n",
        "        bf16=True,\n",
        "        bf16_full_eval=True,\n",
        "        logging_steps=50,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='qwk',\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=5,\n",
        "        dataloader_pin_memory=True,\n",
        "        dataloader_num_workers=2,\n",
        "        report_to=[]\n",
        "    )\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        preds = eval_pred.predictions.reshape(-1)\n",
        "        labels = eval_pred.label_ids.reshape(-1)\n",
        "        preds = np.clip(preds, min_score, max_score)\n",
        "        base_th = np.array([1.5,2.5,3.5,4.5,5.5])\n",
        "        bins = [-np.inf] + base_th.tolist() + [np.inf]\n",
        "        pred_int = np.digitize(preds, bins)\n",
        "        labels_int = labels.astype(int)\n",
        "        return {'qwk': qwk_int(labels_int, pred_int)}\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=valid_ds_ht,\n",
        "        data_collator=collate_fn,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "    # Build validation datasets for TTA\n",
        "    val_chunks_64 = ChunkDataset([train_ids_all[i] for i in va_idx], stride=64)\n",
        "    val_chunks_128 = ChunkDataset([train_ids_all[i] for i in va_idx], stride=128)\n",
        "    val_ht = HTInferDatasetLocal([train_ids_all[i] for i in va_idx])\n",
        "    essay_idx_v64 = np.array(val_chunks_64.essay_idx, dtype=np.int64); weights_v64 = np.array(val_chunks_64.weights, dtype=np.float32)\n",
        "    essay_idx_v128 = np.array(val_chunks_128.essay_idx, dtype=np.int64); weights_v128 = np.array(val_chunks_128.weights, dtype=np.float32)\n",
        "\n",
        "    # Post-hoc checkpoint selection using SW64 QWK\n",
        "    chk_dir = args.output_dir\n",
        "    ckpts = []\n",
        "    if os.path.isdir(chk_dir):\n",
        "        for d in os.listdir(chk_dir):\n",
        "            if d.startswith('checkpoint-'):\n",
        "                try: step = int(d.split('-')[-1])\n",
        "                except: step = -1\n",
        "                ckpts.append((step, os.path.join(chk_dir, d)))\n",
        "    ckpts.sort()\n",
        "    candidates = [p for _, p in ckpts[-3:]] if ckpts else []\n",
        "    best_q = -1.0; best_path = None; best_val_pred64 = None; best_val_pred128 = None; best_val_pred_ht = None\n",
        "\n",
        "    def eval_view(model_path, ds, aggregate, essay_idx, weights=None, n_items=None):\n",
        "        m = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=1, problem_type='regression').to(device)\n",
        "        t = Trainer(model=m, args=args, data_collator=collate_fn)\n",
        "        with torch.no_grad():\n",
        "            flat = t.predict(ds).predictions.reshape(-1)\n",
        "        flat = np.clip(flat, min_score, max_score)\n",
        "        if aggregate:\n",
        "            return length_weighted_aggregate(flat, essay_idx, weights, n_items).astype(np.float32)\n",
        "        else:\n",
        "            return flat.astype(np.float32)\n",
        "\n",
        "    for path in candidates if candidates else [None]:\n",
        "        if path is None:\n",
        "            continue\n",
        "        val_pred64 = eval_view(path, val_chunks_64, True, essay_idx_v64, weights_v64, len(va_idx))\n",
        "        base_th = np.array([1.5,2.5,3.5,4.5,5.5])\n",
        "        bins = [-np.inf] + base_th.tolist() + [np.inf]\n",
        "        q = qwk_int(train_df.loc[va_idx, target_col].astype(int).values, np.digitize(val_pred64, bins))\n",
        "        if q > best_q:\n",
        "            best_q = q; best_path = path; best_val_pred64 = val_pred64\n",
        "            best_val_pred128 = eval_view(path, val_chunks_128, True, essay_idx_v128, weights_v128, len(va_idx))\n",
        "            m_ht = AutoModelForSequenceClassification.from_pretrained(path, num_labels=1, problem_type='regression').to(device)\n",
        "            t_ht = Trainer(model=m_ht, args=args, data_collator=collate_fn)\n",
        "            with torch.no_grad():\n",
        "                flat_ht = t_ht.predict(val_ht).predictions.reshape(-1)\n",
        "            best_val_pred_ht = np.clip(flat_ht, min_score, max_score).astype(np.float32)\n",
        "\n",
        "    assert best_path is not None and best_val_pred64 is not None, '[s2025] No valid checkpoint found for fold %d' % f\n",
        "\n",
        "    # Save OOF per view\n",
        "    oof_64[va_idx] = np.clip(best_val_pred64, min_score, max_score)\n",
        "    oof_128[va_idx] = np.clip(best_val_pred128, min_score, max_score)\n",
        "    oof_ht[va_idx] = np.clip(best_val_pred_ht, min_score, max_score)\n",
        "\n",
        "    # Test inference for all TTA views with best checkpoint\n",
        "    best_model = AutoModelForSequenceClassification.from_pretrained(best_path, num_labels=1, problem_type='regression').to(device)\n",
        "    t_common = Trainer(model=best_model, args=args, data_collator=collate_fn)\n",
        "    with torch.no_grad():\n",
        "        flat64 = t_common.predict(test_chunks_64).predictions.reshape(-1)\n",
        "        flat128 = t_common.predict(test_chunks_128).predictions.reshape(-1)\n",
        "        flat_ht = t_common.predict(test_ht).predictions.reshape(-1)\n",
        "    flat64 = np.clip(flat64, min_score, max_score); flat128 = np.clip(flat128, min_score, max_score); flat_ht = np.clip(flat_ht, min_score, max_score)\n",
        "    test_pred_f64[:, f] = length_weighted_aggregate(flat64, essay_idx_t64, weights_t64, len(test_df)).astype(np.float32)\n",
        "    test_pred_f128[:, f] = length_weighted_aggregate(flat128, essay_idx_t128, weights_t128, len(test_df)).astype(np.float32)\n",
        "    test_pred_fht[:, f] = flat_ht.astype(np.float32)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f'[s2025] Fold {f} done in {time.time()-fold_t0:.1f}s (best SW64 QWK={best_q:.5f})', flush=True)\n",
        "\n",
        "# Save per-view OOF and test (note: only filled folds contain non-zero entries)\n",
        "pd.DataFrame({'essay_id': train_df[id_col], 'oof_deberta': oof_64, 'y': train_df[target_col].astype(int).values}).to_csv('oof_deberta_base_s2025_sw64.csv', index=False)\n",
        "pd.DataFrame({'essay_id': train_df[id_col], 'oof_deberta': oof_128, 'y': train_df[target_col].astype(int).values}).to_csv('oof_deberta_base_s2025_sw128.csv', index=False)\n",
        "pd.DataFrame({'essay_id': train_df[id_col], 'oof_deberta': oof_ht, 'y': train_df[target_col].astype(int).values}).to_csv('oof_deberta_base_s2025_ht.csv', index=False)\n",
        "np.save('test_deberta_base_s2025_sw64.npy', test_pred_f64.mean(axis=1))\n",
        "np.save('test_deberta_base_s2025_sw128.npy', test_pred_f128.mean(axis=1))\n",
        "np.save('test_deberta_base_s2025_ht.npy', test_pred_fht.mean(axis=1))\n",
        "\n",
        "# Also save the TTA-combined view for available folds (0.4*SW64 + 0.4*SW128 + 0.2*HT)\n",
        "oof_tta = 0.4*oof_64 + 0.4*oof_128 + 0.2*oof_ht\n",
        "test_tta = 0.4*test_pred_f64.mean(axis=1) + 0.4*test_pred_f128.mean(axis=1) + 0.2*test_pred_fht.mean(axis=1)\n",
        "pd.DataFrame({'essay_id': train_df[id_col], 'oof_deberta': oof_tta.astype(np.float32), 'y': train_df[target_col].astype(int).values}).to_csv('oof_deberta_base_s2025.csv', index=False)\n",
        "np.save('test_deberta_base_s2025.npy', test_tta.astype(np.float32))\n",
        "print('[s2025] Saved per-view and combined TTA artifacts for seed 2025 (partial folds).', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "a4cda1d8-6ff6-4eb1-b41d-e15f0970a441",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Rebuild s2025 per-view and combined artifacts from existing checkpoints (no training); folds where outputs_fold{f} exist\n",
        "import os, time, numpy as np, pandas as pd, torch\n",
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals() and 'folds_df' in globals() and 'tokenizer' in globals(), 'Run setup cells first.'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "min_score, max_score = 1.0, 6.0\n",
        "n_splits = int(folds_df['fold'].max()) + 1\n",
        "y_int = train_df[target_col].astype(int).values\n",
        "\n",
        "def qwk_int(y_true, y_pred_int):\n",
        "    return cohen_kappa_score(y_true, y_pred_int, weights='quadratic')\n",
        "\n",
        "# Ensure token ids are available\n",
        "if 'train_ids_all' not in globals() or 'test_ids_all' not in globals():\n",
        "    tok_train = tokenizer(train_df[text_col].tolist(), add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)\n",
        "    tok_test  = tokenizer(test_df[text_col].tolist(),  add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)\n",
        "    train_ids_all = tok_train['input_ids']\n",
        "    test_ids_all  = tok_test['input_ids']\n",
        "\n",
        "MAX_LEN_EVAL = MAX_LEN  # use same 512\n",
        "\n",
        "def chunkify(ids, max_len=512, stride=64):\n",
        "    usable = max_len - 2\n",
        "    if len(ids) <= usable:\n",
        "        chunks = [ids]\n",
        "    else:\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        while start < len(ids):\n",
        "            end = min(start + usable, len(ids))\n",
        "            chunks.append(ids[start:end])\n",
        "            if end == len(ids): break\n",
        "            start += stride\n",
        "    input_ids = []; attn = []; weights = []\n",
        "    for ch in chunks:\n",
        "        built = tokenizer.build_inputs_with_special_tokens(ch)\n",
        "        padded = tokenizer.pad({'input_ids':[built]}, padding='max_length', max_length=max_len, return_tensors='pt')\n",
        "        input_ids.append(padded['input_ids'][0]); attn.append(padded['attention_mask'][0]); weights.append(float(len(ch)))\n",
        "    return input_ids, attn, weights\n",
        "\n",
        "class ChunkDatasetStride(Dataset):\n",
        "    def __init__(self, ids_list, stride):\n",
        "        self.inputs=[]; self.attns=[]; self.essay_idx=[]; self.weights=[]\n",
        "        for i, ids in enumerate(ids_list):\n",
        "            inp, att, w = chunkify(ids, MAX_LEN_EVAL, stride=stride)\n",
        "            self.inputs.extend(inp); self.attns.extend(att); self.essay_idx.extend([i]*len(inp)); self.weights.extend(w)\n",
        "    def __len__(self): return len(self.inputs)\n",
        "    def __getitem__(self, idx):\n",
        "        return {'input_ids': self.inputs[idx], 'attention_mask': self.attns[idx]}\n",
        "\n",
        "class HTInferDatasetLocal(Dataset):\n",
        "    def __init__(self, ids_list, head_tokens=256):\n",
        "        self.inputs=[]; self.attns=[]; self.essay_idx=[]\n",
        "        tail = MAX_LEN_EVAL - 2 - head_tokens\n",
        "        for i, ids in enumerate(ids_list):\n",
        "            if len(ids) <= MAX_LEN_EVAL - 2:\n",
        "                core = ids\n",
        "            else:\n",
        "                core = ids[:head_tokens] + (ids[-tail:] if tail>0 else [])\n",
        "            built = tokenizer.build_inputs_with_special_tokens(core)\n",
        "            padded = tokenizer.pad({'input_ids':[built]}, padding='max_length', max_length=MAX_LEN_EVAL, return_tensors='pt')\n",
        "            self.inputs.append(padded['input_ids'][0]); self.attns.append(padded['attention_mask'][0]); self.essay_idx.append(i)\n",
        "    def __len__(self): return len(self.inputs)\n",
        "    def __getitem__(self, idx):\n",
        "        return {'input_ids': self.inputs[idx], 'attention_mask': self.attns[idx]}\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {k: torch.stack([x[k] for x in batch]) for k in batch[0].keys()}\n",
        "\n",
        "def length_weighted_aggregate(flat_preds, essay_idx, weights, n_items):\n",
        "    agg = np.zeros(n_items, dtype=np.float32); wsum = np.zeros(n_items, dtype=np.float32)\n",
        "    np.add.at(agg, essay_idx, flat_preds * weights); np.add.at(wsum, essay_idx, weights)\n",
        "    return agg / np.clip(wsum, 1e-6, None)\n",
        "\n",
        "# Build test datasets\n",
        "test_sw64 = ChunkDatasetStride(test_ids_all, stride=64)\n",
        "test_sw128 = ChunkDatasetStride(test_ids_all, stride=128)\n",
        "test_ht = HTInferDatasetLocal(test_ids_all, head_tokens=256)\n",
        "essay_idx_t64 = np.array(test_sw64.essay_idx, dtype=np.int64); weights_t64 = np.array(test_sw64.weights, dtype=np.float32)\n",
        "essay_idx_t128 = np.array(test_sw128.essay_idx, dtype=np.int64); weights_t128 = np.array(test_sw128.weights, dtype=np.float32)\n",
        "\n",
        "# Holders\n",
        "oof_64 = np.zeros(len(train_df), dtype=np.float32)\n",
        "oof_128 = np.zeros(len(train_df), dtype=np.float32)\n",
        "oof_ht = np.zeros(len(train_df), dtype=np.float32)\n",
        "test_pred_f64 = np.zeros((len(test_df), n_splits), dtype=np.float32)\n",
        "test_pred_f128 = np.zeros((len(test_df), n_splits), dtype=np.float32)\n",
        "test_pred_fht = np.zeros((len(test_df), n_splits), dtype=np.float32)\n",
        "\n",
        "# Evaluate all folds that have outputs directories (dynamic, includes newly trained fold 3)\n",
        "folds_to_eval = [f for f in range(n_splits) if os.path.isdir(f'outputs_fold{f}')]\n",
        "print('[s2025-rebuild] Evaluating folds:', folds_to_eval, flush=True)\n",
        "\n",
        "for f in folds_to_eval:\n",
        "    va_idx = folds_df.index[folds_df['fold']==f].to_numpy()\n",
        "    # Build validation datasets\n",
        "    val_sw64 = ChunkDatasetStride([train_ids_all[i] for i in va_idx], stride=64)\n",
        "    val_sw128 = ChunkDatasetStride([train_ids_all[i] for i in va_idx], stride=128)\n",
        "    val_ht = HTInferDatasetLocal([train_ids_all[i] for i in va_idx], head_tokens=256)\n",
        "    essay_idx_v64 = np.array(val_sw64.essay_idx, dtype=np.int64); weights_v64 = np.array(val_sw64.weights, dtype=np.float32)\n",
        "    essay_idx_v128 = np.array(val_sw128.essay_idx, dtype=np.int64); weights_v128 = np.array(val_sw128.weights, dtype=np.float32)\n",
        "\n",
        "    chk_dir = f'outputs_fold{f}'\n",
        "    ckpts = []\n",
        "    for d in os.listdir(chk_dir):\n",
        "        if d.startswith('checkpoint-'):\n",
        "            try: step = int(d.split('-')[-1])\n",
        "            except: step = -1\n",
        "            ckpts.append((step, os.path.join(chk_dir, d)))\n",
        "    ckpts.sort()\n",
        "    candidates = [p for _, p in ckpts[-3:]] if ckpts else []\n",
        "    assert candidates, f'No checkpoints found for fold {f}'\n",
        "\n",
        "    args = TrainingArguments(output_dir=chk_dir, per_device_eval_batch_size=64, dataloader_num_workers=2, bf16_full_eval=True, report_to=[])\n",
        "\n",
        "    def eval_view(model_path, ds, aggregate, essay_idx, weights=None, n_items=None):\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=1, problem_type='regression').to(device)\n",
        "        trainer = Trainer(model=model, args=args, data_collator=collate_fn)\n",
        "        with torch.no_grad():\n",
        "            flat = trainer.predict(ds).predictions.reshape(-1)\n",
        "        flat = np.clip(flat, min_score, max_score)\n",
        "        if aggregate:\n",
        "            return length_weighted_aggregate(flat, essay_idx, weights, n_items).astype(np.float32)\n",
        "        else:\n",
        "            return flat.astype(np.float32)\n",
        "\n",
        "    best_q = -1.0; best_path = None; best_val64=None; best_val128=None; best_valht=None\n",
        "    for path in candidates:\n",
        "        val64 = eval_view(path, val_sw64, True, essay_idx_v64, weights_v64, len(va_idx))\n",
        "        base_th = np.array([1.5,2.5,3.5,4.5,5.5])\n",
        "        bins = [-np.inf] + base_th.tolist() + [np.inf]\n",
        "        q = qwk_int(y_int[va_idx], np.digitize(val64, bins))\n",
        "        if q > best_q:\n",
        "            best_q = q; best_path = path; best_val64 = val64\n",
        "            best_val128 = eval_view(path, val_sw128, True, essay_idx_v128, weights_v128, len(va_idx))\n",
        "            # HT single view\n",
        "            model_ht = AutoModelForSequenceClassification.from_pretrained(path, num_labels=1, problem_type='regression').to(device)\n",
        "            trainer_ht = Trainer(model=model_ht, args=args, data_collator=collate_fn)\n",
        "            with torch.no_grad():\n",
        "                flat_ht = trainer_ht.predict(val_ht).predictions.reshape(-1)\n",
        "            best_valht = np.clip(flat_ht, min_score, max_score).astype(np.float32)\n",
        "\n",
        "    # Assign OOF\n",
        "    oof_64[va_idx] = np.clip(best_val64, min_score, max_score)\n",
        "    oof_128[va_idx] = np.clip(best_val128, min_score, max_score)\n",
        "    oof_ht[va_idx] = np.clip(best_valht, min_score, max_score)\n",
        "\n",
        "    # Test inference from best checkpoint\n",
        "    best_model = AutoModelForSequenceClassification.from_pretrained(best_path, num_labels=1, problem_type='regression').to(device)\n",
        "    trainer = Trainer(model=best_model, args=args, data_collator=collate_fn)\n",
        "    with torch.no_grad():\n",
        "        flat64 = trainer.predict(test_sw64).predictions.reshape(-1)\n",
        "        flat128 = trainer.predict(test_sw128).predictions.reshape(-1)\n",
        "        flatht = trainer.predict(test_ht).predictions.reshape(-1)\n",
        "    flat64 = np.clip(flat64, min_score, max_score); flat128 = np.clip(flat128, min_score, max_score); flatht = np.clip(flatht, min_score, max_score)\n",
        "    test_pred_f64[:, f] = length_weighted_aggregate(flat64, essay_idx_t64, weights_t64, len(test_df)).astype(np.float32)\n",
        "    test_pred_f128[:, f] = length_weighted_aggregate(flat128, essay_idx_t128, weights_t128, len(test_df)).astype(np.float32)\n",
        "    test_pred_fht[:, f] = flatht.astype(np.float32)\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f'[s2025-rebuild] Fold {f} best SW64 QWK={best_q:.5f}', flush=True)\n",
        "\n",
        "# Save per-view and combined artifacts for s2025\n",
        "pd.DataFrame({'essay_id': train_df[id_col], 'oof_deberta': oof_64, 'y': y_int}).to_csv('oof_deberta_base_s2025_sw64.csv', index=False)\n",
        "pd.DataFrame({'essay_id': train_df[id_col], 'oof_deberta': oof_128, 'y': y_int}).to_csv('oof_deberta_base_s2025_sw128.csv', index=False)\n",
        "pd.DataFrame({'essay_id': train_df[id_col], 'oof_deberta': oof_ht, 'y': y_int}).to_csv('oof_deberta_base_s2025_ht.csv', index=False)\n",
        "np.save('test_deberta_base_s2025_sw64.npy', test_pred_f64.mean(axis=1))\n",
        "np.save('test_deberta_base_s2025_sw128.npy', test_pred_f128.mean(axis=1))\n",
        "np.save('test_deberta_base_s2025_ht.npy', test_pred_fht.mean(axis=1))\n",
        "\n",
        "oof_tta = (0.4*oof_64 + 0.4*oof_128 + 0.2*oof_ht).astype(np.float32)\n",
        "test_tta = (0.4*test_pred_f64.mean(axis=1) + 0.4*test_pred_f128.mean(axis=1) + 0.2*test_pred_fht.mean(axis=1)).astype(np.float32)\n",
        "pd.DataFrame({'essay_id': train_df[id_col], 'oof_deberta': oof_tta, 'y': y_int}).to_csv('oof_deberta_base_s2025.csv', index=False)\n",
        "np.save('test_deberta_base_s2025.npy', test_tta)\n",
        "print('[s2025-rebuild] Saved per-view and combined artifacts from existing checkpoints.', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "4dc25a90-2c85-4bd5-a750-586d7adeb525",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DeBERTa-v3-Large targeted folds (0 first): SW64 checkpoint selection + TTA (SW64, SW128, HT256); save per-view and combined artifacts\n",
        "import os, time, math, random, numpy as np, pandas as pd, torch\n",
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals() and 'folds_df' in globals(), 'Run setup cells first.'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BASE_SEED = 130013  # distinct seed for large model\n",
        "MODEL_NAME_L = 'microsoft/deberta-v3-large'\n",
        "MAX_LEN_L = 512\n",
        "HEAD_TOKENS_L = 256\n",
        "n_splits = int(folds_df['fold'].max()) + 1\n",
        "y = train_df[target_col].astype(int).values\n",
        "min_score, max_score = 1.0, 6.0\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False; torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def qwk_int(y_true, y_pred_int):\n",
        "    return cohen_kappa_score(y_true, y_pred_int, weights='quadratic')\n",
        "\n",
        "# Prepare tokenizer for large\n",
        "tok_l = AutoTokenizer.from_pretrained(MODEL_NAME_L)\n",
        "\n",
        "# Pre-tokenize if not present (reuse if base already built, else with large tokenizer for consistency in special tokens)\n",
        "if 'train_ids_all' not in globals() or 'test_ids_all' not in globals():\n",
        "    print('[v3-large] Pre-tokenizing...', flush=True)\n",
        "    tok_train = tok_l(train_df[text_col].tolist(), add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)\n",
        "    tok_test  = tok_l(test_df[text_col].tolist(),  add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)\n",
        "    train_ids_all = tok_train['input_ids']\n",
        "    test_ids_all  = tok_test['input_ids']\n",
        "\n",
        "def pack_head_tail_l(ids, max_len=MAX_LEN_L, head=HEAD_TOKENS_L):\n",
        "    tail = max_len - 2 - head\n",
        "    if len(ids) <= max_len - 2:\n",
        "        core = ids\n",
        "    else:\n",
        "        core = ids[:head] + (ids[-tail:] if tail>0 else [])\n",
        "    built = tok_l.build_inputs_with_special_tokens(core)\n",
        "    out = tok_l.pad({'input_ids':[built]}, padding='max_length', max_length=max_len, return_tensors='pt')\n",
        "    return out['input_ids'][0], out['attention_mask'][0]\n",
        "\n",
        "def chunkify_stride_l(ids, max_len=MAX_LEN_L, stride=64):\n",
        "    usable = max_len - 2\n",
        "    if len(ids) <= usable:\n",
        "        chunks = [ids]\n",
        "    else:\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        while start < len(ids):\n",
        "            end = min(start + usable, len(ids))\n",
        "            chunks.append(ids[start:end])\n",
        "            if end == len(ids):\n",
        "                break\n",
        "            start += stride\n",
        "    input_ids = []; attn = []; weights = []\n",
        "    for ch in chunks:\n",
        "        built = tok_l.build_inputs_with_special_tokens(ch)\n",
        "        padded = tok_l.pad({'input_ids':[built]}, padding='max_length', max_length=max_len, return_tensors='pt')\n",
        "        input_ids.append(padded['input_ids'][0]); attn.append(padded['attention_mask'][0]); weights.append(float(len(ch)))\n",
        "    return input_ids, attn, weights\n",
        "\n",
        "class TrainHTDatasetL(Dataset):\n",
        "    def __init__(self, ids_list, targets=None):\n",
        "        self.ids_list = ids_list; self.targets = targets\n",
        "    def __len__(self): return len(self.ids_list)\n",
        "    def __getitem__(self, idx):\n",
        "        ids = self.ids_list[idx]\n",
        "        input_ids, attention_mask = pack_head_tail_l(ids)\n",
        "        item = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
        "        if self.targets is not None:\n",
        "            item['labels'] = torch.tensor(float(self.targets[idx]), dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "class ChunkDatasetL(Dataset):\n",
        "    def __init__(self, ids_list, stride):\n",
        "        self.inputs=[]; self.attns=[]; self.essay_idx=[]; self.weights=[]\n",
        "        for i, ids in enumerate(ids_list):\n",
        "            inp, att, w = chunkify_stride_l(ids, MAX_LEN_L, stride=stride)\n",
        "            self.inputs.extend(inp); self.attns.extend(att);\n",
        "            self.essay_idx.extend([i]*len(inp)); self.weights.extend(w)\n",
        "    def __len__(self): return len(self.inputs)\n",
        "    def __getitem__(self, idx):\n",
        "        return {'input_ids': self.inputs[idx], 'attention_mask': self.attns[idx]}\n",
        "\n",
        "class HTInferDatasetL(Dataset):\n",
        "    def __init__(self, ids_list):\n",
        "        self.inputs=[]; self.attns=[]; self.essay_idx=[]\n",
        "        for i, ids in enumerate(ids_list):\n",
        "            inp, att = pack_head_tail_l(ids)\n",
        "            self.inputs.append(inp); self.attns.append(att); self.essay_idx.append(i)\n",
        "    def __len__(self): return len(self.inputs)\n",
        "    def __getitem__(self, idx):\n",
        "        return {'input_ids': self.inputs[idx], 'attention_mask': self.attns[idx]}\n",
        "\n",
        "def collate_fn_l(batch):\n",
        "    return {k: torch.stack([x[k] for x in batch]) for k in batch[0].keys()}\n",
        "\n",
        "def length_weighted_aggregate(flat_preds, essay_idx, weights, n_items):\n",
        "    agg = np.zeros(n_items, dtype=np.float32); wsum = np.zeros(n_items, dtype=np.float32)\n",
        "    np.add.at(agg, essay_idx, flat_preds * weights); np.add.at(wsum, essay_idx, weights)\n",
        "    return agg / np.clip(wsum, 1e-6, None)\n",
        "\n",
        "# Build test datasets for TTA\n",
        "test_chunks_64_l = ChunkDatasetL(test_ids_all, stride=64)\n",
        "test_chunks_128_l = ChunkDatasetL(test_ids_all, stride=128)\n",
        "test_ht_l = HTInferDatasetL(test_ids_all)\n",
        "essay_idx_t64_l = np.array(test_chunks_64_l.essay_idx, dtype=np.int64); weights_t64_l = np.array(test_chunks_64_l.weights, dtype=np.float32)\n",
        "essay_idx_t128_l = np.array(test_chunks_128_l.essay_idx, dtype=np.int64); weights_t128_l = np.array(test_chunks_128_l.weights, dtype=np.float32)\n",
        "\n",
        "# Holders (full length; we'll fill only target folds)\n",
        "oof_64_l = np.zeros(len(train_df), dtype=np.float32)\n",
        "oof_128_l = np.zeros(len(train_df), dtype=np.float32)\n",
        "oof_ht_l = np.zeros(len(train_df), dtype=np.float32)\n",
        "test_pred_f64_l = np.zeros((len(test_df), n_splits), dtype=np.float32)\n",
        "test_pred_f128_l = np.zeros((len(test_df), n_splits), dtype=np.float32)\n",
        "test_pred_fht_l = np.zeros((len(test_df), n_splits), dtype=np.float32)\n",
        "\n",
        "# Targeted folds: now run fold 1; we'll rebuild combined sL artifacts afterward\n",
        "FOLDS_TO_RUN_L = [1]\n",
        "\n",
        "for f in FOLDS_TO_RUN_L:\n",
        "    fold_t0 = time.time()\n",
        "    tr_idx = folds_df.index[folds_df['fold']!=f].to_numpy()\n",
        "    va_idx = folds_df.index[folds_df['fold']==f].to_numpy()\n",
        "    print(f'[v3-large] Fold {f} start: tr={len(tr_idx)} va={len(va_idx)}', flush=True)\n",
        "\n",
        "    seed_everything(BASE_SEED + f)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME_L, num_labels=1, problem_type='regression')\n",
        "    if hasattr(model.config, 'hidden_dropout_prob'): model.config.hidden_dropout_prob = 0.15\n",
        "    if hasattr(model.config, 'attention_probs_dropout_prob'): model.config.attention_probs_dropout_prob = 0.15\n",
        "    model.gradient_checkpointing_enable(); model.to(device)\n",
        "\n",
        "    train_ds = TrainHTDatasetL([train_ids_all[i] for i in tr_idx], train_df.loc[tr_idx, target_col].tolist())\n",
        "    valid_ds_ht = TrainHTDatasetL([train_ids_all[i] for i in va_idx], train_df.loc[va_idx, target_col].tolist())\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'outputsL_fold{f}',\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=32,\n",
        "        gradient_accumulation_steps=8,\n",
        "        evaluation_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        learning_rate=1.1e-5,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type='linear',\n",
        "        warmup_ratio=0.10,\n",
        "        bf16=True,\n",
        "        bf16_full_eval=True,\n",
        "        logging_steps=50,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='qwk',\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=4,\n",
        "        dataloader_pin_memory=True,\n",
        "        dataloader_num_workers=2,\n",
        "        report_to=[]\n",
        "    )\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        preds = np.clip(eval_pred.predictions.reshape(-1), min_score, max_score)\n",
        "        labels = eval_pred.label_ids.reshape(-1)\n",
        "        base_th = np.array([1.5,2.5,3.5,4.5,5.5])\n",
        "        bins = [-np.inf] + base_th.tolist() + [np.inf]\n",
        "        pred_int = np.digitize(preds, bins)\n",
        "        labels_int = labels.astype(int)\n",
        "        return {'qwk': qwk_int(labels_int, pred_int)}\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=valid_ds_ht,\n",
        "        data_collator=collate_fn_l,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "    # Build validation datasets for views\n",
        "    val_chunks_64 = ChunkDatasetL([train_ids_all[i] for i in va_idx], stride=64)\n",
        "    val_chunks_128 = ChunkDatasetL([train_ids_all[i] for i in va_idx], stride=128)\n",
        "    val_ht = HTInferDatasetL([train_ids_all[i] for i in va_idx])\n",
        "    essay_idx_v64 = np.array(val_chunks_64.essay_idx, dtype=np.int64); weights_v64 = np.array(val_chunks_64.weights, dtype=np.float32)\n",
        "    essay_idx_v128 = np.array(val_chunks_128.essay_idx, dtype=np.int64); weights_v128 = np.array(val_chunks_128.weights, dtype=np.float32)\n",
        "\n",
        "    # Post-hoc checkpoint selection with SW64 QWK (last 3-4 checkpoints)\n",
        "    chk_dir = args.output_dir\n",
        "    ckpts = []\n",
        "    if os.path.isdir(chk_dir):\n",
        "        for d in os.listdir(chk_dir):\n",
        "            if d.startswith('checkpoint-'):\n",
        "                try: step = int(d.split('-')[-1])\n",
        "                except: step = -1\n",
        "                ckpts.append((step, os.path.join(chk_dir, d)))\n",
        "    ckpts.sort()\n",
        "    candidates = [p for _, p in ckpts[-4:]] if ckpts else []\n",
        "    best_q = -1.0; best_path = None; best_val64=None; best_val128=None; best_valht=None\n",
        "\n",
        "    def eval_view(model_path, ds, aggregate, essay_idx, weights=None, n_items=None):\n",
        "        m = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=1, problem_type='regression').to(device)\n",
        "        t = Trainer(model=m, args=args, data_collator=collate_fn_l)\n",
        "        with torch.no_grad():\n",
        "            flat = t.predict(ds).predictions.reshape(-1)\n",
        "        flat = np.clip(flat, min_score, max_score)\n",
        "        if aggregate:\n",
        "            return length_weighted_aggregate(flat, essay_idx, weights, n_items).astype(np.float32)\n",
        "        else:\n",
        "            return flat.astype(np.float32)\n",
        "\n",
        "    for path in candidates if candidates else [None]:\n",
        "        if path is None:\n",
        "            continue\n",
        "        val64 = eval_view(path, val_chunks_64, True, essay_idx_v64, weights_v64, len(va_idx))\n",
        "        base_th = np.array([1.5,2.5,3.5,4.5,5.5])\n",
        "        bins = [-np.inf] + base_th.tolist() + [np.inf]\n",
        "        q = qwk_int(y[va_idx], np.digitize(val64, bins))\n",
        "        if q > best_q:\n",
        "            best_q = q; best_path = path; best_val64 = val64\n",
        "            best_val128 = eval_view(path, val_chunks_128, True, essay_idx_v128, weights_v128, len(va_idx))\n",
        "            # HT single-view\n",
        "            mht = AutoModelForSequenceClassification.from_pretrained(path, num_labels=1, problem_type='regression').to(device)\n",
        "            tht = Trainer(model=mht, args=args, data_collator=collate_fn_l)\n",
        "            with torch.no_grad():\n",
        "                flat_ht = tht.predict(val_ht).predictions.reshape(-1)\n",
        "            best_valht = np.clip(flat_ht, min_score, max_score).astype(np.float32)\n",
        "\n",
        "    assert best_path is not None and best_val64 is not None, f'[v3-large] No valid checkpoint for fold {f}'\n",
        "\n",
        "    # Assign OOF\n",
        "    oof_64_l[va_idx] = np.clip(best_val64, min_score, max_score)\n",
        "    oof_128_l[va_idx] = np.clip(best_val128, min_score, max_score)\n",
        "    oof_ht_l[va_idx] = np.clip(best_valht, min_score, max_score)\n",
        "\n",
        "    # Test inference with best checkpoint for all views\n",
        "    best_model = AutoModelForSequenceClassification.from_pretrained(best_path, num_labels=1, problem_type='regression').to(device)\n",
        "    t_common = Trainer(model=best_model, args=args, data_collator=collate_fn_l)\n",
        "    with torch.no_grad():\n",
        "        flat64 = t_common.predict(test_chunks_64_l).predictions.reshape(-1)\n",
        "        flat128 = t_common.predict(test_chunks_128_l).predictions.reshape(-1)\n",
        "        flatht = t_common.predict(test_ht_l).predictions.reshape(-1)\n",
        "    flat64 = np.clip(flat64, min_score, max_score); flat128 = np.clip(flat128, min_score, max_score); flatht = np.clip(flatht, min_score, max_score)\n",
        "    test_pred_f64_l[:, f] = length_weighted_aggregate(flat64, essay_idx_t64_l, weights_t64_l, len(test_df)).astype(np.float32)\n",
        "    test_pred_f128_l[:, f] = length_weighted_aggregate(flat128, essay_idx_t128_l, weights_t128_l, len(test_df)).astype(np.float32)\n",
        "    test_pred_fht_l[:, f] = flatht.astype(np.float32)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f'[v3-large] Fold {f} done in {time.time()-fold_t0:.1f}s (best SW64 QWK={best_q:.5f})', flush=True)\n",
        "\n",
        "# Save per-view OOF and test for large seed prefix 'sL'\n",
        "pd.DataFrame({'essay_id': train_df[id_col], 'oof_deberta': oof_64_l, 'y': y}).to_csv('oof_deberta_base_sL_sw64.csv', index=False)\n",
        "pd.DataFrame({'essay_id': train_df[id_col], 'oof_deberta': oof_128_l, 'y': y}).to_csv('oof_deberta_base_sL_sw128.csv', index=False)\n",
        "pd.DataFrame({'essay_id': train_df[id_col], 'oof_deberta': oof_ht_l, 'y': y}).to_csv('oof_deberta_base_sL_ht.csv', index=False)\n",
        "np.save('test_deberta_base_sL_sw64.npy', test_pred_f64_l.mean(axis=1))\n",
        "np.save('test_deberta_base_sL_sw128.npy', test_pred_f128_l.mean(axis=1))\n",
        "np.save('test_deberta_base_sL_ht.npy', test_pred_fht_l.mean(axis=1))\n",
        "\n",
        "# Also write a default combined with a conservative HT cap (0.55,0.30,0.15) for convenience (final bagger will re-search masked TTA)\n",
        "oof_tta_l = (0.55*oof_64_l + 0.30*oof_128_l + 0.15*oof_ht_l).astype(np.float32)\n",
        "test_tta_l = (0.55*test_pred_f64_l.mean(axis=1) + 0.30*test_pred_f128_l.mean(axis=1) + 0.15*test_pred_fht_l.mean(axis=1)).astype(np.float32)\n",
        "pd.DataFrame({'essay_id': train_df[id_col], 'oof_deberta': oof_tta_l, 'y': y}).to_csv('oof_deberta_base_sL.csv', index=False)\n",
        "np.save('test_deberta_base_sL.npy', test_tta_l)\n",
        "print('[v3-large] Saved per-view and combined TTA artifacts for sL (partial folds).', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "e6f12e9b-804d-4302-9276-5b378d7fd3b1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Rebuild DeBERTa-v3-Large (sL) per-view and combined artifacts from checkpoints across available folds (no retraining)\n",
        "import os, time, numpy as np, pandas as pd, torch\n",
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals() and 'folds_df' in globals(), 'Run setup cells first.'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "min_score, max_score = 1.0, 6.0\n",
        "n_splits = int(folds_df['fold'].max()) + 1\n",
        "y_int = train_df[target_col].astype(int).values\n",
        "\n",
        "MODEL_NAME_L = 'microsoft/deberta-v3-large'\n",
        "MAX_LEN_L = 512\n",
        "HEAD_TOKENS_L = 256\n",
        "\n",
        "def qwk_int(y_true, y_pred_int):\n",
        "    return cohen_kappa_score(y_true, y_pred_int, weights='quadratic')\n",
        "\n",
        "# Large tokenizer\n",
        "if 'tok_l' not in globals():\n",
        "    tok_l = AutoTokenizer.from_pretrained(MODEL_NAME_L)\n",
        "\n",
        "# Ensure token ids are available (build with large tokenizer for consistency if missing)\n",
        "if 'train_ids_all' not in globals() or 'test_ids_all' not in globals():\n",
        "    tok_train = tok_l(train_df[text_col].tolist(), add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)\n",
        "    tok_test  = tok_l(test_df[text_col].tolist(),  add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)\n",
        "    train_ids_all = tok_train['input_ids']\n",
        "    test_ids_all  = tok_test['input_ids']\n",
        "\n",
        "def chunkify_l(ids, max_len=MAX_LEN_L, stride=64):\n",
        "    usable = max_len - 2\n",
        "    if len(ids) <= usable:\n",
        "        chunks = [ids]\n",
        "    else:\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        while start < len(ids):\n",
        "            end = min(start + usable, len(ids))\n",
        "            chunks.append(ids[start:end])\n",
        "            if end == len(ids): break\n",
        "            start += stride\n",
        "    input_ids = []; attn = []; weights = []\n",
        "    for ch in chunks:\n",
        "        built = tok_l.build_inputs_with_special_tokens(ch)\n",
        "        padded = tok_l.pad({'input_ids':[built]}, padding='max_length', max_length=max_len, return_tensors='pt')\n",
        "        input_ids.append(padded['input_ids'][0]); attn.append(padded['attention_mask'][0]); weights.append(float(len(ch)))\n",
        "    return input_ids, attn, weights\n",
        "\n",
        "class ChunkDatasetL(Dataset):\n",
        "    def __init__(self, ids_list, stride):\n",
        "        self.inputs=[]; self.attns=[]; self.essay_idx=[]; self.weights=[]\n",
        "        for i, ids in enumerate(ids_list):\n",
        "            inp, att, w = chunkify_l(ids, MAX_LEN_L, stride=stride)\n",
        "            self.inputs.extend(inp); self.attns.extend(att);\n",
        "            self.essay_idx.extend([i]*len(inp)); self.weights.extend(w)\n",
        "    def __len__(self): return len(self.inputs)\n",
        "    def __getitem__(self, idx):\n",
        "        return {'input_ids': self.inputs[idx], 'attention_mask': self.attns[idx]}\n",
        "\n",
        "class HTInferDatasetL(Dataset):\n",
        "    def __init__(self, ids_list, head_tokens=HEAD_TOKENS_L):\n",
        "        self.inputs=[]; self.attns=[]; self.essay_idx=[]\n",
        "        tail = MAX_LEN_L - 2 - head_tokens\n",
        "        for i, ids in enumerate(ids_list):\n",
        "            if len(ids) <= MAX_LEN_L - 2:\n",
        "                core = ids\n",
        "            else:\n",
        "                core = ids[:head_tokens] + (ids[-tail:] if tail>0 else [])\n",
        "            built = tok_l.build_inputs_with_special_tokens(core)\n",
        "            padded = tok_l.pad({'input_ids':[built]}, padding='max_length', max_length=MAX_LEN_L, return_tensors='pt')\n",
        "            self.inputs.append(padded['input_ids'][0]); self.attns.append(padded['attention_mask'][0]); self.essay_idx.append(i)\n",
        "    def __len__(self): return len(self.inputs)\n",
        "    def __getitem__(self, idx):\n",
        "        return {'input_ids': self.inputs[idx], 'attention_mask': self.attns[idx]}\n",
        "\n",
        "def collate_fn_l(batch):\n",
        "    return {k: torch.stack([x[k] for x in batch]) for k in batch[0].keys()}\n",
        "\n",
        "def length_weighted_aggregate(flat_preds, essay_idx, weights, n_items):\n",
        "    agg = np.zeros(n_items, dtype=np.float32); wsum = np.zeros(n_items, dtype=np.float32)\n",
        "    np.add.at(agg, essay_idx, flat_preds * weights); np.add.at(wsum, essay_idx, weights)\n",
        "    return agg / np.clip(wsum, 1e-6, None)\n",
        "\n",
        "# Build test datasets for three views\n",
        "test_sw64 = ChunkDatasetL(test_ids_all, stride=64)\n",
        "test_sw128 = ChunkDatasetL(test_ids_all, stride=128)\n",
        "test_ht = HTInferDatasetL(test_ids_all, head_tokens=HEAD_TOKENS_L)\n",
        "essay_idx_t64 = np.array(test_sw64.essay_idx, dtype=np.int64); weights_t64 = np.array(test_sw64.weights, dtype=np.float32)\n",
        "essay_idx_t128 = np.array(test_sw128.essay_idx, dtype=np.int64); weights_t128 = np.array(test_sw128.weights, dtype=np.float32)\n",
        "\n",
        "# Holders\n",
        "oof_64 = np.zeros(len(train_df), dtype=np.float32)\n",
        "oof_128 = np.zeros(len(train_df), dtype=np.float32)\n",
        "oof_ht = np.zeros(len(train_df), dtype=np.float32)\n",
        "test_pred_f64 = np.zeros((len(test_df), n_splits), dtype=np.float32)\n",
        "test_pred_f128 = np.zeros((len(test_df), n_splits), dtype=np.float32)\n",
        "test_pred_fht = np.zeros((len(test_df), n_splits), dtype=np.float32)\n",
        "\n",
        "# Folds to evaluate: those with outputsL_fold{f} present\n",
        "folds_to_eval = [f for f in range(n_splits) if os.path.isdir(f'outputsL_fold{f}')]\n",
        "print('[sL-rebuild] Evaluating folds:', folds_to_eval, flush=True)\n",
        "\n",
        "for f in folds_to_eval:\n",
        "    va_idx = folds_df.index[folds_df['fold']==f].to_numpy()\n",
        "    # Build validation datasets\n",
        "    val_sw64 = ChunkDatasetL([train_ids_all[i] for i in va_idx], stride=64)\n",
        "    val_sw128 = ChunkDatasetL([train_ids_all[i] for i in va_idx], stride=128)\n",
        "    val_ht = HTInferDatasetL([train_ids_all[i] for i in va_idx], head_tokens=HEAD_TOKENS_L)\n",
        "    essay_idx_v64 = np.array(val_sw64.essay_idx, dtype=np.int64); weights_v64 = np.array(val_sw64.weights, dtype=np.float32)\n",
        "    essay_idx_v128 = np.array(val_sw128.essay_idx, dtype=np.int64); weights_v128 = np.array(val_sw128.weights, dtype=np.float32)\n",
        "\n",
        "    chk_dir = f'outputsL_fold{f}'\n",
        "    ckpts = []\n",
        "    for d in os.listdir(chk_dir):\n",
        "        if d.startswith('checkpoint-'):\n",
        "            try: step = int(d.split('-')[-1])\n",
        "            except: step = -1\n",
        "            ckpts.append((step, os.path.join(chk_dir, d)))\n",
        "    ckpts.sort()\n",
        "    candidates = [p for _, p in ckpts[-4:]] if ckpts else []\n",
        "    assert candidates, f'No checkpoints found for large fold {f}'\n",
        "\n",
        "    args = TrainingArguments(output_dir=chk_dir, per_device_eval_batch_size=32, dataloader_num_workers=2, bf16_full_eval=True, report_to=[])\n",
        "\n",
        "    def eval_view(model_path, ds, aggregate, essay_idx, weights=None, n_items=None):\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=1, problem_type='regression').to(device)\n",
        "        trainer = Trainer(model=model, args=args, data_collator=collate_fn_l)\n",
        "        with torch.no_grad():\n",
        "            flat = trainer.predict(ds).predictions.reshape(-1)\n",
        "        flat = np.clip(flat, min_score, max_score)\n",
        "        if aggregate:\n",
        "            return length_weighted_aggregate(flat, essay_idx, weights, n_items).astype(np.float32)\n",
        "        else:\n",
        "            return flat.astype(np.float32)\n",
        "\n",
        "    best_q = -1.0; best_path = None; best_val64=None; best_val128=None; best_valht=None\n",
        "    base_bins = [-np.inf,1.5,2.5,3.5,4.5,5.5,np.inf]\n",
        "    for path in candidates:\n",
        "        val64 = eval_view(path, val_sw64, True, essay_idx_v64, weights_v64, len(va_idx))\n",
        "        q = qwk_int(y_int[va_idx], np.digitize(val64, base_bins))\n",
        "        if q > best_q:\n",
        "            best_q = q; best_path = path; best_val64 = val64\n",
        "            best_val128 = eval_view(path, val_sw128, True, essay_idx_v128, weights_v128, len(va_idx))\n",
        "            # HT single-view\n",
        "            model_ht = AutoModelForSequenceClassification.from_pretrained(path, num_labels=1, problem_type='regression').to(device)\n",
        "            trainer_ht = Trainer(model=model_ht, args=args, data_collator=collate_fn_l)\n",
        "            with torch.no_grad():\n",
        "                flat_ht = trainer_ht.predict(val_ht).predictions.reshape(-1)\n",
        "            best_valht = np.clip(flat_ht, min_score, max_score).astype(np.float32)\n",
        "\n",
        "    # Assign OOF for this fold\n",
        "    oof_64[va_idx] = np.clip(best_val64, min_score, max_score)\n",
        "    oof_128[va_idx] = np.clip(best_val128, min_score, max_score)\n",
        "    oof_ht[va_idx] = np.clip(best_valht, min_score, max_score)\n",
        "\n",
        "    # Test inference from best checkpoint\n",
        "    best_model = AutoModelForSequenceClassification.from_pretrained(best_path, num_labels=1, problem_type='regression').to(device)\n",
        "    trainer = Trainer(model=best_model, args=args, data_collator=collate_fn_l)\n",
        "    with torch.no_grad():\n",
        "        flat64 = trainer.predict(test_sw64).predictions.reshape(-1)\n",
        "        flat128 = trainer.predict(test_sw128).predictions.reshape(-1)\n",
        "        flatht = trainer.predict(test_ht).predictions.reshape(-1)\n",
        "    flat64 = np.clip(flat64, min_score, max_score); flat128 = np.clip(flat128, min_score, max_score); flatht = np.clip(flatht, min_score, max_score)\n",
        "    test_pred_f64[:, f] = length_weighted_aggregate(flat64, essay_idx_t64, weights_t64, len(test_df)).astype(np.float32)\n",
        "    test_pred_f128[:, f] = length_weighted_aggregate(flat128, essay_idx_t128, weights_t128, len(test_df)).astype(np.float32)\n",
        "    test_pred_fht[:, f] = flatht.astype(np.float32)\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f'[sL-rebuild] Fold {f} best SW64 QWK={best_q:.5f}', flush=True)\n",
        "\n",
        "# Save per-view and combined artifacts for sL (partial folds supported)\n",
        "pd.DataFrame({'essay_id': train_df[id_col], 'oof_deberta': oof_64, 'y': y_int}).to_csv('oof_deberta_base_sL_sw64.csv', index=False)\n",
        "pd.DataFrame({'essay_id': train_df[id_col], 'oof_deberta': oof_128, 'y': y_int}).to_csv('oof_deberta_base_sL_sw128.csv', index=False)\n",
        "pd.DataFrame({'essay_id': train_df[id_col], 'oof_deberta': oof_ht, 'y': y_int}).to_csv('oof_deberta_base_sL_ht.csv', index=False)\n",
        "np.save('test_deberta_base_sL_sw64.npy', test_pred_f64.mean(axis=1))\n",
        "np.save('test_deberta_base_sL_sw128.npy', test_pred_f128.mean(axis=1))\n",
        "np.save('test_deberta_base_sL_ht.npy', test_pred_fht.mean(axis=1))\n",
        "\n",
        "# Conservative default TTA mix; bagging cell will re-opt with masks\n",
        "oof_tta = (0.55*oof_64 + 0.30*oof_128 + 0.15*oof_ht).astype(np.float32)\n",
        "test_tta = (0.55*test_pred_f64.mean(axis=1) + 0.30*test_pred_f128.mean(axis=1) + 0.15*test_pred_fht.mean(axis=1)).astype(np.float32)\n",
        "pd.DataFrame({'essay_id': train_df[id_col], 'oof_deberta': oof_tta, 'y': y_int}).to_csv('oof_deberta_base_sL.csv', index=False)\n",
        "np.save('test_deberta_base_sL.npy', test_tta)\n",
        "print('[sL-rebuild] Saved per-view and combined artifacts for sL from existing checkpoints.', flush=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[sL-rebuild] Evaluating folds: [0, 1, 4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/176 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/145 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='98' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/98 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/176 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/176 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/95 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/79 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/55 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[sL-rebuild] Fold 0 best SW64 QWK=0.78923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/175 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/145 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='98' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/98 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/175 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/145 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='98' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/98 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/175 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/95 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/79 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/55 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[sL-rebuild] Fold 1 best SW64 QWK=0.76851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/176 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/145 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='98' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/98 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/176 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/145 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='98' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/98 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  1/176 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/95 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/79 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/55 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[sL-rebuild] Fold 4 best SW64 QWK=0.76655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[sL-rebuild] Saved per-view and combined artifacts for sL from existing checkpoints.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}